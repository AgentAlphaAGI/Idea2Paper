{
  "title": "Adaptive Domain Refinery: Co-evolving LLMs with Domain Standards for Trainable Data Quality",
  "abstract": "We transform domain data extraction and cleaning from brittle, heuristic-driven pipelines into an adaptive domain refinery where LLM and domain standards co-evolve under severe compute constraints. Our core insight treats domain data quality as a trainable objective, unifying model adaptation and domain specification into a single theory-guided search. We implement this vision through three interconnected mechanisms: an NTK-informed curriculum extended to Adam that provides differentiable forecasts of fine-tuning dynamics, enabling constraint-aware pruning and efficient single-GPU training; Globally Unique Movement (GUM) neuron prioritization that preserves domain-critical signals while eliminating redundancy, letting domain heuristics emerge as model structure rather than handcrafted rules; and a meta-objective that steers coevolution over prompts, data samples, and cleaning rules toward configurations that maximize measurable clean-extract performance. The result is an executable domain refinery that produces both high-quality domain data and the evolved domain standards themselves, democratizing sophisticated domain data workflows through principled co-optimization rather than manual engineering.",
  "problem_framing": "We reframe domain data extraction and cleaning from brittle, pipeline construction to trainable domain design search. Traditional approaches treat extraction and cleaning as separate engineering steps requiring handcrafted heuristics or brute-force model scaling, breaking under domain shift and lacking principled quality metrics. Under severe compute constraints, we seek data-model configurations that co-evolve to maximize measurable extraction/cleaning success, turning data handling itself into optimization rather than manual engineering. This reframing transforms the task from building pipelines to designing adaptive systems where model and domain mutually constrain and enhance each other through shared trainable objectives.",
  "gap_pattern": "Existing domain data pipelines suffer from fundamental conceptual fragmentation: they separate model training from domain specification, treat data quality as external to the learning process, and rely on brittle heuristics that cannot adapt to domain shift. Current methods lack a unifying principle that makes domain standards trainable alongside model parameters, preventing principled optimization under compute constraints. Without a differentiable forecast of training dynamics and a meta-objective for data-model co-optimization, these approaches cannot achieve the adaptive, co-evolutionary behavior necessary for robust domain refinement. The absence of theory-guided coevolution forces practitioners to choose between manual engineering and computational brute force, limiting both accessibility and principled quality guarantees.",
  "solution": "Our solution unifies model training and domain refinement into an adaptive domain refinery through theory-guided coevolution. We transform the problem from pipeline construction to domain design search by making domain data quality a trainable objective that co-evolves with the LLM. An NTK-informed curriculum, extended to Adam, provides differentiable forecasts of fine-tuning dynamics, enabling early constraint-aware pruning and efficient single-GPU training. GUM-based neuron prioritization preserves domain-critical signals while removing redundant knowledge, allowing domain heuristics to emerge naturally as model structure rather than requiring manual specification. A meta-objective steers joint optimization over prompts, data samples, and cleaning rules, driving the system toward data configurations that maximize measurable clean-extract performance. This creates an executable domain refinery that outputs both refined domain data and evolved domain standards, fundamentally shifting from manual engineering to principled co-optimization under computational constraints.",
  "method_skeleton": "Constraint-aware pretraining pipeline: Re-analyze and modify transformer pretraining to optimize memory and compute under single-GPU, one-day constraints while retaining domain-relevant linguistic priors and enabling later coevolution; NTK-guided curriculum extension to Adam: Extend Neural Tangent Kernel theory to Adam optimizer to provide differentiable forecasts of fine-tuning dynamics, guiding constraint-aware pruning schedules and curriculum design for stable, parameter-efficient adaptation; GUM neuron prioritization: Select neurons based on globally unique movement patterns and sensitivity to domain-specific signals, preserving critical knowledge while eliminating redundancy, enabling domain heuristics to emerge as model structure; Meta-objective coevolution: Define measurable quality objectives and co-optimize prompts, data samples, cleaning rules with the LLM, steering joint search toward high-quality domain configurations; End-to-end adaptive refinery integration: Unify constrained pretraining, NTK-guided fine-tuning, and GUM pruning into a single adaptive system that outputs an executable domain refinery rather than static pipelines.",
  "innovation_claims": [
    "Transform domain data extraction and cleaning from pipeline construction to trainable domain design search by making domain data quality a trainable objective and co-evolving LLM with domain standards through a meta-objective that maximizes measurable clean-extract performance, turning heuristics into emergent model structure rather than handcrafted rules",
    "Reframe fine-tuning under severe constraints by extending Neural Tangent Kernel theory to Adam optimizer to provide differentiable forecasts of training dynamics, enabling constraint-aware curriculum design and early pruning that deliver stable, parameter-efficient adaptation on a single GPU within one day while preserving coevolutionary flexibility",
    "Shift from dense, monolithic architectures to adaptive coevolution by introducing Globally Unique Movement (GUM) neuron prioritization that preserves domain-critical signals while eliminating redundancy, allowing domain heuristics to emerge as model structure and enabling principled data-model co-optimization under computational limits"
  ],
  "experiments_plan": "We evaluate our adaptive domain refinery on three challenging domains: biomedical literature extraction, legal document cleaning, and software repository mining. For each domain, we compare against state-of-the-art pipelines including rule-based systems, fine-tuned language models, and hybrid approaches. Metrics include extraction accuracy, cleaning quality scores, domain adaptation speed, and compute efficiency. We conduct ablation studies on the NTK-informed curriculum, GUM prioritization, and meta-objective components. Additional experiments demonstrate the refinery's ability to generalize to unseen domains and its robustness under varying computational constraints. We provide both quantitative performance results and qualitative analysis of the evolved domain standards to validate our conceptual reframing."
}
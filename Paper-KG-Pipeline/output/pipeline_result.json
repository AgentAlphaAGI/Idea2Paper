{
  "user_idea": "LLM-Assisted Domain Data Extraction and Cleaning",
  "success": true,
  "iterations": 3,
  "selected_patterns": {
    "stability": [
      "pattern_74",
      "pattern_51",
      "pattern_73",
      "pattern_112",
      "pattern_40"
    ],
    "novelty": [
      "pattern_107",
      "pattern_69",
      "pattern_112",
      "pattern_45",
      "pattern_101"
    ],
    "domain_distance": [
      "pattern_73",
      "pattern_40",
      "pattern_112",
      "pattern_74",
      "pattern_101"
    ]
  },
  "final_story": {
    "title": "Adaptive Domain Refinery: Co-evolving LLMs with Domain Standards for Trainable Data Quality",
    "abstract": "We transform domain data extraction and cleaning from brittle, heuristic-driven pipelines into an adaptive domain refinery where LLM and domain standards co-evolve under severe compute constraints. Our core insight treats domain data quality as a trainable objective, unifying model adaptation and domain specification into a single theory-guided search. We implement this vision through three interconnected mechanisms: an NTK-informed curriculum extended to Adam that provides differentiable forecasts of fine-tuning dynamics, enabling constraint-aware pruning and efficient single-GPU training; Globally Unique Movement (GUM) neuron prioritization that preserves domain-critical signals while eliminating redundancy, letting domain heuristics emerge as model structure rather than handcrafted rules; and a meta-objective that steers coevolution over prompts, data samples, and cleaning rules toward configurations that maximize measurable clean-extract performance. The result is an executable domain refinery that produces both high-quality domain data and the evolved domain standards themselves, democratizing sophisticated domain data workflows through principled co-optimization rather than manual engineering.",
    "problem_framing": "We reframe domain data extraction and cleaning from brittle, pipeline construction to trainable domain design search. Traditional approaches treat extraction and cleaning as separate engineering steps requiring handcrafted heuristics or brute-force model scaling, breaking under domain shift and lacking principled quality metrics. Under severe compute constraints, we seek data-model configurations that co-evolve to maximize measurable extraction/cleaning success, turning data handling itself into optimization rather than manual engineering. This reframing transforms the task from building pipelines to designing adaptive systems where model and domain mutually constrain and enhance each other through shared trainable objectives.",
    "gap_pattern": "Existing domain data pipelines suffer from fundamental conceptual fragmentation: they separate model training from domain specification, treat data quality as external to the learning process, and rely on brittle heuristics that cannot adapt to domain shift. Current methods lack a unifying principle that makes domain standards trainable alongside model parameters, preventing principled optimization under compute constraints. Without a differentiable forecast of training dynamics and a meta-objective for data-model co-optimization, these approaches cannot achieve the adaptive, co-evolutionary behavior necessary for robust domain refinement. The absence of theory-guided coevolution forces practitioners to choose between manual engineering and computational brute force, limiting both accessibility and principled quality guarantees.",
    "solution": "Our solution unifies model training and domain refinement into an adaptive domain refinery through theory-guided coevolution. We transform the problem from pipeline construction to domain design search by making domain data quality a trainable objective that co-evolves with the LLM. An NTK-informed curriculum, extended to Adam, provides differentiable forecasts of fine-tuning dynamics, enabling early constraint-aware pruning and efficient single-GPU training. GUM-based neuron prioritization preserves domain-critical signals while removing redundant knowledge, allowing domain heuristics to emerge naturally as model structure rather than requiring manual specification. A meta-objective steers joint optimization over prompts, data samples, and cleaning rules, driving the system toward data configurations that maximize measurable clean-extract performance. This creates an executable domain refinery that outputs both refined domain data and evolved domain standards, fundamentally shifting from manual engineering to principled co-optimization under computational constraints.",
    "method_skeleton": "Constraint-aware pretraining pipeline: Re-analyze and modify transformer pretraining to optimize memory and compute under single-GPU, one-day constraints while retaining domain-relevant linguistic priors and enabling later coevolution; NTK-guided curriculum extension to Adam: Extend Neural Tangent Kernel theory to Adam optimizer to provide differentiable forecasts of fine-tuning dynamics, guiding constraint-aware pruning schedules and curriculum design for stable, parameter-efficient adaptation; GUM neuron prioritization: Select neurons based on globally unique movement patterns and sensitivity to domain-specific signals, preserving critical knowledge while eliminating redundancy, enabling domain heuristics to emerge as model structure; Meta-objective coevolution: Define measurable quality objectives and co-optimize prompts, data samples, cleaning rules with the LLM, steering joint search toward high-quality domain configurations; End-to-end adaptive refinery integration: Unify constrained pretraining, NTK-guided fine-tuning, and GUM pruning into a single adaptive system that outputs an executable domain refinery rather than static pipelines.",
    "innovation_claims": [
      "Transform domain data extraction and cleaning from pipeline construction to trainable domain design search by making domain data quality a trainable objective and co-evolving LLM with domain standards through a meta-objective that maximizes measurable clean-extract performance, turning heuristics into emergent model structure rather than handcrafted rules",
      "Reframe fine-tuning under severe constraints by extending Neural Tangent Kernel theory to Adam optimizer to provide differentiable forecasts of training dynamics, enabling constraint-aware curriculum design and early pruning that deliver stable, parameter-efficient adaptation on a single GPU within one day while preserving coevolutionary flexibility",
      "Shift from dense, monolithic architectures to adaptive coevolution by introducing Globally Unique Movement (GUM) neuron prioritization that preserves domain-critical signals while eliminating redundancy, allowing domain heuristics to emerge as model structure and enabling principled data-model co-optimization under computational limits"
    ],
    "experiments_plan": "We evaluate our adaptive domain refinery on three challenging domains: biomedical literature extraction, legal document cleaning, and software repository mining. For each domain, we compare against state-of-the-art pipelines including rule-based systems, fine-tuned language models, and hybrid approaches. Metrics include extraction accuracy, cleaning quality scores, domain adaptation speed, and compute efficiency. We conduct ablation studies on the NTK-informed curriculum, GUM prioritization, and meta-objective components. Additional experiments demonstrate the refinery's ability to generalize to unseen domains and its robustness under varying computational constraints. We provide both quantitative performance results and qualitative analysis of the evolved domain standards to validate our conceptual reframing."
  },
  "review_history": [
    {
      "pass": false,
      "avg_score": 4.61,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 2.33,
          "feedback": "1. 技术创新性与原创性 (3.0分): 该论文提出的受限预训练、基于神经切线核（NTK）的微调以及全局唯一运动（GUM）剪枝在概念上并未提供显著的原创性。受限预训练本身是资源受限场景下的常见做法；NTK 已在无限宽度网络的理论分析中被广泛讨论，直接将其扩展到 Adam 优化器的做法缺乏新颖的理论贡献；GUM 剪枝本质上与已有的稀疏化方法（如幅度剪枝、结构化剪枝）相似，且未给出与现有方法的明确对比或创新点。整体上，论文的创新性不足以支撑其在顶级会议上的技术贡献。\n2. 方法论严谨性与可实现性 (2.0分): 论文对关键算法细节的描述极其简略，缺乏数学公式、伪代码或实现细节。NTK 指导微调的具体实现方式、稳定性预测模型以及参数高效适应的推导过程未给出；GUM 剪枝的“唯一性”和“敏感性”度量标准未定义，导致方法无法复现。论文声称能够在单 GPU、一天训练约束下完成预训练和微调，但未提供任何计算资源、内存占用或时间复杂度的量化分析，缺乏可实现性验证。\n3. 实验验证与结果可信度 (2.0分): 论文仅提供了实验计划，未呈现任何实验结果或定量指标。缺乏对所提方法在法律、生物医学、电子商务等真实数据集上的性能评估，也没有与强基线（如规则抽取器、密集微调 LLM）的对比实验。缺少消融实验来验证受限预训练、NTK 指导微调和 GUM 剪枝各自贡献。论文声称在单 GPU、一天训练约束下取得“显著性能”，但未提供任何实验数据或误差分析，导致其可信度极低。\n\n总结: 综上所述，该论文在技术创新性、方法论严谨性和实验验证方面均存在严重不足，未能提供足够的技术细节和实证支撑，难以满足顶级会议对技术合理性的要求。因此，建议予以拒绝（Reject）。"
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 5.0,
          "feedback": "1. 技术创新性 (4.5分): 该工作将受限预训练、NTK 引导微调以及 GUM 剪枝三项已有技术进行组合，缺乏根本性的方法创新。组合本身在近两年的顶会中已出现多次，且未提供新的理论或实现细节。\n2. 理论创新性 (5.0分): 将 NTK 扩展至 Adam 的尝试有一定新意，但本质上是对已有 NTK 分析的轻微延伸；GUM 剪枝本质上仍是基于激活唯一性和敏感度的结构化剪枝，缺乏深度理论贡献。\n3. 应用创新性 (5.5分): 将 LLM 辅助用于领域数据抽取和清洗并非新颖，已有大量工作探索类似场景；论文未提出新的任务定义或新的领域适配策略，创新程度有限。\n\n总结: 综合来看，本文在技术、理论和应用三个维度的创新性均未达到顶级会议的要求，整体创新性不足，建议拒稿。"
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 6.5,
          "feedback": "1. 问题定义和动机清晰度 (3.0分): 论文标题和摘要提到了'LLM-assisted domain data extraction and cleaning'，但问题定义部分完全缺失（仅有标题而无内容），导致读者无法理解论文要解决的具体问题（如现有方法的不足、目标场景等）。动机部分在摘要中仅笼统提及'prioritize efficiency and accessibility'，但缺乏详细背景（如资源约束或领域特定挑战），贡献点虽隐含现有启发式方法的缺陷，但未系统阐述问题。这破坏了叙事的起点，使得后续方法部分缺乏上下文连贯性。\n2. 方法描述的连贯性和完整性 (6.0分): 方法概述详细描述了三个组件（重构预训练、NTK引导微调、GUM剪枝），贡献点与之一一对应（从启发式到LLM辅助、理论驱动微调、自适应稀疏性），实验计划包含消融研究验证每个组件，整体逻辑一致。但问题定义缺失导致方法部分孤立，未充分解释为什么这些组件能解决特定问题（如NTK扩展到Adam的合理性未详述），削弱了叙事完整性。\n3. 实验验证的充分性 (7.0分): 实验计划全面，包括三个真实数据集、基线比较（规则基础和LLM）、多指标评估（准确性、吞吐量、能耗）和消融研究，且强调单GPU一天约束下的性能，验证了方法的可扩展性。但问题定义缺失可能影响实验针对性（如未明确领域特定挑战），且方法细节不足（如GUM剪枝的具体实现），导致实验验证部分不够严谨。\n\n总结: 论文在方法设计和实验计划上表现出一定连贯性，但问题定义严重缺失是核心缺陷，导致叙事不完整。整体评分6.5分，属于及格但不足的水平（低于8分优秀），主要因问题定义薄弱和方法细节不足。"
        }
      ],
      "main_issue": "stability",
      "suggestions": [
        "从stability维度选择稳健Pattern",
        "注入成熟方法增强鲁棒性"
      ]
    },
    {
      "pass": false,
      "avg_score": 5.0,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 4.0,
          "feedback": "1. 维度A (4.0分): 理由：NTK 理论向 Adam 优化器的扩展缺乏严谨的数学推导，NTK 假设无限宽网络且使用梯度下降，而 Adam 引入动量和自适应学习率，导致理论保证不明确。GUM（Globally Unique Movement）神经元的“全局唯一运动模式”概念未给出可操作的定义或计算方法，声称域启发式会自然涌现为模型结构缺乏形式化论证。Meta‑objective 的协同演化仅在概念层面描述，未提供具体的优化目标、收敛性分析或算法细节。整体上，理论贡献缺乏坚实的数学基础。\n\n2. 维度B (3.0分): 理由：论文仅给出实验计划，未提供任何实验结果、消融实验或基线对比。评估指标（抽取准确率、清洗质量分、域适应速度、计算效率）过于宽泛，未绑定具体数据集或基准。缺乏代码、数据和可复现实验细节，无法验证所声称的性能提升。实验设计高度概括，缺乏严谨性，未能展示方法的有效性。\n\n3. 维度C (5.0分): 理由：虽然提出约束感知的预训练、剪枝和课程学习可能降低计算成本，但“单 GPU 一天内完成大模型训练”的声明对典型规模的 LLM（如 7B+ 参数）并不现实。论文未提供实证证据证明所提出的剪枝与课程调度能够实现所声称的效率提升。方法在实现细节上不够具体，实际可行性存疑。\n\n总结：综合上述三个维度的评估，论文在理论严谨性、实验验证和实际可行性方面均存在显著不足，整体技术合理性不足，得分 4.0，远低于顶级会议的接受阈值（6 分），建议进行大幅修改或直接拒稿。"
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 4.5,
          "feedback": "1. 维度A (5.0分): 该工作声称将NTK理论扩展到Adam优化器，以提供可微的训练动力学预测。虽然NTK在SGD上已有较多研究，扩展到Adam在概念上并不复杂，且已有类似工作（如NTK for Adam）出现。论文缺乏严谨的理论推导，仅给出概念性描述，创新程度有限。\n2. 维度B (4.0分): GUM神经元优先级选择本质上是一种基于神经元激活模式的剪枝方法，已在SNIP、GraSP、Movement Pruning等工作中出现，只是换了个名称。Meta-objective共进化亦与数据中心AI、联合优化数据与模型的研究高度相似，缺乏根本性的算法创新。\n3. 维度C (4.0分): 将受限预训练、NTK引导微调、GUM剪枝等已知组件整合为端到端系统，虽在系统层面有一定工程价值，但整体创新不足，已有大量工作实现类似的统一管道。\n\n总结: 综上，论文的创新性主要体现在概念包装而非技术突破，整体 novelty 较低，建议拒稿或大幅修改。"
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 6.5,
          "feedback": "1. 问题清晰度与贡献显著性 (7.0分): 论文明确提出将“数据质量”视为可训练目标，并主张通过LLM与领域标准的协同演化来替代人工规则，动机与愿景清晰，贡献点（NTK扩展到Adam、GUM神经优先、端到端 refinery）具有方向性价值。但问题定义缺失具体形式化与约束细节（如“severe compute constraints”的量化、domain standards的表示与演化空间），导致贡献的边界与可复现性不够明确。\n\n2. 方法论连贯性与可实现性 (6.0分): 整体叙事逻辑连贯，从约束感知预训练到NTK引导课程，再到GUM优先与元目标协同演化，形成闭环。但关键环节存在缺口：NTK到Adam的理论扩展与可微预测的数学表述未给出；GUM的“全局唯一运动”度量与选择准则未定义；meta-objective的优化目标、搜索空间与约束未形式化；端到端集成与训练流程伪代码缺失。这些缺口削弱了方法的可实现性与可复现性。\n\n3. 实验验证与证据充分性 (6.5分): 实验计划覆盖三个领域与多类基线，指标较为全面（准确率、清洗质量、适配速度、计算效率），并包含消融与泛化测试。但缺少数据集规模、统计显著性、计算预算与硬件细节、基线实现细节与复现实验流程，导致证据的可信度与可比较性不足。\n\n总结: 论文在问题动机与愿景层面较为完整，方向有创新性，但方法论与实验细节不足，叙事在关键处留白。综合评估为中等偏下，建议接受但需大幅补充理论推导、实现细节与实验证据。"
        }
      ],
      "main_issue": "stability",
      "suggestions": [
        "从stability维度选择稳健Pattern",
        "注入成熟方法增强鲁棒性"
      ]
    },
    {
      "pass": false,
      "avg_score": 4.6000000000000005,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 4.5,
          "feedback": "1. 技术创新性与理论严谨性 (4分): 论文提出了“对抗‑共进化域精炼”这一概念，声称将域标准、LLM 与安全约束统一为可训练的代理。然而，核心的理论贡献——将 Neural Tangent Kernel (NTK) 扩展到 Adam 优化器并加入对抗扰动分析——缺乏严谨的数学推导和可行性论证；GUM 神经元优先级（基于“全局唯一移动模式”）的定义模糊，且未提供任何形式化的判别准则；整体上更像是一套概念性框架，而非可验证的技术方案。\n\n2. 方法描述清晰度与可实现性 (5分): 虽然作者列出了四个关键组件（安全感知预训练、NTK 课程、GUM 优先级、元目标共进化），但每个组件的实现细节、伪代码、计算复杂度以及在单 GPU、一天内完成的可行性分析均缺失。缺乏明确的算法流程图或实现步骤，使得读者难以判断该方案在真实系统中的可操作性。\n\n3. 实验验证与可复现性 (4分): 论文仅提供了实验计划（三个领域、基线对比、指标），并未呈现任何实验结果、消融实验或代码/数据链接。没有实证数据支撑其声称的“高质量域数据 + 对抗鲁棒性”，也未提供可复现的实验环境或评估脚本，导致技术主张无法得到客观检验。\n\n总结: 该工作提出了一个宏大的愿景，试图将域数据抽取与清洗转化为对抗‑共进化的安全设计搜索，但缺乏坚实的理论支撑、明确的实现细节以及任何实验验证。整体技术合理性不足，建议在补充严谨的理论推导、完整算法实现以及充分的实验评估后重新投稿。"
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 4.5,
          "feedback": "1. 维度A (4.5分): 理由: The NTK extension to Adam is a modest theoretical extension; similar ideas have been explored in recent works (e.g., NTK for Adam, adversarial training). The GUM neuron prioritization is a new metric but essentially combines existing neuron pruning and backdoor detection techniques. The meta-objective coevolution is a multi-objective optimization approach that has been applied in other domains. Overall, the theoretical contributions are incremental and lack a fundamental breakthrough.\n2. 维度B (4.5分): 理由: The paper proposes a system that integrates existing components (efficient pretraining, NTK-guided fine-tuning, neuron pruning, coevolution) into a single pipeline. While the framing as 'secure domain design search' is novel, the actual integration is a straightforward combination of known techniques. No new algorithmic paradigm is introduced; the novelty lies mainly in the narrative rather than technical innovation.\n3. 维度C (5.5分): 理由: The experimental plan includes three domains, baseline comparisons, ablation studies, and security stress-tests, which is a reasonable empirical contribution. However, the evaluation methodology is standard and does not introduce novel experimental protocols. The novelty in empirical work is limited to the specific application domain rather than methodological innovation.\n\n总结: The paper presents an ambitious vision of a secure domain refinery but largely assembles existing techniques without delivering a paradigm shift. The theoretical extensions are incremental, the system integration is a common A+B combination, and the empirical work follows standard practices. Consequently, the novelty is insufficient for a top-tier venue, warranting a reject."
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 4.8,
          "feedback": "1. 问题动机与背景 (5.5分): 论文虽指出传统抽取‑清洗管线的脆弱性，但缺少明确的“问题定义”章节，未提供具体业务场景、现有方法的不足之处或量化指标。文献综述、动机阐述以及对安全风险的定量描述均未呈现，导致读者难以判断该研究的核心痛点与创新价值。\n\n2. 方法论连贯性与整合 (5.0分): 文中提出了安全感知NTK课程、GUM神经元优先级以及元目标共演化三大机制，但仅以概念性描述呈现，缺少算法细节、训练流程、损失函数形式以及各模块之间的交互逻辑。理论分析（如NTK向Adam的扩展）缺乏严谨的数学推导，难以验证其可行性与安全性保证。整体叙事呈现为“拼凑式”而非统一的系统设计。\n\n3. 实验验证与结果呈现 (4.0分): 实验计划仅列出评估维度与实验设置，未提供任何实验结果、消融实验或与基线的对比数据。缺少具体的性能指标、计算资源消耗统计以及对安全攻击的防御效果评估，导致读者无法判断所提方法是否真正实现“高质量抽取‑清洗”以及“对抗鲁棒性”。\n\n总结: 论文在叙事完整性上存在显著缺陷：问题动机不够具体、方法论缺乏细节与连贯性、实验部分仅有计划而缺少实证支撑。整体叙事未能形成闭环，导致读者难以评估其学术价值与实际可行性。建议补充明确的问题定义、完整的算法描述以及系统的实验结果，以提升论文的叙事完整性与说服力。"
        }
      ],
      "main_issue": "stability",
      "suggestions": [
        "从stability维度选择稳健Pattern",
        "注入成熟方法增强鲁棒性"
      ]
    }
  ],
  "review_summary": {
    "total_reviews": 3,
    "final_score": 4.6000000000000005
  },
  "refinement_summary": {
    "total_refinements": 3,
    "issues_addressed": [
      "stability",
      "stability",
      "stability"
    ]
  },
  "verification_summary": {
    "collision_detected": false,
    "max_similarity": 0.0
  }
}
"""
åŸºäº Skeleton + Tricks çš„æ··åˆèšç±»ï¼ˆAgglomerativeClustering + K-meansï¼‰ç”Ÿæˆ Patternsã€‚

ä¸ `generate_patterns.py` å¹¶è¡Œä½¿ç”¨ï¼š
- `generate_patterns.py` ä½¿ç”¨çº¯å±‚æ¬¡èšç±»ï¼›
- æœ¬è„šæœ¬åœ¨å±‚æ¬¡èšç±»åŸºç¡€ä¸Šå åŠ  K-means ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡ç°‡å†…ç´§å‡‘åº¦ã€‚

ç”¨æ³•ï¼š
    cd scripts
    python generate_patterns_hybrid.py

è¾“å‡ºæ–‡ä»¶ä»å†™å…¥ä¸Šçº§ç›®å½•çš„ `output/`ï¼Œä¸åŸè„šæœ¬ä¿æŒä¸€è‡´ï¼Œç”±ç”¨æˆ·è‡ªè¡Œé€‰æ‹©é‡‡ç”¨å“ªä¸€ç‰ˆç»“æœã€‚
"""

import os
import json
from collections import Counter

import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize

# å¤ç”¨ç°æœ‰ generate_patterns ä¸­çš„æ ¸å¿ƒé€»è¾‘
from generate_patterns import (
    CLUSTER_PARAMS,
    load_all_papers,
    build_pattern_embeddings,
    analyze_cluster,
    generate_pattern_summary,
    assemble_pattern,
    generate_user_guide,
    generate_statistics,
    cluster_patterns,
)


def cluster_patterns_hybrid(embeddings: np.ndarray) -> np.ndarray:
    """å…ˆç”¨å±‚æ¬¡èšç±»è‡ªé€‚åº”ç¡®å®šç°‡æ•°ï¼Œå†ç”¨ K-means åœ¨æ­¤åŸºç¡€ä¸Šç»†åŒ–ä¼˜åŒ–ã€‚

    è¿”å›ï¼š
        labels_final: æ¯ä¸ªæ ·æœ¬çš„æœ€ç»ˆç°‡æ ‡ç­¾ã€‚
    """

    print("\n" + "-" * 80)
    print("é˜¶æ®µ 1ï¼šå±‚æ¬¡èšç±»ï¼ˆç¡®å®šç°‡æ•°å’Œåˆå§‹ç»“æ„ï¼‰")
    print("-" * 80)

    # å¤ç”¨åŸæœ‰çš„å±‚æ¬¡èšç±»å®ç°ï¼Œè·å¾—åˆå§‹æ ‡ç­¾
    labels_agg = cluster_patterns(embeddings)

    # è®¡ç®—ç°‡æ•°ï¼ˆä¿æŒä¸åŸè„šæœ¬ä¸€è‡´çš„å¤„ç†æ–¹å¼ï¼‰
    unique_labels = sorted(set(labels_agg))
    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
    print(f"\nâœ… å±‚æ¬¡èšç±»è‡ªåŠ¨ç¡®å®šç°‡æ•° k = {n_clusters}")

    print("\n" + "-" * 80)
    print("é˜¶æ®µ 2ï¼šK-means ç»†åŒ–ï¼ˆåœ¨å±‚æ¬¡ç»“æœä¸Šä¼˜åŒ–ç°‡å†…ç´§å‡‘åº¦ï¼‰")
    print("-" * 80)

    # åœ¨è¿›å…¥ K-means å‰å¯¹åµŒå…¥åš L2 å½’ä¸€åŒ–ï¼Œä»¥ä¿è¯ä¸ä½™å¼¦è·ç¦»çš„ä¸€è‡´æ€§
    embeddings_norm = normalize(embeddings, norm="l2")

    # åŸºäºå±‚æ¬¡èšç±»ç»“æœæ„é€ åˆå§‹ä¸­å¿ƒï¼ˆåˆ†å±‚åˆå§‹åŒ–ï¼‰
    initial_centers = []
    for cluster_id in range(n_clusters):
        cluster_indices = [i for i, lab in enumerate(labels_agg) if lab == cluster_id]
        if not cluster_indices:
            continue
        cluster_emb = embeddings_norm[cluster_indices]
        center = cluster_emb.mean(axis=0)
        initial_centers.append(center)

    if len(initial_centers) != n_clusters:
        # ç†è®ºä¸Šä¸åº”å‘ç”Ÿï¼Œä»…ä½œå®‰å…¨å…œåº•
        print("âš ï¸ åˆå§‹ä¸­å¿ƒæ•°é‡ä¸ç°‡æ•°ä¸ä¸€è‡´ï¼Œé€€å›ä½¿ç”¨ K-means++ åˆå§‹åŒ–ã€‚")
        kmeans = KMeans(
            n_clusters=n_clusters,
            init="k-means++",
            n_init=10,
            max_iter=300,
            random_state=42,
        )
    else:
        initial_centers = np.vstack(initial_centers)
        initial_centers = normalize(initial_centers, norm="l2")
        kmeans = KMeans(
            n_clusters=n_clusters,
            init=initial_centers,
            n_init=1,  # åˆå§‹åŒ–å·²ç”±å±‚æ¬¡èšç±»æä¾›ï¼Œæ— éœ€å¤šæ¬¡éšæœºé‡å¯
            max_iter=300,
            random_state=42,
        )

    labels_final = kmeans.fit_predict(embeddings_norm)

    # æ‰“å°ä¼˜åŒ–åçš„èšç±»æ¦‚å†µ
    print(f"\nâœ… K-means ä¼˜åŒ–å®Œæˆ")
    print(f"   ç°‡å†…å¹³æ–¹å’Œï¼ˆinertiaï¼‰: {kmeans.inertia_:.2f}")

    cluster_sizes = Counter(labels_final)
    for cid, size in sorted(cluster_sizes.items(), key=lambda x: -x[1]):
        print(f"   Cluster {cid}: {size} ç¯‡")

    return labels_final


def main() -> None:
    """ä¸»æµç¨‹ï¼šåŸºäºæ··åˆèšç±»ç”Ÿæˆ Patternsã€‚"""

    print("=" * 80)
    print("åŸºäº Skeleton + Tricks çš„æ··åˆèšç±»ï¼ˆå±‚æ¬¡èšç±» + K-meansï¼‰ç”Ÿæˆ Patterns")
    print("=" * 80)

    # 1. åŠ è½½è®ºæ–‡
    print("\nã€Step 1ã€‘åŠ è½½è®ºæ–‡æ•°æ®")
    papers = load_all_papers()
    print(f"âœ… å…±åŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")

    # 2. æ„å»º pattern embeddings
    print("\nã€Step 2ã€‘æ„å»º pattern embeddings")
    embeddings, pattern_data = build_pattern_embeddings(papers)
    print(f"âœ… å®Œæˆ {len(embeddings)} ä¸ª pattern çš„ embedding")

    # 3. æ··åˆèšç±»ï¼ˆå±‚æ¬¡èšç±» + K-meansï¼‰
    print("\nã€Step 3ã€‘æ··åˆèšç±»ï¼ˆAgglomerative + K-meansï¼‰")
    labels = cluster_patterns_hybrid(embeddings)

    # 4. åˆ†ææ¯ä¸ª cluster å¹¶ç”Ÿæˆ pattern
    print("\nã€Step 4ã€‘ç”Ÿæˆ patterns")
    unique_labels = sorted(set(labels))
    n_clusters = len(unique_labels)
    patterns = []

    for cluster_id in range(n_clusters):
        cluster_indices = [i for i, lab in enumerate(labels) if lab == cluster_id]

        if len(cluster_indices) < CLUSTER_PARAMS["min_cluster_size"]:
            print(f"  âš ï¸  Cluster {cluster_id}: {len(cluster_indices)} ç¯‡ (è¿‡å°ï¼Œè·³è¿‡)")
            continue

        cluster_papers = [pattern_data[i] for i in cluster_indices]

        # åˆ†æ cluster
        cluster_analysis = analyze_cluster(cluster_papers, cluster_id)

        # ç”Ÿæˆ summary
        summary = generate_pattern_summary(cluster_analysis)
        print(f"    Summary: {summary[:80]}...")

        # ç»„è£… pattern
        pattern = assemble_pattern(cluster_analysis, summary)
        patterns.append(pattern)

    print(f"\nâœ… å…±ç”Ÿæˆ {len(patterns)} ä¸ª patterns")

    # 5. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ï¼ˆä¸åŸè„šæœ¬ä¿æŒåŒä¸€ç›®å½•ç»“æ„ï¼Œç”±ç”¨æˆ·è‡ªè¡Œé€‰æ‹©é‡‡ç”¨å“ªä¸€ç‰ˆï¼‰
    print("\nã€Step 5ã€‘ç”Ÿæˆè¾“å‡ºæ–‡ä»¶")

    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(os.path.dirname(script_dir), "output")
    os.makedirs(output_dir, exist_ok=True)

    # 5.1 ç»“æ„åŒ– JSON
    structured_path = os.path.join(output_dir, "patterns_structured.json")
    with open(structured_path, "w", encoding="utf-8") as f:
        json.dump(patterns, f, ensure_ascii=False, indent=2)
    print(f"  âœ… {os.path.basename(structured_path)}")

    # 5.2 ç”¨æˆ·æŒ‡å¯¼æ–‡æ¡£
    guide_text = generate_user_guide(patterns)
    guide_path = os.path.join(output_dir, "patterns_guide.txt")
    with open(guide_path, "w", encoding="utf-8") as f:
        f.write(guide_text)
    print(f"  âœ… {os.path.basename(guide_path)}")

    # 5.3 ç»Ÿè®¡æŠ¥å‘Š
    statistics = generate_statistics(patterns)
    stats_path = os.path.join(output_dir, "patterns_statistics.json")
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(statistics, f, ensure_ascii=False, indent=2)
    print(f"  âœ… {os.path.basename(stats_path)}")

    print("\n" + "=" * 80)
    print("ğŸ‰ æ··åˆèšç±»ç‰ˆæœ¬å®Œæˆï¼")
    print("=" * 80)
    print(f"\nç”Ÿæˆäº† {len(patterns)} ä¸ª patternsï¼Œè¦†ç›– {statistics['total_papers']} ç¯‡è®ºæ–‡")
    print(f"å¹³å‡æ¯ä¸ª pattern åŒ…å« {statistics['average_cluster_size']:.1f} ç¯‡è®ºæ–‡")
    print("\næç¤ºï¼šå½“å‰è¾“å‡ºä¼šè¦†ç›– output ç›®å½•ä¸­çš„åŒåæ–‡ä»¶ï¼Œè¯·æ ¹æ®éœ€è¦é€‰æ‹©ä½¿ç”¨åŸç‰ˆæˆ–æ··åˆç‰ˆç»“æœã€‚")


if __name__ == "__main__":
    main()

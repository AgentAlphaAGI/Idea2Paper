"""
Idea2Story Pipeline - ä»ç”¨æˆ· Idea åˆ°å¯å‘è¡¨çš„ Paper Story

å®ç°æµç¨‹:
  Phase 1: Pattern Selection (ç­–ç•¥é€‰æ‹©)
  Phase 2: Story Generation (ç»“æ„åŒ–ç”Ÿæˆ)
  Phase 3: Multi-Agent Critic & Refine (è¯„å®¡ä¸ä¿®æ­£)
  Phase 4: RAG Verification & Pivot (æŸ¥é‡ä¸è§„é¿)

ä½¿ç”¨æ–¹æ³•:
  python scripts/idea2story_pipeline.py "ä½ çš„Ideaæè¿°"
"""

import json
import pickle
import sys
from collections import defaultdict

import numpy as np

# å¯¼å…¥ Pipeline æ¨¡å—
try:
    from pipeline import Idea2StoryPipeline, OUTPUT_DIR
except ImportError:
    # å¦‚æœç›´æ¥è¿è¡Œè„šæœ¬ï¼Œå°è¯•æ·»åŠ å½“å‰ç›®å½•åˆ° path
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from pipeline import Idea2StoryPipeline, OUTPUT_DIR

# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸»å‡½æ•°"""
    # è·å–ç”¨æˆ·è¾“å…¥
    if len(sys.argv) > 1:
        user_idea = " ".join(sys.argv[1:])
    else:
        user_idea = "ä½¿ç”¨è’¸é¦æŠ€æœ¯åšTransformerè·¨é¢†åŸŸæ–‡æœ¬åˆ†ç±»ä»»åŠ¡"

    # åŠ è½½å¬å›ç»“æœï¼ˆè°ƒç”¨ simple_recall_demo çš„ç»“æœï¼‰
    print("ğŸ“‚ åŠ è½½æ•°æ®...")

    try:
        # åŠ è½½èŠ‚ç‚¹æ•°æ®
        with open(OUTPUT_DIR / "nodes_pattern.json", 'r', encoding='utf-8') as f:
            patterns = json.load(f)
        with open(OUTPUT_DIR / "nodes_paper.json", 'r', encoding='utf-8') as f:
            papers = json.load(f)

        print(f"  âœ“ åŠ è½½ {len(patterns)} ä¸ª Pattern")
        print(f"  âœ“ åŠ è½½ {len(papers)} ä¸ª Paper")

        # è¿è¡Œå¬å›ï¼ˆå¤ç”¨ simple_recall_demo çš„é€»è¾‘ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†å¤ç”¨é€»è¾‘ï¼Œç›´æ¥å¯¼å…¥äº† simple_recall_demo
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®å°†å¬å›é€»è¾‘å°è£…ä¸ºç‹¬ç«‹çš„ç±»

        # ä¸´æ—¶ä¿å­˜åŸå§‹ argv
        original_argv = sys.argv.copy()
        sys.argv = ['simple_recall_demo.py', user_idea]

        # è¿è¡Œå¬å›ï¼ˆæ•è·è¾“å‡ºä»¥ä¿æŒæ§åˆ¶å°æ•´æ´ï¼‰
        print("\nğŸ” è¿è¡Œå¬å›ç³»ç»Ÿ...")
        print("-" * 80)

        # ç›´æ¥å¯¼å…¥å¬å›é€»è¾‘
        from simple_recall_demo import (
            NODES_IDEA, NODES_PATTERN, NODES_DOMAIN, NODES_PAPER, GRAPH_FILE,
            compute_similarity, TOP_K_IDEAS, TOP_K_DOMAINS, TOP_K_PAPERS,
            FINAL_TOP_K, PATH1_WEIGHT, PATH2_WEIGHT, PATH3_WEIGHT
        )

        # åŠ è½½æ•°æ®
        with open(NODES_IDEA, 'r', encoding='utf-8') as f:
            ideas = json.load(f)
        with open(NODES_PATTERN, 'r', encoding='utf-8') as f:
            patterns_data = json.load(f)
        with open(NODES_DOMAIN, 'r', encoding='utf-8') as f:
            domains = json.load(f)
        with open(NODES_PAPER, 'r', encoding='utf-8') as f:
            papers_data = json.load(f)
        with open(GRAPH_FILE, 'rb') as f:
            G = pickle.load(f)

        # ã€å…³é”®ä¿®å¤ã€‘åŠ è½½å®Œæ•´çš„ patterns_structured.json ä»¥è·å– skeleton_examples
        patterns_structured_file = OUTPUT_DIR / "patterns_structured.json"
        with open(patterns_structured_file, 'r', encoding='utf-8') as f:
            patterns_structured = json.load(f)

        # æ„å»º pattern_id -> structured_data çš„æ˜ å°„
        structured_map = {}
        for p in patterns_structured:
            pattern_id = f"pattern_{p.get('pattern_id')}"
            structured_map[pattern_id] = p

        # æ„å»ºç´¢å¼•å¹¶åˆå¹¶å®Œæ•´çš„ skeleton_examples
        idea_map = {i['idea_id']: i for i in ideas}
        pattern_map = {}
        for p in patterns_data:
            pattern_id = p['pattern_id']
            # åˆå¹¶ nodes_pattern å’Œ patterns_structured çš„æ•°æ®
            merged_pattern = dict(p)  # å¤åˆ¶åŸºç¡€æ•°æ®
            if pattern_id in structured_map:
                # è¡¥å……å®Œæ•´çš„ skeleton_examples å’Œ common_tricks
                merged_pattern['skeleton_examples'] = structured_map[pattern_id].get('skeleton_examples', [])
                merged_pattern['common_tricks'] = structured_map[pattern_id].get('common_tricks', [])
            pattern_map[pattern_id] = merged_pattern

        domain_map = {d['domain_id']: d for d in domains}
        paper_map = {p['paper_id']: p for p in papers_data}

        # è·¯å¾„1
        path1_scores = defaultdict(float)
        similarities = [(idea['idea_id'], compute_similarity(user_idea, idea['description']))
                       for idea in ideas if compute_similarity(user_idea, idea['description']) > 0]
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_ideas = similarities[:TOP_K_IDEAS]

        for idea_id, similarity in top_ideas:
            idea = idea_map[idea_id]
            pattern_ids = idea.get('pattern_ids', [])
            for pid in pattern_ids:
                path1_scores[pid] += similarity

        # è·¯å¾„2
        path2_scores = defaultdict(float)
        top_idea = idea_map[top_ideas[0][0]] if top_ideas else None
        domain_scores = []

        if top_idea and G.has_node(top_idea['idea_id']):
            for successor in G.successors(top_idea['idea_id']):
                edge_data = G[top_idea['idea_id']][successor]
                if edge_data.get('relation') == 'belongs_to':
                    domain_id = successor
                    weight = edge_data.get('weight', 0.5)
                    domain_scores.append((domain_id, weight))

        domain_scores.sort(key=lambda x: x[1], reverse=True)
        top_domains = domain_scores[:TOP_K_DOMAINS]

        for domain_id, domain_weight in top_domains:
            for predecessor in G.predecessors(domain_id):
                edge_data = G[predecessor][domain_id]
                if edge_data.get('relation') == 'works_well_in':
                    pattern_id = predecessor
                    effectiveness = edge_data.get('effectiveness', 0.0)
                    confidence = edge_data.get('confidence', 0.0)
                    path2_scores[pattern_id] += domain_weight * max(effectiveness, 0.1) * confidence

        # è·¯å¾„3
        path3_scores = defaultdict(float)
        similarities = []
        for paper in papers_data:
            paper_idea = paper.get('idea', {}).get('core_idea', '') or paper.get('abstract', '')[:100]
            if not paper_idea:
                continue

            sim = compute_similarity(user_idea, paper_idea)
            if sim > 0.1 and G.has_node(paper['paper_id']):
                reviews = paper.get('reviews', [])
                if reviews:
                    scores = [r.get('rating', 5) for r in reviews]
                    avg_score = np.mean(scores)
                    quality = (avg_score - 1) / 9
                else:
                    quality = 0.5

                combined = sim * quality
                similarities.append((paper['paper_id'], sim, quality, combined))

        similarities.sort(key=lambda x: x[3], reverse=True)
        top_papers = similarities[:TOP_K_PAPERS]

        for paper_id, similarity, quality, combined_weight in top_papers:
            if not G.has_node(paper_id):
                continue
            for successor in G.successors(paper_id):
                edge_data = G[paper_id][successor]
                if edge_data.get('relation') == 'uses_pattern':
                    pattern_id = successor
                    pattern_quality = edge_data.get('quality', 0.5)
                    path3_scores[pattern_id] += combined_weight * pattern_quality

        # èåˆ
        all_patterns = set(path1_scores.keys()) | set(path2_scores.keys()) | set(path3_scores.keys())
        final_scores = {}
        for pattern_id in all_patterns:
            score1 = path1_scores.get(pattern_id, 0.0) * PATH1_WEIGHT
            score2 = path2_scores.get(pattern_id, 0.0) * PATH2_WEIGHT
            score3 = path3_scores.get(pattern_id, 0.0) * PATH3_WEIGHT
            final_scores[pattern_id] = score1 + score2 + score3

        ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        top_k = ranked[:FINAL_TOP_K]

        # æ„å»ºå¬å›ç»“æœ
        recalled_patterns = [
            (pattern_id, pattern_map.get(pattern_id, {}), score)
            for pattern_id, score in top_k
        ]

        # æ¢å¤ argv
        sys.argv = original_argv

        print("-" * 80)
        print(f"âœ… å¬å›å®Œæˆ: Top-{len(recalled_patterns)} Patterns\n")

        # è¿è¡Œ Pipeline
        pipeline = Idea2StoryPipeline(user_idea, recalled_patterns, papers)
        result = pipeline.run()

        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "final_story.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result['final_story'], f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ æœ€ç»ˆ Story å·²ä¿å­˜åˆ°: {output_file}")

        # ä¿å­˜å®Œæ•´ç»“æœ
        full_result_file = OUTPUT_DIR / "pipeline_result.json"
        with open(full_result_file, 'w', encoding='utf-8') as f:
            json.dump({
                'user_idea': user_idea,
                'success': result['success'],
                'iterations': result['iterations'],
                'selected_patterns': result['selected_patterns'],
                'final_story': result['final_story'],
                'review_history': result['review_history'],
                'review_summary': {
                    'total_reviews': len(result['review_history']),
                    'final_score': result['review_history'][-1]['avg_score'] if result['review_history'] else 0
                },
                'refinement_summary': {
                    'total_refinements': len(result['refinement_history']),
                    'issues_addressed': [r['issue'] for r in result['refinement_history']]
                },
                'verification_summary': {
                    'collision_detected': result['verification_result']['collision_detected'],
                    'max_similarity': result['verification_result']['max_similarity']
                }
            }, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {full_result_file}")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()


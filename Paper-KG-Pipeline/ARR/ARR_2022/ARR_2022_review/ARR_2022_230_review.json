[{"rid": "db8aa6185bc491825bf2992a2710571f719ac68eb37b18c177945628dc57d9f9e7385a81f423fab54ef5a1e10c2a7c49686661def2ad1831924f07ffa0cc9c2e", "reviewer": "Liang Yao", "report": {"paper_summary": "This manuscript summarizes representative shallow and deep learning text classification methods with extensive experimental comparison. The authors categorize text classification literature into BOW-based models, Graph-based Models and Sequence models. They then compare these models on five benchmark datasets and find that an efficient wide MLP can achieve competitive results and should be considered as a strong baseline for text classification tasks. The work is interesting and can make much impact on practitioners. ", "summary_of_strengths": "The paper is technically correct and the finding is interesting. \nThe results have practical impacts. ", "summary_of_weaknesses": "The experiments can be better conducted. Firstly, only 5 small datasets are used, large data sets [1] could be considered. Secondly, the performance of CNN and linear SVM (e.g., LIBLINEAR) should be compared. Lastly, parameter counts and the total runtime of graph-based models are missing.\nThe proposed Wide MLP is not novel.\n[1] Zhang, X., Zhao, J. and LeCun, Y., 2015. Character-level convolutional networks for text classification.  Advances in neural information processing systems, 28, pp.649-657. ", "comments,_suggestions_and_typos": "Page 6, line 542, compact --> complex "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Liang Yao, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 136], [136, 253], [253, 456], [456, 523]], "summary_of_strengths": [[0, 65], [65, 102]], "summary_of_weaknesses": [[0, 41], [41, 123], [123, 209], [209, 291], [291, 327], [327, 372], [372, 436], [436, 504]], "comments,_suggestions_and_typos": [[0, 38]]}}}, {"rid": "b4e81da505ebef5ace8e442e01cccdac5908d729ef226a19898d890b25f745009f4d2b3dc5d8c7dee53e6ca4d4d4fc3ea27434a9079229938acc30a8d4daf193", "reviewer": null, "report": {"paper_summary": "A comprehensive set of experiments are conducted which suggests that baseline wide MLPs with bag-of-words (BoW) input can perform just as well as recently popular graph-based models, such as TextGCN and HeteGCN, in topical text categorization. In addition to wide MLP, sequential BERT and DistilBERT models are fine-tuned, which overall yield state-of-the-art results on 5 well-known text categorization datasets. Parameter counts of the models as well as runtime are also compared, showing the effectiveness of wide MLP with BoW models. ", "summary_of_strengths": "- A comprehensive comparison of models on text categorization datasets demonstrating the effectiveness of a relatively simple baseline model - Consideration of model size, runtime performance in addition to accuracy metrics - Clear description, detailed explanations, acknowledgement of potential limitations, good insights ", "summary_of_weaknesses": "- Limited to multi-class topical text categorization, which is while important, is ultimately just one task - Fair amount of repetition, which is sometimes helpful but can also be a bit distracting (e.g., the difference between inductive learning and transductive learning is explained a couple of times, same with how they used BERT vs. DistilBERT) ", "comments,_suggestions_and_typos": "- Can parameter counts/training time for graph-based models be provided?\n- less classes -> fewer - Minaee et al. (2021) can be cited: Deep Learning–based Text Classification: A Comprehensive Review - Dataset characteristics discussed  (line 359-376) can be combined with Table 2.\n- Missing period (line 224).\n- Sentence line 315-319 is ungrammatical. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 244], [244, 414], [414, 538]], "summary_of_strengths": [[0, 141], [141, 224], [224, 324]], "summary_of_weaknesses": [[0, 108], [108, 350]], "comments,_suggestions_and_typos": [[0, 73], [73, 97], [97, 198], [198, 280], [280, 309], [309, 351]]}}}]
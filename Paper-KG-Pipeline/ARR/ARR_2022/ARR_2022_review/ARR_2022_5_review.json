[{"rid": "6fa9bce38a7d737b41586d8ddc0aa1e8962e82d241876d5bb9b07592de8e6aab2dba7abc8f3fd736e5539c64304e95cdee2e0c4c301a477133c370a101f9f912", "reviewer": null, "report": {"paper_summary": "This paper presents a novel generation task of faithfully reflecting updated information in the text (FRUIT) where the goal is to update an existing article given new evidence. The authors also release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data produced from pairs of Wikipedia snapshots, along with our data generation pipeline and a gold evaluation set of 914 instances whose edits are guaranteed to be supported by the evidence. They provide benchmark results for popular generation systems as well as EDIT5 -- a T5-based approach tailored to editing we introduce that establishes the state of the art. The author's analysis shows that developing models that can update articles faithfully requires new capabilities for neural generation models, and opens doors to many new applications. ", "summary_of_strengths": "1. The paper proposes a novel task that is quite meaningful, as we indeed need to deal with factual information updates during writing periods. \n2. The data annotation process is well documented, and it's nice to have both a silver dataset created using a rather less noisy distant supervision method and a gold dataset with human annotations. \n3. The proposed baseline is complete and it's also an appropriate contribution of a self-designed T5 adaptation for the edits and updates. \n4. Experiments are comprehensive, and the appendixes contain a lot of information and analysis for reference. ", "summary_of_weaknesses": "1. UpdateROUGE is a newly proposed automatic evaluation metric for the task based on an intuitive heuristic. It would be nice if more analysis could be done about this proposed metric, including how it correlates with human evaluation metrics and how robust it is against multiple alternatives to the correct answer. \n2. Several trivial copy baselines are used, but as mentioned as a reference (Wiseman et. al., 2017), there are models that explicitly do data to text generation with copy mechanism (attention based) that could serve as a slightly more intelligent way of performing copying baseline with selection of what to copy. ", "comments,_suggestions_and_typos": "N/A "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 177], [177, 465], [465, 639], [639, 824]], "summary_of_strengths": [[0, 3], [3, 144], [144, 148], [148, 344], [344, 348], [348, 484], [484, 488], [488, 595]], "summary_of_weaknesses": [[0, 3], [3, 109], [109, 317], [317, 321], [321, 407], [407, 632]], "comments,_suggestions_and_typos": [[0, 4]]}}}]
[{"rid": "c29f875efad0be673bd7a12f43f37625d8bdbff8992cc8be203f512f659310b5e5169cdf0336a51cdbc949b35de3b69b1f8eb5fdb69493bd13e8ea7373d3c30c", "reviewer": null, "report": {"paper_summary": "The paper proposes a method for relation extraction by taking advantage of interactions between relations instead of only entities, by constructing both relation and entity embeddings. \nThese representations are further refined via attention mechanisms over the entity-context interactions. \nIn addition, Tucker decomposition is employed in order to align representations of entities and relations in a joint space, predicting if a triple exists or not.\nExperiments are conducted on three datasets, one for document-level relation extraction (DocRED) and two for joint entity and relation extraction (NYT, WebNLG). Comparison with prior works shows some improvement while ablation studies indicate that alignment of entity and relation embeddings contributes to performance. ", "summary_of_strengths": "Strengths of the paper include:  - A new fusion and alignment approach between relation and entity embeddings for relation extraction - Improvement in document-level and sentence-level relation extraction compared with prior work ", "summary_of_weaknesses": "Weaknesses of the paper include: - The introduction of relation embeddings for relation extraction is not new, for example look at all Knowledge graph completion approaches that explicitly model relation embeddings or works on distantly supervised relation extraction. However, an interesting experiment would be to show the impact that such embeddings can have by comparing with a simple baseline that does not take advantage of those.\n- Improvements are incremental across datasets, with the exception of WebNLG. Why mean and standard deviation are not shown for the test set of DocRED?\n- It is not clear if the benefit of the method is just performance-wise. Could this particular alignment of entity and relation embeddings (that gives the most in performance) offer some interpretability? ( perhaps this could be shown with a t-SNE plot, i.e. check that their embeddings are close in space). ", "comments,_suggestions_and_typos": "Comments/Suggestions: - Lines 26-27: Multiple entities typically exist in both sentences and documents and this is the case even for relation classification, not only document-level RE or joint entity and relation extraction.\n- Lines 39-42: Point to figure 1 for this particular example.\n- Lines 97-98: Rephrase the sentence \"one that searches for ... objects\" as it is currently confusing - Line 181, Equations 4: $H^s$, $E^s$, $E^o$, etc are never explained.\n- Could you show ablations on EPO and SEO? You mention in the Appendix that the proposed method is able to solve all those cases but you don't show if your method is better than others.\n- It would be interesting to also show how the method performs when different number of triples reside in the input sequence. Would the method help more sequences with more triples?\nQuestions:  - Improvement still be observed with a better encoder, e.g. RoBERTa-base, instead of BERT?\n- How many seeds did you use to report mean and stdev on the development set?\n- For DocRED, did you consider the documents as an entire sentence? How do you deal with concepts (multiple entity mentions referring to the same entity)? This information is currently missing from the manuscript. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 185], [185, 291], [291, 454], [454, 615], [615, 775]], "summary_of_strengths": [[0, 33], [33, 134], [134, 230]], "summary_of_weaknesses": [[0, 33], [33, 269], [269, 437], [437, 515], [515, 589], [589, 662], [662, 796], [796, 897]], "comments,_suggestions_and_typos": [[0, 22], [22, 226], [226, 288], [288, 390], [390, 461], [461, 504], [504, 647], [647, 773], [773, 829], [829, 841], [841, 932], [932, 1010], [1010, 1078], [1078, 1165], [1165, 1224]]}}}]
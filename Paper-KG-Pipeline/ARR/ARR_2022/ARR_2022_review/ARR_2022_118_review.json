[{"rid": "c4658267362eaf0381ed7da20a5eb33a3928a03dc88d39a4b73c876e365011c1db0852fbe8e2485b9b19903f7b772a9312ab155d23c7f21c9b2904a5abce7b6e", "reviewer": "Zhixuan Zhou", "report": {"paper_summary": "This paper alleviates the generic response issue in generative dialogue models by proposing a novel negative training paradigm called negative distillation. This method goes beyond penalizing high-frequency words, but instead learn multi-level features of generic responses. Extensive experiments show its satisfactory performance in promoting both response diversity and quality. ", "summary_of_strengths": "1. Compared to other diversity promotion techniques, negative distillation captures more characteristics of generic responses, thus achieves better diversity and consistency performance. \n2. Extensive evaluations, both automatic and human-centric, have been carried out to justify the performance. The authors have added experiments to compare negative distillation to baselines outside of the negative training paradigm. Table 11 clearly showcases its satisfactory performance. ", "summary_of_weaknesses": "The current version generally looks good to me, so I'll mainly put forward some suggestions to further polish it up. \n1. The authors could elaborate a bit on the intuition of negative distillation, which seems interesting (i.e., by trying to be different from bad teachers, students can get better). \n2. Does the extent to which the teacher model is negative affect performance of the student model? For example, what if top 10% (instead of top 50%) dialogue pairs with a high source entropy are selected as the negative training set? Will the performence be better or worse? \n3. There're some minor glitches. For example, in Table 4 caption, \"our framework has a higher win rate than baselines.\" The authors are not actually comparing win rate of negative distillation with the baselines. ", "comments,_suggestions_and_typos": "See above. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Zhixuan Zhou, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 157], [157, 275], [275, 381]], "summary_of_strengths": [[0, 3], [3, 187], [187, 191], [191, 298], [298, 422], [422, 479]], "summary_of_weaknesses": [[0, 117], [117, 121], [121, 300], [300, 304], [304, 400], [400, 535], [535, 576], [576, 580], [580, 610], [610, 697], [697, 790]], "comments,_suggestions_and_typos": [[0, 11]]}}}, {"rid": "7c6fa2d43e7c8ae3ddd6df7867dbaf0d7e8fcc695f11b89238410f74be5ce7e6a7389d35e9c1dc3ea6d0a6c20e35f9eac77d9a4277388f68d719a9dcb096b9cd", "reviewer": "Wei Bi", "report": {"paper_summary": "Second-time review and my previous reviewer id is Reviewer XYZ. ", "summary_of_strengths": "See the previous review. ", "summary_of_weaknesses": "See the previous review. ", "comments,_suggestions_and_typos": "Here attach the comments about addressing my previous review.  A1: Would you give some suggestions based on current results about which part can be discarded first if computational efforts are constrained?\nA2: \"We believe any functions that first rise then decrease can also obtain similar results\" -> Do you have any other instantiation? Also, if this argument can be clarified, this sentence should be added to the paper.\nA3:  Based on the explanation, it seems that ND cannot be adapted to beam search, as it cannot achieve better performance. Thus, the discussions from Line494 may be problematic.\nOverall, the revision towards my previous review is not clear and the arguments provided in the responses are not supported by sufficient discussion or experimental results. I would still keep my score to be 3. "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Wei Bi, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 64]], "summary_of_strengths": [[0, 25]], "summary_of_weaknesses": [[0, 25]], "comments,_suggestions_and_typos": [[0, 62], [62, 63], [63, 206], [206, 339], [339, 424], [424, 547], [547, 602], [602, 776], [776, 813]]}}}]
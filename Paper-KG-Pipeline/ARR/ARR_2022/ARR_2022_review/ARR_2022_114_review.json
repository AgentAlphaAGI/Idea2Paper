[{"rid": "fa60f1a1f132578809b55d458098cf7c2899fc3d35febeb6386f1aa13aeebf1576a62ad3c3d55342cf4bf1db6f8e0ce24aaa678fdc064d0586ae23e1e6edd720", "reviewer": "Yuntian Deng", "report": {"paper_summary": "The performance of structured prediction models can be greatly improved by scaling to larger state spaces, yet the inference complexity of these models scales poorly w.r.t. the size of the state space. The goal of this work is to reduce the inference complexity of structured models by factorizing the clique potentials using low-rank tensor decompositions and performing message passing in an induced rank space instead of the original state space.\nThis work makes three contributions: 1. Using the language of factor graph grammars, this work unifies previous low-rank tensor decomposition works such as Yang et al 2021b  and Chiu et al 2021. This work shows that those works are essentially performing message passing on a factor graph with two types of nodes: the original state nodes and auxiliary rank nodes induced by the low-rank tensor decomposition. \n2. On a sub-family of factor graph grammars which subsume most commonly-used structured prediction models such as HMMs, HSMMs, and PCFGs, this work proposes to marginalize the state nodes first and only perform inference in the induced rank nodes, which reduces the complexity by replacing a factor of the state size by a factor of the rank size which is usually smaller. \n3. Empirically this work scales HMMs and PCFGs to very large state spaces and achieves strong performance. ", "summary_of_strengths": "1. This work is insightful in pointing out that by performing message passing only in the rank space after marginalizing the original state nodes (which is a one-time cost), a factor of the number of states in the total complexity can be replaced by a factor of the rank size. This idea is generally applicable to a large family of factor graph grammars that have one external node per hypergraph fragment, and it might enable scaling many structured prediction models. \n2. This work gets strong empirical performance by scaling to very large state spaces when compared to previous structured prediction works. In particular, this work trains the largest-ever PCFG in the task of unsupervised parsing on PTB (to my knowledge) and establishes a new state-of-the-art performance in this particular task. \n3. This work confirms findings of previous works such as Chiu and Rush 2020 that scaling structured prediction models can improve performance. For example, Figure 6 (b) suggests that scaling PCFGs to beyond 10k pre-terminals might further improve modeling performance. ", "summary_of_weaknesses": "By showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials).  For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.\nAs another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\\in \\mathcal{R}^{r \\times r}$ is parameterized as $V U^T$ where $U,V\\in \\mathcal{R}^{r \\times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\\mathcal{R}^{r \\times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here? ", "comments,_suggestions_and_typos": "In regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared. \n2. For each HMM with rank r, add a baseline smaller HMM with state size being r. "}, "scores": {"overall": "5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "Maybe", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yuntian Deng, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 173], [173, 202], [202, 450], [450, 490], [490, 645], [645, 860], [860, 864], [864, 1233], [1233, 1237], [1237, 1341]], "summary_of_strengths": [[0, 3], [3, 277], [277, 470], [470, 474], [474, 611], [611, 802], [802, 806], [806, 946], [946, 1072]], "summary_of_weaknesses": [[0, 425], [425, 426], [426, 718], [718, 848], [848, 1017], [1017, 1289], [1289, 1546], [1546, 1609], [1609, 1676]], "comments,_suggestions_and_typos": [[0, 101], [101, 104], [104, 340], [340, 444], [444, 448], [448, 526]]}}}]
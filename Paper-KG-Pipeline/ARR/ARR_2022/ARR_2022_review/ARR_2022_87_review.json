[{"rid": "f5d14391b40ef9e869292686627ae5faec18d78b9d435160d007bffe2a8ff9c81af1cb18c0dd694d75685fba6500e824421b6e8b6433e6210275268381c13291", "reviewer": null, "report": {"paper_summary": "This paper proposes a new intermediate task for leveraging tabular data for enhancing the performance of an LM on tasks of unstructured text. The enhanced model shows improvement compared to the original T5 model. ", "summary_of_strengths": "- The paper is overall well-written (though the organization seems confusing, to be detailed in \"Weaknesses\") and the figures are helpful for understanding the paper.\n- The method is intuitive and relatively simple. The semi-structured tabular data are easy to obtain (from Wikipedia or Common Crawls) so the application of this method is promising.\n- The introduced momentum sampling strategy is interesting and can be useful for multi-task training.\n- The improvement on the four datasets are significant. ", "summary_of_weaknesses": "### Original comments - The novelty is limited. This paper mainly uses a template-based generation strategy for EG. Similar techniques have been used in prior works of question generation and semantic parsing.\n- The organization of this paper is confusing. Why there is a \"related work\" subsection in Ln. 323 while there is a whole section of related work (Section 6)? If the authors feel like the related work should be discussed earlier, I suggest moving Section 6 to after the introduction.\n- More baselines should be compared. GenBERT is a very relevant baseline but I think other methods that enhance LM with external knowledge (e.g., KBERT, KnowBERT) should also be compared.\n### After revision The authors reorganize the paper and I have seen improvement in the quality of writing. Also, I've seen the authors have addressed the questions of other reviewers. I thus have updated my recommendation from 3.5 to 4. ", "comments,_suggestions_and_typos": "N/A "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 142], [142, 214]], "summary_of_strengths": [[0, 167], [167, 216], [216, 350], [350, 452], [452, 508]], "summary_of_weaknesses": [[0, 22], [22, 48], [48, 116], [116, 210], [210, 257], [257, 305], [305, 369], [369, 494], [494, 531], [531, 682], [682, 789], [789, 866], [866, 919]], "comments,_suggestions_and_typos": [[0, 4]]}}}]
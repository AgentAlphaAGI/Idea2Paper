[{"rid": "057a3f73710f304801787e253e5ffc7e5d2eb9cf4570a5e3c6dc8edf75ebaadb324aff13a67e27a6057b12080f89cd80cc8d0f000bd23657daf14ff9c30856db", "reviewer": null, "report": {"paper_summary": "The paper extensively explores the ways to extend GECTOR model by using larger transformers of different kind, model distillation and ensembling. It allows to achieve new SOTA on BEA2019 test. The main results are: + new SOTA achieved by using an ensemble of large pretrained Transformer models + a single model trained on data annotated by a large ensemble model outperforms standard GECTOR training + Edit majority ensembling works better than probability ensembling ", "summary_of_strengths": "+ new SOTA on BEA 2019 test + a better ensembling method proposed + a nice study on knowledge distillation + the paper is clearly written ", "summary_of_weaknesses": "- Judging the paper for a workshop, I find practically no weaknesses. However, I have minor concerns whether the contribution is significant enough for people outside GEC community - seems to be an extension of earlier published work https://s3.eu-central-1.amazonaws.com/ucu.edu.ua/wp-content/uploads/sites/8/2021/04/Improving-Sequence-Tagging-Approach-for-Grammatical-Error-Correction-Task-.pdf  The authors should properly highlight the contribution made on top of it. ", "comments,_suggestions_and_typos": "- Please explain better how knowledge distillation is performed. Do you mean that you find erroneous sentences in unannotated data and then correct them by a strong ensemble to obtain the target sentence? "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 146], [146, 193], [193, 469]], "summary_of_strengths": [[0, 138]], "summary_of_weaknesses": [[0, 70], [70, 181], [181, 472]], "comments,_suggestions_and_typos": [[0, 65], [65, 205]]}}}, {"rid": "7075eb05a7f3e433a7b8cba6ee73866ee4976dc2da0e5ebc79a7019abe84cc28a909e0e99c0e814b0e8a7535b4b9f9839e7fd1d3d855b902f6f511fc6234b852", "reviewer": null, "report": {"paper_summary": "This paper looks at ensembles of Large Transformers tagger models for Grammatical Error Correction. The authors discovered an ensemble that achieves a strong new SOTA of 76.05 on BEA-2019 test without pretraining on synthetic data. The authors additionally report model distillation results on synthetic data, where a single model achieves near SOTA with 73.21 on the BEA-2019 test. The single model is additionally not SOTA only in comparison to a much larger T5 model.\nThe models utilized in this paper are based on the well known GECToR system. Performance improvements are obtained by: (1) upgrading model size to Large (GECToR used Base), (2) changing tokenizer (GECToR changed tokenizer from the one used during pretraining), (3) filtering of edit free sentences, (4) increasing vocabulary size (the vocabulary of possible edits), (5) ensembling by majority vote on edit spans instead of ensembling on average output tag probabilities, (6) distillation. Two new synthetic datasets are constructed for distillation: one from the One Billion Word Benchmark and one from the The Blog Authorship Corpus.\nTraining is performed in three stages: (1) optionally train on synthetic data, (2) finetune on the edits of Lang-8, NUCLE, FCE, W&I in this order, (3) finetune on W&I only. At inference the paper uses a tweak from GECToR, adjusting the keep probability by a tuned amount. Ensembling is performed over RoBERTa, DeBERTa and XLNet with different model sizes and edit vocabulary sizes. ", "summary_of_strengths": "Strenghts: - Performance reported in the paper is very strong and likely to be of interest to many practitioners in GEC, especially because it is obtained without auto-regressive decoding.\n- Comparison to existing work seems fair and thorough.\n- Work is fully opensourced.\n- The paper presents results for many ensembles and numerous ablations, providing valuable experimental results for future research. ", "summary_of_weaknesses": "Weaknesses: - The paper combines known techniques and datasets in a novel way, but all the elements are from other work on GEC.\n- It would be nice to see more speed comparison between the model in the paper and some of the previous models, particularly on GPU or TPU. ", "comments,_suggestions_and_typos": "Formatting of the paper could be improved, e.g. quotes in the title of section 5.1 are incorrect, several tables exceed the column width or are not centered.\nIronically there are also quite a few grammatical errors. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 100], [100, 232], [232, 383], [383, 471], [471, 548], [548, 960], [960, 1106], [1106, 1279], [1279, 1378], [1378, 1488]], "summary_of_strengths": [[0, 11], [11, 189], [189, 244], [244, 273], [273, 406]], "summary_of_weaknesses": [[0, 12], [12, 128], [128, 268]], "comments,_suggestions_and_typos": [[0, 158], [158, 216]]}}}]
[{"rid": "d3b37745fdec7312511c9e49b025b2d0cecce17cbaf44ed08677185cd690cf916b28e414c20a903c10f10dc89a98693ee077bd84e93faa87cd33731b088d04d2", "reviewer": "Indira Sen", "report": {"paper_summary": "This paper contributes two experiments to investigate how annotator’s backgrounds and sociopolitical attitudes affect their perception of toxic language. The paper finds that having certain characteristics is correlated with perceptions towards racism, for example, believing in freedom of speech, predisposes one to be more lax in annotating anti-Black toxicity. The paper also finds that the widely used toxicity detection tool, Perspective API, mimics the conservative attitudinal profile when it comes anti-Black toxicity. ", "summary_of_strengths": "- Important and valuable work - Going beyond demographic characteristics of annotators, but also including sociopolitical attitudes - Carefully thought-out experiments - Detailed description of some metrics and results - Well-described and contextualized implications of disregarding annotator background and beliefs when creating ", "summary_of_weaknesses": "- Underdefined and conflation of concepts - Several important details missing - Lack of clarity in how datasets were curated prevents one from assessing their validity - Too many results which are not fully justified or explained ", "comments,_suggestions_and_typos": "This is a very important, interesting, and valuable paper with many positives. First and foremost, annotators’ backgrounds are an important factor and should be taken into consideration when designing datasets for hate speech, toxicity, or related phenomena. The paper not only accounts for demographic variables as done in previous work but other attitudinal covariates like attitude towards free speech that are well-chosen. The paper presents two well-thought out experiments and presents results in a clear manner which contain several important findings.\nIt is precisely because of the great potential and impact of this paper, I think the current manuscript requires more consideration and fine-tuning before it can reach its final stage. At this point, there seems to be a lack of important details that prevent me from fully gauging the paper’s findings and claims. Generally: - There were too many missing details (for example, what is the distribution of people with ‘free off speech’ attitudes? What is the correlation of the chosen scale item in the breadth-of-posts study?). On a minor note, many important points are relegated to the appendix.\n- Certain researcher choices and experiment design choices were not justified (for example, why were these particular scales used?)\n- The explanation of the creation of the breadth-of-posts was confusing. How accurate was the classification of AAE dialect and vulgarity?  - The toxicity experiment was intriguing but there was too little space to be meaningful.\nMore concretely, - With regard to terminology and concepts, toxicity and hate speech may be related but are not the same thing. The instructions to the annotators seem to conflate both. The paper also doesn’t present a concrete definition of either. While it might seem redundant or trivial, the wording to annotators plays an important role and can confound the results presented here.\n- Why were the particular scales chosen for obtaining attitudes? Particularly, for empathy there are several scale items [1], so why choose the Interpersonal Reactivity Index?\n- What was the distribution of the annotator’s background with respect to the attitudes? For example, if there are too few ‘free of speech’ annotators, then the results shown in Table 3, 4, etc are underpowered.  - What were the correlations of the chosen attitudinal scale item for the breadth-of-posts study with the toxicity in the breadth-of-workers study?\n- How accurate are the automated classification in the breadth-of-posts experiment, i.e., how well does the states technique differentiate identity vs non-identity vulgarity or AAE language for that particular dataset. Particularly, how can it be ascertained whether the n-word was used as a reclaimed slur or not?  - In that line, Section 6 discusses perceptions of vulgarity, but there are too many confounds here. Using b*tch in a sentence can be an indication of vulgarity and toxicity (due to sexism).\n- In my opinion, the perspective API experiment was interesting but rather shallow. My suggestion would be to follow up on it in more detail in a new paper rather than include it in this one. The newly created space could be used to enter the missing details mentioned in the review.   - Finally, given that the paper notes that MTurk tends to be predominantly liberal and the authors (commendably) took several steps to ensure greater participation from conservatives, I was wondering if ‘typical’ hate speech datasets are annotated by more homogenous annotators compared to the sample in this paper. What could be the implications of this? Do this paper's findings then hold for existing hate speech datasets?\nBesides these, I also note some ethical issues in the ‘Ethical Concerns’ section. To conclude, while my rating might seem quite harsh, I believe this work has great potential and I hope to see it enriched with the required experimental details.\nReferences: [1] Gerdes, Karen E., Cynthia A. Lietz, and Elizabeth A. Segal. \" Measuring empathy in the 21st century: Development of an empathy index rooted in social cognitive neuroscience and social justice.\" Social Work Research 35, no. 2 (2011): 83-93. ", "ethical_concerns": "There is no limitations section, unlike what the authors state in the responsible NLP research checklist. There are however many expositional and ethical limitations in this paper. I do not believe they are grounds for rejection, but I expect that the paper acknowledges and engages with them.\n- The biggest problem, as is the case for abusive language detection in general, is that there is no reflection on the harm to annotators.  - Second, given that the whole idea of the paper is to consider annotator backgrounds, I am left wondering what the backgrounds of the undergraduate annotators for the 15 posts were. Speaking of backgrounds, there is also no reflection on the background of the authors themselves and how that might color their perspectives in this paper (see Sweet, Paige L. \"Who Knows? Reflexivity in Feminist Standpoint Theory and Bourdieu.\" Gender & Society 34, no. 6 (2020): 922-950.) "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Indira Sen, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 154], [154, 364], [364, 527]], "summary_of_strengths": [[0, 30], [30, 132], [132, 168], [168, 219], [219, 331]], "summary_of_weaknesses": [[0, 42], [42, 78], [78, 168], [168, 230]], "comments,_suggestions_and_typos": [[0, 79], [79, 259], [259, 427], [427, 560], [560, 745], [745, 874], [874, 885], [885, 1006], [1006, 1088], [1088, 1158], [1158, 1290], [1290, 1363], [1363, 1429], [1429, 1430], [1430, 1520], [1520, 1537], [1537, 1648], [1648, 1706], [1706, 1770], [1770, 1907], [1907, 1972], [1972, 2083], [2083, 2172], [2172, 2295], [2295, 2296], [2296, 2444], [2444, 2663], [2663, 2759], [2759, 2760], [2760, 2861], [2861, 2951], [2951, 3035], [3035, 3143], [3143, 3235], [3235, 3237], [3237, 3553], [3553, 3593], [3593, 3663], [3663, 3745], [3745, 3908], [3908, 3986], [3986, 4118], [4118, 4164]], "ethical_concerns": [[0, 106], [106, 181], [181, 294], [294, 433], [433, 434], [434, 617], [617, 805], [805, 862], [862, 907]]}}}]
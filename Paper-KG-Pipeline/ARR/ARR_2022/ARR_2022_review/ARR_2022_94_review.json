[{"rid": "6279747572b802d4062cb5fdfc380e494f2000b5bb1c8ed39b4bf52d2e81d9f088f79e842c630b899ae0adaf76584d814a4db1d897f578abe9be351afe58de1a", "reviewer": null, "report": {"paper_summary": "The authors present a strong method for unsupervised constituency parsing that relies on RoBERTa and constituent classification based on an initial set of constituents / distuents. Throughout training, this labeled set is updated with the models own predictions. Extensive results and analysis show the competitiveness of this approach with other unsupervised parsing methods. That being said, some details needed for reproducibility are needed, and claims about the initial set being a “minor” source of supervision are contradicted throughout the text.\nTo clarify the last part above, the issue may be that “minor” is underspecified. Clearly their supervision is relatively easy to acquire, although it appears their approach is sensitive to choice of initial labeled set. ", "summary_of_strengths": "- Simple yet effective method for unsupervised constituency parsing that is EM-like. Competitive with existing models.\n- Provides additional value through interesting ablations and extensive analysis. ", "summary_of_weaknesses": "- It is not clear what effect the initial set of candidate constituents and distuents has on final performance. Multiple times the authors claim this form of supervision is “minimal”, and although it is certainly simple to use, perhaps different initial sets would yield different results. Based on Appendix A.1 it does seem like several ways of exploring initial set were explored, although no results reported. There are some results with this respect in section 5.3 showing that left-branching and random strategies to build initial seed set yield very low performance, and authors also state “the manner in which we perform the initial classification has a strong impact on the final tree structure” here. It would greatly help to be more consistent about the effect of the initial seed set throughout the text.\n- It’s not clear how h_in(i, j) and h_out(i, j) are computed. For h_in, two likely possibilities are: a) RoBERTa is run over only the tokens i:j and the last vector is used to represent the phrase, or b) RoBERTa is run over an entire sentence x, and two or more output vectors are concatenated to get phrase vectors. If (a) is used, this seems clean although a bit expensive. If (b) is used, it seems like this would have trouble distinguishing between inside and outside. In either case, the text should be more clear about how this is done otherwise reproducibility would be quite difficult. ", "comments,_suggestions_and_typos": "- Do you think that validation using performance on the dev set (using early stopping or hyperparam selection) would be helpful? There are many reasons why people are interested in methods that could benefit from validation (even for unsupervised learning) because it can help find a model with a good inductive bias that does not overfit to given labels. I suppose by reporting three settings (inside, inside w/ self-training, inside + outside) you are doing a type of validation over hyperparams.  - Do you think your method of seed bootstrapping would be generally applicable to other types of unsupervised parsing models?\n- What does it mean to re-normalize class probability scores in Fig 3?\n- How much does the set of labels grow each epoch?\n- How did you choose min/max threshold values, and what are these values? Also, how did you decide 1:10 ratio of constituents to distituents? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 181], [181, 263], [263, 377], [377, 555], [555, 636], [636, 775]], "summary_of_strengths": [[0, 85], [85, 119], [119, 201]], "summary_of_weaknesses": [[0, 112], [112, 290], [290, 413], [413, 710], [710, 816], [816, 878], [878, 1133], [1133, 1192], [1192, 1289], [1289, 1410]], "comments,_suggestions_and_typos": [[0, 129], [129, 356], [356, 499], [499, 500], [500, 626], [626, 697], [697, 748], [748, 822], [822, 890]]}}}]
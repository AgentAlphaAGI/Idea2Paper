[{"rid": "8ef3fa547505100a7155ef38240ef5ca45862481fec01c740b08a06955216ff989e9a02a353c520fa4c22462ab04f36a7e8e5c394f29bb81dce5536e9e169cff", "reviewer": null, "report": {"paper_summary": "This review is for a resubmission that I reviewed earlier. Hence, I will be focusing more on the changes from the last submitted version.\nFirst of all, I would like to thank the reviewers for their detailed responses to my, and other reviewers', concerns.\nFor the previous version of this submission, I had two major concerns. First, one of the datasets used by the authors is not public. The authors have committed to releasing the dataset, pending approvals from the concerned organizations. I really appreciate the authors' willingness to do so and look forward to the valuable resource being available for the research community.\nMy other concern was that the job representations learned by the proposed approach were not used in downstream tasks that could illustrate the utility of the representations and how the representations fare with respect to other off-the-shelf representations (say using BERT). The authors have presented results on the next-job prediction task. ", "summary_of_strengths": "- Well-written paper, tackles an interesting application - The authors will be making a new dataset public which would be a useful resource for the community ", "summary_of_weaknesses": "I thank the authors for conducting additional experiments on testing the utility of learned job title representations for the next job prediction task. However, the results reported in the updated draft have reinforced my concerns regarding the effectiveness of the learned job title representations. As Table 3 shows, BERT-based representations outperform on the task when compared with representations learned by the proposed solution under various settings. Even on the classification task, the performance of BERT and Word2Vec methods is reasonably well. Thus, it is not clear what, if any, are the advantages of using the proposed solution for learning job title representations. Some gains in performance, in some settings, could certainly be achieved but at the cost of significantly more computationally expensive solution compared to using off-the-shelf BERT or W2V representations. ", "comments,_suggestions_and_typos": "In sum, I would keep my earlier scores unchanged. While I appreciate the efforts undertaken by the authors to test on a new task, the results, unfortunately, are not infavor of the proposed solution. "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 59], [59, 138], [138, 256], [256, 327], [327, 389], [389, 494], [494, 634], [634, 911], [911, 979]], "summary_of_strengths": [[0, 57], [57, 158]], "summary_of_weaknesses": [[0, 152], [152, 301], [301, 461], [461, 559], [559, 685], [685, 892]], "comments,_suggestions_and_typos": [[0, 50], [50, 200]]}}}]
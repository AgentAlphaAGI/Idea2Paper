[{"rid": "6ecfd29883f4de0596f01657286eca384c81d9340152dd210fe2cefab0b1e738a3ef1bad9c36497cb3f2de154898006c9239150ae44550ebd0e44f4330635be6", "reviewer": "Chen Jia", "report": {"paper_summary": "This paper proposed a new PLM-pruning method based on knowledge distillation, aiming to resolve the overfitting problem under the pretrain-and-finetune paradigm. In the beginning, the authors present a hypothesis that model pruning increases the risk of overfitting at the fine-tuning phase as well as some empirical results that demonstrate this hypothesis (Fig. 2). Then they proposed a knowledge distillation method to resolve such problems for PLM-pruning. The theoretical analysis shows that such a method can make the difference between the predictions of teacher and student so small with high probability (PAC).  The empirical results on the GLUE benchmark also show the effectiveness of the proposed method and achieve the best results compared with a range of baselines.\n----------Overall comments-----------   Overall, the method of PLM-pruning via knowledge distillation by sparse progressive is novel and interesting. Both theoretical and empirical results show the effectiveness of the proposed. The main concern to me are the significance and correctness of the theoretical analysis. ", "summary_of_strengths": "1.  This paper is well-written and well organized. In particular, the motivation is clear and the conclusion makes sense.\n2. The method of sparse progressive distillation for PLM pruning is novel and technical sound.\n3. The theoretical analysis can demonstrate the sparse progressive distillation can be well optimized for PLM pruning to some extend. This can make the work much solid.\n4. The experimental results are strong on a well-studied benchmark, GLUE, and the analysis are also extensive and thorough. ", "summary_of_weaknesses": "The main concern to me is about the theoretical analysis, which seems too brief to reveal the intrinsic mechanism of the proposed methods, details are listed as follows.\n- The corollary in line 245 assumes that $\\mathbf{w}^*_{i}$ $(1 \\leq i \\leq n)$ are drawn i.i.d. from a distribution on $[-1, 1]$. Is this. i.i.d. assumption holds for the proposed networks? Besides, I not that the range of $\\mathbf{W}$, $(-0.5, 0.5)$ is not equal to that of $\\mathbf{w}^*$'s. Could this difference effect that derivations?\n-  In the proposed method, the authors claim that $N$ sparse student modules have probabilities of $p_1, p_2, p_3, ..., p_N$ to substitute the corresponding teacher layers separately. But these probabilities have not been considered in the theoretical derivations. I guess these can affect the results.\n- The nonlinear transformations in the neural works, such as $exp$ and $tanh$ have not been considered in the theoretical derivations. To my understanding, these are non-trivial results and should not be omitted to obtain the Eq. (10) directly.\nAnother concern is why the reported results are all on the dev set but not on the test set, which is uncommon or maybe I have missed some thing. ", "comments,_suggestions_and_typos": "---------Suggestions---------   1. The detailed derivations of corollary and the derivations of Eq. (10) are important, which should be displayed in detail in the appendix or on the main pages.  2.  The Figures and Tables can be moved on the top for better readability.  3. Significance test can be conducted for the results of Tab. 1-3.\n----------Typos----------   - line 249: $\\mathbf{W}$ --> $\\mathbf{w}$. - Eq.(8) $c$ has not been defined.\n- line 269: missing the right $|$. - The title of Tab. 3 is the same as that of Tab. 1. ", "ethical_concerns": "N/A "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Chen Jia, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 162], [162, 368], [368, 461], [461, 620], [620, 781], [781, 931], [931, 1010], [1010, 1099]], "summary_of_strengths": [[0, 3], [3, 51], [51, 122], [122, 125], [125, 217], [217, 220], [220, 351], [351, 386], [386, 389], [389, 510]], "summary_of_weaknesses": [[0, 170], [170, 267], [267, 310], [310, 317], [317, 361], [361, 464], [464, 511], [511, 695], [695, 776], [776, 814], [814, 949], [949, 1059], [1059, 1204]], "comments,_suggestions_and_typos": [[0, 35], [35, 194], [194, 195], [195, 198], [198, 270], [270, 271], [271, 274], [274, 333], [333, 338], [338, 366], [366, 409], [409, 444], [444, 479], [479, 499], [499, 529], [529, 532]], "ethical_concerns": [[0, 4]]}}}]
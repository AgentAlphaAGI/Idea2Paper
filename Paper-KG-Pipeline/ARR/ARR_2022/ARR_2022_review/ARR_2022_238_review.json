[{"rid": "dbdc9d651568ad0167e6ae0f196e85eac0634cf2fa514717df2bf63857f999db1f31460186f8812fbb865f7861e8dc25e6a15248b3c848e9647e5f890c0c4415", "reviewer": null, "report": {"paper_summary": "This paper proposed to use linguistic (phonetic/phonological) features, rather than phoneme identities, for training TTS system. The experiments demonstrate how this method can be useful for low-resource settings in a meta-learning scenario. Another important contribution is the proposed adaptation of the model-agnostic meta-learning (MAML) to TTS, resulting in a novel procedure that the authors call LAML (language-agnostic meta-learning). The study shows that the proposed methods achieve good TTS results (in terms of both intelligibiligy and naturalness) with little training data. ", "summary_of_strengths": "The idea of using articulatory features is rather simple, but results in substantial improvements in terms of training time (in case of training on a single language) compared to the \"traditional\" training mode, and, more importantly, it enables the use of meta-learning for low-resource TTS settings. This seems to me like an important step in developing low-resource TTS, but I have very little knowledge of state-of-the-art TTS system and cannot provide a good evaluation. ", "summary_of_weaknesses": "Some methodological details seem to be rather underspecified in the paper: e.g., what are \"similar results\" in line 216, or what is \"much more difficulto to train\" in line 221, or how many is actually \"many updates\" in line 284, and what is \"subjective assessment\" in line 394?\nThe paper only presents an evaluation on German and may or may not work in the actual low-resource settings. In particular, what are the exact features used? As far as I know, IPA-based features may be tailored towards English or other Indo-European languages, which may be a problem for low-resource languages. ", "comments,_suggestions_and_typos": "Line 517: typo - \"improveD\". \nLine 542: I assume Dutch and Finnish were removed to remove the phonemes that are also present in German, but it would be useful to say that explicitly. Also, isn't there overlap between German and the other training languages? "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 129], [129, 242], [242, 444], [444, 589]], "summary_of_strengths": [[0, 302], [302, 476]], "summary_of_weaknesses": [[0, 278], [278, 387], [387, 436], [436, 590]], "comments,_suggestions_and_typos": [[0, 29], [29, 183], [183, 258]]}}}, {"rid": "58a851a5e8bd4338e751bd63f301990b343577dacb7dd93bac5f6629c470a9efe717c7d668c23cec1633763c45b922db06d87ee8f2fa43a730f25b00bf044d8a", "reviewer": "Jiatong Shi", "report": {"paper_summary": "The paper targets low-resource text-to-speech by using the articulation features instead of phoneme identity. The model is feasible to perform text-to-speech with 30 minutes of training data. Three contributions are (1) articulatory features for TTS; (2) LAML which is a modified version of MAML; (3) Interesting analysis over the framework could be very helpful for practical usage. ", "summary_of_strengths": "(1) The paper proposes a training procedure adopted from MAML so as to fine-tune a TTS model for low-resource scenarios.\n(2) The paper verifies the feasibility of using articulate features for TTS training and its extension for low-resource scenarios.\n(3) Experiments are with exciting discussions over the framework's capacity. ", "summary_of_weaknesses": "The main story is convincing, but some experiments are lacking or incomplete.\n(1) In line 96, the authors state that articulatory features could be beneficial than phoneme identities. While in the experimental section, the authors discuss the results on embedding similarities (articulatory features v.s. phoneme identities) and the convergence time with alignment examples. Indeed, attention alignment could reveal the results, but the reviewer would more want to see the results from your subjective results and also the convergence time for a fully trained model with specific features.\n(2) In section 4.2.1, It would be good to show when the low-resource model could have a matched performance as the baseline. ", "comments,_suggestions_and_typos": "(1) It's not clear why the author would use the phoneme embedding from a tacotron 2 model as gold data, which seems contradictory to the paper's statement. Suppose the articulatory features are learned to be very similar to phoneme embedding. In that case, it's difficult to say the learned feature could be better than phoneme embedding (discussed in 4.1.2).\n(2) The reviewer is interested in how the LAML works in settings where the data size for each language is imbalanced. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Jiatong Shi, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 110], [110, 192], [192, 384]], "summary_of_strengths": [[0, 121], [121, 252], [252, 329]], "summary_of_weaknesses": [[0, 78], [78, 184], [184, 375], [375, 590], [590, 715]], "comments,_suggestions_and_typos": [[0, 156], [156, 243], [243, 360], [360, 478]]}}}]
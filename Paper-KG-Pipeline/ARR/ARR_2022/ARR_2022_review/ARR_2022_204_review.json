[{"rid": "06ef2506cddbabc6f971fd981c0115e074e05625f96fb064ce1622e6ab08a5e9791a16985be5e61fe376bc3c2b29830e39a4bdef10b30a4d56f36841397267c2", "reviewer": "Shuyan Zhou", "report": {"paper_summary": "This paper proposes a new task formulation to solve complex tasks. In this new formulation, there are multiple agents, each of which is capable of solving some specific types of tasks. For example, there can be a QA agent that answers natural language (NL) questions and an instruction following agent that could execute actions to accomplish an NL intent. Given a complex task described in NL, the model is asked to communicate with each agent for their task-specific knowledge and use the returned answers to proceed with the task.\nIn this work, they instantiate the complex task as multi-hop QA and the agents as a TextQA agent that is able to reason over large text corpus, TableQA agents that could answer questions given structured data like tables, and MathQA agent that could perform numerical reasoning. Each agent also has their own auxiliary data like (NL, answer) supervision and their independent KBs. They design a model that is able to decompose a multi-hop question to simple questions that could be answered by one agent. They compare this model with other black-box models that do not perform communication with agents and show significant improvements on a synthetic dataset they create. ", "summary_of_strengths": "- The proposed new task formulation is novel and interesting. Intuitively, it is a promising way to resolve the complex tasks people encounter daily. The paper also provides a detailed and clear definition of this new task. ", "summary_of_weaknesses": "- The instantiation of the task could not fully justify the benefit of the new task formulation. In this new proposed setting, an ideal criterion for designing individual agents is that each has mutually exclusive functionalities, and it is challenging to develop a unified model. For example, the google search agent and the Alexa shopping agent described in the introduction make such a case. However, this work design a synthetic dataset, and the agents are separated by the different forms of knowledge (text vs table) and the different proportions of knowledge in the KB. This separation is OK as long as it could reveal the true distribution in reality -- there is some knowledge that is more accessible through text than structured data and vice versa. However, the data construction process did not consider this and did a random split. A more realistic setting will bring up some interesting questions like \"how does the decomposer know which agent is more suitable to answer the current question?\", \" how can we curate such annotations?\" etc, which are not explicitly touched by the current work. To me, my main takeaway is that question decomposition is helpful, which has been studied in previous works like BREAK (Wolfson el at + 2020). Related to this concern, I also have a question regarding training the question decomposition component. According to F3, the NL questions to the text agent and the table agent look pretty similar (e.g. [table] What movies has #1 written? vs. [text] #1 produces which materials?), what are the supervision signals that hint the model to predict one agent over another?\n- Some descriptions of the experiment setting are somewhat vague, and therefore it is not super clear whether the comparisons are fair. My main question is how factual knowledge is provided to each model? \n     * In *Models with Access to Agent Knowledge*, how do you construct the context? Do you randomly sample some context from the *possible world* of the question? \n     * Do you somehow distinguish the source (e.g., knowledge of TextQA, knowledge of TableQA)? \n     * After decomposing the question through `NextGen`, how do you provide the context when querying an individual agent? Do you provide the ground truth context without distractors? Or do you train some retriever (like in *Models with Fact Supervision*) to retrieve the context? ", "comments,_suggestions_and_typos": "- Some case study and more systematic error analysis can probably help the readers to understand in which cases the proposed method works and how. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Shuyan Zhou, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 67], [67, 185], [185, 357], [357, 534], [534, 813], [813, 915], [915, 1039], [1039, 1207]], "summary_of_strengths": [[0, 62], [62, 150], [150, 224]], "summary_of_weaknesses": [[0, 97], [97, 281], [281, 395], [395, 577], [577, 760], [760, 845], [845, 1011], [1011, 1048], [1048, 1107], [1107, 1250], [1250, 1355], [1355, 1489], [1489, 1531], [1531, 1619], [1619, 1755], [1755, 1824], [1824, 1910], [1910, 1989], [1989, 2086], [2086, 2210], [2210, 2271], [2271, 2368]], "comments,_suggestions_and_typos": [[0, 147]]}}}]
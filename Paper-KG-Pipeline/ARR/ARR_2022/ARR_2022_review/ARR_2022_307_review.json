[{"rid": "e640aeb3c0d1364ba9b7f6cf9726d81ee1b7658aac82daa164eadeb99fe8ab7abfeb9a3f325a392088645947b59609be4db7ac346f54e31d2e0ac5cf46b41b74", "reviewer": null, "report": {"paper_summary": "The authors address a problem that underlies the understanding of procedures: They introduce a task of sequencing unordered subtasks/steps of multimodal instructions, and describe the curation of two datasets (RecipeQA and WikiHow) for this task. \nThey propose a sequence-aware pre-training method for improving current models' ability of reasoning over multimodal information to solve the task (MLM, (patch-based) \"image-swapping\" prediction ((P)ISP), sequential MRM). \nThe models used for the experiments are Faster-RCNN and CLIP (vision), RoBERTa, and the multimodal single-stream models VisualBERT and CLIP-ViL.  Experimental results on the two new datasets show that (i) multimodality is beneficial, where humans seem to more effectively benefit from the visual information than the models, that (ii) models fall short against human performance irrespective of the modality, that (iii) the sequence-aware pre-training method the authors propose slightly improves effectiveness. An analysis on specific categories of WikiHow additionally shows that the multimodal models do not effectively use both modalities. ", "summary_of_strengths": "- Release of two curated datasets for sequential procedure learning - Well written, clear motivation, good experimental methodology (ablation study, comparison of multiple state-of-the-models, comparison against human performance).\n- Clear experimental findings: visual information relevant for sequential procedures, but improved models that more effectively use multimodal information is required, such as to come closer to the human upper bound ", "summary_of_weaknesses": "- Training is performed on each dataset separately. This may result in the models being quite domain-specific, i.e., limiting their generalisation ability.  - Method is limited to 2 sequences (minor point) ", "comments,_suggestions_and_typos": "- PISP objective: I can see how patch-based swapping may help for learning a better intra-modal association in the case where the patches depict some objects the text refers to. But what is the assumption regarding the benefit of general patch-based (as in CLIP-ViL) swapping for the sequencing task (general intra-modal understanding is already the motivation for SMRM, isn't it?)?\n- Sect 4.2.3: Maybe an illustration would help to explain this part, I found it hard to understand.\n- Sect 4.3: The output of the BERSON decoder is simply the order, i.e., integer values (e.g., 0 and 1 in your case of sequences of 2?), isn't it?\n- Section 5: Why did you not also compare against a pre-trained ResNet version, as you did for CLIP (image-only)?\n- Section 5: How would you explain the gap of the effectiveness in the Image-Only setting on WikiHow vs. RecipeQA? For the other settings/models/modalities, the difference in effectiveness is comparably marginal.  **Typos, Grammar, Style, and Presentation Improvements** - l135: \"consists\" -> \"which consists\" or \"consisting\" - Section 3.2: The amount of footnotes could be reduced.  - l318: select *the* same - l.466: *the* text-only "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 247], [247, 470], [470, 617], [617, 983], [983, 1115]], "summary_of_strengths": [[0, 68], [68, 232], [232, 448]], "summary_of_weaknesses": [[0, 52], [52, 156], [156, 157], [157, 206]], "comments,_suggestions_and_typos": [[0, 178], [178, 383], [383, 483], [483, 619], [619, 629], [629, 743], [743, 858], [858, 956], [956, 957], [957, 1014], [1014, 1069], [1069, 1126], [1126, 1127], [1127, 1153], [1153, 1178]]}}}, {"rid": "532fc491f93f0c284582f3e9486ca8322ab9bdb2049bf4ed09f49b8c90259b96f8d87aa1e4435b82edff75935ed83b869ff07ab82a9cc497001bf398c8ba911d", "reviewer": "Yao Zhang", "report": {"paper_summary": "This work focuses on the multi-modal sequence ordering task. The authors first curate two datasets from online instructional manuals. The two datasets are about cooking recipes and “How-To\" instructions (WikiHow) respectively. Moreover, the authors also collect annotations of possible orders alternative to the originally authored ones to create multiple references. Then, the authors measure the ability of state-of-the-art AI techniques to sequence instruction steps. To utilize the multimodal information sufficiently, the authors propose to equip models with the capabilities of performing multimodal grounding with sequential awareness. Through the experiments, the authors provide some insights that will benefit future research. ", "summary_of_strengths": "- The annotation of possible orders can facilitate better evaluation and is a useful resource for future research.\n- The paper is very well written and organized. ", "summary_of_weaknesses": "[Minor Weaknesses] In the Related Work section, there is a lack of literature on the Multimodal Machine Comprehension (M3C) task. M3C has several multi-choice subtasks. One of these subtasks, visual ordering, is strongly relevant to this paper. Therefore, a survey of related work on M3C [1,2] should be conducted.\n[1] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Are you smarter than a sixth grader? Textbook question answering for multimodal machine comprehension.\n[2] Ao Liu, Shuai Yuan, Chenbin Zhang, Congjian Luo,Yaqing Liao, Kun Bai, and Zenglin Xu. 2020. Multi-level multimodal transformer network for multi-modal recipe comprehension. ", "comments,_suggestions_and_typos": "See the {Summary Of Weaknesses} part. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yao Zhang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 61], [61, 134], [134, 227], [227, 368], [368, 471], [471, 643], [643, 737]], "summary_of_strengths": [[0, 115], [115, 163]], "summary_of_weaknesses": [[0, 130], [130, 169], [169, 245], [245, 315], [315, 421], [421, 427], [427, 464], [464, 530], [530, 620], [620, 626], [626, 707]], "comments,_suggestions_and_typos": [[0, 38]]}}}]
[{"rid": "68c709e853fc85c4a51a2bf31e9323929a8768c91c3f21e30970ee38b5a6ecfa7ccb0abbc60c80205f631bbbaa1a83fdcb50ce32cdd591a45dfbf3dbe31d5180", "reviewer": "Younes Samih", "report": {"paper_summary": "This paper tackles probing for Predicate Argument Structures (PASs) in Pretrained Language Models (PLMs).  Using standard probing techniques, the authors claim to probe PLMs for PASs diffrently compared to previous work. They employ probes (linear and non-linear classifiers) to quantify the strength of the learned representation with respect to a linguistic property.  The authors also provide a thorough analysis of how PLMs encode  predicate argument structure information and showcased  how verbal and nominal predicates and their PASs are represented differently. Finally, they empirically demonstrated how the effective integration of  of predicate-argument structure knowledge in SRL systems can boost their accuracy. ", "summary_of_strengths": "1) The paper is written well and is a pleasure to read (although some details are missing) 2) The paper presents detailed results and the findings are interesting. \n3)  Enlightening idea: verbal and nominal predicates and their PASs are represented differently 4) the integration of predicate-argument structure knowledge into a state-of-the-art end-to-end architecture for SRL. ", "summary_of_weaknesses": "Despite the good performance of the probing classifiers, it is difficult to determine whether PASs generalizations are learned, or whether a kind of separate rote memorization of each predicate take place. \nto ascertain that model learn significant generalizations, researchers tend to employ control tasks. Unfortunately, in this paper, the authors reduced the  procedure to initializing the weights of the language model at random. I do not think this is a principled way to ascertain that the intermediate representation actually encodes the property at hand. The authors should  use selectivity instead. It is computed by subtracting the performance of the probe with re-assigned labels from the reported proxy task performance. \nthe authors are asked to refer to Hewitt & Liang, (2019): \"Designing and Interpreting Probes with Control Tasks\" for more details. ", "comments,_suggestions_and_typos": "The paper can be further improved if the authors uses selectivity in their experiments. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Younes Samih, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 106], [106, 221], [221, 370], [370, 570], [570, 726]], "summary_of_strengths": [[0, 164], [164, 379]], "summary_of_weaknesses": [[0, 206], [206, 308], [308, 434], [434, 563], [563, 608], [608, 733], [733, 865]], "comments,_suggestions_and_typos": [[0, 88]]}}}]
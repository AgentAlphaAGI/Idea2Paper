[{"rid": "6009657cb5982238bd67a0f1aa37f7fc5ad6ad71f9cb26e8f6af75969ddbdc1b60b57265bc7f9c233fd94bbc8bc5804029022bac025fba3874a9407e48666751", "reviewer": "Shuo Wang", "report": {"paper_summary": "This paper propose to use an additional loss to automatically learn model confidence for NMT models. Experiments show that the learned confidence can better reflect the quality the model output and using the learned confidence can result in better BLEU score. ", "summary_of_strengths": "1. The work is clearly motivated, the authors aim to learn a better calibrated confidence for NMT models, which is useful in many practical scenarios. \n2. Experiments indicate the effectiveness of the learned confidence. ", "summary_of_weaknesses": "1. Previous studies find that the mis-calibration problem is closely related with model size (Guo et al., 2019; Wang et al., 2020). In this work, the authors did not conduct experiments using larger NMT models, making the results less convincing. \n2. Table 1 shows that the proposed method only brings marginal improvements in terms of Pearson’s correlation. \n3. Table 2 shows that the learned confidence score can result in better BLEU score. To my knowledge, model confidence can affect the BLEU score with difference beam sizes. I wonder wether the proposed method can improve BLEU in larger beam size (i.e., beam size = 100). Moreover, in Table 2, the authors did not compare the proposed method with graduated LS (Wang et al., 2020), which is a closely related baseline. ", "comments,_suggestions_and_typos": "In Table 4, I wonder whether \"the model probability\" is estimated using MC dropout. Since MC dropout is very useful in OOD detection. Please provide more details. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Shuo Wang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 101], [101, 260]], "summary_of_strengths": [[0, 3], [3, 151], [151, 155], [155, 221]], "summary_of_weaknesses": [[0, 3], [3, 132], [132, 247], [247, 251], [251, 359], [359, 363], [363, 444], [444, 532], [532, 630], [630, 776]], "comments,_suggestions_and_typos": [[0, 84], [84, 134], [134, 163]]}}}, {"rid": "fcc2d390db2717ce8d77836ca10248ee73a8d94600bd9528cef5d26d16f18393f230b60463fae854e5f652936577b5b37ccc698e95ca831fa89d8e7a12079552", "reviewer": "Raphael Shu", "report": {"paper_summary": "The authors propose to add a confidence estimating network, acting as a gating unit that allow ground-truth probability to be merged in to the decoder probability. The confidence network will produce a loss by adding up the gating values, which is combined with cross-entropy loss in training with a hyperparameter lambda. ", "summary_of_strengths": "- In Table 1, Conf outperforms TP (model probability) for all language pairs.\n- proposed an lambda annealing schedule to avoid training the confidence network as an finetuning step ", "summary_of_weaknesses": "- In Table 1, although Conf outperforms TP, but the advantage of D-Conf comparing to D-TP is modest.  Given these numbers are Pearson's correlation, practically if the correlation cannot be improved by around 10% in absolute term, it's unlikely people will adopt it for estimating confidence scores given the extra cost on model training, inference cost and hyperparameter optimization.\n- The hyper-parameter is crucial for the proposed loss in this paper. If lambda is too strong, then no confidence will be learned, c_t is always 1. If lambda is too small, perhaps the model will always produce c_t = 0 to peak the ground truth. However, critically, I was not able to find information about how the authors found/optimized the hyper-parameter. ", "comments,_suggestions_and_typos": "Many mistakes, for example Eq.6 does not have the summation. And also grammar/spelling mistakes. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Raphael Shu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 164], [164, 323]], "summary_of_strengths": [[0, 78], [78, 181]], "summary_of_weaknesses": [[0, 101], [101, 102], [102, 387], [387, 457], [457, 535], [535, 631], [631, 746]], "comments,_suggestions_and_typos": [[0, 61], [61, 97]]}}}, {"rid": "70c276a941604f816f20431d12c06be45f7854f0691251ae78bef0a03badc35d74f7bea9f4e0a12885238dcae74a86455c3ead952e19ca136e4eacaf490adc28", "reviewer": "Hanjie Chen", "report": {"paper_summary": "This paper proposed a confidence estimation method for neural machine translation (NMT) by jointly training the NMT model with a confidence network which learns to output a confidence score per example. The confidence score (a scalar between 0 and 1) is used to provide “hints” for the NMT model, that is interpolating the original prediction probabilities with the ground truth probability distribution. Higher confidence indicates less hints provided. The two models are trained jointly, where NMT learns the task and the confidence network learns to produce the correct confidence. Besides, the confidence is also utilized to smooth labels for preventing miscalibration. Experiments on several quality estimation tasks demonstrate the effectiveness of the proposed method in improving model performance and detecting noisy samples and out-of-domain data. ", "summary_of_strengths": "1. \tThis paper focused on an important problem in estimating confidence for poorly calibrated NMT models. Different from previous work based on Monte Carlo dropout, the proposed method, learning confidence estimation during training, is more efficient and may be benefit for future research. \n2. \tThe paper is well-written and easy to follow. The experiments are sufficient and promising. ", "summary_of_weaknesses": "1. \tSince an additional confidence network has been involved in producing confidence score, how to ensure the confidence network would not be over-confident or under-confident? Would this be an endless loop if another network is needed to assess the uncertainty of the confidence network? \n2. \tThe improvement compared to other unsupervised methods is not impressive, while there is still a big gap with the strong QE model BERT-BiRNN. ", "comments,_suggestions_and_typos": "N/A "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Hanjie Chen, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 203], [203, 405], [405, 454], [454, 585], [585, 674], [674, 858]], "summary_of_strengths": [[0, 3], [3, 106], [106, 292], [292, 296], [296, 343], [343, 389]], "summary_of_weaknesses": [[0, 3], [3, 177], [177, 289], [289, 293], [293, 436]], "comments,_suggestions_and_typos": [[0, 4]]}}}]
[{"rid": "4a1c1a54db4c8630e04e4e5f045f69749e7bf8d6b6bf7fe1b5abc8f64b63aac51dbe6e0e4f0a4a667815ef7b8e80d00ca2eac00026243514c420f95834081e42", "reviewer": null, "report": {"paper_summary": "This paper empirically studied CLIP models as few-shot learners for two vision-language understanding tasks: VQA and Visual entailment. In the VQA task, the paper proposed a two-step method to mitigate the gap between natural language description and question answering. In addition, the paper used only a very small set of parameters in CLIP models during fine-tuning, including bias and normalization terms. ", "summary_of_strengths": "It studied how to transfer CLIP zero-shot capabilities into VLU tasks and confirms CLIP models can be good few-shot learners.\nThe paper proposed a two-step prompt generation method to apply CLIP on VQA.\nThe paper identified only a small number of parameters are enough to fine-tune the CLIP few-shot learner. ", "summary_of_weaknesses": "The way of using T5 for template generation is unclear, and lack of evaluation of the template generation quality.\nIn model fine-tuning experiments, it is unclear about the learning rates and epochs for different parameter settings, which could largely affect the results. ", "comments,_suggestions_and_typos": "See the weakness. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 136], [136, 271], [271, 410]], "summary_of_strengths": [[0, 126], [126, 203], [203, 309]], "summary_of_weaknesses": [[0, 115], [115, 273]], "comments,_suggestions_and_typos": [[0, 18]]}}}, {"rid": "7c0a758d59caeaedb368c857bc3a695af2f04d314c12228f0c5e970d6b975ca00df45df67b9ee9585e02f1f78d72bfacac95f9bf8bf0f8f1eaf0e6f4c5391199", "reviewer": null, "report": {"paper_summary": "The authors of this paper propose an empirical study for the zero-shot capabilities of CLIP models and demonstrate that CLIP models have strong few-shot capabilities. The authors propose the TAP-C method for evaluating VQA and demonstrate zero-shot cross-modal transfer capabilities on the visual entailment. ", "summary_of_strengths": "1. The paper conducts rich experiments and presents several interesting insights, which have a certain value to the community. \n2. The paper is well-written and easy to follow. \n3. The proposed two-step prompt generation method is interesting for studying the zero-shot performance. ", "summary_of_weaknesses": "1. There should be more VLU tasks to prove the argument of the paper, e.g., NLVR. ", "comments,_suggestions_and_typos": "1. It is best to use vector graphics in the paper. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 167], [167, 309]], "summary_of_strengths": [[0, 3], [3, 127], [127, 131], [131, 177], [177, 181], [181, 283]], "summary_of_weaknesses": [[0, 3], [3, 82]], "comments,_suggestions_and_typos": [[0, 3], [3, 51]]}}}]
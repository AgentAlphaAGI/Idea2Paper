[{"rid": "df15fa78d3331e468f13cb18ab0eff830f0f65e2611ec4c8709ae57c17d54057fdafa4981ebcef3ea40e9d61d35e7d80ab9fbd01d5b4687cc1bc2cd63d02aeb6", "reviewer": null, "report": {"paper_summary": "This paper proposes a framework for training and extract-the-generate model for the long document summarization task. The premise is that the extractor should pass on important information from the source to the generator for producing abstractive summaries. To train the entire network, there have been three losses defined (generator, oracle, and consistency), of which generator is the decoder loss, oracle is used to optimize the extractor with oracle labels, consistency is used to marginalize dynamic attention with extractor distribution. Results are promising on two long summarization datasets (GovReport and QMSum) and competitive on arXiv. ", "summary_of_strengths": "-The paper is easy-to-follow and understandable in most parts. \n-The problem is well-approached, although it has not been motivated much in the paper. \n-Results outperform the prior SOTA by a large margin on two long datasets (GovReport and QMSum). ", "summary_of_weaknesses": "-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.\n- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here.  - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.\n-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.\n- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. ", "comments,_suggestions_and_typos": "In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 118], [118, 259], [259, 546], [546, 651]], "summary_of_strengths": [[0, 63], [63, 151], [151, 249]], "summary_of_weaknesses": [[0, 144], [144, 226], [226, 386], [386, 540], [540, 613], [613, 742], [742, 743], [743, 870], [870, 1085], [1085, 1233], [1233, 1255]], "comments,_suggestions_and_typos": [[0, 256], [256, 311]]}}}]
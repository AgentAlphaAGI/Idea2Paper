[{"rid": "0f4cb8033e92de1eecebb650474462752b609f5a11cdf226e790163ec2a020aeb4d08d7d750bc75fd8d7c0be415881528a542492eeff9c184f1fd6b090129258", "reviewer": "Rob Voigt", "report": {"paper_summary": "In this paper the authors expand on the CrowS-Pairs dataset from Nangia et al. 2020 for evaluating stereotyping in large language models by constructing a parallel version of the corpus in French, including both translated examples and some novel examples specific to the French context. The authors catalog numerous aspects of the process of doing so, including identifying some issues with the original dataset which they work to remedy. They test multiple BERT variants on the new corpora and show that multilingual BERT exhibits the lowest bias. ", "summary_of_strengths": "The paper is clear and well written. The annotation and corpus development process followed is well-described, and seems incredibly careful and thoughtful. Readers will learn a lot about stereotyping phenomena in language as well as the crucial details of corpus development from reading this paper. I see few flaws; it simply seems like careful, interesting work. ", "summary_of_weaknesses": "The primary one I would mention is when I read the abstract and start of the paper, I was excited about and hoping this paper would include analysis that attempted to quantitatively compare the English and French examples to extract instances of differing bias. The paper does this very well qualitatively and with careful human analysis, but not the sort of quantitative comparison I was expecting - I think this would be an exciting area for future work.\nAlso, is there a notion of statistical significance that could be applied here to understand whether the biases we see are greater than expected by chance? ", "comments,_suggestions_and_typos": "For clarity I would add significantly to captions; for instance, Table 4 should specify the scale of the scores and that e.g. a score of 50 indicates an absence of bias. Also double-check some of your numbers. For instance, the text says \"Multilingual BERT is the model with the lowest degree of bias, with a score of 53 for English and 50.17 for French,\" but the table says 52.9 for English, so why the difference in significant figures?\nThis is likely a topic for future work, but a core question that arose for me is why multilingual BERT displayed so much less bias than the other models. Is this something that could be explored further? "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Rob Voigt, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 288], [288, 440], [440, 550]], "summary_of_strengths": [[0, 37], [37, 156], [156, 300], [300, 365]], "summary_of_weaknesses": [[0, 262], [262, 457], [457, 613]], "comments,_suggestions_and_typos": [[0, 170], [170, 210], [210, 439], [439, 593], [593, 643]]}}}, {"rid": "c139a68e1cc57071a551aed2e93e04883f7538d202c23f452ebbe15a38dbf24c5a5c79bced7f223849d8a5399514ca784ff3fea124d58130e66ee37f7ed1e9b5", "reviewer": null, "report": {"paper_summary": "This paper presents an extension of the CrowS-pairs dataset (Nangia et al., 2016) for French. CrowS-pairs consists of about ~1500 English sentence pairs, where one sentence of each pair shows a certain type of social bias, and the other is an unbiased version of it. In the extension, each sentence has been translated by two authors of the paper, taking care of adjusting the bias properly to French contexts/culture. Moreover, 212 new French sentence pairs have been added in a crowdsourcing process, in which also the translations and the bias types were checked.\nIn experiments on the dataset, the paper analyzes how much bias different BERT-based language models show for French (in comparison to English). Several insights into the problems and solutions of translating biased sentences are given afterwards. ", "summary_of_strengths": "1. Social bias has largely been studied for English only so far. The dataset contributes to enabling multilingual social bias research, and it seems to be constructed thoroughly in different respects. I have little doubt that the data is of reasonable quality. The dataset is promised to be made available.\n2. The paper's discussion convinced me of the quality of the translation, even though I was skeptical at first whether translation can work here. The examples and statistics in Tables 1 and 2, along with the descriptions in the text, emphasize the effort that was put into obtaining sentences that make sense for French (such as adjusting names, culture-related concepts, etc.).\n3. The experimental results give useful insights into the behavior of the language models with respect to social bias in French text. With my given but limited knowledge of French, I was able to follow the discussion well, also because the examples are well-chosen and not too complex. Without any knowledge of French, some details may be hard to assess for readers, but I don't think this anyhow avoidable for such a topic. ", "summary_of_weaknesses": "1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.  2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.\n3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.\n4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better. ", "comments,_suggestions_and_typos": "- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.\n- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?\n- 241: It would also be good to state the maximum number of tasks done by any annotator.\n- Table 3: Right-align the numeric columns.\n- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability.  - Table 4 (2): The table exceeds the page width; that needs to be fixed.\n- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column).  - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.\n- 305/310: Marie/Mary >> I think these should be written the same.\n- 357: The text speaks of \"53\", but I believe the value \"52.9\" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.\n- 575/577: \"1/\" and \"2/\" >> Maybe better use \"(1)\" and \"(2)\"; confused me first. ", "ethical_concerns": "Nothing of concern. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 94], [94, 267], [267, 419], [419, 567], [567, 712], [712, 815]], "summary_of_strengths": [[0, 3], [3, 65], [65, 201], [201, 261], [261, 307], [307, 310], [310, 453], [453, 686], [686, 689], [689, 820], [820, 972], [972, 1111]], "summary_of_weaknesses": [[0, 3], [3, 110], [110, 201], [201, 238], [238, 352], [352, 490], [490, 491], [491, 494], [494, 617], [617, 721], [721, 851], [851, 854], [854, 1044], [1044, 1203], [1203, 1282], [1282, 1285], [1285, 1376], [1376, 1466], [1466, 1575]], "comments,_suggestions_and_typos": [[0, 121], [121, 191], [191, 226], [226, 315], [315, 359], [359, 481], [481, 514], [514, 515], [515, 588], [588, 792], [792, 793], [793, 852], [852, 891], [891, 958], [958, 1044], [1044, 1118], [1118, 1199]], "ethical_concerns": [[0, 20]]}}}]
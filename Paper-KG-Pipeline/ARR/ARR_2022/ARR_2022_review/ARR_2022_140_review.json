[{"rid": "03eb1b3037d43f1d0e8f00b17dbda971f9bcf719a0fd8ba58576d612b9fa14f882a7e75a2f724c6afe76cb89e746ee49b1352d6b2fb275ca47aeacdc751dc8af", "reviewer": "Kang Liu", "report": {"paper_summary": "The paper addressed the knowledge graph completion task and proposed a commonsense-aware knowledge embedding method to learn the distributed representations of the entities and relations in the KG. The main contributions have two folds. The first is to extract commonsense to guide the negative sampling. And the second is to develop a multi-view link prediction mechanism. The experimental results on four datasets show the effectiveness of the proposed method. ", "summary_of_strengths": "Employing the extracted commonsense knowledge to guide the negative sampling is somewhat novel and interesting.\nThe experimental results show that adding the proposed negative sampling method to different kinds of knowledge graph completion methods is useful. ", "summary_of_weaknesses": "The proposed method seems to be only useful for \"N-1\" and \"1-N\" relation types. How it works for \"1-1\" and \"N-N\" situations. The authors should show the detailed performance of different relation types. ", "comments,_suggestions_and_typos": "How does the performance goes when we tune the parameter alpha in equation (6) and (7)? How to determine the value of such a parameter?\nWhen the method extracts commonsense, there is a hierarchy structure among concepts. Therefore, how to determine the head concept and tail concept? Do the method extract grandpa nodes? "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Kang Liu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 198], [198, 237], [237, 305], [305, 374], [374, 463]], "summary_of_strengths": [[0, 112], [112, 260]], "summary_of_weaknesses": [[0, 80], [80, 125], [125, 203]], "comments,_suggestions_and_typos": [[0, 88], [88, 136], [136, 221], [221, 284], [284, 321]]}}}, {"rid": "fb8abed24b895c60af95d65f500024e37b39c870800b09362cb415350eecdb493573e965cb693012ce459c254f4446514a17effdbceccb50b7ce870bb3bc6119", "reviewer": null, "report": {"paper_summary": "This paper presents a link prediction method where knowledge from a commonsense KG is used to guide negative sampling and inference. The general idea is that existing commonsense knowledge is applied to delimit the range of meaningful negative samples. In inference, prediction that can be grounded to commonsense knowledge is also more confident. ", "summary_of_strengths": "The proposed method is well-motivated and easy to follow.   Experiments have shown the method to effectively improve various base models for link prediction.   The description of the method is very clear, with nearly all details well-explained. ", "summary_of_weaknesses": "Training with incorporating of another commonsense view is somewhat incremental and very similar to the referred JOIE in this paper, especially in the inference stage. However, incorporating that view in negative example, IMO, still represents a novel contribution. ", "comments,_suggestions_and_typos": "393: traning -> training "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 133], [133, 253], [253, 348]], "summary_of_strengths": [[0, 58], [58, 158], [158, 245]], "summary_of_weaknesses": [[0, 168], [168, 266]], "comments,_suggestions_and_typos": [[0, 25]]}}}]
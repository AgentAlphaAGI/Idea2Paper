[{"rid": "dfd2ab193176f5966f14f5955687cf4e9b38cfdf5757e0cad23cca2bad60275ec30dfe3c06f55b0bf02741baed5b541ed734f52904675c5eaf7214f95f7a1ae3", "reviewer": null, "report": {"paper_summary": "This work provides a broad and thorough analysis of how 8 different model families and varying model sizes for a total of 28 models perform on the oLMpics benchmark and the psycholinguistic probing datasets from Ettinger (2020). It finds that all models struggle to resolve compositional questions zero-shot and that attributes such as model size, pretraining objective, etc are not predictive of a model's linguistic capabilities. ", "summary_of_strengths": "- The work is well-motivated and clear - A vast selection of models are investigated ", "summary_of_weaknesses": "- While the findings are interesting, there is little to no qualitative analysis to provide insight into why these effects might occur - I would expect an analysis such as this to have at least 3 runs with varying random seeds per model to give greater confidence in the model's abilities. A growing body of work indicates that models' linguistic abilities can vary considerably even across initialisations - The exploration and prompt design to adapt the GPT models to the tasks at hand is quite limited ", "comments,_suggestions_and_typos": "N/A "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 229], [229, 432]], "summary_of_strengths": [[0, 39], [39, 85]], "summary_of_weaknesses": [[0, 135], [135, 290], [290, 407], [407, 505]], "comments,_suggestions_and_typos": [[0, 4]]}}}, {"rid": "d2f89b026470a56fb3b20d882764ded6aaae66e094719dabee68a6f04faf7c111ff5c0f79d1bf1eb93be90b82a8c144f862783328987628be410f73426803ccc", "reviewer": "Tal Linzen", "report": {"paper_summary": "Most behavioral LM analysis work focuses on introducing a particular challenge dataset, and only evaluates a handful of LMs on that dataset. This article reports on a much more extensive experiment: the authors evalute a relatively large number of models (all transformer-based). This make it possible to draw helpful comparative conclusions: for example, model size does not predict performance, and all models fail on some zero-shot tasks, including the ones included in oLMpics which to this human seem quite simple. It also shows that some pessimistic conclusions based on a single model (e.g. Ettinger's negation dataset) are not quite as severe for other models. Overall, it is hard to discern a clear predictor of success (model size, training data size, etc, do not have a consistent effect). ", "summary_of_strengths": "The experiments and post-hoc analyses of the results are straightforward, well-motivated and clearly described. I would like to see this paper published. ", "summary_of_weaknesses": "Nothing serious comes to mind. This isn't exactly a groundbreaking paper in its methodology, but it's important to have this sort of experimental empirical work in the literature. ", "comments,_suggestions_and_typos": "Another study that showed limited correlation between model size and \"zero-shot\" performance is Hu et al 2020 ACL, A Systematic Assessment of Syntactic Generalization in Neural Language Models, https://arxiv.org/abs/2005.03692 . More generally, the paper could engage a bit more with the syntactic evaluation literature, much of which temporally precedes the semantics and commonsense evaluations included in the current paper.\nThe related work section conflates approaches that train a classifier on a model's internal representations (such as Tenney et al) \"zero-shot\" targeted behavioral evaluations such as Ettinger's and Goldberg's that use the model's original objective (word prediction), calling both of them \"probing\". It would be helpful to be clearer about this distinction and clarify which analyses in the paper follow which approach.\nOne suggestion to the authors is to change the title to something that is less focused on BERT, captures some of the more noteworthy empirical findings, and avoid \"muppet\" as a generic term for transformer-based language models.\nl. 331: \"same approach\" - which one? Both?\nl. 439: that that -> that Section 4.7: Consider reminding the reader what the age comparison task is -- I got a little confused here and had to search for the PDF for word \"age\". "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Tal Linzen, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 141], [141, 280], [280, 520], [520, 669], [669, 801]], "summary_of_strengths": [[0, 112], [112, 154]], "summary_of_weaknesses": [[0, 31], [31, 180]], "comments,_suggestions_and_typos": [[0, 229], [229, 428], [428, 728], [728, 848], [848, 1077], [1077, 1114], [1114, 1120], [1120, 1146], [1146, 1299]]}}}]
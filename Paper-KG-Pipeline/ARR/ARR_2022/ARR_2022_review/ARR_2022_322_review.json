[{"rid": "843c023710442ecf3c0ddc5241527b8974fbb7a8d66862d54a37f6fc571b8dadd628512fe83c8af4f76bded68c6feaa22401e38073b19d3afaad1d633a9da48f", "reviewer": "Heng Yu", "report": {"paper_summary": "This paper digs into the evaluation of Non-AutoRegressive machine translation model and reveals several flaws in standard evaluation methodology, to provide a better understanding of the merits and weakness of NAR model.\nA more fair and detailed comparison shows that the speed gap between AR and NAR models quickly shrink in more realistic usage conditions (larger batch or multi-core CPUs), which is kind of against the common belief that NAR model is significantly faster. ", "summary_of_strengths": "This paper shows a bigger picture in the speed evaluation of NAR models, with more settings and test-sets, we can get a better understanding of the merits and flaws of NAR models. I think it will have a positive impact on the research of NAR models.  And the experiment for comparison is solid and fair. ", "summary_of_weaknesses": "My major concern of this paper is the innovativeness.  I think the contribution of the paper lies in more on the engineering side: results on more test-sets to avoid overfitting, speed comparison not only on 1-batch GPU setting but also on x-batch and multi-cpu settings, etc. However, while a lot of content is on engineering side, there is very few new findings on algorithm or model side: the CTC model is not new, and no new evaluation metrics is proposed. \nSo I think the paper would be better to appear in related workshop, rather than a ACL long paper. ", "comments,_suggestions_and_typos": "1. In Table 3, the COMET score is added without enough explanation: why choose this metric and how it fits into this evaluation?   And with the observation of substantial difference in COMET and BLEU score, the author suggest that NAR models may rank poorly in human evaluation, why not add the human evaluation in the paper?  If it does rank poorly, it may reveal another important feature of NAR model. "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Heng Yu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 221], [221, 476]], "summary_of_strengths": [[0, 180], [180, 250], [250, 304]], "summary_of_weaknesses": [[0, 54], [54, 277], [277, 461], [461, 560]], "comments,_suggestions_and_typos": [[0, 3], [3, 129], [129, 326], [326, 405]]}}}]
[{"rid": "1dd29ceec33836eb1c7b716ff5afa91342e5f1f6bdf61d45cc0f5b2aff5870b670a48843f1a4a49eafbe3749ede52ee8dcfa3d975754d0db66c299d1d3fe021b", "reviewer": "Luciano Del Corro", "report": {"paper_summary": "The paper proposes a neural OpenIe system that generates compact extractions. The idea is exciting, and the results look promising. On the downside, the methodology lacks some details which are difficult to understand in the current version: 1. If the decisions are based on pair interactions in the table, how are constituents with more than three words handled? \n2. How are Constraints in 3.1.2 effectively implemented? For instance, how do the symmetry loss looks like? \n3. Methodology described in 3.2 and Figure 3. looks similar to [1], but I do not see the paper cited nor any other using a similar technique. \n4. If the method is trained in 4. Wouldn't the system just learn to reproduce the underlying rules of the automatic benchmark creation? \n5. Maybe a driving example describing step by step the example in Figures 2 and 3 could help to clarify the methodology. \n6. It is not well defined what the authors mean with \"compact\" extractions, maybe a crispier definition can bring a bit of light on the methodology and design choices.\n[1] Matching the Blanks: Distributional Similarity for Relation Learning, Soares et. al. ", "summary_of_strengths": "1. Interesting idea 2. Promising results ", "summary_of_weaknesses": "1. Lacks clarity in some important methodological details and problem definitions (see above) ", "comments,_suggestions_and_typos": "Potentially important citation [1] Matching the Blanks: Distributional Similarity for Relation Learning, Soares et. al. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Luciano Del Corro, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 78], [78, 132], [132, 242], [242, 245], [245, 364], [364, 368], [368, 422], [422, 473], [473, 477], [477, 520], [520, 616], [616, 620], [620, 651], [651, 753], [753, 757], [757, 875], [875, 879], [879, 1044], [1044, 1129], [1129, 1133]], "summary_of_strengths": [[0, 3], [3, 23], [23, 41]], "summary_of_weaknesses": [[0, 3], [3, 94]], "comments,_suggestions_and_typos": [[0, 31], [31, 116], [116, 120]]}}}, {"rid": "9897fd47f27a5635f032746e70df176df4cc22af4388de5be20fd85500491c65361dcf5993d53ef1a5d551443baa5ee0c2770c2778652861390170ef3ac41bf5", "reviewer": null, "report": {"paper_summary": "This work deals with a purported flaw of modern neural OpenIE systems: their tendency to extract overly specific arguments which are not useful for upstream tasks. The authors present a neural OpenIE system which produces “compact” extractions. The concept of “compactness” isn’t explicitly defined, but examples are provided and various proxies are supplied (e.g. triplets should be as short as possible without affecting the semantics, triplets shouldn’t contain arguments which are in themselves triplets).    The suggested solution is innovative in two ways: 1. It is based on a new training and evaluation dataset. This dataset is derived from a standard one using a heuristic algorithm which identifies and extracts compact triplets while filtering out complex ones.  2. A novel OpenIE algorithm is employed: it is based on a slot filling architecture recently suggested for relation extraction, but in this work it’s employed as part of a pipeline system which the authors show further increases performance. ", "summary_of_strengths": "1. The paper deals with an important practical problem of how to optimize the usefulness of Open IE triplets for upstream tasks.  2. The paper is clear and does a great job of reviewing the literature in this area and citing relevant sources.  3. The authors suggest a pipeline approach: a table filling stage for the identifying arguments and predicates, and a second stage model which links the two. I think combining these ideas is indeed novel, and the authors also show that the suggested solution performs better than an end-to-end solution based on table filling alone.  4. Both automatic and manual evaluation methods are used to show that the suggested solution performs better than existing approaches on IE datasets modified to contain compact extractions. ", "summary_of_weaknesses": "1. I think the paper goes a bit too quickly from the problem description of systems producing extractions which are too specific, to their specific flavor of “compact” extractions designed to solve this problem. The paper does cite different sources which explain the problem in more depth, but I don’t think there’s a consensus that the type of compact extractions the authors promote is the ideal solution for the problem. \nE.g. based on the criteria for informative extractions described in the paper “Annotating and predicting non-restrictive noun phrase modifications” (Stanovsky et. al 2016), extracting “Hercule Poirot” “is” “a Belgian detective” from the sentence  “Hercule Poirot is a Belgian detective, created by Agatha Christie” might not be specific enough (as the modification here seems restrictive). More discussion of this criteria and how it relates to compact extractions will be helpful.\n2. Continuing the above point, the evaluation in this paper is intrinsic and uses datasets modified to include extractions of the type the authors are promoting, so comparison to existing systems is not 1:1. Some kind of extrinsic evaluation (e.g. on a KBC or KB alignment task) could be helpful in showing that the suggested openIE method does in fact work better for upstream tasks.\n3. The authors give no indication of whether the modified datasets, code, system, or the data used in manual evaluation will be made available. Assuming these are not public, it is harder still to estimate whether the suggested approach is really a good fit for upstream tasks (this is also why I give this paper low reproducibility scores. If the authors were to share code/data, I would have assigned a higher score). ", "comments,_suggestions_and_typos": "Despite the stated weaknesses, I'm narrowly in favor of accepting this paper. It deals with an important and not frequently discussed practical problem (making OpenIE triplets more useful for upstream tasks), the approach is interesting and the evaluation shows that the system produces triplets of the type the authors advocate more accurately than other systems (I think more can be done to convince that this style of queries is indeed optimal for upstream tasks, but still, this work seems like a step in the right direction).\nI would highly encourage the authors to make the code, datasets and sample extractions public, as it can really increase the impact of this work, allow others to consume and evaluate the outputs in real-world tasks, etc. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "1 = They would not be able to reproduce the results here no matter how hard they tried.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 164], [164, 245], [245, 510], [510, 513], [513, 566], [566, 620], [620, 773], [773, 774], [774, 777], [777, 1016]], "summary_of_strengths": [[0, 3], [3, 129], [129, 130], [130, 133], [133, 243], [243, 244], [244, 247], [247, 402], [402, 577], [577, 578], [578, 581], [581, 768]], "summary_of_weaknesses": [[0, 3], [3, 212], [212, 425], [425, 589], [589, 816], [816, 908], [908, 911], [911, 1116], [1116, 1293], [1293, 1296], [1296, 1437], [1437, 1634], [1634, 1713]], "comments,_suggestions_and_typos": [[0, 78], [78, 531], [531, 752]]}}}]
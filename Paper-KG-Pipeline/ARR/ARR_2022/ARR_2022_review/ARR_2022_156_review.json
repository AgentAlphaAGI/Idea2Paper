[{"rid": "d3bfb0d0dd9547c28c9023c8ef3535b8e7cbb628dca2edb1bb760a964114a916bf35dd34abb10b152f79b8be9c90dae0d0d1fb9035738b9902b7d0c9011fd966", "reviewer": "Apoorv Umang Saxena", "report": {"paper_summary": "The authors propose a new method for the KG-to-text generation task. Similar to earlier work, their method S-OSC leverages pre-trained LMs (PLMs) and models the generation task as a sequence-to-sequence task by linearizing the KG triples. However, unlike previous PLM based methods, S-OSC pays specific attention to two things: (1) order of triples in the linearized sequence should match the ordering in GT sentence (2) part-of-speech tags can help the model know when to copy and when to generate new words. By having additional models/training procedures for these sub-goals, S-OSC is able to outperform the state-of-the-art on 2 public KG-to-text datasets (WebNLG and DART). ", "summary_of_strengths": "1. Through well-motivated and relatively simple modifications to the vanilla PLM-based KG-to-text models, the authors are able to achieve SOTA results on 2 large public datasets (WebNLG and DART). \n2. The authors perform extensive ablation studies and show how each of their modifications impacts model performance. Along with the use of multiple automated metrics (BLEU, ROUGE, METEOR), human evaluation is also carried out. ", "summary_of_weaknesses": "1. The results on WebNLG seem only marginally better compared to the second best model by Li et al [1] (BLEU-4 61.88 -> 61.90, Chrf++ 79.1 -> 79.7, CIDEr score is lower). For DART, there seems to be a significant jump compared to the baselines; however, the model by Li et al [1] is absent from this comparison (Table 2). \n2. It is unclear whether some of the baseline results reported are author reproductions or original reported numbers, since the numbers do not always seem to match what is reported in the original papers. Since the main point of the work is improvement in empirical performance, this needs to be made more clear.\n[1] Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models, Findings of ACL 2021 ", "comments,_suggestions_and_typos": "1. Why is the font in tables so small? Also, spacing between sections seems to have been significantly reduced compared to the ACL template. \n2. The original numbers in [2] for BART-* and T5-* do not exactly match the ones reported in table 1. Why is that?\n[2] Investigating Pretrained Language Models for Graph-to-Text Generation (https://github.com/UKPLab/plms-graph2text) "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Apoorv Umang Saxena, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 69], [69, 239], [239, 510], [510, 679]], "summary_of_strengths": [[0, 3], [3, 197], [197, 201], [201, 316], [316, 426]], "summary_of_weaknesses": [[0, 3], [3, 171], [171, 322], [322, 326], [326, 528], [528, 636], [636, 738]], "comments,_suggestions_and_typos": [[0, 3], [3, 39], [39, 141], [141, 145], [145, 244], [244, 257], [257, 375]]}}}]
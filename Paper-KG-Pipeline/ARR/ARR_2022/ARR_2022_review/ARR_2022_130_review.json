[{"rid": "3331de31268882a7f2c7f5cf76382e4b1f38ad320414db4103ac099eca3e6c01c6054f81c6da2f472d8f8d4638bccadaffad27db112426078538aba688f493da", "reviewer": null, "report": {"paper_summary": "The paper proposes a new dataset for reading comprehension with multi-span answers, MultiSpanQA. \nIn addition, a new model for MultiSpanQA is introduced.\nThe **dataset** is created using a re-annotation of questions with multiple answers from Natural Questions. \nThis re-annotation process consists of two phases: - First, each instance is given one of four categories: (1) Good example, (2) Bad question, (3) Bad answer span(s), and (4) Bad QA pair.\n- Second, instances are categorized to different \"semantic structures\" (for example, whether all answer spans are needed to correctly answer a question, or each span is an answer on its own). \nAn expanded version of the dataset also includes single-span and unanswerable questions for better simulation of real-world scenarios, The **proposed model** is a sequence tagging module (BIO), augmented by three modules: - Semantic structure prediction - Span number prediction - Span adjustment module that combines the predicted number of spans with preliminary predicted spans.\nThe authors demonstrated that their proposed model improves over single-span models (generalized for multi-span QA) and over vanilla sequence tagging models. ", "summary_of_strengths": "- The problem of multi-span QA is very important and helps deviating from the limiting single-span setting - The dataset is potentially useful for the QA community - To the best of my knowledge, the 5-way semantic annotation scheme proposed by the authors is novel (and I found it elegant and important to better understand the task) - The proposed model improves over all baselines ", "summary_of_weaknesses": "1. Section 4 (Models) misses some details about the proposed model. For example, what is the exact inference procedure over $O_p$ (personally I prefer equations over textual descriptions) 2. Evaluation metrics: This subsection is difficult to read and not rigorous. ", "comments,_suggestions_and_typos": "Comments & questions: - Abstract: The sentence in lines 12-17 (\"After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version\") is cumbersome and can be made clearer.  - Do you perform re-annotation for the expanded dataset as well? The text now says \"..and applying the same preprocessing\" (line 355) - this point can be made more clear.\n- What is the inference procedure for single-span (v1)? Is the prediction a long span like in training?\nTypos: - Line 47: \"constitinga\" -> \"consisting\" - Line 216: \"classifies\" -> \"classify\" "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 97], [97, 154], [154, 262], [262, 314], [314, 451], [451, 643], [643, 779], [779, 866], [866, 898], [898, 923], [923, 1026], [1026, 1184]], "summary_of_strengths": [[0, 107], [107, 164], [164, 334], [334, 383]], "summary_of_weaknesses": [[0, 3], [3, 68], [68, 191], [191, 266]], "comments,_suggestions_and_typos": [[0, 22], [22, 356], [356, 357], [357, 422], [422, 528], [528, 584], [584, 632], [632, 639], [639, 680], [680, 719]]}}}, {"rid": "8b153255696c4dbb18f90dd93c19a0f67001c23e5f3bc938e69a28cc738cdcaf0fef16746c5d4683f778f6170cb5c0cd4bf6c1bb1559b8f2b26c0595d22e0d0c", "reviewer": "Kazutoshi Shinoda", "report": {"paper_summary": "This paper tackles question answering under a new setting, where models should extract multiple spans from textual context as an answer set. For this problem, the authors propose a new dataset, MultiSpanQA, consisting of 6.4k questions and 19k unanswerable questions. The context and questions of MultiSpanQA are extracted from Natural Questions and the answer sets are re-annotated by three trained annotators. \nThe authors cast this problem as a sequence tagging problem and show its efficacy by comparison with a simple baseline. ", "summary_of_strengths": "- This paper proposed a multiple-span answer setting for extractive question answering and a dataset, MultiSpanQA, consisting of 6.4k multi-span questions for this problem based on the re-annotation of the Natural Questions dataset.\n- The proposed joint training with span number prediction and structure prediction, and span adjustment module can improve the QA scores on MultiSpanQA compared to a pure sequence tagging baseline. ", "summary_of_weaknesses": "- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me. \n  * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting. \n  * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.\n- The number of questions with multi-span answers in the proposed dataset is small. \n  * The sizes are Train: 6465, Val: 646, Test: 646. \n  * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test. \n  * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. ", "comments,_suggestions_and_typos": "#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].\n[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.\n#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world "}, "scores": {"overall": "2.5 ", "best_paper": "No", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Kazutoshi Shinoda, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 141], [141, 268], [268, 412], [412, 533]], "summary_of_strengths": [[0, 233], [233, 431]], "summary_of_weaknesses": [[0, 106], [106, 370], [370, 584], [584, 668], [668, 721], [721, 860], [860, 980]], "comments,_suggestions_and_typos": [[0, 21], [21, 273], [273, 302], [302, 373], [373, 382], [382, 394], [394, 433], [433, 465]]}}}, {"rid": "a2b8425216b033d2765410e4f0cc2c075850a59fbd8c5394ff5a8fc7fae9bbabd75c9e5e269dbb434d4af5d4ded2ae34fcf7a5cf3dce546b75a8349a077a521c", "reviewer": "Kang Liu", "report": {"paper_summary": "This paper focused on the task of multi-span question answering. The main contribution is to propose a dataset and its expansion for the focused task. The authors also proposed some baselines and show the basic performance of this task. ", "summary_of_strengths": "Multi-span question answering is a new problem in machine reading comprehension. Constructing such a dataset is interesting and beneficial for this research area.\nThe constructed dataset is large, and the passages and questions come from the real texts. ", "summary_of_weaknesses": "I wonder whether some multi-span answer structures are really needed or not? For example, in Table 2, in Conjunction, Multi-part-disjunction (Redunant) and Shared Structure, multi answers are separated by \"and\". If we do not separate them, I think is also ok by regarding them as a single answer.  As mentioned in the abstract part, a new evaluation metric is proposed. What is it? ", "comments,_suggestions_and_typos": "1. In line 047, \"consistinga\" should be \"consisting\". \n2. In Table 5, the performance of \"DESCRIPTION\" is very low. What is that reason? "}, "scores": {"overall": "2.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Kang Liu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 65], [65, 151], [151, 237]], "summary_of_strengths": [[0, 81], [81, 163], [163, 254]], "summary_of_weaknesses": [[0, 77], [77, 212], [212, 297], [297, 298], [298, 370], [370, 382]], "comments,_suggestions_and_typos": [[0, 3], [3, 54], [54, 58], [58, 116], [116, 137]]}}}]
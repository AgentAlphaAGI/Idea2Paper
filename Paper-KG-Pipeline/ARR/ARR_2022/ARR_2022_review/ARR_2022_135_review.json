[{"rid": "3c69c73bc6af054c61a388e263b4764fcb6803cbf1ccff4783c6ae0d2305ba9f6a8a09c60caf98d77cf6debf039e1440671c5748cd270d6d84fa58e82190ba7a", "reviewer": "Giorgio Satta", "report": {"paper_summary": "This draft reports an empirical study comparing the performance of various entropy estimators on natural language data as well as on artificially generated data. ", "summary_of_strengths": "The draft investigates a problem that has not been previously considered in the NLP literature. The draft is also very valuable under a methodological perspective for entropy estimation. ", "summary_of_weaknesses": "Entropy results are provided using the unigram distribution, which is a very poor language model. ", "comments,_suggestions_and_typos": "As far as I can say, this is the first investigation in the NLP literature of various entropy estimators, which makes the draft very valuable.  My main concern is that entropy results are provided limited to the unigram distribution.  It is well-known that unigram distribution is a very poor model for sentence distributions in natural language.  There is no mention in the draft of more powerful language models, such as N-gram (N>1), combined with various smoothing and back-off techniques, why?   Figure 1: it would be nice to see some comments on those local minima in the figure occurring at N=10^5 for English, German and Dutch for both CS and NSB. Do you have any explanation for this? Same thing for HMT, between N=10^2 and N=10^3.  Typos l.166-67: This should should seem l.219: trigramma function > trigamma function Appendix A: sometimes you write p-hat, and sometimes you write p-hat_MLE l.546: truncated sentence?  Appendices C1 and C2:  I think at l.563 the reference should be to Table 3, not Table 4.  And reference to Table 4 should be placed into Appendix C2.  l.569-71: I cannot parse this sentence ", "ethical_concerns": "None "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Giorgio Satta, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 162]], "summary_of_strengths": [[0, 96], [96, 187]], "summary_of_weaknesses": [[0, 98]], "comments,_suggestions_and_typos": [[0, 143], [143, 234], [234, 347], [347, 499], [499, 501], [501, 656], [656, 694], [694, 741], [741, 742], [742, 748], [748, 782], [782, 828], [828, 901], [901, 928], [928, 929], [929, 1018], [1018, 1079], [1079, 1080], [1080, 1119]], "ethical_concerns": [[0, 5]]}}}, {"rid": "9718a739d8caa017664be63a426c74dc7d8daff635c7dfdb44370137bf06fe6e6e1668afefb0860bc026efb02485979591635a255987a98fd6dfde30e4ab8f8c", "reviewer": null, "report": {"paper_summary": "Entropy estimates are a fundamental quantity of interest for answering data-driven information-theoretic questions. However, it is known that the estimation of entropy from raw data can be quite challenging knowing that the MLE estimator in expectation underestimates entropy and the power-law governing linguistic data cannot be effectively captured by such estimates. This study is an empirical comparison of various entropy estimators on natural language data and fills an important research gap. ", "summary_of_strengths": "Strengths: 1) The paper addresses an important research gap in the computational linguistics literature. \n2) It studies a set of 6 different estimators of entropy and their empirical comparisons using both synthetic data and natural language data. \n3) The authors have also replicated recent results that used the plug-in estimator to approximate entropy to show how the results overrepresent the actual effects compared to what could have been when using a power-law sensitive entropy estimator. ", "summary_of_weaknesses": "The paper doesn't have any weaknesses. ", "comments,_suggestions_and_typos": "Some typos to correct.\nLine 045: challening -> challenging Line 310: \"See Fig 2\" could be written in parentheses, instead of making it a separate sentence. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 116], [116, 370], [370, 500]], "summary_of_strengths": [[0, 105], [105, 248], [248, 497]], "summary_of_weaknesses": [[0, 39]], "comments,_suggestions_and_typos": [[0, 23], [23, 156]]}}}, {"rid": "d72c02867ce702f7e553875ad03451e0860fa305526db7863c13003278aa32c8bbf41bec04ce9c895dd7fca3fa468cee271e5dc2413cf2eb8ad02ea322eb5f63", "reviewer": null, "report": {"paper_summary": "This paper compares three entropy estimators with the simplest plug in entropy estimator on linguistic data of CELEX corpora and synthetic data.  The authors compared the quality of estimators on unigram distribution, information study on the association between gendered inanimate nouns and their modifying adjectives, and finally with similarity between grammatical gender partitions between languages. \nAuthors conclude that other estimators are more reliable than the plugin estimator. ", "summary_of_strengths": "This paper investigates some entropy estimators whose application to linguistic data have not appeared in the literature. ", "summary_of_weaknesses": "1. References Although, the authors claim that this is the first literature, there have been abundant application of entropy estimates on linguistic data. Since Brown's Computational Linguistics paper 1983, some related papers have appeared in ACL milieu, but the main debate appeared elsewhere, especially, discussed in the journal of MDPI Entropy, some were even archived in the book forms. Those include multiple articles about the bias of plugin entropy, therefore, authors are recommended to go through them.  Authors are requested to correctly situate this work within the large history of studies on bias of plug in entropy, which is no more than one kind of work to apply entropy estimate to linguistic data.\n2. Soundness of this paper The authors claim at the bottom of p.2, about the plugin estimator,     by Jensen's inequality, this estimator is, in expectation,     a lower bound on the true entropy. \nHowever, by information theory, any entropy estimator is destined to be no more than an upper bound of the true entropy. In Appendix A, the author draws conclusion by mixing the true entropy and the estimated entropy, where plugin entropy is no more than an estimated entropy. Please see the definition of cross entropy.\n3. Experiments This paper only reports the partial and summarized output for each experiment. For example, only the \"bests\" were mentioned, but authors are recommended to show the concrete performances of each estimator.  To investigate the quality of entropy estimates, much more fundamental analysis are conducted within the previous literature, before going on to concrete application such as described in 4.2 and 4.3.  Such includes tests on other corpora than CELEX, how bias depends on the length of corpora etc. For tips, please see the previous litterature. ", "comments,_suggestions_and_typos": "p. 3 first paragraph, \"should\" replicates just below formula (7). "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 145], [145, 405], [405, 490]], "summary_of_strengths": [[0, 122]], "summary_of_weaknesses": [[0, 3], [3, 14], [14, 155], [155, 393], [393, 514], [514, 717], [717, 720], [720, 744], [744, 914], [914, 1036], [1036, 1192], [1192, 1236], [1236, 1239], [1239, 1251], [1251, 1330], [1330, 1457], [1457, 1458], [1458, 1658], [1658, 1755], [1755, 1802]], "comments,_suggestions_and_typos": [[0, 66]]}}}]
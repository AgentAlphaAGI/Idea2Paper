[{"rid": "31bf3ddda37804ff4f8b1bd9bb4c2398575c52476674569b8bdefcb418e77c6c01f7d5e3c90579ccacaf1683ddbff5e262778d373db27eff7a62abd33a72bbc0", "reviewer": null, "report": {"paper_summary": "The goal of the presented research was to improve results for NLP tasks requiring semantic understanding. Bert-like models while performing very well in many NLP tasks give much lower results when the differences in semantics, e.g. negation, are tested. Some solutions for improving handling negation were presented before, here the authors looked also into another semantics related task – synonyms and antonyms detection. They propose a new solution – intermediate training on tasks related to meaning matching.   The authors propose three tasks: masked knowledge retrieval on negated queries (MKR-NQ), masked word retrieval (MWR), and synonym/antonym recognition (SAR). In the first one, they negated the LAMA dataset  - for sentences with one verb the negation was added (or removed) and the different answers were picked.  For the MWR task, queries, like “happy is the synonym of [MASK]”, were used. The number of wrong predictions, i.e. words which are in a different relation to a  given word at the top of the list of the most probable predictions were counted.   In the last task, the relation between a  pair of words  - synonymy or antonymy was predicted. To create this set data from ConceptNet was used.  To illustrate the lack of information of negation and lexical semantics in  Bert models, all three tasks were first solved using several (BERT, RoBERTa, ALBERT, Electra in small and large versions) already trained models;  only for the SAR task the additional training was performed. In general, the results of all the models measured by the hit rate metrics were not good. It looked like the synonymy/antonymy relations cannot be retrieved.  For MKR-NQ and MWR tasks large models were generally worse than small ones. For the SAR task, fine-tuning improved the results significantly.  To encode the needed information in language models, the authors proposed the approach with the intermediate model training on the newly defined meaning matching task. The training set consisted of word-sentence pairs. The same three tasks were then solved using the resulting models and generally, there was a significant increase in the results for the large models. However,  for the Elektra model, the results were worse. \n The authors also checked if the additional training step can negatively influence the results obtained on the other tasks. The results obtained for 8 tasks from the GLUE benchmark did not change much. So the authors conclude that their approach (IM2) is of benefit to learning lexical-semantic information and the meaning of negated expressions. ", "summary_of_strengths": "The authors address the important problem of language models based on distributional hypothesis – words which occur in the same context may be of the same category but semantically significantly different, e.g. ‘cat’/ ‘dog’, ’long’/ ‘short’. It is also hard to differentiate negated and not negated sequences. They propose a method that helps in recognizing negation in a more efficient way than it was proposed in BERTNOT and at the same time helps in differentiating synonyms and antonyms.  The series of experiments are well planned and sufficiently described. ", "summary_of_weaknesses": "The performance for some tasks and some models increased significantly but this is not true in all cases. The fact was explored by the authors who hypothesised that a leading cause is catastrophic forgetting where the model forgets previous knowledge learned through pre-training to accept new information from the intermediate task and confirmed that small models were changed more.  But it does not explain all results (like Bert small better than large even after intermediate training). The authors did also not comment on the difference of behaviour of BERT and Roberta models. ", "comments,_suggestions_and_typos": "One table with all the results - before and after intermediate training would help readers to observe differences. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 106], [106, 254], [254, 424], [424, 514], [514, 673], [673, 827], [827, 905], [905, 1070], [1070, 1167], [1167, 1217], [1217, 1218], [1218, 1502], [1502, 1592], [1592, 1660], [1660, 1737], [1737, 1803], [1803, 1804], [1804, 1972], [1972, 2023], [2023, 2173], [2173, 2230], [2230, 2355], [2355, 2433], [2433, 2578]], "summary_of_strengths": [[0, 242], [242, 310], [310, 492], [492, 564]], "summary_of_weaknesses": [[0, 106], [106, 384], [384, 491], [491, 583]], "comments,_suggestions_and_typos": [[0, 115]]}}}]
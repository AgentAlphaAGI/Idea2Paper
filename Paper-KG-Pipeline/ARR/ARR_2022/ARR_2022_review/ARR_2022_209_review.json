[{"rid": "13677691ef6650a9852c1b70be41d53bf711afd4f38cb9f07d57eeecfff80470d4d81c3da67529e5f1c01088c2885a39cf5e77438cd40300ce91b380bf1f6369", "reviewer": null, "report": {"paper_summary": "The paper proposes a new approach named BEEP to combine information from clinical notes and relevant medical literature to enhance the prediction of patient outcomes (prolonged mechanical ventilation, in-hospital mortality and length of stay). The medical literature is retrieved from PubMed and then ranked per relevance. The embeddings of clinical notes and top relevant literature are then combined to predict the patient outcomes on MIMIC-III datasets. Experiments show improved accuracy on 2 out of 3 tasks. ", "summary_of_strengths": "- Overall well-written narratives with clear descriptions of methodology and experiments, though a lot of information is in the appendix which makes it a bit difficult to switch between main content and appendix.\n- The idea of retrieving medical literature to enhance and provide evidence to patient outcome prediction is attractive. The proposed methods of retrieval and reranking are reasonable.\n- Experiments are well established and clear. The proposed method BEEP shows advantages on PMV and MOR tasks. ", "summary_of_weaknesses": "- The proposed method heavily relies on BERT-based encoders and BERT has a word limit of 512 tokens. But most discharge summaries in MIMIC-III have much more than 512 tokens. This may mean a lot of information in discharge summaries is truncated and the model may not be able to build a comprehensive representation of patient condition.\n- The reliability and interoperability of the proposed method are in doubt based on Figure 3 which shows a high percentage of unhelpful literature is retrieved especially for the LOS task. How will such unhelpful literature impact patient outcomes? How can this be improved?\n- The performance on LOS is not convincing and the paper does not provide much insight on why.\n- The experiments do not seem to consider structured features at all (e.g. 17 clinical features from [1] based on MIMIC-III) which however are critical for patient outcome prediction from both clinical and ML perspectives [2]. The experiments may need a baseline that leverages structured features to show the advantage of using clinical notes and interpret BEEP's performance.\n[1] https://www.nature.com/articles/s41597-019-0103-9 [2] https://arxiv.org/abs/2107.11665 ", "comments,_suggestions_and_typos": "- In the abstract and experiment section, expressions like \"5 points\" are confusing. \" 5% increase\" or \"0.05 increase\" would be clearer.\n- In the abstract, what is \"increasing F1 by up to t points and precision @Top-K by a large margin of over 25%\" based on? The paper may make the statement clearer by mentioning the specific setup that achieves the largest improvement margin.\n- Based on Appendix B, the bi-encoder is trained on TREC 2016 with 30 EHRs. The paper may discuss how representative these 30 EHRs are to MIMIC-III EHRs. Also as 30 is rather small, the paper may discuss whether it is enough empirically.\n- Line 559, the paper may discuss why the model does not perform well on LOS, why a high percentage of unhelpful literature are retrieved (even for correct predictions) and how such a high percentage of unhelpful literature impact the reliability of the model.\n- The paper may discuss why use MeSH rather than other ontologies like UMLS, SNOMED, HPO etc.\n- Are those 8 categories in Appendix I mutually exclusive? ", "ethical_concerns": "A section of ethical consideration will be needed given the nature of this work on clinical data. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 244], [244, 323], [323, 457], [457, 513]], "summary_of_strengths": [[0, 213], [213, 334], [334, 398], [398, 444], [444, 508]], "summary_of_weaknesses": [[0, 101], [101, 175], [175, 338], [338, 527], [527, 587], [587, 613], [613, 708], [708, 935], [935, 1086], [1086, 1177]], "comments,_suggestions_and_typos": [[0, 87], [87, 137], [137, 259], [259, 379], [379, 455], [455, 533], [533, 617], [617, 878], [878, 972], [972, 1031]], "ethical_concerns": [[0, 98]]}}}, {"rid": "b986f917808d098b3c7153bbacd30f309adf77caabf55b172873ff35047c7cb05a6cf17a2e96a01a1cad1da837b32facb04fd26653e58043ce06fb74d512353e", "reviewer": null, "report": {"paper_summary": "This manuscript presents a system that combines both clinical notes and patient-specific medical literature for clinical outcome prediction. The problem is important and interesting. The idea of incorporating patient-specific medical literature is novel. The evaluation of the proposed system is convincing. ", "summary_of_strengths": "A well-written paper with a novel idea. \nThe evaluation is convincing. ", "summary_of_weaknesses": "The technique contribution is not very sufficient as most components of the system use existing methods or tools. ", "comments,_suggestions_and_typos": "As BERT is incapable of processing very long texts (more than 512 tokens) like clinical records [1], other text representation methods should be considered.\n[1] Ding, M., Zhou, C., Yang, H. and Tang, J., 2020. Cogltx: Applying bert to long texts. Advances in Neural Information Processing Systems, 33, pp.12792-12804. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 141], [141, 183], [183, 255], [255, 308]], "summary_of_strengths": [[0, 40], [40, 71]], "summary_of_weaknesses": [[0, 114]], "comments,_suggestions_and_typos": [[0, 157], [157, 210], [210, 247], [247, 318]]}}}, {"rid": "24cde1dedea013072fff85d4bf4c927680cb8964f789735ceafd74b11c97f246100d67ec2f07281c8b3473e05c68b8b452df437d66427d592fdef924e66faa3d", "reviewer": null, "report": {"paper_summary": "In this work, the authors propose a method to improve the prediction of clinical outcomes from clinical notes text by finding relevant evidence from documents in a database and augmenting the input. They mainly experiment with three prediction tasks: ventilation prediction, mortality prediction, and length of stay prediction and propose different ways to combine and fuse the evidence information from documents. Results show some improvements over simple baseline models that use only clinical text.\nThe central architecture of the paper's proposed approaches consists of a sparse-dense retriever and reranker to select relevant documents, and an aggregation strategy to encode the selected documents along with the clinical notes. While the model components are all previously explored, it is interesting to see their application in a clinical outcome prediction setting with notes.\nThe paper is clear in terms of the problem motivation and goals. However, there are some missing details from the explanation of the task setup and the results are slightly unconvincing due to there not being one clear approach (out of four averaging and voting strategies to combine documents). Furthermore, the performance scores are not as high as some other approaches in literature (Rajkomar et al.) that use structured clinical data such as ICD codes in addition to clinical notes. So the much higher additional effort to obtain evidence instead of relying on other structured data in a patient's EHR is not justified. ", "summary_of_strengths": "1. The paper is clear in terms of problem motivation and goals. \n2. The experimental setup in terms of the variety of tasks chosen and the analysis is good. \n3. I like the human evaluation of the evidence analysis. ", "summary_of_weaknesses": "1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date? \n2. There isn't one clear aggregation strategy that gives consistent performance gains across all tasks. So it is hard for someone to implement this approach in practice. ", "comments,_suggestions_and_typos": "1. Experimental setup details: Can you explain how you pick which notes from the patient's EHR do you use an input and how far away are the outcomes from the last note date? Also, how do you select the patient population for the experiments? Do you use all patients and their admissions for prediction? Is the test set temporally split or split according to different patients? \n2. Is precision more important or recall? You seem to consider precision more important in order to not raise false alarms. But isn't recall also important since you would otherwise miss out on reporting at-risk patients? \n3. You cannot refer to appendix figures in the main paper (line 497). You should either move the whole analysis to appendix or move up the figures. \n4. How do you think your approach would compare to/work in line with other inputs such as structured information? AUCROC seems pretty high in other models in literature. \n5. Consider explaining the tasks and performance metrics when you call them out in the abstract in a little more detail. It's a little confusing now since you mention mortality prediction and say precision@topK, which isn't a regular binary classification metric. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 199], [199, 415], [415, 503], [503, 735], [735, 887], [887, 952], [952, 1183], [1183, 1375], [1375, 1512]], "summary_of_strengths": [[0, 3], [3, 64], [64, 68], [68, 157], [157, 161], [161, 215]], "summary_of_weaknesses": [[0, 3], [3, 44], [44, 215], [215, 219], [219, 320], [320, 386]], "comments,_suggestions_and_typos": [[0, 3], [3, 174], [174, 242], [242, 303], [303, 378], [378, 382], [382, 421], [421, 503], [503, 601], [601, 605], [605, 672], [672, 750], [750, 754], [754, 865], [865, 921], [921, 925], [925, 1043], [1043, 1186]]}}}, {"rid": "696b485553604279f62f15fcd651910e1d2e7cff91c06112861936b446cc262d34329f49aaca63dc43347c04038603b687967d1ef0536226e754b0bed5ed85b2", "reviewer": null, "report": {"paper_summary": "An interesting and thorough paper combining clinical text with relevant-to-patient medical literature to improve patient outcome predictions. An excellent and meaningful application of language models to a very important task, attempting to bridge the gap between corpus-based clinical models and Evidence Based Medicine. The work involves training models for Pubmed abstract retrieval based on the patient's admission note (the query); combining the text of the clinical note and the retrieved and re-ranked abstracts for patient outcome predictions, namely predicting prolonged mechanical ventilation, mortality, and length of stay. ", "summary_of_strengths": "A well written and thorough paper. \nExtensive experiments. \nPromising results. \nReproducible work: the code, MIMIC III cohort selection, paper identifiers, and models are publicly available at the time of submission (in an anonymous manner). ", "summary_of_weaknesses": "The novelty of the approach is limited. \nThe attempt to train the document retrieval and outcome prediction jointly is unsuccessful. ", "comments,_suggestions_and_typos": "The related work section needs to be expanded as it omits important prior work on both Patient-Specific Literature Retrieval and Clinical Outcome Prediction.\nYour attempt to train the retrieval and outcome prediction jointly seems unsuccessful, have you considered experimenting with different approaches, such as  Retrieval-Augmented Language Model pre-training and fine-tuning?\nThere are too many references to the appendix in the paper. For example, it will be helpful to include more detail in section \"Learning To Retrieve Using Outcomes\", as opposed to referring to the appendix.  It will be helpful to include ethical concerns, detailing the limitations of the approach regarding practical, clinical application. ", "ethical_concerns": "As suggested to the authors, it will be helpful to include a comment detailing the limitations of the approach regarding practical, clinical application. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 142], [142, 322], [322, 635]], "summary_of_strengths": [[0, 35], [35, 59], [59, 79], [79, 242]], "summary_of_weaknesses": [[0, 40], [40, 133]], "comments,_suggestions_and_typos": [[0, 158], [158, 380], [380, 440], [440, 586], [586, 587], [587, 720]], "ethical_concerns": [[0, 154]]}}}]
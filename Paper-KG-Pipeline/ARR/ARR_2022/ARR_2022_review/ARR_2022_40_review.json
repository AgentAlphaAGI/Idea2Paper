[{"rid": "54234d38c62b4cc488d849bd76221bc6be72b86c2e4a2097099f2aa66b9fdf046c7a5a76e32416cc19a86125d08f89f13e23f75b74af6e96e5cebe933fb3eac5", "reviewer": null, "report": {"paper_summary": "This paper describes a self-generation framework for training ODD and TOD systems, as well as the prediction of the transition turns from ODD to TOD turns. The proposed framework makes use of self-chatting approaches using a SotA system, as well as fine-tuned versions of the chatbot to play the role of a salesman and a user. In addition, an interesting approach for predicting the best turn where to insert the transition and use of zero-shot approaches for intent detection complete the work done by the authors. Finally, human evaluations show the feasibility of the proposed approach. ", "summary_of_strengths": "- The generated dataset is interesting for people working on dialogue evaluation - The proposed framework for self-generating the dataset is very interesting and the results are feasible - The proposed method for intent detection and transition turns are also correct and interesting - The paper is well written and clear to follow. Experimentation and human evaluation is correct ", "summary_of_weaknesses": "- Although author state that components can be replaced by other models for flexibility, authors did not try any change or alternative in the paper to proof the robustness of the proposed framework.\n- Did authors tried using BlenderBot vs 2.0 with incorporated knowledge? it would be very interesting to see how the dialogs can be improved by using domain ontologies from the SGD dataset.  - Although BlenderBot is finetuned on the SGD dataset, it is not clear how using more specific TOD chatbots can provide better results - Lines 159-162: Authors should provide more information about the type/number of personas created, and how the personas are used by the chatbot to generate the given responses.  - It is not clear if authors also experimented with the usage of domain ontologies to avoid the generation of placeholders in the evaluated responses  - Line 211: How many questions were created for this zero-shot intent classifier and what is the accuracy of this system?\n- Line 216: How many paraphrases were created for each question, and what was their quality rate?\n- Line 237: How critical was the finetuning process over the SQuad and CommonsenseQA models?\n- Line 254-257: How many templates were manually created?  - Line 265: How the future utterances are used during evaluation? For the generation part, are the authors generating some sort of sentence embedding representation (similar to SkipThoughs) to learn the generation of the transition sentence? and is it the transition sentence one taken from the list of manual templates? ( In general, this section 2.2.2 is the one I have found less clear)  - Merge SGD: Did authors select the TOD dialogue randomly from those containing the same intent/topic? did you tried some dialogue embedding from the ODD part and tried to select a TOD dialogue with a similar dialogue embedding? if not, this could be an idea to improve the quality of the dataset. this could also allow the usage of the lexicalized version of the SGD and avoids the generation of placeholders in the responses - Line 324: how the repeated dialogues are detected?  - Line 356: how and how many sentences are finally selected from the 120 generated sentences?\n- Lines 402-404: How the additional transitions are generated? using the T5 model? how many times the manual sentences were selected vs the paraphrased ones? ", "comments,_suggestions_and_typos": "- The paper: Fusing task-oriented and open-domain dialogues in conversational agents is not included in the background section and it is important in the context of similar datasets - Probably the word salesman is misleading since by reading some of the generated dialogues in the appendixes, it is not clear that the salesman agent is in fact selling something. It seems sometimes that they are still doing chitchat but on a particular topic or asking for some action to be done (like one to be done by an intelligent speaker) "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 156], [156, 327], [327, 516], [516, 590]], "summary_of_strengths": [[0, 81], [81, 187], [187, 284], [284, 333], [333, 381]], "summary_of_weaknesses": [[0, 199], [199, 272], [272, 389], [389, 390], [390, 525], [525, 703], [703, 704], [704, 855], [855, 977], [977, 1075], [1075, 1168], [1168, 1226], [1226, 1227], [1227, 1293], [1293, 1469], [1469, 1550], [1550, 1618], [1618, 1721], [1721, 1847], [1847, 1916], [1916, 2045], [2045, 2098], [2098, 2099], [2099, 2193], [2193, 2256], [2256, 2276], [2276, 2351]], "comments,_suggestions_and_typos": [[0, 182], [182, 363], [363, 528]]}}}, {"rid": "17c7cb1d81372d0e1dc6209b9c16c87efa581e17d29a5b90292ccf2ba56a9718ca1738f7a4a74a7c105d8726eb648c915ad65193c867806f40d6240d1f392ded", "reviewer": "Chinnadhurai Sankar", "report": {"paper_summary": "The paper proposes a novel and automatic way of transitioning from chit-chat to task-oriented (TOD) mode. They leverate BlenderBot to automatically generate the dialogs with transitions and evaluare their approach using human evaluation.  They automate the process using a series of ML modeling - 1) QA based intent detection for automatically transitioning from chit-chat to task-oriented mode 2) T5 model to generate the transition turn 3) BlenderBot model to simulate both chit-chat turns and TOD turns. ", "summary_of_strengths": "- The proposed dataset could be useful for the broader research dialog research community since there is a dearth of datasets blending chit-chat and task-oriented dialogs.  - The paper is easy to follow and well-written.\n- The method seems general enough to be applicable to other task-oriented datasets with intent annotations. ", "summary_of_weaknesses": "- More details on how exactly the topic related chit-chat turns would have strengthened the paper. What are prompts provided to the blender bot and the impact of different prompts on the quality of generated data?\n- Also, Blenderbot details for TOD simulation can be expanded in section 2.3. For instance, what is the impact of using mergeSGD vs TOD simulation on the overall quality ?\n- The paper seems to lack details on performance of the intent detector model and QA models and their impact on the quality of the dialogs generated. It would be nice to have an ablation study on the quality of dialogs using different intent detectors (including the data used to train). ", "comments,_suggestions_and_typos": "During the transition turn, did the process also check if the user is requesting for more information or a question before switching to TOD setting ? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Chinnadhurai Sankar, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 106], [106, 238], [238, 507]], "summary_of_strengths": [[0, 172], [172, 173], [173, 221], [221, 329]], "summary_of_weaknesses": [[0, 99], [99, 214], [214, 292], [292, 386], [386, 536], [536, 674]], "comments,_suggestions_and_typos": [[0, 150]]}}}, {"rid": "96ec9be29428e0a17ca105d58c549b165b515306b77c29af12c34660fabf6908fbf2415ceac54922e153db5cf90e391ae19dc0df99d9a41b946bd2a6553afcb4", "reviewer": "Jun Xu", "report": {"paper_summary": "This paper focuses on transitioning from chit to task-oriented dialogues. This is important for triggering business opportunities.\nSpecifically, the authors propose a framework to automatically generate dialogues, which start from open-domain social chatting and then gradually transit to task-oriented dialogs.\nThe human evaluation shows that the automatically generated dialogues have a reasonable quality with natural conversation ﬂows from a business point of view. ", "summary_of_strengths": "1. Combining Chit-Chat and Task-Oriented Dialogues is a less studied direction. \n2. Generating dialogs automatically can be useful in the industry. ", "summary_of_weaknesses": "1. From the perspective of research, since the released dataset is automatically generated without further manual revision or annotation, it is hard to say that this work proposes a new research task. Furthermore, the two difficult problems (implicit intent detection and transition utterance generation) deserve a more closer study. \n2. From the perspective of models, the proposed method consists of a list of sub-models. The whole process is too trivial. ", "comments,_suggestions_and_typos": "The authors target an important problem that tries to combine the open-domain dialog and task-oriented dialog. Specifically, they provide a  clear objective for this combination, i.e. triggering the business opportunities. There are at least three basic sub-problems. \n(1) How to capture the interaction between the two types of dialogs? \n(2) How to determine the turning point? \n(3) How to transit properly?\nAll of these sub-problems need to be treated more systematically and carefully.  Other suggestions. \n(1) The introduction is a little wordy and sometimes confusing. For example, in line 78, \"the conversation starts without any speciﬁc goal\". This is confusing, since if a user has no specific goal, why does he or she chat with a salesperson? \n(2) This paper is highly related to dialog recommendation. In line 505-512, the authors claim \"such systems is to only make entity recommendations instead of tasks...\". From my point of view, there is considerable overlap between \"make entity recommendations\" and \"transferring from chit-chat to task-oriented dialogues and completing a task the user may want\". Specifically, in Liu 2020, they have also combined chitchat and task-oriented dialog, and achieved chit to task-oriented transition. Then, the only difference lies is complete the task, which is not the focus of this paper. "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Jun Xu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 74], [74, 131], [131, 312], [312, 470]], "summary_of_strengths": [[0, 3], [3, 80], [80, 84], [84, 148]], "summary_of_weaknesses": [[0, 3], [3, 201], [201, 334], [334, 338], [338, 424], [424, 458]], "comments,_suggestions_and_typos": [[0, 111], [111, 223], [223, 268], [268, 338], [338, 379], [379, 409], [409, 489], [489, 490], [490, 509], [509, 574], [574, 651], [651, 752], [752, 812], [812, 922], [922, 1115], [1115, 1248], [1248, 1339]]}}}]
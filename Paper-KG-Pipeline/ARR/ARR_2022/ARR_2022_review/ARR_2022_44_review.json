[{"rid": "1f3ed846cf117ae2eecd5e3d9d19a85c37510c11c3463afa9ded734ea9f7dc0569a0ad64808fed511202bc3a62103801e85b7e7d14a8617220cf2ec989190cb1", "reviewer": "Fangyu Liu", "report": {"paper_summary": "The paper presents AcTune, an active learning framework that combines self training on high-confident samples and data annotation on low-confident samples. The paper also proposes two new methods: (1) region-based sampling and (2) momentum-based memory bank, to improve the sampling strategy in active learning and to reduce label noise in self-training. The paper provides extensive experiments to show the advantage of the proposed method (both performance and label efficiency) and and offered ablation studies to analyse inner-workings of this method. The paper is overall a solid contribution for combining active learning and self-training in NLP and I recommend acceptance. ", "summary_of_strengths": "The paper proposes a novel and effective framework to combine active learning + self training in NLP. The additional two strategies designed in the paper (region-based sampling and momentum-based memory bank) are well-motivated. The experiments are thorough with convincing ablation studies. ", "summary_of_weaknesses": "No major concerns. ", "comments,_suggestions_and_typos": "n/a "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Fangyu Liu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 156], [156, 355], [355, 556], [556, 681]], "summary_of_strengths": [[0, 102], [102, 229], [229, 292]], "summary_of_weaknesses": [[0, 19]], "comments,_suggestions_and_typos": [[0, 4]]}}}, {"rid": "f587ce83feaea1c1d2ac6b1f4d7d7b7daf42c0e37cdfbb6118d77f213e3fa57a14114915315c888dda774e81cc8cb43f0fb4f1e3fe9326aad776cbbec3dce449", "reviewer": "Yige Xu", "report": {"paper_summary": "This paper proposed ACTUNE, which first requires data annotation based on uncertainty and then uses active self-training to fine-tune a pre-trained language model. The setting of this work is limited labeled data and much more unlabelled data.\nContributions: 1. Design a framework that: (1) explore the high-certainty samples for data annotation; (2) generate pseudo-labels for certain samples. This framework can improve the label efficiency (requires less data annotation to achieve a similar performance) by sampling samples on both uncertainty and diversity. \n2. Explore the SSAL approaches for NLP, which have not been well-studied. \n3. Conduct exhaustive experiments to demonstrate the improvement of ACTUNE. ", "summary_of_strengths": "1. The topic of achieving comparable performance with limited sources is interesting. Different to prompt with few-shot samples, this framework selects the most high-uncertainty data while keeping diversity can be better utilize the information from unlabeled data. \n2. This framework provided a solution for selecting the most informative training samples. \n3. This paper provided details on datasets, hyperparameters, and implementations and is easily followed. ", "summary_of_weaknesses": "1. In my view, ACTUNE tries to search a small subset of the vanilla dataset with high coverage. It seems to explore a sub-optimal within \"small price\", which may have limitations that overdraft the potential with more data. \n2. The comparison between ACTUNE and baseline models seems not fair. According to Section 2.1, the number of labeled samples is 100 (initial labeled data) + 1000 (labeling budget b) = 1100. But the baseline models only have 1000 labeled data. \n3. The presentation of experimental results can be improved:  (1) The figures of experiment results are not well presented. To clearly recognize the details of Fig. 1, Fig. 3, Fig. 4, and Fig. 5, they need to be zoomed in to 300%.  (2) Insufficient color contrast in Fig. 3(c) makes distinguishing the last two lines difficult. ", "comments,_suggestions_and_typos": "Comments: 1. The definition of diversity is unclear. Eq. (5) introduces the inter-class diversity, but In Eq. (8), diversity means sampling from different clusters. \n2. The usage of the momentum-based memory bank is confusing. For example, as shown in Line 268, if $f(x; \\theta^t)=[0.05,0.95]$, and $m_t$ in Eq. (10) is larger than $m_L$ (suppose to be 0.8), then the score of class 1 in $g(x;\\theta^t)$ is larger than 0.8 times 0.95, which is larger than 0.5. In this case, how can predictions in the memory bank affect the current prediction? \n3. I think it would be better if the authors could provide more descriptions in Figure 6. For example, Line 549 says, \"Figure 6(e) and 6(f) show the results\". What can the reader learn if he/she is not familiar with this visualization? The same problem also appears in Line 1176.\nSome typos do not affect understanding: 1. Line 122: we -> We 2. Line 188: result -> results 3. Line 475: Fig. 5(b) and Fig. 1(f) -> Fig. 1(b) and Fig. 1(f) "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yige Xu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 164], [164, 244], [244, 262], [262, 395], [395, 563], [563, 567], [567, 638], [638, 642], [642, 715]], "summary_of_strengths": [[0, 3], [3, 86], [86, 266], [266, 270], [270, 358], [358, 362], [362, 464]], "summary_of_weaknesses": [[0, 3], [3, 96], [96, 224], [224, 228], [228, 294], [294, 415], [415, 468], [468, 472], [472, 531], [531, 593], [593, 700], [700, 701], [701, 797]], "comments,_suggestions_and_typos": [[0, 13], [13, 53], [53, 165], [165, 169], [169, 227], [227, 461], [461, 545], [545, 549], [549, 636], [636, 705], [705, 782], [782, 826], [826, 869], [869, 891], [891, 922], [922, 983]]}}}]
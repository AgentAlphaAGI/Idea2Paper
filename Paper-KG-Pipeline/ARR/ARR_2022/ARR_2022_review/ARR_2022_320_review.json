[{"rid": "31909b71e83378dba8a454ced702369ea4fa8ff0f365f92b534de6f10bfdda989d993d9308750d5fcb88ab6f18c26725b936bb5e69c7bc797b13e99e59b03841", "reviewer": null, "report": {"paper_summary": "This work focuses on span-based Nested NER task. For this task, this paper has first identified a few heterogeneous factors that influence the detection of nested named entities. \nThe factors are tokens, boundaries, labels and related spans. Then it proposes a span-based method with a novel triaffine attention and scoring mechanism to fuse the identified heterogeneous factors for span representation and classification.\nThe proposed method is evaluated on four nested NER datasets: ACE2004, ACE2005, GENIA, and KBP2017. Extensive comparative experiments were performed to show that the proposed method outperforms existing methods. Furthermore, ablation analysis was performed to show the importance of all the components. Results on the long and nested entities indicate the importance of use of triaffine transformations compared to biaffine transformation. ", "summary_of_strengths": "1. A very relevant and challenging task. It has an immense downstream impact in domains like BioNER. \n2. Novel triaffine attention and scoring mechanism exploiting heterogeneous factors. Identification of the heterogeneous factors and utilizing them appropriately. \n3. Well-designed experiments. Comparison with existing methods is quite extensive, i.e., several existing methods were considered for comparison. Ablation analysis and case studies were also performed. \n4. Comprehensive study and well written ", "summary_of_weaknesses": "1. A minor weakness of the paper is a convoluted affine mechanism which may not be easy to interpret.\n2. Some more clarity is required in the method description. Please see the Comments section. ", "comments,_suggestions_and_typos": "1. Line no 84:- \"...may be...\" -> may have been 2. Line no 218:- are the given references for the text encoding the first ones to use such encoding? If not, then please give the appropriate reference. \n3. Line no 203:- Define MLP to avoid any confusion 4. Line no 761:- Rephrase \"We specific seeds\" 5. Another minor issue is it is not clear how the different spans and boundaries are detected "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 49], [49, 179], [179, 242], [242, 423], [423, 523], [523, 635], [635, 726], [726, 863]], "summary_of_strengths": [[0, 3], [3, 41], [41, 101], [101, 105], [105, 187], [187, 265], [265, 269], [269, 296], [296, 412], [412, 468], [468, 472], [472, 509]], "summary_of_weaknesses": [[0, 3], [3, 102], [102, 105], [105, 162], [162, 195]], "comments,_suggestions_and_typos": [[0, 3], [3, 51], [51, 149], [149, 201], [201, 205], [205, 256], [256, 302], [302, 393]]}}}, {"rid": "7b9204699434ff8312efc0b85be09bc0e8b68ed3f9561594785ea7d884b0ea8646efc7d8e1a5e3cd17f8cb3226b6f2e3eadf5a2110a20637e7e000dc6cd83eda", "reviewer": "Songlin Yang", "report": {"paper_summary": "This work improves the span-based methods of nested NER.  The authors propose to use the triaffine scoring function to fuse heterogeneous factors, i.e. tokens, boundaries, labels, other spans, to obtain span representations and label scores.  Experiments and ablation studies confirm the effectiveness. ", "summary_of_strengths": "- The paper is well-written and easy to follow.\n- Ablation studies and analyses are thorough. ", "summary_of_weaknesses": "- The authors claimed SOTA in L114-115, however, as acknowledged by authors in L388-389, the proposed method underperforms several SOTA when both using BERT as the encoder.  Other approaches may obtain higher results when using ALBERT,  so the comparison is not fair and I think the claim is somewhat exaggerated.\n-  I do not think Eq. (11) and Eq. (13) are equivalent. Note that in Eq(1-3), every input of the Triaffine function would be fed into an MLP layer firstly, which is NONLINEAR.  Hence the coefficients \\beta_{i, j, g, r} cannot be extracted to the front (in Eq. 13).  The authors said that Eq. (11) can be decomposed to Eq. (12-13) in L277-278, which is WRONG, and so do Eq.(23-28) in Appendix B.  As a consequence, some ablation studies may not faithfully reflect the advantage of the Triaffine function as the decomposition does not hold mathematically - Triaffine mechanisms are widely used. Extending them to heterogeneous factors is somewhat trivial.  Hence technical contributions and novelties are limited.  First, I do not think boundary tokens are heterogeneous to inside tokens. Using an attention mechanism to compute the weight of each token within the span and using their weighted sum to represent the span, known as attention-pooling [1], has been explored before. [ 2] systematically compare different span representations in different tasks. They find that attention-pooling span representation performs the best in NER ( better than end-point representation, i.e. the baseline model of this work), but has a lower performance in many other tasks. So the result of this work is not surprising, since the authors also use attention-pooling to tackle (nested) NER, except that they replace the naive attention with the triaffine attention mechanism. After obtaining the span representation, it is often to concatenate the boundary token representations to the span representation and feed them to an MLP classifier for labeling. The authors use another triaffine instead. The really exciting part of this work is to use triaffine to fuse information from other related spans. To overcome the large computational complexity, the authors propose to (1) prune span, which is often used. ( 2) use decomposition, but the derivation is wrong as I discussed before. Finally, as other reviewers mentioned, this paper does not raise novel issues.\n[1] What do you learn from context? Probing for sentence structure in contextualized word representations. In ICLR 2019.\n[2] A Cross-Task Analysis of Text Span Representations. ", "comments,_suggestions_and_typos": "Obtaining powerful span representations is crucial to many span-relation tasks [1, 2]. The authors could evaluate the proposed Triaffine mechanism on these tasks to make it more influential [1] Jiang, et al \"Generalizing Natural Language Analysis through Span-relation Representations\" In ACL2020. \n[2] Toshniwal, et al \"A Cross-Task Analysis of Text Span Representations\" In RepL4NLP 2020 ", "ethical_concerns": "None "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Songlin Yang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 57], [57, 242], [242, 303]], "summary_of_strengths": [[0, 48], [48, 94]], "summary_of_weaknesses": [[0, 173], [173, 314], [314, 370], [370, 490], [490, 579], [579, 867], [867, 907], [907, 968], [968, 1026], [1026, 1101], [1101, 1294], [1294, 1371], [1371, 1577], [1577, 1777], [1777, 1956], [1956, 1999], [1999, 2103], [2103, 2213], [2213, 2286], [2286, 2365], [2365, 2401], [2401, 2472], [2472, 2486], [2486, 2542]], "comments,_suggestions_and_typos": [[0, 87], [87, 190], [190, 298], [298, 390]], "ethical_concerns": [[0, 5]]}}}]
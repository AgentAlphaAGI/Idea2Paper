[{"rid": "8b6c32570a76bc3f912c81ca56238483b90d063a4358c30dd7ad577ec4d175db6ca56f62d02e938949fbb3e9f837a2feff5a1e97203beeef39e212d3bea306b1", "reviewer": "Yixin Nie", "report": {"paper_summary": "This paper introduced LimitedInk, a self-explaining model that outperforms the existing baseline on both end-task predictions and human-annotated rationale agreement. More importantly, the paper shows that unlike the finding from the previous studies, the shortest rationales can not present the best explanation for human understanding which call for more careful design on evaluating human rationales. ", "summary_of_strengths": "The paper conducts a human study on rationales. The finding contradicts the typical intuition about rationales that the shortest rationales are the best to explain human understanding. This would encourage a fundamental rethinking and revolution of rationale research. ", "summary_of_weaknesses": "I guess that is due to content limited. It would be better to see some qualitative analysis on why 10-30% rationales can already give decent performance whereas human needs at least 40% rationales. It seems to indicates that the downstream model is making predictions in a way that is very different from how human make predictions. ", "comments,_suggestions_and_typos": "I would suggest adding some qualitative analysis to explain the title of the paper in later version. "}, "scores": {"overall": "4.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yixin Nie, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 167], [167, 404]], "summary_of_strengths": [[0, 48], [48, 185], [185, 269]], "summary_of_weaknesses": [[0, 40], [40, 198], [198, 333]], "comments,_suggestions_and_typos": [[0, 101]]}}}, {"rid": "2b254596d32b539cb9c31d2ccba540b29222f2e14c8f01f1a51bbed21344f5bbdf6a5d20e2168e799b9656c14fdba9cfe8479103ad8a4e1035833e903a04d5af", "reviewer": null, "report": {"paper_summary": "Existing self-explaining models mostly generate the short rationales with the assumption that short rationales are more intuitive to humans, while this work discusses the question that whether the shortest rationale is the most understandable for humans. In this work, the authors design a self-explaining model, LIMITEDINK, that can take controls on rationale length by incorporating contextual information and supporting flexibly extracting rationales at any target length. By generating rationales at different length levels, the authors study how much rationale would be sufficient for humans to confidently make predictions. Experiments on various tasks demonstrate that the proposed method outperforms most prior works, and meanwhile show that the shortest rationales are not the best for human understanding. ", "summary_of_strengths": "1. The method proposed in this work is effective and can outperform several strong baselines on the performance of both label predictions and rationale predictions.\n2. The problem, the effect of the rationales at different length levels, discussed in this work is meaningful and the conclusions may serve as good guidance for further research in this field. ", "summary_of_weaknesses": "1. Although this work points out that shortest rationales are largely not the best for human understanding, the appropriate lengths are still subject to the datasets or even the instances. The length of meaningful rationales may largely depend on the density of the information related to the task. As pointed in Section 5, a more rigorous evaluation is needed to better understand what is a good rationale explanation.\n2. This work does not report how \"short\" the rationales generated by prior works are. As shown in Section 1, recent works agree that good rationales should be \"shortest yet sufficient\", while this work seems to simply focus more on \"shortest\". This brings out the concern that whether the main question discussed in this work can really stand for the trend of current works on this task. \n         (a). I think one potential solution to handle this concern is that - by extending or shortening the golden rationales and see whether such perturbations outperform or underperform the original one. ", "comments,_suggestions_and_typos": "1. I would like to see some examples of the generated rationales at different length levels from the proposed methods, as well as the rationales generated by the baselines. Such examples can help the readers to better understand the influence of rationale lengths. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 255], [255, 476], [476, 630], [630, 816]], "summary_of_strengths": [[0, 3], [3, 165], [165, 168], [168, 358]], "summary_of_weaknesses": [[0, 3], [3, 189], [189, 299], [299, 420], [420, 423], [423, 506], [506, 664], [664, 808], [808, 814], [814, 823], [823, 1016]], "comments,_suggestions_and_typos": [[0, 3], [3, 173], [173, 265]]}}}]
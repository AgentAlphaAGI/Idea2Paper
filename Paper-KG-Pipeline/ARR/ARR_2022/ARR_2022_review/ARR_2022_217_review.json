[{"rid": "e6004c2ee71daf9051207f1b01dcda22334c7706d3d68dff4e49cf02cb41a2c7fb649dffd370c0ead75e629097421399bf1e9cbe077fddbe20fd61566985668a", "reviewer": null, "report": {"paper_summary": "XDBERT is a cross-model model for NLU that is distilled from CLIP-T, the vision-grounded text encoder of CLIP. The resulted model outperforms its teachers on GLUE and other tasks. ", "summary_of_strengths": "- The idea behind the paper is intuitive and straightforward.\n- The results are a little surprising to me and they look good.\n- Experimental details are provided in the appendix, which is good for reproducibility.\n- The authors only use Wiki103 as the distillation corpus, which is easy to reproduce and desirable for computational budgets. ", "summary_of_weaknesses": "### Original comments from previous review - The main concern is the technical novelty of this paper. There is little novelty since the distillation technique is not new while the idea of cross-model training is not new, either.\n- I'm surprised that in Table 1 CLIP-T performs so badly even when finetuned. Can you explain why?\n- Is it possible that the improvement of the distilled cross-modal model comes from ensembling instead of visual grounding?\n- I suggest the authors make the weights publically available (can be anonymous during double-blind reviewing) so the results can be easily verified by the community.\nAlthough some details are vague and need further investigation, I think the contribution is enough to be accepted as a short paper.\n### After rebuttal Thanks for resubmitting the paper with a response. I've carefully read the response.\n- **Re. To Reviewer2.1**: First, distilling B task into A task improves A is not so surprising. This idea has been explored in Intermediate-Task Transfer Learning. I agree that distilling cross-modal model to a single-modal model is somewhat novel. However, even self-distillation can improve accuracy so I have no idea to what extent is cross-modal distillation helpful.\n- **Re. To Reviewer2.2**: Good answer.\n- **Re. To Reviewer2.3**: Sorry for the confusion. Yes, when fine-tuning there is only the BERT encoder used. However, I suspect that substituting CLIP with any pretrained LM would also work. But again, as the authors point out, \"the linguistic competence is CLIP-T very low\" so this paper has some insights and presents previously unknown results. That's why I gave this paper a score of 3.5.\n- **Re. To Reviewer2.4**: That should be easy. Just manually reassign the weights in the original `state_dict` to a `state_dict` of a Hugging Face BERT model. That would work.\nThus, I keep my recommendation of weak acceptance for this paper and I suggest other reviewers consider increasing their scores. ", "comments,_suggestions_and_typos": "N/A "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 111], [111, 180]], "summary_of_strengths": [[0, 62], [62, 126], [126, 214], [214, 341]], "summary_of_weaknesses": [[0, 43], [43, 102], [102, 229], [229, 307], [307, 328], [328, 452], [452, 619], [619, 751], [751, 821], [821, 855], [855, 863], [863, 951], [951, 1019], [1019, 1104], [1104, 1227], [1227, 1235], [1235, 1266], [1266, 1274], [1274, 1317], [1317, 1376], [1376, 1458], [1458, 1615], [1615, 1660], [1660, 1668], [1668, 1707], [1707, 1819], [1819, 1836], [1836, 1965]], "comments,_suggestions_and_typos": [[0, 4]]}}}]
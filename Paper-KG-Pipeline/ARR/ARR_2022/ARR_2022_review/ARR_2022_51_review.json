[{"rid": "cfd85051633de133ab07f6eb88215422cd4fea1e970f47a60175560c73a5df19e2f26529237ad287500e8a0d96716784d4bd7fbde0962b81ab82806c6ec720b4", "reviewer": null, "report": {"paper_summary": "This paper proposes improving many-to-many NMT using a word-level contrastive loss. The paper also shows that the proposed approach’s translation quality correlates with how well sentences can be retrieved using the encoder’s output. ", "summary_of_strengths": "The experiment covers many different setups, with different number of language pairs, a wide variety of languages, and more than 1 neural architecture. The ablation is helpful for explaining where the improvement in NMT performance comes from. ", "summary_of_weaknesses": "1.  The choice of the word-alignment baseline seems odd. The abstract claims that “Word alignment has proven to benefit many-to-many neural machine translation (NMT).” which is supported by (Lin et al., 2020). However, the method proposed by Lin et al was used as baseline. Instead, the paper compared to an older baseline proposed by (Garg et al., 2019). Besides, this baseline by Garg et al (+align) seems to contradict the claim in the abstract since it always performs worse than the baseline without word-alignment (Table 2). If for some practical reason, the baseline of (Lin et al., 2020) can’t be used, it needs to be explained clearly.\n2. In Table 2, the proposed approaches only outperform the baselines in 1 setup (out of 3). In addition, there is no consistent trend in the result (i.e. it’s unclear which proposed method (+w2w) or (+FA) is better). Thus, the results presented are insufficient to prove the benefits of the proposed methods. To better justify the claims in this paper, additional experiments or more in-depth analysis seem necessary.\n3. If the claim that better word-alignment improves many-to-many translation is true, why does the proposed method have no impact on the MLSC setup (Table 3)? Section 4 touches on this point but provides no explanation. ", "comments,_suggestions_and_typos": "1. Please provide more details for the sentence retrieval setup (how sentences are retrieved, from what corpus, is it the same/different to the setup in (Artetxe and Schwenk, 2019) ? ).\nFrom the paper, “We found that for en-kk, numbers of extracted word pairs per sentence by word2word and FastAlign are 1.0 and 2.2, respectively. In contrast, the numbers are 4.2 and 20.7 for improved language pairs”. Is this because word2word and FastAlign fail for some language pairs or is this because there are few alignments between these language pairs? Would a better aligner improve result further?\n2. For Table 3, are the non-highlighted cells not significant or not significantly better? If it’s the latter, please also highlight cells where the proposed approaches are significantly worse. For example, from Kk to En, +FA is significantly better than mBART (14.4 vs 14.1, difference of 0.3) and thus the cell is highlighted. However, from En to Kk, the difference between +FA and mBART is -0.5 (1.3 vs 1.8) but this cell is not highlighted. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 84], [84, 234]], "summary_of_strengths": [[0, 152], [152, 244]], "summary_of_weaknesses": [[0, 3], [3, 57], [57, 168], [168, 210], [210, 274], [274, 356], [356, 531], [531, 645], [645, 648], [648, 737], [737, 862], [862, 954], [954, 1063], [1063, 1066], [1066, 1222], [1222, 1283]], "comments,_suggestions_and_typos": [[0, 3], [3, 186], [186, 331], [331, 403], [403, 546], [546, 593], [593, 596], [596, 684], [684, 787], [787, 922], [922, 1038]]}}}]
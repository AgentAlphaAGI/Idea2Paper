[{"rid": "dd056f7a2dceb082794add58f5c9ac90bf0c4b548fd748bf04ae661653318b8f6a71581632bf40b980fd52fec28ca3e3bfc5ebf480c021391a689bcce40c5f53", "reviewer": "Nikolay Bogoychev", "report": {"paper_summary": "The paper describes an early exiting strategy based on hashing tokens to particular layers. It explores a number of possible hash strategies, and compares with previous work. They justify their work by performing experiments on inferring instance difficulty. ", "summary_of_strengths": "Well described hashing methods and intuition. Well described experimental setup and publishing a dataset.\nVery good speed evaluation, taking batching and real time into account, as FLOPs do not give the complete picture. ", "summary_of_weaknesses": "The paper is difficult to understand for those not familiar with early exiting in deep learning. A better introduction would be helpful.\nAlso the paper looks like 2 short papers stapled together into one. The learning difficulty section and the hash section could easily be two separate papers. ", "comments,_suggestions_and_typos": "The most clear description of early exiting appears in line 338-345. It would be very helpful to the reader for those to appear much ealier in the paper. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Nikolay Bogoychev, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 92], [92, 175], [175, 259]], "summary_of_strengths": [[0, 46], [46, 106], [106, 221]], "summary_of_weaknesses": [[0, 97], [97, 137], [137, 205], [205, 295]], "comments,_suggestions_and_typos": [[0, 69], [69, 154]]}}}, {"rid": "4773827da540051debe65ab52a9b26ee346a3f259683316ae6f62b7d6d506a5947312df950b2459a8f7a107171c9ca75378b393537153fa31bf23c65e23e5035", "reviewer": "Shuhuai Ren", "report": {"paper_summary": "This paper proposes HASHEE, a very simple Hash-based Early Exiting approach. Traditional early exiting methods focus on predicting instance difficulty by heuristic metrics or learnable modules so that easier instances can be assigned to lower layers to exit while more difficult instances can go through more layers. However, the preliminary experiments in this paper show that both human-defined and model-defined instance difficulty is hard to learn. The more important issue in early exiting is to keep the consistency between training and inference, i.e., let similar training instances and inference instances exit from the same layer. To this end, the authors propose to replace the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. This method requires no internal classifiers and can perform token-level early exiting without supervision. The method achieves higher performance with fewer FLOPs compared with strong baselines on classification, regression, and generation tasks, showing its superiority. ", "summary_of_strengths": "__1. The paper is well organized and easy to follow. The proposed method is well illustrated.__\n__2. The proposed method is novel and interesting__ The proposed hash-based early exiting approach removes the learning process and only sticks to the consistency between training and inference, which is novel and interesting.  The authors introduce four kinds of hash functions based on different intuition and find that the frequency hash outperforms other has functions as well as other baselines by a large margin, which is surprising.  __3. The experimental results are impressive__ The improvement brought by HASHEE is very significant and surpasses many strong baselines. Besides, HASHEE can be used in various kinds of tasks including classification, regression, and generation tasks, showing great practicability and universality. ", "summary_of_weaknesses": "__1. The relation between instance difficulty and training-inference consistency remains vague:__      This paper seems to try to decouple the concept of instance difficulty and training-inference consistency in current early exiting works. However, I don't think these two things are orthogonal and can be directly decoupled.  Intuitively, according to the accuracy of the prediction, there are two main situations for training-inference inconsistency: the inconsistent exit makes the prediction during inference better than that during training, and vice versa. The first case is unlikely to occur. For the second case, considering instance difficulty reflects the decline in the prediction accuracy of the instances during training and inference, it may be regarded as the second case of training-inference consistency. Accordingly, I am still a little bit confused about the relation between instance difficulty and training-inference consistency after reading the paper.  I would suggest that the authors calculate the token-level difficulty of the model before and after using the hash function, and perform more analysis on this basis. In fact, if the hash function is instance-level, the sentence-level difficulty of all baselines (including static and dynamic models) can be calculated, which will provide a more comprehensive and fair comparison. \n     __2. Lack of the analysis of the relation between instance-level consistency and token-level consistency:__ The core idea derived from the preliminary experiments is to enhance the instance-level consistency between training and inference, i.e., mapping semantically similar instances into the same exiting layer. However, the practical method introduces the consistency constrain at the token level. The paper doesn’t show that whether the token-level method can truely address or mitigate the inconsistency problem at the instance level.  I would suggest the authors define metrics to reflect the instance-level and token-level consistency, and conduct an experiment to verify that whether they are correlated. ", "comments,_suggestions_and_typos": "1. I didn’t follow the description of the sentence-level hash function from Line 324 to Line 328: If we use the sequence encoder (e.g., Sentence-BERT) as a hash function to directly map the instances to the exiting layer, why do we still need an internal classifier at that layer? And considering all the instances can be hashed by the pre-trained sequence encoder in advance before training (and early exiting), the appearance of label imbalance should not cause any actual harm? Why does it become a problem?\n2. The paper addresses many times (Line 95-97, Line 308-310) that the consistency between training and inference can be easily satisfied due to the smoothness of neural models. I would suggest giving more explanations on this. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Shuhuai Ren, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 77], [77, 317], [317, 453], [453, 641], [641, 778], [778, 886], [886, 1051]], "summary_of_strengths": [[0, 5], [5, 53], [53, 96], [96, 101], [101, 148], [148, 323], [323, 324], [324, 536], [536, 537], [537, 542], [542, 584], [584, 675], [675, 836]], "summary_of_weaknesses": [[0, 5], [5, 241], [241, 327], [327, 328], [328, 564], [564, 601], [601, 823], [823, 976], [976, 977], [977, 1143], [1143, 1357], [1357, 1368], [1368, 1471], [1471, 1677], [1677, 1764], [1764, 1903], [1903, 1904], [1904, 2076]], "comments,_suggestions_and_typos": [[0, 3], [3, 281], [281, 481], [481, 511], [511, 514], [514, 688], [688, 738]]}}}]
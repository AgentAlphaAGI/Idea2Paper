[{"rid": "993a4fa71106b965ffcb5e7c864e68039f6469b258d5aa0bb55f596f9d6badbaa9668fa2d3343c79bb586c8b741bdf5cb1959aa02a293ac978bf0828d709ab5a", "reviewer": "Shuo Wang", "report": {"paper_summary": "The main contribution of this work is three-fold: 1. Unified modeling of hypothesis, source and reference sentences. \n2. Rankding-based synthetic data labeling strategy. \n3. Multi-task training that includes three separate QE tasks. \nThrough extensive experiments and detailed ablation studies, the authors demonstrate the effectiveness of each component of proposed framework. The performance of the proposed method is impressive, which surpasses the winner of WMT QE tasks. ", "summary_of_strengths": "1. Strong performance on three different types of QE tasks. \n2. Clear motivation. ", "summary_of_weaknesses": "I have some questions about this work: 1. Why not evaluate the proposed method on WMT 2020 QE tasks for REF and SRC+REF scenarios. \n2. In Table 2, I wonder why the authors do not report the results of XLM-R+Concat, which is an important baseline for the proposed method. ", "comments,_suggestions_and_typos": "Typos: 081: learning-xbased -> learning-based 208: outputted -> output "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Shuo Wang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 53], [53, 117], [117, 121], [121, 170], [170, 174], [174, 233], [233, 378], [378, 476]], "summary_of_strengths": [[0, 3], [3, 60], [60, 64], [64, 82]], "summary_of_weaknesses": [[0, 39], [39, 42], [42, 131], [131, 135], [135, 271]], "comments,_suggestions_and_typos": [[0, 71]]}}}, {"rid": "facd5d9a648b5bb76a254a7daf33a5ea41ddab1060fc3345ac18d7cf3bc6f0edff32da540c232b5ecabdfee1692ab51e54e68ab7e7093654d516828c9ce74677", "reviewer": "Shiqi Shen", "report": {"paper_summary": "This paper proposed a unified framework to handle reference-only, source-only and source-reference-combined evaluation tasks. They propose monotonic regional attention and unified pretraining to better adapt multi-task training. The experiments show the superiority of their methods across tasks. ", "summary_of_strengths": "This paper proposes a unified framework for translation evaluation, which can be adopted into reference-only, source-only and source-reference-combined tasks. With Monotonic Regional Attention, they unified the inputs in these tasks. Then, they synthesis the labels of data in a rank-based manner. Experimental results and details in ablation studies show the superiority of their methods. ", "summary_of_weaknesses": "1)\tThe data labeling methods transform the prediction score in a ranked way. However, different evaluation approaches may produce different order. In this paper, only one evaluation approaches are used to generate the prediction score. \n2)\tWriting should be improved as the paper is not easy to follow. ", "comments,_suggestions_and_typos": "1)\tAs an abbreviation, PLM in line 67 should be defined first 2)\tThe authors should give some intuitive case about hypothesis, source and reference in experiments 3)\tThe introduction of Hard MRA need improvement. I thought the monotonic means the sequence h->s->r. Because h->s is not allowed, the hard MRA forbids the h->s, s->r, and h->r 4)\tLine081, learning-xbased should be learning-based "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Shiqi Shen, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 126], [126, 229], [229, 297]], "summary_of_strengths": [[0, 159], [159, 234], [234, 298], [298, 390]], "summary_of_weaknesses": [[0, 77], [77, 147], [147, 236], [236, 303]], "comments,_suggestions_and_typos": [[0, 213], [213, 265], [265, 393]]}}}]
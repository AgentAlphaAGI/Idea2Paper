[{"rid": "2dc38b3a3375bd499d4e33815b4d1eb2736c66905f234af908bd514e1fb4506577286c4c1c8a107309ed1a3354a73a90bf58cefbada74c8f21b7ebd8b1229764", "reviewer": null, "report": {"paper_summary": "This paper argues that existing benchmarks for the text-to-SQL task have failed to capture the out-of-domain problem on column operations, matching domain-specific phrases to composite operation over columns. To tackle this problem, the authors propose a synthetic dataset together with a repartitioned train/test SQUALL dataset. On the methodology, prior domain knowledge is injected via column expansion templates, which requires a small amount of manual effort. For example, a \"time_span\" column can be expanded as \"start_time\" and \"end_time\" columns. Schema pruning is also proposed to drop irrelevant columns. In this way, column operations can be reduced to regular column matching. Empirical results on the two datasets show that the proposed method can significantly outperform two strong baseline parsers, seq2seq of Shi et al. (2020) and SmBop of Rubin and Berant (2021). ", "summary_of_strengths": "1. This paper reveals a new perspective of out-of-domain generalization on the text-to-SQL parsing, which I feel is a practical problem and valuable to the community. \n2. A synthetic dataset covering three domains (finance, sports, and science) has been newly created. The existing SQUALL has been repartitioned considering column operations such as field accessors or arithmetic operations. Both datasets target the domain generalization of column operation. \n3. The proposed method may seem to be engineering, where a small amount of manual effort is needed to design templates for schema expansion. But it works well on the two proposed datasets. The improvement against the baseline parsers is significant. And it helps the parser to relieve the burden of reasoning and reduces the column operations to a column matching problem. Schema pruning is also proposed to boost performance. \n4. The experimental study, as well as the discussion, is comprehensive. ", "summary_of_weaknesses": "1. My biggest concern is whether the templates are domain or dataset-specific. The motivation is to address the domain generalization on column operations, but it is implemented by manually crafted ad-hoc templates that could be tailored for the datasets. I understand this paper is the first step to tackle this problem. The authors also say that improvement does exist and automatic schema expansion is worth future study. \n2. I have not extensively studied the benchmarks in the text-to-SQL task. So I'm not sure whether repartitioning the SQUALL is the best choice or any other resources are also available. ", "comments,_suggestions_and_typos": "1. I have a question regarding the schema expansion for the synthetic dataset. According to my understanding,  Table 2 shows the templates used for the SQUALL. What are the templates used for the synthetic dataset? Are they the same as the formulas used in the dataset construction (Table 5)? If so, will this setting make the synthetic dataset easier? The figures in Table 3 demonstrates that the performance on the synthetic dataset is much better than that on the repartitioned SQUALL. \n2. I haven't seen any typos. The paper is clearly written. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 209], [209, 330], [330, 465], [465, 555], [555, 615], [615, 689], [689, 882]], "summary_of_strengths": [[0, 3], [3, 167], [167, 171], [171, 269], [269, 392], [392, 460], [460, 464], [464, 602], [602, 650], [650, 711], [711, 834], [834, 888], [888, 892], [892, 961]], "summary_of_weaknesses": [[0, 3], [3, 79], [79, 256], [256, 322], [322, 425], [425, 429], [429, 500], [500, 612]], "comments,_suggestions_and_typos": [[0, 3], [3, 79], [79, 160], [160, 215], [215, 293], [293, 353], [353, 489], [489, 493], [493, 519], [519, 549]]}}}, {"rid": "20a7357f69396b2ed4763eb65b591e82a4863399650698a508e81b51c5cb3535beb9a5304fc63ce8863548101c926e6214e66a28d86bda5a08043ba02c2333e3", "reviewer": null, "report": {"paper_summary": "Paper deals with a particular problem in designing parser for Text-to-SQL, i.e., mapping phrases in the input question to composite operations over table columns. Paper hypothesize that problem of column operations is challenging for SOTA Text-to-SQL parsers. To verify the hypothesis paper proposes two benchmark datasets: 1) a synthetic dataset, 2)  a different train/test partitioning of the existing SQUALL dataset. The paper shows two SOTA parsers SEQ2SEQ (Shi et al. 2020) and SMBOP (Rubin and Berant2021), struggle on the proposed benchmarks. Paper design two modules: schema expansion and schema pruning which can be added to these SOTA parsers to handle the problem of column operations. ", "summary_of_strengths": "1. Paper shows the weakness of two SOTA Text-to-SQL parsers in handling column operations by providing two benchmark datasets.\n2. Paper design two simple modules schema expansion and schema pruning which utilizes the domain knowledge about different column operations to improve the performance of SOTA Text-to-SQL parsers.\n3. Report 5.1% absolute performance improvement on new SQUALL data split using the domain knowledge. ", "summary_of_weaknesses": "1. A critical weakness of the paper is the lack of novelty and incremental nature of work. The paper addresses a particular problem of column operations in designing semantic parsers for Text-to-SQL. They design a new dataset which is a different train/test split of an existing dataset SQUALL. The other synthetic benchmark paper proposed is based on a single question template, \"What was <column> in <year>?\".\n2. The paper assumes strong domain knowledge about the column types and assumes a domain developer first creates a set of templates based on column types. With the help of these column templates, I think many approaches (parsers) can easily solve the problem. For example, parsers utilizing the SQL grammar to generate the output SQL can use these templates to add new rules that can be used while generating the output. Few such works are 1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. ", "comments,_suggestions_and_typos": "1. It will good if the authors can learn the templates for schema expansion from source domain data. \n2. Compare the proposed approach with methods which uses domain knowledge in the form of grammar. Comparing with below methods will show generality of ideas proposed in the paper in a much better way.\n1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 163], [163, 260], [260, 420], [420, 550], [550, 697]], "summary_of_strengths": [[0, 3], [3, 127], [127, 130], [130, 324], [324, 327], [327, 425]], "summary_of_weaknesses": [[0, 3], [3, 91], [91, 200], [200, 295], [295, 412], [412, 415], [415, 567], [567, 672], [672, 833], [833, 855], [855, 923], [923, 1033], [1033, 1111]], "comments,_suggestions_and_typos": [[0, 3], [3, 101], [101, 105], [105, 200], [200, 303], [303, 306], [306, 374], [374, 484], [484, 562]]}}}, {"rid": "a09847ed94d9d36c62841908a6163e9cbb9e341099ac05ab01ec5a9c9de0e32dd33a5f044591d548fb961736b3ef91c5043ff5a59632d5bfa86eaf5303e5f438", "reviewer": null, "report": {"paper_summary": "This paper identifies an important and challenging problem for out-of-domain generalization of text-to-SQL parsers, which is the alignment between composite column operations and their corresponding domain-specific mentions in natural language. The paper proposes a synthetic dataset to study this, and then a method based on schema expansion and pruning to alleviate the issue.   Though the technical contribution from this work is limited, the identified problem along with the thorough studies using both synthetic and real datasets provide many interesting observations on the out-of-domain generalization of text-to-SQL parsing. I believe this work would be valuable to the community of people working on text-to-SQL parsing. ", "summary_of_strengths": "This work identifies an under-explored problem that is important for out-of-domain generalization of text-to-SQL parsing. Moreover, it provides a simple yet effective expand-then-prune pipeline to address the problem. ", "summary_of_weaknesses": "As mentioned in the paper, the process of expanding columns is quite heuristic, and pruning is arguably tricky to train and tune. ", "comments,_suggestions_and_typos": "Maybe the authors can talk a bit about what the findings of this work imply from the perspective of the creator/maintainers of databases. For example, are the template used for schema expansion helpful for creating a \"parsing-friendly\" schema? "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 245], [245, 379], [379, 381], [381, 634], [634, 731]], "summary_of_strengths": [[0, 122], [122, 218]], "summary_of_weaknesses": [[0, 130]], "comments,_suggestions_and_typos": [[0, 138], [138, 244]]}}}, {"rid": "23f6971e79e116fe974bff7b07d482f1885af95851618a3d58c5d2d5d56c703094b07e02dd06e509d66ec737b89cc503c91998421d8792a24886dea28d3bfaf1", "reviewer": null, "report": {"paper_summary": "This paper studies the text-to-SQL generalization problem, focusing on the Column Operations problem. \nThe contribution of this paper: 1.  Propose two new benchmarks: a synthetic dataset and a train/test repartitioning of the SQUALL dataset, both capable of quantifying out-of-domain generalization on column operations. \n2. Propose a schema expansion method using heuristics to expand columns into sets of derived columns. \n3. Propose a schema pruning method to prune the relevant columns that the final parser needs to look at. ", "summary_of_strengths": "The contribution of this paper: 1.  Propose two new benchmarks: a synthetic dataset and a train/test repartitioning of the SQUALL dataset, both capable of quantifying out-of-domain generalization on column operations. \n     These benchmarks are useful for evaluating the column operation problem. \n2. Propose a schema expansion method using heuristics to expand columns into sets of derived columns. \n     Schema expansion is a good idea for this problem. \n3. Propose a schema pruning method to prune the relevant columns that the final parser needs to look at. \n    Schema pruning is also a good idea. ", "summary_of_weaknesses": "The column operation problem is worth studying, and the proposed ideas are also good, but the implementation is not exciting.\nFirst, Schema Extension requires manual annotation, which means that reasonable schema design and annotation can eliminate the problem of column operation. However,  this conclusion can be also inferred from the previous works about schema linking. So, while this is indeed a solution, the manual schema expansion method is not exciting.  Secondly, I think schema pruning should be directly integrated into the text-to-SQL model and trained together, just like many models on the Spider leaderboard. During the training, the model can learn which columns should be left. Then we do not need to manually find a parameter for the schema pruning module. Unless detailed comparative experimental, I do not think the current schema pruning design is reasonable. ", "comments,_suggestions_and_typos": "An auto Schema Extension method would be more exciting. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 102], [102, 138], [138, 321], [321, 325], [325, 424], [424, 428], [428, 530]], "summary_of_strengths": [[0, 35], [35, 218], [218, 297], [297, 301], [301, 400], [400, 456], [456, 460], [460, 562], [562, 603]], "summary_of_weaknesses": [[0, 126], [126, 282], [282, 375], [375, 464], [464, 465], [465, 626], [626, 697], [697, 777], [777, 883]], "comments,_suggestions_and_typos": [[0, 56]]}}}]
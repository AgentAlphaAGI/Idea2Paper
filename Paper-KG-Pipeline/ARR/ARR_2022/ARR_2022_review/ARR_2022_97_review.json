[{"rid": "90fd08bb12ac39d88c9bcbaf9e4b90216cf152f7391b9c8139538d88d38a65410856a94255acb0a49ae597619b6babb58abc178d2eb9cd25fd1efb9ae74d0e86", "reviewer": null, "report": {"paper_summary": "This paper decomposes the concept-to-text natural language generation task into three stages, (i) lexicalization, (ii) recurrent aggregation and (iii) post-editing.  The intent is to enable large pre-trained models to better leverage transfer learning.  The approach is evaluated on several datasets including Schema-Guided Dialogue, MultiWoZ and WebNLG. ", "summary_of_strengths": "- Decomposing text generation from meaning representations into distinct stages is a constructive, and potentially impactful approach.\n- Results are presented on four datasets with varying degrees of training examples, and ablations were performed.\n- Approach appears to consistently perform better on the Missing Slot Error metric. ", "summary_of_weaknesses": "- Several important aspects of the approach are underspecified: How are the meaning representations segmented into individual facts (Section 2.1)?  What are the inputs/outputs to each stage of the system (Sections 2.2 to 2.4)?  How are the lexicalization, aggregation and post-editing tasks performed (e.g., is this just a T5_small model)?\n- The development and exact form of the loss function (Eq 1) is unclear.\n- The label inference process seems spurious (Section 2.6).  Some examples and statistics would be helpful. ", "comments,_suggestions_and_typos": "The work seems to be presented as an engineering task and provides limited insight into the fundaments of how decomposing the task help language models to consume meaning representations to generate text.  In particular, exploring the potential relationship between lexicalization and missing slot error would have made for a much stronger paper.  Also the level of specificity in Section 2 obscured the exact nature of the approach.\nThe terms s_x and v_x are not clearly defined.  Are these slots and value bindings, respectively?  If so, how are is each derived? "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 165], [165, 253], [253, 355]], "summary_of_strengths": [[0, 135], [135, 249], [249, 333]], "summary_of_weaknesses": [[0, 147], [147, 227], [227, 340], [340, 413], [413, 473], [473, 521]], "comments,_suggestions_and_typos": [[0, 205], [205, 347], [347, 434], [434, 481], [481, 532], [532, 565]]}}}, {"rid": "3440a8cac0acda8d2760dbc4b435400589f1a5f3b2d4bfc9728c8e5f990e7a0ad599d0d2c16f0bdbce56541e6064c9e0138cd6d2691c84a52e25d82c5da47894", "reviewer": null, "report": {"paper_summary": "This paper proposed a hierarchical recurrent aggregation framework to reduce the few-shot NLG task. The proposed framework consists of three modules, lexicalization, aggregation, and post-editing, which are composed of PLMs and do not share parameters. The experimental results show that the proposed method is slightly better than the baseline under low-resource settings. ", "summary_of_strengths": "The method performs better than the compared baseline under low-resource settings, especially for MER metrics. ", "summary_of_weaknesses": "1. ** Poor efficiency**. The proposed method contains three different modules, each containing an unshared PLM so that the amount of parameters is three times that of the End2End method. As T5 is good at multi-task learning, I think the author should consider how to improve the performance of the model in the case of parameter sharing, rather than simply stacking multiple models. \n2. ** Time-consuming**. The proposed method requires recurrent generation so that the speed of response generation will be significantly reduced, especially when the amount of attribute-value pairs is relatively large. It should be noted that the NLG model is only a part of the dialogue system of the pipeline structure, and time efficiency also needs to be taken into consideration. \n3. ** Lack of strong baselines**. The baseline for comparison is only E2E-T5, which is a naive E2E-NLG method and may not be strong enough. Therefore, the improvement of the proposed method over a single T5 model(E2E-T5) may not be a very valuable conclusion.  I think the author should investigate and compare more few-shot NLG methods to enhance the persuasiveness of the experimental results. \n4. Lack of some additional insight into the effectiveness analysis for three modules. \n5. The innovations in this paper are more like tricks for improving metrics. ", "comments,_suggestions_and_typos": "**Suggestions**: See above.\n**Questions**: In table 1, The MER indicators of +aggregation and +post-edit are both higher than 1.1, while the MER of +selection is very low. From Line.402-Line.405, the selection is selecting the one with lower MER from aggregation and post-edit. Why the best MER result is reduced to 0.14 after selection "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 100], [100, 253], [253, 374]], "summary_of_strengths": [[0, 111]], "summary_of_weaknesses": [[0, 6], [6, 187], [187, 383], [383, 390], [390, 603], [603, 769], [769, 776], [776, 910], [910, 1030], [1030, 1166], [1166, 1170], [1170, 1253], [1253, 1257], [1257, 1331]], "comments,_suggestions_and_typos": [[0, 28], [28, 172], [172, 278], [278, 337]]}}}]
[{"rid": "04e97d4c546e9c4af4d723abfb39a7fd257a1e4107ba6b6f800781d793026e08ec305afd199483d4c061cdb15a2ac405f16e43bd49679780f20a937504822a47", "reviewer": "Momchil Hardalov", "report": {"paper_summary": "The paper focuses on the intent understanding of textual TODO notes. The authors propose a weakly supervised approach based on multi-task learning to pre-train better vector representations for the former task. The auxiliary tasks include: (1) autocompletion, MLM objective based on hand-crafted rules, (2) pre-action (what is needed) and goal generation (pseudo intent) and (3) action arguments prediction, using labels from FrameNet. The proposed approach outperforms strong (sentence) embedding baseline models in zero-shot setting. ", "summary_of_strengths": "1. The paper is clearly written and mostly easy to follow, the proposed approach is well motivated. \n2. The task is interesting, especially from a practical point of view. \n3. The presented results are convincing and I think the evaluated baseline models are good enough. One question left unanswered is -- if the selected similarity functions are the best choice for these models, additional ablation is welcome. \n4. Extensive ablation study of the proposed tasks. ", "summary_of_weaknesses": "1. The experiments are held on a private datasets and the exact setup is impossible to reproduce. \n2. A minor point would be that few-shot would be a more realistic setup for that task, as domain-specific TODOs are easy to acquire, however I agree that the current setup is adequate as well. \n3. More error analysis could be useful, especially on the public dataset, as their data could be included without any restrictions, e.g., error types/examples? patterns? Examples when non-contextualized embeddings outperform contextualized ones, or even LITE? ", "comments,_suggestions_and_typos": "I urge the authors to release at least some part of the dataset to the wider public, or under some  end user-agreement.\nComments: 1. I suggest the authors to focus their comparison on word2vec baselines (currently in appendix), instead of Sentence-BERT, as the latter does not show good performance on the short texts. It seems that non-contextualized embeddings are more suitable for the task. \n2. Maybe it makes more sense to try out models pre-trained on conversations, e.g., text from Twitter or natural language conversations. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Momchil Hardalov, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 69], [69, 211], [211, 436], [436, 536]], "summary_of_strengths": [[0, 3], [3, 100], [100, 104], [104, 172], [172, 176], [176, 272], [272, 414], [414, 418], [418, 466]], "summary_of_weaknesses": [[0, 3], [3, 98], [98, 102], [102, 292], [292, 296], [296, 453], [453, 463], [463, 553]], "comments,_suggestions_and_typos": [[0, 120], [120, 133], [133, 319], [319, 395], [395, 399], [399, 532]]}}}]
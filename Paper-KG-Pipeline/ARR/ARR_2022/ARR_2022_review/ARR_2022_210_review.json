[{"rid": "eef4ebc16619c1ad49da471d617fb0eec96eb8d523186f9c9f1a22ec980f38dbdf50c068356ee8041e5d2bf8740d7773582cf2de2169474cba9a539a57666132", "reviewer": null, "report": {"paper_summary": "This seems to be a primarily theoretical ML paper that describes the _softmax bottleneck_ problem in neural LMs, which results from the fact that a single hidden state embedding cannot be close to the embeddings of all possible candidate next words under certain conditions. The paper then proposes an improvement for the traditional softmax use in neural LMs, namely the _multi-facet softmax_ (MFS). This is compared to an existing MoS approach of Yang et al., as well as some other baselines. The modeling experiments show that the described problem indeed occurs in the artificially designed data sets, and that the proposed MFS solution solved this problem. In addition, MFS yields better results than the baselines in terms of language modeling perplexity and in the task of answering ambiguous questions. ", "summary_of_strengths": "The paper describes what seems to me like an important issue that may affect most/many of the existing LMs, and the results seem to be consistent across the presented tasks. ", "summary_of_weaknesses": "The paper is very dense, with a lot of theoretical material, may be more suitable for a ML conference or a journal rather than *ACL conference. Having said that, I lack expertise in ML and may not have a good feel for where this kind of studies belong. ", "comments,_suggestions_and_typos": "- Line 036 - \"probabilities of becoming the next word\" reads awkward to me - Line 046 - should be \"man\" instead of \"king\"?\n- Line 166 - typo, \"thoery\" - Line 214 - theorem 2 could benefit from providing an intuitive explanation - Line 280 - typo, \"there existS\" "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess.", "sentences": {"paper_summary": [[0, 275], [275, 401], [401, 495], [495, 662], [662, 811]], "summary_of_strengths": [[0, 174]], "summary_of_weaknesses": [[0, 144], [144, 253]], "comments,_suggestions_and_typos": [[0, 75], [75, 123], [123, 151], [151, 228], [228, 262]]}}}, {"rid": "a29a1dc746271e6018ad4ecf7a57733b728a0d9eb96d7e2774901756bc16e064b02755cc9b84a287975faea3603a14669d67a6cb6492d6ab13b584229d419504", "reviewer": null, "report": {"paper_summary": "This paper discusses a known problem regarding output distributions over words. \nThe problem originates from using a single hidden state vector and global word vectors for dot product and softmax calculations. \nThis leads to the model not being able to generate arbitrary distribution over words using inference.\nUnderstanding the problem from a geometrically motivated perspective, the authors propose \"multi-facet softmax (MFS)\". \nCompared to the previous methods, MFS essentially introduces two improvements: multiple input hidden states and multiple partitions on the vocabulary.\nThe authors then move on to explain several experiments using GPT2 and BERT (found in the appendix) models, focusing on language modeling tasks. \nThe comparison across different softmax enhancement methods shows that MFS brings noticeable perplexity improvements at a cost of slightly more parameters and inference time similar to that of the other methods, when compared to the softmax baseline. ", "summary_of_strengths": "The paper explains the limitation of expressiveness of the output generation modules of our neural language models. \nAlthough the problem is known before, and sometimes referred to as \"softmax bottleneck\", the authors were able to provide their insights on the issue.\nThe proposed method MFS makes use of multiple input hidden states and multiple partitions on the vocabulary, and can be viewed as a softmax enhancement, which is backed by improved perplexity numbers.\nThe virtue of the paper, from my point of view, has to come from the unified view of all the considered softmax enhancement methods, i.e. the configuration section in Table 1. \nI think this is a very nice addition to the current literature in unifying the methods and providing a systematic view of what people have tried to overcome the aforementioned problem. ", "summary_of_weaknesses": "While debatable, my biggest concern with the paper, or this line of work in general, is if the direction is really worth it.\nLet me explain myself.\nThe paper begins with the famous \"king, man, queen, women\" example and motivates the work by saying the proper next word following the \"After debating whether to bow to the woman or the king first, the jester decided on the\" context should be \"woman\" and \"king\", instead of \"queen\" and \"king\". \nThis \"golden answer\" is already questionable. \nThe model learned from other contexts that \"queen\" could be a candidate here and in some sense, \"queen\" is not such a bad guess for who the \"woman\" is. \nIn other words, the geometrical structure of words is probably what we want the model to learn in the first place.\nJumping out from the example and we could discuss the issue in a broader sense. \nThe \"softmax bottleneck\" originates from the inner dimension of the dot product, i.e. the hidden dimension, being much smaller than the vocabulary size. \nIf we really want to model arbitrary output distributions, then why not just define a fixed set of orthogonal word vectors and let the hidden states do the magic? \nThe limitation of the memory size restricts us to using a hidden dimension size smaller than the vocabulary size, and this results in the low rank of the log probability matrix, and this results in a series of complicated methods that try to overcome the bottleneck. \nIf one looks at the numbers, is 15.64 vs 15.89 PPL (GPT-2 Medium in Table 1) really worth the effort?\nI know this point is quite subjective and it is definitely not personal against the authors. \nI simply want to raise a bit of concern with the direction we are going in. \nThe authors did a good job in providing a unified view of the softmax enhancement methods (including theirs of course), and it is the direction in which the methods move that I have a doubt about. ", "comments,_suggestions_and_typos": "Maybe expand on the hyperparameter choices a bit more. \nHow should one decide on the number of softmaxes, input hidden states and partitions? \nHow did you do it?\nL166, theory.\nL280, there exists... L351, why 0.4 epochs? \nWhich 40% of the training data is used? \nHow is it decided?\nL425, the reference to Tab.2 in the main text happens after the reference to Tab. 3.\nThe following papers could be nice additions to the reference: https://arxiv.org/abs/1412.6623 https://arxiv.org/abs/1704.08424 https://arxiv.org/abs/1705.08039 https://arxiv.org/abs/1806.04313 https://arxiv.org/abs/1910.12554 "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 80], [80, 210], [210, 313], [313, 432], [432, 584], [584, 729], [729, 981]], "summary_of_strengths": [[0, 116], [116, 268], [268, 469], [469, 645], [645, 831]], "summary_of_weaknesses": [[0, 125], [125, 148], [148, 442], [442, 489], [489, 642], [642, 758], [758, 838], [838, 992], [992, 1156], [1156, 1424], [1424, 1527], [1527, 1620], [1620, 1697], [1697, 1895]], "comments,_suggestions_and_typos": [[0, 55], [55, 142], [142, 162], [162, 176], [176, 198], [198, 220], [220, 261], [261, 281], [281, 363], [363, 366], [366, 429], [429, 461], [461, 494], [494, 527], [527, 560], [560, 593]]}}}, {"rid": "3509a36805c40e4daeb6764d44ec941ce2ce3aa3c0c60a392aa14260ac722032ad7ea98de5ebb67addc506cd978eb4241a1d3079302ccde45397f1dfa2a1cf59", "reviewer": "Nikolay Bogoychev", "report": {"paper_summary": "This paper highlights a limitation of conventional softmax classifiers that use a single low rank fixed weight matrix in the softmax layer to compute logits via dot products from a single contextual embedding. This issue is also known as \"the softmax bottleneck\" The limitation is that not all rankings of vocabulary items by probability can be represented and the authors highlight a consequence of this limitation with the following example. \nThey show that there are slot filling templates where two proposed alternatives are clearly expressed in the text. \nWhile we would expect the language model to assign the two largest probabilities to the two alternatives, they show that this is not always possible and sometimes the model must resort to ranking other words not explicitly stated between the two alternatives. \nAfter providing a theoretical proof of the above limitation, the authors propose improving a model (Mixture of Softmax MoS) previously proposed to alleviate representational limitations for low rank softmax layers in two directions:   a) they create logits from multiple representation of the inputs (facets)   b) they partition the vocabulary to enable more expressive distributions over subsets of the vocabulary. \nThey provide empirical evidence that their approach can provide improvements in perplexity in Language Models and top-k accuracy for Question Answering. ", "summary_of_strengths": "The paper is well written.\nThe main idea is clearly expressed and the paper highlights theoretical insights and how they apply to real world examples.\nThe related work section is adequate, albeit incomplete (though I agree the missing references are very difficult to find).\nModest performance improvement over existing methods.\nThe paper brings awareness about how big language models may not necessarily distribute their probabilities in a way humans might anticipate. ", "summary_of_weaknesses": "1) Improvements are overstated. The abstract mentions \" two-fold improvements\" in perplexity, whereas the actual improvements are tiny. What exactly did you mean by 2 fold improvement?\n2) Perplexity is not a suitable metric for the task in question. What you are highlighting is the probability distribution between likely candidates for top-K prediction of a softmax, which is not adequately measured by perplexity. There are other metrics, such as MRR which is mentioned in the appendix. Perhaps switching evaluation based on that would vastly improve the experimental section of your paper. The problem you are highlighting is more linked to ranking rather than perplexity.\nThere are many counter examples where perplexity can be very high but the model ranks the vocabulary correctly. E.g. with |V| = 100 assign the top probabilities as \"woman\" with prob 2/100, \"king\" with prob 1/100 and spread the remaining prob mass over the remaining 98 words - this has extremely high entropy but correct ranking.\n3) Readers unfamiliar with the softmax bottleneck problem will struggle to understand the parallelogram example, or understanding theorem 2. More real estate should be allocated in the main paper to explaining those two, so that the reader is not forced to look at the appendix to understand the main body of the paper.\n4) Line 307-320, it is not at all clear how you guarantee that the top-k words would always fall into different partitions. Could you elaborate on that. How are the partitions computed?\n5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. ", "comments,_suggestions_and_typos": "There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger.\nThe paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful.\nWhy are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format.\nMissing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.\n- Line 195: plane -> hyperplane - Figure 1 caption:  middle -> midpoint - Line 1453: \"the tanh removes the magnitude of facets\" -> Could you explain this in more detail? I could not make sense of why tanh \"removes the magnitude\" - what do you mean specifically by \"magnitude\" here?\n- Line 1457: Why is invertibility important? My reading of section F.1 is: We removed tanh because it worked better empirically (which sounds fine to me, provided it doesn't change the rankings of your proposed models and the baselines).\n- There is a mistake in rows \"MFS - Multi partition\" and \"MFS - Multi Input\" in Table 2. MFS - Multi partition has 1 #P and 9 #I according to the table while Multi Input has 4 #P and 1 #I which does not make sense.\nRephrasing: Line 063: \"output the woman or king.\" - > \"output woman or king\" Line 121: \"is sometimes not be able\" -> \"is sometimes not able\" Line 1271: \"the GPT-2\" -> \"GPT-2\" Line 235: \"multi-mode\" -> \"multimodal\" also line 263. \nLine 911: \"As in section 5\" -> \"As in Section 5\" "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Nikolay Bogoychev, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 210], [210, 444], [444, 560], [560, 821], [821, 1238], [1238, 1392]], "summary_of_strengths": [[0, 27], [27, 151], [151, 275], [275, 329], [329, 471]], "summary_of_weaknesses": [[0, 32], [32, 136], [136, 185], [185, 250], [250, 417], [417, 490], [490, 594], [594, 677], [677, 789], [789, 1007], [1007, 1148], [1148, 1327], [1327, 1451], [1451, 1480], [1480, 1513], [1513, 1627], [1627, 1694]], "comments,_suggestions_and_typos": [[0, 106], [106, 240], [240, 292], [292, 351], [351, 474], [474, 570], [570, 617], [617, 648], [648, 730], [730, 805], [805, 1113], [1113, 1145], [1145, 1185], [1185, 1283], [1283, 1395], [1395, 1440], [1440, 1633], [1633, 1722], [1722, 1848], [1848, 1860], [1860, 1900], [1900, 2077], [2077, 2127]]}}}, {"rid": "05ba17b12c642edea83f5356e76705ea5a46df33ab99029966e87e8756d9a65f6565a009f23a020702d284bfd96e94a04ac5a833f31266134b1bdbf695f6e4d7", "reviewer": "Hao Tang", "report": {"paper_summary": "The paper tackles a fundamental problem of linear layer followed by a softmax. Due to the particular parameterization, there is a problem termed softmax bottleneck, where the degree of freedom for the output word distribution is limited by the dimension of the hidden vector. Building on top of this observation, the paper formally shows that when some word embeddings are linearly dependent, it becomes impossible to produce certain ranking of words. To overcome this problem, special output layers or parameterization needs to be designed. One potential solution is to use a mixture of softmax (MoS), but the paper further argues that MoS is still not expressive enough, and attribute the limitation to the reuse of a single hidden vector. The paper then proposes to use multiple hidden vectors to compute mixture of softmax, and term the approach multi-facet softmax due to the use of multiple projections. Results show that multi-facet softmax can improve over the regular mixture of softmax. ", "summary_of_strengths": "The paper is easy to follow. The approach is very well motivated. The figures are helpful for understanding the different approaches and the proposed approach. ", "summary_of_weaknesses": "The paper really struggles to find a case where the proposed approach is better than the regular MoS. The improvement on natural language seems to be small, and the paper needs to work on a synthetic language to show the gap between MoS and the proposed approach. The paper then looks into a QA task where the uncertainty is high to showcase the benefits of the proposed approach, but the gain seems to be small. Either natural language does not exhibit the variability that fits the theory, or the problem of modeling is not a lack of uncertainty in the output distribution but requires something else.\nThe use of the term bimodal suddenly appears in section 5, and the use of the terms hints that it is the multimodality of the distribution that causes the problem. Indeed, this has been pointed out in Chris Bishop's seminal paper, Mixture Density Networks. However, the connection between multimodality problem and the theory in the paper is missing. It would be much better to draw the connection in section 2, and signpost a little in section 5.\nThere are minor presentation issues. For example, it is not until section 2.2 did I start to understand why the problem stated in the introduction exists. it would be much better to explain the problem in words in the introduction. As another example, the figure contains too many colors, and the colors don't particularly mean anything and are a bit distracting. ", "comments,_suggestions_and_typos": "I would talk about theorem 1 informally in the introduction. I would also remove unnecessary colors in the figures.\nTheorem 2 reads a bit abstract, so it would be much better to relate the theorem with the running, queen-king-woman-man example.\nI also think that the argument about the limitation of MoS, in the second paragraph of section 3.2, is a little dense. It might be better to add more redundancy to that paragraph, using different ways to explain the same concept. "}, "scores": {"overall": "4.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Hao Tang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 79], [79, 276], [276, 452], [452, 542], [542, 742], [742, 910], [910, 997]], "summary_of_strengths": [[0, 29], [29, 66], [66, 160]], "summary_of_weaknesses": [[0, 264], [264, 413], [413, 604], [604, 768], [768, 861], [861, 955], [955, 1052], [1052, 1089], [1089, 1207], [1207, 1284], [1284, 1416]], "comments,_suggestions_and_typos": [[0, 61], [61, 116], [116, 245], [245, 364], [364, 475]]}}}]
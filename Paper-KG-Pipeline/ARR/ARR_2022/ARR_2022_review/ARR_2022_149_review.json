[{"rid": "6b2ca8bb05bc0d53aec7d4cf25940f4cd603d3cd4189c894565711b7a703237bff6efeb3c26e742a3f16ab5e9080a7ae8c948f3c5565c55e74d1c0145696f35b", "reviewer": "Pierre Lison", "report": {"paper_summary": "The paper investigates the problem of detecting implicit offensive comments in dialogue, where multiple interpretative steps may be needed to uncover the offensive nature of the comment. Such interpretative steps may include, among others, the use of common sense knowledge or stereotypes. Along with describing the task and arguing for its importance, the paper presents a new dataset of implicit offensive comments (annotated with the \"reasoning steps\" required to go from the original implicit comment to its explicit equivalent). Finally, the paper shows how how entailment models can be exploited to determine which reasoning step is most likely at each stage. ", "summary_of_strengths": "- The paper addresses an interesting and important question - It is also very clear, especially in its discussion of the task and its relation with previous work - It comes along with a new dataset ", "summary_of_weaknesses": "The models described in the evaluation rely on a number of strong assumptions, as detailed by the authors in the discussion. Although entailment models can be applied to estimate the probability of a given step in the reasoning chain, the paper does not really propose a full model that would apply such steps one by one, using some type of decoding process. ", "comments,_suggestions_and_typos": "The authors have done a very good job at addressing the comments from the previous reviewing round. ", "ethical_concerns": "the ethical concerns are well covered in the paper "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Pierre Lison, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 187], [187, 290], [290, 534], [534, 666]], "summary_of_strengths": [[0, 60], [60, 162], [162, 198]], "summary_of_weaknesses": [[0, 125], [125, 359]], "comments,_suggestions_and_typos": [[0, 100]], "ethical_concerns": [[0, 51]]}}}, {"rid": "6db5d3547297644e1a01b2dd191f13fced41cc86ff3a74d59ca05167091c7b4bbe9f67ec055106ee390fe9cdb497d99a303a97c2d53e008f5f89e93f722c3fbb", "reviewer": null, "report": {"paper_summary": "This paper propose a method to automatically identify offensive instances in natural language when the offensiveness is implicit. The method aims at constructing a chain of logical inference that links the original utterance to an offensive statement. ", "summary_of_strengths": "The proposal is novel and i aims at building a more transparent system, where the reasoning behind the final offensiveness label is actually explicitly defined as a sequence of steps. ", "summary_of_weaknesses": "Several aspects of the proposal are not clear from how it is presented in this paper. The method is based on a highly rich and complex manually-built dataset, which appears to be hard to adapt or scale to other scenarios, domains, abusive phenomena, targets, languages, and so on. The actual effort in terms of time and money is not known, and a discussion on the scalability of the resource is needed.\nThe annotation task also seems to be particularly subjective, dealing with individual perception of what is offensive and what is supposedly common ground knowledge. While the authors' claim \"a person's identity or attributes have not played a critical role in existing OTD research\", there are actually recent works on this direction, e.g. (taken from the perspective data manifesto): https://arxiv.org/pdf/2112.04030.pdf https://ojs.aaai.org/index.php/HCOMP/article/view/7473/7260 https://aclanthology.org/2021.emnlp-main.822.pdf Another, more fundamental issue, that I'd like to see discussed is that reasoning in this work is treated as a seq2seq problem and approached by neutral networks. This feels like a step back with respect to the large literature on semantic parsing and logical-based inference. With the abundance of ontologies, knowledge graphs, and automatic reasoning tools, it is odd that none of them is even mentioned, while the task is reduced to a series of machine learning steps, hindering the transparence of the process. ", "comments,_suggestions_and_typos": "Eleven pages of Appendix are an indication of a great effort that went into this paper. Perhaps parts of it would be better positioned into the main text, in particular the annotators' guidelines. "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 130], [130, 252]], "summary_of_strengths": [[0, 184]], "summary_of_weaknesses": [[0, 86], [86, 281], [281, 403], [403, 569], [569, 935], [935, 1098], [1098, 1212], [1212, 1450]], "comments,_suggestions_and_typos": [[0, 88], [88, 197]]}}}, {"rid": "0909e18d6543fb938fbbc76b929ee91db3362584afe0043413fa43730372504bb836138dc424c5a3f4b33b5ffd8bd9a0131df90ee7c7041ad242384ddcce3441", "reviewer": "Won Ik Cho", "report": {"paper_summary": "This paper formulates implicit offensive text detection (OTD) as a multi-hop reasoning problem that includes the chain of reasoning, generating a dataset that consists of explicit and implicit offensive texts, to demonstrate how the detection is difficult and should be handled.  In detail, the authors assume the circumstance where a personal attribute of reader or listener is pre-defined, attained from PERSON-CHAT corpus. Then, three kinds of statements, namely implicitly offensive, explicitly offensive, and non-offensive ones are created concerning the attribute. Afterwards, a reasoning chain that explains the flow from implicit to an explicitly offensive statement, is augmented. The construction phase is depicted in detail, along with several post-processing rules for chain generation.\nFinally, the authors propose the dataset, Mh-RIOT, and show that the conventional pretrained language models with a simple classification strategy cannot well discern the implicit offensive text, exhibiting about 15% accuracy at maximum. This suggests the new challenge provided by this dataset, but seems to be influential as a future direction of OTD. It is also presented that KIR enhances the overall performance of SOTA OTD models, implying the importance of reasoning-based inference in OTD. ", "summary_of_strengths": "- This paper well formulates the implicit OTD as a multi-hop reasoning problem, constructing a dataset to check the capability of reasoning of SOTA OTD models.\n- The experiments are conducted to examine the use of entailment of models as a part of the multi-hop reasoning approach for implicit OTD, showing that such entailment helps the model find an appropriate reasoning chain to detect implicit offensive texts.\n- The authors distribute the built corpus and knowledge base for future research. ", "summary_of_weaknesses": "- The attribute-based approach can be useful if the attribute is given. This limits the application of the proposed approach if there is no attribute given but the text is implicitly offensive.\n- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction. ", "comments,_suggestions_and_typos": "Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig. 1. If other contexts such as 'S1 is fat/poor' is not given, then the conversation between S1 and S2 seems quite natural. The addressee may ask if bookclubs provide free food without offensive intention. If the word of S2 is to be decided as 'implicitly offensive', then one of the reasoning chain, such as 'you are fat/poor', should be provided as a context. If I correctly understood, I recommend the authors to change the example to more clear-cut and non-ambiguous one.\n- I found some issues in the provided example; i) AIR is still written as ASR, and ii) there are some empty chains in the full file. Hope this to be checked in the final release.\nSuggestions - It would be nice if the boundary between explicit and implicit offensive texts is stated clearly. Is it decided as explicit if the statement contains specific terms and offensive? Or specific expression / sentence structure?\n- Please be more specific on the 'Chain of Reasoning' section, especially line 276.\n- Please describe more on MNLI corpus, on the reason why the dataset is utilized in the training of entailment system.\nTypos - TweetEval << two 'L's in line 349 ", "ethical_concerns": "I think it should be described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restrictions. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Won Ik Cho, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 279], [279, 280], [280, 426], [426, 571], [571, 690], [690, 799], [799, 1037], [1037, 1153], [1153, 1297]], "summary_of_strengths": [[0, 160], [160, 416], [416, 498]], "summary_of_weaknesses": [[0, 72], [72, 194], [194, 341]], "comments,_suggestions_and_typos": [[0, 9], [9, 126], [126, 244], [244, 326], [326, 482], [482, 596], [596, 729], [729, 775], [775, 787], [787, 887], [887, 969], [969, 1014], [1014, 1098], [1098, 1217], [1217, 1223], [1223, 1259]], "ethical_concerns": [[0, 157]]}}}]
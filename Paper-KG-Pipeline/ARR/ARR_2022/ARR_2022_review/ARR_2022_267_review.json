[{"rid": "19bfba0c6a677941f5cbc4abb300a092e2dd6b520a5e920d590329c5670d94b8e0829ff2d771f312f449d1c2fff15cb6611890583e047e3f2e9d16025af1ef0b", "reviewer": "Raphael Shu", "report": {"paper_summary": "The authors proposed an improved version of the $\\alpha$-entmax which does not require sorting during computation, which aims to reduce the latency of the operation. The proposed computation method is based on $\\alpha$-RELU, which replaces the normalization variable in $\\alpha$-entmax to a fixed value. Experiments show that the resultant operator is faster than 1.5-entmax on WMT14 En-De and WMT13 En-Ru translation tasks. ", "summary_of_strengths": "- The proposed method shows that even with a potentially unnormalized distribution, it doesn't seem to harm the decoding performance of the generation model - Experiment results show that the proposed method is constantly faster than $\\alpha$-entmax and the performance is not dropping ", "summary_of_weaknesses": "The authors claim that the alpha entmax is slow because it requires sorting. I checked the code of  $\\alpha$-entmax implemented in https://github.com/deep-spin/entmax and find that the top-k approximation is the default option with K=100 as the default setting. In this case, `torch.topk` is called in the code, and only K numbers are sorted. Therefore, if the author position the proposed method as a fast approximation to the original  $\\alpha$-entmax, I think the authors shall perform more systematical comparison with the top-k approximation.\nIn the paper, the only comparison with top-k approximation is Figure 2 (bottom, center). However, it's actually difficult to make a conclusion for comparing performance and speed based on this figure. It seems that the proposed 1.5-ReLU is still faster in the first 20 hours of training, then the gap closes. Also, as a key performance indicator of the proposed method, it is desirable to have the actual numbers. For example, averaged seconds per batch for training and averaged seconds per 1k decoding requests for inference. It will be even better to profile just the computation time of softmax /  $\\alpha$-entmax /  $\\alpha$-ReLU operators.\nI hope to see more detailed comparison with the top-k variant of $\\alpha$-entmax on (1) translation performance (at least WMT14 En-De) and (2) execution speed (can be seconds per 1k batch, will be better if reporting the running time of just the $\\alpha$-entmax/$\\alpha$-ReLU layer). I might change my assessment according to the results.\nAnother concern is on the distribution, in Figure 7, authors showed that the sums are still concentrating to a certain value. However, the deviation is not small according to this figure. Will this impact the correctness of the language model scores (log p)? Some applications are relying on the scores for reranking purposes. ", "comments,_suggestions_and_typos": "- For evaluation, the datasets that the authors use are pretty old datasets. I'm not against using WMT14 En-De, however, WMT13 En-Ru is rare in the research community recently. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Raphael Shu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 166], [166, 304], [304, 425]], "summary_of_strengths": [[0, 157], [157, 286]], "summary_of_weaknesses": [[0, 77], [77, 262], [262, 343], [343, 548], [548, 637], [637, 749], [749, 857], [857, 962], [962, 1076], [1076, 1194], [1194, 1478], [1478, 1533], [1533, 1659], [1659, 1721], [1721, 1792], [1792, 1860]], "comments,_suggestions_and_typos": [[0, 77], [77, 177]]}}}]
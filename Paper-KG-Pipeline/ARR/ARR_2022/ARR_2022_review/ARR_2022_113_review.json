[{"rid": "67378d8e10dc9158f3d5f981f5a4fe8492dd9f8b9a19060d81a5931a482a2e9662f9a413acfb54a018b2e3b4b5cea379b2e4b96137f6fb4342a1a4963f7c3cbd", "reviewer": null, "report": {"paper_summary": "This paper focuses generally on the task of diverse decoding from conditional language generation models. Specifically, they introduce the idea of best-first search, which unlike traditional beam search involves exploring the output space using a depth-first path completion strategy. As this results in many candidate outputs, this work also utilizes path recombination to combine similar hypotheses. The recombination methods generalize previous work to account for merge a hypothesis with candidates from earlier timesteps, and then propagates any merges back through the original lattice; these changes allow for significantly more merges to occur. The authors evaluate the performance of these methods on document summarization and machine translation. ", "summary_of_strengths": "This paper is very well-written, and clearly articulates the issues with not only beam search, but BFS approaches generally. The methods proposed are relatively novel, and show to explore significantly more of the search space than beam search, and explores in a more principal way than random sampling-based methods. The results indicate that using these methods results in significantly more diversity in the output candidates, which based on the higher oracle ROUGE scores, has the potential to improve performance given an additional re-ranking mechanism. This seems like it could be a non-trivial step forward for decoding strategies.\nThe figures are very compelling and help visualize the mechanisms that differentiate the proposed methods from previous work. Figures 11-13 in particular are fascinating visualizations of the different paths, I greatly appreciate the authors including them in the appendix. In addition, the included error analysis is helpful in identifying and motivating future work, potentially on when to reduce the aggressiveness of path merging. ", "summary_of_weaknesses": "The paper makes several significant claims about best-first search and recombination being much more efficient than beam search, but the discussions on actual run-time are lacking. In the appendix the authors note that the best-first search code could be optimized relatively easily, but one question that doesn’t seem to be answered is: compared to the baseline approaches, how much longer does it take to generate outputs, at least as the method is implemented now?\nWhile the potential for significant improvement on quality scores is there due to the significantly higher portion of the search space explored, the authors don’t actually attempt any kind of re-ranking, which leaves an open question: are we able to actually extract the best output sequence from all the generated candidates? Did the authors try any simple re-ranking mechanisms to do this?\nFor evaluating diversity, one drawback of using distinct unigrams and bigrams is that it weights all n-grams that appear the same, which does not take into account the fact that infrequent n-grams the same. Thus, it would be good to also include Ent-n, and entropy-based metric introduced by Zhang et al (2018) that does account for this. ", "comments,_suggestions_and_typos": "One previous decoding method used the combine similar outputs was to over-generate candidates, and then perform post-decoding clustering to group similar candidates together (Ippolito et al., 2019). While the methods proposed in this paper are distinct, these methods should be mentioned in the related work.\nMissing Citations: - Generating informative and diverse conversational responses via adversarial information maximization (Zhang et al., 2018) - Comparison of Diverse Decoding Methods from Conditional Language Models (Ippolito et al., 2019) "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 106], [106, 285], [285, 402], [402, 653], [653, 758]], "summary_of_strengths": [[0, 125], [125, 318], [318, 560], [560, 640], [640, 766], [766, 914], [914, 1075]], "summary_of_weaknesses": [[0, 181], [181, 468], [468, 795], [795, 860], [860, 1067], [1067, 1199]], "comments,_suggestions_and_typos": [[0, 199], [199, 309], [309, 328], [328, 452], [452, 550]]}}}, {"rid": "6b9e6be45c7620d0c5fccd2b2e663d00fe520937a5949233373bb5143bd5ca8c87de9b77877c05428c38eb98660424a76f9424b3c2cd8b1b2286db521bfe5757", "reviewer": "Nan Jiang", "report": {"paper_summary": "This paper proposes to use the Best-first search algorithm to improve the Beam search method for generating diverse sentences. It constructs a lattice graph along with the hypothesis recombination method to shrink the size of the graph. In experiments, the authors compare with several baselines, showing the advantage of the proposed method. ", "summary_of_strengths": "the paper is clearly written and good to follow. the experiments and analysis are impressive. ", "summary_of_weaknesses": "The methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases. ", "comments,_suggestions_and_typos": "- Figure 2, it is not clear about \"merge target\". If possible, you may use a shorter sentence.\n- Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?\n- Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.\n- Line 154 left, \"including that it optimizes for the wrong objective\". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.\n- For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?\n- Line 377, is BFSZIP an existing work? If so, you need to cite their work.  - In figure 5, the y-axis label may use \"Exact Match ratio\" directly.\n- Line 409, could you cite the \"R2\" metric?\n- Appendix A, the authors state \"better model score cannot result in better hypothesis\". You'd better state clearly what idea hypothesis you want. \" a near-optimal model score\" this sentence is unclear to me, could you explain in detail?\n- In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph? "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Nan Jiang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 127], [127, 237], [237, 343]], "summary_of_strengths": [[0, 49], [49, 94]], "summary_of_weaknesses": [[0, 46], [46, 145], [145, 291]], "comments,_suggestions_and_typos": [[0, 50], [50, 95], [95, 159], [159, 242], [242, 275], [275, 389], [389, 461], [461, 499], [499, 539], [539, 629], [629, 767], [767, 807], [807, 843], [843, 844], [844, 914], [914, 958], [958, 1047], [1047, 1107], [1107, 1196], [1196, 1330], [1330, 1362]]}}}, {"rid": "f4d5395d46a0f2f0866fa4410edd21d000d1791f07ef488bea12fe58ae905b9bb44fd8d0b92f1da10bb6456a4dca4cf8ca35d0350654cb42b3a947b8edb49b7f", "reviewer": "Cunliang Kong", "report": {"paper_summary": "This paper proposes a novel method to explore the search space of neural text generation models. The proposed method includes two key components of a modified best-first search and a path recombination mechanism. The authors conduct experiments on text summarization and machine translation tasks. The experiment results show that the proposed method generates massive-scale candidate sentences and obtain comparable or even better metric scores. ", "summary_of_strengths": "- The description of the proposed approach is clear and easy to follow.\n- The paper presents a well-rounded set of experiments on text summarization and machine translation.\n- The authors provide a lot of details in the appendix, which helps the reproducibility. ", "summary_of_weaknesses": "- Although BFS is briefly introduced in Section 3, it's still uneasy to understand for people who have not studied the problem. More explanation is preferable. ", "comments,_suggestions_and_typos": "- Algorithm 1, line 11: the function s(·) should accept a single argument according to line 198.\n- Figure 6: the font size is a little bit small. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Cunliang Kong, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 97], [97, 213], [213, 298], [298, 447]], "summary_of_strengths": [[0, 72], [72, 174], [174, 263]], "summary_of_weaknesses": [[0, 128], [128, 160]], "comments,_suggestions_and_typos": [[0, 97], [97, 146]]}}}]
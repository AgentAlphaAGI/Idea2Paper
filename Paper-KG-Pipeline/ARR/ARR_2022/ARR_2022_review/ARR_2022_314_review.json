[{"rid": "da9f859a7a13fea1941d46d3cbfe47c7dfbc6325bf93b3f0117dada3fb06c50dc3c9ca15c4ad8d7ff3525640c9e43c457820bfa459301a5e25a2fc32aca79ab7", "reviewer": null, "report": {"paper_summary": "To analyze the attribution from a token to another token in transformer networks more accurately than previous studies, the authors proposed a new method named GlobEnc. Locally (for each layer), they extended the method of Kobayashi et al., 2021 by considering the contribution of residual connection #2 and layer normalization #2 (Figure 1). Globally (for the whole network), they corrected for the heuristic in the method of Abnar and Zuidema 2020.\nExperimental results showed that the proposed method is a faithful index that correlates well with the gradient x input method (Kindermans et al., 2016). In addition, they confirmed that the vector norm-based method (Kobayashi et al. 2020) and the consideration of residual connection (Kobayashi et al., 2021), which were proposed as local analysis methods, contribute to global faithfulness. They also pointed out that considering only layer normalization #1 (Kobayashi et al., 2021) is insufficient, and layer normalization #2 should be considered simultaneously. ", "summary_of_strengths": "1. Transformer networks are now a fundamental tool used not only in NLP but also in a wide range of data sciences, and improving analysis methods is an important research direction. \n1. Well written and easy to understand 1. Clearly mentioned the connections to related literatures 1. Thorough comparison with abundant previous methods in Experiments 1. Provide the insightful result that two Layer normalization in the layer counteract each other. ", "summary_of_weaknesses": "1. The models addressed in the experiments are limited; only the (fine-tuned) BERT-base models are used. It's not clear how generalizable the findings are to other transformer models (larger or smaller models; variants such as RoBERTa). \n1. No justifications for the way to measure the faithfulness of the attribution methods. In experiments, the faithfulness of the attribution methods are measured by calculating the correlation with the gradient x input method (Kindermans et al., 2016); however, it has been pointed out that this method can only capture the global behavior of the model in a very rough way (Sundararajan et al. ICML 2017). I am wondering that if the authors use a more accurate method, such as integrated gradient (Sundararajan et al.), will they get the same results? \n1. Insufficient description about qualitative analysis. How were two input sentences for qualitative analysis (Sec 4.5.5, Figure 1) determined? At least, the authors should mention whether these examples occurred during the fine-tuning process. Furthermore, the attribution values in Fig. 1 are 0 to 1. Did you normalize the vector norms? Also, does the attribution map for the input examples that failed to be classified also work well? \n1. Some points are slightly over-claimed. The proposed method does not consider feed-forward layers; however, some text is described as if the proposed method considers all components in transformer networks. \n    - Title: \"Incorporating the whole encoder layer\"     - l.08:  \"incorporates all the components\" ", "comments,_suggestions_and_typos": "- The authors can mention Geva et al. (EMNLP 2021) to justify excluding the Feed-Forward layer.\n- Although the mixing ratio of the methods with Fixed Residual (W_ {FixedRes} and N_ {FixedRes}) is explained as 0.5, it is actually lower than 0.5 because attention also has the effect of preserving. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 169], [169, 343], [343, 451], [451, 605], [605, 844], [844, 1017]], "summary_of_strengths": [[0, 3], [3, 182], [182, 186], [186, 225], [225, 285], [285, 354], [354, 449]], "summary_of_weaknesses": [[0, 3], [3, 105], [105, 237], [237, 241], [241, 327], [327, 644], [644, 790], [790, 794], [794, 847], [847, 935], [935, 1036], [1036, 1094], [1094, 1130], [1130, 1229], [1229, 1233], [1233, 1272], [1272, 1439], [1439, 1540]], "comments,_suggestions_and_typos": [[0, 96], [96, 297]]}}}, {"rid": "62abb48743e2d28d289c62e00b5b3ad39ac3e34f44188b4f43b5a11e9b37cab1fc4111bf727cc08aaf0dda32b10574fa369876a26e6defafe157b9391a0f99d4", "reviewer": null, "report": {"paper_summary": "The paper introduces a token attribution analysis method for transformer based models building upon previous norm-based and rollout aggregation methods. The proposed approach incorporates all the components of the encoder block except feed forward layers to compute contribution of each token in a sentence. The work provides a detailed quantitative analysis of each component's role, layer-wise evaluation and achieves better results than previous methods on three classification datasets. ", "summary_of_strengths": "1. Paper is well written and easy to follow in general.\n2. Related work is adequately covered.\n3. The background section builds the analysis method quite well citing previous methods wherever necessary.\n4. The major strength and novelty of the paper lies in the detailed analysis, analyzing role of each component and comparison with weight-based, norm-based and gradient based methods. Sub-sections in section 4.5 can be quite relevant for the interpretability research area in general and relevant downstream application areas. ", "summary_of_weaknesses": "1. Although the work is important and detailed, from the novelty perspective, it is an extension of norm-based and rollout aggregation methods to another set of residual connections and norm layer in the encoder block. Not a strong weakness, as the work makes a detailed qualitative and quantitative analysis, roles of each component, which is a novelty in its own right.\n2. The impact of the work would be more strengthened with the proposed approach's (local and global) applicability to tasks other than classification like question answering, textual similarity, etc. ( Like in the previous work, Kobayashi et al. (2020)) ", "comments,_suggestions_and_typos": "1. For equations 12 and 13, authors assume equal contribution from the residual connection and multi-head attention. However, in previous work by Kobayashi et al. (2021), it is observed and revealed that residual connections have a huge impact compared to mixing (attention). This assumption seems to be the opposite of the observations made previously. What exactly is the reason for that, for simplicity (like assumptions made by Abnar and Zuidema (2020))?\n2. At the beginning of the paper, including the abstract and list of contributions, the claim about the components involved is slightly inconsistent with the rest. For instance, the 8th line in abstract is \"incorporates all components\", line 73 also says the \"whole encoder\", but on further reading, FFN (Feed forward layers) is omitted from the framework. This needs to be framed (rephrased) better in the beginning to provide a clearer picture.\n3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.\n4. Will the code be made publicly available with an inference script? It's better to state it in the submission, as it helps in making an accurate judgement that the code will be useful for further research. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 153], [153, 308], [308, 491]], "summary_of_strengths": [[0, 3], [3, 56], [56, 59], [59, 95], [95, 98], [98, 203], [203, 206], [206, 387], [387, 530]], "summary_of_weaknesses": [[0, 3], [3, 219], [219, 372], [372, 375], [375, 574], [574, 626]], "comments,_suggestions_and_typos": [[0, 3], [3, 117], [117, 276], [276, 354], [354, 459], [459, 462], [462, 623], [623, 816], [816, 906], [906, 909], [909, 1121], [1121, 1239], [1239, 1318], [1318, 1321], [1321, 1388], [1388, 1526]]}}}, {"rid": "29d68183bfd749eb98ca25d8f3275d06762a252d2ae2c8714416e61aa6c8f672e5f150c66a611c28326426aea82ea5c9c3c9e04c94b37970ae669deabeb0a5bc", "reviewer": "Hyunsoo Cho", "report": {"paper_summary": "Motivated by the assumption that all the components in the encoder block, in general, are meaningful for token attribution analysis, this work proposes a GlobEnc, which incorporates all the components in the encoder block in the model. ", "summary_of_strengths": "- The proposed method, GlobEnc, expands the scope of analysis from attention block in Transformers to the whole encoder.\n- This paper shares intriguing insights from various experiments.\n- Writing quality is high and validates the proposed method on many different criteria to investigate the role of each component in the encoder. ", "summary_of_weaknesses": "- In token attribution analysis task, most researchers will intuitively agree that self-attention plays a primary role in Transformer architecture. However, why is the sub-network on top of the self-attention block important, and what role the authors expect from them are not well-motivated. A qualitative experiment comparing the token attribution analysis with and without subsequent sub-network (i.e., FFN, residual connection #2, and output LN) would be helpful to elucidate the role of remaining components other than self-attention block.\n- The work appears to be incremental: As mentioned in the paper, Kobayashi et al. (2020, 2021) proposed the analysis method leveraging attention block. While the proposed method extends the existing approach to the entire encoder block, GlobEnc seems to reuse the tactics used in the previous method without any distinctive improvement. ", "comments,_suggestions_and_typos": "- In Equation 6, it would be better to note what γ and ⊙ (element-wise product) represent.\n- In Figure 1, it would be better to compare the attribution map of GlobEnc with the attribution map of other comparable methods (e.g., N_{res}). "}, "scores": {"overall": "2.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Hyunsoo Cho, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 236]], "summary_of_strengths": [[0, 121], [121, 187], [187, 332]], "summary_of_weaknesses": [[0, 148], [148, 293], [293, 546], [546, 698], [698, 883]], "comments,_suggestions_and_typos": [[0, 91], [91, 237]]}}}]
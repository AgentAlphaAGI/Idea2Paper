[{"rid": "0b700376f03b1c3df377fe0a191027d6dff597add85185cdf402bd12f05c6e6edbebdb6cfd2b502e3cb162776458a21933f543ef804eb6790056882b5e62d7ac", "reviewer": null, "report": {"paper_summary": "This paper presents a cross-lingual information retrieval approach using knowledge distillation. The underlying model is ColBERT with XLM-R as the pretained language model. The approach makes use of a teacher model based on query translation and monolingual IR in English. The student model is trained with two objectives. One is an IR objective to match the teacher model's query-passage relevance predictions. The second objective is to learn a representation of the non-english text that most closely matches the teacher's representation at the token level. This relies on a cross lingual token alignment based on greedily aligning tokens with the highest cosine similarity. The authors do abalations of their two objectives and find they are both useful and also compare against fine-tuning ColBERT directly on cross lingual data. On the XOR-TyDi leaderboard, one of this paper's models is the current best. ", "summary_of_strengths": "- Novel approach that does cross lingual IR where the resulting model does not use MT  - New cross lingual token alignment based on multilingual pretrained langauge model - Good abalations and comparisons with fine-tuning on cross lingual data - Strong performance on zero-shot settings as well - The paper has best performance on XOR-TyDi ", "summary_of_weaknesses": "No major weaknesses ", "comments,_suggestions_and_typos": "line 62-64 asks whether a high performance CLIR model can be trained that can be operate without having to rely on MT. But the training process still relies on MT, so this approach does still rely on MT, right? I guess the point is that it only relies on MT at training time and not at evaluation / inference. It might be possible to try to make this clearer. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "Maybe", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 97], [97, 173], [173, 273], [273, 323], [323, 412], [412, 561], [561, 678], [678, 835], [835, 912]], "summary_of_strengths": [[0, 87], [87, 171], [171, 244], [244, 295], [295, 340]], "summary_of_weaknesses": [[0, 20]], "comments,_suggestions_and_typos": [[0, 119], [119, 211], [211, 310], [310, 360]]}}}, {"rid": "a62cf505dc1353e9c6578ecc04f8152b8880014efe8fd6ce2bd5608d2524e27d49d154f12808007e18ad3a905ea956a1475956bfb8a221d51972d830addeb8bd", "reviewer": null, "report": {"paper_summary": "The authors describe a novel approach to Cross-Language Information Retrieval (CLIR). They propose a single-model-based approach. That is, in contrast with classical MT-based CLIR systems, where we can clearly distinguish two phases/stages/subprocesses involved (translation + monolingual retrieval), this is now solved altogether. For simplicity they have used English as the target language for their experiments. The results obtained show state-of-the-art performane according to their metrics.\nTheir proposal takes as basis a ColBERT IR architecture (with proved state-of-the-art performance) and a pre-trained multilingual masked language model (XML-RoBERTa in their experiments). The idea consists on using knowledge distillation (KD) to teach/fine-tune the underlying multilingual model to \"imitate\" the behavior (and, theoretically, the performance) of a monolingual IR system in the target language, even when the CLIR system is taking as input queries in source language, not the target language. This fine-tuning/training process is done in two phases.  A first KD process is focused on teaching the CLIR system to select and rank relevant documents in the same way as the target-language monolingual system. Ideally, the resulting fine-tuned [cross-language] model should be able to obtain the same output results as a monolingual model. In other words, we are teaching the system how to solve the retrieval part of the process. For this purpose, a regular CLIR dataset is needed (parallel queries and their corresponding relevant documents).\nThe second KD process is focused on teaching the CLIR system to encode (internally) a source-language text (e.g. the input query to the CLIR system) in the same way as the target-language monolingual system. Ideally, the resulting fine-tuned [cross-language] model should be able to generate the same internal representation of a text as the monolingual model even when they are taking as input texts (queries) in different languages (but being mutual translations, of course). In other words, we are teaching the system how to solve the translation part of the process. For this purpose, a parallel corpora is needed.  Logically, there are still two issues involved in the joint CLIR process (translation + retrieval), but they are now solved at a time after having teached the model how to do it.   The XOR-TyDi CLIR dataset (7 languages + English) is used for in-domain experiments, and the MKQA dataset (5 common languages with the previous one + English) is used for zero-shot experiments. For simplicity, English is always used as the target language. The following baselines, in increasing performance order, are used: (1) [bottom] ColBERT IR model + XML-RoBERTa multilingual masked language model, fine-tuned with English data       (2) ColBERT IR model + XML-RoBERTa multilingual masked language model, fine-tuned with both English and cross-language data  (3) [top] classical approach: MT + ColBERT IR model, fine-tuned with English data  The results obtained are clearly positive, notably outperforming the lower baselines and approaching the top monolingual state-of-the-art approach. An ablation study is also performed, proving that both KD processes actually contribute to the results. ", "summary_of_strengths": "- Novelty.\n- State-of-the-art results.\n- Quite complete experiments (given the length of the article).\n- Well-written. ", "summary_of_weaknesses": "- I miss the use of more metrics - Given the length of the main article, 5 pages, 3 pages of Appendix may be a bit excessive. ", "comments,_suggestions_and_typos": "- When reading the first part of the article, the authors tend to associate the term \"CLIR\" mainly with theirs and similar approaches, in contrast with the classical MT+monolingual IR approach. However, both of them are CLIR systems and this creates some confusion in the reader.  - Sect. 1 \"Introduction\" contains too many redundancies with respect to the rest of the paper. Taking into account the length of the paper and the Appendix, a reduction of Sect.1 through the removal of unnecessary redundancies would provide space enough to move part of the appendix back to tha main paper, thus making it more complete and self-contained.\n- l.166-181: No data are given here about the parallel corpus used for the term-level alignment. This is explained later, in the appendix, but no clue is given to the reader at this point. The authors should move that part back to the main paper or, at least, indicate that those data are provided in the Appendix. Apart from that, nothing is said about the size of the resultant dictionary.\n- Figure 2: It has not been useful at all, at least for me.\n- l.182-195: According to this, it seems that the resulting CLIR system proposed by the authors would be also able to work not only with queries in the source-language (i.e. non-English), but also with queries in the target-language (i.e. English). Is this the case or just a confusion?\n- Table 1: The reader must be able to easily identify which configuration described in the text corresponds with each raw, and this is not the case. This must be corrected.\n- I have some concerns about the (only) use of R@5kt as metric. Why no Precision-based metric is used? This information would be very useful to have a better view of the performance of the approach. Moreover, which criteria is used to count the tokens? Do you join the text of all the snippets retrieved and, then, you count 5000 tokens?\n- l.266-268: The authors are surprised about the fact that the contribution of the KD with parallel corpus is greater than that of IR triples. I think it makes much sense since you're using a Recall-based metric. Since it is a Recall-based metric, the important thing is that you retrieve the document, regardless of its ranking. In a CLIR context, a more accurate translation (e.g. being able to translate one more term of the input query) allows new matchings, thus improving recall, regardless of the ranking position where the new documents are retrieved.\n- References: They contain the usual missing-uppercases errors.\n- footnotes 3,4: When a reference to a footnote is inserted immediately before a punctuation mark, they should switch positions; e.g. \"(...) OPUS^3. The (...)\" --> \"(...) OPUS.^3 The (...)\"  - l.479-480: How many cases have been analyzed? "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 86], [86, 130], [130, 332], [332, 416], [416, 498], [498, 686], [686, 1007], [1007, 1064], [1064, 1065], [1065, 1220], [1220, 1350], [1350, 1441], [1441, 1555], [1555, 1763], [1763, 2033], [2033, 2126], [2126, 2174], [2174, 2175], [2175, 2354], [2354, 2356], [2356, 2550], [2550, 2613], [2613, 3004], [3004, 3152], [3152, 3256]], "summary_of_strengths": [[0, 11], [11, 39], [39, 103], [103, 119]], "summary_of_weaknesses": [[0, 33], [33, 126]], "comments,_suggestions_and_typos": [[0, 194], [194, 280], [280, 281], [281, 289], [289, 376], [376, 637], [637, 734], [734, 826], [826, 952], [952, 1029], [1029, 1089], [1089, 1338], [1338, 1376], [1376, 1525], [1525, 1549], [1549, 1613], [1613, 1652], [1652, 1748], [1748, 1802], [1802, 1887], [1887, 2030], [2030, 2100], [2100, 2217], [2217, 2447], [2447, 2511], [2511, 2660], [2660, 2702], [2702, 2750]]}}}]
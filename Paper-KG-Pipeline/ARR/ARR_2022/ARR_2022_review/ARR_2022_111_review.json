[{"rid": "4cc4700c648f621fe89d303cbb1db1764efbd3c86208ed01753a8c3f7a7f66b853ffb2f025681819abfb30354a784bb48fcf70f5b99df7f0adbe05153934bbe5", "reviewer": null, "report": {"paper_summary": "This paper presents a method for generating word-level adversarial attacks for structured prediction tasks. Such attacks are modified examples that differ by individual words compared to the original examples, yet receive a substantially different prediction when used as input to a trained predictor.\nThe main contribution of this paper is SHARP, a method that relies on search-based optimization to modify examples while preserving a sentence's fluency and meaning. In a series of extensive experimentation, the authors evaluate the efficacy of this approach and demonstrate its usefulness and efficiency in generating such attacks and in guarding against them.\nWithin the context of adversarial attacks this is an interesting paper that clearly demonstrates a method for generating such attacks. However, I remain unconvinced of the usefulness of such attacks for truly understanding the vulnerability of trained predictors. Adversarial attacks perturb examples such that models fail on the modified examples, but understanding what causes models to fail is a causal problem that remains unanswered in this paper. ", "summary_of_strengths": "In the context of generating adversarial attacks, this paper successfully addresses the problem by adapting several useful search-based methods for its needs.  The authors provide an extremely detailed evaluation and experimental analysis, and clearly demonstrate the efficacy of their approach. ", "summary_of_weaknesses": "While I do see the benefits of generating adversarial attacks and the advantages of building tools that guard against them, I find this line of work, and this paper as part of it, problematic.  In NLP, there is a growing interest in stress-tests that measure model performance in counterfactual scenarios (see CheckList, Counterfactual Invariance to stress tests, etc.). As opposed to adversarial attacks, such papers try to design counterfactual examples that we know we want to be robust to, and carefully measure model sensitivity to them. As this is a causal problem (estimating the effect of a counterfactual on an observed outcome), such methods examine performance with respect to modifications that we can reason about (i.e. flipping gender/race etc.). Reading this paper, I’m still not convinced that we should put such emphasis on examples that are modified based on word-level perturbations generated by a correlation-based search method. As such, I believe that training models to be robust to such attacks is somewhat tangential. Generally speaking, do we have a good reason to believe that these adversarial attacks are realistic? What do we learn from them on the model and its robustness and reliability?\nIf I put aside the power of adversarial attacks in uncovering model vulnerability, I do find this paper interesting and useful within this context. I suggest authors better address the causal nature of generating counterfactual examples, and try to analyze the sensitivity of their method to the type of attacks generated by SHARP. In doing that, this paper can certainly contribute to the growing literature on model vulnerability and robustness. ", "comments,_suggestions_and_typos": "The paper is generally well-written, but may benefit from an additional round of editing.  Some typos for example:  Line 588 - “investigate the sensitivity“ Line 590 -  “as an search problem” Line 595 - “guarantees a better“ "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 108], [108, 302], [302, 468], [468, 664], [664, 799], [799, 928], [928, 1117]], "summary_of_strengths": [[0, 159], [159, 160], [160, 296]], "summary_of_weaknesses": [[0, 193], [193, 194], [194, 371], [371, 543], [543, 761], [761, 950], [950, 1043], [1043, 1145], [1145, 1221], [1221, 1369], [1369, 1553], [1553, 1669]], "comments,_suggestions_and_typos": [[0, 90], [90, 91], [91, 225]]}}}, {"rid": "74619dd7e1e302942143426ecf3f670db7807713703baf890d1222b3f73ade64d6afbad41f6020c16c572ff97aacdb5818af1b38af645648830b40b3fbab9b8d", "reviewer": "Xiaoqing Zheng", "report": {"paper_summary": "The authors propose a black-box adversarial attack method for structured prediction tasks such as POS-tagging and dependency parsing. The authors formulate the adversarial attack under the black-box setting as a combinatorial optimization problem and proposed three search methods (beam search, Metropolis-Hastings sampling, and their combination) to generate textual adversarial examples. They advocate preserving the meaning and fluency of the original text when generating adversarial examples. However, it seems that the adversarial examples against dependency parsing models only need to preserve the same syntactic structure as the original ones, and the constraint on their similarity in semantic properties can be relaxed (Zheng et al., 2020). Besides, the novelty of this study is limited. The objective function used is similar to that of many previous studies. They claim that their attack method does not need to know the training dataset used by the victim models. However, it seems that they train the reference models on the same dataset used by the victim model (at least for the dependency parsing task). They did not compare their search strategy to others, such as genetic algorithm (Alzantot et al., 2018) both in the performance and computational cost.\nIn their revised version (2nd round), they only added a paragraph in the introduction (Line 54-75) and an experiment to investigate “the impact of metric for reference models” (Line 471-496). In the introduction, the authors claimed that structured prediction models are more vulnerable to adversarial examples than text classifiers. In the experiment, they introduced a metric, called OIoU (Opposite Intersection over Union), to evaluate “the diversity degree of reference parsers.” However, the questions raised in the 1st sound still have not been well answered. ", "summary_of_strengths": "The authors proposed a method that combines Metropolis-Hastings (MH) sample and beam search to generate textual adversarial examples, and their experimental results show that the proposed method can better preserve the meaning and fluency as the original texts. ", "summary_of_weaknesses": "The novelty of this study is limited. The objective function used is similar to that of many previous studies. \nThey claim that their attack method does not need to know the training dataset used by the victim models. However, they follow the work of Han et al. (2020) and train the reference models on the same dataset used by the victim model (at least for the dependency parsing task). \nThey did not compare their search strategy to others, such as genetic algorithm (Alzantot et al., 2018). \nIt seems that their method requires much more computational cost to generate adversarial examples, which makes the proposed method harder to be used in practice.\nReference: Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2018. ", "comments,_suggestions_and_typos": "N/A ", "ethical_concerns": "N/A "}, "scores": {"overall": "2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Xiaoqing Zheng, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 134], [134, 390], [390, 498], [498, 752], [752, 799], [799, 872], [872, 978], [978, 1122], [1122, 1274], [1274, 1466], [1466, 1608], [1608, 1758], [1758, 1840]], "summary_of_strengths": [[0, 262]], "summary_of_weaknesses": [[0, 38], [38, 111], [111, 218], [218, 389], [389, 495], [495, 658], [658, 766], [766, 816], [816, 908]], "comments,_suggestions_and_typos": [[0, 4]], "ethical_concerns": [[0, 4]]}}}]
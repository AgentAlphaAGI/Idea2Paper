[{"rid": "a42c480628723affbe6a3d6044ee9416d717cb6c2075135233c3e2960e2cc480a0cd0b4faf5d44572bb15d6197bced37707077a2c1ced77ebaa05dd8c2b5e70a", "reviewer": null, "report": {"paper_summary": "The paper proposes 6 test corpora for vision and language captioning systems that target specific competency. For each competency, examples are generated semi-automatically from existing language + vision tasks, such QA in V7W, and are created in a FOIL style, where one example correctly describes the image, while another example makes a minimal change to caption and does not describe the image. Systems are then challenged to prefer captions that correctly identify the image. The competencies tested include existence, plurality/counting, spatial reasoning (via prepositions), situational knowledge (via imSitu data), and coreference. The paper evaluates several recent pre-training based models, finding that many fail at their challenges, and that the multi-task model 12-in-1, works best. ", "summary_of_strengths": "Proposes a fairly diverse set of challenges that could be a useful diagnostic going forward.\nThe paper evaluates currently relevant model on the diagnostic, establishing clear baselines for their dataset moving forward.  Because the paper encompasses essentially 5 independent datasets, it a very substantial body of work. It seems larger than a standard paper. ", "summary_of_weaknesses": "(being a previous reviewer R BWRg, I will respond to previously identified weakness) I still find the argument of what is and is not included in the diagnostic unclear. In many ways, this seems like a case of a subset of competencies that we have enough visual annotations to semi-automatically create data for. In my opinion, the paper should steer away from making arguments that these examples are deeply linguistic, beyond, involving nouns, counting, verbs, and coreference. As such, I find the title and some of the introduction over-claiming, but, this is really a matter of opinion, resting on what exactly 'linguistic' means.\nThe main body of the paper still lacks examples but I appreciate their inclusion in the appendix. It's very hard to imagine the foils from the descriptions alone. This may be asking a lot, but the paper would be significantly improved if the last page were almost entirely made of examples from the appendix. This is a CVPR style of presentation, and would require significant text trimming.  The examples were good overall, but the co-ref part of the benchmark stands out. It is essentially a QA task, which isn't really compatible with just caption based training that most of the evaluated most are setup to do (with the exception of 12-1). This isn't an issue, because its not really the benchmark's problem, but I am not sure the format of the foil is that sensible. I suspect this will be the least used of the new foils, but I don't have a concrete proposal how it could be improved to really be a captioning task. ", "comments,_suggestions_and_typos": "- "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 110], [110, 399], [399, 481], [481, 640], [640, 797]], "summary_of_strengths": [[0, 93], [93, 220], [220, 221], [221, 323], [323, 362]], "summary_of_weaknesses": [[0, 85], [85, 169], [169, 312], [312, 479], [479, 634], [634, 732], [732, 797], [797, 943], [943, 1026], [1026, 1027], [1027, 1108], [1108, 1278], [1278, 1406], [1406, 1556]], "comments,_suggestions_and_typos": [[0, 2]]}}}]
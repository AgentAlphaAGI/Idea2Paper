[{"rid": "3398ca971c1ee00e7f56db6a5d9069a561805ba4bd33e4cc84ef6a4c1acf331a4d8c1a7e53ebf415d32cbefe4396b1808e031ddcef812a37768f40bde058252f", "reviewer": null, "report": {"paper_summary": "This well-written paper presents \"DIBIMT\", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics. The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems. ", "summary_of_strengths": "-- thorough experimentation -- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation -- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system ", "summary_of_weaknesses": "-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)  -- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes ", "comments,_suggestions_and_typos": "-- some of the formal notations introduced in chapter 3 would probably be easier to understand if put in words -- but I realize that that would make the paper yet longer again "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 194], [194, 394]], "summary_of_strengths": [[0, 307]], "summary_of_weaknesses": [[0, 279]], "comments,_suggestions_and_typos": [[0, 176]]}}}, {"rid": "eb30dde3dc47f3562b84e87a5a5a99053ed6132d71aff1869b3037d09d9d40f79317d3c626808a078709460de2c049cbe0aa99eee589995016a705f6f9d40dd4", "reviewer": null, "report": {"paper_summary": "This paper presents DiBiMT, a new test suite for semantic biases in machine translation. It is modelled on previous test suites such as ContraWSD and MuCoW, but combines manually annotated and curated data with decent multilingual coverage. The dataset creation is explained in detail, and it is then applied to evaluate five machine translation tools, with an extensive analysis of the results. ", "summary_of_strengths": "- The paper introduces a useful dataset for MT evaluation.\n- The paper provides an extensive analysis of existing MT systems on the basis of the dataset. ", "summary_of_weaknesses": "- Some design choices could have been justified in more detail and explained with more examples - The formalization is hard to read at times, with multiple Greek letters and subscripts for somewhat easy-to-grasp concepts - Multilingual coverage could of course be better, but the current limitation is understandable and acceptable given the large amount of manual work involved ", "comments,_suggestions_and_typos": "- L100: Such > This - L182-188: an example would be welcome - §3.2.1: I would have liked to see a discussion on the properties of these examples. Are they shorter/longer than the sentences seen in the average MT training corpora? Are they similar in style/genre/domain? Are these invented examples or are they taken from e.g. news sources? How big is the risk of finding these sentences in the training corpora?\n- L232: why not 2? I don't think that lemmas with only 2 senses are too easy... - L265-268: I would move this paragraph to §3.3.2. The current order suggests that §3.3.1 is done manually as well.\n- L293: Do I understand correctly that as soon as a sentence fails in one of the 5 languages it is discarded? Doesn't this place too harsh of a restriction on sentence selection? It probably doesn't hurt if the number of sentences is not exactly identical for all language pairs, or does it?\n- L349-353: What is the connection of the Isabelle et al. paper with DeepL? How certain is it that the Johnson et al. paper reliably depicts Google's current production system?\n- Figure 2: I would find it more intuitive to report the MISS percentages in a table, analogously to Table 3. Do the MISS percentages correlate with the accuracy figures? Or does it happen that systems with low MISS percentages are penalized for it by low accuracy figures?\n- Figure 3: If the red bars are parts of the grey bars, you could stack them instead of having two separate bars.\n- Table 4 and 5 would be more readable if they were split into two tables each, to have one table per measure. E.g. first put the 8 SFII columns and then the 8 SPDI columns rather than alternating between them.\n- §4.5: I didn't understand this section. An example could help.\n- §4.7: I don't understand why and how you need manual annotation here. The original dataset contains source sentences and good+bad target words. So you should be able to retrieve a translation from your five systems, change the target word, and have a quick manual check for agreement etc. Why do you need to translate the entire sentence manually?\n- Table 7: no need for decimal places if you only have 100 examples - Do you have an idea/conjecture why DeepL is so much better? Is this purely a data curation issue, or could this be due to some architectural changes? It's of course quite frustrating to see that somebody has basically solved the problem but doesn't say how :) - References: Please make sure that abbreviations and language names are capitalized.\n- Appendix: What does \"Back to Model-specific Analyses.\" mean?\n- Appendix: Figure 5 has a column \"%mistakes\", but the other figures have \"Accuracy\". Does this mean that the mean accuracy of DeepL is actually 40% instead of 60%? Please check.\n- Appendix, Table 2: What does the slash in Russian and Chinese mean? Is there no second lexicalization? Why not just leave that line empty? "}, "scores": {"overall": "5 = Top-Notch: This paper has great merit, and easily warrants acceptance in a *ACL top-tier venue.", "best_paper": "Maybe", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 89], [89, 241], [241, 396]], "summary_of_strengths": [[0, 59], [59, 154]], "summary_of_weaknesses": [[0, 96], [96, 221], [221, 379]], "comments,_suggestions_and_typos": [[0, 20], [20, 60], [60, 146], [146, 230], [230, 270], [270, 340], [340, 412], [412, 431], [431, 492], [492, 543], [543, 608], [608, 718], [718, 787], [787, 900], [900, 976], [976, 1077], [1077, 1187], [1187, 1248], [1248, 1351], [1351, 1465], [1465, 1576], [1576, 1676], [1676, 1718], [1718, 1741], [1741, 1813], [1813, 1887], [1887, 2032], [2032, 2091], [2091, 2159], [2159, 2221], [2221, 2311], [2311, 2421], [2421, 2507], [2507, 2564], [2564, 2570], [2570, 2656], [2656, 2735], [2735, 2749], [2749, 2819], [2819, 2854], [2854, 2890]]}}}, {"rid": "db14eda600a57c45529a8ae7db67ef709b17efd9bc501459d16886267bb64db205b07db81311c17bd4e2bec40a9d83d6cff10898a7f5016d5b2799e8ccb4c73b", "reviewer": "Irina Temnikova", "report": {"paper_summary": "This paper presents a completely manually curated evaluation benchmark for semantic biases in Machine Translation for 5 language pairs. 7 SOTA MT commercial and non-commercial systems are tested on the testbed and detailed analysis of the results is given. The paper also presents 4 novel metrics for the task. The analysis includes details on how challenging the dataset is, and the impact of the encoder and decoder. The benchmark will be released. ", "summary_of_strengths": "- the benchmarking testbed for several major language pairs is a useful contribution for the research community - the paper has presented several well-motivated experiments - testing with DeepL Translator and Google Translate provides results, useful for the translation community.\n- the better results of DeepL Translator are interesting for the translation community. ", "summary_of_weaknesses": "1. Although the translators may be highly professional, different translators usually have different choices, so producing annotations by using 1 translator per language does not seem enough. \n2. Also no annotation training details and guidelines are shared, and this is important as professional translators usually have issues with such kind of non-standard tasks. No information about the tool used. \n3. It would have been good to have the number of items per language pair when providing the statistics of the annotated dataset. ", "comments,_suggestions_and_typos": "The example on the first page sounds like figurative use of the word \"to march\". This is an important aspect of mismatched translations. ", "ethical_concerns": "no issues "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Irina Temnikova, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 136], [136, 257], [257, 311], [311, 419], [419, 451]], "summary_of_strengths": [[0, 112], [112, 173], [173, 282], [282, 370]], "summary_of_weaknesses": [[0, 3], [3, 192], [192, 196], [196, 367], [367, 403], [403, 407], [407, 533]], "comments,_suggestions_and_typos": [[0, 81], [81, 137]], "ethical_concerns": [[0, 10]]}}}]
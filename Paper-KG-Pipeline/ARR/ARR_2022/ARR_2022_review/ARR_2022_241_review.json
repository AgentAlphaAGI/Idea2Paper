[{"rid": "38dbfe45b2ac09b26e98586f3703d45d0018c0b3041314a406292dcc678cd6d96a571c33fc968e3d99ddfe224c2856a096dc6dbc7a6ae54465912a5f4bcdb9e7", "reviewer": "Xinjian Li", "report": {"paper_summary": "This work evaluates 4 different ASR models under the single-speaker endangered languages scenario.  - The 4 models are HMM-GMM, Persephone (LSTM), conformer/LF-MMI, and the pretrained wav2vec 2.0 model. The first 3 are trained from scratch and the last one is fine-tuned.\n- Each model is evaluated using an open-sourced pipeline authors developed, the evaluation is done extensively across 1 public dataset and 1 private dataset (11 languages in total) - Authors demonstrate that fine-tuning the pretrained wav2vec model gives significantly better performance except for 1 language.\n- Authors also investigate the performance of using different sizes of the training sets: they show that dropping training set under 99 mins degrade performance significantly. They conclude larger dataset than 99 mins could yield PER below 10% and be useful for language documentation. ", "summary_of_strengths": "- The scenario investigated is indeed very useful as many endangered languages corpus is usually a single-speaker corpus. The conclusion that we might want a larger corpus than 99 min to train a model also is very practical.\n- The pipeline authors have open-sourced on GitHub seem to be well-documented and easy to reproduce - Authors also consider the training time/hardware constraints for each model ", "summary_of_weaknesses": "- the main contribution of fine-tuning a multilingual wav2vec model is not very novel.  - If I understand correctly, only the wav2vec is fine-tuned but the other 3 models are trained from scratch, therefore the comparison across the 4 models might not be very meaningful as it is easy to guess that the fine-tuned one would perform best.\n- detailed comparison and analysis across models are not shown (e.g: what errors does each model make? is there any particular error trend made by each model?) ", "comments,_suggestions_and_typos": "- One point I would like to read more about is why the srs language fails on the XLSR (wav2vec) model, authors mention this is due to an unusually fast convergence but does not explain why this happens. Is it due to the quality of srs dataset or the XLSR model? Shedding more light on this might give us more hints on when we should use wav2vec and when we should refrain from using it.\n- As mentioned in the previous section, it would be interesting if authors can discuss more details of each model (e.g: what error each model make and when should we use each model) "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Xinjian Li, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 99], [99, 100], [100, 203], [203, 272], [272, 453], [453, 583], [583, 759], [759, 869]], "summary_of_strengths": [[0, 122], [122, 225], [225, 325], [325, 403]], "summary_of_weaknesses": [[0, 87], [87, 88], [88, 338], [338, 441], [441, 498]], "comments,_suggestions_and_typos": [[0, 203], [203, 262], [262, 387], [387, 569]]}}}, {"rid": "a94d622987e8d405f19ec3fcc83fb8245e20e1c0312b2bd8ba88bbdba88ca2da9d91c02b7f53d07dcd47ca998d2280e332a25de3b4446b07be3ab759a384de6a", "reviewer": null, "report": {"paper_summary": "This paper is about the design of an automatic phoneme transcription system for transcription assistance. \nIn particular, it targets endangered languages with only one speaker data, and the goal is to reduce the cost of transcription such languages. \nWith all due respect to previous research, the authors note that each of them uses a different system, and that each of them has been tested on a different language. \nThe authors also propose a model that is retrained from pre-trained models in multiple languages, and hypothesize that this will be effective for speech recognition with small amounts of data, such as for endangered languages. \nThe authors designed a unified experimental setup, the STP test bed, and used it to compare 4 different models under 11 different languages. \nThe experiments showed that the system with the multilingual pre-trained model performed better in many languages. \nThe authors also estimated that there is a boundary where the recognition rate drops significantly around 90 minutes of training data. ", "summary_of_strengths": "The strength of this paper is that it uses a unified experimental setup to conduct experiments on endangered language automatic transcriptions, which have traditionally been conducted with different models and different languages. \nThe discussion of the experiment, for example, whether it is suitable for fieldwork or not, is given from a humanistic perspective as well, so that it can be considered as a contribution not only to technology but also to the humanities. \nIn addition, the authors have published a container for reproducing some of the experiments, which is expected to have a significant impact not only on the paper itself but also on future research in this field. ", "summary_of_weaknesses": "The motivation, purpose, and selection of models for the study are appropriate. \nHowever, I have some concerns about the experiment.\n1. Due to the characteristics of endangered language evaluation, I feel that the test data is very limited. \nThe authors split the data 9:1 between training and testing, but for example, with 90 minutes of data, there are only 10 minutes of test data. \nOf course I understand that this is unavoidable due to data limitations, but I think some approach or support for this is needed. \nFor example, cross-validation can be considered. ( Note that I am not referring to the cross-validation set commonly employed in neural net training.) \nOther possibilities include showing that the distribution of phonemes in the test data is not significantly different from the overall distribution, or calculating perplexity. \nAveraging the languages with the same weights is also anxious in terms of the reliability of the test set mentioned above. \nFor experiments with such a small amount of data, I think a confidence interval should be shown.\n2. As the authors describe at the end of their discussion, the number of speakers is very small. \nTherefore, I feel that it is not possible to distinguish whether the experimental results are speaker-dependent or language-dependent. \nOf course I understand the difficulty of the experiments with endangered languages, but this is not a reason to relax the experimental conditions for generalization. \nFor example, I think the authors could conduct a quasi-limited experiment using a European language for which a large amount of data is available. \nIf it is inappropriate in those languages, the authors should explain why.\nI am very sympathetic to the philosophy of this paper and understand its importance. \nHowever, I believe that the experimental setup should be very carefully designed, as this study could be a baseline for future work in this field. ", "comments,_suggestions_and_typos": "Even if I take into account the convenience of fieldwork, there seems to be little need to compare training times. \nAnd if it only takes 24 hours, it seems acceptable. \nIf you want to describe the training time, I encourage you to discuss it more. "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 106], [106, 250], [250, 417], [417, 645], [645, 787], [787, 903], [903, 1039]], "summary_of_strengths": [[0, 231], [231, 470], [470, 683]], "summary_of_weaknesses": [[0, 80], [80, 133], [133, 136], [136, 241], [241, 385], [385, 516], [516, 568], [568, 668], [668, 845], [845, 969], [969, 1067], [1067, 1070], [1070, 1164], [1164, 1300], [1300, 1467], [1467, 1615], [1615, 1691], [1691, 1776], [1776, 1924]], "comments,_suggestions_and_typos": [[0, 115], [115, 168], [168, 248]]}}}, {"rid": "c6950904227f5ab8e9811e50b0723baefe901d1173d4a9ea3e7138cdeab1c5706ef17cb872b3567b19eab0f232ac18c70c5515e5369c45567355ca66659fe021", "reviewer": null, "report": {"paper_summary": "This paper discusses an important task for endangered language documentation -- i.e. phoneme transcription. This typically involves building a phoneme recognition model on a language where the amount of data is in minutes and the number of speakers are often just 1. The authors show that this approach is quite different from the typical ASR task where the models need to be speaker independent. They then compare and contrast the recent ASR architectures towards this scenario and try to understand them in the single speaker scenario. Finally the authors also provide their toolkit for furthering the research towards this important task. ", "summary_of_strengths": "The paper addresses a very real and challenging problem of phoneme transcription for endangered language documentation. The challenge is 2 fold in terms of being single speaker and also being limited in terms of amount of training data. This is quite different from the typical ASR task where the models need to be speaker independent. The paper is well written and also contributes their software.  Among the models that the authors choose, there is a variety in their -   1) Architecture - HMM/DNN vs CTC based models vs LF-MMI models    2) Encoders - Transformer, Conformer and LSTM based encoding   3) Training regime - Pretrained vs Training from scratch Which can help answer the importance of each of them towards this task. I have some reservations with the presentation of the results and feel it can be better. I have provided some detailed comments regarding that in the other sections. ", "summary_of_weaknesses": "I feel the choice of models used by the authors are different on 1 important aspect. It seems like except XLSR-53 all other models were trained from scratch. In my opinion there are 2 important factors in place - (1) single speaker (2) limited data on an endangered language. This can then lead to a variety of questions -  1) Which models (HMM/DNN , CTC , Enc-Dec) is better? \n2) Using pre-trained or training from scratch which is better? \n3) If using pre-trained is it better to use pre-trained model on many languages or just 1 language but multiple speakers is enough.  The results presented by the authors seem to answer these questions but only to some extent. This limits the contribution of this work. It would be great if they can add an additional \"from scratch\" vs \"from pretrained\" results for each of the model choices. ", "comments,_suggestions_and_typos": "1) Could you provide the source of the g2p rules. It's important for readers to replicate your results as the results are provided in PER and PER is also directly dependent on the ground truth phonemes generated by the g2p rule. \n2) Have you considered using a phone based model like Li et. al. 2020 and using that to then convert to phoneme based rules for the endangered language? \n3) XLSR-53 is a pre-trained acoustic model which is trained in a self-supervised manner. However there are many large multilingual or english models available that can be fine-tuned. It would be interesting to see how they work compared to XLSR-53. For example using the KALDI ASPIRE model, or models built in Dalmia et. al 2018, or multilingual models in ESPNET toolkit, or the hidden representations of the Allosaurus model in Li et. al. 2020; as done in -  https://arxiv.org/pdf/2104.01624.pdf. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 108], [108, 267], [267, 397], [397, 538], [538, 642]], "summary_of_strengths": [[0, 120], [120, 237], [237, 336], [336, 399], [399, 400], [400, 472], [472, 660], [660, 732], [732, 821], [821, 898]], "summary_of_weaknesses": [[0, 85], [85, 158], [158, 276], [276, 377], [377, 441], [441, 574], [574, 575], [575, 668], [668, 711], [711, 834]], "comments,_suggestions_and_typos": [[0, 50], [50, 229], [229, 291], [291, 383], [383, 473], [473, 567], [567, 633], [633, 705], [705, 820], [820, 882]]}}}]
[{"rid": "74422589ed375a739e9cded681a52f9ff22834c7cdc647b63086b3a47b6ba43b927827b977434a757af9a870b9580010b82d44b20b80b0474d4ab2ed77fcb21f", "reviewer": "Bo Zheng", "report": {"paper_summary": "The paper proposes an efficient and flexible BERT-based multi-task framework. The framework first trains a single-task model for each task using partial fine-tuning, where the bottom layers of BERT are frozen. Then the task-specific layers in each single-task model are compressed using knowledge distillation. The compressed layers are finally merged into one multi-task model, while the frozen layers can be shared across tasks. Experiments show that the framework can reduce up to 2/3 of the model parameters while hardly hurting the model performance. ", "summary_of_strengths": "1. The paper is well-written, and the method is easy to follow. \n2. In addition to the reduced number of model parameters, the inference speed also increases. ", "summary_of_weaknesses": "1. Missing comparison with other parameter-efficient fine-tuning methods, e.g., adapter-based methods. \n2. The training cost is much larger than conventional fine-tuning or other parameter-efficient fine-tuning methods. ", "comments,_suggestions_and_typos": "See above. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Bo Zheng, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 78], [78, 210], [210, 311], [311, 431], [431, 556]], "summary_of_strengths": [[0, 3], [3, 64], [64, 68], [68, 159]], "summary_of_weaknesses": [[0, 3], [3, 103], [103, 107], [107, 220]], "comments,_suggestions_and_typos": [[0, 11]]}}}, {"rid": "f9d2c04ce99e3997b4a0ec7dce59178a5c9408b3d278ae129cb42eb387b5c7fe6f0d9949493739482f27d4afd072559ba4223d739608d6ac872911c0ecd60c2b", "reviewer": "Zhiqing Sun", "report": {"paper_summary": "The paper proposed a flexible framework for multi-task BERT serving. Specifically, instead of training a single multi-task BERT or training several independent fine-tuned BERT models for different tasks, the proposed framework only partially fine-tune the top layers in the BERT model, which allows these sub-task models to share the parameters in the bottm layers. The paper further proposes to compress the fine-tuned topmost layers into less layers to reduce model parameters. On the GLUE benchmark, the proposed method outperfroms other smaller BERT variants with a better accuracy-model_size trade-off. Overall, the review believes this is a strong short paper. ", "summary_of_strengths": "1. The writing of the paper is clear and the motivation is clear. The proposed method (i.e., combining partial weight sharing and compression) is novel.\n2. The proposed method can compress BERT-base by 60% with only 0.3 performance degeneration on GLUE tasks. The empirical results are quite good. ", "summary_of_weaknesses": "1. For flexible multi-task, the paper proposes to initialize the student from the bottommost Ns layers of the teacher. Besides, the paper proposes to determine the number of task-specific layers for each task based on the marginal benefit. These two design choices seem to be a bit arbitrary. Have the authors tried other strategies such as 1) initialize the student like in PKD-BERT 2) determine the number of layers by total budgets (for example, fine-tune students with different number of layers and set an hard overhead budget for all sub-tasks, like the current 34.3%, and do a brute-force search to fine the optimal student layer configuration)? ", "comments,_suggestions_and_typos": "See Weaknesses "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Zhiqing Sun, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 69], [69, 366], [366, 480], [480, 608], [608, 667]], "summary_of_strengths": [[0, 3], [3, 66], [66, 153], [153, 156], [156, 260], [260, 298]], "summary_of_weaknesses": [[0, 3], [3, 119], [119, 240], [240, 293], [293, 653]], "comments,_suggestions_and_typos": [[0, 15]]}}}, {"rid": "3076e22c80c84ecde98bf48a17760bc2a15b5119149d6987e949b462414b879b2e39bbd48333c5380e15bda1333c544333c619e7349909d71ad67a362624e94f", "reviewer": null, "report": {"paper_summary": "The paper deals with multi-task learning. The authors find that having separate networks to learn separate tasks would lead to good performance, but requires a large memory. Using MT-DNN would save memory, but the results are not satisfying. The authors thus propose an approach that saves memory and at the same time achieves good results on GLUE.\nFigure 1 gives a good summary. First, train task-specific models (but for each model, only finetune the top n layers). Second, do knowledge distillation, so that we can compress the n layers into a smaller number of layers. Third, merge all the models together as shown in Figure 1(c). The GLUE performance is comparable to full fine-tuning (i.e., tuning k models separately where k is the number of tasks), and the proposed approach saves 2/3 of memory. ", "summary_of_strengths": "- Writing is clear.\n- Many baseline experiments are performed, including DistillBERT, BERT-of-Theseus, MT-DNN, and many others.  - The observation about MT-DNN’s degradation on QQP (line 237) is interesting. This observation reminds me of the intermediate training empirical paper, where in Table 1 they find that QQP is a very different task compared to others: https://arxiv.org/pdf/2005.00628.pdf ", "summary_of_weaknesses": "Other models - Another simple baseline is to have two separate models: one for tasks that lead to “task interference” like QQP, another for other tasks. I wonder if this baseline will perform better than the authors’ approaches (both in terms of memory and performance). There could be other ways of clustering the tasks.  - Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors’ approaches perform better? Are the authors’ approaches complementary?  Experimental details - Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary.  More tasks - Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it’s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD?  Motivation - Relatively minor: If there is a much larger number of tasks, the authors’ approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors’ approach is still O(k) where k is the number of task. ", "comments,_suggestions_and_typos": "Abstract: “overhead” -> it’ll be great to elaborate what overhead you’re referring to (especially because this is the abstract).\nMinor: commas should follow “i.e.” Line 136: “We find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods” <- it'll be great if the authors can elaborate. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 42], [42, 174], [174, 242], [242, 349], [349, 380], [380, 468], [468, 573], [573, 635], [635, 804]], "summary_of_strengths": [[0, 20], [20, 128], [128, 129], [129, 208], [208, 400]], "summary_of_weaknesses": [[0, 13], [13, 153], [153, 271], [271, 322], [322, 323], [323, 388], [388, 523], [523, 598], [598, 641], [641, 642], [642, 663], [663, 813], [813, 814], [814, 825], [825, 1044], [1044, 1103], [1103, 1104], [1104, 1115], [1115, 1352]], "comments,_suggestions_and_typos": [[0, 129], [129, 164], [164, 363]]}}}]
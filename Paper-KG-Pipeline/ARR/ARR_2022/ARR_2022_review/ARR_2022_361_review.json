[{"rid": "9c42d14ccc84cf2e86bf494b4f4c72974fc1f55456a9cdd542f9d7654aaf3d5779eb6eac51a4cdb96a2edee8473bb8bdb525a43601d147fbac2e8d290269013a", "reviewer": null, "report": {"paper_summary": "This paper presents AlephBERT, a BERT-based model trained on texts written in Hebrew, a morphologically rich language which is generally hard to model using methods that work well for languages such as English that don't have complex morphology. The authors train the model on a considerably larger corpus (about twice as large), which leads to gains on sentence-level sentiment analysis and token-level NER tasks. Further, the authors experiment with different methods for segmenting the input into morphemes and obtaining morpheme-level contextual representations. They show that a simple pipeline approach in which one first segments the tokens into morphemes using a character-based seq2seq model and then inputs the morpheme tokens into the AlephBERT model trained on the original tokens leads to stronger results than multi-task-learning approaches on a morpheme-level NER task. They also obtain SOTA results for Hebrew morpheme segmentation, POS tagging, and dependency parsing. ", "summary_of_strengths": "- Very strong results on a number of morphological, syntactic and shallow semantic tasks.\n- Detailed experiments using simple methods to obtain morpheme-level contextual representations.\n- The paper is well written and a pleasure to read.\n- Because of all the additional experiments that are only relevant for morphologically rich languages this is not a \"BERT for yet another language\" paper but provides interesting insights that are likely going to be very relevant for researchers and practitioners working with morphologically rich languages ", "summary_of_weaknesses": "- I originally complained about the lack of an ethics statement given that the authors train their model on potentially problematic text from Twitter that has not been verified. The authors now added a paragraph to the main text that briefly touches upon this issue but I would rather like to see a proper ethics statement, so that this information is more visible than it currently is.\n- As also pointed out by other reviewers, the evaluation only happened for Hebrew and it is unclear how language-agnostic this method is in practice. It would be good to discuss that limitation in the conclusion of the paper.\n- A major contribution of this paper is adapting several existing Hebrew tasks to a comprehensive morphological/syntactic/shallow semantic benchmark. However, unless I missed something, this streamlined benchmark will not be released along with the paper and so if someone wanted to build a future LM, they would have to re-do all the work of adapting existing datasets. ", "comments,_suggestions_and_typos": "- line 380: \"BIOSE\" --> \"BIOES\" (line 380) ", "ethical_concerns": "The model has been trained on Twitter data and other data from the web that has not been filtered for hateful or discriminatory content, and it remains unclear what kind of biases the model exhibits and whether the model outputs may be harmful or inappropriate. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 246], [246, 415], [415, 567], [567, 885], [885, 986]], "summary_of_strengths": [[0, 90], [90, 187], [187, 239], [239, 547]], "summary_of_weaknesses": [[0, 178], [178, 387], [387, 537], [537, 613], [613, 763], [763, 984]], "comments,_suggestions_and_typos": [[0, 43]], "ethical_concerns": [[0, 262]]}}}, {"rid": "a6b8a37e042a271d7644c7181ddf5e17f9e6481a6d344996fabe5074cfe5825232992ec497d01c700ccafa7554178f92df43afba14f75b60020e230b83319ff2", "reviewer": null, "report": {"paper_summary": "The paper introduces a large pretrained Hebrew language model based on BERT objectives and architecture. The model has the largest vocabulary  trained on the largest Hebrew corpus to date. The authors also propose a pipeline evaluation for morphologically rich languages and show state-of-the-art performance of their model. ", "summary_of_strengths": "The authors have put effort in creating a large training dataset, as well as bringing several evaluation tasks together to measure progress for Hebrew NLP. The strong Hebrew language model and the evaluation pipeline would be useful for the NLP community. ", "summary_of_weaknesses": "- It lacks novelty that I would expect from a long research paper. It is read more like a report, rather than a research paper.  - The paper could be restructured and some parts could be better written. Please see comments and suggestions for more details.  - Some contributions are not clear, e.g., which datasets were available, which are released (what kind of postprocessing is done)? hard to digest this from text. Some techniques that could be a contribution remain somehow hidden in lines 425-462, and some are not clear.     - Most of the evaluation tasks are already well-known, but had been executed separately. Morpheme-based tasks are more relevant to multilingual probing literature (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) which are not mentioned throughout the paper. ", "comments,_suggestions_and_typos": "- 091: I don't understand how this is relevant to the paper. It is mentioned a couple of times however I haven't seen such an effort, maybe I'm missing something?  - 096: typo: MLR -> MRL - 121: Not clear if you propose these tasks or only combine them into a pipeline? You could write your contributions as bullet points.  - 156: \"objective objectives\" sounds weird.\n- Section 2 can be condensed quite a lot. 133-156 is already known, so too long. I'd be more interested to see a review on multilingual benchmarks and probing tasks, especially for morphologically rich languages. I'd rather see more details of the other Hebrew models.  - 174-176: Again sounds like the authors take GLUE (or similar benchmark) and translate the tasks into Hebrew.  - 178:181: But there is a rich literature on morphological and syntactic probing tasks (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) - Throughout the paper, the authors use the term NLU, however I'm not sure whether some of the lower level tasks count as NLU or syntactic tasks.  - 254: Oscar is introduced here but there is no description until line 276.\n- 313 - footnote 2: What's the postprocessing effort here, not clear.  - 320-346: This could be a table.\n- 330: Introduce SPMR, what does it stand for?\n- 332: which UD version?\n- Table 3: The english transcriptions are needed - footnote 4,5: what's the added value (if any)? why is 5 anonymous, didn't understand?\n- 428-448: I didn't understand how you feed the subword or token representation to a char-LSTM? Doesn't your model need the \"char\" representation?\n- 636-638: Language-agnostic is over claiming here.  - Why Aleph? "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 105], [105, 189], [189, 325]], "summary_of_strengths": [[0, 156], [156, 256]], "summary_of_weaknesses": [[0, 67], [67, 128], [128, 129], [129, 203], [203, 257], [257, 258], [258, 389], [389, 420], [420, 529], [529, 533], [533, 622], [622, 814]], "comments,_suggestions_and_typos": [[0, 61], [61, 163], [163, 164], [164, 188], [188, 270], [270, 323], [323, 324], [324, 368], [368, 410], [410, 449], [449, 581], [581, 637], [637, 638], [638, 749], [749, 750], [750, 909], [909, 1055], [1055, 1056], [1056, 1132], [1132, 1202], [1202, 1203], [1203, 1237], [1237, 1284], [1284, 1309], [1309, 1358], [1358, 1407], [1407, 1446], [1446, 1542], [1542, 1593], [1593, 1645], [1645, 1646], [1646, 1659]]}}}, {"rid": "57b7cf5a90c31b190f7076d107802b5f5bdcfb9d3f4a8b4f7255ca965eca425df62266643ed74f674c022ad1a86f662cef3f3e4ec30a19e1723e563c558c1f17", "reviewer": "Rudolf Rosa", "report": {"paper_summary": "**Note**: *This is only a slight revision of my previous review for a previous version of this paper. I did not re-check all the details of the paper carefully, I mostly focused on checking the parts where I had reservations towards the previous version; I simply hope that the parts which I already found good in the previous version stayed good or were improved in this version. But I found already the previous version of the paper to be very good.*\nThe paper describes a model called AlephBERT, which is a BERT language model for Hebrew that surpasses previous such models thanks to being trained on larger data and with better handling of the morphological richness of Hebrew. The paper also compiles together an evaluation toolkit for evaluating Hebrew language models, based on pre-existing tasks and datasets. The model and all code is planned to be released with the camera-ready version of the paper.\nThe paper is definitely mostly a resource paper: most of the stuff is laborious but mostly straightforward, gathering data from available sources, training a model using existing approaches, compiling a benchmarking toolkit from existing tasks and datasets, and evaluating the trained model with this toolkit. The only part which is more research-heavy is handling the rich morphology of Hebrew, where the authors experiment with introducing a morphological segmentation component into the neural setup (a task which is highly non-trivial for Hebrew).\nThe authors evaluate all of their contributions and prove that each of them brings improvements over the previous state of the art. ", "summary_of_strengths": "The resources created by the authors seem to be extremely useful for nearly anyone dealing with Hebrew in NLP, as large pretrained language models are the core of most current approaches.\nThe approach used for handling complex Hebrew morphology is novel and potentially inspirative for other morphologically complex languages.\nWhile I have a feeling that ACL does not prefer publishing pure resource papers, I believe that in case where the created resource is very useful, these papers should have their place at ACL. Besides, there is also a research component to the paper (although the research component itself would not suffice for a long paper).\nThe paper is very well written and very nice to read and easy to understand. ", "summary_of_weaknesses": "I found several minor problems and uncertainties in the previous version of the paper, but the authors managed to address practically all of these in their revised version. My only remaining reservation thus is towards the claimed but not demonstrated language-agnosticity of the presented approach, which I find to be too strong a claim (or maybe I have a different understanding of what \"language agnostic\" means). ", "comments,_suggestions_and_typos": "In their response to the previous reviews, the authors list the following improvement: \"We describe the Twitter data acquisition and cleanup process.\", but I have not found this improvement in the current version (but I admit I might have simply overlooked it; all I am saying is I did not find it at the places where I would expect it). "}, "scores": {"overall": "4.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Rudolf Rosa, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 102], [102, 381], [381, 453], [453, 682], [682, 818], [818, 911], [911, 1221], [1221, 1463], [1463, 1595]], "summary_of_strengths": [[0, 188], [188, 327], [327, 519], [519, 653], [653, 730]], "summary_of_weaknesses": [[0, 173], [173, 417]], "comments,_suggestions_and_typos": [[0, 152], [152, 338]]}}}]
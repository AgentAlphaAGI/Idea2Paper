[{"rid": "3573c1783420e05e2d9c850b934b303abd649827c3d6e6e311e49c216fe279a4b1ff0d538eb7c10a51097345be3e68d9acffd53abcd13cea909999e95a4336e3", "reviewer": null, "report": {"paper_summary": "This paper focuses on quality estimation of machine translation systems. Using a sentence level QE system, they use feature attribution in a sentence level QE system to determine the errorfull  spans of words in a target translation that most contribute to the QE model score. This enables them to define a semi-supervised training set for a word-level QE method.\nIn addition, they propose to use the word-level QE task as a new benchmark for evaluating explainability methods.\nThey explore 4 models for feature attribution for sentence level QE method: LIME, Information Bottleneck, Integrated Gradients, and Attention. \nThey use 4 metrics: AUC, Average Precision, Recall at top K, and Accuracy at top 1. \nAll the metrics they define cannot handle MT target sentences that are all correct or all wrong as the authors acknowledge. hence they exclude such sentences form further analysis. A serious issue! for example over 56% of Ro-En sentences are discarded.\nFor sentence-level QE they use a SOTA method called TransQuest (using XLM-R base). \nThey experiment with the MTQE-PE data set that has both Direct Assessment (DA) with a scale of 0-100 of MT sentences by humans and Post Editing of these sentences. \nThey only keep sentences with DA score below 70 for the DA task.\nThe major results per the authors are in Table 2. It shows that 3 language pairs that some of the semi-supervised methods outperform the glass-box supervised approach (though it is worse uniformly and significantly than the black box Micro TransQuest approach.)\nThe authors do not provide significance tests or standard deviations of the metrics making it hard to judge if these results are meaningful for discriminating between the approaches. ", "summary_of_strengths": "The paper proposes using feature attribution methods to a extend a sentence-level  QE system to a word-level QE system.\nThey identify a good data set MTQE-PE for analyzing these methods. They study effectively 4 methods for word-level QE with semi-supervised approach and 2 fully supervised word-level QE methods across 3 language pairs. They analyze a high error subset and a low error subset of the MTQE-PE test set. They report results using multiple metrics for word-level QE. ", "summary_of_weaknesses": "1. The fact that the 4 metrics cannot handle perfectly correct MT output or 100% error full MT output is a major weakness particularly for the proposed type of use case which is identifying where the errors are in the MT system output. I recommend the authors define metrics that enable evaluation on the whole test set.  For example one can use an F-measure like metric on all the words in the test set. It can be done either by using a threshold to define an error detection and then measuring Precision and recall of all error events (words) a micro-F or if no threshold is desired for every word a mean square error is measured between system probability of an error and the gold truth (again a micro measure over all word positions) for all sentences in the test set). \n  2. By selecting higher error rate sentences, metrics like accuracy at 1 are measuring more of the tail of the distribution since one is counting if the top scoring event is indeed an error ( a sentence has many errors typically).\n3. Also, given that all 4 metrics are averaged over the test instances, at least a standard deviation should be given to give a sense of the variability of the 4 metrics for a test condition.\n4. Another weakness is that the conclusion of superiority of feature attribution methods is somewhat unreliable given that they hold for RoEn and NeEn but not EsEn (using AP as indicated in the paper). ", "comments,_suggestions_and_typos": "no major comments. "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 73], [73, 277], [277, 364], [364, 478], [478, 621], [621, 706], [706, 831], [831, 888], [888, 905], [905, 960], [960, 1043], [1043, 1208], [1208, 1274], [1274, 1324], [1324, 1536], [1536, 1719]], "summary_of_strengths": [[0, 120], [120, 187], [187, 338], [338, 419], [419, 481]], "summary_of_weaknesses": [[0, 3], [3, 236], [236, 321], [321, 322], [322, 405], [405, 774], [774, 780], [780, 1007], [1007, 1010], [1010, 1199], [1199, 1202], [1202, 1401]], "comments,_suggestions_and_typos": [[0, 19]]}}}, {"rid": "4f10ec5193a009e80509f8af752083af20c189deead2c88813e4fa7441489f0e92bee6da70a91f5524c0a3142d1b8f68242a9157a11e8589f1aa9d62a2597afe", "reviewer": null, "report": {"paper_summary": "This paper presents a semi-supervised method for word-level quality estimation by using sentence level QE models which are mapped to word-level QE using feature attribution (rationale extraction) methods. The core hypothesis is that sentence level quality stems from word level translation errors, thus sentence level models should store information about which individual tokens are likely to be correct/incorrect, and this information can be surfaced by feature attribution methods. This work follows recent research directions in model probing and “bertology”, applying feature attribution methods to QE.  The authors test several methods of feature attribution – empirical results show that feature attribution models trained on sentence-level data outperform a random baseline, but are significantly worse than supervised models trained on word-level QE training data, which is generated by automatically aligning tokens in MT hypotheses with their corresponding human post-edits. ", "summary_of_strengths": "The idea of probing sentence-level models for word-level information is compelling -- the word- and sentence-level QE tasks are well-suited to testing feature attribution methods because standard training and evaluation datasets from the many iterations of the WMT QE shared tasks should allow for straightforward comparison with and leveraging of existing approaches, and the tasks are already divided into different levels of granularity. This is a useful insight from this work that certainly merits future exploration. ", "summary_of_weaknesses": "The empirical evaluations in this work are difficult to interpret, and I am not sure what the key takeaways are. The metrics used in this work (AUC score, recall at K) are non-standard for the tasks under consideration, so it is difficult to interpret the results in the context of word-level QE evaluation. Although it is interesting that sentence-level QE models contain some information about probable word-level translation errors, it is unsurprising that sentence level models would learn to focus on target tokens that are likely to be wrong and I think the paper at least needs more analysis or insight into the details of what is going on to be useful for other researchers in the context of QE (see comments for some ideas).\nAs presented, the work cannot be framed as a new approach to word-level QE because it does not use the same evaluation metrics, and the approach cannot label tokens in a MT hypothesis as Good/Bad, so any comparison with supervised word-level QE approaches is not informative. ", "comments,_suggestions_and_typos": "Possible Analyses / experiments: - what types of errors do the feature attribution models find/miss, what do these models tell us about what sentence-level models are learning?  - what are the dynamics between source and target token attribution? do some inputs attribute more responsibility to the source, while others attribute more to the target hypotheses? why? ( the notion of an \"attribution budget\" could be useful here) - do feature attribution approaches completely miss some types of word-level errors? does this show that sentence level models aren't learning that information? or does it indicate a problem with the data generation method of the word-level QE task? "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 205], [205, 485], [485, 608], [608, 609], [609, 986]], "summary_of_strengths": [[0, 441], [441, 523]], "summary_of_weaknesses": [[0, 113], [113, 308], [308, 734], [734, 1010]], "comments,_suggestions_and_typos": [[0, 33], [33, 177], [177, 178], [178, 247], [247, 361], [361, 368], [368, 428], [428, 513], [513, 589], [589, 678]]}}}, {"rid": "53d0d9ae0394c4f59496513484fb9e9d625c0064283125135b8f81ec3a08afa63dc10427b9d929924d7cd1d29e9ad560df79372aa72a957bbe91a7acca6ea44c", "reviewer": null, "report": {"paper_summary": "This paper describes a semi-supervised approach to machine translation (MT) quality estimation (QE) at word-level. The approach hinges on the assumption that successful QE models rely on translation errors to predict overall sentence quality. It uses the sentence-level features as a weak label for getting predictions for the words in the sentence by exploring a set of feature attribution methods. These methods assign relevance scores that explain model predictions. Overall the paper shows that explanations extracted from sentence-level QE models can be used to detect translation errors. ", "summary_of_strengths": "The paper introduces a novel way to approach word-level QE without having to resort to human annotation. it works losely as a semi-supervised word-level QE method using the sentence-level scores as weak labels for the word-level. This is the major contribution of the paper and its biggest strength. The main reason is because it enables practictioners and researchers of the field to have a word-level prediction without having to incur in the costs of annotating word-level data for training.\nExperimental settings look sound and results indicate that rationale extraction methods are comparable to using unsupervised glass-box methods to word-level QE but inferior to supervised black-box methods (that use human-produced or human-derived labels). ", "summary_of_weaknesses": "It would be great if the code is indeed released if the paper is accepted. This would help a lot with the reproducibility of the method proposed in the paper. Other than that, it is unclear how the approach described here compares with the ones proposed for the Explainable Quality Estimation shared task which propose similar approaches. If the paper is accepted it would be good to position this in relation to these approaches. ", "comments,_suggestions_and_typos": "- Any idea about the difference in performance between using XLMR-base and XLMR-large? "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 115], [115, 243], [243, 400], [400, 470], [470, 594]], "summary_of_strengths": [[0, 105], [105, 230], [230, 300], [300, 495], [495, 751]], "summary_of_weaknesses": [[0, 75], [75, 159], [159, 339], [339, 431]], "comments,_suggestions_and_typos": [[0, 87]]}}}, {"rid": "7566deb6096584a7083b4c4d482f841af8a5da6238fda87877fb7ebd73aff60d57e4e191842789d4fbe52284b5a918f8a9756b3091bb36a0fc422820f890311e", "reviewer": null, "report": {"paper_summary": "This paper proposes to extract word-level quality information for machine translated text from a state-of-the-art sentence-level quality estimation (QE) model, based on pre-trained contextual embeddings, fine-tuned in a semi-supervised way, i.e. using only sentence-level scores for fine-tuning. The main research question the authors focus on is, do QE models infer sentence-level translation quality scores based on word-level errors, which would provide insights on the model decision making (rationales) and alleviate the need of costly error-annotated word-level data. \n  To answer this question, an empirical work is conducted by comparing several interpretability related approaches, so-called feature attribution methods, as well as a supervised and an unsupervised approach, along with a random classification baseline. Experiments include three translation directions towards English and are performed on a publicly available dataset. Results show the superiority of two approaches, namely integrated gradients and attention weights, based on several metrics calculated on a test corpus where sentences containing only or no errors were ignored.\nThe analysis of these results is rather succinct, but proposes nonetheless a few interesting observation nuggets. First, more useful error-related word-level information is encoded at deeper layers of the model, second, both source and target sentences appear to have an impact on feature attribution, third, frequency of words in the machine translation training corpus and sentence-level QE seem to be in a relation of some sort. ", "summary_of_strengths": "Overall, this paper is very well written, with a clear hypothesis and straightforward experimental setup.  The semi-supervised word-level translation errors extraction paradigm is, to the best of my knowledge, relatively novel when using contextual embedding models. The approach proposed by the authors is thus interesting for the research community and could pave the way towards more cost effective translation QE, fine-grained methods for explaining QE models outputs, etc.\nWhile the experimental results do not necessarily emphasize which approach would allow for the best word-level QE results, which is not the scope of the study, they offer a comparison point for future work and validate the use of semi-supervised methods for word-level QE. The analysis provides a few insights on where the information is encoded in the model, as well as the importance of source and target sentences for QE modelling.\nThe use of publicly available resources, including the dataset and the various implementations, ensure a reasonable level of reproducibility. ", "summary_of_weaknesses": "The main weaknesses of this paper are the empirical validation of the method proposed by the authors and the lack of deep analysis. The main findings are limited to applying to QE a set of existing interpretability oriented methods already applied generally to NLP tasks.  For the empirical validation of the method, the evaluation does not reflect a general scenario in machine translation where both perfect and fully erroneous translations exist. Additionally, as shown later in the analysis, there could be a relation between word frequency distributions in the machine translation training data and the ability of the QE model to rely on this information to estimate the sentence-level translation quality. This phenomenon should be investigated further, for instance by replacing the random baseline by a frequency based classifier, as a starting point. The various approaches used as feature attribution methods, along with the current experimental setup, do not offer a clear understanding of what the QE model does unfortunately.\nFor the analysis section, extracting a useful QE signal at deeper layers of the pre-trained model is somewhat expected, but is still interesting to find. However, we need more insights on what kind of information is encoded at each layer, by the use of probes for instance, at a sentence-level basis. As Figures 2 and 3 indicate, variability in feature attribution between layers show that some words, in some context (sentences in this case), receive different scores given an approach. What are the characteristics of these words and sentences? Do feature attribution methods agree or diverge given words in context? ", "comments,_suggestions_and_typos": "It would be interesting to see how relevant to word-level QE is the information encoded at different layers of the model without fine-tuning for sentence-level QE, by using simple classifiers as probes for instance. \n  The feature attribution method based on attention could easily be the focus of the study, whether by using the method proposed by (Kobayashi et al., 2020, “Attention is Not Only a Weight:Analyzing Transformers with Vector Norms”) as a comparison point, by analysing attention heads separately the same way it is conducted per layer, etc. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 296], [296, 574], [574, 829], [829, 945], [945, 1156], [1156, 1270], [1270, 1588]], "summary_of_strengths": [[0, 106], [106, 107], [107, 267], [267, 478], [478, 751], [751, 913], [913, 1055]], "summary_of_weaknesses": [[0, 132], [132, 272], [272, 273], [273, 450], [450, 712], [712, 860], [860, 1039], [1039, 1193], [1193, 1340], [1340, 1527], [1527, 1586], [1586, 1658]], "comments,_suggestions_and_typos": [[0, 216], [216, 557]]}}}]
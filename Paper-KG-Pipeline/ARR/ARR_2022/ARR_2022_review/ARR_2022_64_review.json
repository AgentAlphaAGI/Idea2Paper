[{"rid": "39cfecf18de21fd5ed75ea20882886bcc62b45ca973e1c6a139612c22b7399695443caa2b7b4f9fe702193168fb2cb774ed4ee7e340b36db95e723c2835ba755", "reviewer": null, "report": {"paper_summary": "This paper investigates the effectiveness of entity representations in multilingual language models. The proposed mLUKE model exhibits strong empirical results with the word inputs (mLUKE-W), it also also shows even better performance with the entity representations (mLUKE-E) in cross-lingual transfer tasks. The authors' analysis reveals that entity representations provide more language-agnostic features to solve the downstream tasks. Extensive experimental results suggest a promising direction to pursue further on how to leverage entity representations in multilingual tasks. ", "summary_of_strengths": "1. The authors explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. They train a multilingual language model with 24 languages with entity representations and show mLUKE model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks. \n2. The authors show that a cloze-prompt-style fact completion task can effectively be solved with the query and answer space in the entity vocabulary. \n3. The results show that entity-based prompt elicits correct factual knowledge more likely than using only word representations. ", "summary_of_weaknesses": "Most of languages in LAMA are rich-resourced languages indeed, the authors may need to test mLUKE on some low-resourced languages. ", "comments,_suggestions_and_typos": "This paper has done a solid work on Multilingual Pretrained Language Models. \nThis paper is well written and easy to read. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 101], [101, 310], [310, 439], [439, 583]], "summary_of_strengths": [[0, 3], [3, 114], [114, 317], [317, 321], [321, 469], [469, 473], [473, 599]], "summary_of_weaknesses": [[0, 131]], "comments,_suggestions_and_typos": [[0, 77], [77, 123]]}}}, {"rid": "26e8a5a6daf9cbf400c7d59f2697cad70c104233f31b8e9aa49167d0d14f6e38ace18944945fb75ac80ee8effed0cac3d03889d1030048d7e9ef5544cb8814f7", "reviewer": null, "report": {"paper_summary": "This paper proposed to extend the monolingual entity representation model (LUKE) to the multilingual version and further conduct experiments on several cross-lingual tasks. The experimental results show the superiority of incorporating pre-trained multilingual entity representations. Besides, the authors also had detailed analysis of the benefits of proposed methods. Overall the work is solid with simple ideas and good results. I believe the released entity representation model would be useful for multilingual knowledge-related tasks. ", "summary_of_strengths": "1. Well-written paper and good presentation for the method, tasks as well as analysis.\n2. The method is effective by incorporating entity representations and the performance got improved for various cross-lingual transfer tasks.  3. If the pre-trained model is released, it would benefit more research about utilizing multilingual entity knowledge by either feature extraction or extra entity features. ", "summary_of_weaknesses": "1. The idea is a bit incremental and simply the extension of previous monolingual LUKE. \n2.For the language-agnostic characters of entity representations, the paper has weak analysis on the alignment of entity representations. ", "comments,_suggestions_and_typos": "The authors could add more analysis about the multilingual alignment of entity representations and it would be better to have visualizations or case studies for different types of languages such as language family. We are also interested in whether entities from low-resourced languages are well aligned with the high-resourced ones. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 173], [173, 285], [285, 370], [370, 432], [432, 541]], "summary_of_strengths": [[0, 3], [3, 87], [87, 90], [90, 229], [229, 230], [230, 233], [233, 403]], "summary_of_weaknesses": [[0, 3], [3, 88], [88, 227]], "comments,_suggestions_and_typos": [[0, 215], [215, 334]]}}}, {"rid": "2ddada6f901dd8ad326b27b309f2364352ef26e9d13e71e96e487268cf98bae91ac8ac79dae1e7c8282676d65315763816de11da22c7db635042708d9863a2b8", "reviewer": null, "report": {"paper_summary": "This paper investigates the effectiveness of entity representations in multilingual language models. They argue that using entity representations facilitates cross-lingual transfer by providing language-independent features and present a multilingual extension of LUKE. They investigate two ways of using the entity representations in cross-lingual transfer tasks: (1) perform entity linking for the input text, and append the detected entity tokens to the input sequence;  (2) use the entity [MASK] token from the MEP task as a language-independent feature extractor. \nThe experimental results show that these entity-based approaches consistently outperform word-based baselines, and entity representations provide more language-agnostic features to solve the downstream tasks. \nOverall, this work has conducted a series of solid experiments to demonstrate the effectiveness of entity representations in multilingual tasks and provided in-depth analysis on entity representations. ", "summary_of_strengths": "- investigated the effectiveness of entity representations in multilingual language models.\n- present a multilingual extension of LUKE - conducted a series of solid experiments to demonstrate the effectiveness of entity representations in multilingual tasks and provided in-depth analysis on entity representations.\n- suggest a promising direction to pursue further on how to leverage entity representations in multilingual tasks. ", "summary_of_weaknesses": "- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:     - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction     - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. ", "comments,_suggestions_and_typos": "In line \"544\", I still quite understand why using entity representations can reduce language bias. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 101], [101, 270], [270, 569], [569, 779], [779, 982]], "summary_of_strengths": [[0, 92], [92, 135], [135, 316], [316, 431]], "summary_of_weaknesses": [[0, 152], [152, 334], [334, 435]], "comments,_suggestions_and_typos": [[0, 99]]}}}]
[{"rid": "a0821b6cbd2b3bfbddc644f83e27e1bac50ac0153c23386ca20f5725418f812cc73f2220474452bd2b77d97410b6c4814451212581aec2b8fd71c84521db7a70", "reviewer": "Zhengyuan Liu", "report": {"paper_summary": "In this work, the authors proposed a unified model of task-oriented dialogue understanding and response generation. The two major enhancements are adopting task-oriented dialogue pre-training on a data collection, and introducing the prompt-based learning for the multi-task capability via one model. From the experimental results, the pre-training strategy proved useful to improve the performance on the benchmark MultiWOZ. ", "summary_of_strengths": "While the idea of task-specific pre-training is not new, it is still interesting, and the proposed method proved effective in leveraging the language backbone T5, and can be potentially applied to other models and tasks. ", "summary_of_weaknesses": "1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. \n2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. ", "comments,_suggestions_and_typos": "Here are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? \n2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? \n3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Zhengyuan Liu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 116], [116, 301], [301, 426]], "summary_of_strengths": [[0, 221]], "summary_of_weaknesses": [[0, 3], [3, 142], [142, 146], [146, 270]], "comments,_suggestions_and_typos": [[0, 28], [28, 220], [220, 224], [224, 387], [387, 544], [544, 586], [586, 590], [590, 672], [672, 738]]}}}]
[{"rid": "aa75cb5d1fcdad14f947645ab06b4db48b3cd30fa91947d3667b8a7637b43b2b1070fc11e4e9b623604de1ddb6198bc41812aa98ea7a58e4521a239374596edf", "reviewer": null, "report": {"paper_summary": "The paper describes a new approach towards MeSH label prediction, utilizing the title abstract journal relative information. The proposed model combines BiLSTMs, Dilated CNNs and GCNNs to extract features from abstracts, titles and the mesh term hierarchy respectively. Limiting the search MeSH space with information extraction from metadata (such as other articles published in that journal) allows for a boost in performance by building dynamic attention masks. The final model shows good performance compared to related approaches, one of which uses the full article. ", "summary_of_strengths": "- Utilized information past the document itself to limit the MeSH search space - Introduces novel end-to-end architecture that can be used in other tasks involving scholarly articles - Achieves good performance compared to related approaches. ", "summary_of_weaknesses": "- Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)?  - Some of the design choices are not explained well (e.g. why IDF-weighting) - Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed. ", "comments,_suggestions_and_typos": "- Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used?  - There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance?  - With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms?  - A few minor typos, proof reading should fix them. Nothing major. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 125], [125, 270], [270, 465], [465, 572]], "summary_of_strengths": [[0, 79], [79, 183], [183, 243]], "summary_of_weaknesses": [[0, 103], [103, 180], [180, 181], [181, 258], [258, 360]], "comments,_suggestions_and_typos": [[0, 68], [68, 103], [103, 104], [104, 262], [262, 334], [334, 335], [335, 468], [468, 515], [515, 516], [516, 568], [568, 583]]}}}, {"rid": "bae051cda3d9a5a252fe0afc5242246443a1294f45df167401fe4b7d7b00ade33bfb200008e4b55070d7906002ff37d802176d11c63bb3c3ae46e476e260133f", "reviewer": null, "report": {"paper_summary": "This paper introduces a new method for MeSH indexing, combining multiple methods including, but not limited to, dilated CNNs, masked attention, and graph CNNs. Overall, the proposed approach makes substantial improvements over prior state-of-the-art methods. For example, Micro F1 improves over BERTMeSH from 0.685 to 0.745. Similar improvements are also found for the example-based measures (e.g., example-based F1). Furthermore, a comprehensive ablation study was performed, showing that the label feature model has the largest impact on model performance, yet, other parts of the method still impact performance substantially. ", "summary_of_strengths": "Overall, the paper is well-written and easy to read. Furthermore, the improvement over prior work is substantial. It is neither easy nor trivial to make such considerable performance improvements for MeSH indexing, especially for Micro F1. For instance, BERTMeSH [1] only improves DeepMeSH [2] by only 2% in Micro F1 [1] after five years of work. Hence, seeing a Micro F1 near 0.75 is a huge breakthrough.\nReferences: [1] Peng, Shengwen, et al. \"DeepMeSH: deep semantic representation for improving large-scale MeSH indexing.\" Bioinformatics 32.12 (2016): i70-i79.\n[2] You, Ronghui, et al. \"BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text.\" Bioinformatics 37.5 (2021): 684-692. ", "summary_of_weaknesses": "Overall, there are three major weaknesses in this paper.\nFirst, the paper uses a custom training and validation dataset pulled from PubMed, making comparisons difficult. Using the data from the yearly BioASQ shared tasks would be better to use their data so new methods are more easily comparable. I understand this is common in similar studies (e.g., by BERTMeSH [3]), but a standardized dataset seems possible and useful.\nSecond, while the hyperparameters are discussed, it is not clear whether hyperparameters were optimized for the baseline models. What were the chosen parameters? Was the validation dataset used to optimize them similarly to the proposed method? If so, why is the standard deviation not reported for the baseline models (e.g., in Table 1)? Given the substantial performance differences between the proposed model and prior work, this additional information must be reported to ensure fair comparisons.\nThird, while this may be the first paper to use GCNNs for MeSH indexing, it is widely used for similar biomedical text classification tasks (e.g., ICD Coding). For instance, [1] directly combines BiLSTMs with GCNNs and label features in a very similar manner to the method proposed in this paper, albeit with exceptions such as [1] does not use dilated CNNs. Furthermore, that work has been expanded on to better understand the impact of the GCNNs and whether they are needed [2]. Hence, the paper would substantially help if the related work sections were expanded to include citations with similar methodologies. In my opinion, the \"Dynamic Knowledge-enhanced Mask Attention Module\" is one of the most innovative parts of the paper and should be highlighted more in the introduction.\nReferences: [1] Chalkidis, Ilias, et al. \"Large-Scale Multi-Label Text Classification on EU Legislation.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n[2] Chalkidis, Ilias, et al. \"An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n[3] You, Ronghui, et al. \"BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text.\" Bioinformatics 37.5 (2021): 684-692. ", "comments,_suggestions_and_typos": "Page 3, Line 240-252: There are a few variations of LSTMs [1]. Is the one used in this paper the same as the 1997 paper? \n  Page 2, Line 098-100: The phrase \"latent semantics\" is unclear. It may help the paper if that phrase is expanded, e.g., does this mean the contextual information from combining multiple layers of neural networks?\nPage 4, Line 287: I believe \"edges are implement MeSH hierarchies\" should be \"edges represent relationships in the MeSH hierarchy\" Page 6, Line 416-417: I believe the phrase \", and we converted all words are lowercased\" Should be \", and we converted all words to lowercase\" References: [1] Graves,A. et al. (2012) Supervised Sequence Labelling with Recurrent Neural Networks. Vol. 385. Springer, Berlin. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 160], [160, 259], [259, 325], [325, 418], [418, 630]], "summary_of_strengths": [[0, 53], [53, 114], [114, 240], [240, 347], [347, 406], [406, 418], [418, 527], [527, 565], [565, 705], [705, 742]], "summary_of_weaknesses": [[0, 57], [57, 170], [170, 298], [298, 424], [424, 553], [553, 586], [586, 669], [669, 763], [763, 925], [925, 1085], [1085, 1284], [1284, 1406], [1406, 1540], [1540, 1711], [1711, 1723], [1723, 1817], [1817, 1906], [1906, 1912], [1912, 2045], [2045, 2141], [2141, 2147], [2147, 2287], [2287, 2324]], "comments,_suggestions_and_typos": [[0, 63], [63, 121], [121, 188], [188, 337], [337, 468], [468, 611], [611, 623], [623, 713], [713, 718], [718, 723], [723, 741]]}}}]
[{"rid": "448a43302a3dc30dbf4a14d018e78a21db257918d1bb15611bcd87969a86b69e010c103017fa905e8468895697bf5b2cfc59a529a62020d11d3d4de13d97985f", "reviewer": null, "report": {"paper_summary": "This paper studies style transfer for languages where no style-labelled corpora are available and furthers related prior work that either assumes access to large style-labelled corpora and the more recent few-shot transfer using only 3-10 sentences at inference for extracting the target style.\nThe authors first study the current few-shot style transfer method and find that is is unsuited for Indic languages. To address it's shortcomings, they (i) use an inference-time trick---back-translation of first generating a translation to English and then generating a style-specific exemplar in the target language, (ii) further enhance the style transfer process requiring only a single generation step to develop a new few-shot style transfer training objective that uses paraphrases and style vector differences, and (iii) consider an interpolated model combining the objectives of (i) and (ii), which they term multi-task learning.  They empirically evaluate the performance of the models using available datasets and also create another dataset for the Indic languages studied for validating the automatic metrics. ", "summary_of_strengths": "Strengths: 1) The study extends style transfer from few-shot to zero-shot in low-resource scenarios.\n2) A new dataset of 1000 sentence pairs in each of the four Indic languages (Hindi, Bengali, Kannada and Telugu) with formality and semantic similarity annotations is created.\n3) A multi-task learning-based model and an approach that relies on style vector difference for controlling the training/inference mismatch in style.\n4) Extensive comparison of the different models on 4 languages to demonstrate empirical validity of the models studied. ", "summary_of_weaknesses": "1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.\n2. No details on the. multi-task learning mentioned in Section 4.4 are available.\n3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.\n4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as  the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.\n5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.\n6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.\n7.  The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1,  and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly  the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG  = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.\n8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?\n9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages? ", "comments,_suggestions_and_typos": "See comments/questions in the summary of weaknesses for ways to improve the paper.\nA few typos to be corrected: Line 491 \"help improve\" Line 495: \"performance of across\" Line 496: \"model fail ...since they\" Figure 1, example for \\lambda  = 1.5  nA -> na (short vowel) "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 295], [295, 412], [412, 933], [933, 934], [934, 1117]], "summary_of_strengths": [[0, 11], [11, 101], [101, 277], [277, 427], [427, 547]], "summary_of_weaknesses": [[0, 3], [3, 89], [89, 113], [113, 116], [116, 135], [135, 195], [195, 198], [198, 326], [326, 443], [443, 682], [682, 685], [685, 794], [794, 1009], [1009, 1190], [1190, 1193], [1193, 1352], [1352, 1355], [1355, 1522], [1522, 1525], [1525, 1666], [1666, 1722], [1722, 1793], [1793, 1924], [1924, 2105], [2105, 2186], [2186, 2189], [2189, 2364], [2364, 2432], [2432, 2458], [2458, 2461], [2461, 2592], [2592, 2709], [2709, 2757]], "comments,_suggestions_and_typos": [[0, 83], [83, 268]]}}}, {"rid": "0e30497ecba61ed356a72b0213bc87e06347be6b0164284ff6f25182bf1e310ac082f086cd5e0d6455085d4bf4025289c6f66bb8555e953606e080663f2aa9c1", "reviewer": null, "report": {"paper_summary": "This paper focuses on the few-shot controllable style transfer problem, in which a few style-related exemplars are used during inference. The authors improve the previous Universal Rewriter model with some techniques, including style-controlled back-translation, paraphrase vectors for style extractor, multi-task learning. The automatic and human evaluations on Indic languages show that the proposed method achieves better performance than previous baselines. ", "summary_of_strengths": "- An interesting problem and a nice improvement over the UR model. This paper investigates the few-shot controllable style transfer task, which is more reasonable in practice. Previously, it is hard to clearly define the bound of some \"style\", while this setting avoids this problem with some exemplars. Based on that, the authors integrate style-controlled back-translation, paraphrase vectors differences, and multi-task learning to significantly improve the performance of the UR model.  - The useful annotated dataset for formality transfer research of Indic languages. ", "summary_of_weaknesses": "- This paper only performs human evaluation on Hindi and Bengali languages, not all seven languages. The human evaluation shows that the proposed method cannot keep semantic consistency well compared with the UR model.\n- Some details of human evaluation (Table 3&4) are missing, including the agreement between different human annotations (Kappa value). ", "comments,_suggestions_and_typos": "Refer to Summary Of Weaknesses "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 138], [138, 324], [324, 462]], "summary_of_strengths": [[0, 67], [67, 176], [176, 304], [304, 490], [490, 491], [491, 574]], "summary_of_weaknesses": [[0, 101], [101, 219], [219, 354]], "comments,_suggestions_and_typos": [[0, 31]]}}}]
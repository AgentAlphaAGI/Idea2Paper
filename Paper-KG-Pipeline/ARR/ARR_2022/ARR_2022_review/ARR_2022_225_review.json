[{"rid": "5aa9b857d1598a40a3898382462799cbeb55e1bbc82d939b4e8f69c406805f88a713d73f73c5c4ca094d603405c5c1aac9d5d2beec16005584a485e12190ad0d", "reviewer": null, "report": {"paper_summary": "The paper describes a method of conditional prompt generation, in order to fine-tune pre-trained language models to a new task while keeping most of the parameters frozen. \nThe input sentences are first passed through the frozen LM, then mapped to a prompt shape which is then appended to the input of the same LM. \nThe output is then used to make a task-specific prediction. \nOnly the mapping and the output layers are updated during training, while the rest of the model is kept frozen. ", "summary_of_strengths": "The underlying idea is quite interesting. Essentially it shows that it can be beneficial to pass the same input through the same frozen model twice. This is an interesting finding. \nThe paper is clear and well written. It is mostly quite straightforward with its presentation, objectively showing both the positives and negatives of the proposed model without trying to oversell or overcomplicate it.\nThe revised version has strengthened the paper, adding some necessary experiments and analysis, therefore I have increased my score. ", "summary_of_weaknesses": "When comparing the proposed method to the Compacter, it essentially has a similar number of parameters (only 10% lower) and achieves a similar performance across the datasets. It also likely has a considerably larger computation requirement, as the model needs to run twice for each sentence. \nTherefore, the paper lacks a clear reason for why the proposed method is in any way an improvement over the previously proposed Compactor.\nThere are quite a lot of grammatical errors, especially in the introduction. The paper should be proof-read by a more fluent English speaker.\nThe contributions in the introduction refer to 1.55M as the parameter count for adapter-based methods, but omits Compacter which is also adapter-based and has considerably fewer parameters.\nTable 2 refers to 2 adapter-based methods: Adapter and Compacter. Table 3 refers to 1 adapter-based method: Adapter. But the text in 4.4 states that the adapter-based method in Table 3 is actually the Compacter. This is confusing and it is currently unclear whether the results in Table 3 are indeed for the Compacter model or not. ", "comments,_suggestions_and_typos": "On line 198 it is explained that the second layer has a task-specific prompt, while the first layer has an instance-specific prompt. But RoBERTa has more than 2 layers; it is currently unclear what happens in the other layers.\nThe introduction claims that the downside of existing prompt-tuning methods is that they are \"incompatible with the traditional LM objective\". It is unclear what that means exactly and why the LM objective would be relevant here. The LM objective is only used for pre-training these models and as far as I can see the paper does not try to evaluate this model as a LM. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 172], [172, 315], [315, 376], [376, 489]], "summary_of_strengths": [[0, 42], [42, 149], [149, 181], [181, 219], [219, 401], [401, 534]], "summary_of_weaknesses": [[0, 176], [176, 293], [293, 433], [433, 510], [510, 575], [575, 765], [765, 831], [831, 882], [882, 977], [977, 1097]], "comments,_suggestions_and_typos": [[0, 133], [133, 227], [227, 370], [370, 457], [457, 596]]}}}]
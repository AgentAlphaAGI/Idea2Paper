[{"rid": "08b469f02d27a4b8a5935a4c3b4047690d0eac73be4055b0a3098fcf8eb09983b46e45745d2aaaf18776913247b065b0dde7791968355639e05f9f96d410cb1b", "reviewer": "Hongfei Xu", "report": {"paper_summary": "This paper investigates the ability of the raw sequence-to-sequence model in document-level translation. The perspective is interesting. It proposes a multi-resolutional data processing step which iteratively split document corpus into k (1, 2, 4, 8,...) parts and feeds them to the sentence-to-sentence translation model. They also provide a new document-level dataset focusing on 3 phenomena: tense consistency, conjunction presence, pronoun translation. Empirically, they show that the multi-resolutional data processing can effectively address the learning problem with long sentences, and obtain improvements over some baselines. \nHowever, there are some issues with the paper: 1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. \n2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. \n3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. ", "summary_of_strengths": "This paper investigates the ability of the raw sequence-to-sequence model in document-level translation. The perspective is interesting. It proposes a multi-resolutional data processing step which iteratively split document corpus into k (1, 2, 4, 8,...) parts and feeds them to the sentence-to-sentence translation model. They also provide a new document-level dataset focusing on 3 phenomena: tense consistency, conjunction presence, pronoun translation. Empirically, they show that the multi-resolutional data processing can effectively address the learning problem with long sentences, and obtain improvements over some baselines. ", "summary_of_weaknesses": "1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. \n2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. \n3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. ", "comments,_suggestions_and_typos": "1) It's better to adopt experiment settings consistent with previous work. \n2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved. \n3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021. "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Hongfei Xu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 105], [105, 137], [137, 323], [323, 457], [457, 635], [635, 960], [960, 1028], [1028, 1310], [1310, 1416], [1416, 1518], [1518, 1692]], "summary_of_strengths": [[0, 105], [105, 137], [137, 323], [323, 457], [457, 635]], "summary_of_weaknesses": [[0, 277], [277, 345], [345, 627], [627, 733], [733, 835], [835, 1009]], "comments,_suggestions_and_typos": [[0, 75], [75, 288], [288, 357], [357, 549], [549, 562]]}}}]
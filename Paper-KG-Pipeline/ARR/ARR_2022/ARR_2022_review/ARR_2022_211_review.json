[{"rid": "569724b6337e28b97edaba29d9d2d648cf3d7547edf45c5b08c8eabaf04f8a063008db38e57d23eed6d040937a62023bf5cdb7100eee90b5bb7a4b46a3688342", "reviewer": "Zhiqing Sun", "report": {"paper_summary": "The paper proposes a core-set based token selection algorithm to reduce the computation overhead of Transformer models on sequence-level classification tasks. The main contribution of the paper is to propose a practical core-set selection algorithm which effectively find a $\\delta$-cover of the token embeddings, and thus reduces redundant tokens. Similar to previous work, the sequence-length is gradually reduced from bottom to top, thus reducing the redundancy among tokens. The paper also presents positive results on GLUE and Long-Range-Arena benchmark. ", "summary_of_strengths": "1. A core-set based token selection algorithm is proposed, with a derivation of the error bound. One of the main advantages of core-set based token selection compared to other local pooling/selection strategies is that core-set based token selection allows removing long-range duplications.\n2. A new sequence-length configuration is proposed for gradually reducing the sequence length in the pipeline of encoders, showing its advantages over other alternatives. ", "summary_of_weaknesses": "1. In the experiment on GLUE, there is no comparison of the proposed core-set based method with a simple local-pooling based method. Since this paper is inspired and follows this line of previous work, such a comparison is necessary.\n2. The derivation of the optimization objective is a bit messy (Equation 1-3). For example, the model weights $\\mathbf{w}$ are important optimization variables, but they are totally omitted in the derivation. Therefore, the derived objective for token selection (i.e., Equation 3) is not mathematically reasonable. For the revised version or resubmission, I would suggest the authors re-think or re-write this part. ", "comments,_suggestions_and_typos": "Comments: 1. The proposed method is a model acceleration technique rather than model compression technique (which reduce the model parameters or size). Instead of model compression techniques, I believe other acceleration techniques such as [1-2] should be cited and discussed.\n[1]: Dynamic Early Exiting for Accelerating BERT Inference [2]: BERT Loses Patience: Fast and Robust Inference with Early Exit Questions: 1. How do the authors form clusters for tokens in Figure 1? It's unclear in Section 3.\n2. In the experiments, did the authors preserve the [CLS] token for other baseline methods (i.e., Att, Rand)?\nTypos: line 304: \\mathcal{X} \\cdot \\mathcal{Y} ==> \\mathcal{X} \\times \\mathcal{Y} line 309: |S| = \\ell_j ==> |S_j| = \\ell_j line 323: \\mathcal{L}(x, y, \\tilde{S}, S] ==>\\mathcal{L}(x, y, \\tilde{S}, S)] "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Zhiqing Sun, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 159], [159, 349], [349, 479], [479, 560]], "summary_of_strengths": [[0, 3], [3, 97], [97, 291], [291, 294], [294, 462]], "summary_of_weaknesses": [[0, 3], [3, 133], [133, 234], [234, 237], [237, 313], [313, 443], [443, 549], [549, 650]], "comments,_suggestions_and_typos": [[0, 10], [10, 13], [13, 152], [152, 278], [278, 337], [337, 405], [405, 416], [416, 419], [419, 476], [476, 503], [503, 506], [506, 613], [613, 620], [620, 695], [695, 737], [737, 815]]}}}, {"rid": "c73cf33735ae5af24fdde2536e815b0464984a17fc23086a6b1205cf754731a3100135f13508f7eb265e6160a4c51095f0c7b38bbfc971edb010117ba3fa9918", "reviewer": "Thomas Müller", "report": {"paper_summary": "The authors present a method for implementing transformers for long sequences.\nThe method is based on the finding that tokens in deeper layers of a transformer tend to have similar embeddings. This means that in cases where only the CLS token is used (classification or sentence embedding) tokens can be pruned from the deeper layers without much decrease in model quality.\nThe algorithm used for pruning the tokens is called core-set token selection.\nThe paper has a theorem that states that the loss is bound by the delta-cover core token selection algorithm in that the loss goes to zero as delta goes to zero.\nHowever, it remains somewhat unclear what this means in theory and practise. \nWhat guarantees does this give us in theory? \nHow does the actual implementation differ from the theoretical algorithm? \nIt seems that two potentially important aspects differ: That is the authors use fine-tuning instead of weighing the self attention and use a greedy approximation for the k-Center problem.\nIn general, I find the theorem hard to follow for an average reader of an ACL-like conference. \nBasic concepts such as lambda-Lipschitz should be introduced at least with the basic idea.\nSome parts could also be justified a bit more. For example, the phrase “and the generalization error of models like BERT is known to be small” should be justified with a citation or some explanation.\nThe method is evaluated using the GLUE and LRA benchmarks.\nFor GLUE, the author shows that their method out-performs various baselines at the same speed-up factor and stays behind 1.5 points in accuracy of an unpruned Bert-base model.\nThe authors also show that they can achieve the same average performance as the unpruned baseline at a 1.5X speed up.\nSome ensemble statistics (interquartile range or std deviation) would have been good in the tables to give the reader some idea of what the meaningful differences are in the experiments. \nOn LRA, the authors show that their method outperforms some of the baselines on three of the LRA tasks. Their method stays behind the unpruned model by 0.6 points in accuracy when using Big Bird and out-performs the unpruned baseline when using a Performers model.\nThe authors acknowledge that speed ups in these cases are insignificant because of the shallow nature of the networks used.\nThis makes one wonder if LRA is indeed a good evaluation setup for their method.\nThere is also a sentence that was a bit unclear to me:  L555: “Thus, we only reduce sequence length in the input layer, which is before the first encoder”. Is the statement only true for the baselines or also for the core set selection algorithm? ", "summary_of_strengths": "- The paper is overall well written and well motivated - Novel interesting method in a relevant area of NLP - Strong experimental results on GLUE (1.5X speed-up at no loss in quality (on average) and higher speed ups at reasonable drops in quality) - Theoretical justification of the method ", "summary_of_weaknesses": "- A bit unclear if LRA is actually a good evaluation setup for the method (no speed up are shown and the models used are too shallow) - The presentation of the theorem will be hard to follow for the average audience of an ACL conference. Key concepts should be introduced  - Unclear what the theorem means in theory and practice (what does the theorem gives us in practical terms, is it still valid in practice given the differences of the actual implementation) ", "comments,_suggestions_and_typos": "- Introduce key concepts in the discussion of the theorem - Explain what the theorem gives us in practical terms - Justify the differences from theoretical algorithm and practical implementation "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Thomas Müller, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 79], [79, 193], [193, 374], [374, 452], [452, 614], [614, 691], [691, 737], [737, 812], [812, 1001], [1001, 1096], [1096, 1188], [1188, 1235], [1235, 1388], [1388, 1447], [1447, 1623], [1623, 1741], [1741, 1928], [1928, 2033], [2033, 2194], [2194, 2318], [2318, 2399], [2399, 2555], [2555, 2646]], "summary_of_strengths": [[0, 55], [55, 108], [108, 249], [249, 291]], "summary_of_weaknesses": [[0, 134], [134, 238], [238, 273], [273, 463]], "comments,_suggestions_and_typos": [[0, 58], [58, 113], [113, 195]]}}}, {"rid": "6cdc13ea84ef615cd7e960e589a268df0d01d92068b8711c7aa570e9977e46aa8397e2a0d56caf9d9371023139c5b2b9c3f91ffc2d406fd6f51f7b1b80836e17", "reviewer": "Raphael Shu", "report": {"paper_summary": "In this paper, the authors proposes a new intermediate vector selection algorithm for BERT models, attempting to reduce the inference latency. The proposed core-set selection algorithm works right before the FF layer, which reduces the number of vectors in the input set according to a pre-defined length configuration. The intuition of of the algorithm is similar to DBSCAN with iterations, it starts with a core set containing only CLS embedding. Then it expands the core set by selecting m closest vectors to the core set. The expansion ends when the number of core-set reaching a certain number.\nIn experiments, the authors compare with the attention based method and first K selection method. The experiment results show that by fixing the speedup ratio at 3x, the proposed core-set selection algorithm shows advantage over other competitors.\nOverall, my opinion leans toward an acception. ", "summary_of_strengths": "- This authors put significant amount of content on theoretical justification for the proposed core-set selection algorithm - Results on LRA test set demonstrate that the proposed method can be used to further reduce the space complexity with big bird or performers ", "summary_of_weaknesses": "- Given this research topic, my first question would be: what is the upper bound / oracle performance for \"Select\", say, at the 3X speed up. If we have an oracle selection algorithm, can the performance be fully recovered?  In Table 1, the base average performance is 81.5%, and CS-opt achieves 80%. If the oracle performance is 80%, then we know a performance drop is inevitable, and this \"Select\" problem is fully solved by this paper. If the oracle performance is still 81.5%, then we can think there is still potential for further improvement. To me, such an study gives most valuable insights for understanding the evaluation results.\n- In table 1 and table 2, we can see CS-k-1 has a very close performance comparing to CS-opt. The only evaluation datapoints where CS-k-1 is significantly behind CS-opt is the SST-2 task in Table 1. However, if we go to Table 2, there is no difference between CS-k-1 and CS-opt results on SST-2. If the drop in Table 1 is caused by noise, then indeed CS-k-1 can be a much easier and simpler algorithm in terms of implementation.\n- In Table 4, the dataset that we can see significant gap between CS-opt and CS-k-1 is CIFAR-10 sequence-based classification task. However, I don't think it's proper to draw any key conclusion based on such a non-standard task. It itself is still a good toy task.\n- Overall, the intuition of the algorithm is to keep vectors close to the CLS embedding. I'm worrying that in some complex problems (e.g, QA domain), it might be really difficult for the model to determine the CLS embedding until the last layer. Selection in the first layer based on a noisy CLS embedding may cause the performance to degrade significantly. ", "comments,_suggestions_and_typos": "Line 247 \"However, if two or more tokens are exact duplicates of each other, then one can easily remove the duplicates from the input and modify the self-attention appropriately to get the same CLS embedding at the top layer, and hence the same prediction.\"\nIs this true? As the transformer has positional embedding and the self-attention layer is multi-head. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Raphael Shu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 143], [143, 320], [320, 449], [449, 526], [526, 600], [600, 698], [698, 848], [848, 895]], "summary_of_strengths": [[0, 124], [124, 266]], "summary_of_weaknesses": [[0, 141], [141, 223], [223, 224], [224, 300], [300, 438], [438, 548], [548, 640], [640, 734], [734, 839], [839, 936], [936, 1069], [1069, 1201], [1201, 1298], [1298, 1334], [1334, 1423], [1423, 1580], [1580, 1692]], "comments,_suggestions_and_typos": [[0, 258], [258, 272], [272, 360]]}}}, {"rid": "761253ace767fc010a488ba48756ad609cb1fbb8bb6b90dd6bb38679920b45b01c6710a90d6207eee6c354ef2838c12bb0173a52b91a2878008cc9625999b75e", "reviewer": null, "report": {"paper_summary": "This paper proposes a novel pyramid-BERT to achieve the sequence length reduction across different encoder layers, which benefits the memory and decoding time reduction for downstream classification and ranking tasks. This method is based on the core-set based token selection method which is justified by theoretical results. Experiments on GLUE benchmarks and Long Range Arena datasets demonstrate the effectiveness of the proposed method. ", "summary_of_strengths": "- This paper designs a novel speedup method with core-set based token selection which is justified by theoretical results.\n- Nice experiment results on GLUE benchmarks and Long Range Arena datasets. ", "summary_of_weaknesses": "- This paper lacks experiment comparisons with some very similar approaches, such as centroid transformers and representation pooling, although the authors claim that a thorough comparison is not required.\n- The proposed method seems not to improve the training speed of traditional BERT in the pre-training stage. It means that we only apply this method in the downstream tasks. I wonder about the effectiveness of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). ", "comments,_suggestions_and_typos": "This paper is well written and easy to follow. The authors explore to speed up BERT by selecting core-set tokens and reducing the sequence length across different encoder layers. This idea is interesting, and the motivation is reasonable.  My main concern is the experiment comparisons with similar approaches, such as centroid transformers and representation pooling. Although these methods do not release the code, I think this comparison could strengthen the effectiveness of the proposed method. \nBesides, this method seems to only fit downstream classification and ranking tasks. Thus, I wonder about the performance of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). If we adopt the way of dynamic early exiting, the sequence length reduction across different layers seems to be marginal somehow.\nThe token selection algorithm used in the paper also reminds me of another way that combines the selective encoding method and Gumbel-Softmax for BERT.  Each token predicts the probability of surviving at each layer and adopt the Gumbel-Softmax to obtain the final token that feeds to the next layer.  This method may enhance the robustness of the proposed method, since it traverses more different combinations. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 218], [218, 327], [327, 442]], "summary_of_strengths": [[0, 123], [123, 199]], "summary_of_weaknesses": [[0, 206], [206, 315], [315, 380], [380, 504]], "comments,_suggestions_and_typos": [[0, 47], [47, 179], [179, 239], [239, 240], [240, 369], [369, 500], [500, 585], [585, 713], [713, 843], [843, 995], [995, 1144], [1144, 1256]]}}}]
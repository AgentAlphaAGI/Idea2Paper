[{"rid": "c06c5336dbaf412ee7395c25aa3061dc3921e0460085fe6f98819b94402e329f794cb5c03978342933205cebef78da100b857aeb422856c92f9402cb501a0dc4", "reviewer": null, "report": {"paper_summary": "The authors aim to improve interpretability for structure and style control in knowledge-grounded conversational models. They propose to use two sequential latent variables for structure and style respectively.  1) m - binary indicator for segment boundaries within a sentence 2) z - style controller attribute to switch between content, knowledge, and style decoders. They use a variational framework for training using an evidence lower bound of the likelihood. Overall their encoder-decoder model outperforms the baselines in automatic and human metrics on two knowledge grounded dialog datasets WizardsOfWikipedia and CMU_DoG. Their models is also more robust and generalizable as it consistently outperforms the baselines with 10% or lower amount of in-domain data. While adding adapters to their decoders they can adjust the style (sentiment) of their responses while maintaining decent performance in automatic evaluation. Their metrics pklg and lklg suggest that their models can easily adapt the latent variables' distribution for different datasets. ", "summary_of_strengths": "- Interpretable model which predicts segment boundaries and is able to switch style decoders based on the context. Potentially useful for many applications.\n- Robust and generalizable model which can easily adapt to new styles with limited training data. ", "summary_of_weaknesses": "- The writing structure and flow can be improved. Many of the crucial details regarding automatic labeling, baselines, human evaluation results etc. are moved to appendix which disrupts the reading flow. ", "comments,_suggestions_and_typos": "- Their model shows improvement in low resource setting. But, it will be interesting to see what are the overall gains compared to the baselines with 100% of the in-domain data.\n- missing section in line 492 "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 121], [121, 211], [211, 369], [369, 464], [464, 771], [771, 930], [930, 1060]], "summary_of_strengths": [[0, 115], [115, 157], [157, 255]], "summary_of_weaknesses": [[0, 50], [50, 149], [149, 204]], "comments,_suggestions_and_typos": [[0, 57], [57, 178], [178, 208]]}}}, {"rid": "5dbabd3e6a6001f1e43cb8276d370cb4983d626289a5e91e6ff9479abeeb970446a4a817fd5a576b86599ec777caa72efc3e5488f484a6bdf700ece288664d82", "reviewer": "Yu Cao", "report": {"paper_summary": "This paper explores the knowledge-grounded conversation generation task. Two latent variables are employed for the controlling of types and boundaries for segments during generation, after an auxiliary task to classify segments into knowledge-relevant or knowledge-irrelevant. Such a strategy is fused into a pretrained encoder-decoder architecture, and realizes superior performance than strong baselines on two benchmark datasets. \nAlthough this paper is well-motivated, I do not see many significant revisions for the previous comments. ", "summary_of_strengths": "1. The motivation of this paper is solid, I believe the segmentation based on the relation of text pieces between knowledge is reasonable to solve knowledge-grounded dialogue generation tasks.\n2. The proposed model is basically complete and shows its superiority to strong baselines. It is based on a pre-trained BART model, and the proposed Module Indicator/ Boundary Indicator can help the base model to better combine information from both the context and knowledge in generated responses. ", "summary_of_weaknesses": "1. The writing still needs improvement. There are only some minor revisions compared to the last version, where they fix some typos, moving part of human evaluation results into the main part, and change few expressions to make them more clear. However, some major problems still exist. \n1) The descriptions of the model are difficult for readers to follow, including complex notations and some unclear definitions (e.g.,  the different styles mentioned). \n2) Although it adds more details about human evaluation, there is still no clear definition of metrics pklg and lklg. And I also have concerns about their justification.\n2. An ablation study is still missing. Without such analyses, readers will have no idea about how each component contributes to the final performance. E.g., how the style adapter affects the generated responses, or how it performs if one kind of latent is removed. ", "comments,_suggestions_and_typos": "Please check the weakness section.\n1. A case study with only one sample is insufficient for a competent paper in the dialogue area. Consider adding more samples and putting more highlights on the samples to show your superiority.\n2. Still missing some references that utilize pretrained model in an encoder-decoder architecture for auxiliary-information-grounded dialogue generation.  Typo: L492: Sec ?? "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yu Cao, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 73], [73, 277], [277, 433], [433, 540]], "summary_of_strengths": [[0, 3], [3, 193], [193, 196], [196, 284], [284, 493]], "summary_of_weaknesses": [[0, 3], [3, 40], [40, 245], [245, 287], [287, 456], [456, 575], [575, 627], [627, 630], [630, 666], [666, 778], [778, 892]], "comments,_suggestions_and_typos": [[0, 35], [35, 38], [38, 132], [132, 230], [230, 233], [233, 384], [384, 385], [385, 404]]}}}]
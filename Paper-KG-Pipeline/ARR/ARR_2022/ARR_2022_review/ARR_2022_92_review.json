[{"rid": "de6e9e582d788888209c33864f2c33f88969183462f118433c3de339f4ce516a54325de9505b8b1a5a2b61556ea1a770e0df1cc61d70950bf871a83727eb24a4", "reviewer": "Milad Alshomary", "report": {"paper_summary": "The general motivation of this paper is to provide tools that analyze pairs of review and rebuttal in the peer-review process of scientific work and assess Area Chairs in making sense of this discussion to reach better-informed decisions of acceptance or rejection. For this goal, the main contribution of this paper is a dataset of peer-reviews annotated with discourse labels and relations between the rebuttal and review pairs. The proposed discourse scheme attempts to unify several other discourse labels proposed in previous works. In my opinion, this scheme is complete and well organized on different levels of granularity. Although limited, the paper additionally presents some descriptive analyses of the collected annotations. ", "summary_of_strengths": "- The task is relevant to the community, and efforts towards easing the life of area chairs are appreciated.\n- The constructed discourse labels scheme is nicely detailed and complete.\n- The annotated dataset could benefit future research on discourse analysis of the peer review process ", "summary_of_weaknesses": "- The analysis section is limited and confusing in some parts: \t- Section 4.1 describes only the annotation results, without any further discussion of the implications.\n\t- In Section 4.2, based on Figure 2, the authors conclude that rebuttal sentences are not aligned with review sentences. However, what I see is completely different. The majority of review-rebuttal pairs happen to have a positive correlation of more than 50% unless I am missing something here!\n\t- Section 4.3 wasn't clear for me. I didn't understand how the figure supported the claim that authors often interpret reviews in a way that supports their argumentative goals. ", "comments,_suggestions_and_typos": "- There is missing work of Cheng et al. 2021 (Argument Pair Extraction via Attention guided Multi-Layer Multi-Cross Encoding) - It wasn't clear to me why the authors needed to build their own software for the annotation task. I think the task is similar to any other discourse labeling and relation identification task, and there are a lot of open-source tools that can be used for such annotations.\n- In the introduction, the authors mention something about annotators' training and calibration process, but this wasn't elaborated upon later. I guess some details on this process should be presented in the paper.\n- In Table 6, the header contains abbreviations. It is good to point out what do they mean. "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Milad Alshomary, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 266], [266, 431], [431, 538], [538, 632], [632, 738]], "summary_of_strengths": [[0, 109], [109, 184], [184, 287]], "summary_of_weaknesses": [[0, 63], [63, 169], [169, 291], [291, 336], [336, 465], [465, 501], [501, 643]], "comments,_suggestions_and_typos": [[0, 126], [126, 226], [226, 400], [400, 544], [544, 615], [615, 664], [664, 707]]}}}, {"rid": "eb650fa05410ca9417c00441a5b3f0ab2c0f012054bc332b6b71e12f20c6e3bd86a35f2929d7d8576cb7cd82ea438dd1501b3b750bc0263ddf275343ae614aaf", "reviewer": null, "report": {"paper_summary": "This paper presents a new dataset - DISAPERE, which includes discourse related annotations over scientific peer reviews and rebuttals. Each review is paired with the first rebuttal text. The authors develop four levels for review annotation: (1) review-action, (2) aspect, (3) polarity, and (4) fine-review action; and two levels for rebuttals: (1) argumentative and (2) non-argumentative ones. This dataset could help better characterize the intentions and interactions between reviewers and authors, which in turn can assist decision making for area chair.\nThe authors also tested two machine learning tasks: (1) sentence classification for the proposed schema, and (2) sentence ranking to determine the context mapping from rebuttal to review. Preliminary results show that pre-trained transformer models achieve moderate performance, therefore leaving room for future work. ", "summary_of_strengths": "1. This work releases a new dataset of 506 review-rebuttal pairs with sentence level annotation for discourse related aspects. \n1. The proposed taxonomy is comprehensive and captures various aspects of peer review text. \n1. The authors framed practical machine learning tasks over the dataset, and benchmarked performance of baseline transformer models. \n1. In the updated draft, the authors have accounted for the majority of the comments in the previous review. Overall the paper is more clear, and certain minor mistakes have been rectified. ", "summary_of_weaknesses": "N/A ", "comments,_suggestions_and_typos": "- The hyperlink for footnote 3 and 4 do not seem to work.  - Line 172: an argument level -> on argument level "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 135], [135, 187], [187, 395], [395, 559], [559, 747], [747, 878]], "summary_of_strengths": [[0, 3], [3, 127], [127, 131], [131, 220], [220, 224], [224, 354], [354, 358], [358, 464], [464, 545]], "summary_of_weaknesses": [[0, 4]], "comments,_suggestions_and_typos": [[0, 58], [58, 59], [59, 110]]}}}]
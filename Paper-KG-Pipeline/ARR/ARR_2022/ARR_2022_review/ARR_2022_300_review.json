[{"rid": "759b93caae756ac8b1f80110b9955e713039b05fe0fadb6e3ed4d5078bc3c54daa3eb092f79126acdb48ad3bbd451b28c456d516fc9ae2bde5b6761d51047921", "reviewer": "Yue Yu", "report": {"paper_summary": "This paper suggests a solution to tackle the practical challenges of using active learning (AL) for 2 NLP tasks. Specifically, the authors (1) propose to use a distilled version of a PLM during data collection and then leverage a larger model to annotate unlabeled examples for training on the (presumably more complex) successor model.  (2) propose UPS to reduce candidates for labeling. ", "summary_of_strengths": "1. The paper introduces an important problem: the inefficiency of data acquisition for AL and proposed a useful method. \n2. The author conducts experiments on different tasks/datasets. \n3. The paper is clear and easy to follow. \n4. I appreciate the authors for the efforts of adding extra experiments and writing the response letter. ", "summary_of_weaknesses": "1. In the previous rounds, the reviewer qXqD mentioned that there are no more explanations for the claim `PLASM can generalize to models that are not aligned with DistilBERT/BERT'.  The authors added experiments on XLNet, which partially solves this issue. But I expect the authors to give a deeper analysis for it. Also, it is better for authors to include textCNN/LSTM (1 of them is enough) for text classification, which can show whether non-pretrained models can be used under PLASM.\n2. The authors seems to leverage some existing techniques for AL methods, thus the technical novelty is not very high. ", "comments,_suggestions_and_typos": "- What's the time complexity of Tracin operation?\n- It would be even better to evaluate more datasets for text classification (e.g. SST-2 or TREC-6).  - In PLASM, the authors propose to use pseudo labeling to mitigate the ASM problem. Maybe I do not understand correctly, but using pseudo-labeling for AL is not a new concept. The are many works that leverage this technique for AL. Some relevant papers includes: (1) Semi-supervised active learning for sequence labeling; (2) Rethinking deep active learning: Using unlabeled data at model training; (3) ATM:  An Active self-training framework for label-efficient text classification.  The author could probably elaborate more on the details of pseudo-labeling. "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yue Yu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 113], [113, 337], [337, 389]], "summary_of_strengths": [[0, 3], [3, 120], [120, 124], [124, 185], [185, 189], [189, 228], [228, 232], [232, 334]], "summary_of_weaknesses": [[0, 3], [3, 181], [181, 257], [257, 316], [316, 488], [488, 491], [491, 607]], "comments,_suggestions_and_typos": [[0, 50], [50, 150], [150, 151], [151, 235], [235, 327], [327, 383], [383, 635], [635, 712]]}}}]
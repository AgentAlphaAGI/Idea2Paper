[{"rid": "aa6b6ff5226b0789a70bcf2ab038b9a89465cfe5ad981bef382ae7a1f352e0be4cd9caeaa18dbc82bb1bfc5e11e9ea276fb72c8184fa12f54398b2be1c70e449", "reviewer": null, "report": {"paper_summary": "This paper provides evidence for the hypothesis that learned representations in BERT and RoBERTa encode abstract information about argument structure. Evidence comes from two experimental methods loosely inspired by psycholinguistics: In experiment 1, inspired by the sentence sorting task, they find that similarity scores are higher for representations of sentences (i.e. an average of token embeddings) with the same argument structure (but different verbs) than for sentences with the same verb (but different argument structures). In experiment 2, inspired by jabbewocky priming studies, they find that representations of verbs placed into a novel (and usually ungrammatical or incoherent) syntactic frame have representations that more closely resemble verbs that canonically appear in that frame.  The authors argue that these results provide evidence for abstract constructions corresponding to different argument structures, as proposed in Construction Grammar. There is also evidence that abstractness of argument structure in representations increases as a function of the amount of pretraining: LMs with more pretraining data show a stronger tendency to group sentences by argument structure than by verb identity. ", "summary_of_strengths": "Overall, I think this is a nice paper, with a few clear opportunities for improvement.\n- The representation of argument structure is an underexplored topic in linguistic analysis of LMs in my opinion (considering its importance in linguistics), so this paper is helping to fill that gap.  - The results provide converging evidence that argument structure is encoded in contextual embeddings (though other interpretations remain possible).\n- The experimental paradigms are simple (both rely on comparisons of Euclidean distance in an embedding space), but novel in their choice of which quantities to compare.\n- The paper studies models trained both on different sized datasets and on different languages. This inspires greater confidence in the replicability of the results, and is useful from a cognitive modeling perspective. ", "summary_of_weaknesses": "My main concern is that the experiments do nothing to distinguish between the main claim that argument structure constructions (a la construction grammar) are encoded in models, from an alternative explanation that surface features correlated with (but certainly not) argument structure constructions are what these similarity metrics are picking up on. This is a pernicious problem with template data, but a serious one. For instance, only ditransitive sentences had two proper names (or two animate nouns), only resultatives had adjectives, and only caused-motion sentences had the word \"the\" twice (or two inanimate nouns). Several more such features can be thought of that would identify the template. If the confluence of these features has a greater impact on the sentence representation than the lexical identity of these features, we would still see that sentences with the same argument structure are ranked more similar, but it would not be evidence that an abstract construction is represented, thereby undermining the main claim. ", "comments,_suggestions_and_typos": "Suggestions To address the problem of surface features, there should be a set of control stimuli that preserve many of the relevant surface features while eliminating the possibility of an argument structure construction. This could come in a few forms: - Shuffling the words/phrases in the stimuli - Interleaving the words/phrases of the stimuli with a random word list If LMs still rank altered sentences generated from the same template closer than sentences with the same verb, then it is clearly the function words, number of phrases of a certain type, or order of phrases that determines similarity.\nLn 346: This method for producing sentence embeddings is not immediately problematic, but still just one of many ways that could affect results. What about repeating this at different layers (including the final layer), or studying [CLS] in BERT? Or S-BERT, which is pretrained to produce meaningful sentence embeddings?\nLn 489: Using just the first subword embedding is just one arbitrary choice of several. What about the last subword embedding, an average of all subword embeddings?\nComments - Ln 41: I don’t think these works assume a generative framework. They just acknowledge contrasts in acceptability (which are historically used as evidence in generative grammar…and in other frameworks).\n- Ln 139: This is an uncharitable misrepresentation of lexicalist theories. Most would not argue for multiple lexical entries for “sneeze”, but rather posit some kind of functional head or type shifter for introducing arguments.\n- L445: This experiment makes sense, but it has nothing to do with priming (except for the human study it’s inspired by).  - Ln 545: This is an excellent point, and if you choose to save this for future work, then you should acknowledge this limitation up front and hedge all claims about ASCs Style - Ln 78: “Neural reality” sounds vague and sensationalist (also confusing, as it sounds like it’s about the human brain) - Ln 221: Avoid factives (show that) when discussing experimental results - Ln 335: These models don’t really simulate nonnative English speakers. Nonnative English speakers are fluent in a non-English language, while the miniBERTas are not fully trained on their “L1”.\nQuestions - Ln 487: Why not also test BERT, miniBERTas, and non-English models?\n- Ln 496: How often is the congruent prototype the closest prototype for a particular Jabberwocky verb?\nOther related work - Constructions in MLMs: https://aclanthology.org/2020.conll-1.13/ - Argument structure in NNs: https://aclanthology.org/W19-0129/ "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 151], [151, 536], [536, 804], [804, 805], [805, 971], [971, 1227]], "summary_of_strengths": [[0, 87], [87, 288], [288, 289], [289, 439], [439, 609], [609, 705], [705, 828]], "summary_of_weaknesses": [[0, 354], [354, 422], [422, 627], [627, 706], [706, 1042]], "comments,_suggestions_and_typos": [[0, 12], [12, 222], [222, 254], [254, 299], [299, 371], [371, 606], [606, 751], [751, 853], [853, 927], [927, 1015], [1015, 1092], [1092, 1101], [1101, 1167], [1167, 1305], [1305, 1381], [1381, 1534], [1534, 1656], [1656, 1657], [1657, 1828], [1828, 1834], [1834, 1955], [1955, 2029], [2029, 2102], [2102, 2225], [2225, 2235], [2235, 2305], [2305, 2409], [2409, 2428], [2428, 2495], [2495, 2559]]}}}, {"rid": "8c890587897b7eb6684e2468cb6435d2844312c17e8335f883170fc270c6d418f05d5f6051cb0b4172aa8b08bf4bb0d92c6057d18108313a830726aa593c9bb2", "reviewer": null, "report": {"paper_summary": "The paper focuses on the topic of *argument structure constructions* (ASCs), an important topic in the framework of construction grammars. In construction grammar theory it is argued that the argument structure of a verb is not governed by the lexical entry of the verb itself, but by more abstract argument structure constructions that are independent of the verb. A rich body of work has addressed this topic and it has been demonstrated that humans process argument structure in a way that is explained well by the theory of ASCs.  The authors borrow several experiments from this body of (psycho)linguistic research to assess to what extent current language models comply with construction grammar theory. After a comprehensive background section they introduce a setup consisting of two experiments.  The first experiment is based on the sentence sorting task of Bencini and Goldberg (2020). In the original task 16 sentences are created by combining four verbs with four constructions, after which a participant is asked to sort them based on sentence meaning. If the sorting is done based on the construction type, and not on verb form, this is taken as evidence as abstract argument structure playing a more central role for sentence meaning than verb lexicality does. This paper applies this setup to LMs by clustering the 4x4 sentences based on their (averaged) sentence representations. It is found that all models that were considered (both mono- and multilingual) yield clusters that are closer to construction sorting than verb sorting. This is taken as first evidence of ASCs playing a role in LMs as well.\nThe second experiment is based on that of Johnson and Goldberg (2013). The original experiment utilized a priming-based task to probe for the reality of ASCs. A participant was provided a nonsense sentence of a certain construction, after which they were asked to determine whether a string of characters was a real English word or a non-word. It was shown that the response time for words that are considered prototypical for a certain construction type (e.g. _give_ for ditransitives) was significantly lower for congruent construction-word pairs. In this paper this setup is adapted by comparing the contextualised representation of a semantically implausible verb in a certain ASC with the averaged representation of a prototypical verb. By comparing the similarities of congruent and incongruent construction-verb pairs it can be shown that the argument structure is embedded within the representation of the nonsensical verb, and the authors find significant evidence for this to be the case. ", "summary_of_strengths": "The paper is structured clearly and well written, making it easy to follow the line of argumentation. This paper provides an interesting angle to the recent research direction of applying theories derived from (psycho)linguistics to neural LMs, and manages to find intriguing evidence as to what type of abstract linguistic information is encoded in the representations of a language model.  I expect this paper will open the door for exciting future work that will gain more elaborate insights into the mechanisms deployed by LMs to deal with structural, abstract features of language. The authors show that the rich body of work within psycholinguistics can be directly applied to the current generation of LMs, and I look forward to seeing this direction be explored in more detail by future work. ", "summary_of_weaknesses": "The amount of background provided can be reduced, and consists of quite a few detailed descriptions of topics and experiments that are not directly related to the experiments of the paper (e.g. the Priming paragraph at L210, Novel verbs paragraph at L224). The space that is currently occupied by this extensive background could be used more efficiently, and cutting it down a bit opens up space for additional experiments.  The ‘Jabberwocky constructions’ experiment is quite prone to several potential confounds that need to be explored in more detail in order to ensure that the current set of results truly hints at ‘the neural reality of argument structure constructions’. The fact that the contextualised embeddings of verbs in the same syntactic configuration is highly similar isn’t that surprising in itself (as is noted by the authors as well). The authors decided to drop the ‘priming’ component of the original paper in order to adapt the experiment to LMs, but there are other options that can be explored to align the setup more closely to the original (see section below for some ideas). ", "comments,_suggestions_and_typos": "### Comments / Questions - Could the results of the Sentence Sorting be driven by the fact that sentence embeddings are obtained by averaging over word embeddings? It seems that this procedure would be quite prone to simply cluster based on features stemming from individual tokens, instead of a more general abstract signal. I could imagine that in a BERT-like architecture the representation at the [CLS] position might serve as a sentence representation as well.\n- Alternatively, would it be possible to set up the sentence sorting experiment in such a way that the lexical overlap in between sentences is limited? This is common in structural priming experiments as well, and models are known to rely heavily on lexical heuristics. ,\n- Did you consider different measures of similarity in the Jabberwocky experiment? Euclidean distance might not be the most perfect measure for expressing similarity, and I would suggest looking into alternatives as well, like cosine similarity.  - A bit pedantic, but Jabberwocky words are non-existing nonce words, whereas the setup that the authors arrived at is only semantically nonsensical, yet still made up of existing words (a la ‘Colorless green ideas’). Referring to them as Jabberwocky (L.458) would give the impression of actually using nonce words.\n- How many ASCs have been argued to exist (within English)? Is there a reason why the 4 constructions used in Case Study 1 (_transitive, ditransitive, caused-motion, resultative_; L.165), are slightly different from Case Study 2 (_ditransitive, resultative, caused-motion, removal_; Table 2)?\n--- ### Suggestions: - L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n- I suggest not phrasing the Jabberwocky experiment as “probing” (L.444 ‘method to probe LMs’, L.465 ‘probing strategy’, etc.), given the connotation this term has with training small classifiers on top of a model’s hidden state representations.\n- Could the ‘priming’ aspect of Johnson and Goldberg (2013) in the Jabberwocky experiment perhaps be emulated more closely by framing it as a “Targeted Syntactic Evaluation” task (akin to Marvin & Linzen (2018), a.o.). In the context of priming, a similar setup has recently been utilised by [Sinclair et al. (2021)](https://arxiv.org/pdf/2109.14989.pdf). One could compare the probability of _P(gave | She traded her the epicenter. He)_ to that of _P(gave | He cut it seasonal. She)_, and likewise for the other 2 constructions. This way you wouldn’t run into the confounding issues that stem from using the contextualisation of ‘gave’.  - An additional experiment that might be interesting to explore is by probing for construction type across layers at the position of the verb. In the ‘Jabberwocky’ setup one would expect that at the word embedding level construction information can’t be present yet, but as it is contextualised more and more in each layer the ASC information is likely to increase gradually. Would also be interesting than to see how the curve of a jabberwocky verb compares to that of a sensical/prototypical verb (like _gave_): there is probably _some_ degree of argument structure already encoded in the word embedding there (as a lexicalist would argue), so I would expect probing performance for such verbs to be much higher at lower levels already.  - Adding to the previous point: probing in itself would not even be necessary to gain insight into the layerwise contextualisation; some of the current experiments could be conducted in such a fashion as well.\n--- ### Typos/Style: Very well written paper, no remarks here. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 139], [139, 366], [366, 534], [534, 535], [535, 710], [710, 805], [805, 806], [806, 897], [897, 1067], [1067, 1277], [1277, 1398], [1398, 1551], [1551, 1622], [1622, 1693], [1693, 1781], [1781, 1966], [1966, 2172], [2172, 2364], [2364, 2621]], "summary_of_strengths": [[0, 102], [102, 391], [391, 392], [392, 587], [587, 801]], "summary_of_weaknesses": [[0, 257], [257, 424], [424, 425], [425, 678], [678, 855], [855, 1103]], "comments,_suggestions_and_typos": [[0, 25], [25, 164], [164, 326], [326, 466], [466, 618], [618, 738], [738, 821], [821, 984], [984, 985], [985, 1203], [1203, 1301], [1301, 1361], [1361, 1594], [1594, 1598], [1598, 1615], [1615, 1698], [1698, 1888], [1888, 2016], [2016, 2134], [2134, 2353], [2353, 2490], [2490, 2567], [2567, 2613], [2613, 2664], [2664, 2772], [2772, 2773], [2773, 2916], [2916, 3149], [3149, 3512], [3512, 3513], [3513, 3723], [3723, 3727], [3727, 3786]]}}}, {"rid": "fb4a2b7c398bae50dfab314db4211f66db60cc40866617c48e02e1bec342456adc0f2832a39cd6e7b8a9a494d9414ef2150459f23195e539b540a290d5dc5418", "reviewer": null, "report": {"paper_summary": "The authors are presenting a paper on two multilingual probing tasks, e.g. sentence sorting and modeling of priming of Jabberwocky sentences, in order to test  the reality of argument constructions in the sentence embedding representations of modern pre-trained language models.  Several models trained on different amounts of data have been tested, with the goal of simulating different levels of proficiency in human speakers.\nIn the first sentence sorting experment, inspired by the experiment by Bencini and Goldberg (2000), the authors generate sentences by using verbs that are compatible with 4 different constructions and create sentence embeddings by averaging the token vectors. The results of vector clustering show that in all the tested languages sentences that share the same constructions are more similar than sentences sharing the same main verb, with the only exception of the LM model trained with the  smallest amount of data. Such a finding is certainly interesting as a construction-oriented preference in the original task was observed only in more proficient speakers.\nFor the Jabberwocky experiment, as a difference from the Johnson and Goldberg (2013) setting, sentences for each construction are generated by randomly filling real words in the templates. The contextual embeddings of such sentences are then compared to prototype embeddings of the constructions, which are obtained by averaging the verb embeddings of a prototypical verb for each construction. The authors found that for congruent prototype-Jabberwocky pairings distance are significantly lower, even in the scenario where construction prototypes are built by using relatively low frequency verbs.  The paper is overall well-structured and the experiments are clear and well-presented. Personally, I am not aware of any other work that probes contextualized word representations for argument constructions, and I appreciate that the authors take their inspiration directly from the psycholinguistic literature on the psychological reality of argument constructions. \nAlthough I would still have a couple of questions (see the section below), I feel I can say this work deserves publication. ", "summary_of_strengths": "The paper makes an original contribution to the literature on probing the contextualized representations of pre-trained language models. \nThe experimental hypotheses are clear and the experiments well-designed. ", "summary_of_weaknesses": "No important weaknesses, as far as I can see. ", "comments,_suggestions_and_typos": "QUESTIONS FOR THE AUTHORS - recently, [1] criticized the usage of standard similarity measures such as cosine and Euclidean distance with contextualized embeddings, given that such metrics might be dominated by a small number of outlier dimensions. Do you think this could be affecting your results? Have you thought about using  standardized metrics or a rank-based metric (e.g. Spearman correlation between vectors)?\n- Section 5.1: \"... in cases where a verb is split into multiple subwords, we take the embedding of the first subword token as the verb embedding\". I am not sure I understand the motivation of this step. Could you please elaborate more? Why not taking e.g. the average of the subword embeddings?\nTYPOS AND MINOR ISSUES: l. 264 LSTMS --> LSTMs l. 301 is most similar --> is the most similar EXTRA REFERENCES [1] Timkey and Van Schjindel, 2021. All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. Proceedings of EMNLP. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 279], [279, 429], [429, 689], [689, 947], [947, 1093], [1093, 1282], [1282, 1488], [1488, 1692], [1692, 1693], [1693, 1780], [1780, 2060], [2060, 2185]], "summary_of_strengths": [[0, 137], [137, 211]], "summary_of_weaknesses": [[0, 46]], "comments,_suggestions_and_typos": [[0, 26], [26, 249], [249, 300], [300, 419], [419, 567], [567, 623], [623, 656], [656, 715], [715, 739], [739, 809], [809, 826], [826, 862], [862, 966], [966, 988]]}}}]
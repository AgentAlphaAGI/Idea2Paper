[{"rid": "0805cef3514c6a8a408cb365fd19199aaee44bbc746bb7bc13c51afa2bd0cfdd0e9165c5a40142bf664015f97a59341d81ff0ce405993bbbbaeb8110390c3739", "reviewer": "Xuankai Chang", "report": {"paper_summary": "The paper explored the approaches to use external data to improve the spoken NER performance of both pipeline and E2E-based systems. These approaches include using unlabeled and text data to perform self-training, knowledge distillation and transfer learning. The experimental results showed that all these approaches lead to better performance in both pipeline and E2E-based spoken NER systems. ", "summary_of_strengths": "This work explores many possible approaches in exploring how to use external data for spoken NER task. It covers both the speech and text from data aspects. In terms of the training approaches, the authors tried the self-training, transfer learning and knowledge distillation. This work is meaningful to the community. ", "summary_of_weaknesses": "It is a bit confusing about the word error rates in Fig. 3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. ", "comments,_suggestions_and_typos": "Some grammar issues need to be reviewed. For example, at line 394 in section 4.2:           Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> \"we to ....\" "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Xuankai Chang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 133], [133, 260], [260, 396]], "summary_of_strengths": [[0, 103], [103, 157], [157, 277], [277, 319]], "summary_of_weaknesses": [[0, 60], [60, 129], [129, 195], [195, 247], [247, 292]], "comments,_suggestions_and_typos": [[0, 41], [41, 262]]}}}, {"rid": "a816dd1697f5524eb7e045216cd935593d31c1f257358ad67a2e01fbbaf92a12f666417c1c410aa8858ef985694d15444555daaf1179ec1b4b81fa3ca2adf65a", "reviewer": "Ahmet Üstün", "report": {"paper_summary": "This work focuses on spoken NER task and explores the use of external data in various form which is not annotated for the task such as plain speech audio, plain text, and speech with transcripts. The paper compares the pipeline approach and end-2-end approach to analyze how they benefit from external data. ", "summary_of_strengths": "S1: Impressive improvement in both E2E and pipeline models, up to 16% and 6% respectively.  S2: Experimenting with both E2E and pipeline approaches including respective baseline shows to impact of external data regardless of the approach.  S3: Error analysis in different forms gives some insight about where each approach (E2E or pipeline) differs or fails. ", "summary_of_weaknesses": "W1: Considering that E2E model benefits from mostly training over pseudo-labels (“distillation”), the resulting model actually represents another pipeline, which requires the text-NER. This hurts the main arguments favoring the E2E approach over the pipeline model.\nW2: For the E2E model, for methods that do not use pseudo-labeling, improvements are rather limited. This point needs to be discussed more comprehensively, especially to understand what makes the difference when looking at the comparison for “Un-Sp” external data type ", "comments,_suggestions_and_typos": "- Different fonts for the captions make it harder to read. I suggest using a consistent font for the caption and the main text.  - A sentence that introduces the findings is missing before enumerating points in line 433 - Positions of the figures are not fitting with the explanations, which makes the paper harder to read. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Ahmet Üstün, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 196], [196, 308]], "summary_of_strengths": [[0, 91], [91, 92], [92, 239], [239, 240], [240, 359]], "summary_of_weaknesses": [[0, 185], [185, 266], [266, 367], [367, 535]], "comments,_suggestions_and_typos": [[0, 59], [59, 128], [128, 129], [129, 220], [220, 324]]}}}]
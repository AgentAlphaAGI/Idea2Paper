[{"rid": "267d8cc21e6494de3e7f8dae6c4dac07596c44639abf6652d7d3ae071e4fb97395f8715a3ca6b660ea2aa51bd4f0a8a98d8d2059b958e941eab0e23eb1975655", "reviewer": "Aarne Talman", "report": {"paper_summary": "The paper investigates uncertainty estimation (UE) in two NLP task: named entity recognition and text classification. Uncertainty estimation is an important research area in many machine learning fields, however, as the authors point out, there has been little attention given to UE in natural language processing. The authors explain in detail UE methods and conduct extensive experiments comparing the different methods for a Transformer model trained for the selected tasks. The authors also introduce two new modifications to the existing approaches and show that they are able to improve the existing approaches. ", "summary_of_strengths": "The topic of uncertainty estimation is an interesting and important research area which has not received the attention it deserves in NLP. This paper provides a good overview of the different approaches and shows their relative strengths and weaknesses through extensive experiments.\nThe paper introduces some interesting modifications to the existing approaches (Mahalanobis distance and DPP Monte Carlo dropout) and the authors are able to show that the proposed modifications lead to state-of-the-art performance in most of the selected tasks. The proposed approaches are computationally cheap.\nThe authors connect their research well to previous work and, to the best of my knowledge, the related work section cites the relevant work sufficiently.\nThe paper is well written. The different sections are well structured and the explanation of technical details is clear and concise.\nAll-in-all this is a good paper. ", "summary_of_weaknesses": "The experiments are run using only one model (ELECTRA). It would be interesting to see some comparison between different models. ", "comments,_suggestions_and_typos": "Typo/gramma: Sentence starting on line 614: \"The proposed in this work method based on the Mahalanobis distance and spectral normalization of a weight matrix...\" "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "Maybe", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Aarne Talman, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 118], [118, 315], [315, 478], [478, 618]], "summary_of_strengths": [[0, 139], [139, 284], [284, 547], [547, 598], [598, 752], [752, 779], [779, 885], [885, 918]], "summary_of_weaknesses": [[0, 56], [56, 129]], "comments,_suggestions_and_typos": [[0, 162]]}}}]
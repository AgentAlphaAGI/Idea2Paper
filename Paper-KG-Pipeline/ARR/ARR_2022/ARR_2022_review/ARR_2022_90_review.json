[{"rid": "a13160e3b784b9fddc636569f73cd6ea60629b576b812f9f4363151304c642baac3520c486f1d36ca31414d89b9b50645cfbf6a7911fe45a43b78f3fc14e5fb0", "reviewer": "Ruotian Luo", "report": {"paper_summary": "This paper studies how the vision model impacts the performance of multimodal translation through multiple probing experiments. The authors experiment with the recent ViT models trained on image classification as well as DETR trained on object detection and CATR trained on image captioning. The authors find that stronger vision models can improve probing tasks performance while not helping much on standard MMT. ", "summary_of_strengths": "- The authors propose several probing tasks complementary to Caglayan et al. - The authors show that, while the benefit of a stronger vision model is not obvious under standard MMT task, the vision model can help probing tasks.\n- The authors propose to use selective attention combined with gated fusion to fuse vision features and source text. Such fusion can improve the performance of the probing tasks.\n- The authors provided a very detailed analysis of different components of their methods. ", "summary_of_weaknesses": "- The selective attention is rather straightforward.\n- The motivation of proposing three new probing tasks is not obvious to me. The authors can just simply use the tasks in Caglayan et al. And also the color-based probing is almost the same as the Color Deprivation in Caglayan et al. ", "comments,_suggestions_and_typos": "A general comment: For MMT task, it is already known that the vision model does not contribute much to the standard MMT task. It seems quite artificial to me to create these probing tasks to draw conclusions for MMT. Since we know a stronger vision model helps image captioning, so it is foreseeable that a stronger vision model is helpful when visual information is missing. Can the authors elaborate on how significant the conclusion is and why we need such deliberately designed probing tasks? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Ruotian Luo, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 128], [128, 292], [292, 415]], "summary_of_strengths": [[0, 77], [77, 228], [228, 345], [345, 407], [407, 497]], "summary_of_weaknesses": [[0, 53], [53, 129], [129, 286]], "comments,_suggestions_and_typos": [[0, 126], [126, 217], [217, 376], [376, 497]]}}}, {"rid": "f9a7f56bb8afd06bf59e954ee57ca25e221fa72043b16926b31c13b979b33be207b0da6c838a1b5d93daf2141ed3f99b6f9a7f6df39df61701c2e11d2e34d7d0", "reviewer": "Dong Yu", "report": {"paper_summary": "The paper adopts stronger vision models, such as Vision Transformer to represent images, and it investigates whether stronger vision models lead to better performance in MMT tasks. A selective attention mechanism is proposed for Vision Transformer to correlate words with image patches. The paper also conducts multiple probing experiments to explore the effectiveness of stronger visual features and different learning objectives. ", "summary_of_strengths": "Strength: 1.Popular and stronger models in CV are introduced in the paper, and a corresponding attention method is proposed to utilize the patch-level image representation. The paper discusses whether stronger visual features help improve the performance of MMT, and the answer is positive.\n2.The experiments are conducted on not only the original task, but on multiple probing tasks to demonstrate the effectiveness of the selective attention method. Though improvements brought by ViT with the selective attention are not significant enough, the method still outperforms previous baselines when inputs have masked words. ", "summary_of_weaknesses": "Weakness 1.The figure descriptions need to be more specific. For example, what do the labels, “mask-1”, “mask-2” and so on mean? I think “the limited textual context” should be clearer that it refers to “noun-based probing”. ", "comments,_suggestions_and_typos": "Does this selective attention only work on patch-based vision models? And why use a single-head attention? Do the heads of attention affect the model performance? "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Dong Yu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 181], [181, 287], [287, 432]], "summary_of_strengths": [[0, 173], [173, 291], [291, 452], [452, 623]], "summary_of_weaknesses": [[0, 61], [61, 129], [129, 225]], "comments,_suggestions_and_typos": [[0, 70], [70, 107], [107, 163]]}}}]
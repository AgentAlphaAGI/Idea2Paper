[{"rid": "8cdbb2b513520303dc39890672a3088db78f2234c8bb6d933c898b0961a7497b997110acfcc767fa821dc2ae5ca2b15c3e743768e9b1caa9688c9924900d078a", "reviewer": null, "report": {"paper_summary": "The paper proposes a minor improvement to the existing word sense disambiguation approaches. Its aim is to overcome the problem of training data imbalance (not all words have the same number of senses, and this is related to their frequencies). \nEssentially, the authors assign different weights to words during training, depending on their frequencies. Empirically, this results in improvements in WSD performance on several English datasets. ", "summary_of_strengths": "The proposed re-weighting approach is simple and focused, and the improvements are persuasive. I really like that it is linguistically motivated (based on Zipf's law) . ", "summary_of_weaknesses": "- Although the proposed approach does bring WSD improvements, it is rather incremental. This is probably not a \"weakness\" per se, just that the paper is not an eye-opener.  - The evaluation is done on English data only, which leaves some doubts about how this would work with other languages. ", "comments,_suggestions_and_typos": "L. 135: \" linguistic law exists in many corpora\" --> probably  \"linguistic law is manifested in many corpora\"? The whole statement is a bit strange: Zipf's law is well known to be manifested not \"in many corpora\", but in _all_ human languages, and thus, in any imaginable corpus of natural language texts. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 93], [93, 245], [245, 354], [354, 444]], "summary_of_strengths": [[0, 95], [95, 169]], "summary_of_weaknesses": [[0, 88], [88, 172], [172, 173], [173, 293]], "comments,_suggestions_and_typos": [[0, 111], [111, 306]]}}}, {"rid": "f4fd93843b665513df9be69b38341a6407312cea1e77fad05cc54c07c73031dab1459620984100d59aa8d1b90d9712d1ab1de2efdc7887b20ff3f33e13bff393", "reviewer": null, "report": {"paper_summary": "See the prior review for a summary. Based upon the author response I do raise my score slightly from 2.5 to 3.0 to reflect that the definitions referenced in the author response might be sufficient for a target audience that is intimately familiar with WSD. On the other hand, it remains open as to what the impact of the proposed approach would be on any of the noted downstream applications, or beyond English. While WSD can be considered part of the traditional NLP preprocessing pipeline, it's impact on modern end-to-end solution is likely small. Nevertheless there might be high-impact cases such as token-based retrieval (which is used widely), and investigating the impact of the proposed approach on such applications might provide a convincing data point that can provide evidence for the impact of the proposed work. ", "summary_of_strengths": "See the prior review. ", "summary_of_weaknesses": "See the prior review. ", "comments,_suggestions_and_typos": "See the prior review. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 36], [36, 258], [258, 413], [413, 552], [552, 828]], "summary_of_strengths": [[0, 22]], "summary_of_weaknesses": [[0, 22]], "comments,_suggestions_and_typos": [[0, 22]]}}}]
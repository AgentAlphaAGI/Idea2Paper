[{"rid": "15cebc13f46983afc2df307e9543c3de49d4f0bea1adcfbbe1b003ca287f1511cb80721c1ccbf929caad0b05394782fd9e3523bae2ebde66a10284a643bdf345", "reviewer": null, "report": {"paper_summary": "This work presents a series of research works and their debates around the question \"Is Attention Explanation?\". This work summarizes past research works in a progressively deeper way, from the \"what\" to the \"why\" to the \"how\". Finally, the authors then give a discussion and conclusion for follow-up researchers. ", "summary_of_strengths": "1. The authors present the debate using a line of thinking like a human in this paper. This is certainly a good way to do it, but it is not an efficient way either. \n2. This work is believed to have taken a lot of effort and the motivation of the paper is to be valuable to the community. ", "summary_of_weaknesses": "1. Unfortunately, the paper does not give enough in-depth analysis nor does it indicate some definite future research lines. \n2. And the discussion of the impact of different tasks in this paper is not sufficiently informative and does not fit well with other sections. ", "comments,_suggestions_and_typos": "A guide or summary table can be used to help readers better and more quickly understand past research works. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 113], [113, 228], [228, 314]], "summary_of_strengths": [[0, 3], [3, 87], [87, 165], [165, 169], [169, 289]], "summary_of_weaknesses": [[0, 3], [3, 125], [125, 129], [129, 270]], "comments,_suggestions_and_typos": [[0, 109]]}}}, {"rid": "a872bc9c1e4be3ba277d4b095016a806f5abc5ec21c5c1dd5462423499bac08928b1c11136ec540f6eacf9898531eb6456f931c2bc0236284b4991cf4f90b2ad", "reviewer": null, "report": {"paper_summary": "The main contribution of this paper is an extensive survey of works since 2019 that study the questions around the explanatory power of attention mechanisms. The authors consider the works that augment the analysis done by Jain & Wallace, 19 and Wiegreffe & Pinter 19.  The survey divides the papers studied under following questions -  1. Works that agree/disagree with J&W and W&P conclusions. \n2. Works trying to explain why attention is/is not useful as explanations 3. Works that extend the experiments to other structured predictions tasks + global analyses 4. Evaluation issues with respect to methodology used by J&W and W&P and evaluation using humans 5. Mitigation Strategies on how to make attention more faithful ", "summary_of_strengths": "The main strengths of the work is two-fold - 1. Current literature on attention as explanation debate is spread across large number of works. Previous surveys tend to focus on attention mechanisms themselves and not the specific explanation debate surrounding. This work fills in a much needed gap for an easy introduction to works in this are -- for both outsiders and people who are part of it.\n2. The survey covers the debate extensively from multiple angles -- works that directly build on J&W and W&P and works that either provide theoretical analysis, critic of these works and mitigation strategies.\n3. Clean Organization of previous literature into helpful coherent themes helps with readability and situate the works in perspective of each other. ", "summary_of_weaknesses": "While I don't see any large weaknesses in this work, there are some things that can be updated -  1. Due to space limitations, parts of the survey tend to very quickly skim over individual papers that does particularly help with understanding their main goals. This is specifically a issue in Section 9.1 and the authors may consider expanding there if the paper is accepted to a conference that provide additional pages.\n2. A main thread of work in this area is how attention correlate with other feature attribution methods. High correlation tend to be provided as evidence of better explanation but this may not be true. In general, other feature attribution methods (which may be considered more faithful) like gradient, LIME, SHAP still have blindspots and can provide unintuitive explanations (See Pathologies of gradient based explanations by Eric Wallace or LIME and SHAP fooling papers). A small discussion of this aspect of the attention as explanation debate would be useful. ", "comments,_suggestions_and_typos": "--NA-- "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 158], [158, 269], [269, 270], [270, 340], [340, 396], [396, 400], [400, 474], [474, 567], [567, 664], [664, 725]], "summary_of_strengths": [[0, 45], [45, 48], [48, 142], [142, 261], [261, 397], [397, 400], [400, 607], [607, 610], [610, 756]], "summary_of_weaknesses": [[0, 98], [98, 101], [101, 261], [261, 422], [422, 425], [425, 527], [527, 624], [624, 897], [897, 987]], "comments,_suggestions_and_typos": [[0, 7]]}}}]
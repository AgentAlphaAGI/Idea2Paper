[{"rid": "6187bd8a3be2835d889ed91c85d27c1ab8c74d9fbd1fbb729dbd80786b1c5954748b996a2d72a5cbf4f44b046ba4934a5528b78c3340aeb121f7864f4a7d4f79", "reviewer": null, "report": {"paper_summary": "This work studies using human feedback as contextual bandit learning to improve QA models. The user interacts with a QA model trained initially with conventional supervised learning, providing binary feedback (correct v.s incorrect) to judge the correctness of returned answers. Notably, the author does not really deploy the model. Instead, they derive pseudo-feedbacks from gold labels, which causes a significant gap between their simulation and real-world interaction.  After collecting feedback through simulation, they fine-tune the deployed QA model by maximizing the likelihood of predicted answer with positive feedback and minimizing the likelihood of the one with negative feedback. They consider two learning settings, online and offline learning.\nWithin this framework, they conduct experiments on 6 MRC datasets. The result shows that a QA model initialized with a small number of labeled training data can be improved by a large margin of accuracy after being fine-tuned with derived feedback data. When trained with full training sets in source domains, the QA model can also be adapted to target domains, where only user feedback is available.\nThe author has discussed related work on using interactive feedback for other NLP tasks. Although the author claims that their work is the first for QA, there is significant overlap between this work and Campos et al. (2020), which also uses binary user feedback to improve a Conversational QA model. But it is not cited in the paper.\nReferences: Campos, Jon Ander, et al. \"Improving Conversational Question Answering Systems after Deployment using Feedback-Weighted Learning.\" Proceedings of the 28th International Conference on Computational Linguistics. 2020. ", "summary_of_strengths": "1. This work is well motivated by using human feedback to improve QA models. \n2. Although the employed baseline methods are kind of weak, the improvement is significant. \n3. The paper is well written and easy to follow. ", "summary_of_weaknesses": "1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that “Tokyo” is definitely a wrong answer to the question “What is the capital of South Africa?”. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data? \nIn conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail. \nTherefore, the experiment results cannot support the claim made by authors.\n2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback. \nMoreover,  Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers.  3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. ", "comments,_suggestions_and_typos": "Line 277: “The may be attributed…” -> “This may be attributed… "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 91], [91, 279], [279, 333], [333, 473], [473, 474], [474, 694], [694, 760], [760, 827], [827, 1014], [1014, 1161], [1161, 1250], [1250, 1462], [1462, 1496], [1496, 1508], [1508, 1639], [1639, 1718], [1718, 1724]], "summary_of_strengths": [[0, 3], [3, 77], [77, 81], [81, 170], [170, 174], [174, 220]], "summary_of_weaknesses": [[0, 3], [3, 84], [84, 181], [181, 270], [270, 395], [395, 528], [528, 570], [570, 644], [644, 822], [822, 929], [929, 1006], [1006, 1009], [1009, 1059], [1059, 1178], [1178, 1295], [1295, 1425], [1425, 1587], [1587, 1588], [1588, 1591], [1591, 1629], [1629, 1734], [1734, 1846]], "comments,_suggestions_and_typos": [[0, 63]]}}}, {"rid": "6948377b128ce8c60c32a0fc8cf7ffe45ae2fc1ded70405ea0ede6d577ec7fa193db3011dc9221756d0eaa8b73003124d81f069f9da16fe2d894c05637eefab9", "reviewer": null, "report": {"paper_summary": "This paper focuses on using bandit learning to learn from user feedback for Extractive QA (EQA), the binary supervisory signals from user feedback serve as rewards pushing QA systems to evolve. The learning algorithm aims to maximise the rewards of all QA examples, which consists of online learning and offline learning, the online learning receives user feedback and updates model parameters after seeing one QA example, whereas offline learning updates model parameters after seeing all QA examples.  The experimental results on QA datasets from MRQA support the effectiveness of the proposed bandit learning approach, proving that the proposed approach can consistently improve model’s performance on SQuAD, HotpotQA and NQ in in-domain experiments under online learning especially when there are extremely little QA examples available for SQuAD. Besides, a set of experiments are conducted to investigate the difference between online learning and offline learning, and the importance of model initialisation in the proposed bandit learning approach. ", "summary_of_strengths": "1. The proposed bandit learning approach that learns from user feedback for EQA is novel, which simulates real deployment environment and provides insights for further exploration in bridging the gap between QA model training and deployment. \n2. Empirical results show the effectiveness of the proposed approach, especially the in-domain experimental results for online learning. \n3. Conducting extensive experiments studying the effect of domain transfer and model initialisation. ", "summary_of_weaknesses": "1. The binary reward from user feedback is weak due to the large search space for EQA, resulting in the incapability of providing precise supervisory signals. Need to design a more sophisticated reward. \n2. The proposed approach heavily relies on how accurate the initial model is, which means it is highly sensitive to model initialisation, limiting its usefullness. \n3. In in-domain experiments of online and offline learning, bandit learning approach hurts model’s performance under some scenarios especially for TriviaQA and SearchQA. \n4. Some other papers of learning from feedback for QA should be compared, such as Learning by Asking Questions, Misra et al. CVPR 2017. ", "comments,_suggestions_and_typos": "Questions:      1. Why only use single-pass in online learning? "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 194], [194, 503], [503, 851], [851, 1056]], "summary_of_strengths": [[0, 3], [3, 242], [242, 246], [246, 380], [380, 384], [384, 482]], "summary_of_weaknesses": [[0, 3], [3, 159], [159, 203], [203, 207], [207, 368], [368, 372], [372, 539], [539, 543], [543, 676]], "comments,_suggestions_and_typos": [[0, 19], [19, 64]]}}}]
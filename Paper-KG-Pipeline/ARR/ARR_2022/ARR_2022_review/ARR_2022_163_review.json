[{"rid": "043aba43da4da9c0ab308268b8ace3bbc8c1ac766b70111814b13fb419cb7b1fdfe563dfa98d1263c3c060b2463526f656e0bddd2544c15ee19026645c76bcce", "reviewer": null, "report": {"paper_summary": "This paper proposes a solution for \"Contrastive Conflicts\". What exactly are “Contrastive Conflicts”? They occur when multiple questions are derived from a passage, each with different semantics. The questions are going to be close to the passage in representation space and by transitivity they are going to be close among themselves even though they are semantically different (Transitivity Conflict). In addition to this, if multiple questions derived from the same passage are in the same training batch, then the questions will see that passage as both positive and negative (In-Batch Conflict).\nThe solution proposed by the paper is to use smaller granularity units, i.e. contextualized sentences. Per sentence representations are computed by using per sentence special indicator tokens, then a similar approach to DPR is used to finetune sentence representations. Because different questions have answers in different sentences the contrastive conflict is generally resolved.\nImprovements are reported on NQ, TriviaQA and SQuAD, especially on SQuAD where conflicts are reported to be severe (i.e. often multiple different questions are extracted from the same passage). Extensive experiments show that the method does well even in transfer learning. ", "summary_of_strengths": "Strengths: - The paper obtains small but convincing improvements on NQ and TriviaQA, and large but a bit puzzling results on SQuAD (considering that one of the baselines does not match the DPR paper and that SQuAD can benefit dramatically from combining DPR with BM25, but it is not done in this paper).\n- The paper presents many interesting ablations and transfer learning experiments that help further convince the reader of the efficacy of the method. ", "summary_of_weaknesses": "Weaknesses: - Retrieving (avg # sentences) * 100 sentences (see section 3.3) instead of just 100 sentences seems to be a bit of a cheat. For a strict comparison to DPR, Top-20 and Top-100 performance should be reported with exactly those numbers of retrieved elements and without post-processing on larger sets of retrieved passages. One could argue that allowing for more expensive passage retrieval is what is giving the improvements in this paper, other than for SQuAD where the lower granularity does seem to be helping, except it doesn’t help as much as BM25.\n- The idea of having more granular representations for passage retrieval is far from new. The authors do cite DensePhrases (Lee et al. 2021), but don’t mention that it’s already at lower granularity than passage level. They could also cite for example ColBERT (Khattab et al. 2021).\n- The big improvement reported in Table 2 for SQuAD “Single” is a bit confusing since it relies on a Top-20 number that is much lower that what is reported on the DPR paper (although this seems to be a common problem). On the positive side, the number reported for SQuAD “Multi” matches the DPR paper. ", "comments,_suggestions_and_typos": "Suggestions: - Line 91: the authors claim that contrastive conflicts are *the* cause for bad performance on SQuAD, but the statement seems unjustified at that point. It might make sense to refer to later results in the paper. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 60], [60, 102], [102, 196], [196, 404], [404, 601], [601, 704], [704, 871], [871, 983], [983, 1177], [1177, 1257]], "summary_of_strengths": [[0, 11], [11, 304], [304, 455]], "summary_of_weaknesses": [[0, 12], [12, 137], [137, 334], [334, 565], [565, 655], [655, 784], [784, 848], [848, 1067], [1067, 1150]], "comments,_suggestions_and_typos": [[0, 13], [13, 166], [166, 226]]}}}]
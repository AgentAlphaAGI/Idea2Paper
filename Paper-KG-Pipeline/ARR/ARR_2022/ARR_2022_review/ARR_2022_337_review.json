[{"rid": "de3c8e7b1b41f35cbd5e964397fef97e665ad35d2f3b989493cecb77ee77776a9e0a300d36f7c3c2c4cbc561b69cec073be42cf033541305b52d94cc9c126e40", "reviewer": null, "report": {"paper_summary": "This paper mainly investigates prompt-based finetuning in few-shot learning. It demonstrates two strategies that can reduce the requirement for prompts design and memory cost: null prompts tuning and that it can be made memory efficient by finetuning only the bias terms. ", "summary_of_strengths": "1. It demonstrates that prompt-based finetuning can be achieved via null prompts, that alleviates the effort of designing prompt patterns across different tasks/datasets, which is more generalizable, and questions the requirement of well designed prompt patterns. \n2. It demonstrates that by only finetuning the bias term, the finetuning process can be highly memory efficient, with only updating 0.1% of the parameters. ", "summary_of_weaknesses": "1. From the results in Table 2, it shows that in ALBERT, with null prompt, the performance largely decreases (from 8 #Wins to 1), which raises the question of the necessity and robustness of null prompting. Moreover, it shows that prompt-based finetuning with BitFit can achieve comparable or even better performance, with memory efficiency introduced. It suggests that only using BitFit is a more wise choice. ", "comments,_suggestions_and_typos": "1. It may be better to conduct more experiments across different models. \n2. It may be better to analyze the principle of the success of null prompts instead of only experimental evaluations. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 77], [77, 272]], "summary_of_strengths": [[0, 3], [3, 264], [264, 268], [268, 421]], "summary_of_weaknesses": [[0, 3], [3, 207], [207, 353], [353, 411]], "comments,_suggestions_and_typos": [[0, 3], [3, 73], [73, 77], [77, 192]]}}}, {"rid": "1a0b7f5493a56decef8b7002e14fb55b8408d53ca9ff0d2c1e8ff795023714924a59741178c1deef36469781120330485cbb92e8f3dfaad6d3b6127159c51e54", "reviewer": "Urmish Thakker", "report": {"paper_summary": "This paper tries to simplify the process of prompt engineering for few-shot learning. Specifically, this paper tries to resolve two specific aspects of prompt engineering - 1. How to chose the right prompt? \n2. How to ensure efficient deployment for prompt tuned networks Choosing the right prompt? \nPrompt engineering while successful, requires significant tuning to identify the right prompt. While AutoPrompt methods exist, they start showing benefits only at scale. This paper shows that the complexity around prompt choices can be easily managed by finetuning on Null prompt Efficient deployment? \nFinetuning on null prompt leads to creation of a new model for every task. The authors further use BitFit to reduce the per task parameters for efficient deployment. ", "summary_of_strengths": "The results in the paper show some interesting insights that have not been covered by prior work -  1. Show that parameter efficient fine-tuning techniques like BitFit and Adaptors can work for prompt-tuning in few-shot regime. For example, the original BitFit paper uses the entire training dataset for finetuning. This paper uses 2*K where K=4,8,16 examples from each downstrean tasks for prompt-tuning. Showing BitFit and other parameter efficient techniques work in such few-shot scenarios is an interesting experiment that the community will find useful 2. The ability of null prompts with finetuning to simplify prompt engineering for LM to the scale of 300M parameters will also be a technique of interest of various practitioners in this domain ", "summary_of_weaknesses": "Key weakness - 1. Results covered in existing literature - The key insight of stability after prompt fine-tuning is already covered in prior work (https://aclanthology.org/2021.naacl-main.208.pdf). The authors acknowledge this work and call it simultaneous publication. However, the paper first appeared in Mar 2021 on arxiv and was published at NAACL in Jun 2021. Given that ACL guidelines sets contemporaneous publication limit to 3 months from submission (https://www.aclweb.org/adminwiki/index.php?title=ACL_Policies_for_Submission,_Review_and_Citation), I am not sure if their claims of simultaneous publication is still valid.\n2. I think the writing of the paper can be improved. The paper does not lay down its claims concretely or can be confusing at times. Some examples below -       1. Given that the results in this paper may not scale to larger LM, I think the paper should mention the LM targeted explicitly in Abstract. As a reader, I was looking to understand how this paper compares to AutoPrompt techniques, given that they are also driven by the motivation of simplifying prompt engineering. While the authors compare with this work, they dont talk about it till section 5 (lines 86-88 dont give enough context). \n     2. It can be unclear at times as to what they mean by standard fine-tuning - finetuning all parameters across entire dataset or fine-tuning all parameters for a sampled dataset. This can create confusion while reading the paper 3. Unclear if the results are overfitting to the dataset sampled- In a few shot scenario, its very easy to overfit to the examples sampled. How do these results vary when we sample a different set of \"K\" samples from the dataset? How does it change with a skewed label distribution in the sampled dataset? Experiments around these questions help understand the generalizability of the technique.\n4. The authors talk about null prompt based fine-tuning being more robust (line 91), but do not define robustness in their paper nor show results justifying it.\nQuestions for the authors: 1. What is the value of k in Table 2? \n2. In line 84, when you say \"better few-shot accuracy than standard fine-tuning\" did you mean fine-tuning over the entire dataset or fine-tuning in the few shot scenario? ", "comments,_suggestions_and_typos": "NA "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Urmish Thakker, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 86], [86, 176], [176, 207], [207, 211], [211, 272], [272, 299], [299, 395], [395, 470], [470, 580], [580, 602], [602, 678], [678, 769]], "summary_of_strengths": [[0, 100], [100, 103], [103, 228], [228, 316], [316, 406], [406, 559], [559, 562], [562, 753]], "summary_of_weaknesses": [[0, 18], [18, 198], [198, 270], [270, 365], [365, 633], [633, 636], [636, 686], [686, 766], [766, 797], [797, 935], [935, 1111], [1111, 1232], [1232, 1241], [1241, 1416], [1416, 1466], [1466, 1469], [1469, 1606], [1606, 1696], [1696, 1772], [1772, 1862], [1862, 1865], [1865, 2023], [2023, 2050], [2050, 2053], [2053, 2088], [2088, 2092], [2092, 2260]], "comments,_suggestions_and_typos": [[0, 3]]}}}]
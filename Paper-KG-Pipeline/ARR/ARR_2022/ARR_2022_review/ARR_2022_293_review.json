[{"rid": "616f1e9160debea4910d2e381d7793d0fdf92e90a76420060d99211d5c9fea7770d1ef76fde0ec0be8ad55122fb82e378ea9c27f5f8749cfe3c90de2e692badc", "reviewer": null, "report": {"paper_summary": "The paper presents a novel approach to understanding math problems from their textual formulations. The approach builds on those from related work, choosing syntactic representations. The key novelties are (1) an internal graph representation of the operators and (2) a novel pretraining setting. The model achieves vast improvements over prior art. ", "summary_of_strengths": "The new model addresses several key problems of previous work and appears to contribute a very logically motivated extension, modeling the structure of the required mathematical operations. The model description is clear and the experimental setup and results are reasonably clear and allow for an easy comparison with related work. There is also an ablation study to analyze the contribution of the individual components of the model.  The paper is easy to read. ", "summary_of_weaknesses": "The model section seems to lack comparison with prior work. It is not entirely clear what is novel here and what is taken from prior work. It is also not entirely clear to me if pretraining is performed with data from all tasks and whether the same setup had been used previously. If this is different from prior work, that would be unfair and a major flaw. ", "comments,_suggestions_and_typos": "I'd like to see my doubt about the pretraining cleared up. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 100], [100, 184], [184, 297], [297, 350]], "summary_of_strengths": [[0, 190], [190, 333], [333, 436], [436, 437], [437, 464]], "summary_of_weaknesses": [[0, 60], [60, 139], [139, 281], [281, 358]], "comments,_suggestions_and_typos": [[0, 59]]}}}, {"rid": "346582f5d9e2b75a39a4711cc113a66ae780133986cf48f339cda5219e5808715b9664ec085a5bf6912326817edf962bc9a97367731d58f60bcb399c7603e29b", "reviewer": "Maria Leonor Pacheco", "report": {"paper_summary": "This paper presents COMUS, a method to fuse the representations of the text and the graphical mathematical syntax for math understanding tasks. The method consists of constructing a math syntax graph, and leverage large language models and graph attention networks to represent the language and the symbolic structure, respectively. It proposes a syntax-aware memory network to capture the interactions between the text and formula representations, and it explores the use of pre-training tasks to 1) improve the language representation, 2) improve the graphical representation, and 3) combine them. They show that their method outperforms text-based and graph-based methods, both with and without pre-training, as well as competitive methods designed for math problems (i.e. MathBert) across four tasks in the math domain. The paper presents an ablation study to test the impact of each of the components of the proposed method, as well as experiments that show that the improvement is stable in the low supervision case. ", "summary_of_strengths": "- Paper motivates the need for combining symbolic, structured information and text for math problem understanding. The paper identifies the work done in this space and clearly points the limitations of current methods.  - Experimental setting is clear and well-executed, using several tasks and significance testing. The set of baselines is comprehensive --text/graph methods with and without pre-training, as well methods that combine the two representations.  - Ablation study is comprehensive ", "summary_of_weaknesses": "- I am admittedly not that familiar with the standards in dataset and tasks for this domain, but most of the tasks presented in this paper seem to be based on question type/similarity (knowledge point classification, question relation, question recommendation). What are the opportunities and limitations of this and other competitive approaches when dealing with tasks that require explicit logical/mathematical inference?  - Dataset details -- I couldn’t find any reference to these datasets, are they new? If so, they are missing some important details. How were they annotated? explain and report agreement, and statistics (e.g. label distribution). If you don't have enough space, I would encourage you to add details on the appendix.  - The only other thing that this paper is missing is a discussion on limitations and directions for improvement / future work. I would encourage authors to consider this in the camera-ready version. ", "comments,_suggestions_and_typos": "See above. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Maria Leonor Pacheco, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 144], [144, 333], [333, 600], [600, 824], [824, 1023]], "summary_of_strengths": [[0, 115], [115, 219], [219, 220], [220, 317], [317, 461], [461, 462], [462, 496]], "summary_of_weaknesses": [[0, 262], [262, 424], [424, 425], [425, 509], [509, 557], [557, 582], [582, 654], [654, 740], [740, 741], [741, 868], [868, 940]], "comments,_suggestions_and_typos": [[0, 11]]}}}]
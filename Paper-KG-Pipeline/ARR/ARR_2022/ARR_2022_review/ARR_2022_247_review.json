[{"rid": "58449473f83ef41c2f1e94e8cf6cf5e71a8495031a5ee85bd0d0eb8c3410f0a609f5ba7252346baa1046878d231fa2310909fcdd153afab18576bfafeeffa115", "reviewer": null, "report": {"paper_summary": "Motivated by the shortcomings and relative simplicity of existing natural language understanding (NLU) benchmarks for task-oriented dialogue (ToD), the authors propose NLU++, a novel dataset created by NLU experts that provides fine-grained domain ontologies, contains a large amount of multi-intent sentences, differentiates between domain-specific and generic intents and slots, and is tailored to address problems observed in industrial ToD applications. NLU++ is intended to be challenging to state-of-the-art ToD systems and emphasizes sample quality over quantity. It includes two domains (\"Hotel\" and \"Bank\"), with 1k and 2k samples, respectively.\nThe authors perform a series of intent detection (ID) and slot labeling (SL) experiments based on this new resource, with a particular focus on low-resource settings. For intent detection, MLP classifiers are compared with models based on the question answering (QA) paradigm. The presented findings demonstrate that QA models generally outperform MLP models on the ID task, achieving F1 scores of up to 93 points. SL proves to be more challenging, with the evaluated models obtaining F1 scores of up to 72 points. Unsurprisingly, F1 is substantially lower in the low-resource setting for both tasks. The authors also demonstrate that generic intents transfer across domains, with ID models trained on the one of the two domains being able to detect generic intents in the other domain with substantial accuracy. ", "summary_of_strengths": "- NLU++ represents a high-quality, richly annotated TOD resource that captures the requirements of industrial TOD systems and, as such, is likely to facilitate the development of improved dialogue models that are effective in the low-resource setting (assuming the dateset will be publicly available).\n- The experimental section is extensive enough to showcase the properties of NLU++ and illustrates well as to why it may be of interest to subsequent studies.\n- Cross-domain experiments that focus on domain-general intents and slots are interesting and insightful. ", "summary_of_weaknesses": "- The paper would benefit from a more thorough discussion of the data collection / selection process, as well as from an evaluation of the annotation consistency between the four enlisted NLU experts (e.g. how high is the inter-annotator agreement, does having only four annotators introduce unwanted annotation artifacts or biases, were the annotations validated in any way?).\n- Line 296: A detailed explanation of how the new samples were created should be added.\n- Table 4: It is unclear why MultiWoZ was not included in the comparison. ", "comments,_suggestions_and_typos": "None. ", "ethical_concerns": "Addressed by the authors in \"Ethical Considerations\". "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 458], [458, 571], [571, 655], [655, 822], [822, 932], [932, 1070], [1070, 1170], [1170, 1256], [1256, 1468]], "summary_of_strengths": [[0, 302], [302, 461], [461, 567]], "summary_of_weaknesses": [[0, 378], [378, 466], [466, 540]], "comments,_suggestions_and_typos": [[0, 6]], "ethical_concerns": [[0, 54]]}}}, {"rid": "24c4ce1de0320c577f507e388cc171c9f56cdcf29d39ddfe80193ba47b9c6711f9b9bd48b1e113be98448e7e58c7d8b32a71629363759dd41736354ce7eb15ba", "reviewer": null, "report": {"paper_summary": "This paper presents NLU++, a new dataset for NLU tasks including intent detection and slot labeling across two domains, banking and hotels. The authors emphasize a focus on multi-intent sentences, a combination of cross-domain and domain-specific intents, and high quality through expert annotation. ", "summary_of_strengths": "- The annotated dataset appears to by high-quality and useful to the research community.\n- The authors do an interesting and extensive evaluation of the dataset for intent detection and slot labeling tasks with different baselines. ", "summary_of_weaknesses": "- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text.  - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).\n- Some of the intents do not really confirm to standard definitions of an intent, e.g., \"card\" (Fig 1). This does not actually describe the \"intent\" behind the utterance, which might traditionally be something like \"confirm_arrival\". \" Card\" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as \"less_lower_before\" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.\n- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains. ", "comments,_suggestions_and_typos": "Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).\n- 163: This statement is unsupported \"First, the models went back to focusing on single-turn utterances, which...\" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.\n- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like \"spa\" and \"card\").\n- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).\n- Table 8: Some of the \"description-questions\" shown are ungrammatically, e.g., \"is the intent to ask about some refund?\", or \"is the intent to ask something related to gym?\"\n- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?\nTypos: - 166: Space after footnote 5.\n- 340 (and later): \"Data/Domain Setups\" -> \"Setups\" could either be \"Setup\", or \"Settings\"/\"Configurations\"? "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 140], [140, 300]], "summary_of_strengths": [[0, 89], [89, 232]], "summary_of_weaknesses": [[0, 100], [100, 304], [304, 305], [305, 409], [409, 504], [504, 543], [543, 602], [602, 706], [706, 838], [838, 939], [939, 1013], [1013, 1213], [1213, 1336], [1336, 1479]], "comments,_suggestions_and_typos": [[0, 10], [10, 93], [93, 252], [252, 367], [367, 541], [541, 693], [693, 788], [788, 911], [911, 963], [963, 1030], [1030, 1103], [1103, 1110], [1110, 1141], [1141, 1250]]}}}]
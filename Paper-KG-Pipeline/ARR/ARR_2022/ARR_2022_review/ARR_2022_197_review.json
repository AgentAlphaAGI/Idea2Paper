[{"rid": "2eee4a5d194e4789580dfae74d057d2cc28cbece95a4dda7c0c7e440c1b512ce05bbf1b02285f34702633cc248401e1052170fc6a4477cfec129ac591aef5ada", "reviewer": "Hao Tang", "report": {"paper_summary": "This paper proposes a model to rank pronunciations of a word given a sequence of graphemes. A limited set of potential pronunciations are generated based on orthographic rules. The model takes a sequence of graphemes and a sequence of phones, and produces a score. The approach enumerated the exponentially many sequences generated by the orthographic rules, and find the sequence of phones that achieves the highest score. Empirically, the set of potential pronunciations are not prohibitively large, and is feasible to enumerate. The performance of the model is on par with many other approaches. ", "summary_of_strengths": "I was one of the reviewers in the previous round. I advocated acceptance and I still do. The greatest strength is the simplicity of the approach. The approach does not work for all languages, but it is still valuable for many, such as Thai. ", "summary_of_weaknesses": "The paper is easy to follow, but the presentation can be improved. Below is a list of minor points, and they can be fixed rather easily.\n- The paper does not defend itself well enough. Instead of claiming \"both training and predicting processes are language independent,\" it should openly acknowledge that the approach won't be applicable to certain language orthography that is not very syllabic. This is addressed in the conclusion, but should appear early.\n- The training loss is not explicit. The description in section 3 is not detailed enough.\n- The paragraph and the table right above section 4.1 are good to have, but they feel cherry-picked. The paper should consider discussing other benefits, such as how easy it is to add new words to the model.\n- Section 4.1 is weak, if not hurting the paper. There should be a deterministic mapping (or something very close to deterministic) for Japanese Hiragana, and the results definitely do not help the claim that the approach is language independent. The approach would not be effective if we apply it to Japanese Kanji or Mandarin Chinese. ", "comments,_suggestions_and_typos": "Though there is a accompanying repository, I still think it's worth talking about the model architecture, training hyperparameters, and the training loss.\nThe model is an energy-based model. Given the limited space, there probably isn't much to be said, but it might be worth mentioning so that the readers are aware of this. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Hao Tang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 92], [92, 177], [177, 265], [265, 424], [424, 532], [532, 599]], "summary_of_strengths": [[0, 50], [50, 89], [89, 146], [146, 241]], "summary_of_weaknesses": [[0, 67], [67, 137], [137, 185], [185, 398], [398, 460], [460, 497], [497, 550], [550, 651], [651, 758], [758, 807], [807, 1005], [1005, 1095]], "comments,_suggestions_and_typos": [[0, 155], [155, 191], [191, 326]]}}}]
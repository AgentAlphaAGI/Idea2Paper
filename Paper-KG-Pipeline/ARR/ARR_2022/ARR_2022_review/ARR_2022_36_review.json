[{"rid": "90e04379fb077ffb95194774c8c4d6f2e74a1d3828f2518769ab00f35a80d69327a9545b72f18a23ff9dbf51959a971024bf998443b25750975d7b78aa0e8edb", "reviewer": null, "report": {"paper_summary": "This paper proposes a new task XBRL tagging, which aims to annotate financial documents into the XBRL format. The target entities are mainly numbers, and the model needs to assign different types to these numbers. The size of type set is large, 139. This paper also proposes their own models on this task SEC-BERT-SHAPE. What they do is fine-tuning BERT on financial documents, and replace the fragment of numbers into a special token. They report SOTA performance on this task. ", "summary_of_strengths": "- A new task that could be useful in the financial domain.\n- A dataset for the new task.\n- A model with SOTA performance.\n- Compared with different baseline models. The setting of the experiments is good.\n- The replacing of number fragments is interesting and could be useful for other related tasks. ", "summary_of_weaknesses": "- It is unclear why SEC-BERT is better than FIN-BERT. The only difference seems to be the financial documents that BERT is pre-trained on. The paper fails to analyze why the different choice of documents leads to a large difference in performance.\n- The technical novelty of SEC-BERT is limited, but it is fine to me since the model is not the only contribution in this paper. ", "comments,_suggestions_and_typos": "Why FIN-BERT is much worse than SEC-BERT?\nWhat about the other XBRL tags that are not in the 139 tags? If this task is for real applications, the other tags are also necessary, right? "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 110], [110, 214], [214, 250], [250, 321], [321, 436], [436, 479]], "summary_of_strengths": [[0, 59], [59, 89], [89, 122], [122, 165], [165, 205], [205, 301]], "summary_of_weaknesses": [[0, 54], [54, 139], [139, 248], [248, 377]], "comments,_suggestions_and_typos": [[0, 42], [42, 103], [103, 184]]}}}]
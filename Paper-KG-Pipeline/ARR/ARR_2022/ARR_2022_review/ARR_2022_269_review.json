[{"rid": "19eb70ecad8d0e4be400a93817ad5d9503e06c66b4c7ecf086578690e30c63fe7f59cd51dbe11b28e99e5c0a40c1c6790e22bdc0f9ab82e2790b6d9daa378d6b", "reviewer": "John Ortega", "report": {"paper_summary": "This paper introduces a new method called CrossAligner which is used to transfer knowledge from English (or a high-resource language) to some other language without the presence of data using a zero-shot approach. Several methods that used CrossAligner  such as training techniques for machine translation, dialog classification, and more along with a detailed error analysis are introduced as intrinsic evaluations of the method. ", "summary_of_strengths": "The paper presents statistical rigor on the technique it presents as novel, CrossAligner. \nThe mathematics are sound and the approach makes sense. \nThe qualitative findings are well presented and a deep research analysis covering the architecture, gradient losses, and intent and entity classification is performed. \nAll metrics and results are gotten from previous work and others are introduced in a sound manner, based on investigative findings. ", "summary_of_weaknesses": "In some cases, this paper goes overboard with the analysis in what could seem as an attempt to salvage good results. \nThere are several findings presented on several, almost non-related, tasks and the related work section, while interesting seems like overkill. \nSome of the more important works on translation multi-lingual transfer are omitted. \nThe findings are not outperforming the current state of the art in several tasks and in those tasks where the CrossAligner does outperform the state of the art, it is not bot much (1 point on F-score more or less). \nWhile the statistical rigor is fun to read, I am not sure that the overall findings are novel enough for an ACL paper. \nDespite the statistical rigor and attention paid on the details of error especially from Figure 2 where weighted losses are combined, the error analysis makes a lot of generic claims that do not seem to be based on the statistical proof. ", "comments,_suggestions_and_typos": "Line 048 - There is an assumption that it \"can\" be done based on non-relevant results. \nLine 058 - The intro here talks about alignment but then there seems to be a reference to Question Answering (QA), I understand that the case is not that but the writing here seems to be a little convoluted. \nLines 068 - 080 - Are these lines referring back to the QA idea or is this now a translation problem, how did translation become part of a QA problem? Is it due to the desire to show evidence that the aligner works? Please explain better. \nLine 081 - The related work is a little long, you could mention the two types of transfer and cite some work in a few lines, not this many pages. It is not bad what you have done but not as important I think. Also, you may want to mention the latest low-resource XLM transfer findings from AmericasNLP (Mager et. al) for example to help strengthen this some. \nLine 185 - You are repeating some here. \nLine 173 - borne --> born Line 171-175 - Is there a percentage amount that was observed previously? \nLine 178 - Please explain what you mean by \"losing\" their positional information during translation. \nLine 188 - Algorithm 1, line 14, while it may be obvious, \"y\" is never introduced or explained, is it the labels? \nLine 188 - Algorithm 1, line 25, gradients are explained and shown as a method call, but the main technique (CA) is not really in the algorithm, is the novel idea the logloss gradient transfer? If so, you may want to separate it and show it better. \nLine 238 - \"forms a positive pair\", how is this pair formed by the -log on cosine similarity, doubtful but please explain more. \nLine 286 - Here the datasets point back to QA, but translation is covered, what is this paper about, QA? Intent Classification? Entity Classification? Or, translation? Or all? The line is now unclear. \nLine 310 - A \"minimalist\" setup, you may want to remove some of the related work and expand this, how can your work be reproduced off of these settings? \nLines 320 - 330 - Now, it looks like we are combining tasks again, the other sections here, however, are good. \nLines 381 - 410 - The gains here do not represent a major novelty, in several cases there are losses, which is fine but discouraging. \nLines 430 - 432 - Has this type of loss been proposed in the past for interpretability? If so, please cite. \nLines 441 - 453 - I am not convinced that doing this really helps show the performance, it almost seems as if you are mining to find a difference. Could you not show easily an f-score as you have already done? I understand that it probably isn't the case but the deep dive here is not warranted really in my opinion. \nSection 6.1 - Error Analysis summary, several issues, \"don't carry important sentence level semantics\" - this is not clear, please explain; \"culture and vernacular\" - how did you get that out of your findings?; \" The limits of machine translation\" - please see the AmericasNLP paper mentioned before and cite it as it is important; \"Finally, there were no substantial qualitative...\" - this paper does not show that really. \nSection 7 - The conclusion is weak as far as the results are concerned at a high level. I would suggest that the technique focused on more high level ways of measuring for gains. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: John Ortega, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 214], [214, 431]], "summary_of_strengths": [[0, 90], [90, 147], [147, 316], [316, 449]], "summary_of_weaknesses": [[0, 117], [117, 262], [262, 347], [347, 563], [563, 683], [683, 922]], "comments,_suggestions_and_typos": [[0, 87], [87, 296], [296, 448], [448, 513], [513, 536], [536, 683], [683, 746], [746, 850], [850, 896], [896, 937], [937, 1038], [1038, 1140], [1140, 1255], [1255, 1450], [1450, 1505], [1505, 1634], [1634, 1740], [1740, 1763], [1763, 1786], [1786, 1803], [1803, 1811], [1811, 1836], [1836, 1990], [1990, 2102], [2102, 2237], [2237, 2326], [2326, 2346], [2346, 2494], [2494, 2557], [2557, 2664], [2664, 2878], [2878, 3089], [3089, 3178], [3178, 3269]]}}}, {"rid": "c476e1916a5e04f95f17a9f2fd62d74fa664c84ea9dcaecf96db78dee71eeb770dff7d76c1e4abe6919622db6936040e8ecc60b449d3472f9457a322c9be0d7e", "reviewer": null, "report": {"paper_summary": "This paper proposes a framework for zero shot cross-lingual natural language understanding that makes use of translation systems and bases on learning the alignment from unlabeled parallel data. The authors evaluated their methods on a bunch of datasets about intent recognition and entity recognition tasks and showed some promising results. They also included an interesting qualitative analysis for more insights. ", "summary_of_strengths": "- The authors evaluated carefully their framework on different datasets and their results showed that their proposed framework is promising.\n- The authors provided an error analysis that shows interesting insights for readers. ", "summary_of_weaknesses": "- It is not clear for me about the novelty of the proposed methods.  - The proposed method relies on the quality of translation systems.  - I'm not sure whether the differences of some results are significant (see Table 1).  - The differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective. ", "comments,_suggestions_and_typos": "- Did the authors run their experiments several times with different random initializations? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 195], [195, 343], [343, 417]], "summary_of_strengths": [[0, 141], [141, 227]], "summary_of_weaknesses": [[0, 68], [68, 69], [69, 137], [137, 138], [138, 224], [224, 225], [225, 338], [338, 415]], "comments,_suggestions_and_typos": [[0, 93]]}}}]
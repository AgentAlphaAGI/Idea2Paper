[{"rid": "d3a46425ecebff13b6927a610dc5f2d400c108f66a3a6c8e5814ad8aad6384feca274160a3a306196a4c30bd974c43ed64634b8554cf941d3daa4730cc21d4c2", "reviewer": "Muhua Zhu", "report": {"paper_summary": "This paper proposes an ELECTRA-style tasks for the purpose of cross-lingual pretraining. The resulting pretrained models are competitive against previous  pretrained models. Extensive experiments were conducted to show the effectiveness and efficiency of the proposed approach. ", "summary_of_strengths": "From the experiments we can see that the resulting pretrained models have some advantages. ", "summary_of_weaknesses": "1. The proposed approach to pretraining has limited novelty since it more or less just follows the strategies used in ELECTRA. \n2. It is not clear whether baselines participating in the comparison are built on the same datasets that are used to build XLM-E. ", "comments,_suggestions_and_typos": "1. From the results in Table 1, we can see that XLM-E lags behind baselines in \"Structured Prediction\" tasks while outperforms baselines in  other tasks. Any possible reason for such a phenomenon?\nSome typos and grammatical errors. \n1. \" A detailed efficiency analysis in presented in Section 4.5\". Here \"in\" --> \"is\" 2. \" XLM-E substantially outperform XML on both tasks\". Here \"outperform\" --> \"outperforms\" 3. \"... using parallel corpus\" --> \"using parallel corpora\" "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Muhua Zhu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 89], [89, 174], [174, 278]], "summary_of_strengths": [[0, 91]], "summary_of_weaknesses": [[0, 3], [3, 127], [127, 131], [131, 258]], "comments,_suggestions_and_typos": [[0, 3], [3, 154], [154, 197], [197, 232], [232, 238], [238, 299], [299, 323], [323, 374], [374, 418], [418, 470]]}}}, {"rid": "09290c223be50fea039c864dd823f730df75ee85f0c590eb06167f3ba064f1c299ebd472613f539a5f6768e3f515e0089b07712804763a944f30fe81a6d7bfbe", "reviewer": "Chi-Liang Liu", "report": {"paper_summary": "In this paper, the author proposed XLM-E, a cross-lingual ELECTRA-style pretrained LM, which merges the benefits of the XLM and Electra model. Also, the authors introduce a gated relative position bias which the author claim is helpful to the pretrained model. In the experiments, they show their method achieves better results on most cross-lingual transferability tasks and show XLM-E requires less computation resource than previous methods. Moreover, they compare the cross-lingual alignment ability of XLM-E to several baseline models.\nAlthough the paper is complete and well written, the proposed method is not that exciting. Although I gave him 3.5 points, if I could, I would give him a score between 3-3.5. ( maybe 3.25) ", "summary_of_strengths": "1. The paper is well written and easy to follow. \n2. The experiments are complete but missing some baseline. \n3. The proposed method is much efficient than previous methods. ", "summary_of_weaknesses": "1. It is well known that ELECTRA-style is efficient and the TLM-based model is helpful to the cross-lingual representation. It is no surprise that combining both methods would work. \n2. The TLM-based pre-trained method required translation pairs. I understand most of the baseline required translation pairs, too. However, instead of using translation pairs, I would like to see more research try to achieve better cross-lingual transferability without using translation pairs. \n3. There is no comparison between the usual relative position bias and gated relative position bias. ", "comments,_suggestions_and_typos": "1. missing baseline in Fig.3 and Fig.4: I would like to see the comparison including InfoXLM an XLM-Align. \n2. missing baseline in table 5, 6: I would like to see the comparison including InfoXLM and XLM-Align. \n3. I'm interested in the difference between the results of using usual relative position bias and gated relative position bias. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Chi-Liang Liu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 143], [143, 261], [261, 445], [445, 541], [541, 632], [632, 718], [718, 730]], "summary_of_strengths": [[0, 3], [3, 49], [49, 53], [53, 109], [109, 113], [113, 174]], "summary_of_weaknesses": [[0, 3], [3, 124], [124, 182], [182, 186], [186, 247], [247, 314], [314, 478], [478, 482], [482, 580]], "comments,_suggestions_and_typos": [[0, 3], [3, 107], [107, 111], [111, 211], [211, 215], [215, 340]]}}}]
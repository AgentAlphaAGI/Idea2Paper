[{"rid": "9916122c21008d2809d79170fd22af33b3db6005fa54fc02e83857b0c09d300dec25122a30df4d2c5f9b170d03107ef2a40c9165542a47a2a4c4ee2f298ea9f5", "reviewer": "Divyansh Kaushik", "report": {"paper_summary": "This paper presents the idea of creating domain confused examples paired with contrastive learning for unsupervised domain adaptation. The authors propose identifying these puzzles in the representation space by utilizing adversarial examples---the method searches for an extreme direction that would shift a data point to the direction of the other domain. Then, the contrastive learning setup encodes these original and domain confused examples to be closer to one another, thus pulling examples from different domains closer to the domain discrimination boundary, encouraging models to learn domain invariant representations in the process. The authors further compare this method against other data generation schemes, such as back translation, finding that learning with domain confused examples provides far greater benefit. Further experiments show that the proposed method outperforms several strong baselines on two Amazon reviews benchmarks for sentiment analysis. I think this paper offers valuable contributions, both methodical as well as in the insights that are shared alongside quantitative results. ~I've also read the submitted previous reviews and feel confident in the author response. Overall, I think this paper could be a strong add to an ACL venue but I also have some concerns that I have laid below. I firmly believe that addressing these concerns would strengthen this paper to a great extent.~ I am confident that the authors have addressed my concerns to the extent possible and would recommend acceptance. I have reflected changes to my last review by striking the weaknesses that have been addressed satisfactorily. ", "summary_of_strengths": "- The paper is well motivated, and addresses key limitations posed by both adversarial training for domain adaptation (DANN) and -contrastive learning. Additionally, the connections to and differences from prior work in computer vision are well documented as well.\n- The experimental setup is well designed and the results suggest that the method clearly outperforms several strong baselines on various domain adaptation setups. The ablation studies presented by the authors alongside these results further help establish the efficacy of the proposed approach.\n- The paper is well written and easy to follow. The exposition is clear, which makes the experiments easy to replicate for a reader. ", "summary_of_weaknesses": "- ~I was a bit disappointed to see no error analysis presented in this paper. It would be nice to see some trends and insights into cases when this proposed approach fails to perform well.~ - ~The experimental results seem to be missing significance testing. In absence of it, it's rather unclear whether some small absolute performance improvements are statistically significant. The paper would also benefit from a greater discussion of these close cases. Such a discussion is currently missing.~ - ~A qualitative analysis of the generated domain confused examples would help the reader better understand what these examples could be. Such an analysis is also missing from the paper.~ ", "comments,_suggestions_and_typos": "~How does the issue of instability of DANN relate to the solution proposed by [1] to learn an asymmetrically relaxed alignment? My feeling is that their method would address some of this, plus it's a stronger baseline regardless as it also takes label shift into consideration. Perhaps, try and include it as a baseline?~ ~[1] Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. \" Domain adaptation with asymmetrically-relaxed distribution alignment.\" In International Conference on Machine Learning, pp. 6872-6881. PMLR, 2019.~ It might still be nice to include this in the camera ready. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Divyansh Kaushik, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 135], [135, 358], [358, 644], [644, 831], [831, 975], [975, 1116], [1116, 1206], [1206, 1326], [1326, 1422], [1422, 1536], [1536, 1647]], "summary_of_strengths": [[0, 152], [152, 265], [265, 429], [429, 561], [561, 609], [609, 694]], "summary_of_weaknesses": [[0, 78], [78, 190], [190, 259], [259, 381], [381, 458], [458, 499], [499, 637], [637, 687]], "comments,_suggestions_and_typos": [[0, 128], [128, 278], [278, 322], [322, 391], [391, 462], [462, 515], [515, 526], [526, 539], [539, 599]]}}}]
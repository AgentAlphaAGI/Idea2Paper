[{"rid": "72238c64b0133c55bd0340f893e806207fb2a6a4bcf8b1a82f5be251fd1808a8f6e89fce4d8859277837186b53f01bac53e307a70b0666cb60c68d77651e7c30", "reviewer": null, "report": {"paper_summary": "In this paper authors propose to leverage uni-modal self-supervised learning to promote the multimodal audio visual speech recognition (AVSR). Audio and visual encoders are pre-trained with large-scale uni-modal dataset, with paired audio-visual data used next for the multimodal framework. Combined CTC and seq2seq losses are leveraged. ", "summary_of_strengths": "The proposed methodology is technically sound, and this paper is clearly written overall. Well-designed experiments and ablation studies are presented to show the effectiveness of the proposed approach. ", "summary_of_weaknesses": "I don't find particularly concerns with this paper, though I have questions about several experimental details (see section below for more information). ", "comments,_suggestions_and_typos": "1. In lines 366-367, it's said the relative weights for combining CTC and seq2seq losses are different during training and decoding. Why is it the case? \n2. For results shown in Sections 4.3 and 4.4, is it possible to add error bars? \n3. What is the size of the proposed model, compared to baselines? \n4. In table 3, for the proposed model, WER for the audio-visual setting vs. audio-only setting are close (2.6% vs. 2.7%). Do authors have more justifications on going with multi-modal for this task? "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 143], [143, 291], [291, 338]], "summary_of_strengths": [[0, 90], [90, 203]], "summary_of_weaknesses": [[0, 153]], "comments,_suggestions_and_typos": [[0, 3], [3, 133], [133, 153], [153, 157], [157, 234], [234, 238], [238, 301], [301, 305], [305, 424], [424, 501]]}}}, {"rid": "2eb4344ce741b472e57335138b62c37cfe25eafec0ffbaca887b8360b11fd2983839db46a0cc21579fd32a4bdea26af8bac805bc22e054242cb22415dea44ef1", "reviewer": null, "report": {"paper_summary": "The paper deals with the problem of audio-visual speech recognition (AVSR). The authors note that although the effectiveness of self-supervised learning has been shown in both modalities separately, they largely remain under explored for AVSR. So, they train audio and visual encoders separately using unimodal data and then integrate them together in a multimodal framework for AVSR. They show the effectiveness of their approach through various experiments and improve performance. ", "summary_of_strengths": "- A framework for integrating unimodal self-supervised learned model for multimodal AVSR.\n- The authors show large performance gains over previous work. ", "summary_of_weaknesses": "- The contributions of the paper are not very clear.\n- The explanation of the methods is not well explained.\n- The performance improvement from audio-only to audio-visual is very small (WER: 2.7 to 2.6) and there is significance analysis. This raises a big question on the necessity to add the visual modality in this way and also justification on the extra computation for such a small gain. ", "comments,_suggestions_and_typos": "Questions/Comments: - Why did the authors did not include experiments with external LM for their approach? This will make a better comparison with other models using external LM.\n- Line 183: “we truncate the first convolutional layer in MoCo v2” -> why?\n- In the Audio-only model, what are the differences from Watanabe et al., 2017? This is very important to note the contribution of this work.\n- Line 250: “a canonical transformer decoder” -> what is it?\n- Line 254: “is arguably a decoder” -> why arguably?\n- Line 577: the authors discuss about limited ImageNet data for pretraining. For self-supervised pretraining, the authors are not limited to labeled images as in ImageNet. They can use any image for pretraining. So, I’m not sure why the authors discuss this as a problem.\nTypos: - Line 326: “publicly AVSR” -> publicly available AVSR - Line 527: “randomly crop” -> randomly cropping "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 76], [76, 244], [244, 385], [385, 484]], "summary_of_strengths": [[0, 90], [90, 153]], "summary_of_weaknesses": [[0, 53], [53, 109], [109, 239], [239, 393]], "comments,_suggestions_and_typos": [[0, 20], [20, 107], [107, 179], [179, 254], [254, 334], [334, 396], [396, 457], [457, 510], [510, 587], [587, 682], [682, 722], [722, 782], [782, 789], [789, 844], [844, 893]]}}}, {"rid": "965bb5a3ad429397e09f06916178735cdbf9f5aed8c2b60c72d87a3d9d338f2d4ca3268441e9dcb732a06483a4c04e6a5de3b7b6d4961b98f2e3992276f45a18", "reviewer": null, "report": {"paper_summary": "The paper leverages self-supervised audio and visual representations for audio-visual speech recognition (AVSR) on the LRS2 and LRW. As shown in Figure 1, the audio branch is initialized from a pre-trained wav2vec 2.0, the visual branch is initialized from a pre-trained MoCo v2, and the two branches are fused before seq2seq+CTC decoding. Results on LRS2 show decent improvement over the recent E2E conformer Lip reading work. Ablation studies, noise-robustness, and low-resource AVSR are also helpful. The paper presentation and writings are also clear. However, there is limited novelty as utilizing self-supervised representations for AVSR has been done before. ", "summary_of_strengths": "1. Demonstrate the effectiveness of pre-trained self-supervised representations for AVSR.  2. The paper looks technically correct, and the approach should be easily reproducible. ", "summary_of_weaknesses": "Limited novelty: utilizing self-supervised speech representations for LRS has been conducted in [1], in which PASE is used instead of wav2vec 2. Also, see concurrent work [2] on training a self-supervised audio-visual representation from scratch for AVSR. The seq2seq+CTC joint decoding is also quite standard for ASR.  Reference:  [1] LiRA: Learning Visual Speech Representations from Audio through Self-supervision https://arxiv.org/abs/2106.09171 [2] LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION https://openreview.net/pdf?id=Z1Qlm11uOM ", "comments,_suggestions_and_typos": "I wonder if the authors plan to extend the proposed approach to LRS3? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 133], [133, 340], [340, 428], [428, 504], [504, 556], [556, 666]], "summary_of_strengths": [[0, 3], [3, 90], [90, 91], [91, 94], [94, 179]], "summary_of_weaknesses": [[0, 145], [145, 256], [256, 319], [319, 320], [320, 579]], "comments,_suggestions_and_typos": [[0, 70]]}}}]
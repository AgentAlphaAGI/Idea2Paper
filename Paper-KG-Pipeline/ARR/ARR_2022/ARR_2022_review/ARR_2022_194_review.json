[{"rid": "bbc7d50e7c4592aede7965ad2efb2c10ddc356e8c699a6594d512f677e2b445182f6984b223cab77c78cf828973a49fefb240ca9fce6c28c55b8f8fa9d44883a", "reviewer": null, "report": {"paper_summary": "This paper proposes NoisyTune, which is a method to add noise using the standard deviation of matrix-wise parameters during fine-tuning pretrained language models. The main motivation of using NoisyTune is in line with other work (e.g., by Chen et al. 2020 and Lee et al. 2020) on avoiding large deviation of parameters from the pretrained model to avoid overfitting during finetuning. Experimental results on XTREME and GLEU benchmark datasets suggest that NoisyTune gives small improvement consistently on various settings. ", "summary_of_strengths": "- Simplicity of the method: A matrix-wise standard deviation of parameters is only necessary to use this method - Comparison with a simple baseline of adding a global noise - Parameter matrix-wise distribution is useful for adding noise - Experimental results with four different pretrained language models ", "summary_of_weaknesses": "- The performance gain by using NoisyTune on XTREME and GLEU is very small (less than 1 point in accuracy or F1 score).  - No standard deviation scores are reported, and the comparison of the performance gain vs. standard deviation is not reported.\n- The methods compared in the experiments are only with using global noise. ", "comments,_suggestions_and_typos": "### Comments - I suggest further experimenting with various hyperparameters and would the addition of NoisyTune be complimentary to hyperparameter tuning.\n- Since the motivation of this paper is avoiding overfitting during fine-tuning, I would be very curious to see NoisyTune in few-shot settings, where overfitting is more of an issue.\n- [Minor] For Figure 4, preferable to have legends outside the plot.\n### Questions to the Authors - Are different hyperparameters (e.g., drop out rate) explored? Would it be possible that tuning one of the hyperparameters leads to larger gains than using NoisyTune?\n- What is the standard deviation across multiple runs for e.g., Table 1 and Figure 2? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 164], [164, 386], [386, 526]], "summary_of_strengths": [[0, 112], [112, 173], [173, 237], [237, 307]], "summary_of_weaknesses": [[0, 120], [120, 121], [121, 249], [249, 325]], "comments,_suggestions_and_typos": [[0, 13], [13, 155], [155, 338], [338, 407], [407, 436], [436, 500], [500, 604], [604, 690]]}}}, {"rid": "0f6a2b765962004b043f0fd2cda78b98188478a2b7f68833886ce981d7037dba27376253b861581a1406ebdcdcf5c6c76153326156a785908d4eb6a6f229e8d4", "reviewer": "Jiali Zeng", "report": {"paper_summary": "This paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices. ", "summary_of_strengths": "A simple method ", "summary_of_weaknesses": "There are some questions: Q1. Even simple, I doubt its effectiveness. The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant. \nBesides, please report the standar­­d deviations of 5 times runs in Table 1 and Table 2.\nQ2. The over-fitting problem is more serious in large models, so the experiment of applying NoisyTune on larger PLMs will be more convincing.\nQ3.  The right Fig of Fig3 makes me confusing, which is not consistent with the claim “the relative change of L1-norms becomes smaller when NoisyTune is applied”.\nQ4. Missing the most relevant work, ­­­raise a child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Xu et al., EMNLP 2021. It masks out the gradients of the parameters during the backward process. The difference between NoisyTune and above-mentioned paper is using masking or noisy to perturb parameters. Thus I am very interested to see which method (masking or noisy) will have better benefits. ", "comments,_suggestions_and_typos": "See above "}, "scores": {"overall": "1.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Jiali Zeng, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 177]], "summary_of_strengths": [[0, 16]], "summary_of_weaknesses": [[0, 26], [26, 30], [30, 70], [70, 216], [216, 306], [306, 310], [310, 448], [448, 452], [452, 611], [611, 615], [615, 738], [738, 761], [761, 835], [835, 943], [943, 1035]], "comments,_suggestions_and_typos": [[0, 10]]}}}]
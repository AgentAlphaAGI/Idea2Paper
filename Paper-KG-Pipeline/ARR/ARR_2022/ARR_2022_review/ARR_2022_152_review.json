[{"rid": "1610d011c9b4387992fd53b6494c5743d5bab835302c9990921da95c6773c6eaded9f4786a4459f5c497f65318e23db534f563aa8582440464166cf79b55231e", "reviewer": "Qian Liu", "report": {"paper_summary": "This paper focuses on an important task incomplete utterance rewriting, the goal of which is to rewrite a semantically incomplete sentence into a semantically self-contained utterance. Since several words in the rewritten utterance come from the incomplete utterance, a lot of previous work has modeled this task as some sort of editing task. Following the same line, this paper proposes to leverage a picker and a generator to jointly predict the final utterance. Experimental results on two benchmarks are used to validate the effectiveness of the method. ", "summary_of_strengths": "The paper is well written and easy to follow. The idea of combining the picker and the generator is straightforward and can be integrated easily with generative language models. The experimental results on four datasets validate the effectiveness of the proposed method.\n--- Regarding the concerns I raised in my previous review (Review `XYZ`), I think the resubmission addresses some of my concerns in an appropriate way (see below).\n> 1. This paper claims that the previous method is over fine-tuned on specific datasets (Line 69-71). However, according to my knowledge, the baseline model RUN (Liu et al. 2020) was experimented on four different datasets, including two used in this paper. Why not report experimental results on another two datasets? I suspect that the experimental results in this paper may be also over fine-tuned for specific datasets.\nThis paper adds to the experiments on the proposed datasets, and the results look promising and make me feel more convincing.\n> 2. There are some issues in the experimental setting. First, the comparison to baselines are unfair. If the method in this paper leverages T5 as the encoder, it should at least try to employ the same one to the most advanced baseline (e.g., SARG). Otherwise, the comparison is unfair. Second, according to my knowledge, the reported F3 performance in RUN (Liu et al. 2020) is 47.7 (higher than this paper's result). However, the reproduced result in this paper is 45.1 using the author's code. I am wondering if the paper tuned the baseline performance as in their method.\nThis paper addresses some issue raised by my previous review. For example, they double checked the baseline performance and reported the ones from the original paper. Although the paper did not do a comparison with SARG (plus T5), it also showed sufficient superiority under the limited data setting. ", "summary_of_weaknesses": "Although I am leaning towards accepting the paper, I still doubt the contribution of the picker for the generator. This picker only affects the generator if the T5 encoding representation is altered accordingly. I think the connection between the picker and the generator is somewhat weak. Meanwhile, the proposed `soft` labeling method seems useless, compared with the straightforward `hard` labeling. Considering the `hard` labeling method seems to just pick overlapped words between the incomplete utterances and the rewritten utterances, the nolvety of the proposed method seems smaller.\nBesides the above, the contribution part (line 99-111) is not well structured. I would recommend to add experimental contributions to the third point. ", "comments,_suggestions_and_typos": "There are several minor typos that should be fixed in the revision: - line 080: incorrect use of quotation marks and repeated quotation marks.\n- line 151: why the length of R is not equal to the one of U?\n- line 164: sing -> single - line 177: copes -> copies - line 195: `the` entire input - line 205: `the` next section - line 292: you could use \\softmax to represent the symbol - line 310: you should use \\dot between \\alpha and L_picker - line 318: Table 1 should be at the top of this page - line 325: UIR -> IUR - line 329: what does it mean for `the joint model and the generator`? I think the generator should be part of the joint model?\n- line 364: incorrect use of quotation marks - Table 3: redundant `RUN-BERT` - Table 6: remove bold style on the number `39.2` since it is not the best one "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Qian Liu, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 185], [185, 343], [343, 465], [465, 558]], "summary_of_strengths": [[0, 46], [46, 178], [178, 271], [271, 275], [275, 435], [435, 440], [440, 537], [537, 693], [693, 754], [754, 859], [859, 985], [985, 990], [990, 1041], [1041, 1088], [1088, 1235], [1235, 1272], [1272, 1403], [1403, 1481], [1481, 1560], [1560, 1622], [1622, 1727], [1727, 1861]], "summary_of_weaknesses": [[0, 115], [115, 212], [212, 290], [290, 403], [403, 592], [592, 671], [671, 743]], "comments,_suggestions_and_typos": [[0, 68], [68, 143], [143, 205], [205, 232], [232, 260], [260, 291], [291, 322], [322, 381], [381, 441], [441, 495], [495, 518], [518, 589], [589, 646], [646, 691], [691, 723], [723, 802]]}}}]
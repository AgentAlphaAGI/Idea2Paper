[{"rid": "12ec0f57b4ee64a40bb4c0eacd7edb0ae23929428bbd17607c8439082b8447542bc5708ba691d6ac06352d84f3a05bdf28e0db59cdf9cee8628f95fc0774e5af", "reviewer": null, "report": {"paper_summary": "This paper categorizes and analyzes different responses from benchmarks used to train conversational models and from conversational models. The authors find that benchmarks contain more than 60% hallucinated responses, and that models can amplify the amount of hallucinated responses.\nNote: I've previously reviewed this paper. After reading through the authors' response and the new draft, I've updated my review. ", "summary_of_strengths": "- Novel analysis detailing the frequency of hallucinations and the different response types associated with hallucinations - Includes many details for reproducibility and is nicely organized - Authors addressed much of previous feedback and improved the flow of some sections (e.g., Sec 2) as well ", "summary_of_weaknesses": "This paper in its current state could definitely be useful in facilitating discussions on hallucinations in dialogue models and datasets.  At the same time, I also think this paper could have more impact if it engaged in discussion on the relationship between hallucination and other metrics relevant for dialogue models, e.g., engagingness (specifically, is the takeaway that we should all try to remove hallucinations from datasets that our models are trained on? Would that remove generated hallucinations? Would this removal make conversational models less interesting or engaging? What are potential trade-offs?). I think starting this discussion is relevant and important especially for this \"data quality audit\" work, because this work sets a precedent/\"standard\" measurement for the types of hallucinations present in dialogue benchmarks, and thus the community should be aware of its intended use and potential considerations. ", "comments,_suggestions_and_typos": "- It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.\n- Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - \"Limitations\" section in the Appendix could be moved to the \"Impact Statement & Ethics\" section. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "3 = From the contents of the submission itself, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 140], [140, 285], [285, 328], [328, 415]], "summary_of_strengths": [[0, 123], [123, 191], [191, 298]], "summary_of_weaknesses": [[0, 138], [138, 139], [139, 466], [466, 510], [510, 586], [586, 619], [619, 936]], "comments,_suggestions_and_typos": [[0, 164], [164, 280], [280, 379]]}}}]
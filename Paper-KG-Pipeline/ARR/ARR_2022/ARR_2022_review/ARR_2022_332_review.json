[{"rid": "73837843c65a425ff419296a34871a445f1e2fee4cb8940431054c5ec0c1beb405f43cdd40fd27e52cf27aab7574eb4f160655995346c0889edc41328e588bdb", "reviewer": null, "report": {"paper_summary": "This paper introduces a new image analysis task aiming to identify the time and location of a given image. The authors demonstrate that solving the task requires extracting visual and textual information and searching in a knowledge base. The details of the data collecting process, and an analysis of the collected dataset is provided. The authors study human performance and the performances of several CLIP-based models and show that there is a large gap between machine performance and human performance. ", "summary_of_strengths": "Both the task and the dataset are new. The task requires extracting and reasoning with visual and textual information in the input image, and it is a non-trivial task to collect the dataset. The large gap between human and machine performance suggests that this is a challenge task to solve. ", "summary_of_weaknesses": "- The usefulness of the proposed task is not clearly justified. In the abstract it is briefly mentioned that the proposed task helps with subsequent tasks, but the authors did not provide further discussions and references. Also, it seems that time and location can only be inferred for certain types of images (e.g., news images), and therefore it is not clear how this can be a generally useful task.\n- The authors demonstrated that the proposed task requires some reasoning capability to solve, but they only evaluated basic vision-language retrieval models that do not seem to have the corresponding reasoning capability. There is also not too much analysis of the experiment result, especially on the result that shows time prediction is completely off. ", "comments,_suggestions_and_typos": "- The authors need to provide a compelling argument about the practical value of the proposed task and demonstrate that incorporating the corresponding reasoning capability improves the model performance.\n- It might be useful to provide the chance accuracy so that the readers can have a better idea of how well the baselines perform.\n- Considering that the task can also be formulated as a classification problem, it might be useful to consider a standard softmax classifier. This will also give us an idea about how much performance one can get without transfer learning. Also, the time prediction seems to be completely off; the softmax classifier might be useful for diagnosing the problem. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 107], [107, 239], [239, 337], [337, 509]], "summary_of_strengths": [[0, 39], [39, 191], [191, 292]], "summary_of_weaknesses": [[0, 64], [64, 224], [224, 403], [403, 626], [626, 759]], "comments,_suggestions_and_typos": [[0, 205], [205, 335], [335, 477], [477, 574], [574, 695]]}}}, {"rid": "a06180e20fd21ca2b631519a77f9e26b386f00c5c63f9b1f064c3fad903a26e6b8f1e0ab555b68dcb2a82e86e09abf42a7c451b828ce83c2c2c09819bc4a3c1d", "reviewer": null, "report": {"paper_summary": "In this paper the authors introduce their TARA dataset, which is designed to enable high level reasoning from an image which needs open world knowledge grounded in the visual content. The dataset 16k images from the NYT articles and an additional 61k weakly supervised samples from the wikipedia dataset. The dataset is extremely challenging for the present state of the art models to solve with an extremely wide gap between human and mode performance. They present strong baseline models based on the recent CLIP model and extend it further to adapt it to the present task. An extensive annotation and data cleaning process is followed to ensure that the quality of the data is maintained while collecting a sufficient number of samples. ", "summary_of_strengths": "The paper introduces a very novel and very relevant problem to the area and presents a dataset for the same. The problem of combining open real world knowledge to make inferences about the visual content in an image is something which can find a variety of uses in real life and is something that comes easily to humans, but is extremely hard for our present models to solve making it an interesting problem to enhance the capabilities of present day computer vision systems to take them closer to the goal of AGI. This dataset is a useful step in that direction.\nThe data cleaning and annotation process is very thorough and the process is very clearly described with the use of very relevant and self explanatory visuals making the paper easy to read and follow.  Strong baselines are proposed and evaluated against and a very relevant and useful evaluation metric of example-F1 is used to compare model performance while allowing for accounting for multiple different hierarchies in the labels. ", "summary_of_weaknesses": "1. Some data annotations steps can possibly lead to some spurious labels like the use of geopy for location annotations can introduce annotation errors which could throw off models trained/evaluated on the dataset 2. example-F1 is an unweighted measure and hence weighs the prediction of the century and decade the same as month and date. Maybe using a weighted version of the same to penalize the harder to get labels could help make the evaluation more fair. \n3. Dataset size is a little limited for the use of the more recent and heavy multimodal models which might be needed to reason over a vast amount of open ended data and visual images 4. Simpler baselines like random guess, a simple guess based on a captioning model, more recent visual transformers for feature extraction etc can be added as more baselines ", "comments,_suggestions_and_typos": "1. Try using weighted example F1 to assign different weights to easier prediction labels like the century and harder ones like month and date. Similarly distinguish between location and time prediction  2. Try incorporating more diverse and larger data sources to expand the data beyond NYT and also enhance it in size 3. line 365: size larger than 50, should be defined on a reference image size since it could mean widely different things based on how big the image itself is. \n4. Add more baselines including simpler ones as reference "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 184], [184, 305], [305, 454], [454, 576], [576, 740]], "summary_of_strengths": [[0, 109], [109, 515], [515, 564], [564, 765], [765, 766], [766, 998]], "summary_of_weaknesses": [[0, 3], [3, 217], [217, 339], [339, 461], [461, 465], [465, 648], [648, 819]], "comments,_suggestions_and_typos": [[0, 3], [3, 143], [143, 206], [206, 322], [322, 479], [479, 483], [483, 538]]}}}]
[{"rid": "fb3b9aa95ddf49e8c7dde3292ed0a7f5aa84862ac8fa8ce482dd03bb23b412c9c99d8268b47ebcee72e4ced81f48c81d9e4fe03e640e096b46c86623737620f4", "reviewer": "Feng Nie", "report": {"paper_summary": "The paper proposes a pipeline for zero-shot data-to-text generation. \nThe framework follows traditional data-to-text diagram. It contains 4 steps in general. 1) Template verbalization with  2) ordering module, then the 3) aggregation and 4) sentence compression. \nTo enable training the pipeline model, the paper contributes a large dataset named Wikifluent corpus. The data-text pairs are collected from Wikipedia dump. It also applies some split-and-repharase and coreference models on corresponding texts.  The framework proposed in this paper is similar to rule-based data-to-text generation. The main difference of the proposed method compared to traditional methods, each module is trained using neural networks by leveraging recent pre-trained generation models like BART. The novelty of this paper is limited. The neural pipeline method not entirely new, some of previous papers combine important modules such as planning and ordering of traditional data-to-text generation into an end-to-end neural network. [ 1,2]. \nThe paper show improvements of the proposed method on two public datasets with COPY baseline in automatic evaluation. In the manual evaluation, the paper fails to compare with existing baseline method, leaving the evaluation incomplete. The overall paper structure is messy. A lot of detailed model descriptions are described in experiments, which is quite confusing.  [1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. \n[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. ", "summary_of_strengths": "1. The paper contributes a large scale data-to-text generation dataset WikiFluent.  2. The paper proposes a neural pipeline method leveraging recent pre-trained language models. ", "summary_of_weaknesses": "1. The idea of neural pipeline method for data-to-text generation is not entirely new. The novelty is limited in this paper. \n2. In the experiments, the paper only compare with a weak baseline COPY. There are a lot potential baselines are missing in the experiments. Some existing methods leveraging pre-trained language models (PLMs) should be compared as a baseline. For example, training the PLMs on WIKIFLUENT datasets and then test on the evaluation dataset is a straight forward baseline. \n3. The human evaluation part is quite confusing. No baseline methods are compared (for example COPY). \n4. The paper writing is really confusing. First is the paper structure. A lot of model descriptions are describe in the experiment part. And the section 5.4 is the description of ablation methods, which would be more clear by listing together with other baseline methods. ", "comments,_suggestions_and_typos": "In sentence aggregation model, the details are not quite clear. Given the input sentences, the supervision is comming from the delimiter term. What is the standard of defining the delimiter term? Is it only using strict sentence string matching or semantic matching?  There are some missing references in this paper. \n[1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. \n[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. \n[3] An architecture for data-to-text systems. Reiter et al., 2007.  Typos Line 409 we adopt BART-base for -> we adopt BART-base model for "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Feng Nie, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 69], [69, 126], [126, 158], [158, 263], [263, 366], [366, 421], [421, 509], [509, 510], [510, 597], [597, 780], [780, 818], [818, 1019], [1019, 1025], [1025, 1144], [1144, 1263], [1263, 1301], [1301, 1394], [1394, 1395], [1395, 1491], [1491, 1515], [1515, 1578], [1578, 1604]], "summary_of_strengths": [[0, 3], [3, 83], [83, 84], [84, 87], [87, 178]], "summary_of_weaknesses": [[0, 3], [3, 87], [87, 125], [125, 129], [129, 199], [199, 267], [267, 369], [369, 495], [495, 499], [499, 545], [545, 598], [598, 602], [602, 641], [641, 671], [671, 736], [736, 871]], "comments,_suggestions_and_typos": [[0, 64], [64, 143], [143, 196], [196, 267], [267, 268], [268, 317], [317, 414], [414, 438], [438, 501], [501, 527], [527, 574], [574, 595], [595, 596], [596, 666]]}}}, {"rid": "edeb6b73f5e1cd110d9fcf8979ad3bb447374e25885ac475b989d86003ce7ab9dd829cf7a3030c3dce84f990563e1b6e53dce8b8d8c9c12a8bc012551c41c85d", "reviewer": null, "report": {"paper_summary": "This paper focuses on data to text generation from RDF triples and attribute value pairs. The task involves generating a couple of sentences from a set of triples. As far as the traditional generation pipeline is concerned, it does not include any content selection or document planning. All the triples must be verbalized although their order must be determined and which ones should be grouped (aggregated together) to give rise to a single sentence. Most state of the art models are trained or fine-tuned on datasets developed specifically for this task. The authors argue that training on in-domain datasets leads to overfitting. They propose to render the RDF triples into natural language, aka facts, using domain-specific templates, and then develop general modules which perform the text rewriting operations (ordering, aggregation) required to render the clunky fact-based sentences into fluent text. The rewriting modules are trained on a synthetic corpus which the authors create from the English Wikipedia. ", "summary_of_strengths": "- The paper is well written, and the proposed approach intuitive and conceptually simply.\n- The Wikifluent corpus will be of use for other text-to-text tasks, such as simplification or even document compression.\n- The results are rather encouraging for a zero-shot model.\n- Evaluation is through, and includes ablations and experiments with automatic metrics and humans ", "summary_of_weaknesses": "I am not sure there are many weaknesses as such. Although conceptually simple the approach relies on several tools to create the Wikifluent corpus (e.g., decontextualization, sentence splitting) and domain expertise to turn the trips into facts, so even though it zero shot wrt generation task itself, it is not data-lean at all.  For this reason, I would have liked to see an assessment of how noisy these intermediate steps are. ", "comments,_suggestions_and_typos": "- Please give us some statistics on the templates, how many they are per dataset, how they were developed (by one or several humans, was there an algorithm? Is the process deterministic or is the any ambiguity?). Btw what you call templates in the Appendix should be facts (which you get by application of your templates in the input).  - In the introduction you should give more credit to Myrossef et al. As far as I am aware they were the first who proposed to convert data-to-text generation to text-to-text generation (their model is not zero-shot, and you have enough of a contribution here to give them appropriate credit).  -Perhaps the following dataset is of use for your coreference replacement task: https://arxiv.org/abs/2102.05169 - I would have been curious to see what happens if you applied your approach in a non zero shot manner, i.e., if you fine tuned (using your templates and the Wikifluent corpus) - The paper contains detailed comparison to related systems, it would have been useful to organize these more meaningfully (i.e., fully supervised vs zero-shot, fine-tuned or not), such groupings would help the reader follow the discussion of your results  - "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 90], [90, 164], [164, 288], [288, 453], [453, 558], [558, 634], [634, 910], [910, 1019]], "summary_of_strengths": [[0, 90], [90, 212], [212, 272], [272, 370]], "summary_of_weaknesses": [[0, 49], [49, 330], [330, 431]], "comments,_suggestions_and_typos": [[0, 157], [157, 213], [213, 336], [336, 337], [337, 630], [630, 631], [631, 744], [744, 921], [921, 1178], [1178, 1180]]}}}]
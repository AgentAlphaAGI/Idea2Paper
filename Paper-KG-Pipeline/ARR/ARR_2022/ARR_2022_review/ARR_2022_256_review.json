[{"rid": "4aa14e9c828abc596357fb1d4565dfa3ed88efd4e4b914331b6439e4bc43a902348c83b1d6778bb95579cc25867623313189d9681b2ac961e4d57530af8d33c6", "reviewer": "Xian-Ling Mao", "report": {"paper_summary": "This paper proposes an unsupervised knowledge distillation framework with a multiple-task multiple-teacher model for cross-language named entity recognition. An entity similarity evaluator is proposed to help NER as an auxiliary task. The results show that the approach outperforms baselines on seven languages. ", "summary_of_strengths": "1. \tThis is the first work that using multi-task learning in cross-lingual NER. \n2. \tThe similarity metric model boosts the performance as an auxiliary task part. \n3. \tThe experiments show that the proposed approach performs better than the baselines. ", "summary_of_weaknesses": "1. \tInnovation is weak, only the modification of existing methods. Although existing cross-lingual NER methods don’t consider multi-task learning, it has been widely used in transfer learning/unsupervised learning. \n2. \tThe experiment is not sufficient. A key contribution of this work is the teacher-student framework. But little experiments are carried out to prove its effectiveness. It seems that MTST in ablation study aims to prove it but the description “multiple-teacher to single-teacher” and “teacher and student have the same neural network structure” is confusing. ", "comments,_suggestions_and_typos": "There are some grammatical mistakes, such as a)\tin line 115, it should be \"that introduces an entity similarity evaluator\"; b)\tetc. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Xian-Ling Mao, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 158], [158, 235], [235, 312]], "summary_of_strengths": [[0, 3], [3, 80], [80, 84], [84, 163], [163, 167], [167, 252]], "summary_of_weaknesses": [[0, 3], [3, 67], [67, 215], [215, 219], [219, 254], [254, 320], [320, 387], [387, 577]], "comments,_suggestions_and_typos": [[0, 132]]}}}, {"rid": "9369772dc98f529223d5a6b71ae8305b2df2197c2f889432b8019841ec75d142dfb9b9aadf9de8545748e85e33346596c49a141e841ecf79e00dabe891c7bf75", "reviewer": null, "report": {"paper_summary": "In this paper, the authors propose an unsupervised multiple-task and multiple-teacher model for cross-lingual NER. The student model learns two source language patterns of entity recognition and entity similarity evaluation. They propose a weighting strategy to take consideration of the reliability of the teachers. Experimental results show that the proposed model yields significant improvements on six target language datasets and outperforms the existing state-of-the-art approaches. ", "summary_of_strengths": "1. The authors propose an unsupervised knowledge distillation framework for cross-language named entity recognition and develop a teaching and learning procedure under this framework. \n2. The authors present a novel multiple-task and multiple-teacher model that introduces a entity similarity evaluator to boost the performance of student recognizer on target languages. \n3. Extensive experiments on seven languages compared with SOTA baselines and the results confirm the effectiveness of MTMT model. ", "summary_of_weaknesses": "1. The authors unitize multilingual mBERT as basic sequence feature extractor to achieve the sequence embedding representation, why not try other multilingual pre-trained language models, such as mBART, ERNIE-M, mLUKE and so on. \n2. The languages in CoNLL dataset are some rich-resourced languages indeed, the authors need to test MTMT model on some low-resourced languages.\n[1] Ri R, Yamada I, Tsuruoka Y. mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models[J]. arXiv preprint arXiv:2110.08151, 2021. \n[2] Ouyang X, Wang S, Pang C, et al. Ernie-m: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020. ", "comments,_suggestions_and_typos": "This paper has done a solid work on Cross-Lingual Named Entity Recognition, however, some questions remain to be clarified.\nPlease try more multilingual feature extractors and test MTMT model on some low-resourced languages. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 115], [115, 225], [225, 317], [317, 489]], "summary_of_strengths": [[0, 3], [3, 184], [184, 188], [188, 371], [371, 375], [375, 502]], "summary_of_weaknesses": [[0, 3], [3, 229], [229, 233], [233, 375], [375, 497], [497, 536], [536, 685], [685, 724]], "comments,_suggestions_and_typos": [[0, 124], [124, 225]]}}}, {"rid": "46c5c8a5c4f4c5d7e9293777e0665330bf22d54a80fb391f7119cea4a0292283b104f562f2b885a97d70143031538e86560626c444b3140c6a7093d125225223", "reviewer": "Kaushal Kumar Maurya", "report": {"paper_summary": "The paper proposes a multi-task knowledge distillation framework for unsupervised cross-lingual NER applications. This framework consists of two teacher training models as two tasks, i.e., Entity recognizer teacher and similarity evaluator teacher for entity recognition and token type identification, respectively in the source language. Similarity evaluator teacher model is based on siamese neural network. It is trained as a similarity prediction task with two uniformly sampled tokens from two input sentences. The input sentences are randomly selected from the input dataset. Then a Teacher Student Distillation model is simultaneously trained with both teachers’ soft labels to improve the representation for the target language. For student model training, the loss is constructed as weighted supervisory singles from different teacher models.  The experiments are conducted on three datasets and six low-resource languages. The model performance is compared with several existing baselines. To understand the insights of the model, an ablation study, case study, and other analyses were conducted. ", "summary_of_strengths": "1. The paper is well written and easy to follow. \n2. Cross-lingual NER is a challenging problem, and the proposed approach can be easily scaled to more low-resource languages. \n3. Ablation study and multiple analysis experiments are the strength of the paper. \n4. The idea of using the additional similarity evaluator teacher model in the multi-tasking distillation framework is novel and interesting. ", "summary_of_weaknesses": "1. I am concerned about the intuition/motivation of the Similarity evaluator teacher model. Paper reported a concrete example in lines 70-85. There can be a possibility of negative transfer. Let’s consider a counter-example: suppose for a low-resource language LR with sentences S1 = {w1, w2, w3, w4}, S2 = {w1, w5, w3, w4} and S3 = {w1, w6, w3, w4}. Where (w2, w5) and (w2, w6) are considered as token pairs by Similarity teachers. The English NER model predicts correct labels Lc for w2 and incorrect label Lic for w5 and w6. While training with the student model, it is possible that the similarity evaluator teacher may provide incorrect supervision for w2 as the other two tokens have incorrect labels. \n2. It is unclear how many tokens are uniformly sampled from each sentence while Entity Similarity Pairs Construction and how the token pairing has been done? There should be a label distribution analysis of the source language training dataset used for the Similarity evaluator teacher model. ", "comments,_suggestions_and_typos": "1. Some sentences are confusing. For example (1) lines 7-11 sentence. In particular, what is the meaning of the phrase “identical single task across both domain.” \n2. In line 23, it should be: …on the three datasets…. \n3. In lines 523-524, it is unclear what embedding distribution is considered from the student model; add more detail. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Kaushal Kumar Maurya, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 114], [114, 339], [339, 410], [410, 516], [516, 582], [582, 737], [737, 852], [852, 853], [853, 933], [933, 1000], [1000, 1107]], "summary_of_strengths": [[0, 3], [3, 49], [49, 53], [53, 176], [176, 180], [180, 260], [260, 264], [264, 402]], "summary_of_weaknesses": [[0, 3], [3, 92], [92, 142], [142, 191], [191, 433], [433, 528], [528, 708], [708, 712], [712, 867], [867, 1002]], "comments,_suggestions_and_typos": [[0, 3], [3, 33], [33, 70], [70, 163], [163, 167], [167, 218], [218, 222], [222, 337]]}}}, {"rid": "59538f3e87c3e8f2a66537e5d0cecf6d32fc7d17905cb608f66644496dd8e5dafdf0cfd513c59da774c4de1431edb56b951a4ddb53893aa5ef2c51a46bf4480d", "reviewer": null, "report": {"paper_summary": "The paper studied how to use both knowledge distillation and multitask learning to improve the performance of the cross-lingual NER task. Two student models are supervised by two teacher models. One of them handles the entity recognition task, the other is responsible for entity similarity. Experiments demonstrate the effectiveness of the model. ", "summary_of_strengths": "1. \tThe paper proposes to combine multitask learning and knowledge distillation to discover more knowledge in the cross-lingual NER task. \n2. \tExperiments results are good. ", "summary_of_weaknesses": "1. \tI don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! \n2. \tMore baselines should be contained such XLM, XLMR, mBART. ", "comments,_suggestions_and_typos": "1. The teacher model should also be evaluated to verify whether the distillation process is necessary. \n2. Typo. L-347. ' Figure.4' "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 138], [138, 195], [195, 292], [292, 348]], "summary_of_strengths": [[0, 3], [3, 138], [138, 142], [142, 173]], "summary_of_weaknesses": [[0, 3], [3, 85], [85, 143], [143, 290], [290, 389], [389, 472], [472, 512], [512, 546], [546, 550], [550, 609]], "comments,_suggestions_and_typos": [[0, 3], [3, 103], [103, 107], [107, 113], [113, 122], [122, 132]]}}}]
[{"rid": "bf02e6a3f5c823ead06f01e0ee0a11317b7a8a6f6d4ded461af62d5380a91e25108a020db545f21b64a0a636f9e4e37df65e1007d6c0a718b93b53a43bffe768", "reviewer": null, "report": {"paper_summary": "This paper tackles the problem of retrieval-augmented generation. \nThe authors suggest two ways to enhance such models (e.g. RAG): 1. Training a reranker to better approximate the contribution of each retrieved document to the generation process. The reranker also enables enables retrieved documents from different retrievers (e.g. DPR & BM25) to be combined seamlessly. \n2. Distilling the knowledge from the reranker to the retriever ", "summary_of_strengths": "- The paper reads well and is easy to follow - Strong results across numerous tasks - Nice ablations ", "summary_of_weaknesses": "- Overall, the paper somewhat lacks novelty. Most ideas presented in the paper have already been considered before (for example, end-to-end training; enhancing retriever with knowledge distillation).  - Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.\n- The claim which DPR and BM25 scores aren't comparable is not entirely correct (Line 191). [ 2] show that a hybrid retriever over DPR and BM25 is actually able to improve over both.\n- FiD [3], a state-of-the-art reader for open-domain QA (NQ & TriviaQA) is a missing baseline. Indeed, it seems like the results of Fid are similar to Re$^2$G. If this is indeed the case, it should be reflected in the paper.\n[1] Izacard and Grave. Distilling Knowledge from Reader to Retriever for Question Answering. 2020 [2] Ma et al. A Replication Study of Dense Passage Retriever. 2021 [3] Izacard and Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering ", "comments,_suggestions_and_typos": "- Did you try to marry FiD with your reranking framework, thus eliminating the need for expensive cross-attention in the decoder (by lowering the number of retrieved passages fed to FiD)? "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 66], [66, 134], [134, 247], [247, 372], [372, 376], [376, 436]], "summary_of_strengths": [[0, 45], [45, 84], [84, 101]], "summary_of_weaknesses": [[0, 45], [45, 200], [200, 201], [201, 362], [362, 456], [456, 545], [545, 640], [640, 770], [770, 793], [793, 863], [863, 930], [930, 958], [958, 1045]], "comments,_suggestions_and_typos": [[0, 188]]}}}, {"rid": "ce397a79b9eeb1079597d355633d6a8206fe5b47a1cbe8f6025b51e95b871d239039e086bdc2671c8cb11f1a0ec1062f294ab73d4265df71edc20657e258dacf", "reviewer": null, "report": {"paper_summary": "The paper proposes a novel model called R^2G which combines neural initial retrieval and reranking into a BART-based sequence- to-sequence generation model.\nFurthermore they ensemble different first stage retrieval methods, combining neural retrieval and lexical retrieval with BM25. \nThey propose a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output.\nThey compare their novel model on the KILT leaderboard and demonstrate effectiveness gains compared to relevant related work. ", "summary_of_strengths": "The paper proposes an extension of the RAG architecture by including another lexical index.\nI like the discussion of the different solutions for end-to-end training of the system and also the explanations why certain approaches do not work and the other ones do.\nI also think that the extension of the RAG system with a re-ranking approach is interesting for the task. ", "summary_of_weaknesses": "I am wondering how novel the application of the knowledge distillation from the reranker as teacher model to provide labels to the DPR model is, see Hofstätter et al.\"Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation\" (https://arxiv.org/abs/2010.02666).\nFurthermore I am missing an ablation of training the reranker with the combined scores of the positive passages (line 277) in comparison to the standard way the reranker is trained in Nogueira et al.. I would also appreciate an ablation study of combining DPR and BM25 results with reranking in comparison to established solutions like combination with Reciprocal Rank Fusion (see Chen et al. \"Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models\") or combining the scores with a linear combination of the BM25 and DPR score (see Karpuhkin et al. \"Dense passage retrieval for open-domain question answering\") I am also wondering why only 5 of 11 existing datasets of the KILT leaderboard are selected, I would be interested why you exactly choose these datasets. ", "comments,_suggestions_and_typos": "I think the overall writing is clear and understandable however I thought it could be better organized in some sections: Line 207-2018 describes dense retrieval in the section about re-ranking, I think dense retrieval should be described beforehand and not in the section \"Reranking\" The formulas in Line 310 are not described and embedded in the text, I think describing them in the neighbouring text would make clear why the formulas are there. "}, "scores": {"overall": "2.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 157], [157, 284], [284, 453], [453, 579]], "summary_of_strengths": [[0, 92], [92, 263], [263, 369]], "summary_of_weaknesses": [[0, 293], [293, 494], [494, 726], [726, 922], [922, 1076]], "comments,_suggestions_and_typos": [[0, 121], [121, 284], [284, 447]]}}}]
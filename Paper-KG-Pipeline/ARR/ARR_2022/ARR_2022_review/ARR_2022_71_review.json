[{"rid": "2ef61eb78fdf47c12059e3bf2786a28e20519dde0d8c39af578c9417e7edfb95ea6fa68330fad66de90bba4a0a8891b61269c70b91cd5c981cb6341197744b46", "reviewer": null, "report": {"paper_summary": "This paper presents two approaches for reducing the storage costs in late interaction ranking models. First, the term representations are auto-encoded using \"side information\" (representing the static embedding for the term). A quantization process further reduces the size of the representations on disk. The approach is evaluated on MS MARCO, TREC DL, and CAR and shows that it can effectively reduce the storage burden while having a minimal impact on ranking effectiveness. ", "summary_of_strengths": "- The auto-encoding strategy of using the static embedding as \"side information\" to reduce the size of the representations is clever, novel, and practical. I could easily see this strategy becoming common practice when using these models. \n - The approach can dramatically reduce the storage requirements of late interaction models while having minimal impact on ranking effectivness. \n - The approach is parameterized, allowing it to easily scale to various operating points (trading off storage for performance). ", "summary_of_weaknesses": "- Generally, the evaluation is rather \"narrow\" -- it compares only with variations of the same model (either full cross or split at the same level). It may be helpful to include more baselines to put these results in more context. The baselines presented, however, are rather strong -- with DistilBERT achieving 0.390 MRR@10 on MS MARCO (compared to Nogueria & Cho's 0.365 using BERT-large). \n - The evaluation on CAR appears to have been conducted using \"automatic\" labels (based on the reported number of positive passages per query), even though human-provided labels are available. In general, further details about which version of CAR was used should be provided. \n - Equivalent performance is determined statistically via the failure of a one-sided t-test. Equivalence testing (e.g., TOST) could be used instead to strengthen the statistical claims. \n - It would be nice to have seen this work on other base models-- particularly a T5-based model. ", "comments,_suggestions_and_typos": "- This work may feel more \"at home\" at an IR conference -- I'm not sure how interesting this will be to a broader *CL audience. That being said, I like this work and would be happy to see it presented at an ARR venue as well. \n - Can this approach be used with the ColBERT model, where the ranking scores are based only on the similarities within the last layer? "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 102], [102, 226], [226, 306], [306, 478]], "summary_of_strengths": [[0, 156], [156, 239], [239, 385], [385, 515]], "summary_of_weaknesses": [[0, 149], [149, 231], [231, 392], [392, 586], [586, 670], [670, 764], [764, 857], [857, 955]], "comments,_suggestions_and_typos": [[0, 128], [128, 226], [226, 363]]}}}]
[{"rid": "d0cc8331654d1d5fc3e0abc01c1075a74c2c42f1bb0863ce64825b34380ce2c045eea4b622f0750b906fda8d8a366fca26131b055b58fbc96a342e681f17cc3d", "reviewer": "Xing Wang", "report": {"paper_summary": "This paper investigates whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. The extensive analysis found that Transformer-based architectures are competitive with the E-Z Reader, but only when computing attention flow scores. ", "summary_of_strengths": "- The paper is well written and easy to follow.\n- The experiment results are solid. The authors conduct extensive experiments with different variants of attention weights, and they compare those results with traditional cognitive models named E-Z Reader. The experiment results are solid enough to show that attention flow scores are competitive with the E-Z Reader.\n- As mentioned in this paper, it is the first work that compares fixations between natural reading and task-specific reading on classical NLP tasks including relation extraction and sentiment classification. ", "summary_of_weaknesses": "- The paper is not so clear as the introduction part does not show much background about the cognitive models. The authors may need to refine the writing.  - This paper shows lots of analyses about the attention weights of the Transformer models. Although it presents an in-depth analysis, how to utilize these conclusions is still unclear. If the authors can present a more powerful attention mechanism which correlates better with human attention and improves task performance compared with those baselines, I will be more convinced of the conclusions. ", "comments,_suggestions_and_typos": "Comments:  1.  If a model correlates better with human attention compared to other models, does it show better task performance?\n 2.  Does all the experiments keep the numbers of the parameters of the compared models the same?\nTypos: Line 255 / Line 259:  p < .05 -->  p < 0.05 Line 266:  Correlations grouped by sentence length shows stable values around .6  (SST) and .4-.6 (Wikipedia) except for shorter sen-    tences where correlations fluctuate. -- > 0.6  0.4 0.6 "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Xing Wang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 208], [208, 358]], "summary_of_strengths": [[0, 48], [48, 84], [84, 255], [255, 367], [367, 575]], "summary_of_weaknesses": [[0, 111], [111, 155], [155, 156], [156, 247], [247, 341], [341, 555]], "comments,_suggestions_and_typos": [[0, 14], [14, 129], [129, 133], [133, 227], [227, 455], [455, 470]]}}}, {"rid": "bbc7e938f5511eb8eb8e30da3703e87c3cd679b48fdeb1b7c4931da181700027819be3024fdcff95be8685f1e59bb362f0912ee5e679d30c3562319741bf906f", "reviewer": null, "report": {"paper_summary": "This paper compared the self-attention functions in Transformer models with human eye-tracking data across sentiment analysis and relation extraction tasks. They found several conclusions about the relation between human and machine attentions. ", "summary_of_strengths": "This paper focused on investigating the relation between human and machine attention mechanism which is a very interesting research question. They provided a detailed analysis on the results and discussed the potential implications for NLP and Cognitive Science researches. ", "summary_of_weaknesses": "I like this paper overall, but there are several concerns as follows: 1) there all several experiments and results, the logic/relation between these experiments is not very clear. \n2) the self-attention denotes the attention functions in Transformer models but also the model proposed by Lin et al., (2017). This is very confusing. \n3) the findings of no significant change between BERT model with or without fine-tuning need more analysis. Is this related to the size of the fine-tuning datasets? \n4) The difference showed in Table 1 and Table 2 need significant test. \n5) Why only analysis the self-attention functions in the last layer? and different attention heads may show different patterns, is there any analysis on that? \n6) There has been previous work on comparing fixations between natural reading and task-specific reading on classical NLP tasks, such as 'Deep Neural Networks Evolve Human-like Attention Distribution during Goal-directed Reading Comprehension'. ", "comments,_suggestions_and_typos": "see above "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 157], [157, 245]], "summary_of_strengths": [[0, 142], [142, 274]], "summary_of_weaknesses": [[0, 180], [180, 308], [308, 332], [332, 441], [441, 498], [498, 570], [570, 640], [640, 730], [730, 976]], "comments,_suggestions_and_typos": [[0, 10]]}}}]
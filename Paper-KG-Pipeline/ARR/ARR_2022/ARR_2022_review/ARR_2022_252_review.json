[{"rid": "3e521497d8a22e01c7c3dc64dfca54bcf11a65c980f2592aa438e68185654a9c4a837d594dbb5470bc911d85823ec282d62821090e63c5313442a329883f7fe5", "reviewer": null, "report": {"paper_summary": "The authors propose to use static semi-factual generation + dynamic human-intervened correction to get better performance on OOD and few-shot performance. The method inlcudes 2 major steps: 1) use a rationale extraction model trained on small amount of annotated rationales to highlight rationales and replace non-rationales with synonyms (semi-factual generation) and 2) ask human annotators to identify false rationales (use synonym replacement for the false span) and missing rationales (extract a subsequence) to generate as the new examples. The authors compare with CAD and show better in-domain and OOD performance. ", "summary_of_strengths": "- The idea of using semi-factual generation is novel and interesting.\n- Using raitonale model to expose model reasoning and ask human annotators for minimal effort (only identifies errors and leaves generating new examples to models) can largely reduce the annotation effort.\n- The result that show better OOD performance against CAD with fewer number of examples is interesting as the effort to annotate the data is much less. ", "summary_of_weaknesses": "- Some experiments don’t add much to the analysis: Duplication (line 493) doesn’t seem like a reasonable baseline to compare to. It merely multiplies the 50 examples without adding any new information.\n- The semi-factual generation on model generated rationale in the first step: the replacement could be done on missing rationales, This will remove the correct rationales form the example. It would be better if the paper address this (even simply pointing out that it’s not an issue).\n- Some annotation protocals and their encessities are not explained. E.g., line 223-224 and line 230-231. ", "comments,_suggestions_and_typos": "- Typo line 1: rational-centric → rationale-centric - Was the notation in line 284 intentional? Would look nicer to use $x'$ instead.\n- Not sure if *inductive bias* is the right term to characterize what the annotation captures. Personally, I think of it more as *invariance*. (This point does not factor in to the final score since it's more of a personal take.) "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 155], [155, 547], [547, 623]], "summary_of_strengths": [[0, 70], [70, 276], [276, 428]], "summary_of_weaknesses": [[0, 129], [129, 202], [202, 391], [391, 487], [487, 556], [556, 593]], "comments,_suggestions_and_typos": [[0, 52], [52, 96], [96, 134], [134, 229], [229, 364]]}}}, {"rid": "a8853b09f92393e417af7fa39fe0ff893ee5a5c0a2571ddcfad21ea9db7f70d14a871c9bc38a77ff7624721950c17218e1bcf3a12f84b7c86900b8c6abfd9b7a", "reviewer": null, "report": {"paper_summary": "The paper presents a rational-centric framework with human-in-the-loop to boost model out-of-distribution performance in few-shot learning scenarios. The proposed approach uses static semi-factual generation and human corrections to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalization. Experimental results shows the superiority of the proposed approach on in-distribution and out-of-distribution settings, especially for few-shot learning scenarios. ", "summary_of_strengths": "- Improving out-of-distribution model performance specially in few-shot learning settings is an important problem of real-life need in the NLP community.\n- The proposed approach is simple and can be applied for NLP tasks where rationales can be easily identified and annotated. For instance, in sentiment analysis and text classification tasks.\n- The proposed approach provides cost savings in comparison to alternative approaches for data augmentation, and provides strong out-of-distribution as well as in-distribution performance in few-shot learning settings. ", "summary_of_weaknesses": "- The proposed approach is not fully automatic, and still requires human annotations for identifying rationales and correcting errors from the static semi-factual generation phase. While this annotation effort could be less significant that other data augmentation methods, it still presents a significant cost overhead.\n- It is not clear how to generalize this approach to other NLP tasks aside from sentiment analysis and text classification. For instance, it is not clear how to generalize this approach to other sequence-to-sequence tasks like machine translation.\n- Identifying rationales is not a simple problem, specifically for more complicated NLP tasks like machine translation. ", "comments,_suggestions_and_typos": "The paper is well organized and easy to follow. Figure 2 is a bit cluttered and the \"bold\" text is hard to see, perhaps another color or a bigger font could help in highlighting the human identified rationales better. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 150], [150, 383], [383, 548]], "summary_of_strengths": [[0, 154], [154, 278], [278, 345], [345, 564]], "summary_of_weaknesses": [[0, 181], [181, 321], [321, 445], [445, 569], [569, 689]], "comments,_suggestions_and_typos": [[0, 48], [48, 218]]}}}]
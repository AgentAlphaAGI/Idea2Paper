[{"rid": "6c7386d38647d226e22fb6a21bec815d465a1023fc59c871b959b53ae367be17a28b2839533147dc589795577b8630217bf4ea6311ca501b3d89453b25741324", "reviewer": null, "report": {"paper_summary": "This paper presents a transformer-based model to generate open close tests for language learning and assessment. It is based on ELECTRA and makes use of two loss functions, one of which is standard token classification loss and the other is a language-based loss function that makes sure that a potential gap is not too open-ended. The model shows good performance, evaluated by automatic comparison with gold-standard gaps and also by human expert judgment. The paper will make the dataset available, which may be greatly helpful for future research in education-related NLP. ", "summary_of_strengths": "- The paper provides a good benchmark for a task that has not been studied much before.\n- I appreciate human evaluation in addition to automatic evaluation and qualitative evaluation.\n- The paper is well-written and easy to understand. ", "summary_of_weaknesses": "- An ablation study with the two loss objectives may have been better than a BERT baseline. Instead of comparing an ELECTRA model with the two loss functions with a BERT with one of them, the authors could have compared their model with two ELECTRA models with one of the loss functions only. ", "comments,_suggestions_and_typos": "- Including an example created by BERT and comparing them with one created by the proposed model may be helpful. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 113], [113, 332], [332, 459], [459, 577]], "summary_of_strengths": [[0, 88], [88, 184], [184, 236]], "summary_of_weaknesses": [[0, 92], [92, 293]], "comments,_suggestions_and_typos": [[0, 113]]}}}, {"rid": "4f07dd880235b4dade9d1edea523d9d689b9c28b10a5a91db77bc98a5960270516696c225c92bf8d553c45f810de2bb75ad74ab717ea7a8450c4ac83a8ecade1", "reviewer": null, "report": {"paper_summary": "The paper explore the ELECTRA on constructing open cloze tests. They use ELECTRA's RTE head to do the gap/non gap (gap means the token should be masked in open cloze tests) token-level classification and use ELECTRA's MLM head to do the MLM auxiliary task. Meanwhile, they propose two heuristic methods: 1. a loss forcing gaps to be far away; 2. a post-processing method involving grap's repetition and PoS information. The paper conducts experiments on their collected datasets. They conduct automatic evaluation and human evaluation. Their methods outperformance baselines including: random selection, Exercise Maker ( a rule based method) and BERT. ", "summary_of_strengths": "1. The paper is the first one to explore the ELECTRA on constructing open cloze tests, which seems reasonable. \n2. The evaluation adequate, including automatic evaluation and human evaluation. ", "summary_of_weaknesses": "1. The post-processing seems complicated. \n2. lacks of ablation study of two loss. \n3. Maybe can conduct experiments on other datasets. \n4. Method not novel. ", "comments,_suggestions_and_typos": "1. I wonder the label mapping of  ELECTRA's binary classification head (RTE). In other words, the positive gap label corresponds to the positive RTE label, or not? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 64], [64, 257], [257, 307], [307, 346], [346, 420], [420, 480], [480, 536], [536, 652]], "summary_of_strengths": [[0, 3], [3, 111], [111, 115], [115, 193]], "summary_of_weaknesses": [[0, 3], [3, 42], [42, 46], [46, 83], [83, 87], [87, 136], [136, 140], [140, 158]], "comments,_suggestions_and_typos": [[0, 3], [3, 78], [78, 164]]}}}]
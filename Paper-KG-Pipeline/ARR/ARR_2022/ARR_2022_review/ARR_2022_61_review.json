[{"rid": "3fc189ac8eeb0d461cee1f961009dc0a8e386e21214549d5df3eea92c7bcaf7fa82d2bc9db5419c090668f7977eb65232496c9d5582d6a20bf086dfd958a906c", "reviewer": null, "report": {"paper_summary": "This paper proposes a method for Paraphrase Generation with an explicit control for the desired quality of the generated text. The quality of a paraphrase is defined as a 3 dimensional vector consisting of its semantic similarity with the source sentence, and lexical and syntactic diversity. The quality vector is provided as an input to the model along with the source sentence, and the model is trained to generate paraphrases of the input that express the specified quality. Since, for a given sentence, generating a paraphrase of an arbitrary quality might not be possible, the authors also propose a Quality Predictor model to estimate the typical paraphrase quality of a given sentence by training a regression model on the paraphrase dataset. The predicted quality values are added with different offset values, which can then be used to generate multiple paraphrases, and a method to obtain optimum offset values for a dataset using the dev-set is also proposed. This optimum offset value is then used to specify quality constraints for the test set examples and  accordingly the paraphrases are generated. Through their experiments, the authors show that the quality of the generated paraphrases monotonically increases with the offset specified in the input until a certain point, after which the responsiveness to the input might decrease as it becomes increasingly difficult to generate high quality paraphrases when the requested control values deviates significantly from the typical value. When compared to the baseline, their method outperforms on the three defined quality metrics (semantic similarity, lexical and syntactic diversity), i-BLEU score as well as the human judgements for semantic similarity. Their method also performs competitively with the gold standard defined using the actual paraphrases in the test set, with a better semantic similarity than the actual paraphrases for the most cases and the diversity metrics being often comparable and at times better than the ones in the ground truth test set. ", "summary_of_strengths": "1. The paper addresses two major concerns about the existing work on controlled paraphrase generation. i) The controlled generation approaches typically require specific information like the syntax of the desired target sentence defined in terms of parse trees or POS tags, which might limit their usage for downstream tasks. In comparison, in the proposed method controlling the output quality is much more straightforward and allows the model with flexibility to generate paraphrases of the specified quality. ii) Often, in controlled generation approaches any arbitrary control specification might not be compatible with the source sentence and often in the previous work like in the case of syntactic paraphrasing, there hasn’t been much focus on obtaining these suitable control values for the inputs. The proposed method allows the selection of appropriate quality values for input sentences which help to avoid the specification of incompatible values. Both of these are important contributions to the field of paraphrase generation as well as controlled text generation. \n2. The problem is well formulated and the proposed methods are simple and intuitive. \n3. The experimental setup is thorough with respect to the datasets considered and metrics for evaluating the proposed method with baseline and the gold standard, as well as in terms of analyzing the behavior of their model under different control specifications. I also liked that the evaluation was performed by comparing the generated paraphrases with the input sentence, rather than the reference paraphrase in the test set. To me not only does this make more sense from a paraphrase evaluation perspective, but it also helps us view the reference paraphrases as a gold standard which helps to better interpret the performance of their model as well as the baseline. \n4. The paper is well written and easy to follow. ", "summary_of_weaknesses": "1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. ", "comments,_suggestions_and_typos": "1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? \n2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 127], [127, 293], [293, 479], [479, 751], [751, 972], [972, 1116], [1116, 1506], [1506, 1725], [1725, 2037]], "summary_of_strengths": [[0, 3], [3, 103], [103, 326], [326, 512], [512, 807], [807, 960], [960, 1079], [1079, 1083], [1083, 1165], [1165, 1169], [1169, 1429], [1429, 1594], [1594, 1836], [1836, 1840], [1840, 1886]], "summary_of_weaknesses": [[0, 3], [3, 210], [210, 388]], "comments,_suggestions_and_typos": [[0, 3], [3, 103], [103, 135], [135, 222], [222, 420], [420, 424], [424, 523], [523, 655]]}}}]
[{"rid": "aa58d6b301c8f7acc508015e36d7e9b0fc06eb7af2007afe19aac58699718a360f1087554638b6ebc805c25666ac67359ec09e505418c3fa623b2cf1ba8d7b8b", "reviewer": null, "report": {"paper_summary": "This paper focus on table pretraining via spreadsheet formula to realize numerical reasoning. Concretely, it proposes two effective pretraining tasks: numerical reference prediction and numerical calculation prediction. At the same time, formula mask language model is also employed to get better representation of the spreadsheet formula. Experimental results show that the numerical-reasoning-aware pretraining make the TUTA (tree-based transformer for table pretraining) outperforms SOTA methods on formula prediction, question answering and cell ", "summary_of_strengths": "1. The paper is well-written and easy to follow.\n2. The numerical-reasoning-aware pretraining outperforms SOTA by large margin in three representative tasks.\n3. Analysis is very detailed to further clarify the effectiveness of pretraining. ", "summary_of_weaknesses": "Most of the experiments focus on spreadsheet related task including formula prediction (NCP task helps) and cell type classification (NRP tasks helps). For Table QA task,  it would be better to conduct experiments on more comprehensive datasets like WikiTableQuestion (TableQA), TabFact (Table-base fact verification) following TaPEx. ", "comments,_suggestions_and_typos": "Questions: 1. For table-text reasoning, why NRP task + formula-based prompt improve this reasoning ability? \n2. In Table 2, what about the result of  SpreadsheetCoder under 20% train set? \n3. L174, is there an [SEP] exists between columns, like the delimiter among value cells?\nSuggestions: 1. A little introduction of 'Formula', 'Sketch', 'Range', 'table hierarchies' in session2 will lead to better understanding, which are often used in the latter part. \n2. The order of terms in L288-L290 should be aligned with Figure 2. \n3. In Figure 2, It would be better to replace [ RANDOM …… TOKENS …… from …… VOCAB ] with an specific example. \n4. While the error analysis is conducted in Appendix, there is no case study to make the improvement more concretely.  Typos: 1. Table#1 in Figure 1: (C3-B3)/B3 -> (D3-C3)/C3 "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 94], [94, 220], [220, 340], [340, 550]], "summary_of_strengths": [[0, 3], [3, 49], [49, 52], [52, 158], [158, 161], [161, 240]], "summary_of_weaknesses": [[0, 152], [152, 335]], "comments,_suggestions_and_typos": [[0, 14], [14, 108], [108, 112], [112, 188], [188, 192], [192, 278], [278, 294], [294, 457], [457, 461], [461, 526], [526, 530], [530, 637], [637, 641], [641, 756], [756, 757], [757, 767], [767, 813]]}}}, {"rid": "06b82c4720d419bb56d16f5272bfa123c4037299b66dd31b20e9cb8c4a7651de5fbe198e06b99a6276dc60d3f4189c857529134deb1b8b3651233850caacce54", "reviewer": null, "report": {"paper_summary": "This paper presents a technique for numerical reasoning over tables by leveraging formulas based on the pre-trained model TUTA specifically designed for tables.  FORTAP, a FORmula-driven TAble pre-training model, is proposed in this paper, which incorporates numerical reasoning capabilities based on large-scale spreadsheet formulas.\nThe main contributions are two complementary/auxiliary objective functions:   * Numerical Reference Prediction,   * Numerical Calculation Prediction.  They take relationships between cells into consideration and predict all operators using cell embeddings.  Experimental results show their method achieves better performance over several datasets when compared to both baseline and SOTA models. ", "summary_of_strengths": "- Show the potential of leveraging formulas in table pretraining.\n- Good results based on comprehensive experimental settings. Apart from that, the authors provide very detailed explanations and discussions.  - Very clear structure and good organization. ", "summary_of_weaknesses": "- Baselines chosen in this paper are not strong enough to show the performance brought by the introduced objectives. For example, under the TableQA settings, another model employing SQL named TaPEx (also mentioned in this paper) can be used to compare with the proposed model.\n- To further evaluate the effectiveness of the proposed method, experiments on other tasks (e.g. cell type classification, table type classification) compared to TUTA should be conducted. ", "comments,_suggestions_and_typos": "- More clarification and novelty is needed for contributions in Sec 1.\n- As mentioned in Sec 5, there is another model named TaPEx, which is also designed to improve reasoning skills in table pre-training. Why is there no comparison with TaPEx in the experiment section?\n- TUTA employs several datasets to evaluate the model performance on CTC (cell type classification), why is there only one dataset used in this paper?\n- Is there any deficiency/limitation of introducing these objectives? Will it cause a performance drop on other tasks such as Table Type Classification and CTC? "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 161], [161, 162], [162, 335], [335, 485], [485, 486], [486, 592], [592, 593], [593, 730]], "summary_of_strengths": [[0, 66], [66, 127], [127, 208], [208, 209], [209, 255]], "summary_of_weaknesses": [[0, 117], [117, 277], [277, 465]], "comments,_suggestions_and_typos": [[0, 71], [71, 206], [206, 271], [271, 422], [422, 492], [492, 583]]}}}]
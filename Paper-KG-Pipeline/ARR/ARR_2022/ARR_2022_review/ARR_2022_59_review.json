[{"rid": "f5450641fc7e1c553fb3526d0e3e2401d874e29587f785b72f4dd67a6d6210f803c4acbe0c2334f929b3b3301af4903c63b82cdbd5db63fd0e4d9b09a505f82c", "reviewer": null, "report": {"paper_summary": "The paper mainly proposes an approximation approach for transformer model inference so that it can apply homomorphic encryption. First, the author introduces the current practical homomorphic encryptions and transformer-based model. Second, the author proposes an approximation workflow by separating the fine-tuning stage into several subphases. Third, the author evaluates the approximation performance on different NLP tasks and datasets. ", "summary_of_strengths": "Homomorphic Encryption is widely used for security and privacy protection and this problem is meaningful for real-world applications.  The experiment results in table 1 and table 2 are promising.\nThe paper is well-organized and easy to follow. ", "summary_of_weaknesses": "The author only evaluates tinyBERT and doesn't consider the larger model BERT_base and BERT_large. When the number of encoders increases, the performance loss could drop significantly. ", "comments,_suggestions_and_typos": "in Figure 4, the blue line and orange line are almost overlapping, it is difficult to distinguish. Increasing the width of the line can help. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 129], [129, 233], [233, 347], [347, 442]], "summary_of_strengths": [[0, 134], [134, 135], [135, 196], [196, 244]], "summary_of_weaknesses": [[0, 99], [99, 185]], "comments,_suggestions_and_typos": [[0, 99], [99, 142]]}}}, {"rid": "24bcdd395c9d074324f11b34d7704ed7f9658bd6c815e6ac6cb98f512cfdf3d803f6207c49cabd3a7142e4419e5fa10772d3ac37320b9e45e7de00ce788f847d", "reviewer": "Ning Ding", "report": {"paper_summary": "This paper focuses on alleviating two challenges of privacy-preserving inference of pre-trained models. The first is how to protect users’ plain text data from access by third-party service providers, and the second one is the performance issue. The method of THE-X converts a fine-tuned model into a cloud service and processes users' data without peaking the data itself. And in inference, the text is anonymous and the results are computed in the ciphertext.  Homomorphic encryption (HE) is used to guarantee the solution and multiple approximation components in the transformer model are designed. Empirical results show that THE-X could conduct privacy-preserving inference with promising results. ", "summary_of_strengths": "- This is a pioneering work to explore the privacy-preserving transformer inference with HE. The paper presents a  whole pipeline with multiple approximation methods to support HE operations and conduct such privacy-preserving inference. The problem is worth exploring and the methods are reasonable.  - The paper is well-organized and well-written, it is easy to follow and understand the workflow and the contributions.\n- The paper analyzes the components and different schedules of approximation workflow of THE-X. ", "summary_of_weaknesses": "- If I understand correctly, In Tables 1 and 2, the authors report the best results on the **dev set** with the hyper-parameter search and model selection on **dev set**, which is not enough to be convincing. I strongly suggest that the paper should present the **average** results on the **test set** with clearly defined error bars under different random seeds.  - Another concern is that the method may not be practical. In fine-tuning, THE-X firstly drops the pooler of the pre-trained model and replaces softmax and GeLU, then conducts standard fine-tuning. For the fine-tuned model, they add LayerNorm approximation and d distill knowledge from original LN layers. Next, they drop the original LN and convert the model into fully HE-supported ops. The pipeline is too complicated and the knowledge distillation may not be easy to control.  - Only evaluating the approach on BERTtiny is also not convincing although I understand that there are other existing papers that may do the same thing. For example, a BiLSTM-CRF could yield a 91.03 F1-score and a BERT-base could achieve 92.8. Although computation efficiency and energy-saving are important, it is necessary to comprehensively evaluate the proposed approach. ", "comments,_suggestions_and_typos": "- The LayerNorm approximation seems to have a non-negligible impact on the performances for several tasks. I think it is an important issue that is worth exploring.\n- I am willing to see other reviews of this paper and the response of the authors.  - Line #069: it possible -> it is possible? ", "ethical_concerns": "NA "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Ning Ding, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 104], [104, 246], [246, 374], [374, 462], [462, 602], [602, 703]], "summary_of_strengths": [[0, 93], [93, 238], [238, 301], [301, 302], [302, 422], [422, 518]], "summary_of_weaknesses": [[0, 209], [209, 364], [364, 365], [365, 424], [424, 563], [563, 671], [671, 754], [754, 845], [845, 846], [846, 999], [999, 1090], [1090, 1222]], "comments,_suggestions_and_typos": [[0, 107], [107, 165], [165, 248], [248, 249], [249, 293]], "ethical_concerns": [[0, 3]]}}}]
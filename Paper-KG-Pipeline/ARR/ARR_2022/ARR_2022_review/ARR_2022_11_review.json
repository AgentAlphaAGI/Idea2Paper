[{"rid": "ab89f54929cacdbf98c0dc1870337be76003e140f4aa2692310e7ffdd1b9fcd2029d63f67579ba7aa7cd4ed95a73a63f7734bcd92632f7934d6248f6426fcace", "reviewer": "Fred Popowich", "report": {"paper_summary": "The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process. Their approach focuses on a context in which there is no need to explicitly identify a region corresponding to the answer for a generated question (so the task is “answer-unaware”). ", "summary_of_strengths": "The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful. It also contains well organized related literature. Overall, the paper is well written, and easy to understand for a large audience, with good use of examples. ", "summary_of_weaknesses": "The methodology section could be strengthened by further justification of some of the design decisions; for example, what models were considered in addition to T5 (line 148-152).  Were there other tasks that were considered beyond the three examined (line 153-154).\nThe evaluation section could benefit from further motivation for the three experiments shown; were other considered? Were there other datasets that were considered? ", "comments,_suggestions_and_typos": "How did you determine what questions you were going to ask the annotators? How is it related to what has been done in related research?\nWhat was the process for identifying annotators? Why only three annotators?  Glad to see that the \"full annotation data\" will be available, and I'm assuming that any other required data will also be available to other researchers as well. ", "ethical_concerns": "No ethical concerns. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Fred Popowich, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 207], [207, 389]], "summary_of_strengths": [[0, 181], [181, 233], [233, 341]], "summary_of_weaknesses": [[0, 179], [179, 266], [266, 383], [383, 431]], "comments,_suggestions_and_typos": [[0, 75], [75, 136], [136, 185], [185, 212], [212, 213], [213, 375]], "ethical_concerns": [[0, 21]]}}}]
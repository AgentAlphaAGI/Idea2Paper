[{"rid": "ebec57333f46f2bffc75b6573895650584ed72eaefcf54ab44394a50013760eb2db229be9aea5935ec4a0e98e42e2934a0eba90151207b3b968ba1b49cdafa54", "reviewer": null, "report": {"paper_summary": "This paper proposed a swift-super model pair that achieves significantly lower computation overhead by bypassing most of the simpler examples through the smaller swift model. The main novelty is that the paper proposed an energy-based interpretation of the swift model predictions, and results in estimating the density function of the examples suitable for the swift model. This is new to the community.  In addition, the paper also carefully constructs the probing position of the softmax layer, in order to enable the whole method applicable to seq2seq models that have a decoder. ", "summary_of_strengths": "1. Estimating the density function of the swift model is very interesting, and the author clearly explained the difference between this proposed method and others, such as softmax and entropy-based methods. \n2. Another point is that different from other models that are aimed at fast inference, this model enables their application on a seq2seq setting. \n3. The paper is clearly written, and the experiments are convincing. ", "summary_of_weaknesses": "1. While calculating the speedups, the authors are showing the FLOPs in the figures. Note that, since the proposed method requires each example to pass through a smaller model first and then on the bigger model, the two sequential steps actually _increase_ the processing time for those examples that are passed to the larger model. I suscept that if we show the speedup figures in the \"time\" dimension, it will be less favorable than \"FLOPs.\" So I'd be happy if the authors could also provide these figures in the \"time\" dimension. \n2. Some of the names seem not consistently referred to in the text. See \"comments, suggestions and typos.\" ", "comments,_suggestions_and_typos": "1. If I understand correctly, in Line 283, there seem to be typos on the operators in the equation -F(x;f) < t = log(...). \n2. There is an inconsistency in the definition of \"logits\" between Line 251 (where f_y(x) could negative, and are regarded as logits) and Line 391 (where logits is the normalized conditional probability of the i-th class.) "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 175], [175, 375], [375, 405], [405, 406], [406, 584]], "summary_of_strengths": [[0, 3], [3, 207], [207, 211], [211, 354], [354, 358], [358, 424]], "summary_of_weaknesses": [[0, 3], [3, 85], [85, 333], [333, 444], [444, 533], [533, 537], [537, 602], [602, 641]], "comments,_suggestions_and_typos": [[0, 3], [3, 123], [123, 127], [127, 347]]}}}, {"rid": "b9dd642435d7c4f925a0e47c19372f1a754fb7bff99c879ff12ccfed4a733f7cccf063ab77f07f18f7b54bed25a40d36231d16976912f4d22a1a4b8d75f4a70e", "reviewer": "Daniel F Campos", "report": {"paper_summary": "The authors introduce a dynamic inference approach that combines large and small models to provide the quality of the large model with the inference speed of the small model. Changing the problem of inference selection into a routing problem were based on the initial output of the encoder, an energy-based model determines if the swift (authors naming for the small model) is likely able to produce effective results. If the output of the encoder is far out of the distribution then the large model takes over to ensure there is quality.  To motivate and ground their work, the authors compare their method to the used of random and entropy dynamic inference to show that their energy-based method performs the best.  They evaluate their approach on GLUE and SuperGLUE finding a 3.3x and 2.9 speedup with almost no variation inaccuracy. ", "summary_of_strengths": "1. There is a strong framework for increasing inference without allowing degradation in quality which is seen in other models. \n2. The approach provides an easy-to-understand approach for combining the quality of models which is clearly not tied to any specific model and should scale with continued LM releases. \n3. The approach the authors introduce is able to be combined with distillation which further shows how robust their ideas are general and expandable. ", "summary_of_weaknesses": "1.While the approach proves to be better than entropy or random routing mechanism its not clear how a simple FFN network with a confidence threshold would perform instead. \n2. There is no clear breakdown on how this may work with a variety of architectures such as a swift BART and a Scale T5. ", "comments,_suggestions_and_typos": "1. What is the impact of using two different swift models in this architecture. Models with different random seeds could easily be ensembled and combined with this approach for routing. \n2. How would this approach scale to batch size > 1? \n3. What is the required memory overhead to keep  4. How is the performance impacted by changing the various scale models? Medium, another seed for swift model size, etc. ", "ethical_concerns": "None "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Daniel F Campos, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 175], [175, 419], [419, 539], [539, 540], [540, 718], [718, 838]], "summary_of_strengths": [[0, 3], [3, 127], [127, 131], [131, 313], [313, 317], [317, 464]], "summary_of_weaknesses": [[0, 172], [172, 176], [176, 294]], "comments,_suggestions_and_typos": [[0, 3], [3, 80], [80, 186], [186, 190], [190, 239], [239, 243], [243, 292], [292, 362], [362, 410]], "ethical_concerns": [[0, 5]]}}}, {"rid": "d32ecc3e071e982db4b1a0f762108f2c0dfe984b0cfaf3b57ccf4e8cff7cf517537f06e031e96d15cc310ec990853d72770e2cb655bab27d40cad5106bd0a341", "reviewer": null, "report": {"paper_summary": "This paper \"integrates Super and Swift language models for achieving efficient inference without sacrificing the accuracy\". I.e. it introduces a method to combine a faster but less accurate and a slower but more accurate model so that the final product is about two times faster than Super model but is as efficient. And it is also more memory efficient than many other BERTs as tested on GLUE devset. ", "summary_of_strengths": "1. The paper proposes a what seems to be very efficient approach to decreasing computational costs. Hence, strength 1: efficiency of solution. \n2. There is a lot of evidence that the approach is efficient: diagrams comparing it to state-of-the-art models, the code, lots of fresh and influential sources cited. \n3. The language (wording, grammar, etc) is perfect (minus a couple of typos). \n4. The paper is well-structured, in other words, classically structured: all the usually recommended components are in the right place. \n5. I've checked formulas in the cited source: LeCun et al., 2006. Seems plausible, although the authors got rid of beta and took derivative (meaning 'derivative of a function') of y, and I am missing how this happened. But the limitations in the size of papers wouldn't allow for a longer explanation anyway. From what my brief examination allows to say, the formulas look plausible. ", "summary_of_weaknesses": "1. The approach is derivative, meaning it takes two existing approaches and combines them. It's not bad. I'm fine with derivatives if they work. \n2. I have a feeling that the authors also took two precautions against immediate derivatives from their approach, and that gave them a couple of pages to add to their paper: paragraphs 3.3.1 Softmax-Based Mechanism and  3.3.2 Entropy-Based Mechanism explain why they did not use the softmax score and the entropy score, and that gave them one more paragraph: 4.1.1 Ablation Studies. But I'm also fine with this strategy. ", "comments,_suggestions_and_typos": "Consequently, earlier exists require -> exits Our method is easily.. orthogonal to many other existing methods (and later: To investigate the orthogonality of E-LANG with others)- I don't really get the meaning of 'orthogonal' and  'orthogonality' in this context. Seems that it is in the same direction with the general trends. \nand are also applicable for encoder-decoder -> is also "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 124], [124, 317], [317, 402]], "summary_of_strengths": [[0, 3], [3, 100], [100, 143], [143, 147], [147, 311], [311, 315], [315, 390], [390, 394], [394, 527], [527, 531], [531, 594], [594, 747], [747, 837], [837, 912]], "summary_of_weaknesses": [[0, 3], [3, 91], [91, 105], [105, 145], [145, 149], [149, 529], [529, 567]], "comments,_suggestions_and_typos": [[0, 265], [265, 329], [329, 385]]}}}, {"rid": "5e51a2ad4a8b7f52d8e38c3fb475a77f4cda084e63590f1df77c30688ada697ce19e7579ff1edeaf1d989dd2a36613e13c473e79ad3ff4f2cf35950a55fce4f8", "reviewer": null, "report": {"paper_summary": "This paper proposes a dynamic inference approach to speedup inference with large language models. \nIn particular, an energy based router is adopted to determine whether the swift model or the super model should be used to run inference on the given input example.  Experiments on classification as well as translation tasks show that the proposed approach achieves substantial speedup while retain the accuracy of the super model. ", "summary_of_strengths": "The paper provides solutions for both classification and sequence-to-sequence models. \nMore importantly, the proposed approach is simple and can be easily applied to existing production enviroments. \nThe experiments are solid, the proposed approach is tested against both BERT and T5 models on several benchmarks. \nExtensive ablation studies show that energy based approach is better than alternatives such as softmax-based approach and entropy-based approach. \nIn addition, it is great to see that combining the E-Lang with Distillation further improves model accuracy. \nReleasing the code helps others to reproduce the results. ", "summary_of_weaknesses": "This is no necessarily a weakness:  The approach proposed here reminds me a lot about coarse-to-fine inference [1, 2], where the coarse/weak model is used to drastically pruning the search space in order to speedup inference.  Although these approaches were developed for structured prediction tasks and were designed for non-neural models, it would still be nice to have discussion about the connection between E-lang and this line of work.\n[1] Improved inference for unlexicalized parsing [2] Structured Prediction Cascades ", "comments,_suggestions_and_typos": "It would be interesting to show/compare some examples that are routed to the swift model by energy/softmax/entropy based approaches. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 98], [98, 264], [264, 431]], "summary_of_strengths": [[0, 86], [86, 199], [199, 314], [314, 461], [461, 571], [571, 630]], "summary_of_weaknesses": [[0, 36], [36, 226], [226, 442], [442, 526]], "comments,_suggestions_and_typos": [[0, 133]]}}}]
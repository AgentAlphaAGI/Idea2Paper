[{"rid": "79c17105f5c8d45c44c56b028732e1a935c8d3c2b6c4cf514e0fced63e87cfe3af95e151c474115004b4e4b1d80f8718600760fd5885b52ecd0ad01dae985528", "reviewer": null, "report": {"paper_summary": "The paper proposed boundary smoothing as a regularization technique for span-based neural NER models. The boundary smoothing can mitigate the over-confidence issue by reassigning the entity probabilities from annotated spans to the surrounding ones. The approach is simple, and mainly extends the Yu et al (2020) by adding the boundary smoothing. The experiments show the proposed approach can improve the performance of Yu et al (2020). ", "summary_of_strengths": "The idea of the boundary smoothing is simple, and can improve the performance of the existing NER approach.\nThe in-depth analysis for the over-confidence issue is attractive.\nThe paper is well written. ", "summary_of_weaknesses": "The idea of using boundary information is not completely innovative. For example, the idea of effectively using boundary information has been proposed in Shen et al.(2021).\nThe experiments don’t prove the claims. Firstly, the paper lacks adequate verification for the proposed idea, and evaluated it with only one baseline. Secondly, the paper used RoBERTa while the previous methods used BERT. Thirdly, Yu et al. (2020) conducted the experiments on GENIA, but the paper doesn’t. ", "comments,_suggestions_and_typos": "The paper lacks adequate verification for the idea. The paper claims that the boundary smoothing can be easily integrated into any span-based neural NER systems, but the paper only integrated it into the Yu et al. (2020). The paper should further evaluate the boundary smoothing idea in several SOTA approaches.\nIn addition, the idea of effectively using boundary information has been proposed in Shen et al.(2021), which constructs soft examples for partially matched spans, and trains a boundary regressor with boundary-level Smooth L1 loss.\nIn the experiments, I have some concerns: (1)\tThe paper uses RoBERTa for the experiments, while the baselines used BERT. It may be unfair. \n(2)\tWhy the Baseline outperforms Yu et al. (2020) on ACE 04 and ACE 05 greatly, but has comparable performance on OntoNotes 5 and CoNLL 2003? Yu et al. (2020) also conducted experiments on the GENIA dataset, but the paper doesn’t report the results on this dataset. \n(3)\tAccording to the Table 4, the improvement of BS against LS is marginal.  Some statements are not very accurate. For example, Line 313, the submission says “this work is among the first to introduce a span-based approach to Chinese NER tasks and establish SOTA results.”, however, Shen et al. 2021 proposed a span-based approach previously, and conducted the NER experiment on Chinese Weibo dataset. "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 102], [102, 250], [250, 347], [347, 438]], "summary_of_strengths": [[0, 108], [108, 175], [175, 202]], "summary_of_weaknesses": [[0, 69], [69, 173], [173, 213], [213, 324], [324, 395], [395, 480]], "comments,_suggestions_and_typos": [[0, 52], [52, 222], [222, 312], [312, 544], [544, 665], [665, 683], [683, 826], [826, 950], [950, 1027], [1027, 1028], [1028, 1067], [1067, 1226], [1226, 1354]]}}}, {"rid": "e59cf68e846cef32b4bd1a1156957394e1650a4efa1704ec26a15e98466601456d3c79dc40b96046a46efe1206287ef8f1acbe1984b1c218d09880b0acf0ad06", "reviewer": null, "report": {"paper_summary": "This paper proposes a smoothing regularization technique for NER. The technique proposed targets boundary smoothing where the authors claim that inconsistent boundaries are often seen as a problem in annotated NLP NER datasets. The results after applying the method shows less over-confidence, better model calibration, flatter neural minima and more smoothed loss landscapes which plausibly explain performance improvement rather than directly/only addressing the inconsistent boundary span labeling problem in NER. The result attributes are empirically verified. ", "summary_of_strengths": "1. After 95% performance ranges, bringing improvements to any problem systematically is certainly a challenging dilemma. In this regard, the work proposed in this paper is already notable.\n2. The problem of inconsistent span annotations identified in this paper indeed can also be intuited as an NLP annotation problem hence is a valid one that needs to be addressed. The solution to regularize this problem with smoothing technique for span-based NER method is sound and well justified throughout the paper. The reader would benefit from reading the theory and methods in this paper.\n3. The code is publicly released.\n4. The empirical analysis is well performed. I especially appreciate section 5.2 which directly tries to shed light on the question that formed the problem premise of this work and I quote \"How does boundary smoothing improve the model performance?\" It offers a slight indication of a negative result as in not being able to quantitatively verify it owing to irregularities in distribution between synthesized boundary noise and actual noise in the datasets. Nevertheless, it shows another promising result with smoothed loss landscapes indicating sound machine learning settings.\n5. The fact that boundary smoothing addresses the over-confidence issue of target span predictions is quite meaningful, hence again a plus for this paper. ", "summary_of_weaknesses": "Not quite a weakness of this work, just a question I am wondering about. Considering that the boundary smoothing regularization distributes itself around the entity span, is it that the actual problem of whether such a function addresses the boundary irregularity problem in the NLP datasets can never be verified? It is indeed understood from the method explanation in the paper that it can to some extent. ", "comments,_suggestions_and_typos": "Table 5. Note the caption. There are no results for LS in the table. "}, "scores": {"overall": "5 = Top-Notch: This paper has great merit, and easily warrants acceptance in a *ACL top-tier venue.", "best_paper": "Yes", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 66], [66, 228], [228, 517], [517, 565]], "summary_of_strengths": [[0, 3], [3, 121], [121, 189], [189, 192], [192, 368], [368, 509], [509, 585], [585, 588], [588, 619], [619, 622], [622, 664], [664, 869], [869, 1078], [1078, 1200], [1200, 1203], [1203, 1355]], "summary_of_weaknesses": [[0, 73], [73, 315], [315, 408]], "comments,_suggestions_and_typos": [[0, 9], [9, 27], [27, 69]]}}}]
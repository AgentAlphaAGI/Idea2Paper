[{"rid": "e4ac3556d3516a52885850d0aefd91a4d53eb84f9686b4896f9ced9e9fb4bc3a9cf69127e23e965b27264c491b1c1ced150cc994fe7734b373f96bbd1a67da3a", "reviewer": null, "report": {"paper_summary": "The paper proposed a method for referring image segmentation. The basic architecture is based on the idea of cross-modal transformer. It processed linguistic feature and multi-layers of visual embeddings through the cross-modal transformer to obtain the multi-layer visual-linguistic embeddding. The multi-layer visual-linguistic embedding is further aggregated through the proposed hierarchical cross-modal aggregation. The proposed model achieves better performance than previous baselines. ", "summary_of_strengths": "The task of referring image segmentation is very interesting. I still wonder whether it is necessary to conduct visual grounding on pixel level. Even though, the task is very challenging and interesting. \nThe motivation example in Fig. 1 is pretty interesting. It shows the advantage of the proposed approach. ", "summary_of_weaknesses": "The proposed architecture seems like a cross-modal transformer (3.2 synchronous multi-modal fusion) and a multi-layer feature aggregation (3.3 hierarchical cross-modal aggregation). \nWith this standard cross-modal architecture, it is unclear to me why the proposed approach could resolve the example in Fig. 1, while the other approaches failed to achieve.\nSect. 3.3 puzzled me the most. It is unclear to me why the affinity weights should be defined as Eq.2? Why is the Convolution necessary? What is the motivation of choosing the specific form of hierarchical cross-modal exchange?  I feel the major issue of this paper is the motivation example is not closely connected to the proposed approach. Therefore, it is not clear to me why the proposed approach could solve the motivation example. ", "comments,_suggestions_and_typos": "None "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 62], [62, 134], [134, 296], [296, 421], [421, 493]], "summary_of_strengths": [[0, 62], [62, 145], [145, 204], [204, 261], [261, 310]], "summary_of_weaknesses": [[0, 182], [182, 357], [357, 363], [363, 388], [388, 460], [460, 494], [494, 585], [585, 586], [586, 700], [700, 795]], "comments,_suggestions_and_typos": [[0, 5]]}}}]
[{"rid": "a331c73408dcc74e2e0cddb0cd29ec5d9d58a7772f24c2e2d84f8bcad13f7bd53b391ff113fa24c42e5dbf0a4d22f7f3d682bb72d24f7bb7bc1eabc0a3cfe765", "reviewer": null, "report": {"paper_summary": "This paper is about improving the prosody of neural text-to-speech (NTTS) systems using the surrounding context of a given input text. The study introduced an extension to a well known NTTS system i.e., FastSpeech-2. The extension is a phoneme level conditional VAE. As cited in the current paper both FastSpeech-2 and conditional VAE are already proposed in the literature. The main novelty of this paper is representation of surrounding utterances using a pre-trained  BERT model and generation of prosodically varied samples with the help of learned contextual information. Authors followed standard TTS evaluation protocols to evaluate their proposed architecture, and evaluation results are in favor of the proposed architecture. ", "summary_of_strengths": "- This paper introduced a new component to FastSpeech-2, a well known non-autoregressive NTTS architecture, called as cross utterance conditional VAE (CUC-VAE).  - The CUC-VAE contains two main components 1) cross utterance (CU) embedding and 2) CU enhanced conditional VAE. ", "summary_of_weaknesses": "- As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading.  - The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.\n- It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this.  - In general, there are quite a few things missing -- details provided in comments section. ", "comments,_suggestions_and_typos": "**Typos:** - Background section: \"...high fidelity thank to…\" -> \"...high fidelity thanks to…\" - Background section: \" … Fang et al., 2019).Many…\" -> \" … Fang et al., 2019). Many…\" - Figure-1: \"...which integrated to into…\" -> \"...which integrated into…\" **Comments:** - Author did not mention how the initial durations of phonemes are obtained.\n- Are durations of phonemes predicted in frames or seconds?\n- Figure-1 did not mention how the proposed CUC-VAE TTS system works in the inference time. Moreover, it is hard to understand the color schema followed in the Figure-1, there is no legend.\n- There is no mentioning of train, valid and test set splits in the dataset section.\n- In Table-2 the baseline system received a better MOS score than the baseline + fine-grained VAE and baseline + CVAE, why is it? Whereas in Table-4 the baseline system show high MCD and FFE error than the baseline + fine-grained VAE and baseline + CVAE systems, why is it?\n- How do you represent the reference mel-spectrogram at phoneme level?\n- Did you use pre-trained HiFi-GAN to synthesize speech from the predicted mel-spectrograms? "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 135], [135, 217], [217, 267], [267, 375], [375, 577], [577, 735]], "summary_of_strengths": [[0, 161], [161, 162], [162, 275]], "summary_of_weaknesses": [[0, 147], [147, 148], [148, 378], [378, 544], [544, 545], [545, 637]], "comments,_suggestions_and_typos": [[0, 11], [11, 95], [95, 174], [174, 181], [181, 255], [255, 269], [269, 346], [346, 406], [406, 498], [498, 596], [596, 681], [681, 811], [811, 955], [955, 1026], [1026, 1119]]}}}, {"rid": "e42d651bc09e986d20ae0dffeb058df1964cbd04280553237939209a47f80c069be38d31a79dea7654f05bca31b819bcad9acfb0d1bbc047de61fe954a420162", "reviewer": null, "report": {"paper_summary": "This paper aims to improve the expressiveness of prosody in TTS in an intelligible and natural-sounding way. It is applied for read speech in English.\nThe method uses a conditional variational autoencoder to generate prosodic attributes for input text. The specific innovation of this paper is the cross-utterance conditioning, so that in order to synthesise prosodic speech for an utterance, the VAE is conditioned on text of several preceding and following utterances as well as the current one. This means, for example, that the phrase 'Mary asked the time' is given different prosody in the contexts 'Who asked the time? ------.' vs. '------, and was told it was only five'. The paper is clear about how this methodological contribution builds on and differs from other recent efforts to improve the prosody of TTS.  In addition, a set of several complementary evaluations are applied to judge the TTS output of this method. The subjective evaluations include human perceptions of naturalness, and AB testing for preference of systems with/out the cross-utterance context. Ablation studies demonstrate the contribution of different components. Objective evaluations include word error rate (corresponding to intelligibility), reconstruction error, and expressive diversity (energy and pitch variability). The results show that using cross-utterance context leads to greater expression of appropriate prosodic variation. ", "summary_of_strengths": "1. Extremely clear and informative writing, logical presentation of context/literature, what was done here, how, why, and what doing it that way means. \n2. Good motivation: using realistic linguistic context to determine output prosody, with the practical advantages of non-autoregressive TTS. \n3. Code is available, data and processing are fully described, demo page comparing various TTS outputs is nice too. \n4. The evaluations cover a lot of angles and taken all together they are convincing. \n5. Baselines seem good (though admittedly I'm not an expert here) and particularly appreciate the human judgements baseline for original (i.e. non-TTS, human-spoken) speech. ", "summary_of_weaknesses": "1. The sample that was used for the diversity evaluations evidently was based on 'three phonemes in randomly selected 11 utterances', this seems a bit puzzling as written. Either my understanding of what the sample consisted of is wrong and it could be clarified, or this decision could be briefly justified (e.g. was it the same 3 phoneme types in each utterance or a random 3 phonemes in each, presumably there was some logistical or theoretical reason not to use the full length of the 11 synthesised utterances, and is it safe to assume this sample is representative enough of diversity in the whole?) ", "comments,_suggestions_and_typos": "1. Figure 1 perhaps has space to make some of the text a bit larger "}, "scores": {"overall": "4.5 ", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 109], [109, 151], [151, 253], [253, 498], [498, 634], [634, 679], [679, 820], [820, 821], [821, 929], [929, 1077], [1077, 1148], [1148, 1309], [1309, 1424]], "summary_of_strengths": [[0, 3], [3, 152], [152, 156], [156, 294], [294, 298], [298, 411], [411, 415], [415, 497], [497, 501], [501, 672]], "summary_of_weaknesses": [[0, 3], [3, 172], [172, 606]], "comments,_suggestions_and_typos": [[0, 3], [3, 68]]}}}]
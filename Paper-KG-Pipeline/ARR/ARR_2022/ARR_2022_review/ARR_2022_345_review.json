[{"rid": "1ef6ccfd9e3537ad291ef842c8c6cf501be507c31bdd303a71e6c99d57931bcd905bc77f1b75112a96099d13213440fa5d49c5d4a71cf60f91c36b8e4f8106d3", "reviewer": null, "report": {"paper_summary": "This paper proposes a novel unsupervised sentence embedding framework named DiffCSE. DiffCSE learns sentence embeddings that are aware of, but not necessarily invariant to, MLM-based word replacement. Technically, DiffCSE combines the standard contrastive objective from SimCSE with a difference-prediction objective conditioned on the sentence embedding. Experiments on STS datasets and transfer tasks show the effectiveness of DiffCSE. ", "summary_of_strengths": "This paper is excellent in terms of both writing and technical novelty.\n- Writing: This paper provides sufficient background and includes a comprehensive survey of existing contrastive learning-based sentence embedding frameworks. I was able to read this paper very smoothly.\n- Novelty: The proposed framework is very simple but not naive. It is based on the idea of equivariant contrastive learning, which is first brought to NLP by the authors in the present paper, to my knowledge. The simple yet effective framework should inspire many readers. Besides, the authors confirmed the necessity of the components of the proposed method through thorough ablation studies. The thoroughness is almost unparalleled as a conference paper. ", "summary_of_weaknesses": "I was not able to find any major concerns. Some minor comments are written in the next section. ", "comments,_suggestions_and_typos": "- I am a little unconvinced by the discussion in lines 477 to 486. I speculate that sentence embeddings learned by DiffCSE focus more on superficial information than SimCSE in order to solve replaced token detection, but the result is the opposite. Why?\n- It would be insightful to investigate the impact of the model size of the encoder. In the past few years, I have repeatedly observed that a method that works well on small models has only a marginal impact on large models. This paper will be further strengthened if the effectiveness of the proposed method is shown even when large-scale encoders are employed. "}, "scores": {"overall": "5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "Maybe", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 85], [85, 201], [201, 356], [356, 438]], "summary_of_strengths": [[0, 72], [72, 231], [231, 276], [276, 340], [340, 485], [485, 549], [549, 670], [670, 733]], "summary_of_weaknesses": [[0, 43], [43, 96]], "comments,_suggestions_and_typos": [[0, 67], [67, 249], [249, 254], [254, 339], [339, 479], [479, 617]]}}}]
[{"rid": "211261c10d10f85b9b896e332dd025bf6e68354ff6c14e3e6f5182d7a022616d626664247503843cccbdaa70b8fb3d6295ae3a6c41f4af8ce62735c38a791fa2", "reviewer": null, "report": {"paper_summary": "In this paper, the authors propose to tackle the task of text style transfer where several constraints (topics, pronus, proper nouns, sentence length) are expected. In doing so, the authors propose to frame it as a GAN-based text generation with extra constraints as losses in the latent space. The authors demonstrated in the experiments that the proposed method could train constraints in lexical/syntactic/topics, in both automated and human evaluation. ", "summary_of_strengths": "This paper is generally a sounding work with the nice idea of cooperative loss. Also the experiments are thorough and solid, showing insights form multiple point of views (especially that mentioned in Sec 5. Discussion) as well as human evaluations. I think the readers would gain much from the detailed experiments. ", "summary_of_weaknesses": "I have some concerns on the technical details side. For example, it remains unclear how training examples are prepared (what are the $x_{src}$ and $x_{tgt}$). And probably as a result, it also becomes unclear why two sets of encoder/decoder are needed. An educated guess may be that ${src}$ and ${tgt}$ are in two different domains (like the CycleGAN that the authors mentioned to be the inspiration), but this is also not clarified. As the domain of data matters in the choice of architecture for generative models, this ambiubality makes it hard to gain methodological insights from this work.\nAs most of the constraints are done in the latent space, I would also like to see more quantitative analysis of the latent space (such as whether there is any identified directions in latent space that bares certain semantics) or qualitative ones (such as visualizing the latent space) to gain more insights into the proposed method.\nFurthermore, I also have some moderate concerns regarding the presentation. Although arguably the presentation is much easier to follow compared with the previous PDF version, there are still some issues in the writing of the proposed method: It remains unclear how constraints (Sec 4) are reflected in losses (Sec 3.2.1 and Sec 3.2.2). ", "comments,_suggestions_and_typos": "N/A "}, "scores": {"overall": "3.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 165], [165, 295], [295, 457]], "summary_of_strengths": [[0, 80], [80, 208], [208, 250], [250, 317]], "summary_of_weaknesses": [[0, 52], [52, 159], [159, 253], [253, 434], [434, 596], [596, 930], [930, 1006], [1006, 1267]], "comments,_suggestions_and_typos": [[0, 4]]}}}, {"rid": "ca178041ce19d9f64300b89d7168b8737f071bedfaec410be45427d6f3a936ed1c9887e942f9d20924dc6fc29cddfd9615a43cc5a35147a4c6d051fdd272c28c", "reviewer": null, "report": {"paper_summary": "This paper discusses how to transfer text from one domain to another by preserving some attributes (constraints). The paper proposes an extension to the ARAE architecture. In particular, the authors introduce cooperative losses to better model the constraints. The proposed method seems reasonable and in general using the cooperative losses seem nice. The extensive experimental evaluation seems to confirm the validity of the model. ", "summary_of_strengths": "The presented method is reasonable. Moreover, the presented problem is relevant to the CL community. The experimental evaluation is extensive and it provides many insights. ", "summary_of_weaknesses": "The main concerns with this paper is that it doesn't fully explain some choices in the model (see comments/questions section). Moreover, some parts of the paper are actually not fully clear. Finally, some details are missing, making the paper incomplete. ", "comments,_suggestions_and_typos": "- Algorithm 1 is not really explained. For example, at each step (1, 2, 2a, 3, 3a) are you sampling a different batch from S and T? Is the notation L(X) meaning that you optimize only the parameters X of the architecture?\n- Line 232: When you say you \"mine\", what do you exactly mean? Does this mean you sample P sentences from the set of sentences of S and T with similar constraints?\n- Lines 237-238 and Line 262: Why would you want to use the representation from the critic last layer?  - Line 239: \"Ci are a set of constraints for a sentence\" should be moved before.\n- Table 1: It seems that the results for DRG and ARAE are not averaged over 5 runs (they're exactly the same of the previous paper version) - Table 1: How did you choose the p=0.6?\n- Table 1: row=ARAE, column=POLITICAL-FL It seems this value should be the one in bold.\n- Lines 349-353: It seems you're comparing results for ARAE + CONTRA, ARAE + CLF and ARAE + CONTRA + CL with respect to simple ARAE, while in the text you mention only ARAE + CONTRA and ARAE + CLF.\n- Line 361: and SIM to -> and SIM with respect to - Figure 3: Please, rephrase the caption of the errors bars (or explain it in the text). It is not clear what do you mean.\n- Line 389: You mention here you used different p values as in Table 1. This table doesn't report results with different values for p. - Lines 422-423: why using nucleous sampling when the best results were with greedy decoding? Where does 0.9 come from?\n- In general, in the experiments, what are the source and target domains?\n- Line 426-Table4: What do you want to demonstrate here? Could you add an explanation? What constraints/attributes are preserved? What is the source domain? What is the target domain?\n- Lines 559-560: This is not entirely true. In Cycle Consistency loss you can iterate between two phases of the reconstructions (A-B-A and B-A-B) with two separate standard backpropagation processes.\n- Line 573: works focuses -> works focus "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 114], [114, 172], [172, 261], [261, 353], [353, 435]], "summary_of_strengths": [[0, 36], [36, 101], [101, 173]], "summary_of_weaknesses": [[0, 127], [127, 191], [191, 255]], "comments,_suggestions_and_typos": [[0, 39], [39, 132], [132, 222], [222, 285], [285, 386], [386, 489], [489, 490], [490, 571], [571, 711], [711, 752], [752, 840], [840, 1038], [1038, 1088], [1088, 1177], [1177, 1211], [1211, 1283], [1283, 1346], [1346, 1440], [1440, 1466], [1466, 1540], [1540, 1597], [1597, 1627], [1627, 1670], [1670, 1697], [1697, 1724], [1724, 1768], [1768, 1924], [1924, 1965]]}}}, {"rid": "86457c43345f4b59482376208a496b21bd95a29782aa979e9a8d205335ace99114e4acb17624ddf75457a7545151b870e4da636db5231e613687a553c89d2052", "reviewer": "Nora Hollenstein", "report": {"paper_summary": "This paper presents a new method to constrain certain lexical, syntactic and domain-specific aspects of sentence during style transfer. The method is based on the introduction of two complementary losses, a self-supervised contrastive loss that controls the distance between samples and a classification loss that aims to satisfy the constraints. The method is evaluated on multiple datasets, using a range of automatic metrics as well as a human evaluation. The authors highlight the importance of enforcing such constraints of identity (e.g., text length and descriptiveness) during style transfer, which can be useful for data augmentation and domain adaptation applications. ", "summary_of_strengths": "This work proposes a useful method transfer from vision to NLP. The proposed method can be useful to various applications in NLP, it might improve model robustness in data augmentation and domain adaptation scenarios. The paper accurately summarizes related work and reflects upon open issues. The presented problem is addressed with an appropriate method and is evaluated extensively. ", "summary_of_weaknesses": "I have reviewed this paper in the previous round and was happy to see that the authors have addressed the previously found weaknesses: - Method: The method section has been re-structured and as a result became much easier to read.\n- Human evaluation: The appendix now provides more extensive information, including details on annotator consent.\n- The applications of this work are described more clearly - One thing that is still missing is a short discussion of possible limitations. ", "comments,_suggestions_and_typos": "- Some crucial information is only presented in the appendix (most importantly the details on the evaluation metrics, which cannot be understood without reading the appendix). This should be included in the main paper if accepted. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Nora Hollenstein, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 136], [136, 347], [347, 459], [459, 679]], "summary_of_strengths": [[0, 64], [64, 218], [218, 294], [294, 386]], "summary_of_weaknesses": [[0, 135], [135, 231], [231, 345], [345, 404], [404, 485]], "comments,_suggestions_and_typos": [[0, 176], [176, 231]]}}}]
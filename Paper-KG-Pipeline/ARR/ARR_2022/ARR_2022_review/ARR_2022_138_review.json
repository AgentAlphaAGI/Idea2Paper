[{"rid": "05bb7fb6c2f67986bb57b62cc394f8f195da4af9698e6062b6102bea1dc2fa1a83dc2ee06d1ef4a1953933e8401643b026d63246ad4449320230d22536fead18", "reviewer": null, "report": {"paper_summary": "This paper proposes a new problem of information extraction, text-to-table. Given a text, the model is required to produce the corresponding tables, including both schema and cell values. Two baselines have been developed for this task: a traditional information extraction baseline (relation extraction/name entity recognition); vanilla seq2seq model finetuned from pretrained language models. Apart from that, the authors propose two techniques, table constraint and relation embeddings for the seq2seq model to further boost the performance. The datasets are adapted from four existing table-to-text datasets, i.e., Rotowire, E2E, WikiTableText and WikiBio. The evaluation metric is calculated based on the number of correct non-header cells in the tables. Extensive experiments have been conducted, where the proposed method can beat the two baselines on average. ", "summary_of_strengths": "1. This paper introduces a novel task, text-to-table. It is a reverse task of the original table-to-text task. It could benefit some downstream tasks like text mining. It feels like a good alternative to OpenIE. \n2. The authors propose two simple techniques, i.e., table constraint and relation embeddings, on top of seq2seq models for this task. Specifically, table constraint forces the rows to contain the same number of cells as the header. Relation embeddings are injected into the self-attention mechanism of the transformer to guide the model to specifically attend to the corresponding header when generating a cell value. The proposed methods can help beat the vanilla seq2seq baseline in experiments. \n3. The paper is clearly written except for several typos. ", "summary_of_weaknesses": "1. The problem formulation is flawed. The authors argue that one major motivation of this paper is to learn schema in a data-driven way other than laborious manual schema engineering. However, on the proposed four datasets, I feel that the schemas are somehow easy to learn. For example, on E2E, WikiTableText and WikiBio, only two columns are involved. According to my understanding, there is no discrepancy between training and testing w.r.t. the table schema. Things on the fourth dataset, Rotowire, feel more complicated. There are two tables, containing around 5 and 9 columns respectively. It's not clear whether a difference exists between training and testing. And the evaluation metric only considers the cell values while ignoring the schema. That's another reason I feel the schema is fairly easy to learn in the proposed setting. Furthermore, under the current setting, this problem sounds more like another type of machine reading that the model is asked to fill in the table as the schema is easy to predict. So I highly suggest considering schema generalization where the schemas differ between training and testing and additionally evaluating schema prediction. \n2. It's not surprising that the major performance contribution comes from the pretrained language model (BART). But compared to that, the gain obtained from the proposed method is marginal (Table 5). ", "comments,_suggestions_and_typos": "1. In Section 3 (line 247-252), I am wondering tables are divided into three types. For me, one type (the column header) should work. \n2. Several typos exist in the paper. \n3. Like I suggest in the weaknesses, more exploration of schema learning and generalization will make this paper much stronger and more interesting. Another workaround is to rephrase the motivation to make this paper focus more on the non-header part. ", "ethical_concerns": "n/a "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 76], [76, 188], [188, 395], [395, 545], [545, 661], [661, 760], [760, 868]], "summary_of_strengths": [[0, 3], [3, 54], [54, 111], [111, 168], [168, 212], [212, 216], [216, 347], [347, 445], [445, 631], [631, 711], [711, 715], [715, 770]], "summary_of_weaknesses": [[0, 3], [3, 38], [38, 184], [184, 275], [275, 354], [354, 445], [445, 463], [463, 526], [526, 596], [596, 669], [669, 753], [753, 842], [842, 1023], [1023, 1178], [1178, 1182], [1182, 1291], [1291, 1379]], "comments,_suggestions_and_typos": [[0, 3], [3, 84], [84, 134], [134, 138], [138, 172], [172, 176], [176, 322], [322, 425]], "ethical_concerns": [[0, 4]]}}}, {"rid": "88ed4aaf6d48ced87c005d808b0cfa4d8f384776fc224a1b1971b299b60ea7ee7a2ad281f928cf9c0e9e19f56ddfb72529ddb77f311f2a6081c62a06e87ba70a", "reviewer": null, "report": {"paper_summary": "This paper poses a problem setting named text-to-table, as a new way of information extraction. They study it as an inverse problem of table-to-text (a well-studied problem). To realize this goal,  the author formalizes this task as a seq2seq problem via equipping BART with two extensions: table constraint and table relation embeddings. Experiment on four popular table-to-text dataset reveals the effectiveness of proposed methods (a little marginal). ", "summary_of_strengths": "1. It proposes an interesting task named text-to-table to realize effective information extraction. \n2. The challenging issues discussed in Sec 5.6 and Appendix. F is very clear. ", "summary_of_weaknesses": "1. The paper need further polish to make readers easy to follow.\n2. In Table 6, the improvement of method is marginal and unstable.\n3. The motivation of this new task is not strong enough to convince the reader.  Is it a necessary intermediate task for document summarization and text mining (as stated in L261)?\n4. It directly reverse the table-to-text settings then conducts the experiments on four existing table-to-text datasets. More analysis of the involved datasets is required, such as the number of output tables, the size/schema of the output tables. ", "comments,_suggestions_and_typos": "Questions: 1. L68-L70, Is there any further explanation of the statement \"the schemas for extraction are implicitly included in the training data\"?\n2. How to generate the table content that not shown in the text?\n3. Why not merge Table 1 and Table 2? They are both about the statistics of datasets used in experiments.\n4. What’s the relation between text-to-table task and vanilla summarization task?\n5. How to determine the number of output table(s)? Appendex. C don't provide an answer about this.\n6. What’s the version of BART in Table3 and Table 4?\nSuggestions: 1. The font size of Figure 2 and Figure 3 is too small.\nTypos: 1. L237: Text-to-table -> text-to-table 2. L432: \"No baseline can be applies to all four datasets\" is confusing.\n3. Table 3: lOur method -> Our method "}, "scores": {"overall": "2.5 ", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)", "software": "2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 96], [96, 175], [175, 339], [339, 455]], "summary_of_strengths": [[0, 3], [3, 100], [100, 104], [104, 162], [162, 179]], "summary_of_weaknesses": [[0, 3], [3, 65], [65, 68], [68, 132], [132, 135], [135, 212], [212, 313], [313, 316], [316, 434], [434, 561]], "comments,_suggestions_and_typos": [[0, 11], [11, 14], [14, 148], [148, 151], [151, 213], [213, 216], [216, 251], [251, 319], [319, 322], [322, 401], [401, 404], [404, 452], [452, 462], [462, 500], [500, 503], [503, 553], [553, 569], [569, 622], [622, 632], [632, 669], [669, 672], [672, 742], [742, 745], [745, 780]]}}}]
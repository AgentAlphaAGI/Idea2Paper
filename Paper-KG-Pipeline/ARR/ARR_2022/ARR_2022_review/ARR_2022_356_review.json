[{"rid": "bf4dd391d2c040db6b314ae75e8ac18213b8769c6aa163fdd53a883921985423d9584cfa38c32ca593149520bdec74074db5c2677e143c7d30ffa03395b7e45f", "reviewer": "Yi-Ling Chung", "report": {"paper_summary": "This paper presents a hate speech dataset XTREMESPEECH containing 20,297 social media passages from Brazil, Germany, India and Kenya. Different from most existing datasets, XTREMESPEECH was collected and annotated by fact-checkers who are both independent workers in local communities and targets of online extreme speech themselves. The passages were annotated as derogatory, exclusionary or dangerous, in which the last two requires removal. Annotators also annotated the targets of extreme speech. The paper further establishes XTREMESPEECH baselines on 4 tasks: predicting the extremity of speech, if a passage should be removed, hate target identification, and zero-shot cross-country classification.  The paper is easy to read. While I do like the distinction between different types (levels) of extreme speech, I would expect to see more clarifications with examples among the 3 types extreme speech and if there are culture differences in the annotations across countries. ", "summary_of_strengths": "1. The authors put a huge work in dataset curation. The experiments appear to be reasonable. \n2. A collection of cross-country and multilingual hate speech is provided, in principal, reflecting the current social and political situation in a country. This dataset will be useful for addressing hate speech from a multilingual perspective. ", "summary_of_weaknesses": "1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance \"I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार\" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification? - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558). \nI understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.\n2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria? ", "comments,_suggestions_and_typos": "1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation? \n2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant. \n3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching). "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yi-Ling Chung, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.", "sentences": {"paper_summary": [[0, 134], [134, 334], [334, 444], [444, 501], [501, 706], [706, 707], [707, 734], [734, 981]], "summary_of_strengths": [[0, 3], [3, 52], [52, 93], [93, 97], [97, 251], [251, 339]], "summary_of_weaknesses": [[0, 3], [3, 104], [104, 247], [247, 429], [429, 548], [548, 665], [665, 777], [777, 870], [870, 965], [965, 968], [968, 1147], [1147, 1183], [1183, 1205]], "comments,_suggestions_and_typos": [[0, 3], [3, 73], [73, 99], [99, 103], [103, 181], [181, 254], [254, 303], [303, 307], [307, 470]]}}}]
[{"rid": "4d5fa3ee481b64dfb8c94afe7d4c2cc4accc18a0de05c6d384f60fe12b9e17d728296cddb1eca99970fa6e7fc7c8253a952411295d54db19877e743853abf5dc", "reviewer": null, "report": {"paper_summary": "The paper is essentially an exploration of the design space for seq2seq architectures that produce code from natural language. The paper explores the use of pretrained language models for the encoder as well as the use of an output vocabulary that guarantees grammatically valid code after a mapping is applied, but using Early parser actions as the output. The latter is one of the key contributions of the paper, allowing the comparison of different architectures with direct decoding into code and decoding into grammatical operations.\nThe paper is valuable as a replication and validation of previous work. It finds that the use of copying directly from the language input is the biggest contributor to performance, with grammatical outputs being another strong factor. ", "summary_of_strengths": "This paper validates previous work and makes it clear which components are important to pay attention to when building a text to code model. This will be valuable guidance for future research and implementation. The paper has a detailed discussion of the different settings including a qualitative discussion (which unfortunately had to be moved into the appendix). The paper makes very good use of detailed examples to make it clear what is being done. ", "summary_of_weaknesses": "- The number of datasets used is relatively small, to really access the importance of different design decisions, it would probably be good to use further datasets, e.g., the classical GeoQuery dataset.\n- I would have appreciated a discussion of the statistical properties of the results - with the given number of tests, what is the probability that differences are generated by random noise and does a regression on the different design decisions give us a better idea of the importance of the factors?\n- The paper mentions that a heuristic is used to identify variable names in the Django corpus, however, I could not find information on how this heuristic works. Another detail that was not clear to me is whether the BERT model was fine tuned and how the variable strings were incorporated into the BERT model (the paper mentions that they were added to the vocabulary, but not how). For a paper focused on determining what actually matters in building a text to code system, I think it is important to be precise on these details. ", "comments,_suggestions_and_typos": "It would take some time to implement your task for other corpora, which potentially use different programming languages, but it might be possible to still strengthen your results using bootstrapping. You could resample some corpora from the existing two and see how stable your results are. \nIf you have some additional space, it would also be interesting to know if you have discuss results based on types of examples - e.g., do certain decisions make more of a difference if there are more variables?\nTypos: - Page 1: \"set of value\" -> \"set of values\" \"For instance, Orlanski and Gittens (2021) fine-tunes BART\" -> \"fine-tune\" - Page 2: \"Non determinism\" -> \"Non-Determinism\" "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 127], [127, 358], [358, 539], [539, 611], [611, 774]], "summary_of_strengths": [[0, 141], [141, 212], [212, 366], [366, 454]], "summary_of_weaknesses": [[0, 203], [203, 505], [505, 667], [667, 889], [889, 1037]], "comments,_suggestions_and_typos": [[0, 200], [200, 291], [291, 503], [503, 510], [510, 629], [629, 678]]}}}]
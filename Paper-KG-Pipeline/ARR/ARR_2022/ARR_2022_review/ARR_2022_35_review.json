[{"rid": "4aecdba508d4b3430d7e9fb9b1991de8e04e8fcff4fb43ad45484536a4f0a586e7a1e58298ed4357571dc05915e0825af63939334a5d7c1ce0cbf71c9b72c962", "reviewer": null, "report": {"paper_summary": "Authors first convert text and audio data to phonetic representation. Then they pre-train the language model based on phonetic data and then fine-tune it in the Swahili NER task. Various combinations for pre-learning are considered: only text, only audio, text + audio, text in a more resourced related (Kinyarwanda) language, audio in a more resourced related language.  It is shown that pre-training in a more resourced language gives a slight improvement compared to the model without pre-training in simple NER tasks, where you just need to determine whether or not there are named entities in the text, or to determine the presence and number of named entities in the text. On a more complex NER task, in which you need to determine the type and location of named entities, pre-training gives a significant performance gain in comparison with a model that was not pre-trained. ", "summary_of_strengths": "- It's a sensible idea to use the maximum available resources, and not be limited to text only.\n- The paper is well written. ", "summary_of_weaknesses": "- There is no compelling advantage of adding audio data to pre-training: text-only pre-learning (SNER+ST1) is better than or the same as text-only pre-learning (SNER+SAT).\n- The idea was assessed only on one task (NER), albeit with three varieties. A more extensive assessment and deeper analysis is expected from a long article. ", "comments,_suggestions_and_typos": "- Tables 1&2: please add standard deviations since you anyway have already run each model at least three times - Table 1, caption: consider removing \"calculated with the scikit-learn library. ( Pedregosa et al., 2011)\", because an average can be calculated anywhere (say in Excel), and you don't need the scikit-learn for that.\n- Figure 4: if the y-axis is indeed in percents, then the ticks should be 0, 1, 2, etc. Otherwise, one can think that the improvements are in hundredths of a percent (very small).\n- line 499: corpus (David, 2020) ~~corpus~~ ", "ethical_concerns": "No concerns "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 70], [70, 179], [179, 371], [371, 372], [372, 679], [679, 882]], "summary_of_strengths": [[0, 96], [96, 125]], "summary_of_weaknesses": [[0, 172], [172, 249], [249, 330]], "comments,_suggestions_and_typos": [[0, 111], [111, 194], [194, 328], [328, 416], [416, 508], [508, 552]], "ethical_concerns": [[0, 12]]}}}]
[{"rid": "b9180336235e32229e3d5db6d509179a89c4af064e31f823bfc9b1f2c30afe037c6bd6714829f516cd1924a7b032c6ac3bd52bec91bcd9b6d03e185642a5ad75", "reviewer": "Zirui Wang", "report": {"paper_summary": "This paper studies the problem of lifelong learning in the context of language model pretraining. The goal is to continuously learn new pretraining data domains for a language model such that we do not need to retrain a new one from scratch. They propose a method called ELLE, which aims to expand the current model's width and depth upon a new data source is available. A few related techniques are also applied to further boost performance, e.g. function recovering finetuning, domain specific tokens. Overall, the method is valid with plenty of experimental results, yet its practicality remains unclear. ", "summary_of_strengths": "1. This paper studies lifelong learning for model pretraining. This setup is less studied in lifelong learning literature. \n2. The proposed method works well on all experiments considered, with an astonishing amount of analysis (in the appendix). \n3. The paper is easy to follow and well written. ", "summary_of_weaknesses": "A critical limitation is how practical the method is given the nature of model expansion. This method is studied under a still relatively synthetic setup. There are a lot of problems that I can imagine for real-world large scale models such as GPT3. To mention a few: (1) How should we decide when to expand the model? In reality, these models are trained on O(100B) data, covering a significant portion of the whole internet. So when do we know a new data source emerges? How to detect this? ( 2) Large scale models are trained using sharding systems of thousands of accelerators, it is a challenging technical question to expand them in a way described in this paper. ( 3) Recent works have shown it is possible to continue training language models for either language understanding [1] or additional applications such as code synthesis [2]. Therefore, perhaps these simple approaches are sufficient enough for good generalization.\n[1] Finetuned Language Models Are Zero-Shot Learners. Wei et al., 2021. \n[2] Program Synthesis with Large Language Models. Austin et al., 2021. ", "comments,_suggestions_and_typos": "How is the prompt token computed at inference time? Is it included using nearest neighbor search or not included at all? "}, "scores": {"overall": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Zirui Wang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 98], [98, 242], [242, 371], [371, 504], [504, 608]], "summary_of_strengths": [[0, 3], [3, 63], [63, 123], [123, 127], [127, 247], [247, 251], [251, 297]], "summary_of_weaknesses": [[0, 90], [90, 155], [155, 250], [250, 319], [319, 427], [427, 473], [473, 495], [495, 672], [672, 844], [844, 934], [934, 988], [988, 1006], [1006, 1057], [1057, 1078]], "comments,_suggestions_and_typos": [[0, 52], [52, 121]]}}}]
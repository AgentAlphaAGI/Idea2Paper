[{"rid": "9f16d7fbb89be40b6fa6ff149aa34cfcad08f2a811b1bef57d6136992bef789a65da497e685cbeb4e29335b120aaf58d3d7240a775eb25c9fe8bec3fa6bf4ffc", "reviewer": null, "report": {"paper_summary": "This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task. Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability. In addition, the word embedding and the output representation are used together to compute generation probability. The experiment results show the proposal method outperforms baseline model on two benchmark datasets. ", "summary_of_strengths": "1. The paper is well organized, and it explains the proposed solution clearly. ", "summary_of_weaknesses": "1. The idea is incremental. Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers.\n    [1] Deaton, Jon, et al. \"Transformers and pointer-generator networks for abstractive summarization.\" ( 2019).\n    [2] Prabhu, Nikhil, and Katharina Kann. \" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.\"\n2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version. ", "comments,_suggestions_and_typos": "1. What’s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage? "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 132], [132, 252], [252, 367], [367, 469]], "summary_of_strengths": [[0, 3], [3, 79]], "summary_of_weaknesses": [[0, 3], [3, 28], [28, 139], [139, 246], [246, 253], [253, 299], [299, 374], [374, 377], [377, 429], [429, 523]], "comments,_suggestions_and_typos": [[0, 3], [3, 110]]}}}]
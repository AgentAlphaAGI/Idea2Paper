[{"rid": "afd3a6d8a0a1701f3cfb9f4f4d33c7a5cdab956dc0e3277b0fc691fb05366e612fb54cd58de85735547a3e57573b4c0694c65149392f6c83b32985764a11eeaa", "reviewer": "Nan Jiang", "report": {"paper_summary": "This paper study the learned embeddings' distribution from the CharacterBERT model using a new analysis tool (i.e., information axis). It argues that the currently used character sequence are limited and there are many distorted character sequences that are popular in speech analysis, vocal, pseudowords are not considered. In experiments,  the analysis and results based on the information axis demonstrate the embedding of deep models learned is far apart from the extant character sequences. ", "summary_of_strengths": "The paper is enjoyable to read, which makes it easy to understand the main idea. The new analytical tool (information axis along with UMAP) extends the current analysis over extant words/subwords. The authors give a solid analysis and result on analyzing those \"garble\" (sub-), paralanguage, pseudowords and morphology of words. It is a good practice for ExplainAI that try to understand the learned character-sequence distribution in the high-dimensional space. ", "summary_of_weaknesses": "no major weakness is observed. ", "comments,_suggestions_and_typos": "- The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.\n- For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.\n- It would be better to define some notations and give a clear definition of the \"information axis\", \"word concreteness\" and also \"Markov chain information content\".\n- Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE. "}, "scores": {"overall": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.", "best_paper": "No", "replicability": "5 = They could easily reproduce the results.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Nan Jiang, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 135], [135, 325], [325, 496]], "summary_of_strengths": [[0, 81], [81, 197], [197, 329], [329, 463]], "summary_of_weaknesses": [[0, 31]], "comments,_suggestions_and_typos": [[0, 98], [98, 219], [219, 493], [493, 597], [597, 631], [631, 797], [797, 907], [907, 962], [962, 1041]]}}}]
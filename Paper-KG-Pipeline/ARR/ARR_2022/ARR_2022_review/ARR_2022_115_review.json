[{"rid": "2e201a8e027839a0a11cf2338bcd103dba544ad4421e97e62a9580843bb66aa6270b7c940568df3085e283317f5255589a7e1c410a5e1aad3a2205f640730cef", "reviewer": "Sang-Woo Lee", "report": {"paper_summary": "The authors proposed Context-Aware Language Models (CALM).\nAs far as I understood, CALM changes (relabels) DB in the failed dialogue to make the dialogue successful (dialog task relabeling).\nAs dialog task relabeling is not enough to make successful performance, three additional auxiliary tricks are suggested: task-specific auxiliary loss, task pretraining, and model-based dialogue rollouts.  The proposed method is evaluated in AirDialogue dataset. The authors argue that, in the self-play scenario of AirDialogue dataset, the proposed method achieves state-of-the-art performance and achieved human-level performance. ", "summary_of_strengths": "- The idea of dialog task relabeling is novel. ", "summary_of_weaknesses": "- It is a little bit hard to understand the paper     - I think the proposed idea can be easily explained without POMDP or RL. It is also less persuasive for me that the proposed method is described from the perspective of RL. Thus, I feel the description on POMDP could be removed. \n    - Some relatively important description goes to Appendix, which makes reduced readability.\n- Motivation of the methods is less persuasive. \n    - It is weird for me that dialogue without success becomes successful dialogue by changing the contents of DB. \n    - An example of relabeled dialogue (in Appendix) would help.\n- The used dataset is not been frequently used recently, and recent TOD methods are not also been evaluated. \n    - It seems it is necessary to explain why the method could not be compared to mainstream datasets (e.g., MultiWOZ 2.0) and methodologies.\n- I feel evaluation with self-play is less confident. \n    - Though, previous works evaluate their methods by self-play, I feel that human evaluation is required for robust evaluation. \n    - I think that it is relatively easy to fit the self-play scenario. ", "comments,_suggestions_and_typos": "See the Summary of Weaknesses "}, "scores": {"overall": "2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Sang-Woo Lee, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.", "sentences": {"paper_summary": [[0, 59], [59, 191], [191, 395], [395, 396], [396, 453], [453, 623]], "summary_of_strengths": [[0, 47]], "summary_of_weaknesses": [[0, 127], [127, 227], [227, 283], [283, 379], [379, 427], [427, 543], [543, 609], [609, 718], [718, 861], [861, 915], [915, 1046], [1046, 1119]], "comments,_suggestions_and_typos": [[0, 30]]}}}, {"rid": "a649a9267d49174512ea282c0f21d08b743bf94b491455b77c1a5879cd1b3eb72a1afeac7861274998aafc5eb824313246019b9f6658cf01e02f377b944c7e33", "reviewer": "Yassine Benajiba", "report": {"paper_summary": "Modeling a dialogue as POMDP, this paper reports an approach and a training recipe for end to end solution to generate task oriented dialogue. Differently from existing approaches, the proposed approach doesn't rely on manually designed dialogue states and realization schemas. The approach also resorts to task relabeling and auxiliary loss which prove to be instrumental to achieve good results according the conducted ablation study. Reported results outperform SOTA on AirDialogue dataset. ", "summary_of_strengths": "- Novelty in approaching the end 2 end dialogue generation from a combination of change in loss function and relabeling technique - Results are interesting for an approach where we'd expect the end2end approach to underperform (because structured context and structured output are curcial) or perform at par with pipeline approaches. ", "summary_of_weaknesses": "- Although I don't see anything standing against the publication of this paper, I think results reported on one dataset are a weakness for this paper. Since the main angle of this research work is to build a generalized model for generating dialogue end 2 end, the generalizibility of the model can only be shown by demonstrating good results on a bigger set of data.\n- Authors have not shared any qualitative assessment of the model or error analysis. I think specially when we're talking about an end2end solution, it is very interesting to see undesirable examples. Numbers show how often the model is correct but we don't know how far the model deviates from expected dialogue when it generates incorrect conversations. ", "comments,_suggestions_and_typos": "None "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Yassine Benajiba, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 143], [143, 278], [278, 437], [437, 494]], "summary_of_strengths": [[0, 130], [130, 334]], "summary_of_weaknesses": [[0, 151], [151, 368], [368, 453], [453, 569], [569, 724]], "comments,_suggestions_and_typos": [[0, 5]]}}}]
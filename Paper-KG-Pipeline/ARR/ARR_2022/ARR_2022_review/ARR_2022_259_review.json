[{"rid": "2fcbf792854adc3e835bebe1dc036f6f69d674be2ed2dd0d109bc27164c3de5de3ec5f4996cc7883a351e3340f373941b0c6789ab33d8d5e5a5410c6d5cd8fb3", "reviewer": null, "report": {"paper_summary": "The paper presents a method for representing the relevance of a linguistic dataset to the corresponding language and its speakers. As a proxy for speakers of a certain language the authors use geographical entities, particularly countries. The representation they aim to build relies on entity linking, so the authors explore this problem on several multilingual datasets, and draw conclusions regarding the cross-lingual consistency of NER and EL systems. ", "summary_of_strengths": "The paper addresses an important problem, that gives a new way of assessing the representativeness of a dataset for a specific language. Since such text collections are at the basis of every other language task, and provide language models on which much of the higher level processing is based, it is important to have collections that are representative for the language (and speakers) that are targeted. ", "summary_of_weaknesses": "While the main idea of the paper is valuable and interesting, and thoroughly explored, it is based on some assumptions whose soundness is debatable. Details are in the comments section.\n- there is a disconnect between the visualizations and the rest of the processing.\n- the preprocessing of the datasets (many for low-resource languages) needs resources that are themselves scarce, incomplete, or borrowed from other languages (that may use other scripts, and hence there is a transliteration problem on top of others). This makes the kind of processing presented here a bit unrealistic, in the sense that it could not be deployed on any collected dataset, and give an objective view of the representativeness of that dataset for the corresponding language (this is linked to the first point, and explanations are below) - some information in the data is discarded (topical adjectives, historical entities), and it is not clear what impact using it would have on the final geographical mapping. ", "comments,_suggestions_and_typos": "With regards to the disconnect between the visualizations and the rest of the processing: the visualizations are based on geographical statistics for entities in a text, but these entities are already marked. It would have been useful to see how an end-to-end process performs: apply NER on the NER and QA datasets, and build the same visualizations as in section 3. How do the visualization using imperfect NER/EL resources and processing tools compare to the visualizations obtained on the annotated data? Are they very far apart, or the underlying \"character\" of the dataset is still retrievable even in such imperfect conditions? This links to the second potential weakness, regarding the applicability of this method to newly collected datasets (which is the aim, right?).  The geographical mapping presented is left to the subjective inspection of a human judge. Which is not necessarily bad in itself, but as the more detailed maps in the appendix show, the characteristics of some datasets are very very similar (e.g. for European countries for example, or other geographically close countries). It may be useful to have a more rigorous evaluation of the geographical mapping, by showing that from the geographical distribution of entities, one can predict the country corresponding to the dataset's language. This could be done in an unsupervised manner, or using a linear regression model, or something similarly simple -- maybe by deducting an \"average\" entity geographical distribution model, such that local characteristics become more prominent, or by computing (in an unsupervised manner) some weights that would downplay the contribution of entities from countries that are always represented (like a \"country tfidf\" maybe?).  Some geographical indicators are disregarded, and that may have an impact on the visualizations. Annotating topical adjectives that indicate countries seems doable, based on the anchor texts of links pointing to countries, which are easy to obtain (for some languages). The same for some of the historical entities that no longer exist, but some of which have corresponding GPS coordinates that could be used. The point is that both the resources and the process used to build the geographical maps of the datasets are incomplete. Some are by necessity (because the available resources are incomplete), some by choice (the adjectives and historical figures). We need to know the impact of such processing constraints.\nIt is interesting to analyze the correlation between socio-economic factors, but how does that impact the construction or characteristics of the datasets? Some of these factors -- e.g. the GDP, -- could be (in this experiment) a proxy for the level of web presence of the population, and the level of information digitization of that particular population. Maybe some parameters that measure these issues more explicitly -- which seem more closely relevant to the process of textual collection building -- would provide better insights into data characteristics.\nUsing a country as a proxy for language is useful, but it may skew the data representation, as the authors themselves recognize. What happens with languages that occupy the same country-level geographical space, but are distinct, as happens with multi-lingual countries? The same with languages that cross many borders. A bit more insight into how these are reflected in dataset characteristics and how they impact the usefulness of the dataset would be very useful.\nWhy does the cross-language consistency matter here? Each dataset (for the geographical mapping) is analyzed separately, so while cross-lingual consistency is indeed a problem, it is not clear how it is related to the problem of dataset mapping. Is the cross-lingual consistency a signal of something other than the general performance of NER/EL systems?  Some little typos: were => where (almost everywhere \"were\" appears) then => than (line 319) "}, "scores": {"overall": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.", "best_paper": "No", "replicability": "3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.", "datasets": "3 = Potentially useful: Someone might find the new datasets useful for their work.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 131], [131, 240], [240, 457]], "summary_of_strengths": [[0, 137], [137, 406]], "summary_of_weaknesses": [[0, 149], [149, 186], [186, 269], [269, 521], [521, 822], [822, 996]], "comments,_suggestions_and_typos": [[0, 209], [209, 367], [367, 508], [508, 634], [634, 778], [778, 779], [779, 869], [869, 1104], [1104, 1318], [1318, 1742], [1742, 1743], [1743, 1840], [1840, 2013], [2013, 2153], [2153, 2274], [2274, 2402], [2402, 2461], [2461, 2616], [2616, 2818], [2818, 3024], [3024, 3153], [3153, 3295], [3295, 3344], [3344, 3491], [3491, 3544], [3544, 3737], [3737, 3846], [3846, 3847], [3847, 3939]]}}}]
[{"rid": "3a14a02b9c496d9bcd7b06aedeaa8d9c48fa003e7be1b23f9cb631571dd75d894c70b500313f5f1e5ca1c867cfb04c23cad117d1072ae2d0ebbb7964c16ccdf3", "reviewer": "Thanh-Tung Nguyen", "report": {"paper_summary": "This paper proposes an explicit future information utilization framework to improve monotonic attention mechanisms in simultaneous machine translation. Specifically, the authors try several approaches to integrate the future information from language modeling into the multi-head monotonic attention. Instead of using future information from language modeling directly in the output layer of the attention model, they transform the monotonic attention mechanism to explicitly use the future information. Several experiments show some improvement over the SoTA multi-head monotonic attention models. ", "summary_of_strengths": "This paper proposes an approach to incorporate information from language modeling models into simultaneous neural machine translation. The ideas are novel and the experiments show some improvement over the baseline. ", "summary_of_weaknesses": "We can have more baselines to compare with the proposed models. It should be better if authors compare with some simple combinations of multi-head monotonic attention model and language ( joint training in target sides) or LMs as the re-rank metrics to the predicted target output. ", "comments,_suggestions_and_typos": "If you finetune XLM in the target language, do we get better results for MMA-XLM? The high accuracy of SLM is because of its training on in-domain data. Therefore we should expect the same thing when fine-tuning XLM in the target language. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "3 = Potentially useful: Someone might find the new software useful for their work."}, "meta": {"license": "Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Thanh-Tung Nguyen, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 152], [152, 301], [301, 504], [504, 599]], "summary_of_strengths": [[0, 135], [135, 216]], "summary_of_weaknesses": [[0, 64], [64, 282]], "comments,_suggestions_and_typos": [[0, 82], [82, 153], [153, 240]]}}}]
[{"rid": "c6e68ace9c7dd58c39160972207bd05fde2a14e0f3550613d0de5256d3fc80a07e8f3319fcdb91d97dfb1e9eed12479fae0a07a93fe4e1a075484af8a3ee6e9b", "reviewer": "Rajarshi Bhowmik", "report": {"paper_summary": "This paper proposes an entity linking method that leverages coreferent mentions in a corpus. The model is trained to produce a directed minimum spanning tree where each tree is constrained to have one or more coreferent mentions and a single KG entity to which all coreferent mentions are mapped. The proposed model is evaluated on two benchmark datasets - MedMentions (a dataset for biomedical entity linking ) and ZeShEL (a dataset for zero-shot entity linking). Each of these datasets contains a substantial number of unseen entities in the test set. The proposed method is shown to have improved recall@k for candidate retrieval. A cross-encoder is used for re-ranking the retrieved candidates and it yields comparable or better results to the baseline ", "summary_of_strengths": "1. The affinity-based graph construction with mention-mention and mention-entity edges for entity linking was already explored in prior work. This paper extends this line of work and formulates the entity linking task as a coreference resolution task. \n2. An extensive evaluation is done using different retrieval and tree construction methods that demonstrate the advantages of the model. ", "summary_of_weaknesses": "1. Missing related work: A closely related line of work is collective entity linking that considers multiple mentions in the same document. I think the proposed model is a special case of collective entity linking. However, the paper does not mention this line of work. \n2. No error analysis is performed. So it’s hard to tell in which cases the model does not perform well. An error analysis would help to identify potential limitations of this model and pave way for future model improvements. ", "comments,_suggestions_and_typos": "1. What is the target KB size used for MedMentions in the experiments? Is it the number of unique entities in the labeled partition (as shown in Table 2) or is it the entire UMLS? \n2. What is used as ‘descriptions’ of entities in MedMentions? To the best of my knowledge, every entity in UMLS does not have a description. \n4. How are the training and inference batches constructed? Do they contain mentions from the same document to leverage coreferent mentions as much as possible? Since a fixed batch size (B=128) is used, there is a chance that a document will be split into several batches, or a batch may contain mentions from very different documents. A clear description of how batches are constructed would be helpful. \n4. Does the model need to compute \\psi(m_i, e) for all e in the target KB during inference? If that is the case, then how does it impact the efficiency of the model during inference? "}, "scores": {"overall": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: Rajarshi Bhowmik, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 93], [93, 297], [297, 465], [465, 554], [554, 634], [634, 757]], "summary_of_strengths": [[0, 3], [3, 142], [142, 252], [252, 256], [256, 390]], "summary_of_weaknesses": [[0, 3], [3, 140], [140, 215], [215, 270], [270, 274], [274, 306], [306, 375], [375, 496]], "comments,_suggestions_and_typos": [[0, 3], [3, 71], [71, 180], [180, 184], [184, 243], [243, 322], [322, 326], [326, 382], [382, 483], [483, 658], [658, 727], [727, 731], [731, 820], [820, 911]]}}}, {"rid": "1a78877afd22d0616b89584b687d567dc6f161f1e8c47d409b6753dea1110a268d1a5e116c54e49bc4737022dfa9d0b245f8527f4b57d3b54e6b78bab70498ca", "reviewer": null, "report": {"paper_summary": "The paper aims to find the minimum spanning arborescences in an entity-mention graph to get better mention and entity representations by modeling the coreference relationships. The experimental results demonstrate that the approach can improve the performance of entity liking and the learned representations can be used for better entity coreference discovery. ", "summary_of_strengths": "1. The idea is quite simple but effective, we do not need to add new layers or parameters to boost task performance. \n2. The experiment results show the effectiveness of their proposed method on two datasets for candidate generating, linking accuracy, and entity coreference. ", "summary_of_weaknesses": "1. Some parts of the paper are not clear. \n    1). In Section 2.1 line 165-170, the paper provides some explanation for the definition of the f function, but it is still not clear why such a setting is well-suited for coreference. Suggesting provide more intuitive examples to demonstrate it. \n    2). Section 2.1 line 188-203, it is understandable to use special tokens as input for encoding, but the paper does not clarify which token is used as the representation of a mention span (is it [SATART] or [END]?), and which one is used for the entity representation (is it [CLS]?). \n2. The motivation for using the pruning for the graph is not clear. Why not just use the complete graph for training? ", "comments,_suggestions_and_typos": "As mentioned in the weakness. "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "1 = No usable software released."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 177], [177, 362]], "summary_of_strengths": [[0, 3], [3, 117], [117, 121], [121, 276]], "summary_of_weaknesses": [[0, 3], [3, 42], [42, 51], [51, 231], [231, 293], [293, 302], [302, 513], [513, 581], [581, 585], [585, 650], [650, 700]], "comments,_suggestions_and_typos": [[0, 30]]}}}]
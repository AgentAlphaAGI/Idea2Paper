[{"rid": "3affb22d5291a0f1b271ca9dc4dd222ab62f7f4ab6ccfd912fe8e07e1bde6611b3087a936d228e699535ceb69c29ecadcf5cb58720a18c93acb946af3dd3394d", "reviewer": null, "report": {"paper_summary": "As I am a returning reviewer for the paper I will keep any of my previous points that still hold for this version of the paper and add new ones where appropriate.\nI thank the authors for their response and the changes in the paper. I found much of the response (and changes) to be reasonable, with many of my complaints (especially regarding lack of information on the human evaluation) addressed in this version of the paper. The statistical significance tests are appreciated and they seem to support the initial version's claims.  ====== \t This paper proposes a novel approach to the task of knowledge-grounded dialogue generation, that additionally exploits commonsense external knowledge augmented with named-entity triples extracted from co-reference chains detected in the dialogues themselves. The proposed approach additionally employs a multi-hop attention layer over the dialogue history and corresponding external knowledge to inform the decoding process.\nThe paper presents both automatic and human evaluation over 2 datasets, and discusses a case study. ", "summary_of_strengths": "- The augmentation of common sense knowledge and the application of multi-hop attention over the dialogue history and unstructured knowledge for the purposes of knowledge-grounded dialogue generation are novel and interesting.\n- The proposed approach is experimentally shown to outperform previous work in automatic evaluation. While automatic evaluation is known to be unreliable in single-reference generation tasks, human evaluation results also indicate that the proposed approach outperforms related work on multiple criteria that effectively measure knowledge precision and recall. ", "summary_of_weaknesses": "- Human evaluation suggests that the proposed methods underperforms in fluency. This could be due to their use of a GRU as a decoder. Replacing the decoder with a pretrained language model similar to related work should help fluency. ", "comments,_suggestions_and_typos": "- The paper now has an ablation study that supports each components efficacy, but it's presentation can be improved. Please present the ablation study results in a table and clearly state how each experiment performed numerically. As it is the comparison is quite vague for most of the configurations.\n- The D.2 appendix mentions two human annotators. Based on the author responses, that number should be four.\n- I suggested for the previous version that the paper would be stronger if it also presented results on other knowledge-grounded benchmarks, e.g. DSTC9 and the Doc2Dial benchmark. The author's response to this was reasonable, and I would suggest that the include their reasoning in the paper as a potential limitation of the approach. "}, "scores": {"overall": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.", "sentences": {"paper_summary": [[0, 163], [163, 232], [232, 427], [427, 533], [533, 534], [534, 802], [802, 968], [968, 1068]], "summary_of_strengths": [[0, 227], [227, 328], [328, 588]], "summary_of_weaknesses": [[0, 80], [80, 134], [134, 234]], "comments,_suggestions_and_typos": [[0, 117], [117, 231], [231, 302], [302, 352], [352, 411], [411, 591], [591, 746]]}}}, {"rid": "086c68303939dbb31a634b50a49ef47789ef060639e7701d07594e78aa9a9c6c401855d77457ce497a1b73242782c8b53dd2d2ee1255ed4611f550d442bc4b05", "reviewer": null, "report": {"paper_summary": "This paper proposes a dialog generation model, CNTF, that incorporates named entities and commonsense knowledge triples from ConceptNet. They show their competitive performance (as measured by automatic and human eval metrics) on Wizard of Wikipedia and CMU DoG datasets. ", "summary_of_strengths": "- interesting topic: use of commonsense in response generation - detailed description of approach and methodology - model achieves a higher performance than the baselines. Results also show there's headroom for future engagement and improvement on the topic. ", "summary_of_weaknesses": "- I find the error analysis/discussion/implications of the results section a bit wanting. They note two categorical problems among the model's responses, but there is little discussion that could serve as a launching point for engagement with the readers.  A couple things I point out that could be discussed (that I'm curious about):    * The model seems to make very close use of relations (e.g. RelatedTo) that have been identified by previous work as being somewhat arbitrary and noisy. Any insights you can provide as to the apparent utility?\n   * The five chosen measures in Table 2 is interesting; they must have been chosen for particular reasons in mind. It looks like for the most part, human responses are close to perfect. And the strength of CNTF is in the latter 3 knowledge existence, correctness and relevance; above the most competitive baseline. Can you interpret the results? Any insights you could provide? ", "comments,_suggestions_and_typos": "n/a "}, "scores": {"overall": "3.5 ", "best_paper": "No", "datasets": "1 = No usable datasets submitted.", "software": "4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."}, "meta": {"license": "Copyright © 2021 administered by the Association for Computational Linguistics (ACL)\n                                on behalf of ACL content contributors: None, and other contributors who wish to remain anonymous.\n                                Content is made available under a Creative Commons Attribution 4.0 International Public License.", "author_identity_guess": "1 = I do not have even an educated guess about author identity.", "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.", "sentences": {"paper_summary": [[0, 137], [137, 272]], "summary_of_strengths": [[0, 63], [63, 114], [114, 172], [172, 259]], "summary_of_weaknesses": [[0, 90], [90, 256], [256, 335], [335, 491], [491, 548], [548, 664], [664, 735], [735, 864], [864, 895], [895, 927]], "comments,_suggestions_and_typos": [[0, 4]]}}}]
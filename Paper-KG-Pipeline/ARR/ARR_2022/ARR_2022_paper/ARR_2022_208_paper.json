{
  "name" : "ARR_2022_208_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Platt-Bin: Efficient Posterior Calibrated Training for NLP Classifiers",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning has proven to be tremendously attractive for researchers in fields such as physics, biology, and manufacturing, to name a few (Baldi et al., 2014; Anjos et al., 2015; Bergmann et al., 2014). However, these are fields in which representing model uncertainty is of crucial importance (Gal and Ghahramani, 2016). A common way to incorporate DNNs in other fields is to use the predictions of a trained classifier for decision making in a downstream task. In some cases the effectiveness of the decisions depends on a utility function and it is not enough to simply predict the most likely label for each example. What is needed instead is to quantify model uncertainty about the predictions. Despite promising performance in supervised learning benchmarks in terms of accuracy, DNNs are poor at quantifying predictive uncertainty, and tend to produce overconfident predictions. Overconfident incorrect predictions can be harmful or offensive in NLP applications\n(Amodei et al., 2016), hence proper uncertainty quantification is crucial in practice. Probabilistic uncertainty in machine learning translates to estimation of the probability mass function p(y|x) by the model, where x is the input sample and y is a class label. Recent works have shown that stateof-the art structured prediction models are poorly calibrated. Therefore, blindly using the output of the softmax function output as the model uncertainty is misleading (Kumar and Sarawagi, 2019; Dong et al., 2018; Nguyen and O’Connor, 2015).\nWe are interested in calibrating the posterior estimates, i.e. we wish to get posterior probability estimations that reflect the true probability of the classes. The probability that a system outputs for an event should reflect the true frequency of that event: if an automated diagnosis system says 1,000 patients have cancer with probability 0.1, approximately 100 of them should indeed have cancer (Kumar et al., 2019). Even if the actual mechanism might be difficult to interpret, a calibrated model at least gives us a signal that it “knows what it doesn’t know,” thereby making these models easier to deploy in practice (Jiang et al., 2012). We define perfect calibration as follows.\nP(y|f(x)) = f(x)\nwhere f : X → △K−1 is the probabilistic classifier that maps the samples x ∈ X to the Kdimensional simplex. As majority of the current state-of-the art machine learning models, such as DNNs, do not output calibrated probabilities out of the box (Kuleshov et al., 2018), existing works rely on re-calibration methods that take the output of an uncalibrated model, and transform it into a calibrated probability. One way of addressing this is to use Scaling approaches for re-calibration such as Platt scaling (Platt et al., 1999), isotonic regression (Zadrozny and Elkan, 2002), and temperature scaling (Guo et al., 2017). These methods are widely used and require very few samples,\nhowever it is challenging to calibrate posterior estimates with sub-optimal binning schemes (Kumar et al., 2019). An alternative approach, histogram binning (Zadrozny and Elkan, 2001), outputs probabilities from a finite set. Histogram binning can produce a model that is calibrated, and unlike scaling methods we can measure its calibration error, but it is sample inefficient. In particular, the number of samples required for calibration scales linearly with the number of classes for which probability estimates need to be generated.\nIrrespective of the choice of the calibration method, existing works generally calibrate the posterior distribution predicted from the classifier after training. These post-processing calibration methods re-learn an appropriate distribution from a held-out validation set and then apply it to an unseen test set. The fixed split of the data sets and insufficient number of samples for training the calibration function adversely affects the generalization of post-hoc calibrated classifiers and reduce their accuracy. In this paper we try to address some of the existing challenges in achieving apt calibration. In particular our contributions are:\n• We propose a training technique that optimizes a classification objective for an NLP task by calibrating the posterior distribution while training.\n• We leverage the advantages of both scaling and binning methods and propose a calibration method for NLP classification task which is both sample efficient and verifiable.\n• We demonstrate how the proposed method not only calibrates but also improves the performance of benchmark NLP classification tasks."
    }, {
      "heading" : "2 Related Works",
      "text" : "Model uncertainty estimation and posterior calibration is a topic of continued interest not only in the fields of machine learning and statistics, but also in meteorology (Bröcker, 2009), fairness (Liu et al., 2019), healthcare (Jiang et al., 2012), reinforcement learning (Malik et al., 2019), natural language processing (Card and Smith, 2018), speech recognition (Yu et al., 2011) and economics (Gneiting et al., 2007). In probabilistic models, the principal goal of estimation of the posterior p(y|x) given a sample x ∈ X and a label y ∈ [K], is to assign low confidence to samples that were not explained well\nby the training data. One common way to calibrate multiclass posteriors after training the classifier f : X → R is to treat the problem as K one-vsall binary problems. In this case, model uncertainty is quantified by normalizing the estimation of p(y = k|f(x)k) where f(x)k is the output score of the classifier for sample x and class k. Various binary calibration methods can be used to estimate the marginal posterior over a calibration dataset, ranging from parametric approaches (e.g. Platt scaling, temperature scaling, vector scaling (Platt et al., 1999; Guo et al., 2017)), to non-parametric methods (e.g. quantile or bayesian binning (Zadrozny and Elkan, 2001; Naeini et al., 2015), and isotonic regression (Zadrozny and Elkan, 2002).\nAnother way to reduce the problem to binary calibration is by estimating model accuracy conditioned on its confidence, p(y = ŷ|maxk∈[K] f(x)k). Multiclass calibration aims to estimate the distribution of class labels conditioned on the estimated probability vector, p(y|f(x)). In this case the sample complexity is exponential in the number of classes and therefore with large number of classes, the main challenge is to constrain the hypothesis space with regularization. Some of the proposed methods for this purpose are matrix scaling and Dirichlet scaling which both use linear models for estimation of p(y = k|f(x)) (Guo et al., 2017; Kull et al., 2019), and MLP and order preserving functions (Rahimi et al., 2020a,b).\nAnother approach is to account for model uncertainty via bayesian models. In Bayesian Neural Networks (BNNs) the predictive uncertainty will naturally be high in regions where training data is scarce (MacKay, 1992). However, the marginalization of the weights in BNN is intractable. Consequently, following papers propose various approximations such as variational inference (VI) (Graves, 2011; Blundell et al., 2015). Although BNNs are theoretically proven to control the overconfidence of the model in unseen regions of data space (Kristiadi et al., 2020), they require expensive approximations which limit their application in most modern NLP architectures. In (Joo et al., 2020) the authors model the distribution on posterior probability using a Dirichlet prior distribution and variational inference. MCDropout is a variational approximation of Gaussian processes that avoids explicit modeling of the posterior distribution (Gal and Ghahramani, 2016). Both of these\nmethods require modification of training of the network.\nIn NLP, tasks with structured outputs posterior calibration are particularly challenging. This is because the number of classes are exponentially large and estimation of every posterior density or marginal posterior density is not possible. Previous works such as (Jung et al., 2020; Nguyen and O’Connor, 2015) propose to use the downstream task with small number of classes to perform calibration and estimation of the calibration error. In structured prediction models, calibration is also important for the generation of the structured outputs as the decoding algorithm relies on the posterior estimates to efficiently search through the space of sequences. However, estimation of the sequence calibration error and its correction is intractable. To cope with this problem, approximate calibration methods using a set of interesting events and feature based calibration are proposed in (Kuleshov and Liang, 2015; Jagannatha and Yu, 2020) and an alternative calibration error estimator was proposed using sequence precision scoring function BLEU in (Kumar and Sarawagi, 2019)."
    }, {
      "heading" : "3 Method",
      "text" : "In general, NLP classifiers work by first predicting a posterior probability distribution over all classes and then selecting the class with the largest estimated probability. However, these models are often poorly calibrated. Existing calibration methods re-learn an appropriate distribution from a heldout validation set and then apply it to an unseen test set which degrades the model performance. Alternatively, we can dynamically estimate the required statistics for calibration from the train set during training iterations, thereby minimizing cross-entropy as well as the calibration error as a multi-task setup (Jung et al., 2020). Given a training set D = {(x1, y1)..(xn, yn)}, where xi is an n-dimensional vector of input features and yi is a K-dimensional one-hot vector corresponding to its true label (with K classes), we minimize the loss Ltrain:\nLtrain = Lclass + λLcal (1)\nHere Lclass is the classification loss (for eg. crossentropy) based on the predicted probability pik up-\ndated during training for sample i and class k:\nLclass = − N∑ i=1 K∑ k=1 yiklog(pik)\nLcal is the calibration loss which acts as a regularizer. It essentially tries to minimize the difference between the updated probability p and true posterior probabilities q via a distance function d (eg. mean squared error, KL-divergence, etc.):\nLcal = N∑ i=1 K∑ k=1 d(pik, qik)\nOne crucial step here is to estimate the empirical probability q, which can be done by histogram binning method. Here, we measure the ratio of true labels for each bin split by the predicted posterior p from each update. We store the results in Empirical Probability Matrix Q ∈ RB×K , where B is the number of bins used for each posterior dimension. Histogram binning outputs probabilities from a finite set. Unlike scaling methods, it can produce a model that is calibrated and measure its calibration error. However, the number of samples required to calibrate scales linearly with the number of distinct probabilities B the model can output which can be large in the multi-class setting (Naeini et al., 2014).\nIn this work, we propose an adaptive binning method that circumvents this bottleneck. We leverage the sample efficiency of Platt scaling (Platt et al., 1999) and the verification guarantees of histogram binning (Zadrozny and Elkan, 2001) by defining the Platt-Binning Calibrator. The problem with scaling methods is we cannot estimate their calibration error. The upside of scaling methods is that if the function family has at least one function that can achieve calibration error ϵ they require O(1/ϵ2) samples to reach calibration error ϵ, while histogram binning requires O(B/ϵ2) samples. Platt-Binning Calibrator facilitates estimation of calibration error while being sampleefficient at the same time.\nSince most modern deep learning classifiers do not output calibrated probabilities out of the box, recalibration methods take the output of an uncalibrated model, and transform it into a calibrated probability. That is, given a trained model f : X → [0, 1], let z = f(x). We are given recalibration data T = {(zi, yi)}ni=1 corresponding to model logits\nand the labels, and we wish to learn a calibrator g : [0, 1]→ [0, 1] such that g ◦ f is well-calibrated. Conventional Scaling methods, for example Platt scaling, output a function g:\ng = argmin g∈G ∑ (z,y)∈T l(g(z), y)\nwhere G is a the hypothesis class, g ∈ G is differentiable, and l is a loss function, for example the log-loss or mean-squared error. The advantage of such methods is that they converge very quickly since they only fit a small number of parameters. On the other hand, Histogram binning constructs a set of bins that partitions [0, 1] via a binning scheme. A binning scheme B̂ of size B is a set of B intervals I1, ...IB that partitions [0, 1]. Given p = softmax(z)k ∈ [0, 1], let β(z) = j, where j is the interval that p lands in (p ∈ Ij). The binning scheme, B̂ typically corresponds to choosing bins of equal widths (called equal width binning) or so that each bin contains an equal number of zi values in the calibration dataset (called uniform mass binning). Histogram binning then outputs the average yi value in each bin.\nPlatt-Binning Calibrator builds at the intersection of the above two methods. Given a recalibration data T of size n, Platt-Binning Calibrator outputs ĝβ such that ĝβ ◦ f has a low calibration error by using the following procedure: Step 1: Select g = argming∈G ∑ (z,y)∈T (y − g(z))2 Step 2: Choose the bins so that an equal number of g(zi) in T land in each bin Ij for each j ∈ 1, ..., B —this uniform-mass binning scheme as opposed to equal-width binning is essential for estimating the Expected Calibration Error, ECE.\nECE = 1\nK K∑ k=1 B∑ b=1 Nkb Nk |Qbk − p̄bk|\nwhere p̄bk is the average posterior estimate for class k for samples in b-th bin. Nkb and Nk are the number of samples of class k assigned to bin b and in total, respectively. Step 3: Discretize g, by outputting the average g value in each bin. Let µ(S) = 1|S| ∑ s∈S s denote the mean of a set of values S. We set ĝβ(z) = µ(β(g(z)))- we output the mean value of the bins that g(z) falls in.\nThe motivation behind our method is that the g values in each bin are in a narrower range than the\nlabel values y, so when we take the average we incur lower estimation error. If G is well chosen, our method requires O( 1\nϵ2 +B)samples to achieve\ncalibration error ϵ instead of O(B ϵ2 ) samples for histogram binning. All these steps are performed during training as explained in the pseudo-code in Algorithm 1. To the best of our knowledge, such a formulation is novel among existing calibrators that tackle the problem during training. Also, the whole approach is the first to be utilised to calibrate classifiers in the NLP domain. In the following section we prove the efficacy of our method by carrying out extensive evaluation of the performance of pretrained transformer models such as BERT (Devlin et al., 2018) on simple multi-class text classification tasks. Our motivation comes from the analysis in (Desai and Durrett, 2020) which shows that pretrained models are significantly better calibrated when used out-of-the-box.\nAlgorithm 1 Platt-Binning Calibrated Training Input: Train set D, Bin B, Number of Classes\nK, Number of epochs e, Learning rate η, Number of updating empirical probabilities u\nOutput: Model Parameters Θ Let Q : Empirical Probability Matrix ∈ RB×K Random initialization of Θ for i ∈ {1,2,3, ...e} do\nBreak D into random mini-batches m for m from D do\nif current step mod ⌊e/u⌋ == 0 then p̂(x) = maxk softmax(Θ, D)k, ∀x ∈ D. ŷ = argmaxk softmax(Θ, D)k, ∀x ∈ D. g = argming∈G ∑ (x,y)∈D(1y=ŷ −\ng(p̂(x)))2 Choose bins so that an equal number of g(pi) in D land in each bin bj , j ∈ {1, ...B} Discretize g, by outputting the average g value in each bin bj : ĝβ(pi) = µ[β(g(pi))]\nQ← CalEmpProb(p̂;bj) end Θ← Θ− η∇ΘLtrain(Θ, ĝβ(pi), b)\nend end"
    }, {
      "heading" : "4 Experiment",
      "text" : "In the experiments we finetune the parameters on pre-trained BERT classifier using the regularized loss in equation (1). We compare our method to the following baselines:\n• MLE is the baseline with maximum likelihood training without calibration where we simply report the results of vanilla BERT classifier on the chosen tasks.\n• Platt scaling (posPS) is a post-hoc calibration method where we calibrate the posterior estimations of MLE classifier using Platt scaling (Platt et al., 1999). Formally, the parameters of the calibration functions g(z;W,b) = σ(W · softmax(z) + b) is fit to the validation dataset. Here, σ(.) is the component-wise logistic function. Model is fit using one-vs-all binarization of the classification task. Instead of the estimated posterior- softmax(f(x))k for class k- we return the calibrated value g(f(x)) as the class probablity. Despite its simplicity this method is competitive with the more complex methods when implemented post-hoc (Guo et al., 2017).\n• PosCal end-to-end training calibration using histogram binning (Jung et al., 2020). In this method we have a nested training procedure where in the outer loop we fit a histogram binning scheme with fix widths to each dimension of the posterior estimates of the BERT model. We use Qbk- the ratio of samples of kth class that were assigned to bth bin- as the empirical probability distribution q. In the inner loop we perform the ordinary training iterations over mini-batches of training dataset with cross-entropy loss and regularization term in equation (1) using KL-divergence between softmax output and the estimated empirical distribution.\nLcal = N∑ i=1 K∑ k=1 log softmax(zi)k Qbin(zik)k\nwhere bin(.) returns the index of bin assigned to its input. In the experiments we used λ = 1.0, 10 bin for descretization of q and we update Q after every training epoch.\nWe test the baselines and our method on the benchmark on NLP classification tasks: xSLUE (Kang\nand Hovy, 2019). xSLUE contains classification benchmark on different types of styles such as a level of humor, formality and even demographics of authors. We train our method with two types of calibrators: in the first calibration task we train a calibrator for the most confident prediction of the classifier and call this version plattbintop (PBtop). The pseudocode of this version is illustrated in algorithm (1). In the second version we train a separate Platt scaler and histogram binning for each class in a one-vs-all manner and we call this version of calibration plattbin (PB). While this version is exactly the same as plattbintop for binary tasks, it results in a very different solution for tasks with K > 2. The pseudocode of this version is omitted due to being mainly similar to the other version with one additional loop over the classes at line 7 of algorithm (1) and conversion of label y and ŷ to one-vs-all binary labels. We report task accuracy, F1 score and ECE as the evaluation metrics."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "Table 1 shows task performance and calibration error on xSLUE benchmark datasets. In general, our method outperforms MLE, Poscal and posPS on more than 50% of the datasets, in terms of both model performance and calibration error. For the rest of the datasets, our method gives competitive results. In seven out of nine cases, we reduce the calibration error ECE as compared to PosCal. In cases such as DailyDialog, SentiTreeBank and ShortHumor, the achieved reduction in ECE as compared to all baselines is significant. Note that this reduction has not compromised with the model performance. In fact, cases like SentiTreeBank and ShortRomance even witness a significant improvement in the performance of the model when ECE is reduced. These observations prove the efficacy of our method in maintaining a perfect balance between model performance and model uncertainitya testimony of an ideal calibrator. Post-hoc methods such as posPS might achieve lower calibration error on a couple of datasets, but they fail to attain competitive performance in terms of accuracy. Similarly, in-training methods like PosCal tend to achieve higher accuracy but fail to be consistent in reducing calibration error. Our proposed method (PB or PBtop) hits the sweet-spot between the two extremes and is shown to achieve better results than baselines: highest accuracy except for TroFi, highest F1 score except for TroFi and VUA and lowest\nECE except for TroFi and stanfordpoliteness (Table 1).\nWe now analyse how our method behaves in comparison to MLE at sample level during test time. Table 2 shows a detailed analysis of misclassification made by MLE and Platt-Binning (PB). We see that both the methods have almost comparable performance in columns A1 and A2, with A2 being slightly higher. As such, the number of samples for which MLE and PB gave different predictions (column M ) is actually a small fraction of the total number of test samples used of evaluation of the methods (column Test). We further analyse the number of samples where MLE gave correct predictions while PB failed to do so (column P1) and vice-versa (column P2). In 8 out of 9 datasets, PB demonstrates superior or similar performance (P2 ≥ P1). The difference is insignificant compared to the total size of the test set for the reverse scenario. This quantitative analysis reinstates that our method, PB, has better model performance at test time, thereby establishing that it generalizes well while reducing calibration error.\nWe extend the discussion above by analysing qualitative results in Table 3. We consider three datasetsa two-class classification task StanfordPoliteness, a three-class classification task HateOffensive and a multi-class classification task (K > 3) DailyDialog, and include few test samples where MLE and PB disagreed on the predictions. The corresponding p̂ along with the true label is also depicted.\nIn the first two cases from StanfordPoliteness dataset, the level of politeness (e.g., “Hey!” in S1) or arrogance (e.g., “What?” in S2) indicated on phrases is not captured well by MLE, so it predicts the incorrect label while PB gives a correct prediction. However, for the rest two cases, MLE gives confident correct predictions taking into account phrases such as \"like\" in S1 or a slightly difficult example in S2 but PB fails (only slightly in S2 though) to give correct predictions. Arguing on similar lines for the multi-class case, we witness cases where MLE fails to classify correctly (eg. S1 and S2 in HateOffensive) but PB gives highly confident predictions and vice-versa. From our manual investigation above, we find that statistical knowledge about posterior probability helps correct p̂ while training PB, so making p̂ switch its prediction. For further analysis, we provide more examples in Appendix A.\nIn Figure 1 we show the calibration plots for three datasets: DailyDialog, HateOffensive, and Stan-\nfordPoliteness. We divide test samples according to the most confident estimated posterior into 10 bins. We plot the accuracy of the classifier versus the average classification confidence in each one of the bins in the top row. We also plot the number of samples in each calibration bin versus the classification confidence in the bottom row. Ideally, a calibrated classifier would assign a probability to the top class that is equivalent to its accuracy. Therefore, the accuracy-confidence curve of a calibrated classifier is close to the dashed grey curve in the top row. When Platt-bin and Platt-bin-top are further away from the calibration line it is because the number of samples in corresponding bins are low or even 0 in some cases. The bins with 0 samples in them can be ignored as they don’t play a role in the classifier predictions.\nHowever, the distance of the curves is not enough to determine model calibration as most of the samples are assigned to the bin with highest estimated posterior. Thus, correcting the calibration error in the bins with more samples is more effective in improving the expected calibration error. PlattBinning and Platt-Binning-Top algorithms increase the number of samples with lower classification confidence in all three of the illustrated tasks, while in comparison to MLE with no regularization they only reduce classification accuracy by a negligible amount and even increase the accuracy for HateOffensive task. Although, the classifier become visibily underconfident in HateOffensive task where post-hoc Platt scaling has a more calibrated output. While the ECE doesn’t improve in StanfordPoliteness, Platt-Binning algorithm doesn’t increase the ECE as much as PosCal regularization. We conjecture that such a behavior is demonstrated due to better sample efficiency of our algorithm.\nWe conclude our analysis by observing the effect of two important parameters to this discussion- B: number of histogram bins used for calibration, and λ: strength of the regularization. Figure 2 shows how calibration error (ECE) vary when the number of bins B is varied as {10, ...100}. We see that the calibration error of all the methods have an increasing trend as B is increased. One plausible explanation can be that as we increase the number of bins, we don’t have enough samples per bin to estimate the empirical probabilities accurately. Since calibrated probabilities are used as an estimation of the true probabilities of the classes in case of PosCal and PB, it adds to the error if they are estimated wrongly. Thus, smaller number of bins is preferred, and as evident in Fig. 2, PB achieves lower ECE than PosCal when number of bins is low. The accuracy and F1 scores do not vary much with the number of the bins. Similarly, the performance is not impacted significantly by variations in the value of λ (see Appendix A)"
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we proposed a simple yet effective method called Platt-Binning calibrator for better posterior calibration. Our method has theoretically lower sample complexity than histogram binning, giving us the best of scaling and binning methods.\nAnd unlike the existing post-processing calibration methods, Platt-Binning directly penalizes the difference between the predicted and the true (empirical) posterior probabilities dynamically over the training steps. Our empirical analysis corroborates that Platt-Binning can not only reduce the calibration error but also increase the task performance on the classification benchmarks. For tasks where the reduction in calibration error is low, our method maintains the performance of the model instead of degrading it as seen for other existing calibrators. Moreover, our method can be extended to any classification model as an additional component in the loss function, thus jointly optimised during training. There are many exciting avenues for future works in this regard. It will be interesting to assess how our method can provide advantages in the scenarios of domain adaptation and transfer learning. Moreover, exploring alternatives to the model family G from which estimate ĝ is considered can be a direction of improvement. Lastly, optimizing the overall method for huge datasets can be an essential extension. Our method may also assist in analysing the bias and fairness aspects of the predictions made by NLP classifiers. This can facilitate ethical deployment of NLP models for real-world applications."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Concrete problems in ai safety",
      "author" : [ "Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Mané." ],
      "venue" : "arXiv preprint arXiv:1606.06565.",
      "citeRegEx" : "Amodei et al\\.,? 2016",
      "shortCiteRegEx" : "Amodei et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural networks applied to discriminate botanical origin of honeys",
      "author" : [ "Ofélia Anjos", "Carla Iglesias", "Fátima Peres", "Javier Martínez", "Ángela García", "Javier Taboada." ],
      "venue" : "Food chemistry, 175:128–136.",
      "citeRegEx" : "Anjos et al\\.,? 2015",
      "shortCiteRegEx" : "Anjos et al\\.",
      "year" : 2015
    }, {
      "title" : "Searching for exotic particles in high-energy physics with deep learning",
      "author" : [ "Pierre Baldi", "Peter Sadowski", "Daniel Whiteson." ],
      "venue" : "Nature communications, 5(1):1–9.",
      "citeRegEx" : "Baldi et al\\.,? 2014",
      "shortCiteRegEx" : "Baldi et al\\.",
      "year" : 2014
    }, {
      "title" : "On the use of artificial neural networks in simulation-based manufacturing control",
      "author" : [ "Sören Bergmann", "Sören Stelzer", "Steffen Strassburger." ],
      "venue" : "Journal of Simulation, 8(1):76–90.",
      "citeRegEx" : "Bergmann et al\\.,? 2014",
      "shortCiteRegEx" : "Bergmann et al\\.",
      "year" : 2014
    }, {
      "title" : "Weight uncertainty in neural networks",
      "author" : [ "Charles Blundell", "Julien Cornebise", "Koray Kavukcuoglu", "Daan Wierstra." ],
      "venue" : "arXiv preprint arXiv:1505.05424.",
      "citeRegEx" : "Blundell et al\\.,? 2015",
      "shortCiteRegEx" : "Blundell et al\\.",
      "year" : 2015
    }, {
      "title" : "Reliability, sufficiency, and the decomposition of proper scores",
      "author" : [ "Jochen Bröcker." ],
      "venue" : "Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology and physical oceanography, 135(643):1512–1519.",
      "citeRegEx" : "Bröcker.,? 2009",
      "shortCiteRegEx" : "Bröcker.",
      "year" : 2009
    }, {
      "title" : "The importance of calibration for estimating proportions from annotations",
      "author" : [ "Dallas Card", "Noah A Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Card and Smith.,? 2018",
      "shortCiteRegEx" : "Card and Smith.",
      "year" : 2018
    }, {
      "title" : "Calibration of pretrained transformers",
      "author" : [ "Shrey Desai", "Greg Durrett." ],
      "venue" : "arXiv preprint arXiv:2003.07892.",
      "citeRegEx" : "Desai and Durrett.,? 2020",
      "shortCiteRegEx" : "Desai and Durrett.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Confidence modeling for neural semantic parsing",
      "author" : [ "Li Dong", "Chris Quirk", "Mirella Lapata." ],
      "venue" : "arXiv preprint arXiv:1805.04604.",
      "citeRegEx" : "Dong et al\\.,? 2018",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2018
    }, {
      "title" : "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani" ],
      "venue" : null,
      "citeRegEx" : "Gal and Ghahramani.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "Probabilistic forecasts, calibration and sharpness",
      "author" : [ "Tilmann Gneiting", "Fadoua Balabdaoui", "Adrian E Raftery." ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243–268.",
      "citeRegEx" : "Gneiting et al\\.,? 2007",
      "shortCiteRegEx" : "Gneiting et al\\.",
      "year" : 2007
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "Advances in neural information processing systems, pages 2348–2356.",
      "citeRegEx" : "Graves.,? 2011",
      "shortCiteRegEx" : "Graves.",
      "year" : 2011
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q Weinberger." ],
      "venue" : "arXiv preprint arXiv:1706.04599.",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Calibrating structured output predictors for natural language processing",
      "author" : [ "Abhyuday Jagannatha", "Hong Yu." ],
      "venue" : "arXiv preprint arXiv:2004.04361.",
      "citeRegEx" : "Jagannatha and Yu.,? 2020",
      "shortCiteRegEx" : "Jagannatha and Yu.",
      "year" : 2020
    }, {
      "title" : "Calibrating predictive model estimates to support personalized medicine",
      "author" : [ "Xiaoqian Jiang", "Melanie Osl", "Jihoon Kim", "Lucila Ohno-Machado." ],
      "venue" : "Journal of the American Medical Informatics Association, 19(2):263– 274.",
      "citeRegEx" : "Jiang et al\\.,? 2012",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2012
    }, {
      "title" : "Being bayesian about categorical probability",
      "author" : [ "Taejong Joo", "Uijung Chung", "Min-Gwan Seo." ],
      "venue" : "arXiv preprint arXiv:2002.07965.",
      "citeRegEx" : "Joo et al\\.,? 2020",
      "shortCiteRegEx" : "Joo et al\\.",
      "year" : 2020
    }, {
      "title" : "Posterior calibrated training on sentence classification tasks",
      "author" : [ "Taehee Jung", "Dongyeop Kang", "Hua Cheng", "Lucas Mentch", "Thomas Schaaf." ],
      "venue" : "arXiv preprint arXiv:2004.14500.",
      "citeRegEx" : "Jung et al\\.,? 2020",
      "shortCiteRegEx" : "Jung et al\\.",
      "year" : 2020
    }, {
      "title" : "xslue: A benchmark and analysis platform for cross-style language understanding and evaluation",
      "author" : [ "Dongyeop Kang", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1911.03663.",
      "citeRegEx" : "Kang and Hovy.,? 2019",
      "shortCiteRegEx" : "Kang and Hovy.",
      "year" : 2019
    }, {
      "title" : "Being bayesian, even just a bit, fixes overconfidence in relu networks",
      "author" : [ "Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig." ],
      "venue" : "arXiv preprint arXiv:2002.10118.",
      "citeRegEx" : "Kristiadi et al\\.,? 2020",
      "shortCiteRegEx" : "Kristiadi et al\\.",
      "year" : 2020
    }, {
      "title" : "Accurate uncertainties for deep learning using calibrated regression",
      "author" : [ "Volodymyr Kuleshov", "Nathan Fenner", "Stefano Ermon." ],
      "venue" : "arXiv preprint arXiv:1807.00263.",
      "citeRegEx" : "Kuleshov et al\\.,? 2018",
      "shortCiteRegEx" : "Kuleshov et al\\.",
      "year" : 2018
    }, {
      "title" : "Calibrated structured prediction",
      "author" : [ "Volodymyr Kuleshov", "Percy S Liang." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3474–3482.",
      "citeRegEx" : "Kuleshov and Liang.,? 2015",
      "shortCiteRegEx" : "Kuleshov and Liang.",
      "year" : 2015
    }, {
      "title" : "Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration",
      "author" : [ "Meelis Kull", "Miquel Perello Nieto", "Markus Kängsepp", "Telmo Silva Filho", "Hao Song", "Peter Flach." ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kull et al\\.,? 2019",
      "shortCiteRegEx" : "Kull et al\\.",
      "year" : 2019
    }, {
      "title" : "Verified uncertainty calibration",
      "author" : [ "Ananya Kumar", "Percy S Liang", "Tengyu Ma." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3792–3803.",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Calibration of encoder decoder models for neural machine translation",
      "author" : [ "Aviral Kumar", "Sunita Sarawagi." ],
      "venue" : "arXiv preprint arXiv:1903.00802.",
      "citeRegEx" : "Kumar and Sarawagi.,? 2019",
      "shortCiteRegEx" : "Kumar and Sarawagi.",
      "year" : 2019
    }, {
      "title" : "The implicit fairness criterion of unconstrained learning",
      "author" : [ "Lydia T Liu", "Max Simchowitz", "Moritz Hardt." ],
      "venue" : "International Conference on Machine Learning, pages 4051–4060. PMLR.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The evidence framework applied to classification networks",
      "author" : [ "David JC MacKay." ],
      "venue" : "Neural computation, 4(5):720–736.",
      "citeRegEx" : "MacKay.,? 1992",
      "shortCiteRegEx" : "MacKay.",
      "year" : 1992
    }, {
      "title" : "Calibrated model-based deep reinforcement learning",
      "author" : [ "Ali Malik", "Volodymyr Kuleshov", "Jiaming Song", "Danny Nemer", "Harlan Seymour", "Stefano Ermon." ],
      "venue" : "arXiv preprint arXiv:1906.08312.",
      "citeRegEx" : "Malik et al\\.,? 2019",
      "shortCiteRegEx" : "Malik et al\\.",
      "year" : 2019
    }, {
      "title" : "Binary classifier calibration: Nonparametric approach",
      "author" : [ "Mahdi Pakdaman Naeini", "Gregory F Cooper", "Milos Hauskrecht." ],
      "venue" : "arXiv preprint arXiv:1401.3390.",
      "citeRegEx" : "Naeini et al\\.,? 2014",
      "shortCiteRegEx" : "Naeini et al\\.",
      "year" : 2014
    }, {
      "title" : "Obtaining well calibrated probabilities using bayesian binning",
      "author" : [ "Mahdi Pakdaman Naeini", "Gregory F Cooper", "Milos Hauskrecht." ],
      "venue" : "Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 2015, page",
      "citeRegEx" : "Naeini et al\\.,? 2015",
      "shortCiteRegEx" : "Naeini et al\\.",
      "year" : 2015
    }, {
      "title" : "Posterior calibration and exploratory analysis for natural language processing models",
      "author" : [ "Khanh Nguyen", "Brendan O’Connor" ],
      "venue" : "arXiv preprint arXiv:1508.05154",
      "citeRegEx" : "Nguyen and O.Connor.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen and O.Connor.",
      "year" : 2015
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
      "author" : [ "John Platt" ],
      "venue" : "Advances in large margin classifiers, 10(3):61–74.",
      "citeRegEx" : "Platt,? 1999",
      "shortCiteRegEx" : "Platt",
      "year" : 1999
    }, {
      "title" : "Post-hoc calibration of neural networks",
      "author" : [ "Amir Rahimi", "Kartik Gupta", "Thalaiyasingam Ajanthan", "Thomas Mensink", "Cristian Sminchisescu", "Richard Hartley." ],
      "venue" : "arXiv preprint arXiv:2006.12807.",
      "citeRegEx" : "Rahimi et al\\.,? 2020a",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2020
    }, {
      "title" : "Intra orderpreserving functions for calibration of multi-class neural networks",
      "author" : [ "Amir Rahimi", "Amirreza Shaban", "Ching-An Cheng", "Byron Boots", "Richard Hartley." ],
      "venue" : "arXiv preprint arXiv:2003.06820.",
      "citeRegEx" : "Rahimi et al\\.,? 2020b",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2020
    }, {
      "title" : "Calibration of confidence measures in speech recognition",
      "author" : [ "Dong Yu", "Jinyu Li", "Li Deng." ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, 19(8):2461–2473.",
      "citeRegEx" : "Yu et al\\.,? 2011",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2011
    }, {
      "title" : "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers",
      "author" : [ "Bianca Zadrozny", "Charles Elkan." ],
      "venue" : "Icml, volume 1, pages 609–616. Citeseer.",
      "citeRegEx" : "Zadrozny and Elkan.,? 2001",
      "shortCiteRegEx" : "Zadrozny and Elkan.",
      "year" : 2001
    }, {
      "title" : "Transforming classifier scores into accurate multiclass probability estimates",
      "author" : [ "Bianca Zadrozny", "Charles Elkan." ],
      "venue" : "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 694–699.",
      "citeRegEx" : "Zadrozny and Elkan.,? 2002",
      "shortCiteRegEx" : "Zadrozny and Elkan.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Deep learning has proven to be tremendously attractive for researchers in fields such as physics, biology, and manufacturing, to name a few (Baldi et al., 2014; Anjos et al., 2015; Bergmann et al., 2014).",
      "startOffset" : 140,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "Deep learning has proven to be tremendously attractive for researchers in fields such as physics, biology, and manufacturing, to name a few (Baldi et al., 2014; Anjos et al., 2015; Bergmann et al., 2014).",
      "startOffset" : 140,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "Deep learning has proven to be tremendously attractive for researchers in fields such as physics, biology, and manufacturing, to name a few (Baldi et al., 2014; Anjos et al., 2015; Bergmann et al., 2014).",
      "startOffset" : 140,
      "endOffset" : 203
    }, {
      "referenceID" : 10,
      "context" : "However, these are fields in which representing model uncertainty is of crucial importance (Gal and Ghahramani, 2016).",
      "startOffset" : 91,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "Overconfident incorrect predictions can be harmful or offensive in NLP applications (Amodei et al., 2016), hence proper uncertainty quantification is crucial in practice.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : "Therefore, blindly using the output of the softmax function output as the model uncertainty is misleading (Kumar and Sarawagi, 2019; Dong et al., 2018; Nguyen and O’Connor, 2015).",
      "startOffset" : 106,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "Therefore, blindly using the output of the softmax function output as the model uncertainty is misleading (Kumar and Sarawagi, 2019; Dong et al., 2018; Nguyen and O’Connor, 2015).",
      "startOffset" : 106,
      "endOffset" : 178
    }, {
      "referenceID" : 30,
      "context" : "Therefore, blindly using the output of the softmax function output as the model uncertainty is misleading (Kumar and Sarawagi, 2019; Dong et al., 2018; Nguyen and O’Connor, 2015).",
      "startOffset" : 106,
      "endOffset" : 178
    }, {
      "referenceID" : 23,
      "context" : "1, approximately 100 of them should indeed have cancer (Kumar et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Even if the actual mechanism might be difficult to interpret, a calibrated model at least gives us a signal that it “knows what it doesn’t know,” thereby making these models easier to deploy in practice (Jiang et al., 2012).",
      "startOffset" : 203,
      "endOffset" : 223
    }, {
      "referenceID" : 20,
      "context" : "As majority of the current state-of-the art machine learning models, such as DNNs, do not output calibrated probabilities out of the box (Kuleshov et al., 2018), existing works rely on re-calibration methods that take the output of an uncalibrated model, and transform it into a calibrated probability.",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 36,
      "context" : ", 1999), isotonic regression (Zadrozny and Elkan, 2002), and temperature scaling (Guo et al.",
      "startOffset" : 29,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : ", 1999), isotonic regression (Zadrozny and Elkan, 2002), and temperature scaling (Guo et al., 2017).",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "however it is challenging to calibrate posterior estimates with sub-optimal binning schemes (Kumar et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 35,
      "context" : "An alternative approach, histogram binning (Zadrozny and Elkan, 2001), outputs probabilities from a finite set.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Model uncertainty estimation and posterior calibration is a topic of continued interest not only in the fields of machine learning and statistics, but also in meteorology (Bröcker, 2009), fairness (Liu et al.",
      "startOffset" : 171,
      "endOffset" : 186
    }, {
      "referenceID" : 25,
      "context" : "Model uncertainty estimation and posterior calibration is a topic of continued interest not only in the fields of machine learning and statistics, but also in meteorology (Bröcker, 2009), fairness (Liu et al., 2019), healthcare (Jiang et al.",
      "startOffset" : 197,
      "endOffset" : 215
    }, {
      "referenceID" : 15,
      "context" : ", 2019), healthcare (Jiang et al., 2012), reinforcement learning (Malik et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : ", 2012), reinforcement learning (Malik et al., 2019), natural language processing (Card and Smith, 2018), speech recognition (Yu et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : ", 2019), natural language processing (Card and Smith, 2018), speech recognition (Yu et al.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : ", 2019), natural language processing (Card and Smith, 2018), speech recognition (Yu et al., 2011) and economics (Gneiting et al.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Platt scaling, temperature scaling, vector scaling (Platt et al., 1999; Guo et al., 2017)), to non-parametric methods (e.",
      "startOffset" : 51,
      "endOffset" : 89
    }, {
      "referenceID" : 35,
      "context" : "quantile or bayesian binning (Zadrozny and Elkan, 2001; Naeini et al., 2015), and isotonic regression (Zadrozny and Elkan, 2002).",
      "startOffset" : 29,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "quantile or bayesian binning (Zadrozny and Elkan, 2001; Naeini et al., 2015), and isotonic regression (Zadrozny and Elkan, 2002).",
      "startOffset" : 29,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "Some of the proposed methods for this purpose are matrix scaling and Dirichlet scaling which both use linear models for estimation of p(y = k|f(x)) (Guo et al., 2017; Kull et al., 2019), and MLP and order preserving functions (Rahimi et al.",
      "startOffset" : 148,
      "endOffset" : 185
    }, {
      "referenceID" : 22,
      "context" : "Some of the proposed methods for this purpose are matrix scaling and Dirichlet scaling which both use linear models for estimation of p(y = k|f(x)) (Guo et al., 2017; Kull et al., 2019), and MLP and order preserving functions (Rahimi et al.",
      "startOffset" : 148,
      "endOffset" : 185
    }, {
      "referenceID" : 26,
      "context" : "In Bayesian Neural Networks (BNNs) the predictive uncertainty will naturally be high in regions where training data is scarce (MacKay, 1992).",
      "startOffset" : 126,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "Consequently, following papers propose various approximations such as variational inference (VI) (Graves, 2011; Blundell et al., 2015).",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "Consequently, following papers propose various approximations such as variational inference (VI) (Graves, 2011; Blundell et al., 2015).",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "Although BNNs are theoretically proven to control the overconfidence of the model in unseen regions of data space (Kristiadi et al., 2020), they require expensive approximations which limit their application in most modern NLP architectures.",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 16,
      "context" : "In (Joo et al., 2020) the authors model the distribution on posterior probability using a Dirichlet prior distribution and variational inference.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "MCDropout is a variational approximation of Gaussian processes that avoids explicit modeling of the posterior distribution (Gal and Ghahramani, 2016).",
      "startOffset" : 123,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "Previous works such as (Jung et al., 2020; Nguyen and O’Connor, 2015) propose to use the downstream task with small number of classes to perform calibration and estimation of the calibration error.",
      "startOffset" : 23,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : "Previous works such as (Jung et al., 2020; Nguyen and O’Connor, 2015) propose to use the downstream task with small number of classes to perform calibration and estimation of the calibration error.",
      "startOffset" : 23,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "To cope with this problem, approximate calibration methods using a set of interesting events and feature based calibration are proposed in (Kuleshov and Liang, 2015; Jagannatha and Yu, 2020) and an alternative calibration error estimator was proposed using sequence precision scoring function BLEU in (Kumar and Sarawagi, 2019).",
      "startOffset" : 139,
      "endOffset" : 190
    }, {
      "referenceID" : 14,
      "context" : "To cope with this problem, approximate calibration methods using a set of interesting events and feature based calibration are proposed in (Kuleshov and Liang, 2015; Jagannatha and Yu, 2020) and an alternative calibration error estimator was proposed using sequence precision scoring function BLEU in (Kumar and Sarawagi, 2019).",
      "startOffset" : 139,
      "endOffset" : 190
    }, {
      "referenceID" : 24,
      "context" : "To cope with this problem, approximate calibration methods using a set of interesting events and feature based calibration are proposed in (Kuleshov and Liang, 2015; Jagannatha and Yu, 2020) and an alternative calibration error estimator was proposed using sequence precision scoring function BLEU in (Kumar and Sarawagi, 2019).",
      "startOffset" : 301,
      "endOffset" : 327
    }, {
      "referenceID" : 17,
      "context" : "Alternatively, we can dynamically estimate the required statistics for calibration from the train set during training iterations, thereby minimizing cross-entropy as well as the calibration error as a multi-task setup (Jung et al., 2020).",
      "startOffset" : 218,
      "endOffset" : 237
    }, {
      "referenceID" : 28,
      "context" : "However, the number of samples required to calibrate scales linearly with the number of distinct probabilities B the model can output which can be large in the multi-class setting (Naeini et al., 2014).",
      "startOffset" : 180,
      "endOffset" : 201
    }, {
      "referenceID" : 35,
      "context" : ", 1999) and the verification guarantees of histogram binning (Zadrozny and Elkan, 2001) by defining the Platt-Binning Calibrator.",
      "startOffset" : 61,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "In the following section we prove the efficacy of our method by carrying out extensive evaluation of the performance of pretrained transformer models such as BERT (Devlin et al., 2018) on simple multi-class text classification tasks.",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "Our motivation comes from the analysis in (Desai and Durrett, 2020) which shows that pretrained models are significantly better calibrated when used out-of-the-box.",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "Despite its simplicity this method is competitive with the more complex methods when implemented post-hoc (Guo et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "• PosCal end-to-end training calibration using histogram binning (Jung et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "We test the baselines and our method on the benchmark on NLP classification tasks: xSLUE (Kang and Hovy, 2019).",
      "startOffset" : 89,
      "endOffset" : 110
    } ],
    "year" : 0,
    "abstractText" : "Modern NLP classifiers are known to return uncalibrated estimations of class posteriors. Existing methods for posterior calibration rescale the predicted probabilities but often have an adverse impact on final classification accuracy, thus leading to poorer generalization. We propose an end-to-end trained calibrator, PlattBinning, that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities. Our method leverages the sample efficiency of Platt scaling and the verification guarantees of histogram binning, thus not only reducing the calibration error but also improving task performance. In contrast to existing calibrators, we perform this efficient calibration during training. Empirical evaluation of benchmark NLP classification tasks echoes the efficacy of our proposal.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_316_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compilable Neural Code Generation with Compiler Feedback",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automated code generation (or program synthesis) has attracted much attention (Lu et al., 2021) over the past few years, because of its potential to improve the productivity of developers, as well as to speed up the software development (Parvez et al., 2021; Wang et al., 2021a). In the life cycle of software development, different types of code generation tasks are desired, including code completion (Liu et al., 2020), code generation with naturallanguage descriptions (text-to-code) (Hashimoto et al., 2018), program translation (Chen et al., 2018), and program repair (Li et al., 2020).\nRecently, much effort has been made to advance the development of code generation (Li et al., 2018;\n1Our source code and datasets used in this paper will be released as this paper is accepted.\nLiu et al., 2017), using different logical forms of codes, such as the abstract syntax tree (AST) (Kim et al., 2021; Yin and Neubig, 2017; Rabinovich et al., 2017), sketching (Nye et al., 2019) and program-derived semantics graph (PSG) (Iyer et al., 2020). Benefiting from the strong power of pretraining techniques in natural language processing (NLP), several attempts have been made towards pre-training a language model on large-scale code corpus for code generation, such as CodeGPT (Svyatkovskiy et al., 2020), PLBART (Ahmad et al., 2021), and CodeT5 (Wang et al., 2021b).\nHowever, to the best of our knowledge, existing deep-learning approaches for code generation usually do not consider the compilablility of the generated code, resulting in non-compilable codes. For example, Chen et al. (2021) found that up to 67%- 97% of patches generated by the most advanced deep-learning-based models are non-compilable. We attribute it to the discrepancy between minimizing a loss function of the language model and\ngenerating compilable code. The generation of noncompilable code will waste time of programmers, as well as seriously reduce the trust and satisfaction of developers with the model.\nThis paper focuses on the task of compilable neural code generation. To be more specific, we consider function-level code generation since functions are core components of the code. Note that on the execution of programs, the programs are first compiled into low-level machine code that can be executed. Intuitively, we can measure the compilability of generated programs by simply feeding them into well-known compilers (e.g., LLVM Clang2 for C, and Antlr3 for Java). This motivates us to answer the following research question: can we take the compiler feedback as guidance to generate compilable code?\nIn this paper, we propose COMPCODER, a novel three-stage pipeline utilizing the compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Figure 1 shows an example of Python code completion by COMPCODER, which utilizes the compiler feedback. As shown in the Figures 1(b) and (c), we input the code generated by the language model into the compiler to determine whether it can be successfully compiled. This feedback comes from the compiler should be used to optimize the model.\nOverall, the key contributions of this paper are as follows: • To the best of our knowledge, we are the first\nto introduce the compiler feedback of a program function as guidance to generate compilable code.\n• We propose a three-stage pipeline to refine a pretrained code generator using reinforcement learning and to jointly learn a discriminator to enforce the generator to correct its own mistakes.\n• Comprehensive experiment on two code generation tasks demonstrate the effectiveness of the proposed method. It boost the average compilation rate of CodeGPT from 44.18 to 89.18 in code completion and from 70.3 to 96.2 in text-to-code generation."
    }, {
      "heading" : "2 Preliminary",
      "text" : "In this section, we set out the notations for task formulation, as well as some preliminaries of compiler\n2https://clang.llvm.org 3https://www.antlr.org\nfeedback. Let s ∈ S denote the given input, which can be pieces of partial code, natural-language descriptions, as well as buggy programs. Let t ∈ T denote the generated source code. Formally, the problem of code generation can be formulated as learning a mapping function f between the input space and target code space, e.g., f : S → T . In this paper, we investigate two specific tasks related to code generation, i.e., code completion and text-to-code generation, conditioned on different inputs.\nCode Completion Let c = {c1, c2, . . . , c|c|} denote a sequence of code tokens for program c, where |c| denotes the length of the code. We use notation c1: m ∈ S to refer to the previous code snippet {c1, c2, . . . , cm} and notation cm+1: |c| ∈ T to represent the subsequent code snippet {cm+1, . . . , c|c|}. The code completion task can be defined as generating the subsequent (t) code token sequence cm+1: |c|, given the previous (s) code sequence c1: m.\nText-to-Code Generation Different from code completion, the text-to-code generation aims to generate a whole program based on natural language description. Let d = {d1, d2, . . . , d|d|} refer to a sequence of natural-language tokens. The textto-code generation task can be defined as generating source code c = t ∈ T , given the corresponding natural language description d = s ∈ S.\nCompiler Feedback As the whole program c is generated, no matter from partial code snippets or natural-language descriptions, we feed it into a compiler to test whether it can be compiled successfully. Formally, we define the the compiler feedback as:\nfeedback = 1Compiler(c) , (1)\nwhere the compiler feedback is a binary value (compilable or non-compilable), and c denotes the code snippet fed into compilers. As for the task of textto-code generation, we simply feed the generated code t into compilers, i.e., c = t. As for the task of code completion, we concatenate the partial code with generated code as a whole program, i.e., c = [s; t], where ; is the concatenation operation."
    }, {
      "heading" : "3 COMPCODER",
      "text" : "Figure 2 shows the overall architecture of COMPCODER, which covers three stages, i.e., language\nmodel fine-tuning (Stage 1), compilability reinforcement (Stage 2) and compilability discrimination (Stage 3). In the following subsections, we will elaborate on each stage one by one. We alternately perform Stages 2 and 3, as described in Section 3.4."
    }, {
      "heading" : "3.1 Stage 1: Language Model Fine-Tuning",
      "text" : "As shown in Figure 2(a), we adopt CodeGPT as the generator, which uses GPT-2 (Radford et al., 2019) as the starting point and is continually pre-trained on the large-scale code corpus. Our generator is then fine-tuned on the target task to minimize the cross-entropy loss:\nLG = − 1\n|M| |M|∑ i |V|∑ j Yij log Pij , (2)\nwhere M denotes the set of the generated code tokens, V represents the vocabulary, Yij denotes the label of the code token i in class j, and Pij is the predicted probability of token i in class j.\nDuring training, the generator takes x = {<BOS>, c,<EOS>} as the input in the code completion task, and x = {d,<BOS>, c,<EOS>} as input in the text-to-code generation task, correspondingly. Special tokens <BOS> and <EOS> indicate the start and end symbols of code sequences. After several epochs of supervised fine-tuning on the target task dataset, we save the trained generator, which will be used in the next stage."
    }, {
      "heading" : "3.2 Stage 2: Compilability Reinforcement",
      "text" : "Reinforcement Learning (RL) is a method of learning the optimal policy by obtaining reward signals from the real environment (Sutton and Barto, 1998; Wan et al., 2018). As shown in Figure 2(b), we use the fine-tuned generator ρ (after Stage 1) as the reference model. Then we initialize a policy π = ρ. Given an input sequence s ∈ S, our goal is to find a policy π that generates an output sequence t ∈ T with the objective of maximizing the compilability-based reward. We use RL (i.e., PPO2 version of Proximal Policy Optimization (Schulman et al., 2017)) to directly optimize the expected reward as:\nEπ [r] = Es∼S,t∼π(.|s) [r(s, t)] , (3)\nwhere the policy π is rewarded by the compiler (Eq. 1), r is the reward function. We define r(s, t) = 1.0 iff the code can be compiled by the program compiler and r(s, t) = −1.0 otherwise.\nIt is worth mentioning that code compilability constraints can be strong or weak. Strong Constraint: A long piece of code snippet may not be correctly compiled if a certain token is changed. Weak Constraint: A blank string consisting of whitespace characters can be correctly compiled by the compiler. Concretely, in the text-to-code generation task, if the generator generates a string composed of whitespace characters, the compiler will consider it as a good case. In the code completion task, if the previous code snippet is compilable, the generator can fool the compiler easily. The RL\nis good at making use of this, resulting in the generated codes can be compiled, but seriously deviating from the generation likelihood objective.\nTo avoid active model π being too far away from reference model ρ, we add a Kullback-Leibler (KL) penalty with expectation, e.g., βKL(π, ρ) (Ziegler et al., 2019). Therefore, the modified reward will be reformulated as follows:\nr(s, t) = r(s, t)− β logπ(t|s) ρ(t|s) , (4)\nwhere β is a constant, which plays the role of an entropy bonus, preventing the policy from moving too far from the range where r is valid.\nTo alleviate the imbalance between the reward term and the KL penalty term, so as to improve the stability of training, we use autoregressive finetuning to make the KL penalty term fluctuate within a small range after RL training. This fine-tuning process incorporates a compilability-aware discriminator will be introduced in the next stage."
    }, {
      "heading" : "3.3 Stage 3: Compilability Discrimination",
      "text" : "Figure 3 shows a case of code completion. We mask the last five tokens of a Python function and ask the generator to complete them. The generator generates five candidates with high probabilities. Some minor mistakes prevent four of them from being successfully compiled. We hope the generator can have more perceiving to explicitly distinguish compilable and non-compilable codes generated by itself. In this stage, we design a compilabilityaware discriminator to deal with this issue.\nConcretely, we add a discriminator (a two-layer MLP equipped with the tanh activation function between layers) after the final hidden layer of the generator. As shown in Figure 2(c), given the input sequence s, we perform beam search on the generator to generate top-k candidates. Each candidate\ncode c ∈ Q is labeled by the program compiler as positive (1) or negative (0), depending on whether it can be correctly compiled (see Eq. 1).\nWe use the hidden representation of the last token (<EOS>) as the final representation of the entire code c. Finally, the hidden representation of the last token (<EOS>) is fed into the discriminator for prediction:\nh<EOS> = CodeGPT(s, t) , (5) h ′ <EOS> = Discriminator(h<EOS>) , (6) P (.|t, s) = softmax(h′<EOS>) , (7)\nwhere h<EOS> denotes the representation of the last token <EOS>. The training loss of the discrimination process can be defined as:\nLD = − 1\n|Q+ ∪Q−|  ∑ c∈Q+ log P (1|t, s)\n+ ∑\nc∈Q− log P (0|t, s)\n , (8)\nwhere Q+ and Q− represent positive and negative sets respectively. The parameters of the generator and discriminator will be jointly updated.\nAt this stage, we jointly train the generator and the discriminator, including a generating objective (to learn generator only) and a discriminating objective (to learn generator and discriminator together), as shown in Figure 2(c). The joint training loss is defined as follows:\nL = LG + LD . (9)"
    }, {
      "heading" : "3.4 Overall Pipeline",
      "text" : "Training Procedure We perform an interactive training procedure. Concretely, except that the first epoch contains Stages 1, 2, and 3, each subsequent epoch only consists of Stages 2 and 3. We update\nthe reference model (at Stage 2), and candidates (at Stage 3) in a preset frequency.\nFor better understanding, Stage 2 improves the compilability of generated codes, Stage 3 distinguishes the compilable and non-compilable codes generated by itself. Stage 2 and 3 refine each other and improve the performance iteratively, which is a basic idea of this training procedure. We think that the generator with high compilability (after Stage 2) facilitates the learning of the discriminator (discriminating objective at Stage 3). The autoregressive fine-tuning (generating objective at Stage 3) helps the KL penalty term (at Stage 2) fluctuate in a small range, improving the stability of RL training. At Stage 3, the discriminating objective is optimized by learning the generator and discriminator together, which makes the generator have more perceiving to distinguish compilable and noncompilable codes.\nInference Procedure The model inference consists of two stages. Given an input sequence (s), we perform the beam search on the generator to generate top-k candidates. The code c (in Eq. 1) with the highest compilability probability evaluated by the discriminator will be selected. Then the output t can be obtained as the final result."
    }, {
      "heading" : "4 Experiment Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Evaluation Tasks and Datasets",
      "text" : "We conduct experiments on two tasks: code completion and text-to-code generation. To investigate the compilability of the generated codes, we need to preserve the indentation and newline operations in codes. We also need to make sure that the code and its version belong to the scope of the compiler. Existing datasets on both of the two tasks usually do not serve these considerations. For convenience, we choose Python for experiments, as it is very popular and used in many projects. We conduct all experiments based on Python 3 environment and adopt the codeop4 module to simulate the program compiler. We remove codes that could not be compiled correctly by the compiler.\nCode Completion For the code completion task, we use the Python corpus in CodeSearchNet (Husain et al., 2019). We want to study the compilability of long enough codes, while longer codes mean\n4https://docs.python.org/3.6/library/ codeop.html\nhigher computational overhead. Therefore, we extract 50k compilable Python methods (Python 3 version) with eclectic token lengths ranging from 64 to 96. We randomly select 45k samples for training and the remaining 5k samples for testing. We mask a different number of tokens at the tail of source codes and let the model complete.\nText-to-Code Generation For text-to-code generation task, we adopt the AdvTest dataset (Lu et al., 2021), which contains 251,820 text-Python code pairs. We extract 41k pairs of text and code snippets in Python 3. The code token lengths range from 128 to 170, and the text token lengths are more than 5. We randomly select 40k text-Python code pairs for training, and the remaining 1k textPython code pairs for testing."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "To evaluate the quality of the generated codes, we adopt two widely-used evaluation metrics: Levenshtein Edit Similarity (ES) (Svyatkovskiy et al., 2020) and Compilation Rate (CR) (Kulal et al., 2019). Levenshtein Edit Similarity measures the number of single-character edits required to transform one string into another. It is a critical evaluation metric for the code generation scenario, as it measures the effort required for the developer to correct the code. Compilation Rate measures how many codes can be correctly compiled by the program compiler. For both of these metrics, bigger values indicate better performance."
    }, {
      "heading" : "4.3 Baseline Methods",
      "text" : "We compare our approach with carious state-ofthe-art models in the code completion task and the text-to-code generation task: • BiLSTM is a Seq2Seq model based on Bidirec-\ntional LSTM with an attention mechanism (Luong et al., 2015).\n• Transformer (Vaswani et al., 2017) is the base architecture of CodeGPT. We use 6-layer Transformer decoder to conduct experiments.\n• GPT-2 (Radford et al., 2019) is an autoregressive pre-trained model trained on largescale text corpus.\n• CodeGPT (Svyatkovskiy et al., 2020) is pretrained with source code corpus on the basis of GPT-2 vis causal language modeling objective.\n• PLBART (Ahmad et al., 2021) is based on the BART (Lewis et al., 2020) architecture, which is\npre-trained on large-scale Java and Python corpora via denoising autoencoding.\n• CodeT5 (Wang et al., 2021b) is based on the T5 (Raffel et al., 2020) architecture, which employs denoising sequence-to-sequence pretraining on multiple programming languages."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "In the code completion task, we set the learning rate as 1.5e-5, the batch size as 32, the maximum fine-tuning epoch as 20, the maximum code sequence length as 96. We mask different numbers of code tokens (25, 30, 35, 40, and 45) and ask the model to complete them. We set the minimum generation length as 25, 30, 35, 40, and 45 accordingly. In the text-to-code generation task, we set the learning rate as 1.5e-5, the batch size as 16, the maximum fine-tuning epoch as 20, the maximum text and code sequence length as 32 and 170. We set the minimum generation length as 96 (the generated code is slightly shorter than the ground-truth is allowed). In these two tasks, the generated sequence consists of whitespace characters will be considered as a bad case.\nWe use the Adam optimizer to update model parameters. We train our model on the basis of CodeGPT checkpoint.5 Our model is trained on 2 NVIDIA Tesla V100 with 32GB memory. We employ the same tokenizer as CodeGPT. To train the policy π, we use the PPO2 version of Proximal Policy Optimization (Schulman et al., 2017). In each epoch, we only randomly select 5% training data for the stability of RL training (Stage 2). In other stages (Stages 1 and 3), we use the full training data. To generate candidates (at Stage 3), we set the beam size as 5 in beam search. For efficiency, we update the candidates every 5 epochs.\n5https://huggingface.co/microsoft/ CodeGPT-small-py-adaptedGPT2"
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Code Completion",
      "text" : "Table 1 shows the results of the code completion task. We mask 25 tokens at the tail of code functions and ask the generation model to complete. We can observe that: (1) The codes generated by existing autoregressive models have a low compilation rate. CodeGPT and GPT-2 only achieve 46.84 and 43.26 scores respectively on the Compilation Rate, which means that more than half of the codes generated by them cannot be correctly compiled by the program compiler. (2) COMPCODER significantly improves the Compilation Rate. It obtains 94.48 scores on the Compilation Rate, which is 47.64 points higher than the closest competitor (CodeGPT). (3) When our approach significantly improves the compilation rate, it does not sacrifice the fluency of the generated codes. COMPCODER obtains a comparable and even slightly better Edit Similarity score than other baselines, indicating that it effectively preserves the code fluency.\nFigure 4 presents more results of the code completion task in the setting of completing 30, 35, 40, and 45 tokens. COMPCODER still effectively improves code compilability when generating longer codes. As the completion length increases, our approach outperforms CodeGPT by 49.66, 47.68, 46.64, and 33.36 points in the setting of completing\n30, 35, 40, and 45 tokens, respectively. On average, our approach outperforms CodeGPT by 45 points across a different number of tokens for the task of code completion."
    }, {
      "heading" : "5.2 Text-to-Code Generation",
      "text" : "Table 2 presents the results for the text-to-code generation task. We could see that: (1) COMPCODER significantly outperforms all other models w.r.t. the Compilation Rate. E.g., COMPCODER achieves 23.1 points and 24.3 points improvements when compared with PLBART and CodeT5 respectively. (2) Compared to code completion task (Table 1), all models in the text-to-code generation task have relatively higher compilation rate. One of the main reasons may be: code completion requires that the generated code could be consistent with the existing code, which is a much stronger restriction than code generation."
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "In this experiment, we compare several simplified versions of our model to understand the contribution of different components, including the Reinforcement Learning (RL) component and the discriminator’s effect for both model training (Dtrain) and model inference (Dtest). As a case study, we\ntake the code completion task as an example in the setting of completing 25 tokens and present the results in Table 3.\nSeveral meaningful observations can be drawn: First, both RL (Row 2) and Dtrain (Row 3) effectively increases the code compilation rate of the generation model (CodeGPT in Row 1), which confirms that the two methods we designed can indeed improve the ability of the generator for compilable code generation. Second, applying RL and Dtrain together (Row 4) further improves the compilation rate over their individual contributions. Third, using the discriminator to select output during model inference (Dtest) is beneficial. It further boosts the compilation rate of vanilla “Dtrain” by 17.08% (Row 5 v.s. Row 2) and boosts “RL+Dtrain” by 11.34% (Row 6 v.s. Row 4). Forth, these three components (RL, Dtrain, Dtest) that effectively improve compilation rate do not compromise the generation capability measured by edit similarity."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "To better understand the effectiveness of our proposed approach, we present a case for code completion and text-to-code generation. For both\nCodeGPT and COMPCODER, we present top-1 result in Figure 5. For code completion, we observe that CodeGPT can not complete code with high quality (non-compilable), while COMPCODER can complete the code well, and it is exactly the same for the reference solution. For text-to-code generation, we observe that although both models can not generate exactly the same code as the reference solution, COMPCODER generates a compilable code at the function level. These results reveal the effectiveness of our proposed approach for compilable code generation."
    }, {
      "heading" : "6 Related Work",
      "text" : "Neural Code Generation With the rapid development of Deep Learning (DL), some researchers attempt to use DL for code generation tasks. Liu et al. (2020) proposed a neural architecture for code completion task with multi-task learning based on the architecture of Transformer-XL Dai et al. (2019) and BiLSTM (Schuster and Paliwal, 1997). Kim et al. (2021) presented several ways of feeding the code structure to Transformer (Vaswani et al., 2017) and further improved the accuracy of the code prediction (next token prediction) task. Wei et al. (2019) adopted an encoder-decoder architecture and utilized the relations between code generation and code summarization to improve the performance of both tasks. Hashimoto et al. (2018) proposed a retrieve-and-edit framework for seq2seq model on code autocomplete task.\nBenefiting from the strong power of pre-training techniques in natural language processing (NLP), such as GPT (Radford et al., 2018), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020), some recent works attempt to pre-train language models on the corpus of source code for code generation. Svyatkovskiy et al. (2020) proposed CodeGPT follows the architecture of GPT-2 (Radford et al., 2019), which is pre-trained with a causal language modeling (CLM) objective on large-scale source codes. Liu et al. (2020) proposed CugLM, which adopts the same architecture as CodeGPT, and is pretrained by Masked bidirectional Language Modeling (MLM), Next Code segment Predicting (NCP), and Unidirectional Language Modeling (ULM) objectives. Ahmad et al. (2021) proposed PLBART follows the architecture of BART (Lewis et al., 2020), which is pre-trained on large-scale Java and Python functions paired with natural language comments via denoising autoencoding. Wang et al.\n(2021b) proposed CodeT5 based on the T5 (Raffel et al., 2020) architecture, which is employs denoising sequence-to-sequence pre-training on multiple programming languages.\nReinforced Text Generation Reinforcement learning (Sutton and Barto, 1998) has shown great success in various tasks. It focuses on how agents ought to take actions in an environment to maximize the cumulative reward, is well suited for decision-making tasks. Ranzato et al. (2016) were among the first to apply REINFORCE algorithm (Williams, 2004) to train recurrent neural networks on sequence generation tasks, suggesting that directly optimizing the metric used at the test phase can lead to better results. Chen and Bansal (2018) proposed a hybrid extractive-abstractive architecture with policy-based reinforcement learning. They used an extractor agent to select salient sentences and then employed an abstractor network to rewrite these extracted sentences. Wan et al. (2018) incorporated the tree structure and sequential content of code snippets and designed a deep reinforcement learning framework optimized by the metric of BLEU to improve the performance of the code summarization task. Yao et al. (2019) proposed a reinforcement learning framework, which encourages the code annotation model to generate annotations that can be used for code retrieval tasks. Korbak et al. (2021) proposed an energy-based model with an imposed constraint of generating only compilable sequences to improve compilation rates of generated codes."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "In this paper, we proposed COMPCODER, a novel three-stage (fine-tuning, compilability reinforcement, and compilability discrimination) pipeline utilizing compiler feedback for compilable code generation. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of COMPCODER, e.g., improving the average compilation rate of state-of-the-art CodeGPT from 44.18 to 89.18 in code completion and from 70.3 to 96.2 in text-to-code generation.\nThis work presents our preliminary attempt to generate compilable codes. Yet, considering the compilation rate is not the whole story as it still cannot guarantee the code correctness. As future work, we would like to utilize unit tests to evaluate the code correctness towards building more useful code generation models."
    } ],
    "references" : [ {
      "title" : "Unified pre-training for program understanding and generation",
      "author" : [ "References Wasi Uddin Ahmad", "Saikat Chakraborty", "Baishakhi Ray", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2021 Conference of the North Amer-",
      "citeRegEx" : "Ahmad et al\\.,? 2021",
      "shortCiteRegEx" : "Ahmad et al\\.",
      "year" : 2021
    }, {
      "title" : "Unified pre-training for program understanding and generation",
      "author" : [ "Wasi Uddin Ahmad", "Saikat Chakraborty", "Baishakhi Ray", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv: 2103.06333.",
      "citeRegEx" : "Ahmad et al\\.,? 2021",
      "shortCiteRegEx" : "Ahmad et al\\.",
      "year" : 2021
    }, {
      "title" : "Tree-to-tree neural networks for program translation",
      "author" : [ "Xinyun Chen", "Chang Liu", "Dawn Xiaodong Song." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv: 1805.11080.",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Sequencer: Sequence-to-sequence learning for end-to-end program repair",
      "author" : [ "Zimin Chen", "Steve Kommrusch", "Michele Tufano", "LouisNoël Pouchet", "Denys Poshyvanyk", "Monperrus Martin." ],
      "venue" : "IEEE Transactions on Software Engineering, 47:1943–1959.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G. Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov." ],
      "venue" : "arXiv preprint arXiv: 1901.02860.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "A retrieve-and-edit framework for predicting structured outputs",
      "author" : [ "Tatsunori B. Hashimoto", "Kelvin Guu", "Yonatan Oren", "Percy Liang." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Hashimoto et al\\.,? 2018",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2018
    }, {
      "title" : "A retrieve-and-edit framework for predicting structured outputs",
      "author" : [ "Tatsunori B. Hashimoto", "Kelvin Guu", "Yonatan Oren", "Percy S. Liang." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 31, pages 10052–10062.",
      "citeRegEx" : "Hashimoto et al\\.,? 2018",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2018
    }, {
      "title" : "Codesearchnet challenge: Evaluating the state of semantic code search",
      "author" : [ "Hamel Husain", "Hongqi Wu", "Tiferet Gazit", "Miltiadis Allamanis", "Marc Brockschmidt." ],
      "venue" : "arXiv preprint arXiv: 1909.09436.",
      "citeRegEx" : "Husain et al\\.,? 2019",
      "shortCiteRegEx" : "Husain et al\\.",
      "year" : 2019
    }, {
      "title" : "Software language comprehension using a program-derived semantic graph",
      "author" : [ "Roshni G. Iyer", "Yizhou Sun", "Wei Wang", "Justin Emile Gottschlich." ],
      "venue" : "ArXiv, abs/2004.00768.",
      "citeRegEx" : "Iyer et al\\.,? 2020",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2020
    }, {
      "title" : "Code prediction by feeding trees to transformers",
      "author" : [ "Seohyun Kim", "Jinman Zhao", "Yuchi Tian", "Satish Chandra." ],
      "venue" : "2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 150–162.",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Code prediction by feeding trees to transformers",
      "author" : [ "Seohyun Kim", "Jinman Zhao", "Yuchi Tian", "Satish Chandra." ],
      "venue" : "2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 150–162.",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Energy-based models for code generation under compilability constraints",
      "author" : [ "Tomasz Korbak", "Hady ElSahar", "Marc Dymetman", "Germán Kruszewski." ],
      "venue" : "arXiv preprint arXiv: 2106.04985.",
      "citeRegEx" : "Korbak et al\\.,? 2021",
      "shortCiteRegEx" : "Korbak et al\\.",
      "year" : 2021
    }, {
      "title" : "Spoc: Search-based pseudocode to code",
      "author" : [ "Sumith Kulal", "Panupong Pasupat", "Kartik Chandra", "Mina Lee", "Oded Padon", "Alexander Aiken", "Percy Liang." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Kulal et al\\.,? 2019",
      "shortCiteRegEx" : "Kulal et al\\.",
      "year" : 2019
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Code completion with neural attention and pointer networks",
      "author" : [ "Jian Li", "Yue Wang", "Michael R. Lyu", "Irwin King." ],
      "venue" : "Proceedings of the TwentySeventh International Joint Conference on Artificial Intelligence, pages 4159–4165.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Dlfix: Context-based code transformation learning for automated program repair",
      "author" : [ "Yameng Li", "Shaohua Wang", "Tien Nhut Nguyen." ],
      "venue" : "2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE), pages 602–614.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural code completion",
      "author" : [ "Chang Liu", "Xin Wang", "Richard Shin", "Joseph E. Gonzalez", "Dawn Song" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "A self-attentional neural architecture for code completion with multi-task learning",
      "author" : [ "Fang Liu", "Ge Li", "Bolin Wei", "Xin Xia", "Ming Li", "Zhiyi Fu", "Zhi Jin." ],
      "venue" : "Proceedings of the 28th International Conference on Program Comprehension.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task learning based pre-trained language model for code completion",
      "author" : [ "Fang Liu", "Ge Li", "Yunfei Zhao", "Zhi Jin." ],
      "venue" : "Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, pages 473–485.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "author" : [ "san", "Shao Kun Deng", "Shengyu Fu", "Shujie Liu" ],
      "venue" : "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
      "citeRegEx" : "san et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "san et al\\.",
      "year" : 2021
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to infer program sketches",
      "author" : [ "Maxwell Nye", "Luke B. Hewitt", "Joshua B. Tenenbaum", "Armando Solar-Lezama." ],
      "venue" : "ICML.",
      "citeRegEx" : "Nye et al\\.,? 2019",
      "shortCiteRegEx" : "Nye et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieval augmented code generation and summarization",
      "author" : [ "Md. Rizwan Parvez", "Wasi Uddin Ahmad", "Saikat Chakraborty", "Baishakhi Ray", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:2108.11601.",
      "citeRegEx" : "Parvez et al\\.,? 2021",
      "shortCiteRegEx" : "Parvez et al\\.",
      "year" : 2021
    }, {
      "title" : "Abstract syntax networks for code generation and semantic parsing",
      "author" : [ "Maxim Rabinovich", "Mitchell Stern", "Dan Klein." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol-",
      "citeRegEx" : "Rabinovich et al\\.,? 2017",
      "shortCiteRegEx" : "Rabinovich et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam M. Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "arXiv preprint arXiv: 1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence level training with recurrent neural networks. arXiv preprint arXiv: 1511.06732",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "Proximal policy optimization algorithms",
      "author" : [ "John Schulman", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov." ],
      "venue" : "arXiv preprint arXiv: 1707.06347.",
      "citeRegEx" : "Schulman et al\\.,? 2017",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2017
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K. Paliwal." ],
      "venue" : "IEEE Trans. Signal Process., 45:2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Introduction to reinforcement learning",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Intellicode compose: code generation using transformer",
      "author" : [ "Alexey Svyatkovskiy", "Shao Kun Deng", "Shengyu Fu", "Neel Sundaresan." ],
      "venue" : "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foun-",
      "citeRegEx" : "Svyatkovskiy et al\\.,? 2020",
      "shortCiteRegEx" : "Svyatkovskiy et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving automatic source code summarization via deep reinforcement learning",
      "author" : [ "Yao Wan", "Zhou Zhao", "Min Yang", "Guandong Xu", "Haochao Ying", "Jian Wu", "Philip S. Yu." ],
      "venue" : "Proceedings of the 33rd ACM/IEEE International Conference on Auto-",
      "citeRegEx" : "Wan et al\\.,? 2018",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2018
    }, {
      "title" : "Syncobert: Syntax-guided multi-modal contrastive pre-training for code representation",
      "author" : [ "Xin Wang", "Yasheng Wang", "Fei Mi", "Pingyi Zhou", "Yao Wan", "Xiao Liu", "Li Li", "Hao Wu", "Jin Liu", "Xin Jiang." ],
      "venue" : "arXiv preprint arXiv: 2108.04556.",
      "citeRegEx" : "Wang et al\\.,? 2021a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
      "author" : [ "Yue Wang", "Weishi Wang", "Shafiq Joty", "Steven C.H. Hoi." ],
      "venue" : "arXiv preprint arXiv:2109.00859.",
      "citeRegEx" : "Wang et al\\.,? 2021b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Code generation as a dual task of code summarization",
      "author" : [ "Bolin Wei", "Ge Li", "Xin Xia", "Zhiyi Fu", "Zhi Jin." ],
      "venue" : "arXiv preprint arXiv: 1910.05923.",
      "citeRegEx" : "Wei et al\\.,? 2019",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams." ],
      "venue" : "Machine Learning, 8:229–256.",
      "citeRegEx" : "Williams.,? 2004",
      "shortCiteRegEx" : "Williams.",
      "year" : 2004
    }, {
      "title" : "Coacor: Code annotation for code retrieval with reinforcement learning",
      "author" : [ "Ziyu Yao", "Jayavardhan Reddy Peddamail", "Huan Sun." ],
      "venue" : "The World Wide Web Conference.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "A syntactic neural model for general-purpose code generation",
      "author" : [ "Pengcheng Yin", "Graham Neubig." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 440–450.",
      "citeRegEx" : "Yin and Neubig.,? 2017",
      "shortCiteRegEx" : "Yin and Neubig.",
      "year" : 2017
    }, {
      "title" : "Fine-tuning language models from human preferences",
      "author" : [ "Daniel M. Ziegler", "Nisan Stiennon", "Jeff Wu", "Tom B. Brown", "Alec Radford", "Dario Amodei", "Paul Christiano", "Geoffrey Irving." ],
      "venue" : "ArXiv, abs/1909.08593.",
      "citeRegEx" : "Ziegler et al\\.,? 2019",
      "shortCiteRegEx" : "Ziegler et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : ", 2021) over the past few years, because of its potential to improve the productivity of developers, as well as to speed up the software development (Parvez et al., 2021; Wang et al., 2021a).",
      "startOffset" : 149,
      "endOffset" : 190
    }, {
      "referenceID" : 36,
      "context" : ", 2021) over the past few years, because of its potential to improve the productivity of developers, as well as to speed up the software development (Parvez et al., 2021; Wang et al., 2021a).",
      "startOffset" : 149,
      "endOffset" : 190
    }, {
      "referenceID" : 19,
      "context" : "In the life cycle of software development, different types of code generation tasks are desired, including code completion (Liu et al., 2020), code generation with naturallanguage descriptions (text-to-code) (Hashimoto et al.",
      "startOffset" : 123,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : ", 2020), code generation with naturallanguage descriptions (text-to-code) (Hashimoto et al., 2018), program translation (Chen et al.",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : ", 2018), program translation (Chen et al., 2018), and program repair (Li et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : ", 2017), using different logical forms of codes, such as the abstract syntax tree (AST) (Kim et al., 2021; Yin and Neubig, 2017; Rabinovich et al., 2017), sketching (Nye et al.",
      "startOffset" : 88,
      "endOffset" : 153
    }, {
      "referenceID" : 41,
      "context" : ", 2017), using different logical forms of codes, such as the abstract syntax tree (AST) (Kim et al., 2021; Yin and Neubig, 2017; Rabinovich et al., 2017), sketching (Nye et al.",
      "startOffset" : 88,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : ", 2017), using different logical forms of codes, such as the abstract syntax tree (AST) (Kim et al., 2021; Yin and Neubig, 2017; Rabinovich et al., 2017), sketching (Nye et al.",
      "startOffset" : 88,
      "endOffset" : 153
    }, {
      "referenceID" : 23,
      "context" : ", 2017), sketching (Nye et al., 2019) and program-derived semantics graph (PSG) (Iyer et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : ", 2019) and program-derived semantics graph (PSG) (Iyer et al., 2020).",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 33,
      "context" : "Benefiting from the strong power of pretraining techniques in natural language processing (NLP), several attempts have been made towards pre-training a language model on large-scale code corpus for code generation, such as CodeGPT (Svyatkovskiy et al., 2020), PLBART (Ahmad et al.",
      "startOffset" : 231,
      "endOffset" : 258
    }, {
      "referenceID" : 0,
      "context" : ", 2020), PLBART (Ahmad et al., 2021), and CodeT5 (Wang et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 27,
      "context" : "As shown in Figure 2(a), we adopt CodeGPT as the generator, which uses GPT-2 (Radford et al., 2019) as the starting point and is continually pre-trained on the large-scale code corpus.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 32,
      "context" : "Reinforcement Learning (RL) is a method of learning the optimal policy by obtaining reward signals from the real environment (Sutton and Barto, 1998; Wan et al., 2018).",
      "startOffset" : 125,
      "endOffset" : 167
    }, {
      "referenceID" : 35,
      "context" : "Reinforcement Learning (RL) is a method of learning the optimal policy by obtaining reward signals from the real environment (Sutton and Barto, 1998; Wan et al., 2018).",
      "startOffset" : 125,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : ", PPO2 version of Proximal Policy Optimization (Schulman et al., 2017)) to directly optimize the expected reward as:",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "Code Completion For the code completion task, we use the Python corpus in CodeSearchNet (Husain et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 33,
      "context" : "To evaluate the quality of the generated codes, we adopt two widely-used evaluation metrics: Levenshtein Edit Similarity (ES) (Svyatkovskiy et al., 2020) and Compilation Rate (CR) (Kulal et al.",
      "startOffset" : 126,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : ", 2020) and Compilation Rate (CR) (Kulal et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "We compare our approach with carious state-ofthe-art models in the code completion task and the text-to-code generation task: • BiLSTM is a Seq2Seq model based on Bidirectional LSTM with an attention mechanism (Luong et al., 2015).",
      "startOffset" : 210,
      "endOffset" : 230
    }, {
      "referenceID" : 34,
      "context" : "• Transformer (Vaswani et al., 2017) is the base architecture of CodeGPT.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 27,
      "context" : "• GPT-2 (Radford et al., 2019) is an autoregressive pre-trained model trained on largescale text corpus.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 33,
      "context" : "• CodeGPT (Svyatkovskiy et al., 2020) is pretrained with source code corpus on the basis of GPT-2 vis causal language modeling objective.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "• PLBART (Ahmad et al., 2021) is based on the BART (Lewis et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : ", 2021) is based on the BART (Lewis et al., 2020) architecture, which is",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 37,
      "context" : "• CodeT5 (Wang et al., 2021b) is based on the T5 (Raffel et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 28,
      "context" : ", 2021b) is based on the T5 (Raffel et al., 2020) architecture, which employs denoising sequence-to-sequence pretraining on multiple programming languages.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 30,
      "context" : "To train the policy π, we use the PPO2 version of Proximal Policy Optimization (Schulman et al., 2017).",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 34,
      "context" : "(2021) presented several ways of feeding the code structure to Transformer (Vaswani et al., 2017) and further improved the accuracy of the code prediction (next token prediction) task.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 26,
      "context" : "Benefiting from the strong power of pre-training techniques in natural language processing (NLP), such as GPT (Radford et al., 2018), BART (Lewis et al.",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : ", 2018), BART (Lewis et al., 2020), and T5 (Raffel et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : ", 2020), and T5 (Raffel et al., 2020), some recent works attempt to pre-train language models on the corpus of source code for code generation.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "(2020) proposed CodeGPT follows the architecture of GPT-2 (Radford et al., 2019), which is pre-trained with a causal language modeling (CLM) objective on large-scale source codes.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "(2021) proposed PLBART follows the architecture of BART (Lewis et al., 2020), which is pre-trained on large-scale Java and Python functions paired with natural language comments via denoising autoencoding.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "(2021b) proposed CodeT5 based on the T5 (Raffel et al., 2020) architecture, which is employs denoising sequence-to-sequence pre-training on multiple programming languages.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 32,
      "context" : "Reinforced Text Generation Reinforcement learning (Sutton and Barto, 1998) has shown great success in various tasks.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 39,
      "context" : "(2016) were among the first to apply REINFORCE algorithm (Williams, 2004) to train recurrent neural networks on sequence generation tasks, suggesting that directly optimizing the metric used at the test phase can lead to better results.",
      "startOffset" : 57,
      "endOffset" : 73
    } ],
    "year" : 0,
    "abstractText" : "Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model the code generation as text generation, either constrained by the grammar structure in the decoder, or driven by pretrained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for the compilability of generated programs. To improve the compilability of generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing the compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation-related tasks demonstrate the effectiveness of our proposed approach, e.g. improving the success rate of compilation from 44.18 to 89.18 in code completion on average, and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT.1",
    "creator" : null
  }
}
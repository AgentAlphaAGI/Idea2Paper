{
  "name" : "ARR_2022_231_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "With the prevalence of large neural networks with millions or billions of parameters, model compression is gaining prominence for facilitating efficient, eco-friendly deployment for machine learning applications. Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b). Previous works often train a large model as the “teacher”; then they fix the teacher and train a “student” model to mimic the behavior of the teacher, in order to transfer the knowledge from the teacher to the student.\n1The code will be released upon acceptance.\nHowever, this paradigm has the following drawbacks: (1) The teacher is unaware of the student’s capacity. Recent studies in pedagogy suggest student-centered learning, which considers students’ characteristics and learning capability, has shown effectiveness improving students’ performance (Cornelius-White, 2007; Wright, 2011). However, in conventional knowledge distillation, the student passively accepts knowledge from the teacher, without regard for the student model’s learning capability and performance. Recent works (Park et al., 2021; Shi et al., 2021) introduce student-aware distillation by jointly training the teacher and the student with task-specific objectives. However, there is still space for improvement since: (2) The teacher is not optimized for distillation. In previous works, the teacher is often trained to optimize its own inference performance. However, the teacher is not aware of the need to transfer its knowledge to a student and thus usually does so suboptimally. A real-world analogy is that a PhD student may have enough knowledge to solve problems themselves, but requires additional teaching training to qualify as a professor.\nTo address these two drawbacks, we propose Knowledge Distillation with Meta Learning (MetaDistil), a new teacher-student distillation framework using meta learning (Finn et al., 2017) to exploit feedback about the student’s learning progress to improve the teacher’s knowledge transfer ability throughout the distillation process. On the basis of previous formulations of bi-level optimization based meta learning (Finn et al., 2017), we propose a new mechanism called pilot update that aligns the learning of the bi-level learners (i.e., the teacher and the student).We illustrate the workflow of MetaDistil in Figure 1. The teacher in MetaDistil is trainable, which enables the teacher to adjust to its student network and also improves its “teaching skills.” Motivated by the idea of studentcentered learning, we allow the teacher to adjust\nits output based on the performance of the student model on a “quiz set,” which is a separate reserved data split from the original training set. For each training step, we first copy the student S to S′ and update S′ by a common knowledge distillation loss. We call this process a “teaching experiment.” In this way, we can obtain an experimental student S′ that can be quizzed. Then, we sample from the quiz set, and calculate the loss of S′ on these samples. We use this loss as a feedback signal to meta-update the teacher by calculating second derivatives and performing gradient descent (Finn et al., 2017). Finally, we discard the experimental subject S′ and use the updated teacher to distill into the student S on the same training batches. The use of meta learning allows the teacher model to receive feedback from the student in a completely differentiable way. We provide a simple and intuitive approach to explicitly optimize the teacher using the student’s quiz performance as a proxy.\nTo test the effectiveness of MetaDistil, we conduct extensive experiments on text and image classification tasks. MetaDistil outperforms knowledge distillation by a large margin, verifying the effectiveness and versatility of our method. Also, our method achieves state-of-the-art performance compressing BERT (Devlin et al., 2019) on the GLUE benchmark (Wang et al., 2019) and shows competitive results compressing ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) on CIFAR-100 (Krizhevsky et al., 2009). Additionally, we design experiments to analyze and explain the improvement. Ablation studies show the effectiveness of our proposed pilot update and dynamic distillation. Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably\nbecause of its ability to adjust its parameters."
    }, {
      "heading" : "2 Related Work",
      "text" : "Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., 2020; Zhou et al., 2020, 2021). Knowledge distillation is a prominent method for training compact networks to achieve comparable performance to a deep network. Hinton et al. (2015) first introduced the idea of knowledge distillation to exploit the “dark knowledge” (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model. Since its introduction, several works (Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Park et al., 2019; Sun et al., 2019; Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer. In the context of knowledge distillation, MetaDistil shares some common ideas with the line of work that utilizes a sequence of intermediate teacher models to make the teacher network better adapt to the capacity of the student model throughout the training process, including teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020) and route constraint optimization (RCO) (Jin et al., 2019). However, the intermediate teachers are heuristically selected independently of the training process and the evolution of the teacher network is discrete. In contrast, MetaDistil employs meta learning to make the teacher model adapt to the current state of the student model and provide a continuously evolving meta-teacher that can better teach the student. Concurrently, Park et al. (2021) and Shi et al. (2021) propose to update the teacher model jointly with the student model\nwith task specific objectives (e.g., cross-entropy loss) during the KD process and add constraints to keep student and teacher similar to each other. Their approaches makes the teacher model aware of the student model by constraining the teacher model’s capacity. However, the teacher models in their methods are still not optimized for knowledge transfer. In addition, Zhang et al. (2018) introduced deep mutual learning where multiple models learn collaboratively and teach each other throughout the training process. While it is focused on a different setting where different models have approximately the same capacity and are learned from scratch, it also encourages the teacher model to behave similarly to the student model. Different from all aforementioned methods, MetaDistil employs meta learning to explicitly optimize the teacher model for better knowledge transfer ability, and leads to improved performance of the resulting student model.\nMeta Learning The core idea of meta learning is “learning to learn,” which means taking the optimization process of a learning algorithm into consideration when optimizing the learning algorithm itself. Meta learning typically involves a bi-level optimization process where the inner-learner provides feedback for optimization of the meta-learner. Successful applications of meta learning include learning better initialization (Finn et al., 2017), architecture search (Liu et al., 2019), learning to optimize the learning rate schedule (Baydin et al., 2018), and learning to optimize (Andrychowicz et al., 2016). These works typically aim to obtain an optimized meta-learner (i.e., the teacher model in MetaDistil), while the optimization of the inner-learner (i.e., the student model in MetaDistil), is mainly used to provide learning signal for the meta optimization process. This is different from the objective of knowledge distillation where an optimized student model is the goal. Recently, there have been a few works investigating using this bi-level optimization framework to obtain a better inner-learner. For example, meta pseudo labels (Pham et al., 2020) uses meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model. These methods adapt the same bi-level optimization process as previous works where the goal is to obtain an optimized meta-learner. In these approaches,\nduring each iteration, the meta-learner is optimized for the original inner-learner and then applied to the updated inner-learner in the next iteration. This leads to a mismatch between the meta-learner and the inner-learner, and is therefore suboptimal for learning a good inner-learner. In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problem, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.\nMeta Knowledge Distillation Recently, some works on KD take a meta approach. Pan et al. (2020) proposed a framework to train a metateacher across domains that can better fit new domains with meta-learning. Then, traditional KD is performed to transfer the knowledge from the metateacher to the student. Liu et al. (2020) proposed a self-distillation network and utilizes meta-learning to train a label-generator, which is a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers. Different from the above, MetaDistil is a general knowledge distillation method that exploits meta-learning to allow the teacher to learn to teach dynamically. Instead of merely training a meta-teacher, our method uses meta-learning throughout the procedure of knowledge transfer, making the teacher model compatible for the student model for every training example during each training stage."
    }, {
      "heading" : "3 Knowledge Distillation with Meta Learning",
      "text" : "An overview of MetaDistil is presented in Figure 1. MetaDistil includes two major components. First, the meta update enables the teacher model to receive the student model’s feedback on the distillation process, allowing the teacher model to “learn to teach” and provide distillation signals that are more suitable for the student model’s current capacity. The pilot update mechanism ensures a finergrained match between the student model and the meta-updated teacher model."
    }, {
      "heading" : "3.1 Background",
      "text" : ""
    }, {
      "heading" : "3.1.1 Knowledge Distillation",
      "text" : "Knowledge distillation algorithms aim to exploit the hidden knowledge from a large teacher network, denoted as T , to guide the training of a shallow student network, denoted as S. To help transfer the knowledge from the teacher to the student, apart\nfrom the original task-specific objective (e.g., crossentropy loss), a knowledge distillation objective which aligns the behavior of the student and the teacher is included to train the student network. Formally, given a labeled dataset D of N samples D = {(x1, y1) , . . . , (xN , yN )}, we can write the loss function of the student network as follows,\nLS (D; θS ; θT ) = 1\nN N∑ i=1 [αLT (yi, S (xi; θS))\n+ (1− α)LKD (T (xi; θT ) , S (xi; θS))] (1)\nwhere α is a hyper-parameter to control the relative importance of the two terms; θT and θS are the parameters of the teacher T and student S, respectively. LT refers to the task-specific loss and LKD refers to the knowledge distillation loss which measures the similarity of the student and the teacher. Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error between student and teacher logits, the similarity between the student and the teacher’s attention distribution, etc. We do not specify the detailed form of the loss function because MetaDistil is a general framework that can be easily applied to various kinds of KD objectives as long as the objective is differentiable with respect to the teacher parameters. In the experiments of this paper, we use mean squared error between the hidden states of the teacher and the student for both our method and the KD baseline since recent study Kim et al. (2021) finds that it is more stable and slightly outperforms than KL divergence."
    }, {
      "heading" : "3.1.2 Meta Learning",
      "text" : "In meta learning algorithms that involve a bi-level optimization problem (Finn et al., 2017), there exists an inner-learner fi and a meta-learner fm. The inner-learner is trained to accomplish a task T or a distribution of tasks with help from the metalearner. The training process of fi on T with the help of fm is typically called inner-loop, and we can denote f ′i(fm) as the updated inner-learner after the inner-loop. We can express f ′i as a function of fm because learning fi depends on fm. In return, the meta-learner is optimized with a meta objective, which is generally the maximization of expected performance of the inner-learner after the innerloop, i.e., f ′i(fm). This learning process is called a meta-loop and is often accomplished by gradient\ndescent with derivatives of L(f ′i(fm)), the loss of updated inner-leaner on some held-out support set (i.e., the quiz set in our paper)."
    }, {
      "heading" : "3.2 Methodology",
      "text" : ""
    }, {
      "heading" : "3.2.1 Pilot Update",
      "text" : "In the original formulation of meta learning (Finn et al., 2017), the purpose is to learn a good metalearner fm that can generalize to different innerlearners fi for different tasks. In their approach, the meta-learner is optimized for the “original” innerlearner at the beginning of each iteration and the current batch of training data. The updated metalearner is then applied to the updated inner-learner and a different batch of data in the next iteration. This behavior is reasonable if the purpose is to optimize the meta-learner. However, in MetaDistil, we only care about the performance of the only innerlearner, i.e., the student. In this case, this behavior leads to a mismatch between the meta-learner and the inner-learner, and is therefore suboptimal for learning a good inner-learner. Therefore, we need a way to align and synchronize the learning of the meta- and inner-learner, in order to allow an update step of the meta-learner to have an instant effect on the inner-learner. This instant reflection prevents the meta-learner from catastrophic forgetting (McCloskey & Cohen, 1989). To achieve this, we design a pilot update mechanism. For a batch of training data x, we first make a temporary copy of the inner-learner fi and update both the copy f ′i and the meta learner fm on x. Then, we discard f ′i and update fi again with the updated fm on the same data x. This mechanism can apply the impact of data x to both fm and fi at the same time, thus aligns the training process. Pilot update is a general technique that can potentially be applied to any meta learning application that optimizes the inner-learner performance. We will describe how we apply this mechanism to MetaDistil shortly and empirically verify the effectiveness of pilot update in Section 4.2."
    }, {
      "heading" : "3.2.2 Learning to Teach",
      "text" : "In MetaDistil, we would like to optimize the teacher model, which is fixed in traditional KD frameworks. Different from previous deep mutual learning (Zhang et al., 2018) methods that switch the role between the student and teacher network and train the original teacher model with soft labels generated by the student model or recent works (Shi et al., 2021; Park et al., 2021) that\nAlgorithm 1 Knowledge Distillation with Meta Learning (MetaDistil) Require: student θS , teacher θT , train set D, quiz setQ Require: λ, µ: learning rate for the student and the teacher 1: while not done do 2: Sample batch of training data x ∼ D 3: Copy student parameter θS to student θ′S 4: Update θ′S with x and θT : θ\n′ S ← θ′S − λ∇θ′SLS(x; θS ; θT )\n5: Sample a batch of quiz data q ∼ Q 6: Update θT with q and θ′S : θT ← θT − µ∇θTLT (q, θ ′ S(θT )) 7: Update original θS with x and the updated θT : θS ← θS − λ∇θSLS(x; θS ; θT ) 8: end while\nupdate the teacher model with a task-specific loss during the KD process, MetaDistil explicitly optimizes the teacher model in a “learning to teach” fashion, so that it can better transfer its knowledge to the student model. Concretely, the optimization objective of the teacher model in the MetaDistil framework is the performance of the student model after distilling from the teacher model. This “learning to teach” paradigm naturally fits the bi-level optimization framework in the meta learning literature.\nIn the MetaDistil framework, the student network θS is the inner-learner and the teacher network θT is the meta-learner. For each training step, we first copy the student model θS to an “experimental student” θ′S . Then given a batch of training examples x and the learning rate λ, the experimental student is updated in the same way as conventional KD algorithms:\nθ′S(θT ) = θS − λ∇θSLS(x; θS ; θT ). (2)\nTo simplify notation, we will consider one gradient update for the rest of this section, but using multiple gradient updates is a straightforward extension. We observe that the updated experimental student parameter θ′S , as well as the student quiz loss lq = LT (q, θ′S(θT )) on a batch of quiz samples q sampled from a held-out quiz set Q, is a function of the teacher parameter θT . Therefore, we can optimize lq with respect to θT by a learning rate µ:\nθT ← θT − µ∇θTLT ( q, θ′S(θT ) ) (3)\nWe evaluate the performance of the experimental student on a separate quiz set to prevent overfitting the validation set, which is preserved for model selection. After meta-updating the teacher model, we then update the “real” student model in the same way as described in Equation 2. Intuitively, optimizing the teacher network θT with Equation 3\nis maximizing the expected performance of the student network after being taught by the teacher with the KD objective in the inner-loop. This metaobjective allows the teacher model to adjust its parameters to better transfer its knowledge to the student model. We apply the pilot update strategy described in Section 3.2.1 to better align the learning of the teacher and student. The complete algorithm is shown in Algorithm 1."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We evaluate MetaDistil on two commonly used classification benchmarks for knowledge distillation in both Natural Language Processing and Computer Vision (see Appendix A).\nSettings For NLP, we evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019). Specifically, we test on MRPC (Dolan & Brockett, 2005), QQP2 and STS-B (Conneau & Kiela, 2018) for Paraphrase Similarity Matching; SST-2 (Socher et al., 2013) for Sentiment Classification; MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016) and RTE (Wang et al., 2019) for the Natural Language Inference; CoLA (Warstadt et al., 2019) for Linguistic Acceptability. Following previous studies (Sun et al., 2019; Jiao et al., 2019; Xu et al., 2020), our goal is to distill BERT-Base (Devlin et al., 2019) into a 6-layer BERT with the hidden size of 768. The reported results are in the same format as on the GLUE leaderboard. For MNLI, we report the results on MNLI-m and MNLI-mm, respectively. For MRPC and QQP, we report both F1 and accuracy. For STS-B, we report Pearson and Spearman correlation. The metric for CoLA is Matthew’s correlation. The other tasks use accuracy as the metric.\n2https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs\nFollowing previous works (Sun et al., 2019; Turc et al., 2019; Xu et al., 2020), we evaluate MetaDistil in a task-specific setting where the teacher model is fine-tuned on a downstream task and the student model is trained on the task with the KD loss. We do not choose the pretraining distillation setting since it requires significant computational resources. We implement MetaDistil based on Hugging Face Transformers (Wolf et al., 2020).\nBaselines For comparison, we report the results of vanilla KD and patient knowledge distillation (Sun et al., 2019). We also include the results of progressive module replacing (Xu et al., 2020), a state-of-the-art task-specific compression\nmethod for BERT which also uses a larger teacher model to improve smaller ones like knowledge distillation. In addition, according to Turc et al. (2019), the reported performance of current taskspecific BERT compression methods is underestimated because the student model is not appropriately initialized. To ensure fair comparison, we re-run task-specific baselines with student models initialized by a pretrained 6-layer BERT model and report our results in addition to the official numbers in the original papers. We also compare against deep mutual learning (DML) (Zhang et al., 2018), teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020), route con-\nstraint optimization (RCO) (Jin et al., 2019), proximal knowledge teaching (ProKT) (Shi et al., 2021), and student-friendly teacher network (SFTN) (Park et al., 2021), where the teacher network is not fixed. For reference, we also present results of pretraining distilled models including DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), MiniLM v1 and v2 (Wang et al., 2020b,a). Note that among these baselines, PKD (Sun et al., 2019) and Theseus (Xu et al., 2020) exploit intermediate features while TinyBERT and the MiniLM family use both intermediate and Transformer-specific features. In contrast, MetaDistil uses none of these but the vanilla KD loss (Equation 1).\nTraining Details For training hyperparameters, we fix the maximum sequence length to 128 and the temperature to 2 for all tasks. For our method and all baselines (except those with officially reported numbers), we perform grid search over the sets of the student learning rate λ from {1e-5, 2e-5, 3e-5}, the teacher learning rate µ from {2e-6, 5e-6, 1e-5}, the batch size from {32, 64}, the weight of KD loss from {0.4, 0.5, 0.6}. We randomly split the original training set to a new training set and the quiz set by 9 : 1. For RCO, we select four unconverged teacher checkpoints as the intermediate training targets. For TAKD, we use KD to train a teacher assistant model with 10 Transformer layers."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "We report the experimental results on both the development set and test set of the eight GLUE tasks (Wang et al., 2019) in Table 1. MetaDistil achieves state-of-the-art performance under the task-specific setting and outperforms all KD baselines. Notably, without using any intermediate or model-specific features in the loss function, MetaDistil outperforms methods with carefully designed features, e.g., PKD and TinyBERT (without data augmentation). Compared with other methods with a trainable teacher (Zhang et al., 2018; Mirzadeh et al., 2020; Jin et al., 2019; Shi et al., 2021), our method still demonstrates superior performance. As we analyze, with the help of meta learning, MetaDistil is able to directly optimize the teacher’s teaching ability thus yielding a further improvement in terms of student accuracy. Also, we observe a performance drop by replacing pilot update with a normal update. This ablation study verifies the effectiveness of our proposed pilot update mechanism. Moreover, MetaDistil achieves\nvery competitive results on image classification as well, as described in Section A.2."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Why Does MetaDistil Work?",
      "text" : "We investigate the effect of meta-update for each iteration. We inspect (1) the validation loss of S′ after the teaching experiment and that of S after the real distillation update, and (2) the KD loss, which describes the discrepancy between student and teacher, before and after the teacher update.\nWe find that for 87% of updates, the student model’s validation loss after real update (Line 7 in Algorithm 1) is smaller than that after the teaching experiment (Line 4 in Algorithm 1), which would be the update to the student S in the variant without pilot update. This confirms the effectiveness of the pilot update mechanism on better matching the student and teacher model.\nMoreover, we find that in 91% of the first half of the updates, the teacher becomes more similar (in terms of logits distributions) to the student after the meta-update, which indicates that the teacher is learning to adapt to a low-performance student (like an elementary school teacher). However, in the second half of MetaDistil, this percentage drops to 63%. We suspect this is because in the later training stages, the teacher needs to actively evolve itself beyond the student to guide the student towards further improvement (like a university professor).\nFinally, we try to apply a meta-learned teacher to a conventional static distillation and also to an unfamiliar student. We describe the results in details in Section A.3."
    }, {
      "heading" : "5.2 Hyper-parameter Sensitivity",
      "text" : "A motivation of MetaDistil is to enable the teacher to dynamically adjust its knowledge transfer in an optimal way. Similar to Adam (Kingma & Ba, 2015) vs. SGD (Sinha & Griscik, 1971; Kiefer et al., 1952) for optimization, with the ability of dynamic adjusting, it is natural to expect MetaDistil to be more insensitive and robust to changes of the settings. Here, we evaluate the performance of MetaDistil with students of various capability, and a wide variety of hyperparameters, including loss weight and temperature.\nStudent Capability To investigate the performance of MetaDistil under different student capacity, we experiment to distill BERT-Base into\nBERT-6L, Medium, Small, Mini and Tiny (Turc et al., 2019) with conventional KD and MetaDistil. We plot the performance with the student’s parameter number in Figure 2. Additionally, we show results for different compression ratio in Appendix B.\nLoss Weight In KD, tuning the loss weight is non-trivial and often requires hyperparameter search. To test the robustness of MetaDistil under different loss weights, we run experiments with different α (Equation 1). As shown in Figure 3, MetaDistil consistently outperforms conventional KD and is less sensitive to different α.\nTemperature Temperature is a re-scaling trick introduced in Hinton et al. (2015). We try different temperatures and illustrate the performance of KD and MetaDistil in Figure 4. MetaDistil shows better performance and robustness compared to KD."
    }, {
      "heading" : "5.3 Limitation",
      "text" : "Like all meta learning algorithms, MetaDistil inevitably requires two rounds of updates involving both first and second order derivatives. Thus, MetaDistil requires additional computational time and memory than a normal KD method, which can be a limitation of our method. We compare the computational overheads of MetaDistil with other methods in Table 2. Although our approach takes more time to achieve its own peak performance, it can match up the performance of PKD (Sun et al.,\n2019) with a similar time cost. The memory use of our method is higher than PKD and ProKT (Shi et al., 2021). However, this one-off investment can lead to a better student model for inference, thus can be worthy."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, we present MetaDistil, a knowledge distillation algorithm powered by meta learning that explicitly optimizes the teacher network to better transfer its knowledge to the student network. The extensive experiments verify the effectiveness and robustness of MetaDistil. For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation."
    }, {
      "heading" : "A MetaDistil for Image Classification",
      "text" : "In addition to BERT compression, we also provide results on image classification. Also, we conduct experiments of static teaching and cross teaching, to further verify the effectiveness of MetaDistil of adapting to different students.\nA.1 Experimental Settings\nFor CV, following the settings in Tian et al. (2020), we experiment with the image classification task on CIFAR-100 (Krizhevsky et al., 2009) with studentteacher combinations of different capacity and architectures, including ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015). Additionally, we run a distillation experiment between different architectures (a ResNet teacher to a VGG student). We report the top-1 test accuracy of the compressed student networks. We inherit all hyperparameters from Tian et al. (2020) except for the teacher learning rate, which is grid searched from {1e-4, 2e-4, 3e-4}. We randomly split the original training set to a new training set and the quiz set by 9 : 1. We compare our results with a state-of-the-art distillation method, CRD (Tian et al., 2020) and other commonly used knowledge distillation methods (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018) including ProKT (Shi et al., 2021) which has a trainable teacher.\nA.2 Image Recognition Results\nWe show the experimental results of MetaDistil distilling ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) with five different teacher-student pairs. MetaDistil achieves comparable performance to CRD (Tian et al., 2020), the current state-of-the-art distillation method on image classification while outperforming all other baselines with complex features and loss functions. Notably, CRD introduces additional negative sampling and contrastive training while our method achieves comparable performance without using these tricks. Additionally, we observe a substantial performance drop without pilot update, again verifying the importance of this mechanism.\nA.3 Static Teaching and Cross Teaching\nIn MetaDistil, the student is trained in a dynamic manner. To investigate the effect of such a dynamic\ndistillation process, we attempt to use the teacher at the end of MetaDistil training to perform a static conventional KD, to verify the effectiveness of our dynamic distillation strategy. As shown in Table 4, on both experiments, dynamic MetaDistil outperforms conventional KD and static distillation with the teacher at the end of MetaDistil training.\nAs mentioned in Section 3.2, a meta teacher is optimized to transfer its knowledge to a specific student network. To justify this motivation, we conduct experiments using a teacher optimized for the ResNet-32 student to statically distill to the ResNet-20 student, and also in reverse. As shown in Table 4, the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20, respectively. This confirms our motivation that the meta teacher in MetaDistil can adjust itself according to its student."
    }, {
      "heading" : "B Results of Different Compression Ratios",
      "text" : "In this section, we present additional experimental results in settings with different compression ratios to further demonstrate the effectiveness of MetaDistil on bridging the gap between the student and teacher capacity. Specifically, we conduct experiments in the following two settings: (1) distilling BERT-base into a 4-layer BERT (110M→52M) and (2) distilling BERT-large into a 6-layer BERT (345M→66M). The results are shown in Table 4 and Table 5, respectively. We can see that MetaDistil consistently outperforms PKD and ProKT in both settings. This confirms the effectiveness of MetaDistil and also show its ability to adapt the teacher model to the student model, since the gap between teacher and student is even larger in these settings."
    } ],
    "references" : [ {
      "title" : "Variational information distillation for knowledge transfer",
      "author" : [ "Sungsoo Ahn", "Shell Xu Hu", "Andreas C. Damianou", "Neil D. Lawrence", "Zhenwen Dai" ],
      "venue" : null,
      "citeRegEx" : "Ahn et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ahn et al\\.",
      "year" : 2019
    }, {
      "title" : "Online learning rate adaptation with hypergradient descent",
      "author" : [ "Atilim Gunes Baydin", "Robert Cornish", "David MartínezRubio", "Mark Schmidt", "Frank Wood" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Baydin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Baydin et al\\.",
      "year" : 2018
    }, {
      "title" : "Senteval: An evaluation toolkit for universal sentence representations",
      "author" : [ "Alexis Conneau", "Douwe Kiela" ],
      "venue" : "In LREC,",
      "citeRegEx" : "Conneau and Kiela.,? \\Q2018\\E",
      "shortCiteRegEx" : "Conneau and Kiela.",
      "year" : 2018
    }, {
      "title" : "Learner-centered teacherstudent relationships are effective: A meta-analysis",
      "author" : [ "Jeffrey Cornelius-White" ],
      "venue" : "Review of educational research,",
      "citeRegEx" : "Cornelius.White.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cornelius.White.",
      "year" : 2007
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B. Dolan", "Chris Brockett" ],
      "venue" : "In IWP@IJCNLP,",
      "citeRegEx" : "Dolan and Brockett.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine" ],
      "venue" : "In Doina Precup and Yee Whye Teh (eds.),",
      "citeRegEx" : "Finn et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge transfer via distillation of activation boundaries formed by hidden neurons",
      "author" : [ "Byeongho Heo", "Minsik Lee", "Sangdoo Yun", "Jin Young Choi" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Heo et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Heo et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean" ],
      "venue" : "arXiv preprint arXiv:1503.02531,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu" ],
      "venue" : null,
      "citeRegEx" : "Jiao et al\\.,? \\Q1909\\E",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 1909
    }, {
      "title" : "Knowledge distillation via route constrained optimization",
      "author" : [ "Xiao Jin", "Baoyun Peng", "Yichao Wu", "Yu Liu", "Jiaheng Liu", "Ding Liang", "Junjie Yan", "Xiaolin Hu" ],
      "venue" : null,
      "citeRegEx" : "Jin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Stochastic estimation of the maximum of a regression function",
      "author" : [ "Jack Kiefer", "Jacob Wolfowitz" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Kiefer and Wolfowitz,? \\Q1952\\E",
      "shortCiteRegEx" : "Kiefer and Wolfowitz",
      "year" : 1952
    }, {
      "title" : "Paraphrasing complex network: Network compression via factor transfer",
      "author" : [ "Jangho Kim", "Seonguk Park", "Nojun Kwak" ],
      "venue" : "In NeurIPS,",
      "citeRegEx" : "Kim et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Comparing kullbackleibler divergence and mean squared error loss in knowledge distillation",
      "author" : [ "Taehyeon Kim", "Jaehoon Oh", "Nakyil Kim", "Sangwook Cho", "Se-Young Yun" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Kim et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky",
      "year" : 2009
    }, {
      "title" : "Metadistiller: Network selfboosting via meta-learned top-down distillation",
      "author" : [ "Benlin Liu", "Yongming Rao", "Jiwen Lu", "Jie Zhou", "Cho-Jui Hsieh" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "DARTS: differentiable architecture search",
      "author" : [ "Hanxiao Liu", "Karen Simonyan", "Yiming Yang" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "Michael McCloskey", "Neal J Cohen" ],
      "venue" : "In Psychology of learning and motivation,",
      "citeRegEx" : "McCloskey and Cohen.,? \\Q1989\\E",
      "shortCiteRegEx" : "McCloskey and Cohen.",
      "year" : 1989
    }, {
      "title" : "Improved knowledge distillation via teacher assistant",
      "author" : [ "Seyed-Iman Mirzadeh", "Mehrdad Farajtabar", "Ang Li", "Nir Levine", "Akihiro Matsukawa", "Hassan Ghasemzadeh" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Mirzadeh et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mirzadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Meta-kd: A meta knowledge distillation framework for language model compression across domains",
      "author" : [ "Haojie Pan", "Chengyu Wang", "Minghui Qiu", "Yichang Zhang", "Yaliang Li", "Jun Huang" ],
      "venue" : null,
      "citeRegEx" : "Pan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning studentfriendly teacher networks for knowledge distillation",
      "author" : [ "Dae Young Park", "Moon-Hyun Cha", "Changwook Jeong", "Daesin Kim", "Bohyung Han" ],
      "venue" : "arXiv preprint arXiv:2102.07650,",
      "citeRegEx" : "Park et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2021
    }, {
      "title" : "Relational knowledge distillation",
      "author" : [ "Wonpyo Park", "Dongju Kim", "Yan Lu", "Minsu Cho" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Park et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning deep representations with probabilistic knowledge transfer",
      "author" : [ "Nikolaos Passalis", "Anastasios Tefas" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Passalis and Tefas.,? \\Q2018\\E",
      "shortCiteRegEx" : "Passalis and Tefas.",
      "year" : 2018
    }, {
      "title" : "Correlation congruence for knowledge distillation",
      "author" : [ "Baoyun Peng", "Xiao Jin", "Dongsheng Li", "Shunfeng Zhou", "Yichao Wu", "Jiaheng Liu", "Zhaoning Zhang", "Yu Liu" ],
      "venue" : null,
      "citeRegEx" : "Peng et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta pseudo labels",
      "author" : [ "Hieu Pham", "Zihang Dai", "Qizhe Xie", "Minh-Thang Luong", "Quoc V Le" ],
      "venue" : "arXiv preprint arXiv:2003.10580,",
      "citeRegEx" : "Pham et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2020
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Rajpurkar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "In Yoshua Bengio and Yann LeCun (eds.), ICLR,",
      "citeRegEx" : "Romero et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf" ],
      "venue" : null,
      "citeRegEx" : "Sanh et al\\.,? \\Q1910\\E",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 1910
    }, {
      "title" : "Learning from deep model via exploring local targets, 2021",
      "author" : [ "Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li" ],
      "venue" : "URL https://openreview. net/forum?id=5slGDu_bVc6",
      "citeRegEx" : "Shi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2021
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Naresh K. Sinha", "Michael P. Griscik" ],
      "venue" : "IEEE Trans. Syst. Man Cybern.,",
      "citeRegEx" : "Sinha and Griscik.,? \\Q1971\\E",
      "shortCiteRegEx" : "Sinha and Griscik.",
      "year" : 1971
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Patient knowledge distillation for BERT model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu" ],
      "venue" : "In EMNLP-IJCNLP,",
      "citeRegEx" : "Sun et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Contrastive representation distillation",
      "author" : [ "Yonglong Tian", "Dilip Krishnan", "Phillip Isola" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Tian et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Similarity-preserving knowledge distillation",
      "author" : [ "Frederick Tung", "Greg Mori" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Tung and Mori.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tung and Mori.",
      "year" : 2019
    }, {
      "title" : "Well-read students learn better: The impact of student initialization on knowledge distillation",
      "author" : [ "Iulia Turc", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : "arXiv preprint arXiv:1908.08962,",
      "citeRegEx" : "Turc et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Turc et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers",
      "author" : [ "Wenhui Wang", "Hangbo Bao", "Shaohan Huang", "Li Dong", "Furu Wei" ],
      "venue" : "arXiv preprint arXiv:2012.15828,",
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman" ],
      "venue" : "TACL,",
      "citeRegEx" : "Warstadt et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel R. Bowman" ],
      "venue" : "In NAACLHLT,",
      "citeRegEx" : "Williams et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush" ],
      "venue" : "In EMNLP (Demos),",
      "citeRegEx" : "Scao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Student-centered learning in higher education",
      "author" : [ "Gloria Brown Wright" ],
      "venue" : "International Journal of Teaching and Learning in Higher Education,",
      "citeRegEx" : "Wright.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wright.",
      "year" : 2011
    }, {
      "title" : "Bert-of-theseus: Compressing BERT by progressive module replacing",
      "author" : [ "Canwen Xu", "Wangchunshu Zhou", "Tao Ge", "Furu Wei", "Ming Zhou" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Xu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2017
    }, {
      "title" : "Deep mutual learning",
      "author" : [ "Ying Zhang", "Tao Xiang", "Timothy M. Hospedales", "Huchuan Lu" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT loses patience: Fast and robust inference with early exit",
      "author" : [ "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian J. McAuley", "Ke Xu", "Furu Wei" ],
      "venue" : "NeurIPS,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving sequence-to-sequence pre-training via sequence span rewriting",
      "author" : [ "Wangchunshu Zhou", "Tao Ge", "Ke Xu", "Furu Wei" ],
      "venue" : "arXiv preprint arXiv:2101.00416,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 28,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 25,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 0,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 23,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 8,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 13,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 30,
      "context" : ", 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Peng et al., 2019; Ahn et al., 2019; Park et al., 2019; Passalis & Tefas, 2018; Heo et al., 2019; Kim et al., 2018; Shi et al., 2021; Sanh et al., 2019; Jiao et al., 2019; Wang et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 376
    }, {
      "referenceID" : 3,
      "context" : "Recent studies in pedagogy suggest student-centered learning, which considers students’ characteristics and learning capability, has shown effectiveness improving students’ performance (Cornelius-White, 2007; Wright, 2011).",
      "startOffset" : 185,
      "endOffset" : 222
    }, {
      "referenceID" : 43,
      "context" : "Recent studies in pedagogy suggest student-centered learning, which considers students’ characteristics and learning capability, has shown effectiveness improving students’ performance (Cornelius-White, 2007; Wright, 2011).",
      "startOffset" : 185,
      "endOffset" : 222
    }, {
      "referenceID" : 22,
      "context" : "Recent works (Park et al., 2021; Shi et al., 2021) introduce student-aware distillation by jointly training the teacher and the student with task-specific objectives.",
      "startOffset" : 13,
      "endOffset" : 50
    }, {
      "referenceID" : 30,
      "context" : "Recent works (Park et al., 2021; Shi et al., 2021) introduce student-aware distillation by jointly training the teacher and the student with task-specific objectives.",
      "startOffset" : 13,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "To address these two drawbacks, we propose Knowledge Distillation with Meta Learning (MetaDistil), a new teacher-student distillation framework using meta learning (Finn et al., 2017) to exploit feedback about the student’s learning progress to improve the teacher’s knowledge transfer ability throughout the distillation process.",
      "startOffset" : 164,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "On the basis of previous formulations of bi-level optimization based meta learning (Finn et al., 2017), we propose a new mechanism called pilot update that aligns the learning of the bi-level learners (i.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "derivatives and performing gradient descent (Finn et al., 2017).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "Also, our method achieves state-of-the-art performance compressing BERT (Devlin et al., 2019) on the GLUE benchmark (Wang et al.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 38,
      "context" : ", 2019) on the GLUE benchmark (Wang et al., 2019) and shows competitive results compressing ResNet (He et al.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : ", 2019) and shows competitive results compressing ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) on CIFAR-100 (Krizhevsky et al.",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 44,
      "context" : "Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., 2020; Zhou et al., 2020, 2021).",
      "startOffset" : 98,
      "endOffset" : 140
    }, {
      "referenceID" : 28,
      "context" : "Since its introduction, several works (Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Park et al., 2019; Sun et al., 2019; Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer.",
      "startOffset" : 38,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "Since its introduction, several works (Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Park et al., 2019; Sun et al., 2019; Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer.",
      "startOffset" : 38,
      "endOffset" : 163
    }, {
      "referenceID" : 34,
      "context" : "Since its introduction, several works (Romero et al., 2015; Zagoruyko & Komodakis, 2017; Tung & Mori, 2019; Park et al., 2019; Sun et al., 2019; Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer.",
      "startOffset" : 38,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "In the context of knowledge distillation, MetaDistil shares some common ideas with the line of work that utilizes a sequence of intermediate teacher models to make the teacher network better adapt to the capacity of the student model throughout the training process, including teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020) and route constraint optimization (RCO) (Jin et al.",
      "startOffset" : 325,
      "endOffset" : 348
    }, {
      "referenceID" : 11,
      "context" : ", 2020) and route constraint optimization (RCO) (Jin et al., 2019).",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "Successful applications of meta learning include learning better initialization (Finn et al., 2017), architecture search (Liu et al.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : ", 2017), architecture search (Liu et al., 2019), learning to optimize the learning rate schedule (Baydin et al.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : ", 2019), learning to optimize the learning rate schedule (Baydin et al., 2018), and learning to optimize (Andrychowicz et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 26,
      "context" : "For example, meta pseudo labels (Pham et al., 2020) uses meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "In meta learning algorithms that involve a bi-level optimization problem (Finn et al., 2017), there exists an inner-learner fi and a meta-learner fm.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "In the original formulation of meta learning (Finn et al., 2017), the purpose is to learn a good metalearner fm that can generalize to different innerlearners fi for different tasks.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 46,
      "context" : "Different from previous deep mutual learning (Zhang et al., 2018) methods that switch the role between the student and teacher network and train the original teacher model with soft labels generated by the student model or recent works (Shi et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 30,
      "context" : ", 2018) methods that switch the role between the student and teacher network and train the original teacher model with soft labels generated by the student model or recent works (Shi et al., 2021; Park et al., 2021) that",
      "startOffset" : 178,
      "endOffset" : 215
    }, {
      "referenceID" : 22,
      "context" : ", 2018) methods that switch the role between the student and teacher network and train the original teacher model with soft labels generated by the student model or recent works (Shi et al., 2021; Park et al., 2021) that",
      "startOffset" : 178,
      "endOffset" : 215
    }, {
      "referenceID" : 38,
      "context" : "Settings For NLP, we evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 33,
      "context" : "Specifically, we test on MRPC (Dolan & Brockett, 2005), QQP2 and STS-B (Conneau & Kiela, 2018) for Paraphrase Similarity Matching; SST-2 (Socher et al., 2013) for Sentiment Classification; MNLI (Williams et al.",
      "startOffset" : 137,
      "endOffset" : 158
    }, {
      "referenceID" : 41,
      "context" : ", 2013) for Sentiment Classification; MNLI (Williams et al., 2018), QNLI (Rajpurkar et al.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : ", 2018), QNLI (Rajpurkar et al., 2016) and RTE (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 38,
      "context" : ", 2016) and RTE (Wang et al., 2019) for the Natural Language Inference; CoLA (Warstadt et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 40,
      "context" : ", 2019) for the Natural Language Inference; CoLA (Warstadt et al., 2019) for Linguistic Acceptability.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "Following previous studies (Sun et al., 2019; Jiao et al., 2019; Xu et al., 2020), our goal is to distill BERT-Base (Devlin et al.",
      "startOffset" : 27,
      "endOffset" : 81
    }, {
      "referenceID" : 44,
      "context" : "Following previous studies (Sun et al., 2019; Jiao et al., 2019; Xu et al., 2020), our goal is to distill BERT-Base (Devlin et al.",
      "startOffset" : 27,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : ", 2020), our goal is to distill BERT-Base (Devlin et al., 2019) into a 6-layer BERT with the hidden size of 768.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 37,
      "context" : "The student is initialized with a 6-layer pretrained BERT (Turc et al., 2019) thus has a better performance than the original implementation.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 34,
      "context" : "Following previous works (Sun et al., 2019; Turc et al., 2019; Xu et al., 2020), we evaluate MetaDistil in a task-specific setting where the teacher model is fine-tuned on a downstream task and the student model is trained on the task with the KD loss.",
      "startOffset" : 25,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : "Following previous works (Sun et al., 2019; Turc et al., 2019; Xu et al., 2020), we evaluate MetaDistil in a task-specific setting where the teacher model is fine-tuned on a downstream task and the student model is trained on the task with the KD loss.",
      "startOffset" : 25,
      "endOffset" : 79
    }, {
      "referenceID" : 44,
      "context" : "Following previous works (Sun et al., 2019; Turc et al., 2019; Xu et al., 2020), we evaluate MetaDistil in a task-specific setting where the teacher model is fine-tuned on a downstream task and the student model is trained on the task with the KD loss.",
      "startOffset" : 25,
      "endOffset" : 79
    }, {
      "referenceID" : 34,
      "context" : "Baselines For comparison, we report the results of vanilla KD and patient knowledge distillation (Sun et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 44,
      "context" : "We also include the results of progressive module replacing (Xu et al., 2020), a state-of-the-art task-specific compression method for BERT which also uses a larger teacher model to improve smaller ones like knowledge distillation.",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 46,
      "context" : "We also compare against deep mutual learning (DML) (Zhang et al., 2018), teacher assistant knowledge distillation (TAKD) (Mirzadeh et al.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : ", 2018), teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020), route con-",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "straint optimization (RCO) (Jin et al., 2019), proximal knowledge teaching (ProKT) (Shi et al.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : ", 2019), proximal knowledge teaching (ProKT) (Shi et al., 2021), and student-friendly teacher network (SFTN) (Park et al.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 22,
      "context" : ", 2021), and student-friendly teacher network (SFTN) (Park et al., 2021), where the teacher network is not fixed.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "Note that among these baselines, PKD (Sun et al., 2019) and Theseus (Xu et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 44,
      "context" : ", 2019) and Theseus (Xu et al., 2020) exploit intermediate features while TinyBERT and the MiniLM family use both intermediate and Transformer-specific features.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 38,
      "context" : "We report the experimental results on both the development set and test set of the eight GLUE tasks (Wang et al., 2019) in Table 1.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 46,
      "context" : "Compared with other methods with a trainable teacher (Zhang et al., 2018; Mirzadeh et al., 2020; Jin et al., 2019; Shi et al., 2021), our method still demonstrates superior performance.",
      "startOffset" : 53,
      "endOffset" : 132
    }, {
      "referenceID" : 20,
      "context" : "Compared with other methods with a trainable teacher (Zhang et al., 2018; Mirzadeh et al., 2020; Jin et al., 2019; Shi et al., 2021), our method still demonstrates superior performance.",
      "startOffset" : 53,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "Compared with other methods with a trainable teacher (Zhang et al., 2018; Mirzadeh et al., 2020; Jin et al., 2019; Shi et al., 2021), our method still demonstrates superior performance.",
      "startOffset" : 53,
      "endOffset" : 132
    }, {
      "referenceID" : 30,
      "context" : "Compared with other methods with a trainable teacher (Zhang et al., 2018; Mirzadeh et al., 2020; Jin et al., 2019; Shi et al., 2021), our method still demonstrates superior performance.",
      "startOffset" : 53,
      "endOffset" : 132
    }, {
      "referenceID" : 37,
      "context" : "BERT-6L, Medium, Small, Mini and Tiny (Turc et al., 2019) with conventional KD and MetaDistil.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 30,
      "context" : "The memory use of our method is higher than PKD and ProKT (Shi et al., 2021).",
      "startOffset" : 58,
      "endOffset" : 76
    } ],
    "year" : 0,
    "abstractText" : "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.1",
    "creator" : null
  }
}
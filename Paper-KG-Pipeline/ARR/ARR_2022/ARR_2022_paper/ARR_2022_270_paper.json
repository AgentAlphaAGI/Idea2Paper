{
  "name" : "ARR_2022_270_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "You Don’t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers’ Private Personas",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Social chatbots have been widely used to benefit many applications from answering factual questions to showing emotional companionship. With recent progress in large pretrained language models (Radford et al., 2019; Yang et al., 2019), some attempts (Wolf et al., 2019; Zhang et al., 2020; Ham et al., 2020; Shen et al., 2021; Sevegnani et al., 2021; Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs). To train such LM-based chatbots, private conversations are collected. Unfortunately, large language models tend to memorize training data and some private data can be recovered from models via black-box training data extraction attacks (Carlini et al., 2021). Besides such memorization problems, “overlearning” on simple training objectives can reveal sensitive attributes indirectly related to the learning task (Song and Shmatikov,\n2020). LM-based social chatbots essentially inherit the privacy issues of general LMs and the overlearning problem.\nFor example, as Figure 1 shows, when using a fine-tuned GPT-2 as the encoder and decoder of an LM-based social chatbot, if the learned representation of each utterance can be obtained by an adversary, then the adversary can build a classifier to predict the persona information based on the representation. As shown by the example, for five out of 14 utterances, the attacker can successfully predict the persona, which can be harmful if the users (speakers of the utterances) do not prefer to reveal the persona information. Thus, in practice, when deploying such kinds of chatbots in real applications, we should first make sure that no private information can be leaked by the models.\nTo systematically study the privacy issues in LMbased social chatbots, there are several challenges. First, there is no existing data that can be used to quantify how much private information is revealed by an LM. Second, there has been no existing work showing how to attack utterance-level representations to obtain sensitive information. Third, there has been no existing LM-based chatbot that can defend against persona inference attacks, and no study shows how to protect both known and unknown persona attributes.\nIn this paper, to address the above challenges, we use the fine-tuned GPT-2 as our chatbot. We first collect a dataset by aligning personas with corresponding utterances in PersonaChat dataset (Zhang et al., 2018). Then we show that “overlearning” can happen for LM-based chatbots to reveal personas of speakers. We build a single external multi-layer perception (MLP) attacker model to perform black-box persona inference attacks on the utterance-level embeddings. With no access to parameters of the chatbot, the attacker model can infer speakers’ personas with 37.59% accuracy over 4,332 personas. The high accuracy of\nthe attacker model implies that the utterance-level embeddings have potential vulnerabilities to reveal speakers’ private persona attributes. Thus, it is necessary to improve training algorithms to address such overlearning issues. Finally, we apply defense learning strategies on the GPT-2 to prevent such black-box attacks. We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2018) as additional defense objectives to train the GPT-2 and decrease the attacker’s persona inference accuracy to 0.53%. Our contributions can be summarized as follows:\n1): To the best of our knowledge, we are the first to disclose and analyze the persona inference attack for LM-based chatbots and treat it as a privacy risk.\n2): We propose an effective defensive training algorithm to prevent dialog representations from leaking personas of the corresponding speakers by uniform distribution approximation and mutual information minimization.\n3): We conduct extensive experiments to quantify both privacy and utility of proposed defense mechanisms. Besides solving the persona leakage issue, the proposed training algorithm has nearly no negative influence on utility."
    }, {
      "heading" : "2 Related Work",
      "text" : "Language models trained on private data suffer privacy risks of revealing sensitive information. Previous researches mainly considered black-box at-\ntacks that assumed attackers only had access to inputs and outputs of language models. Carlini et al. (2021) performed black-box model inversion attack on GPT-2 through descriptive prompts with beam search. Lehman et al. (2021) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information. Furthermore, given black-box access to a language model’s pre-train and finetune stages, Zanella-Béguelin et al. (2020) showed that sensitive sequences of the fine-tuning dataset can be extracted. For the distributed client-server setup, Malekzadeh et al. (2021) considered the sensitive attribute leakage from the server side with honest-but-curious (HBC) classifiers.\nWhat is worse, for an LM-based chatbot, its training conversations are prone to include more private attributes than other commonly-used corpora for language modeling like BooksCorpus (Zhu et al., 2015) and Wikipedia. Tigunova et al. (2019) proposed Hidden Attribute Model (HAM) to extract professions and genders of speakers from various dialog datasets. Wu et al. (2020) further applied Attribute Extractor to generate speakers’ attribute triplets flexibly and suggested downstream tasks based on the triplets. Pan et al. (2020) exploited embeddings of language models to recover inputs’ digits and keywords. Though the setup of this work is similar to ours, they merely considered simple cases of data recovery with given rules and suffered\ngreat utility degradation to obtain optimal defense performance. For our work, there is no fixed pattern or rule for the model input. Instead of finding keywords or recovering digits, we aim to infer more complicated private attributes from such embeddings. Moreover, our proposed defenses have almost no influence on the utility."
    }, {
      "heading" : "3 Attacking on Casual Language Model",
      "text" : "In this section, we illustrate black-box persona inference attacks on GPT-2 and our defense strategies. In Section 3.1, we first give the problem formulation. Then we describe the attack in Section 3.2. Lastly, we comprehensively explain our proposed defense strategies in Section 3.3."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "We assume that there is a GPT-2 based chatbot f pretrained on private conversations D. Only casual language modeling is used to train the chatbot:\nLf (u; θf ) = − |u|∑ i=1 log(Pr(wi|c, w0, w1, ..., wi−1)), (1)\nwhere f refers to the LM-based chatbot with given utterance u = {w0, w1, ..., w|u|−1} and previous context c. An adversary owns one external annotated dialog dataset Da = {(U1, s1), (U2, s2), ..., (Un, sn)} with n conversations where Ui indicates a list of utterances {ui1, ui2, ..., uini} of i-th conversation and si corresponds to a list of sensitive personas {si1, si2, ..., sini} for corresponding utterance. Each persona skj is an integer that can be mapped to its persona according to a predefined dictionary and 0 ≤ skj ≤ C − 1 where C is the total number of predefined persona attributes. The goal of the adversary is to infer speakers’ personas s from utterances’ embeddings f(u) where u and s refer to any utterance and its persona label."
    }, {
      "heading" : "3.2 Black-box Persona Inference Attack",
      "text" : "The persona inference attack can be viewed as a supervised classification task. For the black-box attack setup, the adversary can only query the target dialog model f with access to embeddings of adversary’s inputs and cannot access or modify model parameters θf . As shown in the left part of Figure 2, the adversary tries to build its attacker model A with its external data Da and dialog model f . The persona predictor’s output A(f(u)) is the estimated probability distribution over C persona attributes. Its loss function LA exploits cross-entropy\nbetween the predicted distribution and ground truth distribution that can be formulated as:\nLA(ukj , skj ; θA) = CE(A(f(ukj)), skj), (2)\nwhere CE refers to cross-entropy loss between persona label skj and A(f(ukj)).\nA well-performed persona predictorA can cause great privacy threats. For Machine Learning as a Service (MLaaS), A can be directly applied to perform a man-in-the-middle attack on the application programming interfaces. Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker model A to collect personas of service users."
    }, {
      "heading" : "3.3 Defense Learning Strategies",
      "text" : "The simple LM training objective only considers the utility of chatbots. In later experiment sections, we show that LM brings severe overlearning issues. To avoid black-box persona inference attacks, more constraints on privacy should be added for training LM-based chatbots. Ideally, to achieve an optimal privacy-preserving chatbot against persona inference attacks, the probability distribution of the attacker model A should be close to the uniform distribution. That is, the adversary cannot improve its inference accuracy from posterior estimation A(f(u)) and the accuracy is no better than making random guesses on the persona attributes. Moreover, the constraints on privacy should have minor degradation on the utility to maintain the strong generation ability of chatbots.\nFollowing the intuition that the adversary cannot obtain better results than a random guess, we propose KL loss that aims to flatten the persona predictor’s estimated distribution. Based on minimizing the mutual information between hidden states f(u) of chatbots and private persona attributes s, we propose MI loss."
    }, {
      "heading" : "3.3.1 KL Loss",
      "text" : "KL loss aims to minimize the Kullback–Leibler divergence between A(f(u)) and the uniform distribution. It flattens the distribution of A(f(u)) so that the adversary cannot gain any useful knowledge after training attacker model A. The KL divergence between the uniform distribution and A(f(u)) can be formulated as:\nDKL(UNI||A(f(u))) = − 1\nC C−1∑ k=0 log(CPr(k|f(u), θA)),\n(3)\nwhere UNI indicates the uniform distribution and k indicates the k-th persona label. For optimization, we can leave out constant terms (Mireshghallah et al., 2021) and obtain the following loss function:\nLD(u; θA) = − 1\nC C−1∑ k=0 Pr(k|f(u), θA). (4)\nHowever, from the perspective of defenders, they have no access to attacker model A and its parameters. Instead, they can build their own persona predictor as a fake attacker. More specifically, they may mimic the adversary to annotate a dataset D′a and a persona predictor Ap. Then the KL loss becomes:\nLkl(u; θAp , θf ) = − 1\nC C−1∑ k=0 Pr(k|f(u), θAp), (5)\nwhere parameters of the chatbot θf and the fake attacker θAp are updated via KL loss. The intuition is to train the chatbot together with a fake attacker to prevent model overlearning by flattening the attacker model’s distribution."
    }, {
      "heading" : "3.3.2 MI Loss",
      "text" : "The privacy constraint requires that hidden representations should not reveal the persona attributes. In other words, given any utterance u and persona s behind the utterance u, we want to minimize the mutual information between f(u) and s:\nmin θf I(f(u); s). (6)\nFollowing the derivation in (Song et al., 2018) and (Li et al., 2020), the upper bound can be formulated as:\nI(f(u); s) ≤ Eq(f(u)) DKL(q(s|f(u))||p(s)), (7)\nwhere p(s) can be any distribution for s, q(x) refers to probability distribution of model f parameterized by θf and f(u) is assumed to be sampled from the conditional distribution q(f(u)|x, s). However, q(s|f(u)) is hard to estimate. Instead, we use pΨ(s|f(u)) to approximate q(s|f(u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2018):\nEq(f(u)) DKL(q(s|f(u))||p(s)) ≥ Eq(f(u))[log pΨ(s|f(u))− log p(s)].\n(8)\nTherefore, our objective in Equation 6 can be formulated as an adversarial training objective:\nmin θf max Ψ\nEq(f(u))[log pΨ(s|f(u))− log p(s)]. (9)\nlog p(s) is independent of f(u), and we may leave this term out in Equation 9:\nmin θf max Ψ\nEq(f(u))[log pΨ(s|f(u))]. (10)\nThen, Equation 10 illustrates an adversarial game between an adversary pΨ who manages to infer s from f(u) and a defender who modifies θf to protect s from persona inference attack. Adversarial training is widely used to protect sensitive\nfeatures in natural language processing (Elazar and Goldberg, 2018; Coavoux et al., 2018; Li et al., 2018). Using the persona predictor model Ap with softmax activation to learn pΨ, we obtain the final objective for the defender:\nmin θAp max θf\nCE(Ap(f(u)), s). (11)\nWe can rewrite Equation 11 into two losses: Lmi1(ukj , skj ; θAp) = CE(Ap(f(ukj)), skj) and Lmi2(ukj , skj ; θf ) = −CE(Ap(f(ukj)), skj) for the fake adversary and the chatbot respectively. Then our MI loss can be formulated as:\nLmi = λ0Lmi1 + Lmi2, (12)\nwhere λ0 controls the ratio between two the fake attacker Ap and the defensed chatbot f ."
    }, {
      "heading" : "3.3.3 Overall Loss",
      "text" : "The right part of Figure 2 illustrates how the chatbot is trained to address the black-box attack. The loss function for the defender combines KL loss, MI loss and LM loss. Notice that the fake adversary objective in MI loss violates KL loss which tries to make the distribution of Ap flatten. Our proposed loss assigns more weights to the KL loss:\nL = Lf + λ1Lkl + λ2Lmi, (13)\nwhere λ1 and λ2 are hyper-parameters and λ1 ≥ 10λ2 to flatten the distribution of Ap. Though the chatbot trained with overall loss L still cannot interfere training process ofA during black-box attacks, L aims to mitigate persona overlearning issues of f to address such persona inference attacks."
    }, {
      "heading" : "4 Experiment",
      "text" : "In this section, we conduct experiments to evaluate the performance of privacy and utility for the proposed defense learning strategies. In Section 4.1, we give our experimental setting in detail. In Section 4.2, we show the attacking performance with and without defense. In Section 4.3, we perform ablation study on defense objectives. In Section 4.4, we use automatic metrics to evaluate chatbots’ utility. We conduct various attack setups in Section 4.5 and perform a case study in Section 4.6."
    }, {
      "heading" : "4.1 Experimental Setting",
      "text" : "Dataset. To train the GPT-2 as our chatbot, we use the DialoGPT (Zhang et al., 2020) pretrained on Reddit comment chains. Then we use PersonaChat\ndataset (Zhang et al., 2018) to fine-tune the GPT2. To obtain annotated dataset Da for the adversary, we align personas to corresponding utterances through positive (utterance,persona) pairs provided in Dialogue NLI (Welleck et al., 2019) dataset. For those utterances with no annotations, we assign label −1 to them. We reshuffle the dataset to balance the label distribution among train/val/test datasets with the ratio of 8 : 1 : 1. We first let the attacker and defender share the same training data. In later sections, we will separate the annotated data for the adversary and defender with no overlap. A summary statistics of Da is shown in Table 1.\nAttacker model. In our experiment, we use a 2-layer neural network with cross-entropy loss as the attacker model. The attacker model exploits the final layer embedding of the last token “<|endoftext|>” from the GPT-2 as model input. We also try other attacker model architectures (transformer block based attackers) and input embeddings (average of all embeddings in the final layer of GPT-2), but the attacking performance is worse than the 2-layer model mentioned above.\nEvaluation Metrics. The evaluation metrics are based on privacy and utility. For privacy, we use persona inference accuracy and weighted F1score to evaluate the attacker’s performance. We also use Bayesian Privacy (BP) (Gu et al., 2021a) to quantify the attacker’s privacy loss for the estimated persona distribution. Top-k accuracy is reported in the Appendix. For utility, we apply BERTScore (Zhang* et al., 2020), Distinct (Li et al., 2016), BLEU (Papineni et al., 2002) and perplexity (PPL) as evaluation metrics. BERTScore and BLEU measure similarity between generated outputs and ground truth while Distinct (Dist) focuses on diversity. Perplexity shows the uncertainty when the LM model fits the data."
    }, {
      "heading" : "4.2 Privacy",
      "text" : "Attacks without Defense. We list the attacking performance of A in multiple scenarios shown in\nTable 2. To demonstrate the overlearning issue of GPT-2, we consider 2 baseline attacks. If the adversary has no knowledge about persona attributes distribution, then he can randomly guess over 4,332 labels (Random Pred). Otherwise the adversary can perform Best Guess that only guesses the most frequent persona in the dataset. LM indicates the attacker performance that only language modeling objective is applied to train the chatbot without any defense mechanism. From the table, the test persona inference accuracy on the LM achieves 37.59% while guessing on the label with most occurrences merely has 0.72% accuracy. That is, the black-box persona inference attack has 52× the accuracy of guessing. The huge performance gap between the attacker model and the baseline guess method indicates that simple language modeling objective has serious overlearning issues that unintentionally capture private personas of speakers.\nAttacks on the Defensed LM. To avoid the persona overlearning issue, we use additional defense objectives illustrated in Section 3.3. LM+KL+MI utilizes language modeling, KL loss and MI loss in Equation 13 to train the GPT-2. As demonstrated in Table 2, the attacker performance on LM+KL+MI significantly reduces the attacking accuracy from 37.59% to 0.53% and F1-score drops from 0.37 to nearly 0. This defense mechanism can even outperform Best Guess in terms of privacy protection. That is, even if the adversary annotates its own dataset to train an attacker model, the attacking performance is still worse than simply guessing the most frequent label. As a result, the black-box persona prediction attack becomes useless after applying the defenses for the chatbot. The adversary cannot obtain any speaker’s persona from the embedding f(u) by training A.\nTo learn why the proposed defenses work so\nwell, we further examine the ratio of the most frequent predicted label (Max-Ratio) among all predictions. The accuracy of Best Guess reveals that the most frequent label in the test set has a ratio of 0.72%. After applying KL loss and MI loss, the attacker model tends to make predictions on a single label. For LM+KL+MI, the Max-Ratio even occupies 81.87% predictions. This implies that the proposed defense strategies may have the potential to fool the attacker model to make wrong predictions on a single slot. We will further investigate this implication in later sections.\nOverall, the above experiment demonstrates that our proposed defense learning strategies can effectively mitigate the persona overlearning issue and avoid black-box persona inference attacks."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "To show the effectiveness of proposed KL loss and MI loss and how they affect the performance of black-box persona inference attacks, we consider the inclusion and exclusion of proposed defense objectives. The result is shown in Table 2. LM+KL indicates the GPT-2 is trained with language modeling and KL loss. LM+MI applies language modeling and MI loss. From the table, it can be seen that LM+KL, LM+MI and LM+KL+MI are all able to reduce the test accuracy of the attacks. The KL loss is weaker from the perspective of defense, but it tends to flatten the estimated persona distribution with much smaller Max-Ratio. The LM+MI shares similar test accuracy and F1-score with LM+KL+MI, but nearly all predictions are made on a single persona label with a ratio of 99.84%. This suggests that MI loss causes the attacker model to predict all labels on a single persona attribute. After KL loss is applied on LM+KL+MI, the Max-Ratio drops to 81.87%. As discussed earlier, high MaxRatio may also cause privacy leakage. Suppose the adversary knows the persona with Max-Ratio, then he can improve his guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification). These results verify that KL loss introduces flatter estimation and MI loss is more effective against persona overlearning, which conforms to our intuition of their objectives in Section 3.3."
    }, {
      "heading" : "4.4 Utility",
      "text" : "Besides privacy, utility is another key objective to train a chatbot. Several automatic metrics are considered to evaluate the generation performance. For\ngeneration, we use GPT-2 to generate responses of the second speaker (Human B in Figure 1) with all previous turns as context. Then we compared the generated model outputs with ground truth replies. We use Dist-1 and Dist-2 to count ratios of distinct unigrams and bigrams. BLEU-1, BLEU-2 and BLEU-4 are applied to evaluate generation similarity with ground truth. Due to the one-to-many nature of chit-chats, the BLEU is not adequate to compare generated responses with ground truth. Hence, we adapt Precision, Recall and Precision of BERTScore to measure the similarity in the embedding space.\nThe evaluation result is shown in Table 3, where same models from Table 2 are evaluated. The result indicates that adding KL loss will increase the perplexity greatly from 14.8 to 28.9. After combining KL loss with MI loss, its perplexity decreases to 19.674. A plausible explanation is that KL loss confuses the persona predictor and indirectly increases the uncertainty of the GPT-2. All GPT-2 models have relatively low BLEU scores due to the one-to-many mapping between contexts and responses. For Distinct and BERTScore, there are only minor differences between LM and defensed LMs. Though the uncertainty increases after applying KL loss and MI loss, it does no harm to the quality of generation. In summary, there is almost no negative influence on the utility after applying the proposed defense strategies."
    }, {
      "heading" : "4.5 More Setups on Attacks",
      "text" : "Attacks on Imbalanced Data Distribution. Previous black-box attacks usually assume that the annotated dataset Da must share similar data distri-\nbution with the defender’s training data. To examine the performance of defense strategies on unseen personas, we assign the adversary’s datasetDa with labels that the defender cannot acquire. We split data with 500 persona labels that are uniquely held by the adversary. The defender owns 8,031 conversations with persona labels ranging from 500 to 4,331 while the adversary holds 2,376 dialogues with persona labels ranging from 0 to 4,331. For testing, 500 conversations with persona labels ranging from 0 to 4,331 are used.\nUnder imbalanced data distribution, the attack on the defensed LM has Acc 0.47%, F1 1.90e-3 and Max-Ratio 94.06%. The persona inference accuracy is still very low and the attacker model tends to predict more on a single persona label than the balanced data distribution setup. This result shows that the proposed overall loss can also prevent black-box persona inference attacks on unseen personas. It also verifies previous suggestions that combining LM loss with MI loss may fool the attacker model to make wrong predictions.\nAttacks on Fewer Persona Labels. The above experiments are based on 4,332 persona labels. In fact, many personas share similar meanings and can be further clustered. Besides, to better evaluate privacy loss for the estimated distribution, a smaller label space is preferred. Therefore, it is necessary to consider defense performance on a smaller label space. We use Sentence-BERT (Reimers and Gurevych, 2020) to embed all persona sentences and perform k-means clustering on the embeddings to obtain 8 clusters. We manually checked these clusters and classified them as cars, food, animals (pets), family information, hobbies, jobs, personal\ninformation and music tastes respectively. To evaluate how the clustering performs, we randomly sample 100 utterances with clustered labels and invite two volunteers to inspect those samples. Both of them agree on 90% of the clustered annotations. After manual inspection of the remaining 10% annotations, the clustering error rate is 8%. Following previous imbalanced data split, we assign data in the first 3 clusters only to the adversary to make the data distribution imbalanced. Here, the defender owns 6,654 conversations with persona labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7. For testing, 500 conversations with persona labels ranging from 0 to 7 are used.\nThe attacking performance for both unseen labels and all labels is displayed in Table 4. BPu measures the KL divergence DKL(F0||A(f(u))) where F0 refers to uniform distribution. For imbalanced data distribution with a small label space, our proposed defenses can still achieve much lower attack accuracy than LM on both Unseen and Overall. However, for Overall, LM+KL+MI has higher accuracy with a lower F1-score compared with two baselines. This indicates that proposed defenses fail to protect privacy as we desired in the baselines. For BPu, LM+KL+MI are around 10 times smaller than LM. It means that after applying defense objectives, the attacker’s estimated distribution is much closer to the uniform distribution. Thus the effectiveness of the KL loss is verified. In addition, Max-Ratio with 8 clusters on Unseen is smaller than 4,332 labels even though the distribution of 8 clusters is obviously tighter. Still, the Max-Ratio of 58.15% accounts for a much larger fraction than other predictions. In summary, the above results imply that for the smaller label space, our proposed defense objectives are still effective even on unseen\npersona labels."
    }, {
      "heading" : "4.6 Case Study",
      "text" : "In Figure 3, we give an example of the persona inference attack, where conversations are generated between the chatbot and the user with the given context. We manually mark True/False on the predicted results. As shown in the figure, there are several successful attacks on LM and no correct prediction on the defensed LM. For attacks on LM, speakers’ hobbies and jobs can be inferred. For incorrect predictions, the attacker model can still predict context-aware personas. After applying proposed defense learning strategies, the predicted personas become irrelevant with context and mostly predict “My favorite color is blue.” In fact, it is the most frequent prediction for LM+KL+MI over 4,332 persona labels. This attack example illustrates that our defense objectives can prevent the black-box persona inference attack from inferring relevant personas."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we show that LM-based chatbots tend to reveal personas of speakers and propose effective defense objectives to prevent GPT-2 from overlearning. Unlike other works that suffer from utility degradation, our defense learning strategies do no harm to the powerful generation ability of LM-based chatbots. We conduct extensive experiments to evaluate both privacy and utility. We perform black-box persona inference attacks under various setups to demonstrate the robustness of proposed defense learning strategies. In addition, we use automatic metrics to show that proposed defense learning strategies maintain the utility. For future work, we suggest working on flattening the distributions of attacker models."
    }, {
      "heading" : "6 Ethical Considerations",
      "text" : "We declare that all authors of this paper acknowledge the ACM Code of Ethics and honor the code of conduct. This work essentially considers blackbox attacks on the private persona attributes and proposes effective learning strategies to prevent chatbots from overlearning private personas.\nDataset. During our dataset collection, all the conversations and personas are collected from publicly available datasets including PersonaChat and DNLI. All the speakers are anonymized and no identifiable personal information is included.\nModel. For training our LM-based chatbots, we follow standard methods. We are well aware of the bias issue inside current language models. In the future, if there are other fairer language models, we will extend our defenses on them."
    }, {
      "heading" : "A Training details.",
      "text" : "For each conversation, the utterances are concatenated by the special token “<|endoftext|>” to train the GPT-2. To decode outputs from GPT-2, we apply the Nucleus Sampling (Holtzman et al., 2020) method. We set top-p = 0.9 with a temperature coefficient 0.9 to sample words from the GPT-2. For optimization, we set 2 AdamW optimizers (Loshchilov and Hutter, 2019) for the chatbot and the persona predictor respectively. The learning rate is 3e-5 with linear warm-up and decay. For hyper-parameters, we set λ0 = 1, λ1 = 10 and λ2 = 1."
    }, {
      "heading" : "B Comparison of Internal Distribution between A and Ap",
      "text" : "To make predictions on personas, the argmax function is used for the estimated distribution of persona predictors. However, the internal distribution conveys crucial information about how the persona predictors estimate f(u). We follow the setup of imbalanced data split of 8 clusters in Section 4.5 to examine persona predictors of attacker A and fake attacker Ap.\nFigure 4 shows the data distribution of the test set and average distribution after softmax activation over the 8 labels for attacker A and defender Ap. For attacker A, we consider the attack on LM and LM+KL+MI. The defenderAp tends to have a large difference with Data and tries to flatten its distribution among its own training set (the last 5 labels). This behavior conforms to the KL loss’s objective that aims to flatten the distribution and deviate from the ground truth distribution. For attacker A, distributions of both LM and LM+KL+MI seem close to the ground truth distribution. This indicates that the attacker model A can still learn statistical in-\nformation about personas. However, its attacking performance is poor. The poor performance implies our proposed defense learning strategies may obfuscate Attacker for estimating single sample f(u) and finally make the wrong prediction."
    }, {
      "heading" : "C More on Case Study",
      "text" : "C.1 Example of Generation\nTo show an intuition view on utility, we provide one generation sample shown in Figure 5. Both LM and LM+KL+MI are able to generate fluent and proper relies. Moreover, they tend to maintain coherence with previous contexts. For example, it is mentioned in the context that Human B is a vegan and both chatbots respond that they do not eat meat for the food preference. This generation example shows that proposed defense learning objectives preserve the model utility.\nC.2 More Examples of Persona Inference\nHere, we give two more examples of the persona inference attacks in Table 6. The first example shows one successful defense. For the second example, both attackers with and without defense fail to predict the ground truth persona. Still, we can see that\nLM+KL+MI predicts personas that are irrelevant to the context. However, LM’s output “I know how to play the guitar.” is much closer to the context about music and instruments. Without any defense, the above examples show that the attacker model can still predict context-aware personas even if its predictions are wrong. After applying the proposed defenses, the attacker model cannot predict meaningful personas relevant to the context."
    }, {
      "heading" : "D Evaluation on Top-k Accuracy",
      "text" : "Previous experiments mainly consider accuracy as the evaluation metric. In this section, we use top-k accuracy for the black-box persona inference attacks to measure privacy protection. As shown in\nTable 5, our defense is much more robust than LM when k ≤ 50. When k is larger than 500, the defense degrades rapidly as k increases. This result implies that the ground truth personas mostly lie in the top 2,000 predictions even if the defense is applied. For a smaller k, our proposed defense learning strategies are still effective."
    } ],
    "references" : [ {
      "title" : "Extracting training data from large language",
      "author" : [ "Nicholas Carlini", "Florian Tramer", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom Brown", "Dawn Song", "Ulfar Erlingsson", "Alina Oprea", "Colin Raffel" ],
      "venue" : null,
      "citeRegEx" : "Carlini et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Carlini et al\\.",
      "year" : 2021
    }, {
      "title" : "Privacy-preserving neural representations of text",
      "author" : [ "Maximin Coavoux", "Shashi Narayan", "Shay B. Cohen." ],
      "venue" : "Proceedings of EMNLP 2018, pages 1–10.",
      "citeRegEx" : "Coavoux et al\\.,? 2018",
      "shortCiteRegEx" : "Coavoux et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial removal of demographic attributes from text data",
      "author" : [ "Yanai Elazar", "Yoav Goldberg." ],
      "venue" : "Proceedings of EMNLP 2018, pages 11–21.",
      "citeRegEx" : "Elazar and Goldberg.,? 2018",
      "shortCiteRegEx" : "Elazar and Goldberg.",
      "year" : 2018
    }, {
      "title" : "Federated deep learning with bayesian privacy",
      "author" : [ "Hanlin Gu", "Lixin Fan", "Bowen Li", "Yan Kang", "Yuan Yao", "Qiang Yang." ],
      "venue" : "arXiv preprint arXiv:2109.13012.",
      "citeRegEx" : "Gu et al\\.,? 2021a",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2021
    }, {
      "title" : "PRAL: A tailored pre-training model for task-oriented dialog generation",
      "author" : [ "Jing Gu", "Qingyang Wu", "Chongruo Wu", "Weiyan Shi", "Zhou Yu." ],
      "venue" : "Proceedings of ACL 2021, pages 305–313.",
      "citeRegEx" : "Gu et al\\.,? 2021b",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2021
    }, {
      "title" : "End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2",
      "author" : [ "Donghoon Ham", "Jeong-Gwan Lee", "Youngsoo Jang", "Kee-Eung Kim." ],
      "venue" : "Proceedings of ACL 2020, pages 583– 592.",
      "citeRegEx" : "Ham et al\\.,? 2020",
      "shortCiteRegEx" : "Ham et al\\.",
      "year" : 2020
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "Proceedings of ICLR 2020.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Does BERT pretrained on clinical notes reveal sensitive data",
      "author" : [ "Eric Lehman", "Sarthak Jain", "Karl Pichotta", "Yoav Goldberg", "Byron Wallace" ],
      "venue" : "In Proceedings of NAACL",
      "citeRegEx" : "Lehman et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lehman et al\\.",
      "year" : 2021
    }, {
      "title" : "Tiprdc: Task-independent privacy-respecting data crowdsourcing framework for deep learning with anonymized intermediate representations",
      "author" : [ "Ang Li", "Yixiao Duan", "Huanrui Yang", "Yiran Chen", "Jianlei Yang." ],
      "venue" : "Proceedings of the ACM SIGKDD",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of NAACL 2016, pages 110–119.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards robust and privacy-preserving text representations",
      "author" : [ "Yitong Li", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Proceedings of ACL 2018 (Volume 2: Short Papers), pages 25–30.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "Proceedings of ICLR 2019.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Honest-but-curious nets: Sensitive attributes of private inputs can be secretly coded into the classifiers’ outputs",
      "author" : [ "Mohammad Malekzadeh", "Anastasia Borovykh", "Deniz Gündüz." ],
      "venue" : "Proceedings of the ACM CCS 2021.",
      "citeRegEx" : "Malekzadeh et al\\.,? 2021",
      "shortCiteRegEx" : "Malekzadeh et al\\.",
      "year" : 2021
    }, {
      "title" : "Privacy regularization: Joint privacy-utility optimization in LanguageModels",
      "author" : [ "Fatemehsadat Mireshghallah", "Huseyin Inan", "Marcello Hasegawa", "Victor Rühle", "Taylor Berg-Kirkpatrick", "Robert Sim." ],
      "venue" : "Proceedings of the NAACL 2021, pages 3799–3807.",
      "citeRegEx" : "Mireshghallah et al\\.,? 2021",
      "shortCiteRegEx" : "Mireshghallah et al\\.",
      "year" : 2021
    }, {
      "title" : "Privacy risks of general-purpose language models",
      "author" : [ "Xudong Pan", "Mi Zhang", "Shouling Ji", "Min Yang." ],
      "venue" : "Proceedings of 2020 IEEE Symposium on Security and Privacy (SP), pages 1314–1331.",
      "citeRegEx" : "Pan et al\\.,? 2020",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2020
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of ACL 2002, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of EMNLP 2020, pages 4512–4525.",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "OTTers: One-turn topic transitions for open-domain dialogue",
      "author" : [ "Karin Sevegnani", "David M. Howcroft", "Ioannis Konstas", "Verena Rieser." ],
      "venue" : "Proceedings of ACL 2021, pages 2492–2504.",
      "citeRegEx" : "Sevegnani et al\\.,? 2021",
      "shortCiteRegEx" : "Sevegnani et al\\.",
      "year" : 2021
    }, {
      "title" : "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "author" : [ "Weizhou Shen", "Junqing Chen", "Xiaojun Quan", "Zhixian Xie." ],
      "venue" : "Proceedings of the AAAI 2021, volume 35, pages 13789– 13797.",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Overlearning reveals sensitive attributes",
      "author" : [ "Congzheng Song", "Vitaly Shmatikov." ],
      "venue" : "Proceedings of ICLR 2020.",
      "citeRegEx" : "Song and Shmatikov.,? 2020",
      "shortCiteRegEx" : "Song and Shmatikov.",
      "year" : 2020
    }, {
      "title" : "Learning controllable fair representations",
      "author" : [ "Jiaming Song", "Aditya Grover", "Shengjia Zhao", "Stefano Ermon." ],
      "venue" : "arXiv preprint arXiv:1812.04218.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Listening between the lines: Learning personal attributes from conversations",
      "author" : [ "Anna Tigunova", "Andrew Yates", "Paramita Mirza", "G. Weikum." ],
      "venue" : "Proceedings of WWW 2019, page 1818–1828.",
      "citeRegEx" : "Tigunova et al\\.,? 2019",
      "shortCiteRegEx" : "Tigunova et al\\.",
      "year" : 2019
    }, {
      "title" : "Dialogue natural language inference",
      "author" : [ "Sean Welleck", "Jason Weston", "Arthur Szlam", "Kyunghyun Cho." ],
      "venue" : "Proceedings of ACL 2019, pages 3731– 3741.",
      "citeRegEx" : "Welleck et al\\.,? 2019",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfertransfo: A transfer learning approach for neural network based conversational agents",
      "author" : [ "Thomas Wolf", "Victor Sanh", "Julien Chaumond", "Clement Delangue." ],
      "venue" : "arXiv preprint, arXiv:1901.08149.",
      "citeRegEx" : "Wolf et al\\.,? 2019",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Getting to know you: User attribute extraction from dialogues",
      "author" : [ "Chien-Sheng Wu", "Andrea Madotto", "Zhaojiang Lin", "Peng Xu", "Pascale Fung." ],
      "venue" : "Proceedings of LREC 2020, pages 581–589.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Proceedings of NIPS 2019, pages 5754–5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Analyzing information leakage of updates to natural language models",
      "author" : [ "Santiago Zanella-Béguelin", "Lukas Wutschitz", "Shruti Tople", "Victor Ruehle", "Andrew Paverd", "Olga Ohrimenko", "Boris Köpf", "Marc Brockschmidt." ],
      "venue" : "Proceedings of ACM CCS",
      "citeRegEx" : "Zanella.Béguelin et al\\.,? 2020",
      "shortCiteRegEx" : "Zanella.Béguelin et al\\.",
      "year" : 2020
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi" ],
      "venue" : "In Proceedings of ICLR",
      "citeRegEx" : "Zhang. et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang. et al\\.",
      "year" : 2020
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of ACL 2020:",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler." ],
      "venue" : "Proceedings of ICCV 2015, pages 19–27.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "With recent progress in large pretrained language models (Radford et al., 2019; Yang et al., 2019), some attempts (Wolf et al.",
      "startOffset" : 57,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "With recent progress in large pretrained language models (Radford et al., 2019; Yang et al., 2019), some attempts (Wolf et al.",
      "startOffset" : 57,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : ", 2019), some attempts (Wolf et al., 2019; Zhang et al., 2020; Ham et al., 2020; Shen et al., 2021; Sevegnani et al., 2021; Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs).",
      "startOffset" : 23,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : ", 2019), some attempts (Wolf et al., 2019; Zhang et al., 2020; Ham et al., 2020; Shen et al., 2021; Sevegnani et al., 2021; Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs).",
      "startOffset" : 23,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : ", 2019), some attempts (Wolf et al., 2019; Zhang et al., 2020; Ham et al., 2020; Shen et al., 2021; Sevegnani et al., 2021; Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs).",
      "startOffset" : 23,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : ", 2019), some attempts (Wolf et al., 2019; Zhang et al., 2020; Ham et al., 2020; Shen et al., 2021; Sevegnani et al., 2021; Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs).",
      "startOffset" : 23,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : ", 2019), some attempts (Wolf et al., 2019; Zhang et al., 2020; Ham et al., 2020; Shen et al., 2021; Sevegnani et al., 2021; Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs).",
      "startOffset" : 23,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : ", 2019), some attempts (Wolf et al., 2019; Zhang et al., 2020; Ham et al., 2020; Shen et al., 2021; Sevegnani et al., 2021; Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs).",
      "startOffset" : 23,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "Unfortunately, large language models tend to memorize training data and some private data can be recovered from models via black-box training data extraction attacks (Carlini et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 188
    }, {
      "referenceID" : 20,
      "context" : "Besides such memorization problems, “overlearning” on simple training objectives can reveal sensitive attributes indirectly related to the learning task (Song and Shmatikov, 2020).",
      "startOffset" : 153,
      "endOffset" : 179
    }, {
      "referenceID" : 28,
      "context" : "We first collect a dataset by aligning personas with corresponding utterances in PersonaChat dataset (Zhang et al., 2018).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2018) as additional defense objectives to train the GPT-2 and decrease the attacker’s persona inference accuracy to 0.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 31,
      "context" : "What is worse, for an LM-based chatbot, its training conversations are prone to include more private attributes than other commonly-used corpora for language modeling like BooksCorpus (Zhu et al., 2015) and Wikipedia.",
      "startOffset" : 184,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "For optimization, we can leave out constant terms (Mireshghallah et al., 2021) and obtain the following loss function:",
      "startOffset" : 50,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "(6) Following the derivation in (Song et al., 2018) and (Li et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : ", 2018) and (Li et al., 2020), the upper bound can be formulated as:",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "Instead, we use pΨ(s|f(u)) to approximate q(s|f(u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2018):",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "features in natural language processing (Elazar and Goldberg, 2018; Coavoux et al., 2018; Li et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "features in natural language processing (Elazar and Goldberg, 2018; Coavoux et al., 2018; Li et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "features in natural language processing (Elazar and Goldberg, 2018; Coavoux et al., 2018; Li et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : "To train the GPT-2 as our chatbot, we use the DialoGPT (Zhang et al., 2020) pretrained on Reddit comment chains.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "To obtain annotated dataset Da for the adversary, we align personas to corresponding utterances through positive (utterance,persona) pairs provided in Dialogue NLI (Welleck et al., 2019) dataset.",
      "startOffset" : 164,
      "endOffset" : 186
    }, {
      "referenceID" : 3,
      "context" : "We also use Bayesian Privacy (BP) (Gu et al., 2021a) to quantify the attacker’s privacy loss for the estimated persona distribution.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : "For utility, we apply BERTScore (Zhang* et al., 2020), Distinct (Li et al.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 9,
      "context" : ", 2020), Distinct (Li et al., 2016), BLEU (Papineni et al.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : ", 2016), BLEU (Papineni et al., 2002) and perplexity (PPL) as evaluation metrics.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "We use Sentence-BERT (Reimers and Gurevych, 2020) to embed all persona sentences and perform k-means clustering on the embeddings to obtain 8 clusters.",
      "startOffset" : 21,
      "endOffset" : 49
    } ],
    "year" : 0,
    "abstractText" : "Social chatbots, also known as chit-chat chatbots, evolve rapidly with large pretrained language models. Despite the huge progress, privacy concerns have arisen recently: training data of large language models can be extracted via model inversion attacks. On the other hand, the datasets used for training chatbots contain many private conversations between two individuals. In this work, we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet. We show that speakers’ personas can be inferred through a simple neural network with high accuracy. To this end, we propose effective defense objectives to protect persona leakage from hidden states. We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve language models’ powerful generation ability.",
    "creator" : null
  }
}
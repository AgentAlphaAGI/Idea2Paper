{
  "name" : "ARR_2022_48_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multimodal sentiment analysis (MSA) has been an emerging research field for its potential applications in human-computer interaction. How to effectively fuse multimodal information including textual, acoustic, and visual to predict the sentiment is a very challenging problem and has been\naddressed by many previous studies. Some works focus on introducing additional information into the fusing model, such as the alignment information between different modal features (Wu et al., 2021) and unimodal sentiment labels (Yu et al., 2021). And other works consider the semantic gaps between multimodal data and adopt the adversarial learning (Mai et al., 2020) and multi-task learning (Hazarika et al., 2020) to map different modal features into a shared subspace.\nDespite the apparent success of the current stateof-the-art models, their performance decreases sharply, when deployed in the real world. The\nreason is that the input texts are provided by the ASR models, which usually are with errors because of the limitation of model capacity. To further analyze this problem, we build three real-world multimodal sentiment analysis datasets based on the existing dataset, CMU-MOSI(Zadeh et al., 2016). Specifically, we adopt three widely used ASR APIs including SpeechBrain, IBM, and iFlytek to process the original audios and obtain the recognized texts. Then, we replace the gold texts in CMUMOSI with the ASR results and get three realworld datasets, namely MOSI-SpeechBrain, MOSIIBM, and MOSI-iFlytek. We evaluate the current state-of-the-art model, Self-MM(Yu et al., 2021), and report the mean absolute error (MAE) on the multimodal sentiment analysis task. As we can see in Figure 1(a), when the model is deployed in the real world, there is an obvious drop in model performance.\nThe further in-depth analysis of ASR errors shows that the sentiment word substitution error can hurt the MSA model directly. The reason is that the sentiment words in the text are the most important clues in the textual modality for detecting sentiment and incorrectly recognizing them could change the sentiment conveyed by the text. To have an intuitive understanding of the sentiment word substitution error, we take an example in Figure 1(b). The gold text is “And I was really upset about it\", but the ASR model (SpeechBrain) recognizes the sentiment word “upset\" wrongly as “set\", which results in the change of the sentiment semantics of the text and directly affects the MSA model performance. We list the percentages of the sentiment word substitution error on the MOSI dataset for three ASR APIs in Figure 1(b). The percentage of the sentiment word substitution error on the MOSI-IBM is 17.6%, which means about 17 of 100 utterances have this type of error. To further demonstrate the negative effect of the substitution error on the MSA models, we split the test data of MOSI-IBM into two groups by whether there is a substitution error. We evaluate Self-MM on the test data and observe that the misclassification rate of the group in which the substitution error exists is higher than the other group (29.9% vs 15.8%). This result indicates that the sentiment word substitution error could hurt the state-of-the-art MSA model.\nTo tackle this problem, we propose the sentiment word aware multimodal refinement model, which can detect the positions of the sentiment words in\nthe text and dynamically refine the word embeddings in the detected positions by incorporating multimodal clues. The basic idea of our approach is shown in Figure 1(c). We consider leveraging the multimodal sentiment information, namely the negative sentiment conveyed by the low voice and sad face, and textual context information to help the model reconstruct the sentiment semantics for the input embeddings. Specifically, we first use the sentiment word location module to detect the positions of sentiment words and meanwhile utilize the strong language model, BERT, to generate the candidate sentiment words. Then we propose the multimodal sentiment word refinement module to refine the word embeddings based on the multimodal context information. The refinement process consists of two parts, filtering and adding. We apply the multimodal gating network to filter out useless information from the input word embeddings in the filtering process and use the multimodal sentiment word attention network to leverage the useful information from candidate sentiment words as the supplement to the filtered word embeddings in the adding process. Finally, the refined sentiment word embeddings are used for multimodal feature fusion.\nWe conduct extensive experiments on the MOSISpeechBrain, MOSI-IBM, and MOSI-iFlytek datasets to demonstrate the effectiveness of our proposed model. The experimental results show that: (1) There is an obvious performance drop for the state-of-the-art MSA model, when the model is deployed in the real world taking the ASR outputs as the input of textual modality; (2) Our proposed model outperforms all baselines, which can dynamically refine the sentiment word embeddings by leveraging multimodal information.\nThe main contributions of this work are as follows: (1) We propose a novel sentiment word aware multimodal refinement model for multimodal sentiment analysis, which can dynamically reconstruct the sentiment semantics of the ASR texts with errors by utilizing the multimodal sentiment information resulting in more robust sentiment prediction; (2) We validate the negative effect of the sentiment word substitution error on the state-of-the-art MSA model through the in-depth analysis; (3) We evaluate our model on three real-world datasets, and the experimental results demonstrate that our model outperforms all baselines."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multimodal sentiment analysis has gained increasing attention from the community recently and some process has been made. In general, there are three findings presented by previous work.\nPerforming the cross-modal alignment is helpful for multimodal feature fusion. Chen et al. (2017) considered that the holistic features mainly contain global information, which may fail to capture local information. Therefore, they applied the force-alignment to align the visual and acoustic features with the words and further obtained the word-level features. To effectively fuse them, they proposed the GME-LSTM(A) model, which consists of two modules, the gated multimodal embedding and the LSTM with the temporal attention. However, obtaining the word-level features needs to perform the force-alignment, which is time-consuming. To address it, Tsai et al. (2019) proposed the MulT model, which uses the crossmodal attention to align different modal features implicitly. Instead of performing the alignment in the time dimension, some works focusing on semantic alignment. Hazarika et al. (2020) considered that the semantic gaps between heterogeneous data could hurt the model performance and proposed the MISA model, which maps the different modal data into a shared space before multimodal feature fusion. Wu et al. (2021) first utilized the cross-modal prediction task to distinguish the shared and private semantics of non-textual modalities compared to the textual modality and then fuse them. The above works show that performing the cross-modal alignment is helpful for multimodal feature fusion.\nTraining the MSA models in an end-to-end manner is more effective. Most of the previous studies adopt a two-phase pipeline, first extracting unimodal features and then fusing them. Dai et al. (2021) considered that it may lead to suboptimal performance since the extracted unimodal features are fixed and cannot be further improved benefiting from the downstream supervisory signals. Therefore, they proposed the multimodal endto-end sparse model, which can optimize the unimodal feature extraction and multimodal feature fusion jointly. The experimental results on the multimodal emotion detection task show that training the models in an end-to-end manner can obtain better results than the pipeline models.\nLeveraging the unimodal sentiment labels to\nlearn more informative unimodal representations is useful for multimodal feature fusion. Yu et al. (2020) considered that introducing the unimodal sentiment labels can help the model capture the unimodal sentiment information and model the difference between modalities. Motivated by it, they built the CH-SIMS dataset, which contains not only the multimodal sentiment labels but also unimodal sentiment labels. And based on it, they proposed a multi-task learning framework to leverage two types of sentiment labels simultaneously. However, this method needs unimodal labels, which is absent for most of the existing datasets. To address it, Yu et al. (2021) proposed the Self-MM model, which first generates the unimodal labels by utilizing the relationship between the unimodal and multimodal labels and then uses the multi-task learning to train the model. These two works both address the usefulness of introducing unimodal labels.\nHowever, even though lots of models are proposed and obtain promising results on the benchmark datasets, there are few works considering the noisy inputs when the MSA models are deployed in the real world. Chen et al. (2017) presented the Gated Multimodal Embedding to filter out the noises from the acoustic and visual data. Pham et al. (2019) considered that visual and acoustic data may be absent and proposed the MCTN model to handle it. Liang et al. (2019) and Mittal et al. (2020) also mainly focused on dealing with the noises introduced by the visual and acoustic data, and their models are based on the word-level features, which are obtained by aligning the audios with the gold texts. There is only one work (Dumpala et al., 2018) considering that the texts are output by the ASR models, which may be erroneous. But this work does not study how do the ASR errors affect the MSA models and does not evaluate the SOTA MSA models on the datasets. Besides, the proposed model needs the gold texts when training, which is time-consuming and labor-consuming.\nComparing to the above works, we evaluate the SOTA MSA models on the real-world datasets and observe that the performance of models decreases sharply because of the erroneous ASR texts. Through in-depth analysis of the ASR outputs, we find the sentiment word substitution error in the ASR texts could hurt the MSA models directly. To address it, we propose the sentiment word aware multimodal refinement model, which only uses the\nASR texts in the training and testing phrases."
    }, {
      "heading" : "3 Approach",
      "text" : "In this section, we describe the sentiment word aware multimodal refinement model in detail. An illustration of our proposed model is given in Figure 2. Our model consists of three modules including the sentiment word location module, multimodal sentiment word refinement module, and multimodal feature fusion module. We first use the sentiment word location module to detect the possible positions of sentiment words and then utilize the multimodal sentiment word refinement module to dynamically refine the word embeddings in the detected positions. Finally, the refined word embeddings are fed into the multimodal feature fusion module to predict the final sentiment labels."
    }, {
      "heading" : "3.1 Sentiment Word Position Detection",
      "text" : "The core idea of the sentiment word position detection module is to find out the possible positions of sentiment words in the ASR texts. Note that, it is different from locating sentiment words depending on the word semantics, since the ASR models may recognize a sentiment word as a neutral word, which makes it hard to locate correctly. For example, given a gold text “And I was really upset about it\", the ASR model recognizes it as “And I was really set about it\". It is easy for the model to label the word “set\" as a neutral word. Therefore, we choose to detect the position of the sentiment words instead of locating them.\nTo achieve it, we consider adopting a powerful language model, since the language model can model the context information of the sentiment words such as syntactic and grammatical information and predict the appropriate words for the target position. Specifically, we choose the BERT model (Devlin et al., 2019) as our language model since the masked language modeling pretraining objective meets our needs perfectly. Given the sentence {w1, w2, ..., wnl}, we first mask each word wi in the sentence sequentially, and in practice, we replace the word with the special word [MASK]. For example, we mask the first word in the sentence and obtain {[MASK], w2, ..., wnl}. And then we use the BERT model to predict the possible words in the position of the masked word. We sort the predicted candidate words by the prediction probabilities and get the Top-k candidate words Ci = {ci1, ci2, ..., cik}.\nNext, we distinguish the sentiment words from the candidates using the sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005) and ki is the number of sentiment words. The larger the number is, the more possible the position is. And we obtain the most possible position of sentiment word, s = argmax({k1, k2, ..., knl}). Considering that in some cases there is not a sentiment word in the sentence, we use a sentiment threshold to filter out the impossible ones. In practice, we use the gate mask p to record it, and p is 1 if ks is larger than k/2 and 0 otherwise."
    }, {
      "heading" : "3.2 Multimodal Sentiment Word Refinement",
      "text" : "In order to reduce the negative effects of the ASR errors, we propose the multimodal sentiment word refinement module, in which we refine the word embeddings of sentiment words from two aspects. One is that we uses the multimodal gating network to filter out the useless information from the input word embeddings. The other one is that we design the multimodal sentiment attention network to incorporate the useful information from candidate words generated by the BERT model.\nGiven an utterance, which includes three modal unaligned features, word embeddings, acoustic features, and visual features, we denote them as xi = {xit : 1 ≤ t ≤ ni, xit ∈ Rd i x}, i ∈ {l, v, a}. To obtain the multimodal information corresponding to each word, We utilize the pseudo-alignment method to align the features. We split the the acoustic and visual features into non-overlapping feature groups, of which lengths are ⌊ na nl ⌋ and ⌊ nv nl ⌋ respectively, and average the features in each group and obtain the aligned features, ui = {uit : 1 ≤ t ≤ nl, uit ∈ Rd i x}, i ∈ {v, a}.\nTo obtain the context-aware representations, we apply the BERT model and LSTM networks to encode the features, producing hi = {hit : 1 ≤ t ≤ nl, hit ∈ Rd i h}, i ∈ {v, a, l}. Besides, we also use an LSTM network to fuse the acoustic and visual features for capturing high-level sentiment semantics and obtain hva = {hvat : 1 ≤ t ≤ nl, h va t ∈ Rd va h }.\nhl = BERT(xl)\nhv = LSTMv(u v) ha = LSTMa(u a)\nhva = LSTMva([u v;ua])\n(1)\nSubsequently, We propose the multimodal gating network to filter the word embedding, which is im-\nplemented by a non-linear layer. The motivation is that the ASR model may recognize incorrectly the sentiment word resulting in the corrupted sentiment semantics of the text. Therefore, we leverage the multimodal sentiment information to decide how much information of the input word embedding to pass. Specifically, we concatenate the unimodal context-aware representations, hls, h v s , h a s , and bimodal representation hvas in the detected position s and feed them into a non-linear neural network, producing the gate value gv. And then the gate value is used to filter out the useless information from the word embedding. To make the model ignore the impossible one, we use the gate mask p to achieve it.\ngv = Sigmoid(W1([h l s;h v s ;h a s ;h va s ]) + b1) rv = (1− gvp)xls (2)\nwhere W1 ∈ R1× ∑ i∈{l,v,a,va} d i h , b1 ∈ R1 are the parameters of the multimodal gating network. Furthermore, we propose a novel multimodal sentiment word attention network to leverage the sentiment-related information from the candidate words, more than half of which are sentiment words, generated by the BERT model to complement the word embeddings. For example, the ASR\nmodel recognizes the “upset\" as “set\", we first want to remove the useless information of “set\" and then incorporate the information of negative sentiment words to reconstruct the original sentiment semantics. We use a linear layer to implement the multimodal sentiment word attention network. We first concatenate the word embedding xc s t of the candidate word cst and multimodal representations, h v s , has , and h va s at the most possible time step s. Then, we pass them to the linear layer and obtain the attention score get . The attention scores are fed into a softmax function to obtain the attention weights. Finally, we apply the weights to the candidate word embeddings and get the sentiment embedding re.\nget = W2([x cst ;hvs ;h a s ;h va s ]) + b2\nwet = eg e t∑k\nt=1 e get\nre = k∑\nt=1\nwetx cst\n(3)\nwhere W2 ∈ R1×(d l x+\n∑ i∈{v,a,va} d\ni h), b2 ∈ R1\nare the parameters of the multimodal sentiment word attention network.\nIn addition, there may not be suitable words in the candidate words. Hence, we incorporate the\nembedding of the special word [MASK], xmask, to let the BERT model handle this problem based on the context. We then design an aggregation network to balance the contributions of the special word embedding xmask and the sentiment embedding re. Finally, we add the radd to the filtered word embedding uls and obtain the refined word embedding rl for the target word.\ngmask = Sigmoid(W3([r e;xmask]) + b3)\nradd = gmaskre + (1− gmask)xmask\nrl = (gvp)radd + rv\n(4)\nwhere W3 ∈ R1×2d l x , b3 ∈ R1 are the trainable\nparameters."
    }, {
      "heading" : "3.3 Multimodal Feature Fusion",
      "text" : "We describe our multimodal feature fusion module in the section. We first use the BERT model to encode the refined word embeddings zl = {xl1, xl2, ..., rl, .., xlnl} and take the representation of [CLS] as the textual representation, which is denoted as vl. And then we use two LSTM networks to encode the visual and acoustic features and take the representations of the first words as the visual representation vv and acoustic representation va. Finally, we fuse them using a non-linear layer to capture the interactions between them.\nvl = BERTtextual(z l) vv = LSTMvisual(x v) va = LSTMacoustic(x a)\nvf = Relu(W4([v l; vv; va]) + b4)\n(5)\nwhere W4 ∈ Rd f v×(dlv+dav+dvv), b4 ∈ Rd f v are the trainable parameters of the fusion network. We utilize a linear layer to predict the final sentiment regression labels.\npf = W5v f + b5 (6)\nwhere W5 ∈ R1×d f v , b5 ∈ R1 are the trainable parameters of the prediction network. Besides, to enhance the model to capture unimodal sentiment information, we use the Unimodal Label Generation Module (ULGM) (Yu et al., 2021) to generate pseudo unimodal sentiment labels and adopt them to train our model in a multi-task learning manner. For more details, we refer you to Yu et al. (2021)."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We build three real-world datasets including MOSISpeechBrain, MOSI-IBM, and MOSI-iFlytek, on CMU-MOSI(Zadeh et al., 2016).\nCMU-MOSI CMU multimodal opinion-level sentiment intensity (CMU-MOSI) consists of 93 videos collected from the YouTube website. The length of the videos varies from 2-5 mins. These videos are split into 2,199 short video clips and labeled with sentiment scores from -3 (strongly negative) to 3 (strongly positive). For multimodal features, we extract the visual features using Facet, which can extract the facial action units (Ekman et al., 1980) from each frame. The acoustic features are obtained by applying COVAREP (Degottex et al., 2014), which includes 12 Mel-frequency cepstral coefficients (MFCCs) and other low-level features.\nHowever, the provided texts of the utterances in the MOSI dataset are manually transcribed from the corresponding videos by the expert transcribers, which is unrealistic for the real-world applications to obtain the texts in such a way. To evaluate the models in the real world, we replace the manually gold texts in the dataset with the texts output by the ASR models. We adopt a strong ASR model and two widely used commercial APIs to produce the texts. The utilized ASR model released by Ravanelli et al. (2021) is built on the transformer encoder-decoder framework and trained on the Librispeech dataset(Panayotov et al., 2015). The commercial APIs used by us are IBM1 and iFlytek2 speech-to-text APIs, which are wildly used by researchers and software developers. Finally, we apply the three ASR models to transcribe the videos into texts and construct three new datasets, namely MOSI-SpeechBrain, MOSI-IBM, and MOSI-iFlytek. Noted that, we do not adopt MOSEI (Bagher Zadeh et al., 2018), because it does not provide the original video clips for the extracted features and annotated sentiment labels, and we can not process the original audios."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "For the MOSI-SpeechBrain, MOSI-IBM, and MOSI-iFlytek datasets, following previous work\n1https://www.ibm.com/cloud/watson-speech-to-text 2https://global.xfyun.cn/products/lfasr\n(Yu et al., 2021), we take 2-class accuracy(Acc2), F1 score(F1), mean absolute error (MAE), and correlation(Corr) as our evaluation metrics."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare our proposed model with the following baselines 3. TFN (Zadeh et al., 2017) uses the three-fold Cartesian product to capture unimodal, bimodal, and trimodal interactions. LMF (Liu et al., 2018) uses the low-rank tensors to accelerate the multimodal feature fusion process. MulT (Tsai et al., 2019) uses the cross-modal transformers to fuse multimodal features. MISA (Hazarika et al., 2020) adopts multi-task learning to map different modal features into a shared subspace. Self-MM (Yu et al., 2021) first generates the pseudo unimodal sentiment labels and then adopts them to train the model in a multi-task learning manner."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Quantitative Results",
      "text" : "In Table 1, we show the results on the MOSISpeechBrain, MOSI-IBM, MOSI-iFlytek datasets. And we also list the results of the SOTA model, Self-MM, on the original MOSI dataset in the last row of the table for the performance comparison between Self-MM in the ideal world and real world. As we can see from the results, Self-MM obtains\n3Because applying the force-alignment using the errorous ASR texts leads to cascading errors resulting in poor aligned features, we only take the models using unaligned features as our baselines for a fair comparison.\nthe best results on the MOSI-Gold dataset than the other datasets, which demonstrates that the ASR errors hurt the MSA models. We also observe that the better ASR model can help the MSA models achieve better performance. But it should be noted that, according to the analysis in the previous section, current ASR models still can not produce satisfactory results for the MSA models in the real world.\nComparison between the feature-based models including TFN, LMF, and MulT and finetuningbased baselines such as MISA and Self-MM, we can find that finetuning-based models obtain better results. We consider that the finetuning-based models can adapt the BERT encoder to the target task and learning more informative textual representations, which also makes them benefit more as the quality of texts increases.\nComparing to the baselines especially Self-MM, our model achieves better performance in all evaluation metrics since our model can detect the substitution error of the sentiment words and then refine the word embeddings to reconstruct the sentiment semantics in the textual modality by filtering out useless information from the input words and incorporating useful information from the candidate words generated by the language model. We also observe that the improvement of our model compared with Self-MM on MOSI-iFlytek is smaller. We consider that the main reason is fewer sentiment word substitution errors on MOSI-iFlytek."
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "We conduct the ablation experiments to distinguish the contribution of each part. There are several different variants of our model. SWRM is our proposed full model. SWRM w/o Position does not use the sentiment word position location module and only uses the information of the special word [MASK] to dynamically refine all words. SWRM w/o Attention only incorporates the information of the special word [MASK] to refine the word in the multimodal sentiment word refinement module. SWRM w/o Multi-modal only performs the multimodal sentiment word attention and multimodal gating network based on the textual features without the acoustic and visual features.\nTable 2 shows the results of the variants of our model. After ablating the sentiment word position location module, SWRM w/o Position obtains worse results than SWRM, which indicates that finding the right word for refinement is very important. The comparison between SWRM w/o Attention and SWRM w/o Position further demonstrates this conclusion. SWRM w/o Attention first detects the right position and then incorporates the information of the special word [MASK], which achieves better performance than SWRM w/o Position. But SWRM w/o Attention is still worse than SWRM, which shows using the attention network to incorporating extra information from the candidate words is useful for refinement. Comparing the SWRM w/o Multi-modal between SWRM, we can find that the model benefits from the visual and acoustic features. It is in line with our expectations since the sentiment information provided by the multimodal features can help the model detect the sentiment word and incorporate the sentimentrelated information from the candidate words."
    }, {
      "heading" : "5.3 Case Study",
      "text" : "To have an intuitive understanding of our proposed model, we show a case in Figure 3. We can see that our model first detects the most possible po-\nsition based on the context and then finds that the input word in the position may be recognized incorrectly since there is a mismatch between the negative word “cruel\" and either the smile or the excited tone. Hence our model decides to incorporate the related sentiment information from the candidate words to refine the word embedding. As shown in Figure 3, our model pays more attention to the candidate words \"special\", \"cool\", and \"awesome\". The word \"cool\" is exactly the gold word and the others have the same sentiment polarity as it. Beneficial from the attended candidate words, our model refines the input word and reconstructs its sentiment semantics. Finally, the refined word embeddings are fed into the multimodal feature fusion module to predict the sentiment label."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we observe an obvious performance drop when the SOTA MSA model is deployed in the real world, and through in-depth analysis, we find that the sentiment word substitution error is a very important factor causing it. To address it, we propose the sentiment word aware multimodal refinement model, which can dynamically refine the word embeddings and reconstruct the corrupted sentiment semantics by incorporating the multimodal sentiment information. We evaluate our model on MOSI-SpeechBrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the effectiveness of our approach. For future work, we will explore leveraging the multimodal information to detect the sentiment word positions."
    }, {
      "heading" : "A Training Details",
      "text" : "We use Adam as the optimizer and the learning rate is 5e-5. The sentiment threshold is set to 0.5 while detecting the sentiment word position. The number of the candidate words k is 50. All experiments are run on an Nvidia Tesla P100 GPU. We run five times and report the average performance. The random seeds we used are 1111,1112, 1113, 1114, and 1115."
    } ],
    "references" : [ {
      "title" : "Multimodal language analysis in the wild: CMUMOSEI dataset and interpretable dynamic fusion graph",
      "author" : [ "AmirAli Bagher Zadeh", "Paul Pu Liang", "Soujanya Poria", "Erik Cambria", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 56th Annual Meeting of",
      "citeRegEx" : "Zadeh et al\\.,? 2018",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal sentiment analysis with wordlevel fusion and reinforcement learning",
      "author" : [ "Minghai Chen", "Sen Wang", "Paul Pu Liang", "Tadas Baltrušaitis", "Amir Zadeh", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 19th ACM International Conference on",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Multimodal end-to-end sparse model for emotion recognition",
      "author" : [ "Wenliang Dai", "Samuel Cahyawijaya", "Zihan Liu", "Pascale Fung." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Dai et al\\.,? 2021",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2021
    }, {
      "title" : "Covarep—a collaborative voice analysis repository for speech technologies",
      "author" : [ "Gilles Degottex", "John Kane", "Thomas Drugman", "Tuomo Raitio", "Stefan Scherer." ],
      "venue" : "2014 ieee international conference on acoustics, speech and signal processing (icassp), pages",
      "citeRegEx" : "Degottex et al\\.,? 2014",
      "shortCiteRegEx" : "Degottex et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment classification on erroneous asr transcripts: a multi view learning approach",
      "author" : [ "Sri Harsha Dumpala", "Imran Sheikh", "Rupayan Chakraborty", "Sunil Kumar Kopparapu." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pages",
      "citeRegEx" : "Dumpala et al\\.,? 2018",
      "shortCiteRegEx" : "Dumpala et al\\.",
      "year" : 2018
    }, {
      "title" : "Facial signs of emotional experience",
      "author" : [ "Paul Ekman", "Wallace V Freisen", "Sonia Ancoli." ],
      "venue" : "Journal of personality and social psychology, 39(6):1125.",
      "citeRegEx" : "Ekman et al\\.,? 1980",
      "shortCiteRegEx" : "Ekman et al\\.",
      "year" : 1980
    }, {
      "title" : "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "author" : [ "Devamanyu Hazarika", "Roger Zimmermann", "Soujanya Poria." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Multimedia, MM ’20, page",
      "citeRegEx" : "Hazarika et al\\.,? 2020",
      "shortCiteRegEx" : "Hazarika et al\\.",
      "year" : 2020
    }, {
      "title" : "Mining opinion features in customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "AAAI, volume 4, pages 755–760.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Learning representations from imperfect time series data via tensor rank regularization",
      "author" : [ "Paul Pu Liang", "Zhun Liu", "Yao-Hung Hubert Tsai", "Qibin Zhao", "Ruslan Salakhutdinov", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the As-",
      "citeRegEx" : "Liang et al\\.,? 2019",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient lowrank multimodal fusion with modality-specific factors",
      "author" : [ "Zhun Liu", "Ying Shen", "Varun Bharadhwaj Lakshminarasimhan", "Paul Pu Liang", "AmirAli Bagher Zadeh", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 56th Annual Meeting of",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "author" : [ "Sijie Mai", "Haifeng Hu", "Songlong Xing." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):164–172.",
      "citeRegEx" : "Mai et al\\.,? 2020",
      "shortCiteRegEx" : "Mai et al\\.",
      "year" : 2020
    }, {
      "title" : "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "author" : [ "Trisha Mittal", "Uttaran Bhattacharya", "Rohan Chandra", "Aniket Bera", "Dinesh Manocha." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Mittal et al\\.,? 2020",
      "shortCiteRegEx" : "Mittal et al\\.",
      "year" : 2020
    }, {
      "title" : "Librispeech: an asr corpus based on public domain audio books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206–5210.",
      "citeRegEx" : "Panayotov et al\\.,? 2015",
      "shortCiteRegEx" : "Panayotov et al\\.",
      "year" : 2015
    }, {
      "title" : "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "author" : [ "Hai Pham", "Paul Pu Liang", "Thomas Manzini", "LouisPhilippe Morency", "Barnabás Póczos." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial",
      "citeRegEx" : "Pham et al\\.,? 2019",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2019
    }, {
      "title" : "SpeechBrain: A general-purpose speech toolkit",
      "author" : [ "François Grondin", "William Aris", "Hwidong Na", "Yan Gao", "Renato De Mori", "Yoshua Bengio." ],
      "venue" : "ArXiv:2106.04624.",
      "citeRegEx" : "Grondin et al\\.,? 2021",
      "shortCiteRegEx" : "Grondin et al\\.",
      "year" : 2021
    }, {
      "title" : "Multimodal transformer for unaligned multimodal language sequences",
      "author" : [ "Yao-Hung Hubert Tsai", "Shaojie Bai", "Paul Pu Liang", "J. Zico Kolter", "Louis-Philippe Morency", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Asso-",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Recognizing contextual polarity in phraselevel sentiment analysis",
      "author" : [ "Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann." ],
      "venue" : "Proceedings of human language technology conference and conference on empirical methods in natural language processing,",
      "citeRegEx" : "Wilson et al\\.,? 2005",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2005
    }, {
      "title" : "A text-centered shared-private framework via cross-modal prediction for multimodal sentiment analysis",
      "author" : [ "Yang Wu", "Zijie Lin", "Yanyan Zhao", "Bing Qin", "LiNan Zhu." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "CH-SIMS: A Chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "author" : [ "Wenmeng Yu", "Hua Xu", "Fanyang Meng", "Yilin Zhu", "Yixiao Ma", "Jiele Wu", "Jiyun Zou", "Kaicheng Yang." ],
      "venue" : "Proceedings of the 58th Annual Meet-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis",
      "author" : [ "Wenmeng Yu", "Hua Xu", "Ziqi Yuan", "Jiele Wu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):10790–10797.",
      "citeRegEx" : "Yu et al\\.,? 2021",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2021
    }, {
      "title" : "Tensor fusion network for multimodal sentiment analysis",
      "author" : [ "Amir Zadeh", "Minghai Chen", "Soujanya Poria", "Erik Cambria", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Zadeh et al\\.,? 2017",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2017
    }, {
      "title" : "Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "author" : [ "Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency" ],
      "venue" : null,
      "citeRegEx" : "Zadeh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Some works focus on introducing additional information into the fusing model, such as the alignment information between different modal features (Wu et al., 2021) and unimodal sentiment labels (Yu et al.",
      "startOffset" : 145,
      "endOffset" : 162
    }, {
      "referenceID" : 20,
      "context" : ", 2021) and unimodal sentiment labels (Yu et al., 2021).",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "And other works consider the semantic gaps between multimodal data and adopt the adversarial learning (Mai et al., 2020) and multi-task learning (Hazarika et al.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : ", 2020) and multi-task learning (Hazarika et al., 2020) to map different modal features into a shared subspace.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "To further analyze this problem, we build three real-world multimodal sentiment analysis datasets based on the existing dataset, CMU-MOSI(Zadeh et al., 2016).",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 20,
      "context" : "We evaluate the current state-of-the-art model, Self-MM(Yu et al., 2021), and report the mean absolute error (MAE) on the multimodal sentiment analysis task.",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "There is only one work (Dumpala et al., 2018) considering that the texts are output by the ASR models, which may be erroneous.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "Specifically, we choose the BERT model (Devlin et al., 2019) as our language model since the masked language modeling pretraining objective meets our needs perfectly.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "Besides, to enhance the model to capture unimodal sentiment information, we use the Unimodal Label Generation Module (ULGM) (Yu et al., 2021) to generate pseudo unimodal sentiment labels and adopt them to train our model in a multi-task learning manner.",
      "startOffset" : 124,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "which can extract the facial action units (Ekman et al., 1980) from each frame.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "The acoustic features are obtained by applying COVAREP (Degottex et al., 2014), which includes 12 Mel-frequency cepstral coefficients (MFCCs) and other low-level",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "(2021) is built on the transformer encoder-decoder framework and trained on the Librispeech dataset(Panayotov et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "(Yu et al., 2021), we take 2-class accuracy(Acc2), F1 score(F1), mean absolute error (MAE), and",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 21,
      "context" : "TFN (Zadeh et al., 2017) uses the three-fold Cartesian product to capture unimodal, bimodal, and trimodal interactions.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "LMF (Liu et al., 2018) uses the low-rank tensors to accelerate the multimodal feature fusion process.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "MulT (Tsai et al., 2019) uses the cross-modal transformers to fuse multimodal features.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "Self-MM (Yu et al., 2021) first generates the pseudo unimodal sentiment labels and then adopts them to train the model in a multi-task learning manner.",
      "startOffset" : 8,
      "endOffset" : 25
    } ],
    "year" : 0,
    "abstractText" : "Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed. However, the performance of the state-of-the-art models decreases sharply when they are deployed in the real world. We find that the main reason is that real-world applications can only access the text outputs by the automatic speech recognition (ASR) models, which may be with errors because of the limitation of model capacity. Through further analysis of the ASR outputs, we find that in some cases the sentiment words, the key sentiment elements in the textual modality, are recognized as other words, which makes the sentiment of the text change and hurts the performance of multimodal sentiment models directly. To address this problem, we propose the sentiment word aware multimodal refinement model (SWRM), which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clues. Specifically, we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings. The refined embeddings are taken as the textual inputs of the multimodal feature fusion module to predict the sentiment labels. We conduct extensive experiments on the real-world datasets including MOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the effectiveness of our model, which surpasses the current state-of-the-art models on three datasets. Furthermore, our approach can be adapted for other multimodal feature fusion models easily.",
    "creator" : null
  }
}
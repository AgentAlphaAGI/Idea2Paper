{
  "name" : "ARR_2022_94_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Co-training an Unsupervised Constituency Parser with Weak Supervision",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models (PLMs) have become a standard tool in the Natural Language Processing (NLP) toolkit, offering the benefits of learning from large amounts of unlabeled data while providing modular function in many NLP tasks that require supervision. Recent work has shown that PLMs capture different types of linguistic regularities and information, for instance, the lower layers capture phrase-level information which becomes less prominent in the upper layers (Jawahar et al., 2019), span representations constructed from these models can encode rich syntactic phenomena, like the ability to track subject-verb agreement (Goldberg, 2019), dependency trees can be embedded within the geometry of BERT’s hidden states (Hewitt and Manning, 2019), and most relevantly to\n1For code or data, please contact the authors.\nWhen s(i, j) is the probability of a span (i, j) being in the correct tree, this formulation gives the tree with highest expected number of correct constituents ?. This formulation has been used recently by several unsupervised constituency parsing algorithms ?.\n3 TRAINING ALGORITHM\nAt the core of our approach lies the notion of inside and outside strings. For a given sentence x = x1 · · ·xn and a span (i, j), the inside stri g of span (i, j) is the sequence xi · · ·xj while the\noutside string is the pair (x1 · · ·xi 1, xj+1 · · ·xn).\nThese two types of strings provide two views of a given possible splitting point in the syntax tree.\nWe offer three ways, with increasing complexity, to bootstrap a score function that helps identify\nwhether a node should dominate a given span.\nThe main idea behind this bootstrapping is to start with a small seed set of training examples (x, i, j, b) where (i, j) is a span in sentence x d b is 1 or 0, depending on whether the span (i, j) is dominated by a node in the syntactic tree.\nBootstrapping the seed set is dependent only on either the inside string or the outside string, and the corresponding classifier build from this bootstrap ed seed set returns a probability p(b | x, i, j). Once a classifier is learned using the bootstrapping seed set, the classifier is applied on the training set, and the seed set is added more examples where the classifier is confident of the label b. This is also known as self-training ?. In the next three sections, we present three learning algorithms of increasing complexity in their use of inside and outside strings\n2\nWhen s(i, j) is the probability of a span (i, j) being in the correct tree, this formulation gives the tree with highest expected number of correct constituents ?. This formulation has been used recently by several unsupervised constituency parsing algorithms ?.\n3 TRAINING ALGORITHM\nAt the core of ur approac lies the notion of inside and outside trings. For a giv sentence x = x1 · · ·xn d a span (i, j), the inside string of span (i, j) i the sequence xi · · ·xj whil the\noutside string is the pair (x1 · · ·xi 1, xj+1 · · ·xn).\nThese two types of strings provide t o views of given possible splitting point in the syntax tree.\nWe offer three ways, with increasing complexity, to bootstrap a score function that helps identify\nwhether a node should dominate a given span.\nThe main idea behind this bootstrapping is to start with a small seed set of training examples (x, i, j, b) where (i, j) is a span in a sentence x and b is 1 or 0, depending on whether the span (i, j) is dominated by a node in the syntactic tree.\nBootstrapping the seed set is dependent only on either the inside string or the outside string, and the corresponding classifier build from this boo strapp d seed set returns a probability p(b | x, i, j). Once a classifier is learned using th bootstrap ing seed set, the classifier is applied on the training set, and the seed set is added more examples where the classifier is confident of the label b. This is also known as self-training ?. In the next three sections, we present three learning algorithms of increasing complexity in their use of inside and outsid strings\n2\nWhen s(i, j) is the probability of a span (i, j) being in the correct tree, this formulation gives the tree with highest expected number of correct constituents ?. This formulation has been used recently by several unsupervised constituency parsing algorithms ?."
    }, {
      "heading" : "3 TRAINING ALGORITHM",
      "text" : "At the c e of our approach lies the notion of inside and outside strings. For a given sentence x = x1 · · ·xn and a span (i, j), the inside string of span (i, j) is the sequence xi · · ·xj while the\noutside string is the pair (x1 · · ·xi 1, xj+1 · · ·xn).\nThese two types of strings provide two views of a given possible splitting point in the syntax tree.\nWe of er three w ys, with increasing complexity, to bootstrap a score function that helps identify\nwhether a node should dominate a given span.\nThe main idea behi d this bootstrapping is to start with a small seed set of training examples (x, i, j, b) where (i, j) is a span in a sentence x and b is 1 or 0, depending on whether the span (i, j) is dominated by a node in the syntactic tree.\nBootstrapping the se d set is dependent only on either the inside string or the outside string, and the corres onding classifier build from this bootstrapped seed set returns a probability p(b | x, i, j). Once a classifier is learned using the bootstrapping seed set, the classifier is applied on the training set, a d the seed set is added more examples where the classifier is confident of the label b. This is also known as self-training ?. I the next three sections, we present three learning algorithms of increasing complexity in their use of inside and outside strings\n2\nWhen s(i, j) is the probability of a span (i, j) being in the correct tree, this formulation gives the tree with highest expected number of correct constituents ?. This formulation has been used recently by several unsupervised constituency parsing algorithms ?.\n3 TRAINING ALGORITHM\nAt the core of our appr ach lies the notion of inside and outside strings. For a given sentence x = x1 · · ·xn and a span (i, j), the inside string of span (i, j) is the sequence xi · · ·xj while the\noutside string is the pair (x1 ·xi 1, xj+1 · · ·xn).\nThese wo types of strings provide two views of a given p ssible splitti g point in the syntax tree.\nW offer three ways, with increasing complexity, to bootstrap a score function that helps identify\nwhether a node should dominate a given span.\nThe main id a behind this bootstrapping is to start with a small seed set of training examples (x, i, j, b) where (i, j) is a span in a sentence x and b is 1 or 0, depending on whether the span (i, j) is dominated by a node in the syntactic tree.\nBootstrapping the seed set is dependent only on either the inside string or the outside string, and the corresponding classifier build from this bootstrapped seed set returns a probability p(b | x, i, j). Once a classifier is learned using the bootstrapping seed set, the classifier is applied on the training set, and the seed set is added more examples where the classifier is confident of the label b. This is also known as self-training ?. In the next thr e sections, we present three learning algorithms of increasing complexity in their use of inside and outside strings\n2\nFigure 1: A depiction of a syntax tree, with the inside string as depicted by the sequence xi · · ·xj and the outside string as depicted by the sequence (x1 · · ·xi−1, xj+1 · · ·xn) that provides external context for the inside representations.\nthis paper, syntactic information via self-att ntion mechanisms (Wang et al., 2019; Kim et al., 2020).\nWe offer another perspective on the way PLMs represent syntactic information. We demonstrate the usability of PLMs to capture syntactic information by developing an u supervised parsing model that makes heavy use of PLMs. The learning algorithm is light in the injection of hard bias to parse text, emphasizing the role of PLMs in capturing syntactic information.\nOur approach to unsupervised parsing is inspired by recent work in the area of spectral learning for parsing (Cohen et al., 2014, 2013) and unsupervised estimation of probabilistic context-free grammars (PCFGs; Clark and Fijalkow, 2020). At its core, our learning algorithm views the presence or absence of a node dominating a substring in the final parse tree as a latent variable, where patterns of co-occurrence of the string that the node dominates (the “inside” string) and the rest of the sentence (the “outside” string) dictate whether the node is present or not. With spectral learning for latent-variable PCFGs (L-PCFGs; Cohen et al., 2012) the notion of inside trees versus outside trees is important, but in our case, given that the trees are not present during learning, we have to further specialize it to\nextract information only from the strings. Consider the diagram of a syntax tree in Figure 1, decomposed into two parts. Following the main notion in spectral learning, each of these parts (the orange part and the blue part) is a “view” of the whole tree that provides information on the identity of the node that spans the words xi · · ·xj . In the case of the tree being unobserved during training, we have to rely only on the substrings that are spanned by the blue part or the orange part, to hypothesize whether indeed a node exists there.\nTo represent the inside and outside views, we make use of PLMs. We encode these substrings, and then bootstrap a classifier that determines whether a given span is a constituent or not. The bootstrapping process alternates between the two views, and at each point adds predictions on the training set that it is confident about to train a new classifier. This can be thought of as a form of co-training (Yarowsky, 1995; Blum and Mitchell, 1998), a training technique that relies on multiple views of training instances. We formulate the task of identifying constituents and distituents (referring to spans that are not constituents) in a sentence as a binary classification task by devising a strategy to convert the unlabeled data into a classification task. Firstly, we build a sequence classification model by fine-tuning a Transformer-based PLM on the unlabeled training sentences to distinguish between the true and false inside strings of constituents. Secondly, we use the highly-confident inside strings to produce the outside strings. Additionally, through the use of semi-supervised learning techniques, we jointly use both the inside and outside passes to enrich the model’s ability to determine the breakpoints in a sentence. Our final model achieves 63.1 sentence F1 averaged over multiple runs with random seed on the Penn Treebank test set. We also report strong results for the Japanese and Chinese treebanks."
    }, {
      "heading" : "2 Problem Formulation and Inference",
      "text" : "We give a treatment to the problem of unsupervised constituency parsing. In that setup, the training algorithm is given an unlabeled corpus (set of sentences) and its goal is to learn a function mapping a sentence x to an unlabeled phrase-structure tree y that indicates the constituents in x. In previous work with models such as the Constituent-Context Model (CCM; Klein and Manning 2002), the Dependency Model with Valence (DMV; Klein and\nManning 2005), and Unsupervised Maximum Likelihood estimator for Data-Oriented Parsing (UMLDOP; Bod 2006), the parts of speech (POS) of the words in x are also given as input both during inference and during training, but we do not make use of such POS tags.\nInference While our learning algorithm is grammarless, for inference we make use of a dynamic programming algorithm, akin to CYK, to predict the parse tree. Inference assumes that each possible span in the tree was scored with a score function s(i, j) where i and j are endpoints in the sentence. The score function is learned through our algorithm. We then proceed by finding the tree t∗ such that:\nt∗ = argmax t∈T ∑ (i,j)∈t s(i, j),\nwhere T is the set of possible binary trees over the sentence and (i, j) ∈ t, with a slight abuse of notation, denotes that the span (i, j) appears in t.\nWhen s(i, j) is the probability of a span (i, j) being in the correct tree, this formulation gives the tree with the highest expected number of correct constituents (Goodman, 1996). This formulation has been used recently by several unsupervised constituency parsing algorithms (Kim et al., 2019b,a; Cao et al., 2020; Li et al., 2020a)."
    }, {
      "heading" : "3 Training Algorithm",
      "text" : "At the core of our approach lies the notion of inside and outside strings. For a given sentence x = x1 · · ·xn and a span (i, j), the inside string of span (i, j) is the sequence xi · · ·xj while the outside string is the pair (x1 · · ·xi−1, xj+1 · · ·xn). We denote by hin(i, j) representations for inside strings and hout(i, j) representations for outside strings. Both are vectors derived from a PLM (RoBERTa (Liu et al., 2019), as we see later).\nThese two types of strings provide two views of a given possible splitting point in the syntax tree. We offer three ways, with increasing complexity, to bootstrap a score function that helps identify whether a node should dominate a given span. The main idea behind this bootstrapping is to start with a small seed set of training examples (x, i, j, b) where (i, j) is a span in a sentence x and b ∈ {0, 1}, depending on whether the span (i, j) is dominated by a node in the syntactic tree or not. Bootstrapping the seed set is dependent only on either the inside string or the outside string, and\nthe corresponding classifier built from this bootstrapped seed set returns a probability p(b | x, i, j). Once a classifier is learned using the bootstrapping seed set, the classifier is applied on the training set, and the seed set is added to more examples where the classifier is confident of the label b. This is also known as self-training (McClosky et al., 2006, 2008).\nIn the next three sections, we present three learning algorithms of increasing complexity in their use of inside and outside strings."
    }, {
      "heading" : "3.1 Modeling Using Inside Strings",
      "text" : "The inside model min which is modeled at a sentence level, computes an inside score sin(i, j) from the inside vector representation hin(i, j) of each span in the unlabeled input training sentence U. To compute hin(i, j), we fine-tune the sequence classification model that encodes a fixed-vector representation for each token in the dataset. This captures the phrase information of the inner content in the span. In order to prepare the features for the inside model, we make use of a seed bootstrapping technique (Section 4.2.1). Once we build the inside model min, we get the most confidently-classified inside strings from U based on a set threshold τ = (τmin, τmax). Here, τmin and τmax, form the confidence bounds to select distituents and constituents respectively. We select a random sample of c constituents and d distituents with appropriate labels from these most confident inside strings comprising the labeled inside set I."
    }, {
      "heading" : "3.2 Modeling Using Inside and Outside Strings",
      "text" : "To perform the iterative self-training procedure, we follow the steps as detailed in Figure 2. While building the outside model, we extract the tokens at the span boundaries of the pair of outside strings, which is of the form consisting of the triple (xi−1, [MASK], xj+1). The outside model computes an outside score sout(i, j) from the outside vector representation hout(i, j) of each span, which models the contextual information of the span. To compute hout(i, j), we extract the triple for every span (i, j) in the dataset and fine-tune another sequence classification model that encodes a fixed-vector representation for each triple."
    }, {
      "heading" : "3.3 An Iterative Co-training Algorithm",
      "text" : "Co-training (Blum and Mitchell, 1998) is a classic multi-view training method, which trains a clas-\nsifier by exploiting two (or more) views of the training instances. Our final learning algorithm is indeed inspired by it, where we consider the inside and the outside strings to be the two views. Once we have the inside min and the outside classifiers mout that are trained on their respective conditionally independent inside hin(i, j) and outside hout(i, j) feature sets, we can make use of an iterative approach. At each iteration, only the inside strings Î that are confident to be likely the insides of constituents and distituents according to the outside model are moved to the labeled training set of the inside model I. Thus, the outside model (teacher) provides the labels to the inside strings on which the inside model (student) is uncertain. Similarly, only the outside strings Ô that are confident to be the likely outsides of constituents and distituents according to the inside model are moved to the labeled training set of the outside model O. Thus, the inside model provides the labels to the outside strings on which the outside model is uncertain. We describe the steps in Figure 3. Finally, we combine the scores obtained by the inside and the outside model to get the score s(i, j) for each span:\ns(i, j) = sin(i, j) · sout(i, j).\nCo-training requires the two views to be independent of each other conditioned on the label of the training instance. This is the type of assumption that, for example, PCFGs satisfy, when breaking a tree into an outside and inside tree: the two trees\nare conditionally independent given the nonterminal that connects them. In our case, we satisfy this assumption by creating inside and outside string representations separately, as we see later in Section 4."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Data",
      "text" : "We evaluate our methodology on the Penn Treebank (PTB; Marcus et al. 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test). For preprocessing, we keep all punctuation and remove any trailing punctuation. To maintain the unsupervised nature of our experiments, we avoid the common practice of using gold parses of the validation set for either early stopping (Shen et al., 2018, 2019; Drozdov et al., 2019) or hyperparameter tuning (Kim et al., 2019a). Additionally, we experiment on Chinese with version 5.1 of the Chinese Penn Treebank (CTB; Xue et al. 2005) with the same splits as in Chen and Manning (2014), and the Japanese Keyaki Treebank (KTB; Butler et al. 2012). For KTB, we shuffle the corpus and use 80% of the sentences for training, 10% for validation, and 10% for testing."
    }, {
      "heading" : "4.2 Multi-view Learning",
      "text" : "In this section, we devise the task of identifying constituents in a sentence by training two mod-\nels with different views of the data. Ideally, these views complement each other and help each model improve the performance of the other."
    }, {
      "heading" : "4.2.1 Seed Bootstrapping",
      "text" : "We treat identifying constituents from unlabeled sentences as a sequence classification task. To generate the constituent class, we take the complete sentence (start:end), as a sentence in itself is a constituent, and also the largest among all of its other constituents. To generate the distituent class, we take (start:end-1), · · · , (start:end-6) slices, where start and end denote the 0th and Nth position (sentence length) respectively. We select the distituents in this manner because the longer the sentence, there would be a significantly unlikely chance that the span of the constituents extends till the very end of the sentence. Additionally, we make use of casing-specific information by adding contiguous title-case words while allowing only the apostrophe mark. Since all of the sentences for the constituent class start with capital letters, we identify the most common first word and generate lower-case equivalents of contiguous title-case words, which starts with it to account for bias due to the casing of spans. While we do use a fixed template to perform the seed bootstrapping process, this is part of the inductive bias of the algorithm, and is minimal. In our analysis, we assume the language is already known before and thereby its structure (left/right-branching), a form of weak supervision.\nFor CTB, we follow the exact same process as PTB for preparing the input data for the firstlevel sequence classifier, but we do not rely on case-specific information and perform no postprocessing. Meanwhile, since KTB is a treebank of a strongly left-branching language, we design our modeling approach slightly differently compared to before, although along the same style. To prepare the data for the sequence classifier, we choose the slice (start:end) in the sentence to label the constituent class, whereas, (start+1:end), · · · , (start+4:end) slices are chosen to label the distituent class. We also split the sentences on “*” mark and treat the resulting fragmented parts as constituents too. Our training does not depend on the development set with the gold-standard annotated trees since we base the necessary string slicing decision on the feedback from the validation split after the bootstrapping procedure in an iterative fashion (increment/decrement the value\nof slice counter by 1) until we see a degradation in performance (measured using F1 score) on the synthetic set of trees."
    }, {
      "heading" : "4.2.2 Inside Model",
      "text" : "We fine-tune the RoBERTa model with a sequence classification head on top using a cross-entropy loss (see Section A.1 in Appendix for training and hyperparameter details). As we supply input data, the entire pre-trained RoBERTaBASE model and the additional untrained classification layer is trained on our specific downstream task. The inside model is evaluated on MCC (Matthews Correlation Coefficient) as well as F1 because the classes are imbalanced. After fine-tuning, our best inside model achieves 0.28 MCC and 0.42 F1 on the internal validation set. Finally, we fine-tune the inside model on the unlabeled training sentences that generates an inside score sin(i, j) for every span. Since our major focus was on PTB, we have listed a few heuristics that inject further bias into the algorithm acting as the another form of weak supervision. Moreover, incorporating such rules was not necessary for CTB and KTB as our models showed superior performance without them.\nOnce we compute the inside score, sin(i, j), we use the following refinement strategies to prune out false constituents: We treat punctuation characters to mark the boundaries of a span and penalize any span that crosses its demarcated punctuation region (indicated by a span) by assigning a negative penalty of 0.25. Additionally, we delete any constituent if it starts or ends with the most common word succeeding the comma punctuation. Next, we take the most common starting word and check if its accompanying word does not belong to either the stop word or is present in the top 20 most frequent tokens of the PTB training set. We assign the scores of these corresponding spans in the CYK chart cell to the maximum value. Intuitively, from the linguistic definition of constituents, we refrain from bracketing if we identify a contiguous group of rare words (tokens not in the top 1000 most frequent list). These heuristics only contribute to a certain extent in making the parser strong, and should be considered as a standard post-processing step. Overall, we observe 3.8 F1 improvements in the case of the inside model. We further note that the contribution due to additional heuristics is much less than the combined self-training and cotraining gains since their effect becomes insignificant after multiple iterations of the self-training\nprocess due to the predictions approximately following the template rules. As described in Figure 2, we perform self-training on the inside model for three iterations.2"
    }, {
      "heading" : "4.2.3 Outside Model",
      "text" : "We extract the outside strings of spans having the inside score satisfying a pre-determined cutoff value. The values of lower and upper bounds of the threshold are chosen to ensure the distribution of class labels is about 1:10 (with the distituent class being the majority). This is done to ensure that the model does not predict constituents aggressively in the case of an equal split. Moreover, from a linguistic standpoint, we can be certain that the distituents must necessarily outnumber the constituents. We treat the outside strings satisfying the upper and lower bounds of the threshold as gold-standard outside of constituents and distituents respectively. As done previously, we fine-tune the outside model on the unlabeled training sentences that generates an outside score sout(i, j) for every span."
    }, {
      "heading" : "4.2.4 Jointly learning with Inside and Outside Models",
      "text" : "Once we have the outside model, we run it on the training sentences and choose the outside string that the classifier is highly confident about. We extract their inside strings again using the same bounds of the threshold as done previously and retrain the inside model on the old highly confident inside strings along with the new inside strings obtained from the highly confident outside strings. Similarly, the same technique can be applied to the outside model to augment its input data too. We repeat this process twice (Figure 3)."
    }, {
      "heading" : "4.3 Evaluation",
      "text" : "We report the F1 score with reference to gold trees in the PTB test set (section 23). Following prior work (Kim et al., 2019a; Shen et al., 2018, 2019; Cao et al., 2020), we remove punctuation and collapse unary chains before evaluation, and calculate F1 ignoring trivial spans, i.e., single-word spans and whole-sentence spans, and we perform the averaging at sentence-level (macro average) rather than span-level (micro average), which means that we compute F1 for each sentence and later average over all sentences. We also mention the oracle\n2We only use the top 5K inside strings for self-training to cover maximum possible iterations as it is representative of the whole training set in terms of the average sentence length and punctuation marks.\nbaseline, which is the highest possible score with binarized trees since we compare them against nonbinarized gold trees according to the convention, as most unsupervised parsing methods output fully binary trees. We additionally use the standard PARSEVAL metric computed by the evalb program.3 Although evalb calculates the micro average F1 score, it differs from our micro average metric in that it will count the whole sentence spans and duplicated spans are calculated and not removed. Following the recommendations put forth by previous work that has done a comprehensive empirical evaluation on this topic (Li et al., 2020b), we report results on both length ≤ 10 as well as all-length test data."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "Table 1 shows the unlabeled F1 scores for our model compared to existing unsupervised parsers on PTB. The vanilla inside model is in itself competitive and is already in the range of previous best models like DIORA (Drozdov et al., 2019), Compound PCFG (Kim et al., 2019a).4 See Appendix\n3https://nlp.cs.nyu.edu/evalb 4We do not include the results of Shi et al. (2021) in our analysis because their boost in the performance is contingent on the nature of the supervision data (especially the QA-SRL dataset) rather than on the actual learning process itself. Furthermore, the authors mention that a vast amount of hyperlinks\nModel CTB Mean Max\nTrivial Baselines:\nLeft Branching (LB) 9.7 Random Trees 15.7 16.0 Right Branching (RB) 20.0\nA.5 to assess our model’s performance on unsupervised labeled parsing.\nWe further evaluate how our method works for languages with different branching types – Chinese (right-branching) and Japanese (left-branching). We use Transformer models for the representations of the spans for both Chinese and Japanese. See Section A.1 in the Appendix for training details. Tables 2 and 3 shows the results for CTB and KTB respectively. Moreover, we do not include a few models chosen previously for PTB during our analysis, as extending those models for CTB or KTB is non-trivial due to several reasons:= such as lack of domain-related datasets (as DIORA uses SNLI and MultiNLI for training), and lack of linguistic knowledge expertise (not easily cross-lingual transferable notion for designing constituency tests).\nFigure 7 in the Appendix shows step-wise qualitative analysis for a sample sentence taken from the PTB training set. See Figures 8 and 9 in Appendix to see the visualization for an example tree at every stage of the pipeline for CTB and KTB respectively. As we can observe from all the example tree outputs, the parser using the inside and outside models after the co-training stage produces fewer crossing brackets than the vanilla inside model."
    }, {
      "heading" : "5.1 Effect of Self-Training",
      "text" : "PLMs that possess rich contextualized textual representations can assist parsing when we have a\nmatch syntactic constituents, hence restricting the scope for the actual algorithm to derive meaningful trees.\nlarge volume of unlabeled data. For this reason, we might expect that self-training in combination with pre-training adds no extra information to the finetuned parser. However, we find that self-training improves the performance of the parser by about 9.8%, demonstrating that self-training provides advantages complementary to the pre-trained contextualized embeddings (see Table 5 in Appendix for a more detailed analysis at different stages)."
    }, {
      "heading" : "5.2 Effect of Co-training",
      "text" : "The question of how to integrate multi-view information is important. One of the options would be to concatenate both the inside and outside vectors while performing training and inference. With this approach, we see negligible improvement. This corroborates the effectiveness of co-training compared with concatenation: the simple concatenation strategy cannot fully harvest the information corresponding to each view and indeed render the optimization intractable. After co-training, the parser achieves 63.1 F1 averaged over four runs, outperforming the previous best-published result (see Table 6 in Appendix to view the improvement at each step). Figure 4 compares the performance of different models over varying sentence length (see Figure 5 in Appendix to understand the extent to which bootstrapping helps compared to the vanilla inside model)."
    }, {
      "heading" : "5.3 Effect of Distituent Selection",
      "text" : "To understand the extent to which the type of the disitituent selection impacts the performance, we assess two settings on the PTB – random and left-\nbranching bias. In the random setting, we select distituents from the slice (start:r), where r is a random number generated between start+1 and end-1, both inclusive. This produces 19.3 F1 for the inside model. Whereas, in the left-branching bias setting, we prepare the seed bootstrapping process as explained in the Section 4.2.1 similar to KTB (a left-branching treebank). This results in 11.2 F1 score for the inside model. Hence, the manner in which we perform the initial classification has a strong impact on the final tree structures."
    }, {
      "heading" : "5.4 Linguistic Error Analysis",
      "text" : "Table 4 shows that our model achieves strong accuracy while predicting all the phrase types except for the Adjective Phrase (ADJP). We list some of the most common mistakes our parse makes and suggest likely explanations for each:\nBracketing inner NP of a definite Noun Phrase. When a definite article is linked with a singular noun, the inner spans need to be shelved, accommodating the larger span with the definite article. E.g.: the [ stock market ]\nGrouping NP too early overlooking broader context. Due to the way it is trained, the parser aggressively groups rare words in the corpus. Building a better outside model can fix this type of error to a considerable extent. Eg: Shearson [ Lehman Hutton ] Inc.\nOmitting conjunction joining two phrases. It shows poor signs of understanding co-ordination cases in which conjunction is an adjacent sibling of the nodes being shifted, or is the leftmost or rightmost node being shifted. E.g.: Notable [ & Quotable ]\nConfusing contractions with Possessives. Due to the presence of a lot of contraction phrases like {they’re, it’s}, the parser confuses it with that of the Possessive NPs, causing unnecessary splitting.\nExpanding the contractions can be a good way to correct these systematic errors. E.g.: the company [ ’s $ 488 million in 1988 ]"
    }, {
      "heading" : "6 Related Work",
      "text" : "Our weakly-supervised parser is comparable in behavior to a fully unsupervised parser as it does not rely on syntactic annotations.\nLearning from distant supervision: A related work to ours (Shi et al., 2021) uses answer fragments and webpage hyperlinks to mine syntactic constituents for parsing. Many previous studies depend on punctuation as a strong signal to detect constituent boundaries (Spitkovsky et al., 2013; Parikh et al., 2014).\nIncorporating bootstrapping techniques: Cotraining (Yarowsky, 1995; Blum and Mitchell, 1998) and self-training (McClosky et al., 2006; Steedman et al., 2003) are bootstrapping methods that attempt to convert a fully unsupervised learning problem to a semi-supervised learning form. More recently, Mohananey et al. (2020); Shi et al. (2020); Steedman et al. (2003) have shown the benefits of using self-training as a standard post-hoc processing step for unsupervised parsing models.\nUsing Inside-Outside representations constructed with a latent tree chart parser: Drawing inspiration from the inside-outside algorithm (Baker, 1979), DIORA (Drozdov et al., 2019) optimizes an autoencoder objective and computes a vector representation for each node in a tree by combining child representations recursively. To recover from errors and make DIORA more robust to local errors when computing the best parse in the bottom-up chart parsing, an improved variant of DIORA, S-DIORA (Drozdov et al., 2020) achieves it.\nInducing tree structure by introducing an induc-\ntive bias to recurrent neural networks: PRPN (Shen et al., 2018) introduces a neural parsing network that has the ability to make differentiable parsing decisions using structured attention mechanism to regulate skip connections in an RNN. ON-LSTM (Shen et al., 2019) enables hidden neurons to learn information by a combination of gating mechanism as well as activation function. In URNNG, Kim et al. (2019b) employs parameterized function over latent trees to handle intractable marginalization and inject strong inductive biases for the unsupervised learning of the recurrent neural network grammar (RNNG) (Dyer et al., 2016). Peng et al. (2019) introduces PaLM that acts as an attention component on top of RNN.\nEnhancing PCFGs: Compound PCFG (Kim et al., 2019a) which consists of a Variational Autoencoder (VAE) with a PCFG decoder, found the original PCFG is fully capable of inducing trees if it uses a neural parameterization. Jin et al. (2019) show that the flow-based PCFG induction model is capable of using morphological and semantic information in context embeddings for grammar induction. Zhu et al. (2020) proposes neural LPCFGs to simultaneously induce both constituents and dependencies.\nConcerning PLMs: Tree Transformer (Wang et al., 2019) adds locality constraints to the Transformer encoder’s self-attention such that the attention heads resemble a tree structure. More recently, Kim et al. (2020) extract trees from pre-trained transformers.\nRefining based on constituency tests: With the help of transformations and RoBERTa model to make grammaticality decisions, (Cao et al., 2020) were able to achieve strong performance for unsupervised parsing."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We propose a simple yet effective method which is the first of its kind in achieving performance comparable to the supervised binary tree RNNG model and setting a new SOTA for unsupervised parsing using weak supervision. Our model generalizes to multiple languages of known treebanks. We have done comprehensive linguistic error analysis showing a step-by-step breakdown of the F1 performance for the inside model versus the insideoutside model with a co-training-based approach. The effectiveness of our multi-view learning strategy is clearly evident in our experiments."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Training Details\nWe use the Adam optimizer and, on the bootstrapped dataset, fine-tune roberta-base with learning rate 3e− 5, batch size 32, epochs 3, maximum sequence length 128, for all our models. The values were chosen as default based on sequence classification tasks on the GLUE benchmark5 as mentioned in HuggingFace Transformers.6 We use a train/validation random split of 80/20 on the bootstrapped dataset to monitor the validation loss and perform early stopping. Note that the development set of PTB is kept untouched. We set the patience value at 3. Model checkpointing, as well as logging, is carried out after every 100 steps. We use a single GPU, Nvidia GeForce RTX 2070 (8GB GDDR5 RAM) to conduct all our experiments. The estimated training time for the inside model is about 0.2h, inside model with self-training (3 iterations) is about 36h, and inside-outside model with cotraining (2 iterations) is about 45h. While the inference time for all the models is roughly 0.2h.\nFor the Chinese monolingual experiment, we use bert-base-chinese which is trained on cased Chinese Simplified and Traditional text, and for Japanese monolingual experiment, we use cl-tohoku/bert-base-japanese which is trained on Japanese Wikipedia available at https://huggingface.co/models.\nTraining Data We tried several strategies to augment the distituent class for our models, but without concrete gains. Some of those include word deletion (randomly selects tokens in the sentence and replace them by a special token), span deletion (Same as word deletion, but puts more focus on deleting consecutive words), reordering (randomly sample several pairs of span and switch them pairwise) and substitution (sample some words and replace them with synonyms).\nA.2 Effect of Bootstrapping\nAs shown in Figure 5, the final model with cotraining identifies constituents from shorter sentences (WSJ-10) much more precisely compared to the rest of the models. There is a lower performance in F1 around sentence length of 50-55 zone,\n5https://gluebenchmark.com/ 6https://huggingface.co/transformers/\nv2.3.0/examples.html#glue\nbut that improves for longer sentences.7\nA.3 Stages of Self-Training Self-training boosts the performance of the inside model by 5.5 F1 points as shown in Table 5.8\nA.4 Stages of Co-Training After co-training, the performance of the insideoutside joint model increases by 1.7 F1 points as shown in Table 6. Compared to using self-training, one of the reasons the benefit is not significant may be attributed to the fact that the inside vectors (built upon Transformer architecture) inherently possesses contextual knowledge due to being trained on a large corpus.\nA.5 Unsupervised Labeled Parsing We explore unsupervised labeled constituency parsing to identify meaningful constituent spans such as Noun Phrases (NP) and Verb Phrases (VP) to see if the parser can extract such labels. Labeled parsing is usually evaluated on whether a span has the correct label. We can effectively induce\n7For evaluating PTB and CTB, we use Yoon Kim’s script available at https://github.com/ harvardnlp/compound-pcfg. Whereas for evaluating KTB, we use Jun Li’s script available at https: //github.com/i-lijun/UnsupConstParseEval.\n8For analysis purposes, we use the test set instead of the standard validation set to avoid tuning on the test set based on feedback received from the validation set to keep the nature of our experiments purely unsupervised.\nspan labels using the clustering of the learned phrase vectors from the inside and outside strings. When labeling a gold bracket, our method achieves 61.2 F1 on the full PTB test set and is comparable with the current best model, DIORA. See Figure 6 to view the visualization of induced and linguistic alignment. RoBERTa does not strictly output word-level vectors. Rather, the output are subword vectors which we aggregate with meanpooling to achieve a word-level representation using SentenceTransformers.9 We use 600 codes while doing the clustering initially, such that we are left with about 25 clusters after the most common label assignment process, i.e., the number of distinct phrase types. The phrase clusters are assigned to {‘NP’: 7, ‘PP’: 5, ‘WHPP’: 3, ‘ADVP’: 3, ‘ADJP’: 2, ‘S’: 2, ‘WHADVP’: 1, ‘UCP’: 1, ’VP’: 1,\n‘PRN’: 1, ‘QP’: 1, ‘SBAR’: 1, ‘WHNP’: 1, ‘CONJP’: 1} according to the majority gold labels in that cluster. These 14 assigned phrase types correspond with the 14 most frequent labels. Table 8 lists the induced non-terminal grouped across different clusters and also their correctness in identifying the gold labels. The further course of action would be to have a joint single model that is capable of achieving both bracketing and labeling. Further, these induced labels can function as features for the inside and outside models to achieve even better predictive ability. It also warrants a multi-lingual exploration in this area.\nA.6 Non-Terminal Label Alignment Figure 6 shows the alignment between gold and induced labels. We observe that some of the induced non-terminals clearly align to linguistic nonterminals. For instance, S-2 non-terminal has a high resemblance with NP. Similarly, S-8 has a high resemblance with ADVP.\n9https://github.com/UKPLab/ sentence-transformers\nDEBUG 0 MAX_ERROR 1 CUTOFF_LEN 10 LABELED 0 DELETE_LABEL_FOR_LENGTH -NONEEQ_LABEL ADVP PRT\nTable 7: The hyperparameters used for evalb .\nS SBAR NP VP PP ADJP ADVP OTHER accuracy\nLABEL\nS-1\nS-2\nS-3\nS-4\nS-5\nS-6\nS-7\nS-8\nS-9\nS-10\nS-11\nS-12\nS-13\nS-14\nC LU\nST ER\n_I D\n0\n20\n40\n60\n80\n100\nFigure 6: Alignment between induced and gold labels of the top-performing clusters. We cluster the constituent inside vectors derived from the ground truth parse (without labels) using the K-Means algorithm and assign each constituent with the most common label within its cluster. Accuracy is the probability of correctly predicting the most common label."
    } ],
    "references" : [ {
      "title" : "Trainable grammars for speech recognition",
      "author" : [ "J.K. Baker." ],
      "venue" : "Speech communication papers presented at th 97th Meeting of the Acoustical Society of America, pages 547–550, Boston, MA.",
      "citeRegEx" : "Baker.,? 1979",
      "shortCiteRegEx" : "Baker.",
      "year" : 1979
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "Avrim Blum", "Tom Mitchell." ],
      "venue" : "Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, page 92–100, New York, NY, USA. Association for Com-",
      "citeRegEx" : "Blum and Mitchell.,? 1998",
      "shortCiteRegEx" : "Blum and Mitchell.",
      "year" : 1998
    }, {
      "title" : "An all-subtrees approach to unsupervised parsing",
      "author" : [ "Rens Bod." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 865–872, Sydney, Aus-",
      "citeRegEx" : "Bod.,? 2006",
      "shortCiteRegEx" : "Bod.",
      "year" : 2006
    }, {
      "title" : "Keyaki treebank: phrase structure with functional information for japanese",
      "author" : [ "Alastair Butler", "Tomoko Hotta", "Ruriko Otomo", "Kei Yoshimoto", "Zhen Zhou", "Hong Zhu." ],
      "venue" : "Proceedings of Text Annotation Workshop.",
      "citeRegEx" : "Butler et al\\.,? 2012",
      "shortCiteRegEx" : "Butler et al\\.",
      "year" : 2012
    }, {
      "title" : "Unsupervised parsing via constituency tests",
      "author" : [ "Steven Cao", "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4798–4808, Online. Association for Computational",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar. Association",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Consistent unsupervised estimators for anchored PCFGs",
      "author" : [ "Alexander Clark", "Nathanaël Fijalkow." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:409–422.",
      "citeRegEx" : "Clark and Fijalkow.,? 2020",
      "shortCiteRegEx" : "Clark and Fijalkow.",
      "year" : 2020
    }, {
      "title" : "Spectral learning of latent-variable PCFGs",
      "author" : [ "Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Cohen et al\\.,? 2012",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2012
    }, {
      "title" : "Experiments with spectral learning of latent-variable PCFGs",
      "author" : [ "Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Cohen et al\\.,? 2013",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders",
      "author" : [ "Andrew Drozdov", "Subendhu Rongali", "Yi-Pei Chen", "Tim O’Gorman", "Mohit Iyyer", "Andrew McCallum" ],
      "venue" : "In Proceedings of the 2020 Con-",
      "citeRegEx" : "Drozdov et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Drozdov et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised labeled parsing with deep inside-outside recursive autoencoders",
      "author" : [ "Andrew Drozdov", "Patrick Verga", "Yi-Pei Chen", "Mohit Iyyer", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Drozdov et al\\.,? 2019",
      "shortCiteRegEx" : "Drozdov et al\\.",
      "year" : 2019
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Assessing bert’s syntactic abilities",
      "author" : [ "Yoav Goldberg." ],
      "venue" : "ArXiv, abs/1901.05287.",
      "citeRegEx" : "Goldberg.,? 2019",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2019
    }, {
      "title" : "Parsing algorithms and metrics",
      "author" : [ "Joshua Goodman." ],
      "venue" : "34th Annual Meeting of the Association for Computational Linguistics, pages 177–183, Santa Cruz, California, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Goodman.,? 1996",
      "shortCiteRegEx" : "Goodman.",
      "year" : 1996
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Deep inside-outside recursive autoencoder with all-span objective",
      "author" : [ "Ruyue Hong", "Jiong Cai", "Kewei Tu." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3610–3615, Barcelona, Spain (Online). Inter-",
      "citeRegEx" : "Hong et al\\.,? 2020",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2020
    }, {
      "title" : "What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy",
      "author" : [ "Ganesh Jawahar", "Benoît Sagot", "Djamé Seddah." ],
      "venue" : "Associa-",
      "citeRegEx" : "Jawahar et al\\.,? 2019",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised learning of PCFGs with normalizing flow",
      "author" : [ "Lifeng Jin", "Finale Doshi-Velez", "Timothy Miller", "Lane Schwartz", "William Schuler." ],
      "venue" : "9",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction",
      "author" : [ "Taeuk Kim", "Jihun Choi", "Daniel Edmiston", "Sang goo Lee." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Compound probabilistic context-free grammars for grammar induction",
      "author" : [ "Yoon Kim", "Chris Dyer", "Alexander Rush." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385, Florence, Italy. Asso-",
      "citeRegEx" : "Kim et al\\.,? 2019a",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised recurrent neural network grammars",
      "author" : [ "Yoon Kim", "Alexander Rush", "Lei Yu", "Adhiguna Kuncoro", "Chris Dyer", "Gábor Melis." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Kim et al\\.,? 2019b",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "A generative constituent-context model for improved grammar induction",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 128–135, Philadelphia, Pennsyl-",
      "citeRegEx" : "Klein and Manning.,? 2002",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2002
    }, {
      "title" : "Natural language grammar induction with a generative constituent-context model",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "Pattern Recognit., 38:1407–1419.",
      "citeRegEx" : "Klein and Manning.,? 2005",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2005
    }, {
      "title" : "Heads-up! unsupervised constituency parsing via self-attention heads",
      "author" : [ "Bowen Li", "Taeuk Kim", "Reinald Kim Amplayo", "Frank Keller." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "An empirical comparison of unsupervised constituency parsing methods",
      "author" : [ "Jun Li", "Yifan Cao", "Jiong Cai", "Yong Jiang", "Kewei Tu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3278–3283, On-",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Building a large annotated corpus of English: The Penn Treebank",
      "author" : [ "Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz." ],
      "venue" : "Computational Linguistics, 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Effective self-training for parsing",
      "author" : [ "David McClosky", "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152– 159, New York City, USA. Association for Compu-",
      "citeRegEx" : "McClosky et al\\.,? 2006",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2006
    }, {
      "title" : "When is self-training effective for parsing? In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561–568, Manchester, UK",
      "author" : [ "David McClosky", "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Coling 2008 Organizing",
      "citeRegEx" : "McClosky et al\\.,? 2008",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2008
    }, {
      "title" : "Self-training for unsupervised parsing with PRPN",
      "author" : [ "Anhad Mohananey", "Katharina Kann", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced",
      "citeRegEx" : "Mohananey et al\\.,? 2020",
      "shortCiteRegEx" : "Mohananey et al\\.",
      "year" : 2020
    }, {
      "title" : "Spectral unsupervised parsing with additive tree metrics",
      "author" : [ "Ankur P. Parikh", "Shay B. Cohen", "Eric P. Xing." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1062–",
      "citeRegEx" : "Parikh et al\\.,? 2014",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2014
    }, {
      "title" : "PaLM: A hybrid parser and language model",
      "author" : [ "Hao Peng", "Roy Schwartz", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural language modeling by jointly learning syntax and lexicon",
      "author" : [ "Yikang Shen", "Zhouhan Lin", "Chin wei Huang", "Aaron Courville." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Ordered neurons: Integrating tree structures into recurrent neural networks",
      "author" : [ "Yikang Shen", "Shawn Tan", "Alessandro Sordoni", "Aaron Courville." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "On the role of supervision in unsupervised constituency parsing",
      "author" : [ "Haoyue Shi", "Karen Livescu", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7611–7621, Online. As-",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning syntax from naturally-occurring",
      "author" : [ "Tianze Shi", "Ozan İrsoy", "Igor Malioutov", "Lillian Lee" ],
      "venue" : null,
      "citeRegEx" : "Shi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2021
    }, {
      "title" : "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2941–2949",
      "author" : [ "bracketings" ],
      "venue" : null,
      "citeRegEx" : "bracketings.,? \\Q2021\\E",
      "shortCiteRegEx" : "bracketings.",
      "year" : 2021
    }, {
      "title" : "Breaking out of local optima with count transforms and model recombination: A study in grammar induction",
      "author" : [ "Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Spitkovsky et al\\.,? 2013",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2013
    }, {
      "title" : "Bootstrapping statistical parsers from small datasets",
      "author" : [ "Mark Steedman", "Miles Osborne", "Anoop Sarkar", "Stephen Clark", "Rebecca Hwa", "Julia Hockenmaier", "Paul Ruhlen", "Steven Baker", "Jeremiah Crim." ],
      "venue" : "10th Conference of the European Chap-",
      "citeRegEx" : "Steedman et al\\.,? 2003",
      "shortCiteRegEx" : "Steedman et al\\.",
      "year" : 2003
    }, {
      "title" : "Tree transformer: Integrating tree structures into self-attention",
      "author" : [ "Yaushian Wang", "Hung-Yi Lee", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "The penn chinese treebank: Phrase structure annotation of a large corpus",
      "author" : [ "Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Martha Palmer." ],
      "venue" : "Nat. Lang. Eng., 11(2):207–238.",
      "citeRegEx" : "Xue et al\\.,? 2005",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2005
    }, {
      "title" : "Unsupervised word sense disambiguation rivaling supervised methods",
      "author" : [ "David Yarowsky." ],
      "venue" : "33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, Massachusetts, USA. Association for Computational",
      "citeRegEx" : "Yarowsky.,? 1995",
      "shortCiteRegEx" : "Yarowsky.",
      "year" : 1995
    }, {
      "title" : "The return of lexical dependencies: Neural lexicalized PCFGs",
      "author" : [ "Hao Zhu", "Yonatan Bisk", "Graham Neubig." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:647–661. 11",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Recent work has shown that PLMs capture different types of linguistic regularities and information, for instance, the lower layers capture phrase-level information which becomes less prominent in the upper layers (Jawahar et al., 2019), span representations constructed from these models can encode rich syntactic phenomena, like the ability to track subject-verb agreement (Goldberg, 2019), dependency trees can be embedded within the geometry of BERT’s hidden states (Hewitt and Manning, 2019), and most relevantly to",
      "startOffset" : 213,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : ", 2019), span representations constructed from these models can encode rich syntactic phenomena, like the ability to track subject-verb agreement (Goldberg, 2019), dependency trees can be embedded within the geometry of BERT’s hidden states (Hewitt and Manning, 2019), and most relevantly to",
      "startOffset" : 146,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : ", 2019), span representations constructed from these models can encode rich syntactic phenomena, like the ability to track subject-verb agreement (Goldberg, 2019), dependency trees can be embedded within the geometry of BERT’s hidden states (Hewitt and Manning, 2019), and most relevantly to",
      "startOffset" : 241,
      "endOffset" : 267
    }, {
      "referenceID" : 39,
      "context" : "this paper, syntactic information via self-att ntion mechanisms (Wang et al., 2019; Kim et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "this paper, syntactic information via self-att ntion mechanisms (Wang et al., 2019; Kim et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : ", 2014, 2013) and unsupervised estimation of probabilistic context-free grammars (PCFGs; Clark and Fijalkow, 2020).",
      "startOffset" : 81,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "With spectral learning for latent-variable PCFGs (L-PCFGs; Cohen et al., 2012) the notion of inside trees versus outside trees is important, but in our case, given that the trees are not present during learning, we have to further specialize it to",
      "startOffset" : 49,
      "endOffset" : 78
    }, {
      "referenceID" : 41,
      "context" : "This can be thought of as a form of co-training (Yarowsky, 1995; Blum and Mitchell, 1998), a training technique that relies on multiple views of training instances.",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "This can be thought of as a form of co-training (Yarowsky, 1995; Blum and Mitchell, 1998), a training technique that relies on multiple views of training instances.",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "When s(i, j) is the probability of a span (i, j) being in the correct tree, this formulation gives the tree with the highest expected number of correct constituents (Goodman, 1996).",
      "startOffset" : 165,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "This formulation has been used recently by several unsupervised constituency parsing algorithms (Kim et al., 2019b,a; Cao et al., 2020; Li et al., 2020a).",
      "startOffset" : 96,
      "endOffset" : 153
    }, {
      "referenceID" : 23,
      "context" : "This formulation has been used recently by several unsupervised constituency parsing algorithms (Kim et al., 2019b,a; Cao et al., 2020; Li et al., 2020a).",
      "startOffset" : 96,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "Both are vectors derived from a PLM (RoBERTa (Liu et al., 2019), as we see later).",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "3 An Iterative Co-training Algorithm Co-training (Blum and Mitchell, 1998) is a classic multi-view training method, which trains a clasInputs: I represents the labeled inside set; U is a set of Unlabeled training sentences;",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 26,
      "context" : "1 Data We evaluate our methodology on the Penn Treebank (PTB; Marcus et al. 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test).",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "To maintain the unsupervised nature of our experiments, we avoid the common practice of using gold parses of the validation set for either early stopping (Shen et al., 2018, 2019; Drozdov et al., 2019) or hyperparameter tuning (Kim et al.",
      "startOffset" : 154,
      "endOffset" : 201
    }, {
      "referenceID" : 40,
      "context" : "1 of the Chinese Penn Treebank (CTB; Xue et al. 2005) with the same splits as in Chen and Manning (2014), and the Japanese Keyaki Treebank (KTB; Butler et al.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "2005) with the same splits as in Chen and Manning (2014), and the Japanese Keyaki Treebank (KTB; Butler et al. 2012).",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 19,
      "context" : "Following prior work (Kim et al., 2019a; Shen et al., 2018, 2019; Cao et al., 2020), we remove punctuation and collapse unary chains before evaluation, and calculate F1 ignoring trivial spans, i.",
      "startOffset" : 21,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "Following prior work (Kim et al., 2019a; Shen et al., 2018, 2019; Cao et al., 2020), we remove punctuation and collapse unary chains before evaluation, and calculate F1 ignoring trivial spans, i.",
      "startOffset" : 21,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "Unsupervised Parsing approaches: PRPN† (Shen et al., 2018) 37.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "(2019a) and take the baseline numbers of certain models from (Kim et al., 2019a; Cao et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "(2019a) and take the baseline numbers of certain models from (Kim et al., 2019a; Cao et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : "Following the recommendations put forth by previous work that has done a comprehensive empirical evaluation on this topic (Li et al., 2020b), we report results on both length ≤ 10 as well as all-length test data.",
      "startOffset" : 122,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "The vanilla inside model is in itself competitive and is already in the range of previous best models like DIORA (Drozdov et al., 2019), Compound PCFG (Kim et al.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 32,
      "context" : "Unsupervised Parsing approaches: PRPN (Shen et al., 2018) 30.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "Unsupervised Parsing approaches: PRPN (Shen et al., 2018) 27.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 35,
      "context" : "Learning from distant supervision: A related work to ours (Shi et al., 2021) uses answer fragments and webpage hyperlinks to mine syntactic constituents for parsing.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 37,
      "context" : "Many previous studies depend on punctuation as a strong signal to detect constituent boundaries (Spitkovsky et al., 2013; Parikh et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 142
    }, {
      "referenceID" : 30,
      "context" : "Many previous studies depend on punctuation as a strong signal to detect constituent boundaries (Spitkovsky et al., 2013; Parikh et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 142
    }, {
      "referenceID" : 41,
      "context" : "Incorporating bootstrapping techniques: Cotraining (Yarowsky, 1995; Blum and Mitchell, 1998) and self-training (McClosky et al.",
      "startOffset" : 51,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Incorporating bootstrapping techniques: Cotraining (Yarowsky, 1995; Blum and Mitchell, 1998) and self-training (McClosky et al.",
      "startOffset" : 51,
      "endOffset" : 92
    }, {
      "referenceID" : 27,
      "context" : "Incorporating bootstrapping techniques: Cotraining (Yarowsky, 1995; Blum and Mitchell, 1998) and self-training (McClosky et al., 2006; Steedman et al., 2003) are bootstrapping methods that attempt to convert a fully unsupervised learning problem to a semi-supervised learning form.",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 38,
      "context" : "Incorporating bootstrapping techniques: Cotraining (Yarowsky, 1995; Blum and Mitchell, 1998) and self-training (McClosky et al., 2006; Steedman et al., 2003) are bootstrapping methods that attempt to convert a fully unsupervised learning problem to a semi-supervised learning form.",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "Using Inside-Outside representations constructed with a latent tree chart parser: Drawing inspiration from the inside-outside algorithm (Baker, 1979), DIORA (Drozdov et al.",
      "startOffset" : 136,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "Using Inside-Outside representations constructed with a latent tree chart parser: Drawing inspiration from the inside-outside algorithm (Baker, 1979), DIORA (Drozdov et al., 2019) optimizes an autoencoder objective and computes a vector representation for each node in a tree by combining child representations recursively.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 9,
      "context" : "To recover from errors and make DIORA more robust to local errors when computing the best parse in the bottom-up chart parsing, an improved variant of DIORA, S-DIORA (Drozdov et al., 2020) achieves it.",
      "startOffset" : 166,
      "endOffset" : 188
    }, {
      "referenceID" : 32,
      "context" : "Inducing tree structure by introducing an inductive bias to recurrent neural networks: PRPN (Shen et al., 2018) introduces a neural parsing network that has the ability to make differentiable parsing decisions using structured attention mechanism to regulate skip connections in an RNN.",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 33,
      "context" : "ON-LSTM (Shen et al., 2019) enables hidden neurons to learn information by a combination of gating mechanism as well as activation function.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "(2019b) employs parameterized function over latent trees to handle intractable marginalization and inject strong inductive biases for the unsupervised learning of the recurrent neural network grammar (RNNG) (Dyer et al., 2016).",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 19,
      "context" : "Enhancing PCFGs: Compound PCFG (Kim et al., 2019a) which consists of a Variational Autoencoder (VAE) with a PCFG decoder, found the original PCFG is fully capable of inducing trees if it uses a neural parameterization.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 39,
      "context" : "Concerning PLMs: Tree Transformer (Wang et al., 2019) adds locality constraints to the Transformer encoder’s self-attention such that the attention heads resemble a tree structure.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "Refining based on constituency tests: With the help of transformations and RoBERTa model to make grammaticality decisions, (Cao et al., 2020) were able to achieve strong performance for unsupervised parsing.",
      "startOffset" : 123,
      "endOffset" : 141
    } ],
    "year" : 0,
    "abstractText" : "We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two classifiers, we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse. A seed bootstrapping technique prepares the data to train these classifiers. Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F1 on the English (PTB) test set. In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results.1",
    "creator" : null
  }
}
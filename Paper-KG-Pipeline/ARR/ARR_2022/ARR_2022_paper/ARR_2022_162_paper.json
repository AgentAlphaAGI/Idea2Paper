{
  "name" : "ARR_2022_162_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we introduce ELECTRA-style tasks (Clark et al., 2020b) to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability."
    }, {
      "heading" : "1 Introduction",
      "text" : "It has become a de facto trend to use a pretrained language model (Devlin et al., 2019; Dong et al., 2019; Yang et al., 2019b; Bao et al., 2020) for downstream NLP tasks. These models are typically pretrained with masked language modeling objectives, which learn to generate the masked tokens of an input sentence. In addition to monolingual representations, the masked language modeling task is effective for learning cross-lingual representations. By only using multilingual corpora, such pretrained models perform well on zero-shot cross-lingual transfer (Devlin et al., 2019; Conneau et al., 2020), i.e., fine-tuning with English training data while directly applying the model to other target languages. The cross-lingual transferability can be further improved by introducing external pre-training tasks using parallel corpus, such as translation language modeling (Conneau and Lample, 2019), and crosslingual contrast (Chi et al., 2021b). However, previous cross-lingual pre-training based on masked language modeling usually requires massive computation resources, rendering such models quite expensive. As shown in Figure 1, our proposed XLM-E achieves a huge speedup compared with well-tuned pretrained models.\nIn this paper, we introduce ELECTRA-style tasks (Clark et al., 2020b) to cross-lingual language model pre-training. Specifically, we present two discriminative pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Rather than recovering masked tokens, the model learns to distinguish the replaced tokens in the corrupted input sequences. The two tasks build input sequences by replacing tokens in multilingual sentences, and translation pairs, respectively. We also describe the pretraining algorithm of our model, XLM-E, which is pretrained with the above two discriminative tasks. It provides a more compute-efficient and sampleefficient way for cross-lingual language model pretraining.\nWe conduct extensive experiments on the XTREME cross-lingual understanding benchmark\nto evaluate and analyze XLM-E. Over seven datasets, our model achieves competitive results with the baseline models, while only using 1% of the computation cost comparing to XLM-R. In addition to the high computational efficiency, our model also shows the cross-lingual transferability that achieves a reasonably low transfer gap. We also show that the discriminative pre-training encourages universal representations, making the text representations better aligned across different languages.\nOur contributions are summarized as follows:\n• We explore ELECTRA-style tasks for crosslingual language model pre-training, and pretrain XLM-E with both multilingual corpus and parallel data.\n• We demonstrate that XLM-E greatly reduces the computation cost of cross-lingual pretraining.\n• We show that discriminative pre-training tends to encourage better cross-lingual transferability."
    }, {
      "heading" : "2 Background: ELECTRA",
      "text" : "ELECTRA (Clark et al., 2020b) introduces the replaced token detection task for language model pre-training, with the goal of distinguishing real input tokens from corrupted tokens. That means the text encoders are pretrained as discriminators rather than generators, which is different from the previous pretrained language models, such as BERT (Devlin et al., 2019), that learn to predict the masked tokens.\nELECTRA trains two Transformer (Vaswani et al., 2017) encoders, serving as generator and discriminator, respectively. The generator G is typically a small BERT model trained with the masked language modeling (MLM; Devlin et al. 2019) task. Consider an input sentence x = {xi}ni=1 containing n tokens. MLM first randomly selects a subset M ⊆ {1, . . . , n} as the positions to be masked, and construct the masked sentence xmasked by replacing tokens inM with [MASK]. Then, the generator predicts the probability distributions of the masked tokens pG(x|xmasked). The loss function of the generator G is:\nLG(x;θG) = − ∑ i∈M log pG(xi|xmasked). (1)\nThe discriminator D is trained with the replaced token detection task. Specifically, the discriminator takes the corrupted sentences xcorrupt as input, which is constructed by replacing the tokens inM with the tokens sampled from the generator G:{\nx corrupt i ∼ pG(xi|xmasked), i ∈M x\ncorrupt i = xi, i 6∈ M\n(2)\nThen, the discriminator predicts whether xcorrupti is original or sampled from the generator. The loss function of the discriminator D is\nLD(x;θD) = − n∑ i=1 log pD(zi|xcorrupt) (3)\nwhere zi represents the label of whether x corrupt i is the original token or the replaced one. The final loss function of ELECTRA is the combined loss of the generator and discriminator losses, LE = LG + λLD.\nCompared to generative pre-training, ELECTRA uses more model parameters and training FLOPs per step, because it contains a generator and a discriminator during pre-training. However, only the discriminator is used for fine-tuning on downstream tasks, so the size of the final checkpoint is similar to BERT-like models in practice."
    }, {
      "heading" : "3 Methods",
      "text" : "Figure 2 shows an overview of the two discriminative tasks used for pre-training XLM-E. Similar to ELECTRA described in Section 2, XLM-E has two Transformer components, i.e., generator and discriminator. The generator predicts the masked tokens given the masked sentence or translation pair, and the discriminator distinguishes whether the tokens are replaced by the generator."
    }, {
      "heading" : "3.1 Pre-training Tasks",
      "text" : "The pre-training tasks of XLM-E are multilingual replaced token detection (MRTD), and translation replaced token detection (TRTD).\nMultilingual Replaced Token Detection The multilingual replaced token detection task requires the model to distinguish real input tokens from corrupted multilingual sentences. Both the generator and the discriminator are shared across languages. The vocabulary is also shared for different languages. The task is the same as in monolingual ELECTRA pre-training (Section 2). The only"
    }, {
      "heading" : "Is original?",
      "text" : "difference is that the input texts can be in various languages.\nWe use uniform masking to produce the corrupted positions. We also tried span masking (Joshi et al., 2019; Bao et al., 2020) in our preliminary experiments. The results indicate that span masking significantly weakens the generator’s prediction accuracy, which in turn harms pre-training.\nTranslation Replaced Token Detection Parallel corpora are easily accessible and proved to be effective for learning cross-lingual language models (Conneau and Lample, 2019; Chi et al., 2021b), while it is under-studied how to improve discriminative pre-training with parallel corpora. We introduce the translation replaced token detection task that aims to distinguish real input tokens from translation pairs. Given an input translation pair, the generator predicts the masked tokens in both languages. Consider an input translation pair (e,f). We construct the input sequence by concatenating the translation pair as a single sentence. The loss function of the generator G is:\nLG(e,f ;θG) =− ∑ i∈Me log pG(ei| [e;f ]masked)\n− ∑ i∈Mf log pG(fi| [e;f ]masked)\nwhere [; ] is the operator of concatenation, and Me,Mf stand for the randomly selected masked positions for e and f , respectively. This loss function is identical to the translation language modeling loss (TLM; Conneau and Lample 2019). The discriminator D learns to distinguish real input tokens from the corrupted translation pair. The corrupted translation pair (ecorrupt,f corrupt) is con-\nstructed by replacing tokens with the tokens sampled from G with the concatenated translation pair as input. Formally, ecorrupt is constructed by{\ne corrupt i ∼ pG(ei| [e;f ] masked), i ∈Me e\ncorrupt i = ei, i 6∈ Me\n(4)\nThe same operation is also used to construct f corrupt. Then, the loss function of the discriminator D can be written as\nLD(e,f ;θD) = − ne+nf∑ i=1 log pD(ri| [e;f ]corrupt)\n(5)\nwhere ri represents the label of whether the i-th input token is the original one or the replaced one. The final loss function of the translation replaced token detection task is LG + λLD."
    }, {
      "heading" : "3.2 Pre-training XLM-E",
      "text" : "The XLM-E model is jointly pretrained with the masked language modeling, translation language modeling, multilingual replaced token detection and the translation replaced token detection tasks. The overall training objective is to minimize\nL = LMLM(x; θG) + LTLM(e,f ; θG) + λLMRTD(x; θD) + λLTRTD(e,f ; θD)\nover large scale multilingual corpus X = {x} and parallel corpus P = {(e,f)}. We jointly pretrain the generator and the discriminator from scratch. Following Clark et al. (2020b), we make the generator smaller to improve the pre-training efficiency."
    }, {
      "heading" : "3.3 Gated Relative Position Bias",
      "text" : "We propose to use gated relative position bias in the self-attention mechanism. Given input tokens {xi}|x|i=1, let {hi} |x| i=1 denote their hidden states in Transformer. The self-attention outputs {h̃i}|x|i=1 are computed via:\nqi,ki,vi = hiW Q,hiW K ,hiW V (6)\naij ∝ exp{ qi · kj√ dk + ri−j} (7)\nh̃i = |x|∑ j=1 aijvi (8)\nwhere ri−j represents gated relative position bias, each hi is linearly projected to a triple of query, key and value using parameter matrices WQ,WK ,WV ∈ Rdh×dk , respectively.\nInspired by the gating mechanism of Gated Recurrent Unit (GRU; Cho et al. 2014), we compute gated relative position bias ri−j via:\ng(update), g(reset) = σ(qi · u), σ(qi · v) r̃i−j = wg (reset)di−j\nri−j = di−j + g (update)di−j + (1− g(update))r̃i−j\nwhere di−j is learnable relative position bias, the vectors u,v ∈ Rdk are parameters, σ is a sigmoid function, and w is a learnable value.\nCompared with relative position bias (Parikh et al., 2016; Raffel et al., 2020; Bao et al., 2020), the proposed gates take the content into consideration, which adaptively adjusts the relative position bias by conditioning on input tokens. Intuitively, the same distance between two tokens tends to play different roles in different languages."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Setup",
      "text" : "Data We use the CC-100 (Conneau et al., 2020) dataset for the replaced token detection task. CC100 contains texts in 100 languages collected from the CommonCrawl dump. We use parallel corpora for the translation replaced token detection task, including translation pairs in 100 languages collected from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), WikiMatrix (Schwenk et al., 2019), and CCAligned (El-Kishky et al., 2020).\nFollowing XLM (Conneau and Lample, 2019), we sample multilingual sentences to balance the\nlanguage distribution. Formally, consider the pretraining corpora in N languages with mj examples for the j-th language. The probability of using an example in the j-th language is\npj = mαj∑N k=1m α k\n(9)\nThe exponent α controls the distribution such that a lower α increases the probability of sampling examples from a low-resource language. In this paper, we set α = 0.7.\nModel We use a Base-size 12-layer Transformer (Vaswani et al., 2017) as the discriminator, with hidden size of 768, and FFN hidden size of 3, 072. The generator is a 4-layer Transformer using the same hidden size as the discriminator (Meng et al., 2021). See Appendix A for more details of model hyperparameters.\nTraining We jointly pretrain the generator and the discriminator of XLM-E from scratch, using the Adam (Kingma and Ba, 2015) optimizer for 125K training steps. We use dynamic batching of approximately 1M tokens for each pre-training task. We set λ, the weight for the discriminator objective to 50. The whole pre-training procedure takes about 1.7 days on 64 Nvidia A100 GPU cards. See Appendix B for more details of pre-training hyperparameters."
    }, {
      "heading" : "4.2 Cross-lingual Understanding",
      "text" : "We evaluate XLM-E on the XTREME (Hu et al., 2020b) benchmark, which is a multilingual multitask benchmark for evaluating cross-lingual understanding. The XTREME benchmark contains seven cross-lingual understanding tasks, namely part-of-speech tagging on the Universal Dependencies v2.5 (Zeman et al., 2019), NER named entity recognition on the Wikiann (Pan et al., 2017; Rahimi et al., 2019) dataset, cross-lingual natural language inference on XNLI (Conneau et al., 2018), cross-lingual paraphrase adversaries from word scrambling (PAWS-X; Yang et al. 2019a), and cross-lingual question answering on MLQA (Lewis et al., 2020), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020a).\nBaselines We compare our XLM-E model with the cross-lingual language models pretrained with multilingual text, i.e., Multilingual BERT (MBERT; Devlin et al. 2019), MT5 (Xue et al., 2021), and XLM-R (Conneau et al., 2020), or\npretrained with both multilingual text and parallel corpora, i.e., XLM (Conneau and Lample, 2019), INFOXLM (Chi et al., 2021b), and XLMALIGN (Chi et al., 2021c). The compared models are all in Base size. In what follows, models are considered as in Base size by default.\nResults We use the cross-lingual transfer setting for the evaluation on XTREME (Hu et al., 2020b), where the models are first fine-tuned with the English training data and then evaluated on the target languages. In Table 1, we report the accuracy, F1, or Exact-Match (EM) scores on the XTREME cross-lingual understanding tasks. The results are averaged over all target languages and five runs with different random seeds. We divide the pretrained models into two categories, i.e., the models pretrained on multilingual corpora, and the models pretrained on both multilingual corpora and parallel corpora. For the first setting, we pretrain XLM-E with only the multilingual replaced token detection task. From the results, it can be observed that XLM-E outperforms previous models on both settings, achieving the averaged scores of 67.6 and 69.3, respectively. Compared to XLM-R, XLM-E (w/o TRTD) produces an absolute 1.2 improvement on average over the seven tasks. For the second setting, compared to XLM-ALIGN, XLM-E produces an absolute 0.4 improvement on average. XLM-E performs better on the question answering tasks and sentence classification tasks while preserving reasonable high F1 scores on structured prediction tasks. Despite the effectiveness of XLM-E, our model requires substantially lower computation cost than XLM-R and XLM-ALIGN. A detailed\nefficiency analysis in presented in Section 4.5."
    }, {
      "heading" : "4.3 Ablation Studies",
      "text" : "For a deeper insight to XLM-E, we conduct ablation experiments where we first remove the TRTD task and then remove the gated relative position bias. Besides, we reimplement XLM that is pretrained with the same pre-training setup with XLM-E, i.e., using the same training steps, learning rate, etc. Table 2 shows the ablation results on XNLI and MLQA. Removing TRTD weakens the performance of XLM-E on both downstream tasks. On this basis, the results on MLQA further decline when removing the gated relative position bias. This demonstrates that XLM-E benefits from both TRTD and the gated relative position bias during pre-training. Besides, XLM-E substantially outperform XLM on both tasks. Notice that when removing the two components from XLM-E, our model only requires a multilingual corpus, but still achieves better performance than XLM, which uses an additional parallel corpus."
    }, {
      "heading" : "4.4 Scaling-up Results",
      "text" : "Scaling-up model size has shown to improve performance on cross-lingual downstream tasks (Xue et al., 2021; Goyal et al., 2021). We study the scalability of XLM-E by pre-training XLM-E models using larger model sizes. We consider two larger model sizes in our experiments, namely Large and XL. Detailed model hyperparameters can be found in Appendix A. As present in Table 3, XLM-EXL achieves the best performance while using significantly fewer parameters than its counterparts. Besides, scaling-up the XLM-E model size consistently improves the results, demonstrating the effectiveness of XLM-E for large-scale pre-training."
    }, {
      "heading" : "4.5 Training Efficiency",
      "text" : "We present a comparison of the pre-training resources, to explore whether XLM-E provides a more compute-efficient and sample-efficient way for pre-training cross-lingual language models. Table 4 compares the XTREME average score, the number of parameters, and the pre-training computation cost. Notice that INFOXLM and XLMALIGN are continue-trained from XLM-R, so the total training FLOPs are accumulated over XLM-R.\nTable 4 shows that XLM-E substantially reduces the computation cost for cross-lingual language model pre-training. Compared to XLM-R and XLM-ALIGN that use at least 9.6e21 training FLOPs, XLM-E only uses 9.5e19 training FLOPs in total while even achieving better XTREME performance than the two baseline models. For the set-\nting of pre-training with only multilingual corpora, XLM-E (w/o TRTD) also outperforms XLM-R using 6.3e19 FLOPs in total. This demonstrates the compute-effectiveness of XLM-E, i.e., XLM-E as a stronger cross-lingual language model requires substantially less computation resource."
    }, {
      "heading" : "4.6 Cross-lingual Alignment",
      "text" : "To explore whether discriminative pre-training improves the resulting cross-lingual representations, we evaluate our model on the sentence-level and word-level alignment tasks, i.e., cross-lingual sentence retrieval and word alignment.\nWe use the Tatoeba (Artetxe and Schwenk, 2019) dataset for the cross-lingual sentence retrieval task, the goal of which is to find translation pairs from the corpora in different languages. Tatoeba consists of English-centric parallel corpora covering 122 languages. Following Chi et al. (2021b) and Hu et al. (2020b), we consider two settings where we use 14 and 36 of the parallel corpora for evaluation, respectively. The sentence representations are obtained by average pooling over hidden vectors from a middle layer. Specifically, we use layer-7 for XLM-R and layer-9 for XLM-E. Then, the translation pairs are induced by the nearest neighbor search using the cosine similarity. Table 5 shows the average accuracy@1 scores under the two settings of Tatoeba for both the xx→ en and en → xx directions. XLM-E achieves 74.4 and 72.3 accuracy scores for Tatoeba-14, and 65.0 and 62.3 accuracy scores for Tatoeba-36, providing notable improvement over XLM-R. XLM-E performs slightly worse than INFOXLM. We believe the cross-lingual contrast (Chi et al., 2021b) task explicitly learns the sentence representations, which makes INFOXLM more effective for the cross-lingual sentence retrieval task.\nFor the word-level alignment, we use the word\nalignment datasets from EuroParl1, WPT20032, and WPT20053, containing 1,244 translation pairs annotated with golden alignments. The predicted alignments are evaluated by alignment error rate (AER; Och and Ney 2003):\nAER = 1− |A ∩ S|+ |A ∩ P | |A|+ |S|\n(10)\nwhere A,S, and P stand for the predicted alignments, the annotated sure alignments, and the annotated possible alignments, respectively. In Table 6 we compare XLM-E with baseline models, i.e., fast align (Dyer et al., 2013), XLM-R, and XLMALIGN. The resulting word alignments are obtained by the optimal transport method (Chi et al., 2021c), where the sentence representations are from the 9-th layer of XLM-E. Over the four language pairs, XLM-E achieves lower AER scores than the baseline models, reducing the average AER from 21.05 to 19.32. It is worth mentioning that our model requires substantial lower computation costs than the other cross-lingual pretrained language models to achieve such low AER scores. See the detailed training efficiency analysis in Section 4.5. It is worth mentioning that XLM-E shows notable improvements over XLM-E (w/o TRTD) on both tasks, demonstrating that the translation replaced token detection task is effective for crosslingual alignment."
    }, {
      "heading" : "4.7 Universal Layer Across Languages",
      "text" : "We evaluate the word-level and sentence-level representations over different layers to explore whether the XLM-E tasks encourage universal representations.\n1www-i6.informatik.rwth-aachen.de/ goldAlignment/\n2web.eecs.umich.edu/˜mihalcea/wpt/ 3web.eecs.umich.edu/˜mihalcea/wpt05/\nAs shown in Figure 3, we illustrate the accuracy@1 scores of XLM-E and XLM-R on Tatoeba cross-lingual sentence retrieval, using sentence representations from different layers. For each layer, the final accuracy score is averaged over all the 36 language pairs in both the xx → en and en → xx directions. From the figure, it can be observed that XLM-E achieves notably higher averaged accuracy scores than XLM-R for the top layers. The results of XLM-E also show a parabolic trend across layers, i.e., the accuracy continuously increases before a specific layer and then continuously drops. This trend is also found in other crosslingual language models such as XLM-R and XLMAlign (Jalili Sabet et al., 2020; Chi et al., 2021c). Different from XLM-R that achieves the highest accuracy of 54.42 at layer-7, XLM-E pushes it to layer-9, achieving an accuracy of 63.66. At layer10, XLM-R only obtains an accuracy of 43.34 while XLM-E holds the accuracy score as high as 57.14.\nFigure 4 shows the averaged alignment error rate (AER) scores of XLM-E and XLM-R on the word alignment task. We use the hidden vectors from\ndifferent layers to perform word alignment, where layer-0 stands for the embedding layer. The final AER scores are averaged over the four test sets in different languages. Figure 4 shows a similar trend to that in Figure 3, where XLM-E not only provides substantial performance improvements over XLM-R, but also pushes the best-performance layer to a higher layer, i.e., the model obtains the best performance at layer-9 rather than a lower layer such as layer-7.\nOn both tasks, XLM-E shows good performance for the top layers, even though both XLM-E and XLM-R use the Transformer (Vaswani et al., 2017) architecture. Compared to the masked language modeling task that encourages the top layers to be language-specific, discriminative pre-training makes XLM-E producing better-aligned text representations at the top layers. It indicates that the cross-lingual discriminative pre-training encourages universal representations inside the model."
    }, {
      "heading" : "4.8 Cross-lingual Transfer Gap",
      "text" : "We analyze the cross-lingual transfer gap (Hu et al., 2020b) of the pretrained cross-lingual language models. The transfer gap score is the difference between performance on the English test set and the average performance on the test set in other languages. This score suggests how much end task knowledge has not been transferred to other languages after fine-tuning. A lower gap score indicates better cross-lingual transferability. Table 7 compares the cross-lingual transfer gap scores on five of the XTREME tasks. We notice that XLM-E obtains the lowest gap score only on PAWS-X. Nonetheless, it still achieves reasonably low gap scores on the other tasks with such low computation cost, demonstrating the cross-lingual transferability of XLM-E. We believe that it is more difficult to achieve the same low gap scores when the model obtains better performance."
    }, {
      "heading" : "5 Related Work",
      "text" : "Learning self-supervised tasks on large-scale multilingual texts has proven to be effective for pretraining cross-lingual language models. Masked language modeling (MLM; Devlin et al. 2019) is typically used to learn cross-lingual encoders such as multilingual BERT (mBERT; Devlin et al. 2019) and XLM-R (Conneau et al., 2020). The crosslingual language models can be further improved by introducing external pre-training tasks using parallel corpora. XLM (Conneau and Lample, 2019) introduces the translation language modeling (TLM) task that predicts masked tokens from concatenated translation pairs. ALM (Yang et al., 2020) utilizes translation pairs to construct codeswitched sequences as input. InfoXLM (Chi et al., 2021b) considers an input translation pair as crosslingual views of the same meaning, and proposes a cross-lingual contrastive learning task. Several pre-training tasks utilize the token-level alignments in parallel data to improve cross-lingual language models (Cao et al., 2020; Zhao et al., 2021; Hu et al., 2020a; Chi et al., 2021c).\nIn addition, parallel data are also employed for cross-lingual sequence-to-sequence pre-training. XNLG (Chi et al., 2020) presents cross-lingual masked language modeling and cross-lingual autoencoding for cross-lingual natural language generation, and achieves the cross-lingual transfer for NLG tasks. VECO (Luo et al., 2020) utilizes crossattention MLM to pretrain a variable cross-lingual language model for both NLU and NLG. mT6 (Chi et al., 2021a) improves mT5 (Xue et al., 2021) by learning the translation span corruption task on parallel data. ∆LM (Ma et al., 2021) proposes to align pretrained multilingual encoders to improve cross-lingual sequence-to-sequence pre-training."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduce XLM-E, a cross-lingual language model pretrained by ELECTRA-style tasks. Specifically, we present two pre-training tasks, i.e., multilingual replaced token detection, and translation replaced token detection. XLM-E outperforms baseline models on cross-lingual understanding tasks although using much less computation cost. In addition to improved performance and computational efficiency, we also show that XLM-E obtains the cross-lingual transferability with a reasonably low transfer gap."
    }, {
      "heading" : "A Model Hyperparameters",
      "text" : "Table 8 and Table 9 shows the model hyperparameters of XLM-E in the sizes of Base, Large, and XL. For the Base-size model, we use the same vocabulary with XLM-R (Conneau et al., 2020) that consists of 250K subwords tokenized by SentencePiece (Kudo and Richardson, 2018). For the models in Large size and XL size, we use VoCap (Zheng et al., 2021) to allocate a 500K vocabulary for models in Large size and XL size."
    }, {
      "heading" : "B Hyperparameters for Pre-Training",
      "text" : "As shown in Table 10, we present the hyperparameters for pre-training XLM-E. We use the batch size of 1M tokens for each pre-training task. In multilingual replaced token detection, a batch is constructed by 2,048 length-512 input sequences, while the input length is dynamically set as the length of the original translation pairs in translation replaced token detection."
    }, {
      "heading" : "C Hyperparameters for Fine-Tuning",
      "text" : "In Table 11, we report the hyperparameters for finetuning XLM-E on the XTREME end tasks."
    } ],
    "references" : [ {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7(0):597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "UniLMv2: Pseudo-masked language models for unified language model pre-training",
      "author" : [ "Hangbo Bao", "Li Dong", "Furu Wei", "Wenhui Wang", "Nan Yang", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Songhao Piao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual alignment of contextual word representations",
      "author" : [ "Steven Cao", "Nikita Kitaev", "Dan Klein." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "mT6: Multilingual pretrained text-to-text transformer with translation pairs",
      "author" : [ "Zewen Chi", "Li Dong", "Shuming Ma", "Shaohan Huang", "Xian-Ling Mao", "Heyan Huang", "Furu Wei." ],
      "venue" : "arXiv preprint arXiv:2104.08692.",
      "citeRegEx" : "Chi et al\\.,? 2021a",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "Cross-lingual natural language generation via pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Wenhui Wang", "XianLing Mao", "Heyan Huang." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, New York, NY, USA, February",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou." ],
      "venue" : "Pro-",
      "citeRegEx" : "Chi et al\\.,? 2021b",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving pretrained cross-lingual language models via self-labeled word alignment",
      "author" : [ "Zewen Chi", "Li Dong", "Bo Zheng", "Shaohan Huang", "XianLing Mao", "Heyan Huang", "Furu Wei." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association",
      "citeRegEx" : "Chi et al\\.,? 2021c",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
      "author" : [ "Jonathan H. Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transactions of the",
      "citeRegEx" : "Clark et al\\.,? 2020a",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020b",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 7057–7067. Curran Associates, Inc.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple, fast, and effective reparameterization of ibm model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A Smith." ],
      "venue" : "Proceedings of the 2013 9",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "CCAligned: A massive collection of cross-lingual web-document pairs",
      "author" : [ "Ahmed El-Kishky", "Vishrav Chaudhary", "Francisco Guzmán", "Philipp Koehn." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "El.Kishky et al\\.,? 2020",
      "shortCiteRegEx" : "El.Kishky et al\\.",
      "year" : 2020
    }, {
      "title" : "Larger-scale transformers for multilingual masked language modeling",
      "author" : [ "Naman Goyal", "Jingfei Du", "Myle Ott", "Giri Anantharaman", "Alexis Conneau." ],
      "venue" : "arXiv preprint arXiv:2105.00572.",
      "citeRegEx" : "Goyal et al\\.,? 2021",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2021
    }, {
      "title" : "Explicit alignment objectives for multilingual bidirectional encoders",
      "author" : [ "Junjie Hu", "Melvin Johnson", "Orhan Firat", "Aditya Siddhant", "Graham Neubig." ],
      "venue" : "arXiv preprint arXiv:2010.07972.",
      "citeRegEx" : "Hu et al\\.,? 2020a",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "arXiv preprint arXiv:2003.11080.",
      "citeRegEx" : "Hu et al\\.,? 2020b",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings",
      "author" : [ "Masoud Jalili Sabet", "Philipp Dufter", "François Yvon", "Hinrich Schütze." ],
      "venue" : "Findings of the Association for Computational Linguis-",
      "citeRegEx" : "Sabet et al\\.,? 2020",
      "shortCiteRegEx" : "Sabet et al\\.",
      "year" : 2020
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "arXiv preprint arXiv:1907.10529.",
      "citeRegEx" : "Joshi et al\\.,? 2019",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, San Diego, CA.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "The IIT Bombay English-Hindi parallel corpus",
      "author" : [ "Anoop Kunchukuttan", "Pratik Mehta", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, Miyazaki, Japan. European Language",
      "citeRegEx" : "Kunchukuttan et al\\.,? 2018",
      "shortCiteRegEx" : "Kunchukuttan et al\\.",
      "year" : 2018
    }, {
      "title" : "MLQA: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "In",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "VECO: Variable encoder-decoder pre-training for cross-lingual understanding and generation",
      "author" : [ "Fuli Luo", "Wei Wang", "Jiahao Liu", "Yijia Liu", "Bin Bi", "Songfang Huang", "Fei Huang", "Luo Si." ],
      "venue" : "arXiv preprint arXiv:2010.16046.",
      "citeRegEx" : "Luo et al\\.,? 2020",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2020
    }, {
      "title" : "DeltaLM: Encoder-decoder pre-training for language generation and translation by augmenting",
      "author" : [ "Shuming Ma", "Li Dong", "Shaohan Huang", "Dongdong Zhang", "Alexandre Muzio", "Saksham Singhal", "Hany Hassan Awadalla", "Xia Song", "Furu Wei" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2021
    }, {
      "title" : "COCO-LM: Correcting and contrasting text sequences for language model pretraining",
      "author" : [ "Yu Meng", "Chenyan Xiong", "Payal Bajaj", "Saurabh Tiwary", "Paul Bennett", "Jiawei Han", "Xia Song." ],
      "venue" : "arXiv preprint arXiv:2102.08473.",
      "citeRegEx" : "Meng et al\\.,? 2021",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2021
    }, {
      "title" : "A systematic comparison of various statistical alignment models",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Computational linguistics, 29(1):19–51.",
      "citeRegEx" : "Och and Ney.,? 2003",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2003
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "A decomposable attention model for natural language inference",
      "author" : [ "Ankur Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249–2255,",
      "citeRegEx" : "Parikh et al\\.,? 2016",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia",
      "author" : [ "Holger Schwenk", "Vishrav Chaudhary", "Shuo Sun", "Hongyu Gong", "Francisco Guzmán." ],
      "venue" : "arXiv preprint arXiv:1907.05791.",
      "citeRegEx" : "Schwenk et al\\.,? 2019",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel data, tools and interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation, pages 2214–2218, Istanbul, Turkey. European Language Resources Association.",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008. Curran As-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Xue et al\\.,? 2021",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "Alternating language modeling for cross-lingual pre-training",
      "author" : [ "Jian Yang", "Shuming Ma", "Dongdong Zhang", "Shuangzhi Wu", "Zhoujun Li", "Ming Zhou." ],
      "venue" : "Thirty-Fourth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
      "author" : [ "Yinfei Yang", "Yuan Zhang", "Chris Tar", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Yang et al\\.,? 2019a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Cur-",
      "citeRegEx" : "Yang et al\\.,? 2019b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal dependencies 2.5. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL)",
      "author" : [ "Daniel Zeman", "Joakim Nivre", "Mitchell Abrams" ],
      "venue" : "Faculty of Mathematics and Physics,",
      "citeRegEx" : "Zeman et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zeman et al\\.",
      "year" : 2019
    }, {
      "title" : "Inducing language-agnostic multilingual representations",
      "author" : [ "Wei Zhao", "Steffen Eger", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, pages 229–240, Online.",
      "citeRegEx" : "Zhao et al\\.,? 2021",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "Allocating large vocabulary capacity",
      "author" : [ "Bo Zheng", "Li Dong", "Shaohan Huang", "Saksham Singhal", "Wanxiang Che", "Ting Liu", "Xia Song", "Furu Wei" ],
      "venue" : null,
      "citeRegEx" : "Zheng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2021
    }, {
      "title" : "The united nations parallel corpus v1",
      "author" : [ "Michał Ziemski", "Marcin Junczys-Dowmunt", "Bruno Pouliquen." ],
      "venue" : "0. In LREC, pages 3530–3534. 11",
      "citeRegEx" : "Ziemski et al\\.,? 2016",
      "shortCiteRegEx" : "Ziemski et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In this paper, we introduce ELECTRA-style tasks (Clark et al., 2020b) to cross-lingual language model pre-training.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "It has become a de facto trend to use a pretrained language model (Devlin et al., 2019; Dong et al., 2019; Yang et al., 2019b; Bao et al., 2020) for downstream NLP tasks.",
      "startOffset" : 66,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "It has become a de facto trend to use a pretrained language model (Devlin et al., 2019; Dong et al., 2019; Yang et al., 2019b; Bao et al., 2020) for downstream NLP tasks.",
      "startOffset" : 66,
      "endOffset" : 144
    }, {
      "referenceID" : 41,
      "context" : "It has become a de facto trend to use a pretrained language model (Devlin et al., 2019; Dong et al., 2019; Yang et al., 2019b; Bao et al., 2020) for downstream NLP tasks.",
      "startOffset" : 66,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "It has become a de facto trend to use a pretrained language model (Devlin et al., 2019; Dong et al., 2019; Yang et al., 2019b; Bao et al., 2020) for downstream NLP tasks.",
      "startOffset" : 66,
      "endOffset" : 144
    }, {
      "referenceID" : 14,
      "context" : "By only using multilingual corpora, such pretrained models perform well on zero-shot cross-lingual transfer (Devlin et al., 2019; Conneau et al., 2020), i.",
      "startOffset" : 108,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "By only using multilingual corpora, such pretrained models perform well on zero-shot cross-lingual transfer (Devlin et al., 2019; Conneau et al., 2020), i.",
      "startOffset" : 108,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "The cross-lingual transferability can be further improved by introducing external pre-training tasks using parallel corpus, such as translation language modeling (Conneau and Lample, 2019), and crosslingual contrast (Chi et al.",
      "startOffset" : 162,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "The cross-lingual transferability can be further improved by introducing external pre-training tasks using parallel corpus, such as translation language modeling (Conneau and Lample, 2019), and crosslingual contrast (Chi et al., 2021b).",
      "startOffset" : 216,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "We also present XLM-R (Conneau et al., 2020), InfoXLM (Chi et al.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : ", 2020), InfoXLM (Chi et al., 2021b), and XLMAlign (Chi et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we introduce ELECTRA-style tasks (Clark et al., 2020b) to cross-lingual language model pre-training.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "ELECTRA (Clark et al., 2020b) introduces the replaced token detection task for language model pre-training, with the goal of distinguishing real input tokens from corrupted tokens.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "That means the text encoders are pretrained as discriminators rather than generators, which is different from the previous pretrained language models, such as BERT (Devlin et al., 2019), that learn to predict the masked tokens.",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 37,
      "context" : "ELECTRA trains two Transformer (Vaswani et al., 2017) encoders, serving as generator and discriminator, respectively.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "The generator G is typically a small BERT model trained with the masked language modeling (MLM; Devlin et al. 2019) task.",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "We also tried span masking (Joshi et al., 2019; Bao et al., 2020) in our preliminary experiments.",
      "startOffset" : 27,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "We also tried span masking (Joshi et al., 2019; Bao et al., 2020) in our preliminary experiments.",
      "startOffset" : 27,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Translation Replaced Token Detection Parallel corpora are easily accessible and proved to be effective for learning cross-lingual language models (Conneau and Lample, 2019; Chi et al., 2021b), while it is under-studied how to improve discriminative pre-training with parallel corpora.",
      "startOffset" : 146,
      "endOffset" : 191
    }, {
      "referenceID" : 6,
      "context" : "Translation Replaced Token Detection Parallel corpora are easily accessible and proved to be effective for learning cross-lingual language models (Conneau and Lample, 2019; Chi et al., 2021b), while it is under-studied how to improve discriminative pre-training with parallel corpora.",
      "startOffset" : 146,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "Inspired by the gating mechanism of Gated Recurrent Unit (GRU; Cho et al. 2014), we compute gated relative position bias ri−j via:",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Data We use the CC-100 (Conneau et al., 2020) dataset for the replaced token detection task.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 45,
      "context" : "We use parallel corpora for the translation replaced token detection task, including translation pairs in 100 languages collected from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al.",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 25,
      "context" : ", 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), WikiMatrix (Schwenk et al.",
      "startOffset" : 20,
      "endOffset" : 47
    }, {
      "referenceID" : 36,
      "context" : ", 2018), OPUS (Tiedemann, 2012), WikiMatrix (Schwenk et al.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 35,
      "context" : ", 2018), OPUS (Tiedemann, 2012), WikiMatrix (Schwenk et al., 2019), and CCAligned (El-Kishky et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Following XLM (Conneau and Lample, 2019), we sample multilingual sentences to balance the language distribution.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 37,
      "context" : "Model We use a Base-size 12-layer Transformer (Vaswani et al., 2017) as the discriminator, with hidden size of 768, and FFN hidden size of 3, 072.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "former using the same hidden size as the discriminator (Meng et al., 2021).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "Training We jointly pretrain the generator and the discriminator of XLM-E from scratch, using the Adam (Kingma and Ba, 2015) optimizer for 125K training steps.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "We evaluate XLM-E on the XTREME (Hu et al., 2020b) benchmark, which is a multilingual multitask benchmark for evaluating cross-lingual understanding.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 42,
      "context" : "5 (Zeman et al., 2019), NER named entity recognition on the Wikiann (Pan et al.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 31,
      "context" : ", 2019), NER named entity recognition on the Wikiann (Pan et al., 2017; Rahimi et al., 2019) dataset, cross-lingual natural language inference on XNLI (Conneau et al.",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : ", 2019), NER named entity recognition on the Wikiann (Pan et al., 2017; Rahimi et al., 2019) dataset, cross-lingual natural language inference on XNLI (Conneau et al.",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : ", 2019) dataset, cross-lingual natural language inference on XNLI (Conneau et al., 2018), cross-lingual paraphrase adversaries from word scrambling (PAWS-X; Yang et al.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 40,
      "context" : ", 2018), cross-lingual paraphrase adversaries from word scrambling (PAWS-X; Yang et al. 2019a), and cross-lingual question answering on MLQA (Lewis et al.",
      "startOffset" : 67,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "2019a), and cross-lingual question answering on MLQA (Lewis et al., 2020), XQuAD (Artetxe et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : ", 2020), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : ", Multilingual BERT (MBERT; Devlin et al. 2019), MT5 (Xue et al.",
      "startOffset" : 20,
      "endOffset" : 47
    }, {
      "referenceID" : 38,
      "context" : "2019), MT5 (Xue et al., 2021), and XLM-R (Conneau et al.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "2019), INFOXLM (Chi et al., 2021b), and XLMALIGN (Chi et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "Results We use the cross-lingual transfer setting for the evaluation on XTREME (Hu et al., 2020b), where the models are first fine-tuned with the English training data and then evaluated on the target languages.",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 38,
      "context" : "Scaling-up model size has shown to improve performance on cross-lingual downstream tasks (Xue et al., 2021; Goyal et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : "Scaling-up model size has shown to improve performance on cross-lingual downstream tasks (Xue et al., 2021; Goyal et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "We use the Tatoeba (Artetxe and Schwenk, 2019) dataset for the cross-lingual sentence retrieval task,",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "We believe the cross-lingual contrast (Chi et al., 2021b) task explicitly learns the sentence representations, which makes INFOXLM more effective for the cross-lingual sentence retrieval task.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : ", fast align (Dyer et al., 2013), XLM-R, and XLMALIGN.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "The resulting word alignments are obtained by the optimal transport method (Chi et al., 2021c), where the sentence representations are from the 9-th layer of XLM-E.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "This trend is also found in other crosslingual language models such as XLM-R and XLMAlign (Jalili Sabet et al., 2020; Chi et al., 2021c).",
      "startOffset" : 90,
      "endOffset" : 136
    }, {
      "referenceID" : 37,
      "context" : "On both tasks, XLM-E shows good performance for the top layers, even though both XLM-E and XLM-R use the Transformer (Vaswani et al., 2017) architecture.",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "We analyze the cross-lingual transfer gap (Hu et al., 2020b) of the pretrained cross-lingual language models.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "Masked language modeling (MLM; Devlin et al. 2019) is typically used to learn cross-lingual encoders such as multilingual BERT (mBERT; Devlin et al.",
      "startOffset" : 25,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "2019) is typically used to learn cross-lingual encoders such as multilingual BERT (mBERT; Devlin et al. 2019) and XLM-R (Conneau et al.",
      "startOffset" : 82,
      "endOffset" : 109
    }, {
      "referenceID" : 39,
      "context" : "ALM (Yang et al., 2020) utilizes translation pairs to construct codeswitched sequences as input.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "InfoXLM (Chi et al., 2021b) considers an input translation pair as crosslingual views of the same meaning, and proposes a cross-lingual contrastive learning task.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "XNLG (Chi et al., 2020) presents cross-lingual masked language modeling and cross-lingual autoencoding for cross-lingual natural language generation, and achieves the cross-lingual transfer for NLG tasks.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 27,
      "context" : "VECO (Luo et al., 2020) utilizes crossattention MLM to pretrain a variable cross-lingual language model for both NLU and NLG.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 38,
      "context" : ", 2021a) improves mT5 (Xue et al., 2021) by learning the translation span corruption task on parallel data.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "∆LM (Ma et al., 2021) proposes to align pretrained multilingual encoders to improve cross-lingual sequence-to-sequence pre-training.",
      "startOffset" : 4,
      "endOffset" : 21
    } ],
    "year" : 0,
    "abstractText" : "In this paper, we introduce ELECTRA-style tasks (Clark et al., 2020b) to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_204_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hey AI, Can You Solve Complex Tasks by Talking to Agents?",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A common research avenue pursued these days is to train monolithic language models with billions of parameters to solve every language understanding and reasoning challenge. In contrast, humans often tackle complex tasks by breaking them down into simpler sub-tasks, and solving these by interacting with other people or automated agents whose skillsets we are familiar with. This approach allows us to learn to solve new complex tasks quickly and effectively, by building upon what’s already known. Can AI systems learn to do the same?\nTo facilitate research in this direction, we propose a new reasoning challenge and a benchmark called COMMAQA where, in addition to the usual\nend-task supervision, one has access to a set of predefined AI agents with examples of their natural language inputs.1 Importantly, the target end-task is designed to be too difficult for current models to learn based only on end-task supervision. The goal is instead to build models that learn to solve the target task by decomposing it into sub-tasks solvable by these agents, and interacting with these agents in natural language to do so.\nAs a motivating example, consider the interaction depicted in Figure 1 where a system is asked to buy a book series with a certain property. The system breaks this goal down, using agent-1 (here Google Assistant) to identify the referenced book series as well as the list of books in that series, and then using agent-2 (here Amazon Alexa) to make the purchase. While both of these agents interact with the system in natural language, they have notably different skill sets, rely on privately held knowledge sources, and have been built at an enormous cost. At the same time, neither agent by itself can accomplish the original goal.\nAn alternative to building such a system that in1Our benchmark will be released upon publication.\nteracts with existing agents is to teach all requisite sub-tasks and skills to a large black-box system, say via multi-task learning (Khashabi et al., 2020; Gupta et al., 2021). This, however, is not only wasteful in terms of time and resources, but often also infeasible. For example, agents such as Google Assistant and OpenAI GPT-3 use private knowledge resources and are computationally expensive to train even once. It would thus be nearly impossible to build a single system with the capabilities of both of these agents.\nWe note that agents need not be sophisticated AI assistants. An agent may simply be a previously developed question-answering (QA) model, a math module, a function of textual input, an image captioning system—anything the community already knows how to build. The goal is to learn to leverage existing agents for more complex tasks.\nTo enable the development of general systems for this task, we identify the minimal inputs that must be assumed for the task to be learnable— training data for the complex task, existing agents that together can solve the complex task, and examples of valid questions that can be asked of these agents (capturing the agents’ capabilities). We build a new synthetic benchmark dataset called COMMAQA (Communicating with agents for QA), containing three complex multihop QA tasks (involving Explicit, Implicit, and Numeric reasoning) and four input QA agents that can solve these tasks.\nWe demonstrate that black-box models struggle on COMMAQA even when provided with auxiliary data, such as domain-relevant agent knowledge. On the other hand, a model that leverages the agents (Khot et al., 2021) can achieve very high accuracy but relies on auxiliary supervision (decomposition annotations). While it is possible to identify valid decompositions using just the endtask labels, the search space is extremely large and naïve approaches, as we show, help only with one of the datasets. COMMAQA thus serves as a new challenge for the NLP community.\nContributions: We (1) propose a new challenge of learning to solve complex tasks by communicating with agents; (2) develop a synthetic multi-hop QA dataset COMMAQA with three reasoning types; (3) provide auxiliary training data and a compositional generalization test set; (4) demonstrate the challenging nature of COMMAQA for black-box models; and (5) show the promise of compositional models that learn to communicate with agents."
    }, {
      "heading" : "2 Related Work",
      "text" : "Semantic Parsing typically focuses on mapping language problems to executable symbolic representation based on a pre-defined grammar (Krishnamurthy et al., 2017; Chen et al., 2020). Similar ideas are also found in the area of program synthesis (Gulwani, 2011; Desai et al., 2016). These goals, like ours, seek to simplify complex problems into simpler executable forms, without relying on explicit intermediate annotation (Clarke et al., 2010; Berant et al., 2013). We, however, diverge from this line by seeking agent communication in free-form language, not bound to any pre-specified set of operations or domain specific languages.\nMulti-hop QA focuses on reasoning with multiple facts. Despite the development of many multihop QA datasets (Khashabi et al., 2018; Yang et al., 2018; Khot et al., 2020; Geva et al., 2021) and models (Min et al., 2019b; Pan et al., 2021), existing benchmarks often contain single-hop shortcuts (Min et al., 2019a), resulting in brittle models (Gardner et al., 2020) and little progress towards true multi-hop reasoning (Trivedi et al., 2020). Additionally these datasets often contain sub-problems not solvable by existing models, further disincentivising the development of compositional models (Khot et al., 2021).\nQuestion Decomposition is used to solve multihop QA but the resulting models (Talmor and Berant, 2018; Min et al., 2019b; Perez et al., 2020; Khot et al., 2021) are often dataset-specific, rely on decomposition annotations, and limited to one or two QA agents. To address these limitations, our proposed challenge covers three dataset types and four agents. Additionally, models are expected to learn to decompose the task by interacting with the agents, rather than relying on human annotations.\nSynthetic Reasoning Challenges have recently been proposed (Lake and Baroni, 2018; Sinha et al., 2019; Clark et al., 2020) to help systematically identify the weaknesses of existing models and inspire modeling innovation (Liu et al., 2021). Our new tasks are unique and focus on simulating complex agent interaction to motivate the development of decomposition-based modeling approaches."
    }, {
      "heading" : "3 Challenge Task Definition",
      "text" : "We formalize the new challenge task of learning to talk with agents to solve complex tasks. To ensure generality of solutions, we identify minimal inputs for the task to be well-defined and learnable.\nFirst we must define fi, the agents or models that solve simpler sub-tasks.2 Minimally, we need to define the space of valid inputs Li for each agent fi, i.e., how can they be invoked. For a system to identify the appropriate agent for each sub-task, we also need to define the capabilities of each agent. Since these agents are often defined for natural language tasks, the space of inputs captures the capabilities of these agents too. For instance, \"Buy the book ‘Harry Potter and the Sorcerer’s Stone’\" captures the Alexa agent’s capability of buying books. Instead of complex formal specifications of the agent’s capabilities, we use natural language inputs as a rich and convenient representation.\nNext, we need a target task T that can be solved via a composition of the capabilities of {fi}.3 Finally, to pose this as a machine learning problem, we need training data D = {(xk, yk)}Nk=1 for T . Since collecting annotations for complex tasks can be difficult, D is expected to be relatively small. Models must therefore use the available agents, instead of learning the complex task from scratch.\nGiven these pre-requisites, we can define the challenge task as follows: Challenge: Learn a model to solve a complex task T , given only: - Training dataset D = {(xk, yk)}Nk=1 for T ; - Agents {f1, . . . , fm} that can help solve T ; - Examples from the space Li of valid inputs for each agent fi that captures its capabilities.\nOne example of this challenge is answering multi-hop questions given two agents: an opendomain TextQA agent f1 and an open-domain TableQA agent f2. Agent f1 can use large textual corpora to answer questions such as \"Who directed Kill Bill?\". Agent f2 can use tables (e.g., Filmography tables) to answer questions such as \"List the movies directed by Quentin Tarantino\". Finally, the training data T for the complex task would contain examples such as (\"What movies has the director of Kill Bill appeared in?\", [\"Reservoir Dogs\", ...,]).\nAuxiliary Information. Apart from the above minimal pre-requisites, in some cases we may be able to obtain additional training supervision, or additional data about the internals of the agents. We emphasize that such auxiliary information may not\n2As mentioned earlier, we use agents to refer interchangeably to models, assistants, or functions that take free-text as input and produce free-text as output.\n3Existing datasets lack this requirement, making it impossible to focus only on the agent communication aspect.\nalways be available (e.g., when using a proprietary agents such as Alexa). A general-purpose system should thus ideally learn to solve this challenge without relying on it. However, it’s acceptable for initial methods to use some auxiliary signals as stepping stones toward more general systems.\nWe consider two kinds of such information— auxiliary supervision for the complex task’s training examples (xk, yk) ∈ D, and auxiliary data about the agents {fi} themselves (not tied to D).\nFor auxiliary supervision, we consider having access to annotated decompositionDk of a complex task training input xk into valid inputs for various agents. We also consider annotated gold facts Fk that could be used to answer xk.\nFor auxiliary data, we consider having access to the training data used to build the agents, or the underlying knowledge base Ki used by them (and possibly even a question-specific relevant subset Kik). In the example above, Ki would be equivalent to the entire text and table corpora used by the agents, and Kik could be the texts and tables relevant to the movie domain. Such information can be used to train a stronger black-box model on the end-task, e.g. fine-tuning on the agent’s training data first or using the gold facts to identify relevant context. These approaches that circumvent the agents are not the target of our dataset, but we nevertheless evaluate them to highlight their limits.\nIn summary, we have the following potential auxiliary information as stepping stones: Auxiliary Supervision for (xk, yk) ∈ D: - Gold Decomposition Dk for xk - Gold Knowledge Fk for xk\nAuxiliary Data for agents {fi}: - Training data Df i = {(uij , vij)} M j=1 for agent fi, where uij ∈ Li and vij = fi(uij) - Complete knowledge resource Ki used by fi, or a manageable subset Kik ⊂ Ki containing Fk"
    }, {
      "heading" : "4 Dataset: COMMAQA Benchmark",
      "text" : "We next propose a new benchmark dataset COMMAQA that enables the development of models that can learn to communicate with existing agents. Specifically, we provide a collection of three synthetic datasets where each question is answerable by talking to simple QA agents.\nWe choose QA as the underlying task and use QA agents for this challenge because the question-answer format can capture a broad range\nof tasks (Gardner et al., 2019) while also naturally surfacing the capability of each agent. For instance, the question \"What are the key frames in v?\" describes a capability of the invoked agent (namely, identifying key frames), in addition to the specific inputs. We next describe our framework for building COMMAQA, which we believe can be extended to other complex tasks, e.g., video summarization."
    }, {
      "heading" : "4.1 Agent Definition",
      "text" : "To define the i-th agent, we build a knowledge base that captures its internal knowledge resource Ki. We use natural language question templates to define the set of questions that this agent can answer over this internal knowledge. For example, given a KB with relations such as \"directed(x, y)\", the agent would answer questions based on the template: \"Who directed the movie __?\"\nKnowledge Base, Ki. To build the knowledge base, we define a KB schema as a set of binary relations between entity types, e.g., director(movie, person). We build a list of entity names that belong to each entity type. To avoid potential conflicts with the LM’s pre-training knowledge, all entity names are generated non-existent words.4\nRather than building a static and very large KB, we sample a possible world independently for each question, by sub-sampling entities for each entity type and then randomly assigning the KB relations between these entities. This prevents memorization of facts across the train and test splits, which in the past has led to over-estimation of QA model performance (Lewis et al., 2021). This also encourages models to learn proper multi-hop reasoning using the agents, rather than memorizing answers.\n4https://www.thisworddoesnotexist.com/\nExamples of Valid Inputs. To define the space of valid inputs for each agent fi, we define a set of question templates that can be answered by it over Kik (e.g., Who directed __?). We construct questions corresponding to a relation in both directions, e.g., \"Who all directed __?\" and \"For which movies was __ a director?\". To emulate redundancy in natural language, we specify multiple phrasings for the same question. We use these templates to generate examples of valid inputs in Li by grounding them with entities of the appropriate entity type (e.g., Who directed Kill Bill?).\nTo ensure generalization to a broad set of tasks, we do not limit the questions to only single span answers. Depending on the question, the agent can produce answers as a single string (span, boolean or a number), a list of strings (e.g., \"Which movies did Spielberg direct?\"), or a map (e.g., \"What are the states and their capitals in USA?\").\nImplementation. To answer the question, agents convert questions into queries against the internal knowledge (based on the templates) which we implement as a symbolic function (written in Python), instead of a model. While a language model might be able to generalize to out-of-distribution variations in language, its behavior can be often unpredictable. By implementing the agents as patternbased functions, we ensure that the resulting systems would stay within the language constraints of each agent and generalize to restricted language models. Additionally, this enables faster development of approaches without spending resources on running a large-scale LM for each agent."
    }, {
      "heading" : "4.2 Complex Task Definition",
      "text" : "Given the space of valid input questions for each agent, we construct training examples for the\ncomplex task using templated theories. These theories consist of a complex question template and a composition rule expressed as a sequence of questions asked to appropriate agents. For example, \"What movies have the directors from $1 directed?\"\n#1 = [textqa] \"Who is from the country $1?\"\n#2 = [tableqa] \"Which movies has #1 directed?\"\nComposition Operators. While this simple theory would work for single span answers, these agents often return list or map answers. Even within this simple example, there can be multiple directors from a given country and this list can not be directly fed to the tableqa model, i.e., \"Which movies has [...] directed?\". This problem gets even more challenging with complex structures. E.g., maintaining a map structure while operating on the values of the map (see 3rd row in Table 1).\nTo handle the different answer structures, we define a special set of compositional operators in Table 1. These operators take agent fi, a structured answer a, and a query with a placeholder as inputs, and execute a set of queries (as defined by the pseudo-code in Table 1) against fi. These operators are inspired by QDMR (Wolfson et al., 2020), although we modify them to allow us to actually execute these operators. E.g., the \"project\" operator in QDMR with the input \"return directors of #1?\" does not specify how to execute this query or its output format (list or map). Our operation (project) [textqa] \"Who are the directors of #1?\" is more precise as it specifies how to use the answers in #1 to generate a map between movies and their directors.\nWe also define a set of agent-independent data structure transformations in Table 2, e.g., convert a map into a list of its keys. Since longer chains of reasoning are prone to more errors (Fried et al., 2015; Khashabi et al., 2019), we don’t model these simple transformations as additional reasoning steps (both in the definition of the theory and the target decompositions that we want systems to learn). Instead, we concatenate compositional operators with transformations to create about 20 new, combined operators such that the transformations can be applied in addition to an operation in a single step, e.g., projectValues_Values_Flat operation performs the projectValues operation followed by the Values and Flat transformation.\nGiven these operators, the final theory for the above example would look like:\nTransf. Procedure FLAT Flatten list of lists into a single list UNIQUE Return the unique items from a list KEYS Return the list of keys from a map VALUES Return the list of values from a map\nBuilding Examples. Given the KB schema, question templates for each agent and theories, we can now build the examples for the complex task as described in Fig. 2. As described before, we first sample a possible world based on the KB schema. We assign each relation to one of the agents, i.e., only this agent would answer questions about this relation. This captures the multi-modality of knowledge, e.g., movie awards might be described in text or in a table, but a person’s date of birth is most likely described in text. We use the templated theories to construct questions by grounding the placeholders. We select m valid questions5 for each sampled KB such that each theory has equal number of examples across the dataset.\nAs a stepping stone towards solving the full challenge and to evaluate the limits of current baselines given oracle supervision, we provide auxiliary information as mentioned in Sec. 3. Specifically, we provide the gold decomposition Dk for each example xk using the same language as the theories (see Fig. 3). We verbalize each relation to create the underlying knowledge resource Kik used by the agent fi (e.g., relation director(M, P) is converted into \"M was a movie directed by P\" or \"movie: M ; director: P\" depending on the agent assigned to this relation). For each training example, we collect the facts used by each agent in the decomposition and treat these as the gold facts Fk."
    }, {
      "heading" : "4.3 COMMAQA Dataset",
      "text" : "We use the above framework to build three datasets capturing three challenges in multi-hop reasoning.\nCOMMAQA-E: Explicit Decomposition. This dataset consists of multi-hop questions from the movie domain where the reasoning needed to answer the question is Explicitly described in the\n5Has a non-empty answer and up to five answer spans\nquestion itself (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2021). For example, \"What awards have the movies directed by Spielberg won?\". We use a TextQA and TableQA agent where certain relations can be either be expressed in text or table (more details in App. Fig. 5).\nCOMMAQA-I: Implicit Decomposition. This dataset consists of multi-hop questions where the reasoning needed is Implicit (Khot et al., 2020; Geva et al., 2021), for example, \"Did Aristotle use a laptop?\". Inspired by such questions in StrategyQA (Geva et al., 2021), we create this dataset using three agents(TextQA, KBQA and MathQA) with just two question styles: (1) \"What objects has __ likely used?\" and (2) \"What objects has __ helped make?\". However each question has three possible strategies depending on the context (see App. Fig. 6 for more details). This is a deliberate choice as similar sounding questions can have very different strategies in a real world setting, e.g., \"Did Steve Jobs help develop an Iphone?\" vs. \"Did Edison help develop the television?\".\nCOMMAQA-N: Numeric Decomposition. This dataset consists of Numeric (also referred to as discrete) reasoning questions (Dua et al., 2019; Amini et al., 2019) requiring some mathematical operation, in addition to standard reasoning. For example, \"Who threw javelins longer than 5 yards?\". We create this dataset in the sports domain with TextQA, TableQA and MathQA agents (more details in App. Fig. 7).\nDataset Statistics. The final dataset consists of the three QA sub-datasets described above (key statistics in Table 7 in the App.). We have 10K total examples in each dataset with 80%/10%/10% train/dev/test split. To prevent models from guessing answer spans, we introduce more distractors by\nsampling a large number of facts for COMMAQA-E and COMMAQA-I. This results in a larger number of facts in the KB (∼170) and larger length of the KB in these two datasets(∼2500 tokens). Since COMMAQA-N can have derived answers from numeric reasoning and has longer chains (avg #steps 4.7 vs. 2.7 in COMMAQA-E), we do not need a large number of distractor facts (80 facts/KB).\nMetrics. The answer yk to each question xk in COMMAQA is an unordered list of single-word entities.6 Due to the nature of the dataset, a model that performs the desired reasoning should be able to output yk correctly, barring entity permutation. Hence, we use exact match accuracy as the metric.7 The appendix also reports a softer metric, F1 score."
    }, {
      "heading" : "5 Experiments",
      "text" : "We next evaluate state-of-the-art models on COMMAQA, with and without auxiliary information."
    }, {
      "heading" : "5.1 Models",
      "text" : "Models with Access to Agent Knowledge: Given access to the facts associated with each (train or test) question xk, i.e., each agent’s domain-relevant knowledge Kik, the facts can be concatenated together to create a context and frame the challenge as a reading comprehension (RC) task.8 We train two standard black-box models, T5-Large and UnifiedQA-Large (Khashabi et al., 2020),9 to gen-\n6Although not in the current dataset, entities in the unordered list yk may be repeated, i.e., we have a multi-set.\n7Our implementation uses \"exact match\" in the DROP multi-span evaluator, which accounts for entity reordering.\n8We reiterate that it is often unreasonable to expect access to Ki and especially Kik. This model tries to solve COMMAQA without invoking agents, which deviates from the purpose of our benchmark dataset. Nevertheless, we conduct experiments in this setting for completeness.\n9We use T5 models as they can handle longer contexts.\nerate answers10 given the question and context. Models with Fact Supervision: If, in addition to access to the underlying knowledgeKik, we also have the auxiliary supervision for the gold facts Fk, we can use this annotation to train a model to first retrieve a small subset of relevant facts from Kik. We train a RoBERTa-Large model on the gold facts and select the top-scoring facts to produce a shorter context that fits in 512 tokens. In addition to the T5-Large models, we also train a T5-3B model on the QA task given the short context. We, however, did not observe any increase in score when using T5-3B, and thus we did not try even larger models.\nModels with Decomposition Supervision: Given decomposition supervision, we can develop a model that actually solves the task of communicating with the agents. Specifically, we use the Text Modular Network (TMN) framework (Khot et al., 2021) that trains a NextGen model that communicates with the agents. This model is trained to produce the next question (including operation and agent) in a decomposition chain, given the questions and answers so far, which is then executed against the agent to produce the answer for the current step. Additionally this framework samples multiple questions at each step of the chain to search11 for the most likely chain of reasoning. We refer to this model as TMN-S when we use this search and TMN-G when we greedily select the most likely question at each step.\nModels with No Supervision: Finally, we develop a baseline approach that directly targets the challenge task without relying on any auxiliary information. To this end, we generate the training data for NextGen via distant supervision. Specifically, we perform a brute-force search where we\n10We alphabetically sort answers for a deterministic order. 11Score is the sum log likelihood of the generated questions.\nsample l questions at each step for up to o steps.12 The operations are chosen randomly but we only consider the applicable operations (e.g., \"select\" for the first step). We use lexical overlap between the questions in the examples of valid inputs and the complex question to avoid wasteful random sampling.13 We assume all chains that lead to the gold answer represent valid decompositions, and use them to build the training dataset for TMNs. We refer to these generated decompositions as D̂k. More details are deferred to Appendix B."
    }, {
      "heading" : "5.2 Results",
      "text" : "Table 3 reports the accuracy of these four classes of models on the COMMAQA dataset.\nBlack-box models struggle on COMMAQA: Due to the large number of distractors, black-box models struggle to learn the task across all three datasets with average accuracy below 20. The extremely low performance on COMMAQA-E is especially notable, given that the reasoning needed for each question is explicitly described. While these models are able to solve similar datasets (Yang et al., 2018), the low scores on our synthetic dataset with more distractors indicates that they are still unable to truly learn this kind of reasoning.\nFact annotations help but insufficient: The models trained on shorter context (obtained by relying on gold fact training annotation) are able to take advantage of the reduced number of distractors, improving their score to about 45 pts across all datasets. However, even with the larger 3B model, there is no noticeable improvement, indicating 45 pts being roughly a ceiling for these models.\nCOMMAQA can be solved by talking to the agents: The TMN model trained on the gold de-\n12o is set based on the length of the rules in each dataset, i.e., o = 3 for COMMAQA-E, o = 4 for I, o = 7 for N.\n13We also found random generally performed worse.\ncomposition annotations can solve this task. This experiment shows that COMMAQA is noise-free, unambiguous, and solvable by a model that learns to talk to the agents (as designed). Note that greedily selecting the next question results in much lower performance on the two datasets (E and I) that have multiple decompositions for the same question.\nNaïve Search is insufficient: In the final two rows, we evaluate the brute-force search approach to generate training data for TMNs. For COMMAQA-I, we don’t find even a single chain that leads to the gold answer, resulting in no training data. With COMMAQA-E and COMMAQA-N, we do find valid decompositions for a subset of the questions (cf. Table 6 in the Appendix for statistics), but they are insufficient to train an effective NextGen model. Expanding the search to l=20 helps achieve near 100% accuracy on COMMAQAE (with∼700K agent calls). However, we don’t observe any gains on COMMAQA-I and COMMAQAN with even 2M agent calls (see App. C)."
    }, {
      "heading" : "5.3 Compositional Generalization",
      "text" : "We also design compositional generalization test sets COMMAQA-ECG and COMMAQA-NCG. Specifically we create questions using novel composition of queries that have been seen during training but never together in this form. For instance, we create a new question \"What awards have the di-\nrectors of the __ winning movies received?\", given that the model was trained on questions such as \"What awards have the actors of the __ winning movies received?\", \"What movies have the directors from __ directed?\", and \"What movies have people from the country __ acted in?\".\nAs shown in Table 4, all models exhibit a drop in accuracy relative to their score in Table 3, but the compositional model trained on gold decomposition still outperforms black-box models. Our error analysis of TMN-S on COMMAQA-Eidentified this key issue: While TMN-S learns to generalize, it generates questions outside the space of valid agent inputs (e.g., \"Who are the directors in the movie __?\" vs. \"Which movies has __ directed?\")."
    }, {
      "heading" : "6 Closing Remarks",
      "text" : "We motivated a new challenge task of solving complex task by communicating with existing AI agents. Developing approaches for this challenge, we argue, can result in more generalizable and efficient models. Towards this goal, we introduced a new benchmark dataset COMMAQA which involves multi-hop questions with three multi-hop reasoning challenges, all solvable by composing four QA agents. Experiments with state-of-art language models indicated that they struggle to solve COMMAQA, even when provided with agents’ internal knowledge. In contrast, a model that is able to learn to communicate with the agents, albeit using annotated decompositions, is able to solve this task. These results point to the need for and the potential of such approaches, but without reliance on auxiliary annotations, to solve complex tasks.\nCOMMAQA is only one instantiation of our overall framework. One can extend it in many ways, such as using LMs to enrich lexical diversity, emulating the behavior of imperfect real-world agents that even attempt to answer out-of-scope questions, and diversifying to other reasoning types such as Boolean questions where using distant supervision is even harder (Dasigi et al., 2019)."
    }, {
      "heading" : "A Multiple Answers in a Question",
      "text" : "If a question refers to multiple answers, e.g. \"Is #3 a part of #2?\", the operator execution is unclear. To handle such cases, the operator must specify the answer to operate over as a parameter. E.g. (filter(#3)) [mathqa] \"Is #3 a part of #2?\" would filter the answers in #3 whereas (filter(#2)) [mathqa] \"Is #3 a part of #2?\" would filter the answers in #2."
    }, {
      "heading" : "B Search Approach",
      "text" : "We describe the approach used to build the training data D̂ using the simple search technique in more detail. To generate the space of possible decompositions, for each question, we first select f operations from the list of valid operations in Table 5. We only consider these operations as these are the only operators needed for COMMAQA. Note that even with this restricted set of operators, models struggle on COMMAQA-I and COMMAQA-N. Additionally, we only consider the select operation for the first step. For all subsequent steps, we only consider replacements of __ with a previous answer index.\nTo select the questions, we first simplify the space of inputs by converting the questions into Fill-In-The-Blank (FITB) questions by removing the named entities. E.g \"Who was born in 1991?\" is changed to \"Who was born in __?\". This is also a necessary step as the operators need questions with placeholders to handle structured answers. At every step, we expand this pool of questions by replacing the blanks with entities in the complex question and any answer index from the previous steps (e.g. #1, #2 in the third step of a decomposition). To avoid wasteful sampling, we use lexical overlap between questions in this expanded question pool and the input question to identify the top g most relevant questions. The agent associated with each question is tracked throughout this process.\nIn the end, we consider the cross product between the f operations and g questions to produce l = f × g total questions at each steps. These l questions are then executed using the appropriate agent and only the successful questions (i.e. answered by the agent) are considered for the next step. This is the key reason why the search space is much smaller than lo for o reasoning steps.\nTable 6 presents the overall statistics of the search approach."
    }, {
      "heading" : "C Search Cost vs Accuracy",
      "text" : "One could always exhaustively search for all possible decompositions to reproduce the gold decompositions for all the questions. But this would be computationally highly expensive as each call to the agent would often invoke a large-scale LM or a complex AI assistant. To characterize the computational cost of these approaches, we extend the search parameter to include l=15 and l=20 (capped at 5M agent calls) and compute the accuracy of the TMN-S model trained on the resulting dataset (shown in Fig. 4). We can achieve close to 100% accuracy on COMMAQA-E whe the search is sufficiently exhaustive(about 700K model calls) mainly due to the shorter rules and the lexical signal. COMMAQA-I and COMMAQA-N, on the other hand, even with an order of magnitude increase in the number of agent calls, we don’t observe any increase in the model accuracy."
    }, {
      "heading" : "D Black Box Models",
      "text" : "We train the T5 models on each of the three datasets to generate the answer given the question and facts. We format the input sequence as <concatenated facts> Q: <question> A:. Since many of the answers can be multiple spans, we sort14 and concatenate them into a single string with ‘+’ as the separator. As noted in Table 7, the verbalized facts can result in a context over 2K tokens long. We trained T5-Large models on A100 80G GPUs and RTX8000s to train on such a long context. Transformers designed for longer documents (Beltagy et al., 2020; Zaheer et al., 2020) would be able to handle such contexts more efficiently but generally under-perform due to sparse attention. Hence we don’t evaluate them here."
    }, {
      "heading" : "E TMN",
      "text" : "We train a T5-Large model as the NextGen model for TMNs. We used a batch size of 64, lr of 5e6, 5 epochs and warmup of 1000 steps in all our experiments. During inference, we use a beam size of 10 and select 5 questions at each step. We use nucleus sampling with p=0.95 and k=10. For greedy search, we use the same parameters but select one question at each step. We use the sum log likelihood of each generated question as the score of the reasoning chain.\n14To ensure a deterministic order, we sort the answers in alphabetical order."
    } ],
    "references" : [ {
      "title" : "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
      "author" : [ "Aida Amini", "Saadia Gabriel", "Shanchuan Lin", "Rik Koncel-Kedziorski", "Yejin Choi", "Hannaneh Hajishirzi." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Amini et al\\.,? 2019",
      "shortCiteRegEx" : "Amini et al\\.",
      "year" : 2019
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantic Parsing on Freebase from Question-Answer Pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension",
      "author" : [ "Xinyun Chen", "Chen Liang", "Adams Wei Yu", "Denny Zhou", "Dawn Song", "Quoc V. Le." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers as soft reasoners over language",
      "author" : [ "Peter Clark", "Oyvind Tafjord", "Kyle Richardson." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Driving semantic parsing from the world’s response",
      "author" : [ "James Clarke", "Dan Goldwasser", "Ming-Wei Chang", "Dan Roth." ],
      "venue" : "CoNLL, pages 18–27.",
      "citeRegEx" : "Clarke et al\\.,? 2010",
      "shortCiteRegEx" : "Clarke et al\\.",
      "year" : 2010
    }, {
      "title" : "Iterative search for weakly supervised semantic parsing",
      "author" : [ "Pradeep Dasigi", "Matt Gardner", "Shikhar Murty", "Luke Zettlemoyer", "Eduard Hovy." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Dasigi et al\\.,? 2019",
      "shortCiteRegEx" : "Dasigi et al\\.",
      "year" : 2019
    }, {
      "title" : "Program synthesis using natural language",
      "author" : [ "Aditya Desai", "Sumit Gulwani", "Vineet Hingorani", "Nidhi Jain", "Amey Karkare", "Mark Marron", "Subhajit Roy." ],
      "venue" : "Proceedings of the 38th International Conference on Software Engineering, pages 345–",
      "citeRegEx" : "Desai et al\\.,? 2016",
      "shortCiteRegEx" : "Desai et al\\.",
      "year" : 2016
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "Higherorder lexical semantic models for non-factoid answer reranking",
      "author" : [ "Daniel Fried", "Peter A. Jansen", "Gus Hahn-Powell", "Mihai Surdeanu", "Peter E. Clark." ],
      "venue" : "TACL, 3:197–210.",
      "citeRegEx" : "Fried et al\\.,? 2015",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "Sameer Singh", "Noah A. Smith", "Sanjay Subramanian", "Eric Wallace", "Ally Quan Zhang", "Ben Zhou." ],
      "venue" : "Findings of EMNLP.",
      "citeRegEx" : "Singh et al\\.,? 2020",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2020
    }, {
      "title" : "Question answering is a format; when is it useful? ArXiv, abs/1909.11291",
      "author" : [ "Matt Gardner", "Jonathan Berant", "Hannaneh Hajishirzi", "Alon Talmor", "Sewon Min" ],
      "venue" : null,
      "citeRegEx" : "Gardner et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2019
    }, {
      "title" : "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
      "author" : [ "Mor Geva", "Daniel Khashabi", "Elad Segal", "Tushar Khot", "Dan Roth", "Jonathan Berant." ],
      "venue" : "TACL.",
      "citeRegEx" : "Geva et al\\.,? 2021",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2021
    }, {
      "title" : "Automating string processing in spreadsheets using input-output examples",
      "author" : [ "Sumit Gulwani." ],
      "venue" : "ACM Sigplan Notices, 46(1):317–330.",
      "citeRegEx" : "Gulwani.,? 2011",
      "shortCiteRegEx" : "Gulwani.",
      "year" : 2011
    }, {
      "title" : "Towards general purpose vision systems",
      "author" : [ "Tanmay Gupta", "Amita Kamath", "Aniruddha Kembhavi", "Derek Hoiem." ],
      "venue" : "ArXiv, abs/2104.00743.",
      "citeRegEx" : "Gupta et al\\.,? 2021",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2021
    }, {
      "title" : "Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps",
      "author" : [ "Xanh Ho", "A. Nguyen", "Saku Sugawara", "Akiko Aizawa." ],
      "venue" : "COLING.",
      "citeRegEx" : "Ho et al\\.,? 2020",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2020
    }, {
      "title" : "On the possibilities and limitations of multi-hop reasoning under linguistic imperfections",
      "author" : [ "Daniel Khashabi", "Erfan Sadeqi Azer", "Tushar Khot", "Ashish Sabharwal", "Dan Roth." ],
      "venue" : "arXiv: Computation and Language.",
      "citeRegEx" : "Khashabi et al\\.,? 2019",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2019
    }, {
      "title" : "Looking beyond the surface:a challenge set for reading comprehension over multiple sentences",
      "author" : [ "Daniel Khashabi", "Snigdha Chaturvedi", "Michael Roth", "Shyam Upadhyay", "Dan Roth." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Khashabi et al\\.,? 2018",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2018
    }, {
      "title" : "UnifiedQA: Crossing format boundaries with a single QA system",
      "author" : [ "Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabhwaral", "Oyvind Tafjord", "Peter Clark", "Hannaneh Hajishirzi." ],
      "venue" : "Findings of EMNLP.",
      "citeRegEx" : "Khashabi et al\\.,? 2020",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "QASC: A dataset for question answering via sentence composition",
      "author" : [ "Tushar Khot", "Peter Clark", "Michal Guerquin", "Paul Edward Jansen", "Ashish Sabharwal." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Khot et al\\.,? 2020",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2020
    }, {
      "title" : "Text modular networks: Learning to decompose tasks in the language of existing models",
      "author" : [ "Tushar Khot", "Daniel Khashabi", "Kyle Richardson", "Peter Clark", "Ashish Sabharwal." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Khot et al\\.,? 2021",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural semantic parsing with type constraints for semi-structured tables",
      "author" : [ "Jayant Krishnamurthy", "Pradeep Dasigi", "Matt Gardner." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Krishnamurthy et al\\.,? 2017",
      "shortCiteRegEx" : "Krishnamurthy et al\\.",
      "year" : 2017
    }, {
      "title" : "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
      "author" : [ "Brenden Lake", "Marco Baroni." ],
      "venue" : "ICML, pages 2873–2882.",
      "citeRegEx" : "Lake and Baroni.,? 2018",
      "shortCiteRegEx" : "Lake and Baroni.",
      "year" : 2018
    }, {
      "title" : "Question and answer test-train overlap in open-domain question answering datasets",
      "author" : [ "Patrick Lewis", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "EACL.",
      "citeRegEx" : "Lewis et al\\.,? 2021",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2021
    }, {
      "title" : "Can small and synthetic benchmarks drive modeling innovation? a retrospective study of question answering modeling approaches",
      "author" : [ "Nelson F Liu", "Tony Lee", "Robin Jia", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:2102.01065.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Compositional questions do not necessitate multi-hop reasoning",
      "author" : [ "Sewon Min", "Eric Wallace", "Sameer Singh", "Matt Gardner", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "ACL.",
      "citeRegEx" : "Min et al\\.,? 2019a",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-hop reading comprehension through question decomposition and rescoring",
      "author" : [ "Sewon Min", "Victor Zhong", "Luke S. Zettlemoyer", "Hannaneh Hajishirzi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Min et al\\.,? 2019b",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised multi-hop question answering by question generation",
      "author" : [ "Liangming Pan", "Wenhu Chen", "Wenhan Xiong", "MinYen Kan", "William Yang Wang." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Pan et al\\.,? 2021",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised question decomposition for question answering",
      "author" : [ "Ethan Perez", "Patrick Lewis", "Wen-tau Yih", "Kyunghyun Cho", "Douwe Kiela." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Perez et al\\.,? 2020",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2020
    }, {
      "title" : "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
      "author" : [ "Koustuv Sinha", "Shagun Sodhani", "Jin Dong", "Joelle Pineau", "William L Hamilton." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Sinha et al\\.,? 2019",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2019
    }, {
      "title" : "The web as a knowledge-base for answering complex questions",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Talmor and Berant.,? 2018",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2018
    }, {
      "title" : "Is multihop QA in DiRe condition? Measuring and reducing disconnected reasoning",
      "author" : [ "H. Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Trivedi et al\\.,? 2020",
      "shortCiteRegEx" : "Trivedi et al\\.",
      "year" : 2020
    }, {
      "title" : "MuSiQue: Multihop questions via single-hop question composition",
      "author" : [ "H. Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal." ],
      "venue" : "ArXiv, abs/2108.00573.",
      "citeRegEx" : "Trivedi et al\\.,? 2021",
      "shortCiteRegEx" : "Trivedi et al\\.",
      "year" : 2021
    }, {
      "title" : "Break it down: A question understanding benchmark",
      "author" : [ "Tomer Wolfson", "Mor Geva", "Ankit Gupta", "Matt Gardner", "Yoav Goldberg", "Daniel Deutch", "Jonathan Berant." ],
      "venue" : "TACL.",
      "citeRegEx" : "Wolfson et al\\.,? 2020",
      "shortCiteRegEx" : "Wolfson et al\\.",
      "year" : 2020
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "gambilla was invented in 2005",
      "author" : [ "Triclops studied chasmogon in college. flawpack was first invented in the year" ],
      "venue" : "Kapod studied duriel in college. noosecutter is commonly used in the field of blaubrudin.",
      "citeRegEx" : "year,? 1943",
      "shortCiteRegEx" : "year",
      "year" : 1943
    }, {
      "title" : "chickenshaw was invented in 1940",
      "author" : [ "Chaudelaire died in" ],
      "venue" : "Dentalogy works as a scritigraphy. flawpack was first invented in the year 1989. Stoptite was born in 1937.",
      "citeRegEx" : "in,? 1980",
      "shortCiteRegEx" : "in",
      "year" : 1980
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "teracts with existing agents is to teach all requisite sub-tasks and skills to a large black-box system, say via multi-task learning (Khashabi et al., 2020; Gupta et al., 2021).",
      "startOffset" : 133,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : "teracts with existing agents is to teach all requisite sub-tasks and skills to a large black-box system, say via multi-task learning (Khashabi et al., 2020; Gupta et al., 2021).",
      "startOffset" : 133,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "On the other hand, a model that leverages the agents (Khot et al., 2021) can achieve very high accuracy but relies on auxiliary supervision (decomposition annotations).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "Semantic Parsing typically focuses on mapping language problems to executable symbolic representation based on a pre-defined grammar (Krishnamurthy et al., 2017; Chen et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "Semantic Parsing typically focuses on mapping language problems to executable symbolic representation based on a pre-defined grammar (Krishnamurthy et al., 2017; Chen et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : "Similar ideas are also found in the area of program synthesis (Gulwani, 2011; Desai et al., 2016).",
      "startOffset" : 62,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "Similar ideas are also found in the area of program synthesis (Gulwani, 2011; Desai et al., 2016).",
      "startOffset" : 62,
      "endOffset" : 97
    }, {
      "referenceID" : 17,
      "context" : "Despite the development of many multihop QA datasets (Khashabi et al., 2018; Yang et al., 2018; Khot et al., 2020; Geva et al., 2021) and models (Min et al.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 34,
      "context" : "Despite the development of many multihop QA datasets (Khashabi et al., 2018; Yang et al., 2018; Khot et al., 2020; Geva et al., 2021) and models (Min et al.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "Despite the development of many multihop QA datasets (Khashabi et al., 2018; Yang et al., 2018; Khot et al., 2020; Geva et al., 2021) and models (Min et al.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Despite the development of many multihop QA datasets (Khashabi et al., 2018; Yang et al., 2018; Khot et al., 2020; Geva et al., 2021) and models (Min et al.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 26,
      "context" : ", 2021) and models (Min et al., 2019b; Pan et al., 2021), existing benchmarks often contain single-hop short-",
      "startOffset" : 19,
      "endOffset" : 56
    }, {
      "referenceID" : 27,
      "context" : ", 2021) and models (Min et al., 2019b; Pan et al., 2021), existing benchmarks often contain single-hop short-",
      "startOffset" : 19,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "cuts (Min et al., 2019a), resulting in brittle models (Gardner et al.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 31,
      "context" : ", 2020) and little progress towards true multi-hop reasoning (Trivedi et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "ther disincentivising the development of compositional models (Khot et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "Synthetic Reasoning Challenges have recently been proposed (Lake and Baroni, 2018; Sinha et al., 2019; Clark et al., 2020) to help systematically identify the weaknesses of existing models and inspire modeling innovation (Liu et al.",
      "startOffset" : 59,
      "endOffset" : 122
    }, {
      "referenceID" : 29,
      "context" : "Synthetic Reasoning Challenges have recently been proposed (Lake and Baroni, 2018; Sinha et al., 2019; Clark et al., 2020) to help systematically identify the weaknesses of existing models and inspire modeling innovation (Liu et al.",
      "startOffset" : 59,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "Synthetic Reasoning Challenges have recently been proposed (Lake and Baroni, 2018; Sinha et al., 2019; Clark et al., 2020) to help systematically identify the weaknesses of existing models and inspire modeling innovation (Liu et al.",
      "startOffset" : 59,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : ", 2020) to help systematically identify the weaknesses of existing models and inspire modeling innovation (Liu et al., 2021).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "of tasks (Gardner et al., 2019) while also naturally surfacing the capability of each agent.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "This prevents memorization of facts across the train and test splits, which in the past has led to over-estimation of QA model performance (Lewis et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 33,
      "context" : "These operators are inspired by QDMR (Wolfson et al., 2020), although we modify them to allow us to actually execute these operators.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "Since longer chains of reasoning are prone to more errors (Fried et al., 2015; Khashabi et al., 2019), we don’t model these simple transformations as additional reasoning steps (both in the definition of the theory and",
      "startOffset" : 58,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "Since longer chains of reasoning are prone to more errors (Fried et al., 2015; Khashabi et al., 2019), we don’t model these simple transformations as additional reasoning steps (both in the definition of the theory and",
      "startOffset" : 58,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "This dataset consists of multi-hop questions where the reasoning needed is Implicit (Khot et al., 2020; Geva et al., 2021), for example, \"Did Aristotle use a laptop?\".",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : "This dataset consists of multi-hop questions where the reasoning needed is Implicit (Khot et al., 2020; Geva et al., 2021), for example, \"Did Aristotle use a laptop?\".",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : "Inspired by such questions in StrategyQA (Geva et al., 2021), we create this dataset using three agents(TextQA, KBQA and MathQA)",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "as discrete) reasoning questions (Dua et al., 2019; Amini et al., 2019) requiring some mathematical operation, in addition to standard reasoning.",
      "startOffset" : 33,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "as discrete) reasoning questions (Dua et al., 2019; Amini et al., 2019) requiring some mathematical operation, in addition to standard reasoning.",
      "startOffset" : 33,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "8 We train two standard black-box models, T5-Large and UnifiedQA-Large (Khashabi et al., 2020),9 to gen-",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "Specifically, we use the Text Modular Network (TMN) framework (Khot et al., 2021) that trains a NextGen model that commu-",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 34,
      "context" : "While these models are able to solve similar datasets (Yang et al., 2018), the low scores on our synthetic dataset with more distractors indicates that they are still unable to truly learn this kind of reasoning.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "One can extend it in many ways, such as using LMs to enrich lexical diversity, emulating the behavior of imperfect real-world agents that even attempt to answer out-of-scope questions, and diversifying to other reasoning types such as Boolean questions where using distant supervision is even harder (Dasigi et al., 2019).",
      "startOffset" : 300,
      "endOffset" : 321
    } ],
    "year" : 0,
    "abstractText" : "Training giant models from scratch for each complex task is resourceand data-inefficient. To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language. We design a synthetic benchmark, COMMAQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents. For instance, using text and table QA agents to answer questions such as \"Who had the longest javelin throw from USA?\". We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent’s knowledge and gold facts supervision. In contrast, models that learn to communicate with agents outperform black-box models, reaching scores of 100% when given gold decomposition supervision. However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive. We will release COMMAQA, along with a compositional generalization test split, to advance research in this direction.",
    "creator" : null
  }
}
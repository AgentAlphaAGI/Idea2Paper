{
  "name" : "ARR_2022_13_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DIBIMT: A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Lexical ambiguity poses one of the greatest001 challenges in the field of Machine Transla-002 tion. Over the last years, multiple efforts have003 been undertaken to investigate incorrect trans-004 lations caused by the polysemous nature of005 words. As a result, some studies posited that006 models pick up semantic biases existing in007 the training data, thus producing translation008 errors. In this paper, we present DIBIMT,009 the first entirely manually-curated evaluation010 benchmark which enables an extensive study011 of semantic biases in Machine Translation in012 five different language combinations, namely013 English and one of the following languages:014 Chinese, German, Italian, Russian and Span-015 ish. Furthermore, we test state-of-the-art Ma-016 chine Translation systems, both commercial017 and non-commercial, against our new test bed018 and provide a thorough statistical and linguis-019 tic analysis of the results.020\n1 Introduction021\nThe polysemous nature of words poses a long-022 standing challenge in a wide range of Natural Lan-023 guage Processing (NLP) tasks such as Word Sense024 Disambiguation (Navigli, 2009) (WSD), Informa-025 tion Retrieval (Krovetz and Croft, 1992; Krovetz,026 2021) (IR) and Machine Translation (Tang et al.,027 2019; Emelin et al., 2020) (MT).028 In MT, some research works have addressed029 the ability of systems to disambiguate polysemous030 words. For instance, given the sentence Geograph-031 ically speaking, England marches with Scotland,032 the polysemous target word marches unequivocally033 means to have a common frontier and therefore034 a possible translation into Italian could be: Da035 un punto di vista geografico l’Inghilterra confina036 con la Scozia. However, almost all tested MT037 systems propose the following translation: Da un038 punto di vista geografico l’Inghilterra marcia con039 la Scozia in which the verb marciare means to040 walk. This is one of many examples that seem to041\nencourage a deeper performance analysis in sce- 042 narios in which MT systems are required to deal 043 with polysemous words and, specifically, with in- 044 frequent meanings of polysemous words. Although 045 state-of-the-art MT systems, both commercial and 046 non-commercial, achieve impressive BLEU scores 047 on standard benchmarks, in our work we demon- 048 strate that they still present significant limitations 049 when dealing with infrequent word senses, which 050 standard metrics fail to recognize. 051\nIn the last years, attempts have been made to 052 investigate the aforementioned phenomena. In fact, 053 recent studies observed a direct correlation between 054 semantic biases in the training data and semantic 055 errors in translation. However, their findings are 056 limited by the following shortcomings: i) they are 057 not entirely manually-curated; ii) they heavily rely 058 on automatically-generated resources to determine 059 the correctness of a translation; and iii) they do not 060 cover multiple language combinations. 061\nIn this work, we address the illustrated draw- 062 backs and present DIBIMT, to the best of our 063 knowledge, the first fully manually-curated evalua- 064 tion benchmark aimed at investigating the impact 065 of semantic biases in MT in five language combi- 066 nations. Such benchmark allows the community 067 not only to better explore the described phenomena, 068 but also to devise innovative MT systems which 069 better deal with lexical ambiguity. Specifically, the 070 contributions of the present work are threefold: 071\n• We present DIBIMT, a novel gold-quality test 072\nbed for semantic biases in MT that goes be- 073 yond a simple accuracy score, covering five 074 language combinations, namely English and 075 one of the following languages: Chinese, Ger- 076 man, Italian, Russian and Spanish; 077\n• We define four novel metrics that better clarify 078\nthe semantic biases within MT models; 079\n• We provide a thorough statistical and linguis- 080\ntic analysis in which we compare 7 state-of- 081\nthe-art MT systems, both commercial and non-082 commercial, against our new benchmark. Fur-083 thermore, we extensively discuss the results.084\nWe will release DIBIMT as a closed benchmark085 with a public leaderboard.086\n2 Related Work087\nOver the course of the last years, several ap-088 proaches to the evaluation of the lexical choice089 in MT were proposed. To this end, cross-lingual090 datasets were created in which, given a target word091 in context, systems were required to provide the092 translation or a substitute in a target language (Vick-093 rey et al., 2005; Mihalcea et al., 2010; Lefever and094 Hoste, 2013).095 More recently, Gonzales et al. (2017) put for-096 ward ContraWSD, a dataset which includes 7.2K097 instances of lexical ambiguities for German →098 English, and 6.7K for German → French. Such099 dataset pairs every reference translation with a100 set of contrastive examples which contain incor-101 rect translations of a polysemous target word. For102 each instance, the answer provided by systems is103 considered correct if the reference translation is104 scored higher. Based on a denoised version of105 the ContraWSD dataset and focusing on the lan-106 guage combination German → English, Gonzales107 et al. (2018) present the Word Sense Disambigua-108 tion Test Suite which, unlike ContraWSD, eval-109 uates MT output directly rather than by scoring110 translations. The suite consists of a collection of111 3.2K sentence pairs in which the German source112 sentences contain one ambiguous target word. As113 target words, the authors considered only words in114 German whose translation into English does not115 cover multiple senses, thus making the evaluation116 more straightforward. Despite their effectiveness,117 such benchmarks do not allow systems to be tested118 in multiple language combinations, and only cover119\na very limited number of words and senses. To 120 address these limitations, Raganato et al. (2019) 121 proposed MuCoW, an automatically-created test 122 suite covering 16 language pairs with more than 123 200K sentences derived from word-aligned parallel 124 corpora. 125\nFurther research works investigated the disam- 126 biguation capabilities of MT systems by exploring 127 their internal representations (Marvin and Koehn, 128 2018; Michel et al., 2019) or improving them via 129 context-aware word embeddings (Liu et al., 2017). 130 More recently, Emelin et al. (2020) introduced a 131 statistical method for the identification of disam- 132 biguation errors in neural MT (NMT) and demon- 133 strated that models capture data biases within the 134 training corpora, which leads models to produce 135 incorrect translations. Although the authors expect 136 their approach to be transferable to other language 137 combinations, they only focus on German → En- 138 glish. 139\nBased on the findings and open research ques- 140 tions raised in the aforementioned works, the 141 present paper aims at investigating not only the 142 presence, but also, most importantly, the nature 143 and properties of semantic biases in MT in mul- 144 tiple language combinations, via a novel entirely 145 manually-curated benchmark called DIBIMT and 146 a thorough performance analysis. 147\n3 Building DIBIMT 148\nThe DIBIMT benchmark focuses on detecting 149 Word Sense Disambiguation biases in NMT, i.e., 150 biases of certain words towards some of their more 151 frequent meanings. The creation of such a dataset 152 requires i) a set of unambiguous and grammatically- 153 correct sentences containing a polysemous target 154 word; ii) a set of correct and incorrect translations 155 of each target word into the languages to be cov- 156 ered. Figure 1 depicts an example of a dataset item. 157\n3.1 Preliminaries 158\nBabelNet Similarly to previous works, we rely 159 on BabelNet1 (Navigli et al., 2021), a large multi- 160 lingual encyclopedic dictionary whose nodes are 161 concepts represented by synsets, i.e., sets of syn- 162 onyms, containing lexicalizations in multiple lan- 163 guages and coming from various heterogeneous 164 resources, including, inter alia, WordNet and Wik- 165 tionary. Let us define B as an abstraction that al- 166 lows us to query BabelNet, specifically, the subset 167\n1https://babelnet.org\nof synsets in BabelNet that contain at least one168 sense2 from WordNet and one or more senses in169 languages other than English,3 while only consider-170 ing senses coming from high-quality sources, i.e.,171 language-specific WordNet(s).172\nFormal Notation Given a generic synset σ, we173 define ΛL(σ) as the set of lexicalizations of σ174 in language L contained within B. As an exam-175 ple, let us consider the synset σ̃ corresponding176 to the vegetation meaning of the word plant. σ̃177 contains lexicalizations in different languages, in-178 cluding: PflanzeDE , plantEN , floraEN , plantaES ,179 floraES , piantaIT and floraIT . Hence, ΛEN(σ̃) =180 {plant, flora}, while ΛES(σ̃) = {planta, flora}.181 Furthermore, let λP represent a (lemma, part182 of speech) pair, where P is the part of speech.183 We denote ΩL(λP ) = {σ1, . . . , σn} as the set of184 synsets which contain λP as a lexicalization in lan-185 guage L according to B. Additionally, we define186 δL(λP ) = |ΩL(λP )| as the polysemy degree, i.e.,187 the number of senses, of λP .188\n3.2 Sentence Selection Process189\nIn this section, we detail the creation process of our190 dataset, i.e. the selection of our sentences as well191 as the construction and filtering of our items.192\nItem Structure and Notation Before we pro-193 ceed, let us formally state how each item in the194 dataset is structured: given a source sentence195 s = [w1, . . . , wn] as a sequence of words, and196 given a target word4 wi in s tagged with some197 synset σ, we consider X = (s, wi, σ) as an initial198 item of the dataset, i.e., an instance composed of199 an English sentence s, a target word wi and its as-200 sociated synset σ; such instance can be annotated201 for candidate translations of wi in some language202\nL. We also denote λX P as the (lemma, POS) pair of203 wi.204\n3.2.1 Starting Sentence Pool205\nWe collect our initial items from two main sources:206 WordNet (Miller et al., 1990) and Wiktionary.5207\n2A “sense” is a lexicalization of a specific synset in some language. Henceforth, we will refer to lexicalizations and senses interchangeably.\n3Specifically, we consider synsets that have lexicalizations in English, Italian, German, Russian, Spanish and Chinese.\n4For simplicity, we use the term word here, but our work focuses on multi-word expressions as well (both in source and translated sentences).\n5We use the dump of September 2021.\nSpecifically, we use the examples from WordNet 208 Tagged Glosses (Langone et al., 2004), where each 209 sentence’s target word was manually associated 210 with its synset6, gracefully providing the first batch 211 of initial items. 212\nAs for Wiktionary, instead, we start by obtaining 213 every usage example s and its associated defini- 214 tion d (filtering out archaic usages and slang), then, 215 we automatically extract the target words from the 216 corresponding example.7 The only step left to con- 217 struct an initial item is to associate a sense σ with 218 the word wi used in the example s. We perform 219 this association in two phases: first, we try to map 220 the definition d related to the example s to a Babel- 221 Net synset by relying on the mappings available in 222 BabelNet 5 between WordNet and Wiktionary, dis- 223 carding examples for which this association could 224 not be found; second, we manually validate and 225 correct these successful associations to ensure high 226 quality of our initial items. 227\n3.2.2 Sentence Filtering 228\nWe apply a filtering step to the original sentences in 229 order to select examples that are likely to be more 230 challenging for the models to translate: i) we dis- 231 card every sentence X for which δEN(λ X P ) < 3, 232 i.e., we only retain sentences whose associated 233 (lemma, POS) pair has a polysemy degree of at 234 least 3 in BEN; ii) we only retain at most one sen- 235 tence per sense per source8; iii) differently from 236 previous works, which impose a strict requirement 237 on synsets that are monosemous in the target lan- 238 guage, we only require that all their lexicalizations 239 uniquely identify them within all the other possible 240 senses of λX P . 241\nAs an example, let us consider the nominal 242 senses of the word “bank”: among them, one rep- 243 resents a specific aviation maneuver. In Italian, 244 this synset has lexicalizations “sbandamento” and 245 “avvitamento”: although both of these might take 246 on different senses in Italian (e.g., “sbandamento” 247 might represent a “drift” performed with a car), 248 none of them would have “bank” as an English 249 lexicalization, which, for Italian, respects our third 250 condition. If the same holds true for all languages, 251\n6Which we convert from WordNet to BabelNet. 7In Wiktionary, target words are marked in bold inside the example sentence. 8The reasoning for this choice is twofold: on the one hand, oftentimes Wiktionary has multiple examples, for the same synset, that differ in only one or two words, thus we skip them to avoid repetitions; on the other hand, we obtain an increase in sense coverage without worsening the annotator load.\nthe synset passes the test and thus the sentence is252 retained.253\n3.3 Annotating the Dataset254\nAs the set of initial items is ready, we can proceed255 with the annotation phase, which will produce our256 annotated items.257\nFor instance, given a language L and an initial258 item X = (s, wi, σ), we associate a set of good259 (GL) and bad (BL) translation candidates to X ,260 which represent words that we do (and do not)261 expect to see in a translation of sentence s in lan-262 guage L. Finally, we refer to XL as an annotated263 item, i.e., the tuple (s, wi, σ,GL,BL).264 Given the expertise required to carry out this265 task, we rely on the work of three highly qualified266 translators: one for Italian, German and Russian,267 one for Spanish and one for Chinese.268\n3.3.1 Pre-annotation Item Creation269\nBefore moving forward with the annotation phase,270 in a fashion similar to that of previous works based271 on BabelNet, we pre-populate the sets of good (GL)272 and bad (BL) lexicalizations of item X in language273 L with those in B. Formally, we assign GL =274 ΛL(σ), i.e., the set of lemmas in language L of the275 BabelNet synset associated with σ; furthermore,276\nwe set BL = ⋃\nσ̂∈ΩL(λ X P )\\{σ} ΛL(σ̂), i.e., the set277 of all lemmas in language L of BabelNet synsets278 associated with any σ̂ excluding σ. With this step,279 we produce an automatically populated version of280 our annotated items.281\n3.3.2 Annotation Guidelines282\nWe instruct annotators to update the set of good283 (GL) and bad (BL) translated lexicalizations of284 wi ∈ s such that it does reflect what a human trans-285 lator would use in the context of that sentence.9286\nWe also instruct annotators to mark sentences as287 invalid if they are either idiomatic expressions, or288 sentences in which wi represents a proper noun, or289 sentences which do not provide enough context to290 properly disambiguate wi to σ.291\n3.3.3 Resulting Dataset292\nOur annotators went over around 800 sentences,293 discarding 200 of them, finally obtaining 600 an-294 notated items in 5 languages. Due to a coverage295 issue of Russian in BabelNet, we maintain only296\n9Any lexicalization of σ in L that is removed from GL is automatically placed in BL.\nsentences tagged with nominal or verbal synsets. 297 Dataset statistics are reported in Table 1. 298\nWe note that, as expected, the lexicalizations 299 found in B have been substantially refined by our 300 annotators in all languages, as reported in Table 2. 301 Indeed, across languages, on average, 54% of the 302 “good lemmas” are original (i.e., added by our an- 303 notators), while 42% of lemmas coming from B are 304 removed. More importantly, on average, only 55% 305 of the time two sentences target the same sense 306 they also share the same lexicalizations. 307\nThis leads us to a straightforward but important 308 conclusion: depending on the context of the sen- 309 tence, a lexicalization for a given synset might be 310 correct or not. Examined jointly, these metrics sug- 311 gest that relying on synset lexicalizations from Ba- 312 belNet alone is prone to producing mistakes, either 313 from BabelNet’s intrinsic noise or from the lack of 314 different granularity of synsets and contextualized 315 words. 316\n3.4 Analysis Procedure 317\nDIBIMT’s analysis procedure is fairly simple: 318 given an annotated item XL = (s, wi, σ,GL,BL) 319\nand a model M, we compute tL = ML(s), i.e.,320 the translation of s in language L according to M.321 Then, we use Stanza (Qi et al., 2020) to perform322 tokenization, part-of-speech tagging and lemmati-323 zation of tL and, finally, we check if there is any324 match10 between lemmas of the translated sentence325 and those contained in GL or BL. In case there326 is no match, we mark the translation as a MISS;327 otherwise, we mark it as GOOD or BAD depending328 on which set matched the lemma.329\nThis produces an analyzed item, which for sim-330\nplicity we denote as XM L\n= (XL, tL,R, ωL),331 where R is one of GOOD, BAD or MISS and ωL332 represents the matched lemma in case there was a333 match (GOOD or BAD), ǫ otherwise.334\n4 Results and Discussion335\nWe now: i) use DIBIMT to carry out an evalu-336 ation of 7 different machine translation systems;337 ii) report their results, including a thorough sta-338 tistical and linguistic evaluation; iii) extensively339 discuss our findings, providing multiple measures340 of semantic bias; iv) offer some insights on the341 causes of such biases. In the Appendix we include342 a model-specific breakdown of the various scores343 and metrics reported throughout this section.344\n10A more detailed description of the analysis procedure is provided in the Appendix.\n4.1 Compared Systems 345\nWe test a wide range of models, both commercial 346 and non-commercial, and report their performances 347 on DIBIMT’s evaluation metrics: 348\n• DeepL Translator11, a state-of-the-art com- 349\nmercial NMT system (Isabelle et al., 2017). 350\n• Google Translate12, arguably the most popu- 351\nlar commercial NMT system (Johnson et al., 352 2017). 353\n• OPUS (Tiedemann and Thottingal, 2020), the 354\nsmallest state-of-the-art NMT model available 355 to date, a base Transformer (each model has 356 approximately 74M parameters) trained on a 357 single language pair on large amounts of data. 358\n• MBart50 (Tang et al., 2020), multilingual 359\nBART fine-tuned on the translation task for 360 50 languages (610M parameters). We refer to 361 MBart50 as the English-to-many model, and 362 to MBart50MTM as the many-to-many model. 363\n• M2M100 (Fan et al., 2021), a multilin- 364\ngual model able to translate from/to 100 365 languages. We test both versions of the 366 model, the 418M parameter one (which we 367 dub M2M100) and the 1.2B parameter one 368 (dubbed M2M100LG). 369\n4.2 Discussion of MISS 370\nFigure 2 reports general results of the analysis per 371 (model, language) pair. Given the high percentage 372 of analyzed items classified as MISS, we asked our 373 annotators to perform an inspection on a random 374 sample of 70 items per language to unearth the 375 reasons, with varying results. We identified multi- 376 ple causes, namely: word omission in the transla- 377 tion (around 19% of items, mostly in Chinese and 378 Italian); issues with Stanza’s tokenization (around 379 11%, mostly Chinese and Russian) and lemmati- 380 zation (around 12%, mostly Italian and German); 381 words translated as themselves (around 5%, often 382 in multilingual neural models); words translated 383 with terms outside the scope of the source sen- 384 tence13 (around 23%); missing terms from either 385 BL (around 18%) or GL (around 11%). We set out 386 to thoroughly investigate and tackle these issues 387 and translation phenomena as future work. 388\n11https://deepl.com/ 12https://translate.google.com/ 13An example is the sentence he is a crack shot, where the word shot is translated by MBart50 in Italian as “schianto”, which means “someone very good looking”.\n4.3 General Results389\nTable 3 reports accuracy for non-MISS analyzed390 items (i.e., #GOOD#GOOD+#BAD ). With the sole exception391 of DeepL, which largely outperforms every other392 competitor, models achieve extremely low scores,393 in the range of 18%-32%. Surprisingly, Google394 Translate performs worst across languages.395\n4.4 Analyzing the Semantic Biases396\nIn addition to accuracy, DIBIMT analyzes the se-397 mantic biases of a translation model via four novel398 metrics, which we define in detail in what follows.399\nSense Frequency Index Influence We study the400 sensitivity of models to disambiguate senses with401 respect to their frequency. To do that, let us define402 µλP (σ) as the index of synset σ in ΩEN(λP ) or-403\ndered according to WordNet’s sense frequency, as 404 computed from SemCor. That is, index k means 405 that synset σ is the k-th most frequent meaning for 406 λP . 407\nIn Figure 3(a), we plot the number and percent- 408 age of errors made on average by the models, group- 409 ing items by µ λX P\n(σX), where X is a non-MISS 410 analyzed item. As expected, the less frequent a 411 meaning is for a given word, the harder it is for the 412 model to correctly disambiguate it. 413\nFinally, given a (model, language) pair, we de- 414 fine the Sense Frequency Index Influence (SFII) as 415 the average percentage of mistakes, for each group, 416 that we detected. Values are reported in Table 4. In- 417 terestingly, DeepL proves once again to be the best, 418 obtaining a score of 51%, far below the average 419 80% achieved by the other models. 420\nSense Polysemy Degree Importance Similarly421 to SFII, we also study the extent to which the poly-422 semy degree, i.e., how many senses a given word423 can take upon, impacts the models’ disambigua-424 tion capabilities. This experiment mirrors SFII, but425 groups items by their lemma’s polysemy degree426 δEN(λ X P ) instead of µ. Figure 3(b) reports the re-427 sults on all items. Unsurprisingly, much like for the428 frequency index, we observe that higher polysemy429 leads to more errors, confirming that models still430 struggle with very polysemous words. Similarly431 to SFII, SPDI is defined as the average percentage432 of mistakes at varying polysemy degrees, and its433 values are reported in Table 4: once again, DeepL434 outperforms all other systems by a large margin,435 confirming to be the least biased across the board.436\nMost and More Frequent Senses To further cor-437 roborate our findings about semantic biases, we438 study how often models predict senses that are439 more frequent than the target one. Given a BAD440\nanalyzed item XM L , we denote σ̂ as the synset as-441\nsociated with the wrongly translated lemma ωL. 14 442 Then, we check the frequency of σ and σ̂ with443\nrespect to λX P : if µ λX P (σ̂) < µ λX P (σ), then the sys-444 tem’s disambiguation steered towards a sense that445 is more frequent than the target one, which we446 dub More Frequent Sense (MFS+); additionally, if447\nµ λX P (σ̂) = 1, the model disambiguated the source448 word wi to the Most Frequent Sense (MFS) of the449\nassociated lemma λX P . The results of both these450 analyses are reported in Table 5.451 We can observe a few interesting results: first, on452 average, almost 60% of the time a mistake reflects453 the Most Frequent Sense of the target word (second-454 last column); second, almost 90% of the mistakes455 concern translations towards more frequent senses456\n14In case there are multiple possible synsets, we take the most frequent according to µ\nλX P\n, as we need to rely on the\nassumption that the surface form represents the intrinsic disambiguation performed by the NMT system.\nof the target word (last column). Although it might 457 seem straightforward, NMT models are still highly 458 biased towards senses that are more likely to en- 459 counter during training; while this could be related 460 to the pattern-matching nature of neural networks, 461 it also heavily depends on the training data the 462 model was trained upon and needs to be further 463 investigated in future research. 464\n4.5 Is the encoder disambiguating? 465\nWe try to assess to what extent, in a multilin- 466 gual encoder-decoder architecture, the encoder is 467 determining the implicit disambiguation of the 468 source sentence before generating the translation. 469 Given a model M,15 two languages L1 and L2 470\n15We disregard OPUS here as it is a set of bilingual models, rather than a single model capable of translating into multiple\nand an initial item X , we take M’s analyzed471 items XM\nL1 and X M L2 16 and check if translations472 in L1 and L2 have a synset in common, i.e.,473 |ΩL1(ωL1) ∩ ΩL2(ωL2)| > 0. The results of this474 experiment are reported in Figure 4.475\nWe observe that, on average, this phenomenon476 occurs around 70% of the time. Hence, it is safe to477 assume that, while the encoder certainly plays an478 important role in the disambiguation of the input479 sentence, the decoder is also contributing signifi-480 cantly. Another interesting observation is that the481 alphabet of the target language does not seem to482 have any influence, as language pairs involving483 Russian report scores that are very similar to those484 of the other three European languages. We attribute485 lower scores in Chinese to coverage issues in Ba-486 belNet, which would hinder a correct fulfillment of487 the condition defined for this experiment.488\n4.6 How challenging is DIBIMT?489\nGiven the low performances achieved by MT mod-490 els, we test a WSD system on the English sentences491 within DIBIMT, both to assess the toughness of492 our system and to establish an additional baseline.493 We use ESCHER17 (Barba et al., 2021), a state-494 of-the-art model on English WSD. Interestingly,495 ESCHER achieves an overall accuracy score of496 66.33, almost 15 points lower than the results on497 the standard WSD benchmark (80.7 on ALL, Ra-498 ganato et al., 2017), therefore confirming the chal-499 lenging nature of DIBIMT. Furthermore, in order500 to estimate the difference in disambiguation capa-501 bility between NMT models and a dedicated WSD502 system, we compute ESCHER’s performances on503 the set of English sentences of non-MISS analyzed504 items for each (model, language) pair. We report505 these results in Table 6, whose accuracy scores can506 be directly compared to Table 3.507\nlanguages. We also disregard DeepL and Google Translate as their architecture is proprietary.\n16We skip item X if either XML1 or X M L2 is a MISS. 17The publicly available version trained on SemCor only.\nAs expected, the average MT accuracy is signif- 508 icantly lower than ESCHER’s, with the sole ex- 509 ception of DeepL, which manages to surpass it on 510 German and Russian. These results clearly prove 511 that current NMT models are still not on par with 512 dedicated systems, and thus that they might benefit 513 from their inclusion within the NMT ecosystem. 514\n4.7 Is this a decoding issue? 515\nAs a final experiment, we assess whether the seman- 516 tic biases are caused by search errors (i.e., failures 517 of the decoding algorithm) or model errors (i.e., 518 the models deemed their translations the best pos- 519 sible). For each (model M, language L) pair, we 520 sample a BAD translation (tBAD) and ask annota- 521 tors to translate it into L (tGOOD), then compute the 522 perplexities according to M with the correspond- 523 ing English sentence s, i.e., pGOOD = pM(tGOOD|s) 524 and pBAD = pM(tBAD|s). We repeat this sampling 525 50 times per (M, L) pair and check how often 526 pBAD > pGOOD. Table 7 shows that, on average, this 527 happens in 93% of cases, thus confirming that most 528 semantic biases are embedded within models and 529 not caused by the decoding strategy. 530\n5 Conclusions 531\nIn this work, we presented DIBIMT, a novel bench- 532 mark for measuring and understanding semantic 533 biases in NMT, which goes beyond simple accuracy 534 metrics and provides novel metrics that summarize 535 how biased NMT models are. We tested DIBIMT 536 on 7 widely adopted NMT systems, extensively 537 discussing their performances and providing novel 538 insights on the possible causes and relations of se- 539 mantic biases within NMT models. 540\nFurthermore, statistics of our annotations sug- 541 gest that, when dealing with translations, synsets’ 542 lexicalizations cannot be used interchangeably, as 543 their choice heavily depends on the context. 544\nIn the future, we plan to improve DIBIMT’s 545 handling of MISS, widen language coverage and 546 expand the annotations. 547\nReferences548\nEdoardo Barba, Tommaso Pasini, and Roberto Navigli.549 2021. Esc: Redesigning wsd with extractive sense550 comprehension. In Proceedings of the 2021 Con-551 ference of the North American Chapter of the Asso-552 ciation for Computational Linguistics: Human Lan-553 guage Technologies, pages 4661–4672.554\nDenis Emelin, Ivan Titov, and Rico Sennrich. 2020.555 Detecting word sense disambiguation biases in ma-556 chine translation for model-agnostic adversarial at-557 tacks. arXiv preprint arXiv:2011.01846.558\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi559 Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep560 Baines, Onur Celebi, Guillaume Wenzek, Vishrav561 Chaudhary, et al. 2021. Beyond english-centric mul-562 tilingual machine translation. Journal of Machine563 Learning Research, 22(107):1–48.564\nAnnette Rios Gonzales, Laura Mascarell, and Rico Sen-565 nrich. 2017. Improving word sense disambigua-566 tion in neural machine translation with sense embed-567 dings. In Proceedings of the Second Conference on568 Machine Translation, pages 11–19.569\nAnnette Rios Gonzales, Mathias Müller, and Rico Sen-570 nrich. 2018. The word sense disambiguation test571 suite at wmt18. In Proceedings of the Third Confer-572 ence on Machine Translation: Shared Task Papers,573 pages 588–596.574\nPierre Isabelle, Colin Cherry, and George Foster. 2017.575 A challenge set approach to evaluating machine576 translation. In Proceedings of the 2017 Conference577 on Empirical Methods in Natural Language Process-578 ing, pages 2486–2496, Copenhagen, Denmark. As-579 sociation for Computational Linguistics.580\nMelvin Johnson, Mike Schuster, Quoc Le, Maxim581 Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,582 Fernanda Viégas, Martin Wattenberg, Greg Corrado,583 et al. 2017. Google’s multilingual neural machine584 translation system: Enabling zero-shot translation.585 Transactions of the Association for Computational586 Linguistics, 5:339–351.587\nRobert Krovetz. 2021. Lexical acquisition and infor-588 mation retrieval. In Lexical Acquisition: Exploiting589 On-Line Resources to Build a Lexicon, pages 45–64.590 Psychology Press.591\nRobert Krovetz and W Bruce Croft. 1992. Lexical592 ambiguity and information retrieval. ACM Trans-593 actions on Information Systems (TOIS), 10(2):115–594 141.595\nHelen Langone, Benjamin R Haskell, and George A596 Miller. 2004. Annotating WordNet. In Proc. of the597 Workshop Frontiers in Corpus Annotation.598\nEls Lefever and Véronique Hoste. 2013. Semeval-2013599 task 10: Cross-lingual word sense disambiguation.600 In Second Joint Conference on Lexical and Compu-601 tational Semantics (* SEM), Volume 2: Proceedings602\nof the Seventh International Workshop on Semantic 603 Evaluation (SemEval 2013), pages 158–166. 604\nFrederick Liu, Han Lu, and Graham Neubig. 2017. 605 Handling homographs in neural machine translation. 606 arXiv preprint arXiv:1708.06510. 607\nRebecca Marvin and Philipp Koehn. 2018. Exploring 608 word sense disambiguation abilities of neural ma- 609 chine translation systems (non-archival extended ab- 610 stract). In Proceedings of the 13th Conference of the 611 Association for Machine Translation in the Americas 612 (Volume 1: Research Track), pages 125–131. 613\nPaul Michel, Xian Li, Graham Neubig, and 614 Juan Miguel Pino. 2019. On evaluation of ad- 615 versarial perturbations for sequence-to-sequence 616 models. arXiv preprint arXiv:1903.06620. 617\nRada Mihalcea, Ravi Sinha, and Diana McCarthy. 618 2010. Semeval-2010 task 2: Cross-lingual lexical 619 substitution. In Proceedings of the 5th international 620 workshop on semantic evaluation, pages 9–14. 621\nGeorge A Miller, Richard Beckwith, Christiane Fell- 622 baum, Derek Gross, and Katherine J Miller. 1990. 623 Introduction to wordnet: An on-line lexical database. 624 International journal of lexicography, 3(4):235– 625 244. 626\nRoberto Navigli. 2009. Word sense disambiguation: A 627 survey. ACM computing surveys (CSUR), 41(2):1– 628 69. 629\nRoberto Navigli, Michele Bevilacqua, Simone Conia, 630 Dario Montagnini, and Francesco Cecconi. 2021. 631 Ten years of babelnet: A survey. In Proceedings of 632 the Thirtieth International Joint Conference on Arti- 633 ficial Intelligence, IJCAI-21, pages 4559–4567. 634\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, 635 and Christopher D. Manning. 2020. Stanza: A 636 Python natural language processing toolkit for many 637 human languages. In Proceedings of the 58th An- 638 nual Meeting of the Association for Computational 639 Linguistics: System Demonstrations. 640\nAlessandro Raganato, Jose Camacho-Collados, and 641 Roberto Navigli. 2017. Word sense disambiguation: 642 A unified evaluation framework and empirical com- 643 parison. In Proceedings of the 15th Conference of 644 the European Chapter of the Association for Compu- 645 tational Linguistics: Volume 1, Long Papers, pages 646 99–110, Valencia, Spain. Association for Computa- 647 tional Linguistics. 648\nAlessandro Raganato, Yves Scherrer, and Jörg Tiede- 649 mann. 2019. The mucow test suite at wmt 2019: Au- 650 tomatically harvested multilingual contrastive word 651 sense disambiguation test sets for machine transla- 652 tion. In Proceedings of the Fourth Conference on 653 Machine Translation (Volume 2: Shared Task Papers, 654 Day 1), pages 470–480. 655\nGongbo Tang, Rico Sennrich, and Joakim Nivre.656 2019. Encoders help you disambiguate word senses657 in neural machine translation. arXiv preprint658 arXiv:1908.11771.659\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-660 man Goyal, Vishrav Chaudhary, Jiatao Gu, and An-661 gela Fan. 2020. Multilingual translation with exten-662 sible multilingual pretraining and finetuning. arXiv663 preprint arXiv:2008.00401.664\nJörg Tiedemann and Santhosh Thottingal. 2020.665 OPUS-MT — Building open translation services for666 the World. In Proceedings of the 22nd Annual Con-667 ferenec of the European Association for Machine668 Translation (EAMT), Lisbon, Portugal.669\nDavid Vickrey, Luke Biewald, Marc Teyssier, and670 Daphne Koller. 2005. Word-sense disambiguation671 for machine translation. In Proceedings of human672 language technology conference and conference on673 empirical methods in natural language processing,674 pages 771–778.675\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien676 Chaumond, Clement Delangue, Anthony Moi, Pier-677 ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-678 icz, Joe Davison, Sam Shleifer, Patrick von Platen,679 Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,680 Teven Le Scao, Sylvain Gugger, Mariama Drame,681 Quentin Lhoest, and Alexander M. Rush. 2020.682 Transformers: State-of-the-art natural language pro-683 cessing. In Proceedings of the 2020 Conference on684 Empirical Methods in Natural Language Processing:685 System Demonstrations, pages 38–45, Online. Asso-686 ciation for Computational Linguistics.687\nA Analysis Procedure Details688\nOur analysis procedure, which we described in689 Section 3.4, involves steps that go beyond simple690 lemma matching. For instance, in case of multi-691 word expressions, we allowed annotators to specify692 a wildcard, i.e., any number of tokens (including693 zero) were allowed to expand and still trigger a694 match. Additionally, since stanza has multi-word695 expansion tokenization for some of the languages696 in our list, when available, we try to perform match-697 ing on both the list of words (alongside the list of698 tokens) in the translated sentence. Finally, in case699 no match is produced by the aforementioned steps,700 we apply a surface-level string matching heuris-701 tic which, especially in Chinese, helps us increase702 coverage.703\nB Neural Models Implementation704\nWe use HuggingFace’s Transformers library (Wolf705 et al., 2020) for all neural models. As per stan-706 dard practice, we generate translations using beam707 search as decoding algorithm with beam size 5.708\nC Model-specific Analyses709\nWe include model-specific analyses with per-710 language breakdown of the scores achieved on our711 benchmark. The column named ESCHER repre-712 sents the scores of the WSD system on the subset713 of sentences of that model in that language, and714 should be treated as an additional baseline to com-715 pare with the Accuracy achieved by the system.716 Everything is detailed in Section 4 of the paper.717\n• DeepL718\n• Google719\n• OPUS720\n• M2M100721\n• M2M100-LG722\n• MBart50723\n• MBart50-MTM724\nDeepL725\nBack to Model-specific Analyses.726\n%MISS %mistakes MFS MFS+ SFII SPDI ESCHER\nDE 36.07 74.33 53.12 84.38 28.90 35.86 66.86 ES 25.22 57.74 59.56 87.98 46.21 56.10 67.89 IT 20.38 53.39 68.08 86.38 49.22 57.75 66.67 RU 35.21 71.24 50.46 83.49 33.78 42.06 66.76 ZH 31.86 46.00 49.07 88.89 59.58 64.97 68.42\nMean 29.75 60.54 56.06 86.22 43.54 51.35 67.32\n2 4 6 8 10 12 14 0\n50\n100\n150\n0\n0.2\n0.4\n0.6\n0.8\n1\n#i te m s\n(a) Sense Frequency Index\n0 10 20 30 40 0\n50\n100\n150\n0\n0.2\n0.4\n0.6\n0.8\n1\n% m is ta ke s\n(b) Sense Polysemy Degree\nOPUS729\nBack to Model-specific Analyses.730\n%MISS Accuracy MFS MFS+ SPDI SFII ESCHER\nDE 37.84 27.99 56.98 87.92 76.25 79.85 66.95 ES 25.65 36.57 64.23 91.24 69.15 74.89 66.83 IT 29.11 29.95 64.48 89.66 72.02 80.59 65.82 RU 45.84 40.44 48.95 84.21 69.50 68.68 69.21 ZH 38.31 27.75 51.71 87.45 75.66 79.96 69.88\nMean 35.35 32.54 57.27 88.10 72.52 76.79 67.74\nM2M100-LG733\nBack to Model-specific Analyses.734\n%MISS Accuracy MFS MFS+ SPDI SFII ESCHER\nDE 42.02 26.96 59.13 87.30 74.71 78.90 67.18 ES 36.69 29.92 61.54 88.08 73.89 79.89 66.86 IT 37.07 25.14 63.18 88.81 76.10 78.69 68.50 RU 42.50 34.31 45.98 84.38 70.01 75.01 67.69 ZH 39.73 22.35 59.35 92.45 82.17 88.79 69.82\nMean 39.60 27.73 57.84 88.20 75.38 80.26 68.01\nMBart50-MTM737\nBack to Model-specific Analyses.738"
    } ],
    "references" : [ {
      "title" : "Beyond english-centric mul",
      "author" : [ "Chaudhary" ],
      "venue" : null,
      "citeRegEx" : "Chaudhary,? \\Q2021\\E",
      "shortCiteRegEx" : "Chaudhary",
      "year" : 2021
    }, {
      "title" : "Lexical acquisition and infor",
      "author" : [ "Robert Krovetz" ],
      "venue" : null,
      "citeRegEx" : "Krovetz.,? \\Q2021\\E",
      "shortCiteRegEx" : "Krovetz.",
      "year" : 2021
    }, {
      "title" : "Annotating WordNet",
      "author" : [ "Miller." ],
      "venue" : "Proc. of the",
      "citeRegEx" : "Miller.,? 2004",
      "shortCiteRegEx" : "Miller.",
      "year" : 2004
    }, {
      "title" : "Word sense disambiguation: A",
      "author" : [ "Roberto Navigli" ],
      "venue" : null,
      "citeRegEx" : "Navigli.,? \\Q2009\\E",
      "shortCiteRegEx" : "Navigli.",
      "year" : 2009
    }, {
      "title" : "Multilingual translation with exten",
      "author" : [ "gela Fan" ],
      "venue" : null,
      "citeRegEx" : "Fan.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fan.",
      "year" : 2020
    }, {
      "title" : "Word-sense disambiguation",
      "author" : [ "Daphne Koller" ],
      "venue" : null,
      "citeRegEx" : "Koller.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koller.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The polysemous nature of words poses a long022 standing challenge in a wide range of Natural Lan023 guage Processing (NLP) tasks such as Word Sense 024 Disambiguation (Navigli, 2009) (WSD), Informa025 tion Retrieval (Krovetz and Croft, 1992; Krovetz, 026 2021) (IR) and Machine Translation (Tang et al.",
      "startOffset" : 167,
      "endOffset" : 182
    } ],
    "year" : 0,
    "abstractText" : "Lexical ambiguity poses one of the greatest 001 challenges in the field of Machine Transla002 tion. Over the last years, multiple efforts have 003 been undertaken to investigate incorrect trans004 lations caused by the polysemous nature of 005 words. As a result, some studies posited that 006 models pick up semantic biases existing in 007 the training data, thus producing translation 008 errors. In this paper, we present DIBIMT, 009 the first entirely manually-curated evaluation 010 benchmark which enables an extensive study 011 of semantic biases in Machine Translation in 012 five different language combinations, namely 013 English and one of the following languages: 014 Chinese, German, Italian, Russian and Span015 ish. Furthermore, we test state-of-the-art Ma016 chine Translation systems, both commercial 017 and non-commercial, against our new test bed 018 and provide a thorough statistical and linguis019 tic analysis of the results. 020",
    "creator" : null
  }
}
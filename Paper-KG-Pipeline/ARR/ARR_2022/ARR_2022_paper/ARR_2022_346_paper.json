{
  "name" : "ARR_2022_346_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "How do we answer complex questions: Discourse structure of long form answers",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "While many information seeking questions can be answered by a short text span, requiring a short span answer significantly limits the types of questions that can be addressed as well as the extent of information that can be conveyed. Recent work (Fan et al., 2019; Krishna et al., 2021) explored long form answers, where answers can be free-form texts consisting of multiple sentences. Their multi-sentence nature leads to interesting and nuanced discourse within the answers, where the answerer can provide information, hedge, explain, provide examples, point to other sources, and more. Answerers can flexibly structure and organize these elements to provide a coherent, concise answer.\nThe complexity and flexibility of long form answers pose fresh challenges to the evaluation of long form question answering systems, in stark contrast to short span-based answers where match-\ning spans (Rajpurkar et al., 2016; Joshi et al., 2017) provides a reliable proxy. A recent study (Krishna et al., 2021) demonstrated that automatic metrics like ROUGE (Lin, 2004) are not meaningful for this task and can be easily gamed. Our experiments find that even reliable human preference testing is challenging given the complexity of long form answers, which motivates us to look into the discourse structure of long form answers.\nWe take a linguistically informed approach with the dual purpose of (a) to better understand the structure of long form answers, and (b) to assist the evaluation of long-form QA systems. By characterizing the communicative functions of sentences in long form answers (which we call roles),1 e.g., signaling the organization of the answer, directly answering the question, giving an example, providing background information, etc., we analyze human-written, and machine-generated long form answers. Furthermore, our framework combines functional structures with the notion of information\n1Functional structures have been studied in various other domains (discussed in Sections 3 and 8).\nsalience by designating a role for sentences that convey the main message of an answer.\nWe collect annotations on two datasets, ELI5 (Fan et al., 2019) and Natural Questions (NQ) (Kwiatkowski et al., 2019), which contains long form answers written by search users and from Wikipedia page respectively. In total, we provide fine-grained roles for 3.3K sentences (0.5K examples) and coarse annotation for 6K sentences (1.3K examples). We also annotate a small number (94) of machine-generated answers from a state-of-theart long form question answering system (Krishna et al., 2021) and provide rich analysis about their respective discourse structures. Our analysis demonstrates that studying answer structure can reveal a significant gap between machine-generated answers and human-written answers. We also present a competitive baseline model for automatic role classification, which performs on par with human agreement when trained with our annotated data. Lastly, our dataset yields a novel extractive summarization dataset, providing a benchmark for studying domain transfer in summarization and enabling QA models to provide concise answers to complex queries. We will release all our data and code at http://anonymous.co."
    }, {
      "heading" : "2 Revisiting Human Evaluation of Long Form Answers",
      "text" : "Recent work (Krishna et al., 2021) dissected the evaluation of long form answers, showing the limitations of lexical matching based automatic metrics. Given the flexibility of long form answers, they suggest human evaluation would be the most appropriate. Initial work (Fan et al., 2019) showed humans could differentiate good answers from bad answers. We further look into the reliability of human evaluation in the context of improved model and multiple human written answers – Can humans consistently choose which long form answer is better than the other?\nWe conduct A/B testing with the long form answers generated from a state-of-the-art LFQA system (Krishna et al., 2021) achieving a high ROUGE score (23.19), and human written answers (H). Their model uses passage retriever (Guu et al., 2020), and generates answers based on the retrieved passage with a routing transformer model (Roy et al., 2021). We look at answers generated from randomly retrieved passages (M-R) and answers generated from the top retrieved passage (M-P).\nWe sample three types of pairs (H, H), (M-P, MR), (H, M-P), 50 pairs for each type. Given a pair of answers, annotators are asked to choose among four options—equally good, equally bad, prefer A, prefer B. The annotators are linguistics undergraduates, trained for our discourse task (Section 4). We collect preferences from three annotators per answer pair and compute inter annotator agreement. The agreement is low across the board, with Fleiss Kappa (Fleiss and Cohen, 1973) value of 0.20 for (H, H), 0.23 for (H, M-P), and 0.26 (M-P, MR).This shows that when both answers are fluent, it is hard to conduct reliable humans A/B testing.\nWe empirically verify that evaluating long-form answers is challenging even for humans, as suggested by Krishna et al. (2021). Humans have to evaluate the correctness and sufficiency of the answer, as well as the quality of lengthy machinegenerated text (fluency, coherence, etc). This motivates us to study the discourse structure of long form answers, with the focus of evaluating the quality of lengthy generated texts."
    }, {
      "heading" : "3 Defining Answer Discourse Structure",
      "text" : "We study the discourse structure of long form answers based on functional roles of sentences in the paragraph. Functional structures characterize the communicative role a linguistic unit plays; as such, they vary across genres as the goals of communication also vary. In scientific or technical articles, these roles can be background, method, findings (Kircz, 1991; Liddy, 1991; Mizuta et al., 2006), while in news, they can be main event or anecdotes (Van Dijk, 2013; Choubey et al., 2020).\nThese structures are related to, though distinct from, coherence discourse structures (Hobbs, 1985). The latter characterizes how each unit (e.g., adjacent clauses or sentences) relates to others through semantic relations such as temporal, causal, etc.; such structures can be trees that hierarchically relate adjacent units (Mann and Thompson, 1988) or graphs (Lascarides and Asher, 2008). In contrast, functional roles describe how information is organized to serve the communication goal, in our case, providing the answer. Functional roles will not only inform theoretically-motivate research in long form question answering, but also as we show in Section 5, reflect the quality of answers depending on how humans and models can understand the roles in an answer sentence.\nWe developed our ontology from long form an-\nRole Why does salt bring out the flavor in most foods?\nswers in online community forum (subreddit Explain Like I’m Five (ELI5)), hence answers in different domains (e.g., textbooks) can contain roles beyond our ontology. We describe our six sentencelevel discourse roles for long form answers here:"
    }, {
      "heading" : "Answer-Summary (Sum), Answer (Ans). An",
      "text" : "answer sentence directly addresses the question. Here we distinguish between the the main content of the answer (henceforth answer summary) vs. sentences which explain or elaborate on the summary, shown in Table 1. The summaries play a more salient role than non-summary answer sentences, and can often suffice by themselves as the answer to the question. This is akin to argumentation structure that hierarchically arranges main claims and supporting arguments (Peldszus and Stede, 2013), and news structure that differentiates between main vs. supporting events (Van Dijk, 2013).\nOrganizational sentences (Org.) Rather than conveying information of the answer, the major role of an organizational sentence is to inform the reader how the answer will be structured. We found two main types of such sentences; the first signals an upcoming set of items of parallel importance:\n[A]: There are a few reasons candidates with “no chance\" to win keep running. 1) They enjoy campaigning[...]\nThe other type indicates that part of the answer is upcoming amidst an established flow; in the example below, the answerer used a hypophora:\n[A]: It might actually be a mosquito bite. I find the odd mosquito in my house in the winter from time to time, and I’m in Canada.[...] So why does it happen more often when you shower? It’s largely because [...]\nExamples (Ex.) Often people provide examples in answers; these are linguistically distinct from other answer sentences in the sense that they are more specific towards a particular entity, concept, or situation. This pattern of language specificity can also be found in example-related discourse relations (Louis and Nenkova, 2011; Li and Nenkova, 2015), or through entity instantiation (MacKinlay and Markert, 2011):\n[Q]: What is it about electricity that kills you? [A]: [...] For example, static electricity consists of tens of thousands of volts, but basically no amps. [...]\nWe found that examples in human answers are often not signaled explicitly, and often contain hypothetical situations:\n[Q]: Were major news outlets established with political bias or was it formed over time? [A]: [...]This is impossible due to the problem of “anchoring.” Consider a world where people on the right want the tax rate to be 1% lower and people on the left want the tax rate to be 1% higher[...]\nAuxiliary information (Aux.) These sentences provide information that are related to what is discussed in the answer, but not asked in the question. It could be background knowledge that the answerer deemed necessary or helpful, e.g.,\n[Q]: Why is it better to use cloning software instead of just copying and pasting the entire drive? [A]: When you install an operating system, it sets up what’s called a master file table, which [...] are important for the OS to work properly. [...] Simply copy-pasting files doesn’t copy either of these, meaning if you want to back up an OS installation you should clone the disk instead.\nor related content that extends the question, e.g.,\n[Q]: what is the difference between mandi and kabsa? [A]: [...] A popular way of preparing meat is called mandi. [...] Another way of preparing and serving meat for kabsa is mathbi , where seasoned meat is grilled on flat stones that are placed on top of burning embers.\nNotably, the removal of auxiliary information would still leave the answer itself intact.\nMiscellaneous (Misc.) We observe various roles that, although less frequent, show up consistently in human answers. These include sentences that acknowledge the limitation of the answer or specify the scope of the answer; describe the original source of the answer; express sentiment about the question or the answer; and refer to other answers in the platform (examples can be found in A.2.1). We group them into a miscellaneous role.\nData Validity Summary Role\nNQ 263 (1494) 202 (1077) 131 (698) ELI5 1035 (6575) 834 (5400) 411 (2674)"
    }, {
      "heading" : "4 Data and Annotation",
      "text" : ""
    }, {
      "heading" : "4.1 Source Datasets",
      "text" : "We use two existing datasets with long form answers: ELI5 (Fan et al., 2019) and Natural Questions (Kwiatkowski et al., 2019). To make annotation task manageable, we filter answers with more than 15 sentences and those with less than 3 sentences, removing about 40% of examples.\nELI5 ELI5 presents QA pairs where question and answers are constructed from the Reddit forum,2 and has been used as the main dataset for long form QA research (Jernite, 2020; Krishna et al., 2021). In addition to answers in the original datasets, we annotate small amount of machine generated answers from Krishna et al. (2021). We discuss this data in Section 5 separately; analyses in this section do not include this data.\nNatural Questions (NQ) NQ contains questions from Google search queries, which is then annotated with span-based answers or paragraph level answers from Wikipedia passages. In open retrieval setting, NQ has been exclusively used for its short span based answers (Lee et al., 2019), removing questions with paragraph level answers. We identify NQ contains fair amount of complex queries, and repurpose it to study long form answers for the first time. Many NQ questions can be answered with a short entity (e.g., how many episodes in season 2 breaking bad?), but many others questions require paragraph length answer (e.g., what does the word china mean in chinese?). This provides complementary answers compared to ELI5 dataset, as answers are not written specifically for the questions but harvested from pre-written Wikipedia paragraphs. Thus, this simulates scenarios where machines retrieve paragraphs that can serve as answers instead of generating them.3\n2https://www.reddit.com/r/explainlikeimfive/ 3We perform additional filtering for NQ question to iden-\ntify complex questions. Details can be found in A.3\nQA Pair Validity Upon manual inspection, we found in the datasets some long form answers do not address the question, as identified in the ELI5 paper (Fan et al., 2019) which reports 10% of answers to be insufficient. We further remove examples where questions are nonsensical or have presuppositions rejected by the answer; and for simplicity, cases with more than one sub-questions in the question (e.g., what Tor is, and why everyone praises it as the king of proxies?). During our annotation, annotators first determine the validity of the QA pair, and proceed to discourse annotation only if they consider the QA pair valid. Details about invalid QA pair identified is in A.2.2.4"
    }, {
      "heading" : "4.2 Annotation",
      "text" : "Annotators We collect annotations via two channels: US-based crowdsource workers on Amazon Mechanical Turk and undergraduate students majoring in linguistics, who are native speakers in English. We aimed to collect all data from crowdsourcing, but making fine-grained role distinction is challenging for untrained annotators. However, they can reliably identify valid QA pairs and sentences serving the Answer-Summary role. Thus, we rely on crowdworkers to identify valid QA pairs and summary sentences for each valid answer. Total of 29 crowdworkers worked on our task.\nOur six undergraduate students then provided fine-grained role annotations (including summary) for a subset of QA pairs annotated as valid by crowdworkers. We first qualified and then provided training materials to both groups of annotators. The annotation guideline can be found in A.4. We pay crowd workers $0.5 per example, and our undergraduate annotators $13 / hour.\nAnnotated Data Table 2 presents the data statistics. We collected validity and summary annotations for over 1K answers through crowdsourcing, and fine-grained role annotations for about half of them. As our tasks are complex and somewhat subjective, we collected three way annotations.\nWe consider a QA pair valid if all annotated it as valid, and invalid if more than two annotated it as invalid. If two annotators considered valid, we collect one additional annotation and consider it valid if and only if the additional annotator marked it as\n4The categories are not mutually exclusive, and we let annotators to pick any of them when an example belongs to multiple categories.\nvalid.5 We consider the majority role (i.e. chosen by two or more than two annotators) as the gold label. When all annotators chose different roles, they resolved the disagreement through adjudication. We report inter-annotator agreement before the adjudication.\nInter-annotator Agreement We find modest to high agreement for all annotation tasks: For crowdworkers, Fleiss Kappa was 0.53 for summary annotation, 0.51 for validity annotation. For student annotators, Fleiss Kappa was 0.45 for six-way role annotation and 0.52 for summary annotation.\nAnalysis Table 3 summarizes the label distribution. The proportion of valid QA pairs is similar between the two datasets, yet the role distribution varies significantly. Figure 2 shows the confusion matrix between pairs of annotations, with the numbers normalized by row. We observe frequent confusion between answer vs. answer-summary, and answer vs. auxiliary information.\nAround half of the sentences serve roles other than directly answering the questions, such as providing auxiliary information or giving an example. NQ shows higher proportion of auxiliary information, as the paragraphs are written independent of the questions and no miscellaneous sentences.\nFigure 3 presents the distribution of each role per its relative location in the answer. Organizational sentences typically locate at the beginning of the answer, examples often in the middle, with\n5The validity agreement improves to 0.70 after reannotation process.\nan increasing portion of auxiliary information towards the end. Despite the significant differences in the proportion of different discourse roles, the positioning of the roles is similar across NQ and ELI5. We also note a lead bias, as many summary sentences are found at the beginning of the answer."
    }, {
      "heading" : "5 Discourse Structure of Machine Generated Answers",
      "text" : "Having observed that humans can reliably assign discourse roles to sentences in (human-written) long form answers, we investigate the discourse structure of machine generated answers. We look into machine generated answers in our initial A/B testing (Section 2) and label them with the same annotation process. These machine generated answers report automatic score (ROUGE-L: 22.74) comparable to that of human written answers we annotate for role (ROUGE-L: 22.2).\nWe collect validity annotation on 94 machine generated answers, and 42 are considered invalid, among which 40 of them are marked as “no valid answer” by at least one annotator and 29 are marked as so by at least two annotators, suggesting that generated answers can achieve high automatic score without answering the question.6\nWe proceed to collect sentence-level role annotations on 52 valid generated long form answers. For the answer role annotation, the annotators disagree substantially more as compared to the humanwritten answers, with a Fleiss kappa of 0.31 (vs. 0.45 for human-written answers), suggesting that the discourse structure of machine generated answers are less clear, even to our trained annotators.\nThe answer role distribution of machine generated answers is very different from that of the human written answers (Figure 4). Machine generated answers contain more sentences which provide auxiliary information, and fewer summary sen-\n6The Fleiss’s kappa of QA pair validity is 0.36, substantially lower than the agreement on human written answers (0.51) while annotated by the same set of annotators.\ntences. We also include the portion of “disagreed” sentences where all three annotators chose different roles, which again shows that annotators find the discourse roles of generated sentences confusing. This suggests that model generated answers, despite having high ROUGE scores, is ill-structured."
    }, {
      "heading" : "6 Automatic Discourse Analysis",
      "text" : "With the dataset we collected on human written answers, we study how easy it is for models to identify discourse roles for each answer sentence in a valid QA pair.7 Such a model can enable automatic discourse analysis on long form answers.\nTask / Data / Metric Given a question q and its long form answer consisting of sentences s1, s2...sn, the goal is to assign each answer sentence si one of the six roles defined in Section 3.\nWe randomly split the long form answers in our role annotations into train, validation and test sets with a 70%/15%/15% split. We set apart role annotations for machine generated answers and use those for testing only.\nWe report accuracy with respect to the majority role label (or adjudicated one, if majority doesn’t exist) (Acc), match on any label from any annotator (Match-Any), and Macro-F1 of the six roles.\nLower bounds We present two simple baselines to provide lower bounds: (1) Majority: We pre-\n7We do not automatically classify QA pair validity, as it requires in-depth world knowledge which is beyond the scope of our study.\ndict the most frequent labels in the training data: Answer-Summary. (2) Summary-lead: We predict first two sentences as Answer-Summary, and the rest of the sentences as Answer.\nClassification Models We use the [CLS] token from RoBERTa (Liu et al., 2019) which encodes [question <q> ans1 ... <start> ansi <end> ...], where ansi encodes the ith sentence in the answer. The model is trained to predict one of the six roles for ansi. The model encodes different sentences in the same answer separately.\nSeq2Seq We use two variations (base, large) of T5 (Raffel et al., 2019), which take the concatenation of question and answer, and output the roles for each sentence sequentially. We fine-tune the model on our role dataset, by setting the input sequence to be [question [1] ans1 [2] ans2 ...], where ansi denotes the ith sentence in the answer. The target output sequence is set to [[1] role1 [2] role2 [3]...], where rolei is the corresponding role for ansi.\nHuman performance We provide two approximations for human performance: upperbound (u) and lowerbound (l). (1) HUMAN (U): We compare each individual annotator’s annotation with the majority label. This inflates human performance as one’s own judgement affected the majority label. (2) HUMAN (L): We compare pairs of annotation and calculate average F1 and accuracy of all pairs. For Match-any, we compute the match for each annotation against the other two annotations.\nResult: Human Written Answers Table 4 reports overall results and Table 5 reports results per each role. T5-large, pre-trained on a large amount of data, shows noticeable gains compared to simple baselines, and is the closest to HUMAN (U). We find Miscellaneous, Example and Summary roles are easier to identify compared to Answer and Auxiliary Information, which are often confused with each other for the annotators as well.\nResults: Machine Generated Answers We evaluate our role classifier on the machine generated answers that we annotated, and found that both"
    }, {
      "heading" : "System Acc Match-Any Macro F1",
      "text" : "RoBERTa (Acc: 0.43; Match-Any: 0.60; Macro F1: 0.35) and T5-large model (Acc: 0.50; Match-Any: 0.68; Macro F1: 0.42) report worse performance as compared to human written answers in test set. This echoes our observation that humans agree less when annotating machine generated answers.\nUsecase: Role Model Uncertainty We further look into whether uncertainty of the model can be used as a proxy to detect poorly organized discourse structure.8 Our trained role classifier is less certain when predicting the roles for machine generated sentences (average entropy on humanwritten evaluation data / machine-generated answer sentences: 0.81/0.97). Similar to our manual analysis (Section 5), automatic analysis from role classifier sets machine generated answers apart from human written answers.\nWe plot sentences grouped by their predicted distribution entropy (x-axis) and human agreement (i.e., how many annotators selected the same role during annotation). Figure 5 shows, consistently across human-generated and machine-generated answers, that the more human annotators agree, the lower the classification prediction entropy. This shows that role model entropy can be used to reflect the quality of machine-generated answers without a reference answer, although it wouldn’t evaluate the correctness of the answers.\n8We use RoBERTa for this analysis for simplicity of calculating prediction entropy."
    }, {
      "heading" : "7 Summarizing Long Form Answers",
      "text" : "Lastly, we repurpose our role annotation dataset to summarize long form answers. Finding key sentences from long form answers has a practical appeal as users prefer concise answers (Choi et al., 2021). Our role annotations yield an extractive summarization dataset with 1K answer paragraphs in a new domain of long form answers, with 2.5 sentences marked as summary by one or more annotators.\nTask / Data / Metric Given a question q and its long form answer consisting of sentences s1, s2, ...sn, the goal is identifying a subset of sentences which can summarize the long form answer.\nWe merge the answer paragraphs from two datasets and randomly split them into train, validation and test sets with a 70%/15%/15% split.9 We use all three annotations as gold summary during both training and evaluation, which yields a summarization dataset with multiple references. We therefore report weighted precision, recall and F1 scores (Xu et al., 2016). The precise definition of our weighted metric can be found in A.6.\nLower Bounds We present two simple baselines, all taking a fixed number of sentences per paragraph, chosen from [1, 2, 3] based on the performance on validation set. (1) RANDOM: A random set of three answer sentences (2) LEAD: The lead three sentences of the paragraph.\nModels We use an extractive summarization model (PreSumm) (Liu and Lapata, 2019) trained on the CNN/DailyMail dataset. PreSumm uses BERT to encode a document and outputs a score for each sentence to determine whether it belongs to the summary or not. We select the threshold of the score based on results on the validation set. We present results on the original model and the\n9We keep the split aligned with that of the role dataset."
    }, {
      "heading" : "System P R F1",
      "text" : "same model finetuned with our data. We use T5 to identify summary sentences as in role classification, only changing the categories from six-way roles to binary label.\nHuman performance We approximate human performance with role annotation. Considering 3- way crowdsourced label as gold, we compute performance of summary annotation mapped from the role annotation by undergraduate annotators. We report results from two sets of summary sentences: (1) HUMAN (M): the set of sentence annotated as \"Answer-Summary\" by more than one annotator. (2) HUMAN (A): the set of sentence annotated as \"Answer-Summary\" by any annotator.\nResult Table 6 reports results on summary task. Lead baseline shows strong performances as was in other domains. The model trained on CNN/Daily mail dataset (Hermann et al., 2015) outperforms lead baseline slightly, but falls behind the model fine-tuned on our dataset. The T5 model fine-tuned on our summary dataset performs the best. The results suggests a significant domain difference between newswire text (where lead is more prominent (Liu and Lapata, 2019)) and long form answers. Thus, our dataset could support future research in extractive summarization across domains."
    }, {
      "heading" : "8 Related Work",
      "text" : "Discourse structure. Our work is closely related to functional structures defined through content types explored in other domains; prior work has affirmed the usefulness of these structures in downstream NLP tasks. In news, Choubey et al. (2020) adopted Van Dijk (2013)’s content schema cataloging events (e.g., main event, anecdotal), which they showed to improve the performance of event coreference resolution. In scientific writing, content types (e.g., background, methodology) are shown to be useful for summarization (Teufel and\nMoens, 2002; Cohan et al., 2018), information extraction (Mizuta et al., 2006; Liakata et al., 2012), and information retrieval (Kircz, 1991; Liddy, 1991). The discourse structure of argumentative texts (e.g., support, rebuttal) (Peldszus and Stede, 2013; Becker et al., 2016; Stab and Gurevych, 2017) has also been applied on argumentation mining. To the best of our knowledge, no prior work has studied the discourse structure of long-form answers.\nQuestion Answering. Recent work (Cao and Wang, 2021) have investigated the ontology of questions, which includes comparison questions, verification questions, judgement questions, etc. We construct the ontology of functional roles of answer sentences. One of the roles in our ontology is summary, yielding an extractive summarization dataset. This shares motivation with a line of work studying query-focused summarization (Xu and Lapata, 2020). Lastly, our work build up on two datasets containing long form answers (Kwiatkowski et al., 2019; Fan et al., 2019) and extends the analysis of long form answers from earlier studies (Krishna et al., 2021)."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We present a linguistically motivated study of long form answers. We find humans employ various strategies – introducing sentences laying out the structure of the answer, proposing hypothetical and real examples, and summarizing main points – to organize information. Our study also reveals deficient discourse structures of machine-generated answers, showing potential for using discourse analysis to assist in evaluating long form answers in multiple ways. For instance, highlighting summary sentence(s) or sentence-level discourse role could be helpful for human evaluators to dissect long form answers, whose length has been found to be challenging for human evaluation (Krishna et al., 2021). Trained role classifier can also evaluate the discourse structure of machine-generated answers. Future work can explore using sentences belonging to the summary role to design evaluation metrics that focuses on the core parts of the answer (Nenkova and Passonneau, 2004), for assessing the correctness of generated the answer. Exploring controllable generation, such as encouraging models to provide summaries or examples, would be another exciting avenue for future work."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Human A/B Testing",
      "text" : "We present the agreement for our human evaluation in Table 7, as well as agreement with prior study (Krishna et al., 2021), by calculating agreement for 4-way annotations, including their human evaluation."
    }, {
      "heading" : "A.2 Examples",
      "text" : ""
    }, {
      "heading" : "A.2.1 Miscellaneous Roles",
      "text" : "Some sentences specify the limitation of the answer:\n[Q]: Why are there such drastic differences in salaries between different countries? [A]: I’m going to avoid discussing service industries, because[...] I’m mostly talking tech. [...]\nSome sentences mainly state where the answer came from, e.g.,\n[Q]: Why Does a thermostat require the user to switch between heat and cool modes, as opposed to just setting the desired temperature? [A]: The person who installed my heat pump (which has all three modes) explained this to me. [...]\nor pointing to other resources: [Q]: Why did Catholicism embrace celibacy and why did Protestantism reject it? [A]: raskhistorians has a few excellent discussions about this. [...]\nAnswerers also express sentiment: [Q]: Why did Catholicism embrace celibacy and why did Protestantism reject it? [A]: Good God, the amount of misinformation upvoted is hurting. [...]\nA.2.2 Invalid QA We provide definitions, as well as examples of each invalid QA types. Table 9 elaborates samples identified as invalid during our annotation.\nNo valid answer The answer paragraph doesn’t provide a valid answer to the question.\n[Q]: How does drinking alcohol affect your ability to lose weight? [A]: Alcohol itself is extremely calorically dense.Doesn’t really matter whether you’re drinking a light beer or shots, alcohol itself has plenty of calories. Just think of every three shots as eating a mcdouble, with even less nutritional value."
    }, {
      "heading" : "A/B # Kappa Kappa (prior)",
      "text" : "Nonsensical question The question is nonsensical and it is unclear what is asked.\n[Q]: asia vs rest of the world cricket match\nMultiple questions asked More than one question are asked in the question sentence.\n[Q]: what is a limpet and where does it live\nAssumptions in the question rejected The answer focuses on rejecting assumptions in the question, without answering the question.\n[Q]: Why is it that as we get older, we are able to handle eating hotter foods [A]: I’m not sure I accept the premise.Children in cultures where spicy food is common, think nothing of it.My nephews had no problem eating hot peppers when they were very young because it was just a normal part of their diet.[...]"
    }, {
      "heading" : "A.2.3 Role annotation",
      "text" : "We include example role annotations in Table 10.\nA.3 Implementation Details We use pytorch-transformers Wolf et al. (2019) to implement our classification models. The hyperparameters are manually searched by the authors.\nQuestion classification model A difficulty in repurposing NQ is that not all questions with paragraph answers actually need multiple sentences. To identify such complex questions, we built a simple"
    }, {
      "heading" : "Reason NQ ELI5",
      "text" : "BERT-based classifier, trained to distinguish NQ questions with short answers (i.e., less than five tokens) and ELI5 questions.\nFor the question classification model, we use the [CLS] token from BERT model to perform prediction. We use the original split from the ELI5 dataset, and split the NQ open’s validation set into val and test set. We fine-tuned the bert-base-uncasedmodel for 3 epochs, with an initial learning rate of 5e− 5 and batch size of 32.We use the model with the highest validation F1 as the question classifier, which achieves F1 of 0.97 and 0.94 on validation and test set respectively. We then run this classifier to select the non factoid questions from NQ questions with long form answers, which classifies around 10%, out of the 27,752 NQ long questions as non-factoid. Examples are in Table 8.\nRole classification model For RoBERTa classification model, we use the roberta-large model. The training batch size is set to 64, with initial learning rate as 5e − 5. The model is optimized with AdamW optimizer and a linear learning rate schedule. We train the model for 10 epochs and report result of the model with best validation accuracy, averaged across three different random seeds.\nFor Seq2Seq T5 models, we limit the input/output to be 512/128 tokens. For evaluating the predicted roles, we parse the output string and only take the first k roles into account, k being the number of sentences in the answer paragraph. If the model predicted less than k roles, we pad a dummy role for the remaining sentences. We finetune the model with batch size of 16 and initial learning rate of 1e− 4, with AdamW optimizer and a linear learning rate schedule. We train the model for 30 epoches and report result of the model with best validation accuracy, averaged across three different random seeds.\nSummary Identification model For the PreSumm model, we fine-tune by continuing training\nfrom the checkpoint of BertSumExt, following the original set up from the paper.\nFor the T5 model, we use the same hyperparamters as the role classification models.\nFor both setup, we report the results on the model with highest validation macro F1."
    }, {
      "heading" : "A.4 Annotation Interface",
      "text" : "Figure 6 is the annotation guideline presented to the annotators (we present Step 1 and Step 3 for crowdworkers, Step 2 and Step 3 for student annotators). We didn’t capture the extended example section here due to space.\nFigure 7 and 8 are screenshots of the annotation interface.\nA.5 Role classification experiment results"
    }, {
      "heading" : "A.5.1 Per-role metrics",
      "text" : "We report detailed per-role metrics for validation and test set in Table 12."
    }, {
      "heading" : "A.5.2 Experiments on RoBERTa models",
      "text" : "We report additional experiments (Table 13 and Table 14) that we have conducted with RoBERTa model, with several variations of the input data. We follow the same experiment set up mentioned in Section A.3.\n• Answer sentence only (Ans): This model takes the answer sentence as the input. (i.e. ansi for the ith sentence in the answer).\n• Question, Answer sentence (Ans-q): This model takes the answer sentence with question\npreppended as input. (i.e. [question <q> ansi] for the ith sentence in the answer).\n• Answer context (Context): This model takes the whole answer paragraph as input, with special tokens indicating the sentence being classified. (i.e. [ans1 ... <start> ansi <end> ...] for the ith sentence in the answer).\n• Question, Answer context (Context-q): This model takes the whole answer paragraph with question preppended as input. This is the setting we reported in Section 6."
    }, {
      "heading" : "A.6 Summary Identification Evaluation Metric",
      "text" : "We report weighted precision, recall and F1 scores between system selected summary sentences and ground truth annotation, which consists of (si, wi, yi), where wi is the weight and yi is the label. If none of the annotator select si as a summary, yi is 0 and wi is 3 (all three annotators agree that this is not a summary). Otherwise, wi equals to the number of annotators who selected sentence si as summary sentence and yi is 1. Assuming model predicts binary decision on whether si belongs to summary or not, denoted as ŷi, we compute weighted TP , FP , TN and FN . Using TP as an example, we calculate it by TP = ∑ si wi, if yi == ŷi and yi == 1. We then use these weighted values to compute precision, recall and F1."
    } ],
    "references" : [ {
      "title" : "Argumentative texts and clause types",
      "author" : [ "Maria Becker", "Alexis Palmer", "Anette Frank." ],
      "venue" : "Proceedings of the Third Workshop on Argument Mining (ArgMining2016), pages 21–30.",
      "citeRegEx" : "Becker et al\\.,? 2016",
      "shortCiteRegEx" : "Becker et al\\.",
      "year" : 2016
    }, {
      "title" : "Controllable openended question generation with a new question type ontology",
      "author" : [ "Shuyang Cao", "Lu Wang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
      "citeRegEx" : "Cao and Wang.,? 2021",
      "shortCiteRegEx" : "Cao and Wang.",
      "year" : 2021
    }, {
      "title" : "Decontextualization: Making sentences stand-alone",
      "author" : [ "Eunsol Choi", "Jennimaria Palomaki", "Matthew Lamm", "Tom Kwiatkowski", "Dipanjan Das", "Michael Collins." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:447–461.",
      "citeRegEx" : "Choi et al\\.,? 2021",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2021
    }, {
      "title" : "Discourse as a function of event: Profiling discourse structure in news articles around the main event",
      "author" : [ "Prafulla Kumar Choubey", "Aaron Lee", "Ruihong Huang", "Lu Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Choubey et al\\.,? 2020",
      "shortCiteRegEx" : "Choubey et al\\.",
      "year" : 2020
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of the 2018 Conference of the North",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "ELI5: Long form question answering",
      "author" : [ "Angela Fan", "Yacine Jernite", "Ethan Perez", "David Grangier", "Jason Weston", "Michael Auli." ],
      "venue" : "ACL.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "author" : [ "Joseph L. Fleiss", "Jacob Cohen." ],
      "venue" : "Educational and Psychological Measurement, 33(3):613– 619.",
      "citeRegEx" : "Fleiss and Cohen.,? 1973",
      "shortCiteRegEx" : "Fleiss and Cohen.",
      "year" : 1973
    }, {
      "title" : "Realm: Retrievalaugmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang." ],
      "venue" : "arXiv preprint 2002.08909.",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomás Kociský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "On the coherence and structure of discourse",
      "author" : [ "Jerry R Hobbs" ],
      "venue" : null,
      "citeRegEx" : "Hobbs.,? \\Q1985\\E",
      "shortCiteRegEx" : "Hobbs.",
      "year" : 1985
    }, {
      "title" : "Explain anything like i’m five:a model for open domain long form question answering",
      "author" : [ "Yacine Jernite" ],
      "venue" : null,
      "citeRegEx" : "Jernite.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jernite.",
      "year" : 2020
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S Weld", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint 1705.03551.",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Rhetorical structure of scientific articles: the case for argumentational analysis in information retrieval",
      "author" : [ "Joost G Kircz." ],
      "venue" : "Journal of documentation.",
      "citeRegEx" : "Kircz.,? 1991",
      "shortCiteRegEx" : "Kircz.",
      "year" : 1991
    }, {
      "title" : "Hurdles to progress in long-form question answering",
      "author" : [ "Kalpesh Krishna", "Aurko Roy", "Mohit Iyyer." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Krishna et al\\.,? 2021",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2021
    }, {
      "title" : "Natural Questions: a benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "TACL.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Segmented discourse representation theory: Dynamic semantics with discourse structure",
      "author" : [ "Alex Lascarides", "Nicholas Asher." ],
      "venue" : "Computing meaning, pages 87–124. Springer.",
      "citeRegEx" : "Lascarides and Asher.,? 2008",
      "shortCiteRegEx" : "Lascarides and Asher.",
      "year" : 2008
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "ACL.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Fast and accurate prediction of sentence specificity",
      "author" : [ "Junyi Jessy Li", "Ani Nenkova." ],
      "venue" : "TwentyNinth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Li and Nenkova.,? 2015",
      "shortCiteRegEx" : "Li and Nenkova.",
      "year" : 2015
    }, {
      "title" : "Automatic recognition of conceptualization zones in scientific articles and two life science applications",
      "author" : [ "Maria Liakata", "Shyamasree Saha", "Simon Dobnik", "Colin Batchelor", "Dietrich Rebholz-Schuhmann." ],
      "venue" : "Bioinformatics, 28(7):991–1000.",
      "citeRegEx" : "Liakata et al\\.,? 2012",
      "shortCiteRegEx" : "Liakata et al\\.",
      "year" : 2012
    }, {
      "title" : "The discourse-level structure of empirical abstracts: An exploratory study",
      "author" : [ "Elizabeth DuRoss Liddy." ],
      "venue" : "Information Processing & Management, 27(1):55–81.",
      "citeRegEx" : "Liddy.,? 1991",
      "shortCiteRegEx" : "Liddy.",
      "year" : 1991
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic identification of general and specific sentences by leveraging discourse annotations",
      "author" : [ "Annie Louis", "Ani Nenkova." ],
      "venue" : "Proceedings of 5th international joint conference on natural language processing, pages 605–613.",
      "citeRegEx" : "Louis and Nenkova.,? 2011",
      "shortCiteRegEx" : "Louis and Nenkova.",
      "year" : 2011
    }, {
      "title" : "Modelling entity instantiations",
      "author" : [ "Andrew MacKinlay", "Katja Markert." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages 268–274.",
      "citeRegEx" : "MacKinlay and Markert.,? 2011",
      "shortCiteRegEx" : "MacKinlay and Markert.",
      "year" : 2011
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization",
      "author" : [ "William C Mann", "Sandra A Thompson." ],
      "venue" : "Text-interdisciplinary Journal for the Study of Discourse, 8(3):243–281.",
      "citeRegEx" : "Mann and Thompson.,? 1988",
      "shortCiteRegEx" : "Mann and Thompson.",
      "year" : 1988
    }, {
      "title" : "Zone analysis in biology articles as a basis for information extraction",
      "author" : [ "Yoko Mizuta", "Anna Korhonen", "Tony Mullen", "Nigel Collier." ],
      "venue" : "International journal of medical informatics, 75(6):468–487.",
      "citeRegEx" : "Mizuta et al\\.,? 2006",
      "shortCiteRegEx" : "Mizuta et al\\.",
      "year" : 2006
    }, {
      "title" : "Evaluating content selection in summarization: The pyramid method",
      "author" : [ "Ani Nenkova", "Rebecca J Passonneau." ],
      "venue" : "Proceedings of the human language technology conference of the north american chapter of the association for computational linguistics:",
      "citeRegEx" : "Nenkova and Passonneau.,? 2004",
      "shortCiteRegEx" : "Nenkova and Passonneau.",
      "year" : 2004
    }, {
      "title" : "From argument diagrams to argumentation mining in texts: A survey",
      "author" : [ "Andreas Peldszus", "Manfred Stede." ],
      "venue" : "International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1–31.",
      "citeRegEx" : "Peldszus and Stede.,? 2013",
      "shortCiteRegEx" : "Peldszus and Stede.",
      "year" : 2013
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint 1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient content-based sparse attention with routing transformers",
      "author" : [ "Aurko Roy", "Mohammad Taghi Saffar", "Ashish Vaswani", "David Grangier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:53–68.",
      "citeRegEx" : "Roy et al\\.,? 2021",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2021
    }, {
      "title" : "Parsing argumentation structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "Computational Linguistics, 43(3):619–659.",
      "citeRegEx" : "Stab and Gurevych.,? 2017",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2017
    }, {
      "title" : "Summarizing scientific articles: experiments with relevance and rhetorical status",
      "author" : [ "Simone Teufel", "Marc Moens." ],
      "venue" : "Computational linguistics, 28(4):409–445.",
      "citeRegEx" : "Teufel and Moens.,? 2002",
      "shortCiteRegEx" : "Teufel and Moens.",
      "year" : 2002
    }, {
      "title" : "News as discourse",
      "author" : [ "Teun A Van Dijk." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Dijk.,? 2013",
      "shortCiteRegEx" : "Dijk.",
      "year" : 2013
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "2021), by calculating agreement for 4-way annotations, including their human evaluation",
      "author" : [ "study (Krishna" ],
      "venue" : "Roles",
      "citeRegEx" : ".Krishna,? \\Q2021\\E",
      "shortCiteRegEx" : ".Krishna",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Recent work (Fan et al., 2019; Krishna et al., 2021) explored long form answers, where answers can be free-form texts consisting of multiple sentences.",
      "startOffset" : 12,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "Recent work (Fan et al., 2019; Krishna et al., 2021) explored long form answers, where answers can be free-form texts consisting of multiple sentences.",
      "startOffset" : 12,
      "endOffset" : 52
    }, {
      "referenceID" : 30,
      "context" : "ing spans (Rajpurkar et al., 2016; Joshi et al., 2017) provides a reliable proxy.",
      "startOffset" : 10,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "ing spans (Rajpurkar et al., 2016; Joshi et al., 2017) provides a reliable proxy.",
      "startOffset" : 10,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "A recent study (Krishna et al., 2021) demonstrated that automatic metrics like ROUGE (Lin, 2004) are not meaningful for this task and can be easily gamed.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : ", 2021) demonstrated that automatic metrics like ROUGE (Lin, 2004) are not meaningful for this task and can be easily gamed.",
      "startOffset" : 55,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "We collect annotations on two datasets, ELI5 (Fan et al., 2019) and Natural Questions (NQ) (Kwiatkowski et al.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "We also annotate a small number (94) of machine-generated answers from a state-of-theart long form question answering system (Krishna et al., 2021) and provide rich analysis about their respective discourse structures.",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "Recent work (Krishna et al., 2021) dissected the evaluation of long form answers, showing the limitations of lexical matching based automatic metrics.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "Initial work (Fan et al., 2019) showed humans could differentiate good answers from bad answers.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "We conduct A/B testing with the long form answers generated from a state-of-the-art LFQA system (Krishna et al., 2021) achieving a high ROUGE score (23.",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "Their model uses passage retriever (Guu et al., 2020), and generates answers based on the retrieved passage with a routing transformer model (Roy et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 31,
      "context" : ", 2020), and generates answers based on the retrieved passage with a routing transformer model (Roy et al., 2021).",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "The agreement is low across the board, with Fleiss Kappa (Fleiss and Cohen, 1973) value of 0.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "In scientific or technical articles, these roles can be background, method, findings (Kircz, 1991; Liddy, 1991; Mizuta et al., 2006), while in news, they can be main event or anecdotes (Van Dijk, 2013; Choubey et al.",
      "startOffset" : 85,
      "endOffset" : 132
    }, {
      "referenceID" : 19,
      "context" : "In scientific or technical articles, these roles can be background, method, findings (Kircz, 1991; Liddy, 1991; Mizuta et al., 2006), while in news, they can be main event or anecdotes (Van Dijk, 2013; Choubey et al.",
      "startOffset" : 85,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "In scientific or technical articles, these roles can be background, method, findings (Kircz, 1991; Liddy, 1991; Mizuta et al., 2006), while in news, they can be main event or anecdotes (Van Dijk, 2013; Choubey et al.",
      "startOffset" : 85,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : ", 2006), while in news, they can be main event or anecdotes (Van Dijk, 2013; Choubey et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "These structures are related to, though distinct from, coherence discourse structures (Hobbs, 1985).",
      "startOffset" : 86,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "; such structures can be trees that hierarchically relate adjacent units (Mann and Thompson, 1988) or graphs (Lascarides and Asher, 2008).",
      "startOffset" : 73,
      "endOffset" : 98
    }, {
      "referenceID" : 15,
      "context" : "; such structures can be trees that hierarchically relate adjacent units (Mann and Thompson, 1988) or graphs (Lascarides and Asher, 2008).",
      "startOffset" : 109,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : "This is akin to argumentation structure that hierarchically arranges main claims and supporting arguments (Peldszus and Stede, 2013), and news structure that differentiates between main vs.",
      "startOffset" : 106,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "This pattern of language specificity can also be found in example-related discourse relations (Louis and Nenkova, 2011; Li and Nenkova, 2015), or through entity instantiation (MacKinlay and Markert, 2011):",
      "startOffset" : 94,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "This pattern of language specificity can also be found in example-related discourse relations (Louis and Nenkova, 2011; Li and Nenkova, 2015), or through entity instantiation (MacKinlay and Markert, 2011):",
      "startOffset" : 94,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "This pattern of language specificity can also be found in example-related discourse relations (Louis and Nenkova, 2011; Li and Nenkova, 2015), or through entity instantiation (MacKinlay and Markert, 2011):",
      "startOffset" : 175,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "We use two existing datasets with long form answers: ELI5 (Fan et al., 2019) and Natural Questions (Kwiatkowski et al.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "ELI5 ELI5 presents QA pairs where question and answers are constructed from the Reddit forum,2 and has been used as the main dataset for long form QA research (Jernite, 2020; Krishna et al., 2021).",
      "startOffset" : 159,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : "ELI5 ELI5 presents QA pairs where question and answers are constructed from the Reddit forum,2 and has been used as the main dataset for long form QA research (Jernite, 2020; Krishna et al., 2021).",
      "startOffset" : 159,
      "endOffset" : 196
    }, {
      "referenceID" : 16,
      "context" : "In open retrieval setting, NQ has been exclusively used for its short span based answers (Lee et al., 2019), removing questions with paragraph level answers.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "3 QA Pair Validity Upon manual inspection, we found in the datasets some long form answers do not address the question, as identified in the ELI5 paper (Fan et al., 2019) which reports 10% of answers to be insufficient.",
      "startOffset" : 152,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "Classification Models We use the [CLS] token from RoBERTa (Liu et al., 2019) which encodes [question <q> ans1 .",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "Seq2Seq We use two variations (base, large) of T5 (Raffel et al., 2019), which take the concatenation of question and answer, and output the roles for each sentence sequentially.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "Finding key sentences from long form answers has a practical appeal as users prefer concise answers (Choi et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "Models We use an extractive summarization model (PreSumm) (Liu and Lapata, 2019) trained on the CNN/DailyMail dataset.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "The model trained on CNN/Daily mail dataset (Hermann et al., 2015) outperforms lead baseline slightly, but falls behind the model fine-tuned on our dataset.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 21,
      "context" : "The results suggests a significant domain difference between newswire text (where lead is more prominent (Liu and Lapata, 2019)) and long form answers.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 33,
      "context" : ", background, methodology) are shown to be useful for summarization (Teufel and Moens, 2002; Cohan et al., 2018), information extraction (Mizuta et al.",
      "startOffset" : 68,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : ", background, methodology) are shown to be useful for summarization (Teufel and Moens, 2002; Cohan et al., 2018), information extraction (Mizuta et al.",
      "startOffset" : 68,
      "endOffset" : 112
    }, {
      "referenceID" : 26,
      "context" : ", 2018), information extraction (Mizuta et al., 2006; Liakata et al., 2012), and information retrieval (Kircz, 1991; Liddy, 1991).",
      "startOffset" : 32,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : ", 2018), information extraction (Mizuta et al., 2006; Liakata et al., 2012), and information retrieval (Kircz, 1991; Liddy, 1991).",
      "startOffset" : 32,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : ", 2012), and information retrieval (Kircz, 1991; Liddy, 1991).",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : ", 2012), and information retrieval (Kircz, 1991; Liddy, 1991).",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 28,
      "context" : ", support, rebuttal) (Peldszus and Stede, 2013; Becker et al., 2016; Stab and Gurevych, 2017) has also been applied on argumentation mining.",
      "startOffset" : 21,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : ", support, rebuttal) (Peldszus and Stede, 2013; Becker et al., 2016; Stab and Gurevych, 2017) has also been applied on argumentation mining.",
      "startOffset" : 21,
      "endOffset" : 93
    }, {
      "referenceID" : 32,
      "context" : ", support, rebuttal) (Peldszus and Stede, 2013; Becker et al., 2016; Stab and Gurevych, 2017) has also been applied on argumentation mining.",
      "startOffset" : 21,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "Recent work (Cao and Wang, 2021) have investigated the ontology of questions, which includes comparison questions, verification questions, judgement questions, etc.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "Lastly, our work build up on two datasets containing long form answers (Kwiatkowski et al., 2019; Fan et al., 2019) and extends the analysis of long form answers from earlier studies (Krishna et al.",
      "startOffset" : 71,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and extends the analysis of long form answers from earlier studies (Krishna et al., 2021).",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "For instance, highlighting summary sentence(s) or sentence-level discourse role could be helpful for human evaluators to dissect long form answers, whose length has been found to be challenging for human evaluation (Krishna et al., 2021).",
      "startOffset" : 215,
      "endOffset" : 237
    }, {
      "referenceID" : 27,
      "context" : "Future work can explore using sentences belonging to the summary role to design evaluation metrics that focuses on the core parts of the answer (Nenkova and Passonneau, 2004), for assessing the correctness of generated the answer.",
      "startOffset" : 144,
      "endOffset" : 174
    } ],
    "year" : 0,
    "abstractText" : "Long form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions. However, little prior work exists on this task. To better understand this complex task, we study the functional structure of long form answers on two datasets, Natural Questions (Kwiatkowski et al., 2019) and ELI5 (Fan et al., 2019). Our main goal is to understand how humans organize information to craft complex answers. We develop an ontology of sentence-level functional roles for long form answers, and annotate 3.3k sentences in 542 examples. Our annotated data enables training a reliable role classifier that can be used for automatic analysis and thus reveals machine generated answers are structured worse than human written answers. Our data further yields an extractive summarization dataset for long form answers, giving models the ability to identify a concise answer to a complex query.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_90_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Vision Features in Multimodal Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multimodal machine translation (MMT) has emerged as an active field of research which marries the worlds of computer vision (CV) and natural language processing (NLP) (Specia et al., 2016). Early models of this kind produce a translation given the fused representation of both the visual and textual inputs (Caglayan et al., 2016; Libovický and Helcl, 2017; Calixto and Liu, 2017). As expected, such a paradigm achieves promising BLEU improvements and inspires the community to follow up.\nBut soon researchers find that MMT systems do not act as what we would ordinarily design: the vision modality contributes to translation little. For example, it is not harmful to MMT systems when the input image is irrelevant to the text (Grönroos et al., 2018; Lala et al., 2018), or even when the vision features are absent (Elliott, 2018). More recently, Wu et al. (2021) have pointed out that the use of the vision modality is a way of regularization for training but not a complement to the text\nmodality. As another response to the analysis of MMT, Caglayan et al. (2019) investigate how the vision features correlate to the text. They find that the input image helps translation when some of the input words are masked.\nNote that previous work has for the most part focused on integrating off-the-shelf vision models (such as ResNet-50) into MMT. The underlying assumption here is that the existing vision models are powerful enough to encode the image. This implicitly ignores the quality of vision models in representing images. But computer vision is facing a new trend by moving from CNNs to Transformer as the backbone model (Dosovitskiy et al., 2021; Liu et al., 2021b; Carion et al., 2020). A natural question that arises is: how will MMT systems behave if stronger vision models are adopted?\nIn this work, we address this question by a systematic study of using various vision models in MMT, in particular using the most successful models in recent studies (such as Vision Transformer, or ViT for short). We find that the patching method used in Transformer-based vision models offers an opportunity to detail the patch-level contribution of the image. This leads us to develop a selective attention model to correlate words with image patches. Beyond this, we introduce object-detection and image captioning features into MMT for further improvements of the vision models (Carion et al., 2020; Fang et al., 2021).\nFollowing (Caglayan et al., 2019)’s work, we design more detailed probing tasks to examine to what degree the vision modality contributes to MMT. We run an extensive set of experiments on En-De and En-Fr MMT tasks. Our findings are\n• Stronger vision models help. For example, ViT can beat ResNet-50 on the probing tasks though the superiority is not significant on standard MMT data.\n• Automatic evaluation on current MMT tasks\nmight not be a good indicator for the effectiveness of MMT models. For example, models enhanced with object-detection and image captioning features yield good BLEU scores on the original MMT task but show modest or no contributions on the probing tasks.\nWe hope that the results here can inspire more research on exploring better vision models and evaluation methods for multimodal NLP."
    }, {
      "heading" : "2 Preliminary",
      "text" : "We start with description of the probing tasks. It is followed by a design of vision features and a selective attention mechanism for introducing ViTlike representations into MMT."
    }, {
      "heading" : "2.1 Insufficient Text Generation",
      "text" : "To know how an image contributes to translation, a way is to mask some of the input words (call this insufficient text) and force the translation model to learn from the image. Following previous design of color deprivation and entity-based masking, we present detailed probing tasks which are complementary to Caglayan et al. (2019)’s work. In preliminary experiments1, we find that “color”, “character” and “noun” are three kinds of words which could be complemented according to the vision modality once the corresponding texts are masked. The following probing tasks are designed accordingly.\nColor-based Probing In training, all source words referring to a color are replaced by a special token [Mask_C]. There are 8, 919 sentences involving color words, and nearly one third of them involve more than one color. It is worth noting that each color may have two or more translations due to the rich morphology in German and French. For example, the English “green” can be translated to “grün”, “grüne”, “grünes”, “grüner”, “grünen” and “grünem” in German. We design two criteria to\n1We choose the Multi30K En-De and En-Fr datasets for experiments.\nmeasure the accuracy of translation. The first criterion is strict. The correct translation requires generating the same color and the same gender as in reference translations. The second criterion is relaxed and all translations expressing the same color are correct.\nCharacter-based Probing For character words, we choose “man”, “woman”, “people”, “men”, “girl” and “boy”. Each character word has a single translation only, except for “people”. Because about 60% sentences contain character words in our training data, they are reasonable indicators of assessing the ability of inferring correct translations from the input image. Here we use [MASK_P] for masking.\nNoun-based Probing For more complex scenarios, a sentence can be masked with several kinds of ambiguous words, such as animals, clothing, and vehicles, provided by Flickr30K (Plummer et al., 2015). High-frequency words labeled with noun (or nouns) are more likely to be masked as [MASK_N] (or [MASK_NS])). See Table 1 for example insufficient text with different numbers of masks."
    }, {
      "heading" : "2.2 Various Vision Features",
      "text" : "In addition to ResNet-50, we choose several Transformer-based vision models.\n• General Backbone. Vision Transformer (ViT) and Swin Transformer are popular models in computer vision (Dosovitskiy et al., 2021; Liu et al., 2021b). We use ViT with various model capacities to vary from weak to strong ViT models.\n• Object-detection. For pretrained objectdetection vision models, we choose DETR (Carion et al., 2020) and QueryInst (Fang et al., 2021) for their strong performance.\n• Image Captioning. For image captioning models, we choose CATR because it is a\nTransformer-based image captioning architecture and can be easily implemented on top of ViT.\nWe form a number of vision features by combining the methods described above. More details are presented in Section 3."
    }, {
      "heading" : "2.3 Selective Attention",
      "text" : "ViT and related models perform in almost the same way as Transformer in NLP (Vaswani et al., 2017). Unlike the general models in CV, ViT does not represent the image as a single vector. Instead, it generates a sequence of patches for image representation. An advantage of this design is that we can use the attention mechanism to correlate image patches to words. Thus, we present a selective attention model to model the patch-level contribution of the image. See Figure 1 for the architecture.\nText-only Transformer Transformer follows an encoder-decoder paradigm (the purple region in Figure 1) . The encoder is a stack of identical layers. Each layer consists of a self-attention (SAN) block and a feedforward network (FFN) block. The decoder shares a similar design with the encoder, but with an additional cross-attention block.\nGated Fusion Gated fusion mechanism is a popular technique for fusing representations from different sources (Wu et al., 2021; Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020). Given the text input X text and the image input X img, the text representation H text and the image feature H img can be defined as:\nH text = TransformerEncoder(X text) (1) H img = W ViT(X img) (2)\nwhereW is a projection matrix to convert the shape of ViT(X img) into that of H text. Note that ViT(·) can be replaced by other vision models, e.g. DETR, Swin Transformer and etc. Then, the gate λ ∈ [0, 1] and the fuzed output are defined as:\nλ = Sigmoid(UH text + V H img) (3) HOut = (1− λ) ·H text + λ ·H img (4)\nwhere U and V are trainable variables. λ controls how much visual information is kept. Then, the fusion vector HOut is fed into the decoder. See the pink region in Figure 1 for an illustration of the gated fusion models.\nSelective Attention After obtaining the text and image representations (or features), we use a singlehead attention network to correlate words with image patches, where the query, key and value are H text, H img and H img, respectively. Then the selective attention output H imgattn is defined to be:\nH img attn = Softmax( QKT√ dk )V (5)\nwhere dk is the same as the dimension of H text because a single head is used. Then the fused representation could be obtained by using Eqs. 3 and 4 and replacing H img with H imgattn."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We conducted experiments on the widely used Multi30K benchmark (Elliott et al., 2016). The training and validation sets consisted of 29,000 and 1,014 instances, respectively. We reported the results on the Test2016, Test2017 and MSCOCO test sets (Elliott et al., 2017). Note that MSCOCO is more challenging for MMT models due to the outof-domain instances with ambiguous verbs. Following the setup in (Wu et al., 2021), we learned a joint BPE code for 10,000 merging operations for both the source and target languages, resulting in vocabularies of 9,716 and 9,548 entries for the En-De and En-Fr tasks."
    }, {
      "heading" : "3.2 Experimental Setups",
      "text" : "We followed the Wu et al. (2021)’s work to conduct experiments with Transformer-Tiny configuration, which is more suited for small datasets like Multi30K. Note that smaller models even obtains\nhigher BLEU scores than pervious MMT models. Similar observations have been discussed when building context-aware machine translation models (Li et al., 2020). The model consists of 4 encoder and decoder layers. The hidden size is 128 and the filter size of FFN is 256. There are 4 heads in the multi-head self-attention mechanism. We set the dropout as 0.3 and the label smoothing as 0.1.\nOur implementation was based on Fairseq (Ott et al., 2019). For training, we used Adam Optimizer (Kingma and Ba, 2015) with β1 = 0.1, β2 = 0.98 and = 10−8. We adopted the same learning rate schedule as (Vaswani et al., 2017), where the learning rate first increased linearly for warmup = 2000 steps from 1e−7 to 5e−3. After the warmup, the learning rate decayed proportionally to the inverse square root of the current step. Each training batch contained 4,096 tokens. We also adopted the early-stop training strategy (Zhang et al., 2020) to avoid the overfitting issue.\nFor evaluation, we averaged the last 10 checkpoints for more reliable results. The width of beam\nsize was set to 5. The performance was measured by BLEU and METEOR for all test sets. Also we used accuracy for evaluation on the probing tasks."
    }, {
      "heading" : "3.3 Results",
      "text" : "Table 2 summarizes the results on standard MMT data. Each model was evaluated on three test sets on two language pairs. We see, first of all, that the improvements of previous methods (Rows 2-4) over the tiny baseline are marginal in terms of both BLEU and METEOR. This confirms the assumption that the visual features is not fully used if the text is complete (Caglayan et al., 2019). When switching the vision features from ResNet (Row.5) to ViT (Row.6), there are no significant BLEU gains. Then, we test them on the proposed probing\ntasks to examine the “real” contribution to MMT.\nColor-based Probing Table 3 shows the accuracy on the color-based probing task. We see that the accuracy improvement of the gated fusion method is marginal by both restrict and relaxed criteria. However, replacing ResNet by ViT yields gains of over 8 accuracy points across three test sets on En-De task. Similar improvements are observed on the En-Fr task. The finding here indicates that stronger vision features are helpful for representing the visual information. Moreover, selective attention can make better use of the ViT features, achieving +20 accuracy gains on three test sets. This verifies the conjecture that the selective attention can further enhance the fused representation\nfor the ViT features.\nCharacter-based Probing Table 4 shows similar results as in Table 3. ViT with selective attention performs the best. While the gated fusion method with ResNet feature behaves the worst, even compared with the text-only Transformer.\nNoun-based Probing Figure 2 plots the results of noun-based masking. It again verifies the above conjecture. The histograms in blue and red denote the results on the En-De and En-Fr tasks, respectively. The ViT features can significantly outperform the ResNet features across all masking methods on the two language pairs. We also observe that the gap between the ResNet and ViT features are gradually enlarged as more nouns are masked. This confirms the results in (Dosovitskiy et al., 2021)."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 How Vision Features Improve the MMT",
      "text" : "We further explore the impact of model capacity. Here, we report the results of ViT and Swin Transformer because they are strong models in recent studies. Our conjecture here is that larger ViT/Swin models can describe the image more accurately, which enables the text encoder to receive richer complementary information. Figure 3 depicts the BLEU scores in progressive noun masking scenarios. Intuitively, larger ViT and Swin models provide more complementary knowledge to complete the insufficient text representations.\nNevertheless, a counterintuitive phenomenon is the inferiority of Swin across all scenarios in the same configuration, though it outperforms ViT on most computer vision benchmarks. We attribute the reason to short length of the patch sequence. In\npatching, ViT has a length of 577 (576 sequence segments and a special token CLS) when the image resolution and the patching size are 384× 384 and 16×16. However, Swin has a fixed sequence length (49) restricted by the shifted window operation. This leads to more fine-grained local features for ViT, which is beneficial to the selective attention mechanism for extracting more relevant pieces."
    }, {
      "heading" : "4.2 Impact of Learning Objectives",
      "text" : "Then, we investigate the impact of the enhanced vision features on MMT. Previous studies have already attempted to leverage object-detection features (Zhao et al., 2020; Wang and Xiong, 2021) but the observation here is slightly different. Beyond the object-detection pretrained features, we also take the image captioning task into account.\nRows 11-13 in Table 2 summarize the results of the three enhanced vision features on the standard MMT data, and Figure 4 depicts the results on insufficient texts. Here we choose ViT-Tinybased models for comparison due to the similar model capacity they own2. We see that not only the object-detection (DETR and QueryInst), but also the image captioning (CATR) pretrained features obtain superior performance compared with ViT-tiny (Row 8) when the text is complete. It is consistent with previous findings (Yin et al., 2020; Zhao et al., 2020). However, the advantages do not persist when switching to limited text scenarios. A possible explanation is that these methods are sensitive to the quality of the extracted objects. We leave this as future work.\n2Only pretrained vision models in a 256 hidden-size are available"
    }, {
      "heading" : "4.3 Impact of Resolution and Patching Size",
      "text" : "It is well-known that higher resolutions are beneficial to the accuracy improvement in computer vision tasks (Dosovitskiy et al., 2021). Despite the success of the Transformer architecture, recent studies show that the success of ViT mainly comes from the successful use of the patching schema (Dosovitskiy et al., 2021). Here, we compare MMT systems with different resolutions and patch sizes based on ViT-Base. The results on three probing tasks (see Table 5) again confirm the above assumption that fine-grained vision features are more suited for the selective attention. Also, the attention map visualized in Figure 5 demonstrate that high resolution with fine-grained patching schema can attend to correct region of the image for each masked token. For example, both models pay the right attention to the masked character and noun, but the model with low resolution fails to detect the right region of color. The finding here may shed light to other multimodal tasks, such as VQA."
    }, {
      "heading" : "4.4 Incongruent Decoding",
      "text" : "Incongruent decoding is a widely used manner to evaluate whether the vision modality contributes to the text (Caglayan et al., 2019, 2021). Table 6 shows that incongruent decoding causes obvious BLEU drops except for the ResNet feature.\nViT beats the ResNet with gated fusion. It yields higher BLEU scores with congruent decoding and exhibits larger BLEU drop with incongruent decoding. We also find that the ViT features learned from scratch is also insensitive to the vision modality. This is reasonable that the learned vision systems are not sufficiently strong due to the data scarcity of Multi30K. Thus the vision modality acts more like noise signals. In addition, focusing on the results of pretrained selective attention + ViT, the gap between congruent and incongruent decoding gradually becomes larger. Also, the ensemble vision features perform the best. These results indicate that visual contexts help."
    }, {
      "heading" : "4.5 Case Study",
      "text" : "Finally, we compare several real cases. We choose gated fusion (CNN) (Wu et al., 2021) and selective attention + ViT_Base (ViT) for comparison. The qualitative examples in Table 7 demonstrate that the visual modality is complementary rather than redundant if the text is insufficient. To figure out whether the German translation is right or not, we provide the human-translation results. For example, ViT can fill in the masked entities and generate the correct translations even four entities were masked. Unfortunately, CNN incorrectly judges the man as a woman. Also, it cannot distinguish the right color\nof shirt due to the complex background. When given a more complex image, it is still a challenge for ViT to generate the totally right translation."
    }, {
      "heading" : "5 Related Work",
      "text" : "Multimodal machine translation is a cross-domain task in the filed of machine translation. Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017; Elliott and Kádár, 2017; Delbrouck and Dupont, 2017). However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020; Liu et al., 2021a). To address the above issue, Yao and Wan (2020) proposed a multimodal self-attention to consider the relative difference of information between two modalities. Similarly, Liu et al. (2021a) used a Gumbel Softmax to achieve the same goal.\nResearchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in\nCaglayan et al. (2019)’s work. They pointed out that the visual modality is still useful when the linguistic context is scarce, but is less sensitive when exposed to complete sentences. More recently, Wu et al. (2021) attributed the BLEU gain on MMT tasks to the regularization training. Caglayan et al. (2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this work, we show that stronger vision features (e.g. ViT-like models) strengthen MMT systems on three proposed probing tasks. We present a selective attention method for ViT-based models to make better use of the patch-level representation. The result here shows a promising line of research on developing better vision models for multimodal tasks. Our code and metrics for probing tasks will be open source soon."
    } ],
    "references" : [ {
      "title" : "Multimodal attention for neural machine translation",
      "author" : [ "Ozan Caglayan", "Loïc Barrault", "Fethi Bougares." ],
      "venue" : "CoRR, abs/1609.03976.",
      "citeRegEx" : "Caglayan et al\\.,? 2016",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2016
    }, {
      "title" : "Cross-lingual visual pretraining for multimodal machine translation",
      "author" : [ "Ozan Caglayan", "Menekse Kuyu", "Mustafa Sercan Amac", "Pranava Madhyastha", "Erkut Erdem", "Aykut Erdem", "Lucia Specia." ],
      "venue" : "Proceedings of the 16th Conference of the European",
      "citeRegEx" : "Caglayan et al\\.,? 2021",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2021
    }, {
      "title" : "Probing the need for visual context in multimodal machine translation",
      "author" : [ "Ozan Caglayan", "Pranava Madhyastha", "Lucia Specia", "Loïc Barrault." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Caglayan et al\\.,? 2019",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2019
    }, {
      "title" : "Incorporating global visual features into attention-based neural machine translation",
      "author" : [ "Iacer Calixto", "Qun Liu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 992–1003, Copenhagen, Denmark. Asso-",
      "citeRegEx" : "Calixto and Liu.,? 2017",
      "shortCiteRegEx" : "Calixto and Liu.",
      "year" : 2017
    }, {
      "title" : "End-to-end object detection with transformers",
      "author" : [ "Nicolas Carion", "Francisco Massa", "Gabriel Synnaeve", "Nicolas Usunier", "Alexander Kirillov", "Sergey Zagoruyko." ],
      "venue" : "Computer Vision - ECCV 2020 16th European Conference, Glasgow, UK, August",
      "citeRegEx" : "Carion et al\\.,? 2020",
      "shortCiteRegEx" : "Carion et al\\.",
      "year" : 2020
    }, {
      "title" : "An empirical study on the effectiveness of images in multimodal neural machine translation",
      "author" : [ "Jean-Benoit Delbrouck", "Stéphane Dupont." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 910–",
      "citeRegEx" : "Delbrouck and Dupont.,? 2017",
      "shortCiteRegEx" : "Delbrouck and Dupont.",
      "year" : 2017
    }, {
      "title" : "Adversarial evaluation of multimodal machine translation",
      "author" : [ "Desmond Elliott." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2974–2978, Brussels, Belgium. Association for Computational Lin-",
      "citeRegEx" : "Elliott.,? 2018",
      "shortCiteRegEx" : "Elliott.",
      "year" : 2018
    }, {
      "title" : "Findings of the second shared task on multimodal machine translation and multilingual image description",
      "author" : [ "Desmond Elliott", "Stella Frank", "Loïc Barrault", "Fethi Bougares", "Lucia Specia." ],
      "venue" : "Proceedings of the Second Conference on Machine Transla-",
      "citeRegEx" : "Elliott et al\\.,? 2017",
      "shortCiteRegEx" : "Elliott et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi30k: Multilingual englishgerman image descriptions",
      "author" : [ "Desmond Elliott", "Stella Frank", "Khalil Sima’an", "Lucia Specia" ],
      "venue" : "In Proceedings of the 5th Workshop on Vision and Language,",
      "citeRegEx" : "Elliott et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Elliott et al\\.",
      "year" : 2016
    }, {
      "title" : "Imagination improves multimodal translation",
      "author" : [ "Desmond Elliott", "Ákos Kádár." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 130–141, Taipei, Taiwan. Asian Federation of",
      "citeRegEx" : "Elliott and Kádár.,? 2017",
      "shortCiteRegEx" : "Elliott and Kádár.",
      "year" : 2017
    }, {
      "title" : "Instances as queries",
      "author" : [ "Yuxin Fang", "Shusheng Yang", "Xinggang Wang", "Yu Li", "Chen Fang", "Ying Shan", "Bin Feng", "Wenyu Liu." ],
      "venue" : "CoRR, abs/2105.01928.",
      "citeRegEx" : "Fang et al\\.,? 2021",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2021
    }, {
      "title" : "The MeMAD submission to the WMT18 multimodal",
      "author" : [ "Stig-Arne Grönroos", "Benoit Huet", "Mikko Kurimo", "Jorma Laaksonen", "Bernard Merialdo", "Phu Pham", "Mats Sjöberg", "Umut Sulubacak", "Jörg Tiedemann", "Raphael Troncy", "Raúl Vázquez" ],
      "venue" : null,
      "citeRegEx" : "Grönroos et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grönroos et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Sheffield submissions for WMT18 multimodal translation shared task",
      "author" : [ "Chiraag Lala", "Pranava Swaroop Madhyastha", "Carolina Scarton", "Lucia Specia." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 624–",
      "citeRegEx" : "Lala et al\\.,? 2018",
      "shortCiteRegEx" : "Lala et al\\.",
      "year" : 2018
    }, {
      "title" : "Does multi-encoder help? a case study on contextaware neural machine translation",
      "author" : [ "Bei Li", "Hui Liu", "Ziyang Wang", "Yufan Jiang", "Tong Xiao", "Jingbo Zhu", "Tongran Liu", "Changliang Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention strategies for multi-source sequence-to-sequence learning",
      "author" : [ "Jindřich Libovický", "Jindřich Helcl." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics 9",
      "citeRegEx" : "Libovický and Helcl.,? 2017",
      "shortCiteRegEx" : "Libovický and Helcl.",
      "year" : 2017
    }, {
      "title" : "Dynamic context-guided capsule network for multimodal machine translation",
      "author" : [ "Huan Lin", "Fandong Meng", "Jinsong Su", "Yongjing Yin", "Zhengyuan Yang", "Yubin Ge", "Jie Zhou", "Jiebo Luo." ],
      "venue" : "MM ’20: The 28th ACM International Conference on Mul-",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Gumbel-attention for multi-modal machine translation",
      "author" : [ "Pengbo Liu", "Hailong Cao", "Tiejun Zhao." ],
      "venue" : "CoRR, abs/2103.08862.",
      "citeRegEx" : "Liu et al\\.,? 2021a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Swin transformer: Hierarchical vision transformer using shifted windows",
      "author" : [ "Ze Liu", "Yutong Lin", "Yue Cao", "Han Hu", "Yixuan Wei", "Zheng Zhang", "Stephen Lin", "Baining Guo." ],
      "venue" : "CoRR, abs/2103.14030.",
      "citeRegEx" : "Liu et al\\.,? 2021b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
      "author" : [ "Bryan A. Plummer", "Liwei Wang", "Chris M. Cervantes", "Juan C. Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik." ],
      "venue" : "2015 IEEE International",
      "citeRegEx" : "Plummer et al\\.,? 2015",
      "shortCiteRegEx" : "Plummer et al\\.",
      "year" : 2015
    }, {
      "title" : "A shared task on multimodal machine translation and crosslingual image description",
      "author" : [ "Lucia Specia", "Stella Frank", "Khalil Sima’an", "Desmond Elliott" ],
      "venue" : "In Proceedings of the First Conference on Machine Translation: Volume",
      "citeRegEx" : "Specia et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Efficient objectlevel visual context modeling for multimodal machine translation: Masking irrelevant objects helps grounding",
      "author" : [ "Dexin Wang", "Deyi Xiong." ],
      "venue" : "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Confer-",
      "citeRegEx" : "Wang and Xiong.,? 2021",
      "shortCiteRegEx" : "Wang and Xiong.",
      "year" : 2021
    }, {
      "title" : "Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation",
      "author" : [ "Zhiyong Wu", "Lingpeng Kong", "Wei Bi", "Xiang Li", "Ben Kao." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Multimodal transformer for multimodal machine translation",
      "author" : [ "Shaowei Yao", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4346– 4350, Online. Association for Computational Lin-",
      "citeRegEx" : "Yao and Wan.,? 2020",
      "shortCiteRegEx" : "Yao and Wan.",
      "year" : 2020
    }, {
      "title" : "A novel graph-based multi-modal fusion encoder for neural machine translation",
      "author" : [ "Yongjing Yin", "Fandong Meng", "Jinsong Su", "Chulun Zhou", "Zhengyuan Yang", "Jie Zhou", "Jiebo Luo." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation with universal visual representation",
      "author" : [ "Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Ad-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Double attention-based multimodal neural machine translation with semantic image regions",
      "author" : [ "Yuting Zhao", "Mamoru Komachi", "Tomoyuki Kajiwara", "Chenhui Chu." ],
      "venue" : "Proceedings of the 22nd Annual Conference of the European Association for",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Multimodal machine translation (MMT) has emerged as an active field of research which marries the worlds of computer vision (CV) and natural language processing (NLP) (Specia et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 0,
      "context" : "Early models of this kind produce a translation given the fused representation of both the visual and textual inputs (Caglayan et al., 2016; Libovický and Helcl, 2017; Calixto and Liu, 2017).",
      "startOffset" : 117,
      "endOffset" : 190
    }, {
      "referenceID" : 15,
      "context" : "Early models of this kind produce a translation given the fused representation of both the visual and textual inputs (Caglayan et al., 2016; Libovický and Helcl, 2017; Calixto and Liu, 2017).",
      "startOffset" : 117,
      "endOffset" : 190
    }, {
      "referenceID" : 3,
      "context" : "Early models of this kind produce a translation given the fused representation of both the visual and textual inputs (Caglayan et al., 2016; Libovický and Helcl, 2017; Calixto and Liu, 2017).",
      "startOffset" : 117,
      "endOffset" : 190
    }, {
      "referenceID" : 11,
      "context" : "For example, it is not harmful to MMT systems when the input image is irrelevant to the text (Grönroos et al., 2018; Lala et al., 2018), or even when the vision features are absent (Elliott, 2018).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "For example, it is not harmful to MMT systems when the input image is irrelevant to the text (Grönroos et al., 2018; Lala et al., 2018), or even when the vision features are absent (Elliott, 2018).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : ", 2018), or even when the vision features are absent (Elliott, 2018).",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : "But computer vision is facing a new trend by moving from CNNs to Transformer as the backbone model (Dosovitskiy et al., 2021; Liu et al., 2021b; Carion et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "But computer vision is facing a new trend by moving from CNNs to Transformer as the backbone model (Dosovitskiy et al., 2021; Liu et al., 2021b; Carion et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "Beyond this, we introduce object-detection and image captioning features into MMT for further improvements of the vision models (Carion et al., 2020; Fang et al., 2021).",
      "startOffset" : 128,
      "endOffset" : 168
    }, {
      "referenceID" : 10,
      "context" : "Beyond this, we introduce object-detection and image captioning features into MMT for further improvements of the vision models (Carion et al., 2020; Fang et al., 2021).",
      "startOffset" : 128,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "Following (Caglayan et al., 2019)’s work, we design more detailed probing tasks to examine to what degree the vision modality contributes to MMT.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : "Noun-based Probing For more complex scenarios, a sentence can be masked with several kinds of ambiguous words, such as animals, clothing, and vehicles, provided by Flickr30K (Plummer et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 196
    }, {
      "referenceID" : 18,
      "context" : "Vision Transformer (ViT) and Swin Transformer are popular models in computer vision (Dosovitskiy et al., 2021; Liu et al., 2021b).",
      "startOffset" : 84,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "For pretrained objectdetection vision models, we choose DETR (Carion et al., 2020) and QueryInst (Fang et al.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : ", 2020) and QueryInst (Fang et al., 2021) for their strong performance.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "ViT and related models perform in almost the same way as Transformer in NLP (Vaswani et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : "Gated Fusion Gated fusion mechanism is a popular technique for fusing representations from different sources (Wu et al., 2021; Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 182
    }, {
      "referenceID" : 27,
      "context" : "Gated Fusion Gated fusion mechanism is a popular technique for fusing representations from different sources (Wu et al., 2021; Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "Gated Fusion Gated fusion mechanism is a popular technique for fusing representations from different sources (Wu et al., 2021; Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 182
    }, {
      "referenceID" : 26,
      "context" : "Gated Fusion Gated fusion mechanism is a popular technique for fusing representations from different sources (Wu et al., 2021; Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "We conducted experiments on the widely used Multi30K benchmark (Elliott et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "We reported the results on the Test2016, Test2017 and MSCOCO test sets (Elliott et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "Following the setup in (Wu et al., 2021), we learned a joint BPE code for 10,000 merging operations for both the source and target languages, resulting in vocabularies of 9,716 and 9,548 entries for the En-De and En-Fr tasks.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "Similar observations have been discussed when building context-aware machine translation models (Li et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "Our implementation was based on Fairseq (Ott et al., 2019).",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "For training, we used Adam Optimizer (Kingma and Ba, 2015) with β1 = 0.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : "We adopted the same learning rate schedule as (Vaswani et al., 2017), where the learning rate first increased linearly for warmup = 2000 steps from 1e−7 to 5e−3.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "We also adopted the early-stop training strategy (Zhang et al., 2020) to avoid the overfitting issue.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "This confirms the assumption that the visual features is not fully used if the text is complete (Caglayan et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 28,
      "context" : "Previous studies have already attempted to leverage object-detection features (Zhao et al., 2020; Wang and Xiong, 2021) but the observation here is slightly different.",
      "startOffset" : 78,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "Previous studies have already attempted to leverage object-detection features (Zhao et al., 2020; Wang and Xiong, 2021) but the observation here is slightly different.",
      "startOffset" : 78,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "It is consistent with previous findings (Yin et al., 2020; Zhao et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 77
    }, {
      "referenceID" : 28,
      "context" : "It is consistent with previous findings (Yin et al., 2020; Zhao et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "We choose gated fusion (CNN) (Wu et al., 2021) and selective attention + ViT_Base (ViT) for comparison.",
      "startOffset" : 29,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017; Elliott and Kádár, 2017; Delbrouck and Dupont, 2017).",
      "startOffset" : 104,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : "Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017; Elliott and Kádár, 2017; Delbrouck and Dupont, 2017).",
      "startOffset" : 104,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017; Elliott and Kádár, 2017; Delbrouck and Dupont, 2017).",
      "startOffset" : 104,
      "endOffset" : 180
    }, {
      "referenceID" : 25,
      "context" : "However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020; Liu et al., 2021a).",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020; Liu et al., 2021a).",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018).",
      "startOffset" : 131,
      "endOffset" : 146
    } ],
    "year" : 0,
    "abstractText" : "Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning). We develop a selective attention model to study the patch-level contribution of an image in MMT. On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the vision modality. Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased.",
    "creator" : null
  }
}
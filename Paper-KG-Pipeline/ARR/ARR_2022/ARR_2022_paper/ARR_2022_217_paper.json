{
  "name" : "ARR_2022_217_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "XDBERT: Distilling Visual Information to BERT via Cross-Modal Encoders to Improve Language Understanding",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Transformer-based models are extensively used in natural language understanding (NLU) tasks, and some prominent pretraining strategies include BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), and ELECTRA (Clark et al., 2020). Despite their differences in curating the learning objectives, they all utilize text-based datasets only. In the real world, however, humans can benefit from the visual modality when acquiring knowledge from language; an obvious example is learning visually grounded words, such as colors and shapes.\nSome studies have succeeded with visually grounded information used in NLU. ViCo (Gupta et al., 2019) learned visual co-occurrences in text and reported superior performance to GloVe in word analogy problems. Zhang et al. (2020) and Huang et al. (2020) used images to boost translation performance in supervised and unsupervised\nsettings. Tan and Bansal (2020) reported improvements over BERT on NLU by proposing the concept of vokenization.\nAnother branch of research focuses on solving multimodal downstream tasks such as visual question answering and image retrieval Li et al. (2019) , Lu et al. (2019) , Su et al. (2020) and Li et al. (2020) trained visual-text transformers, while LXMERT (Tan and Bansal, 2019) uses different encoders for text and image and a cross-modal encoder. Tan and Bansal (2020) tested these models with general language understanding evaluation (GLUE Wang et al. (2018)) and found that the performance does not exceed using BERT (Appendix A), drawing the conclusion that vision-and-language pretraining on visually-grounded language dataset failed to distill useful information for general NLU. CLIP (Radford et al., 2021) utilizes contrastive loss to reach SOTA on zero-shot image classification in a retrieval fashion.\nIn this work, we established the link between pretrained multimodal transformers and visuallygrounded language learning. We devised a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, briefed by CLIP-T) to pretrained language transformers (BERT/ELECTRA). The usage of a visually grounded text-transformer as a teacher allows us to implement straightforward and nonfuzzy adapting tasks for distillation. We show that it is mathematically logical that the CLIP-T output approximates visual features (Sec. 2.2), and also the linguistic competence of CLIP-T is low (Sec. 3), to prove that the distilled information is predominantly visual and thus non-trivial to the pretrained-language transformer despite having textual inputs.\nMethodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller\nthan the original pretraining steps). While adapting pretrained-BERT, we favor a document-level corpus (wiki103) over a vision-language corpus (MSCOCO) due to claims from Devlin et al. (2019)1 and results from Tan and Bansal (2020). The adapting tasks are joint masked language modeling (MLM), same sentence prediction, and CLIP token classification tasks, which are resemblant of BERT pretraining tasks to cater to the languageheavy characteristics of NLU. We do ablation studies to show that each of the task provides improvement (Appendix D).\nDuring finetuning, we finetune XDBERT (crossmodal distilled BERT), which is the language encoder after adaptation. We evaluate the linguistic capabilities of the model by finetuning on GLUE, situations with adversarial generations (SWAG (Zellers et al., 2018)) benchmarks, and readability benchmarks2. The resulting XDBERT outperforms pretrained BERT, proving that our adaptation strategy distills useful visual knowledge into BERT (right of Figure 1). We provide analysis to show that the improvements are visually grounded.\nWe summarize our contribution as follow:\n• We explore distilling visual information from a pretrained multimodal transformer to a pretrained language transformer and improved NLU performance.\n• Our adapting method is efficient and exten-\n1\"It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the BillionWord Benchmark in order to extract long contiguous sequences\"\n2https://www.kaggle.com/c/commonlitreadabilityprize\nsible to different combinations of pretrainedlanguage encoders (BERT/ELECTRA)."
    }, {
      "heading" : "2 Proposed Method",
      "text" : "The training process consists of three phases: pretraining, adaptation, and finetuning (Figure 1). Our proposed method focuses on the adaptation phase with pretrained models, so pretraining is not a part of our experiment, but we explain all three phases for completeness. The adaptation phase incorporates the cross-modal transformer structure to jointly learn from CLIP-T and BERT outputs."
    }, {
      "heading" : "2.1 Model Architecture",
      "text" : "The cross-modal transformer (middle of Figure 1) consists of a cross-modal encoder, CLIP-T and BERT. CLIP-T has the same module connections as BERT with only parameter differences (specifications in Appendix B). The cross-modal encoder consists of repeating cross-modal encoder layers, which is an extension to single-modality encoder layers (layers of BERT/CLIP-T) in Figure 2. The added cross-attention module follows the attention\nformula (Vaswani et al., 2017): Attention output = softmax ( Q ∗ KT / √ D ) V\nfor queries (Q), keys (K) and values (V) of dimension D, however, Q is generated from a modality other than K and V. We choose the number of cross-modal encoder layers to be 2."
    }, {
      "heading" : "2.2 Pretraining",
      "text" : "BERT is trained using the next sentence prediction and masked language modeling. CLIP is an imagetext matching system with two components, a text encoder (CLIP-T), and an image encoder (CLIPViT), which learn to encode paired inputs to closer output embeddings via contrastive loss. The trained representation has the following properties:\ncos(Hi, Vi) >> cos(Hi, Vj)(i ̸= j)\ncos(Hi, Vi) >> cos(Hj , Vi)(i ̸= j)\nwhere Hi is the CLIP text encoder output of Xi, and Vi is the CLIP image encoder output of Yi. The text-image input (Xi, Yi) is paired, and every (Xj , Yk) (j ̸= k) is a non-pair. Since Hi and Vi are normalized and have a length of 1, we have\nHi ≈ Vi\nTherefore, we use the CLIP text encoder output to approximate CLIP image encoder output for a straightforward adaptation process."
    }, {
      "heading" : "2.3 Adaptation",
      "text" : "We define three adapting tasks that can be learned in a self-supervised manner, which is visualized in Figure 1. In these tasks, BERT and CLIP-T takes sentences A and B respectively as input as we condition the output. Our adapting tasks closely follow BERT text pretraining strategies to retain linguistic competence. Unlike pretraining, the adaptation is computationally inexpensive, as we found that training 1 epoch on wiki103 was already effective. Further training details can be found in Appendix C."
    }, {
      "heading" : "2.3.1 Joint Masked Language Modeling (MLM)",
      "text" : "The MLM objective teaches the model to reconstruct masked tokens. The masked ratio and masked token replacement probabilities follow Devlin et al. (2019). Since there is no equivalent of a [MASK] token in CLIP, we leave the sentence as is."
    }, {
      "heading" : "2.3.2 Same sentence prediction (MATCH)",
      "text" : "When choosing the input sentences for BERT and CLIP-T, we make the inputs nonidentical 50% of the time. A binary classifier over [CLS] differentiates between the two cases. This motivates the [CLS] output to encode sentence related information, and trains the cross-attention weights."
    }, {
      "heading" : "2.3.3 CLIP Token Classification",
      "text" : "This is the MLM objective done on the CLIP-T side of the full model, but the CLIP-T sentence input tokens is neither being masked nor replaced. This loss prevents the cross-modal matching to only focus on common trivial words. We show the attention maps of the cross-modal encoders in Appendix D to verify this."
    }, {
      "heading" : "2.4 Finetuning",
      "text" : "Finetuning follows the methods described in Devlin et al. (2019), and is applied to the language encoder only (XDBERT), therefore the number of parameters are kept equal to pretrained-BERT."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "We evaluated our model on three NLU benchmarks, namely GLUE, SWAG and READ. We tested our pretraining strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large. We fix the finetuning parameters between models where comparison is intended, and select the median result of multiple runs. Details of finetuning are provided in Appendix C.\nTable 1 shows experimental results. Each of our XD-model constantly outperforms the original encoder (For fair comparison, we train the original encoder with one more epoch of wiki103). We found that performance gains are more significant on smaller datasets (RTE, MRPC, STSB, CoLA), indicating that visual features help increase generalization when the amount of training data is limited. The gains are also significant on the readability benchmark (READ).\nWe show that the results of finetuning CLIPT alone on GLUE does not perform well. Since the language capability of the CLIP-T model is weak, the distilled information obtained by XDBERT/XDELECTRA is predominantly visual.\nIt is also possible to finetune the entire crossmodal transformer after adaptation. The performance further increases but the model has more parameters. The results are in Appendix C.3."
    }, {
      "heading" : "4 Analysis",
      "text" : "To justify the use of a cross-modal encoder, we first conducted a pairwise projection weighted canonical correlation analysis (PWCCA) on word embeddings. The PWCCA is a good measure to determine how close the distributions of two vector groups are to each other. The PWCCA results in Table 2 show low scores on both BERT/CLIP and ELECTRA/CLIP before co-training, so the cross-modal encoder is useful in learning from both distributions.\nWe inspect the RTE results of 5 runs in detail to show that the improvements are likely from visual\ninformation of CLIP-T. Over the 5 runs, XDBERTb has accumulated +55 more correct classifications than BERT-b, or +3.97%(11/277) gain in performance. We separate the RTE entries into three different corpora and analyze them as a whole, based on whether XDBERT-b improves classification over BERT-b. XDBERT-b gains performance on 52 entries, while performing on par or worse on 225 entries. We found out that the better-performing entries have a shorter sentence length and their tokens have a larger visually grounded ratio (Figure 3). The enhancement of visually grounded token representations is a rough indicator that XDBERT has obtained distilled visual information from CLIP-T. We show examples of each category in Appendix E."
    }, {
      "heading" : "5 Ablation study",
      "text" : "We briefly describe the ablation studies on the adaptation process. We tried various combinations of adaptation tasks and found out that using all three yielded the best results. We also tried to reduce the number of cross-modal encoder layers to one; however, no further improvements were made upon the visually grounded language encoder. We tested these changes on RTE, MPRC, STSB, and CoLA on 5 random seeds, and the results are shown in Appendix D."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this study, we explored using cross-modal encoders to distill visual information to BERT. We adapted the model with multiple objectives, and we were able to achieve improved performance on NLU tasks. Our adaptation techniques are computationally inexpensive and straightforward. Furthermore, our method is language encoder agnostic, as we show similar performance gains on XDELECTRA."
    }, {
      "heading" : "B Modeling sequences on CLIP",
      "text" : "While BERT and CLIP have similar forwarding mechanisms, the specifications of the transformer architecture are different, resulting in challenges to jointly model both models (Table 3).\nMismatching dimensions pose a problem in cross-attention. We use a linear transformation to generate Q, K, and V of matching dimensions, but clarify that this linear transformation layer exists in the original LXMERT setting where hidden representations have unified dimensions.\nWe modify the input to address the mismatched max_len of the two systems. In the joint MLM, we used a fixed sequence length of 512 for the BERT. However, the same cannot be done for CLIP as the maxmum model sequence length is 77 for CLIP. We found that most BERT sequences (>99%) of length 512 encode into CLIP sequences of length less than 693, so we pad the CLIP sequence to length 693, and then split the CLIP sequence into 9 sub-sequences of length 77. Therefore, a batch of inputs will contain BERT inputs of size (batch_size, 512) and CLIP inputs of size (batch_size, 9, 77). The output was resized to (batch_size, 693) in the cross-modal encoder. The issue is also present in the finetuning phase, and the maxmum sequence length of GLUE and SWAG is 128; therefore we\nused 2 blocks of CLIP sub-sequences to model it. For bi-sequence classification tasks such as RTE and MRPC, we ensure that separate sentences do not use the same block in the CLIP encoder. Therefore, uni-sequence classification tasks will have a CLIP input size of (batch_size, 2, 77) and the bisequence classification task will have a CLIP input size of (batch_size, 4, 77)."
    }, {
      "heading" : "C Further Training Details",
      "text" : "C.1 Adaptation\nWe use publicly available wiki103 and preprocessing methods similar to Tan and Bansal (2020) 3. Wiki103 (500MB) is a subset of the Wikipedia corpus consisting of only good and featured articles. The adaptation of 1 epoch on wiki103 finished in 35 minutes on 8 V100s (BERT-base). We trained for at most 20 epochs( 16k steps) and found that further adaptation steps did not increase scores in early epochs, and significantly decreased performance in late epochs. We used the following parameters for adaptation : learning rate = 1e-4, max_epoch = 40 (although we stopped early due to plummeting performance), warmup ratio = 0.05\nC.2 Finetuning\nThe learning rates are listed in Table 4.\nWe used a warmup ratio of 0.1, with a learning rate decay of 0.9, and trained the model for 3 epochs. We report the median results of 5 runs on different random seeds, except for RTE, which is unstable; therefore, we report the median results of 9 runs instead. The reproduce results of ELECTRA on RTE and STSB are lower than values reported by Clark et al. (2020) because we did not start from an MNLI checkpoint.\nC.3 Finetuning with Full Model\nSince our cross-modal transformer itself is can also be viewed as a language encoder, finetuning can\n3https://github.com/airsplay/ vokenization\nbe done on the full model. This approach, however, adds extra parameters to pretrained-BERT, so comparison with pretrained-BERT is not intended, instead, we focus on showing the feasibility of this approach. The number of additional parameters is only a function of the hidden size in BERT/ELECTRA, so when the language encoder is large, the ratio of additional parameters is much more insignificant. To simplify notations, we use X(language encoder) to represent the full model. The number of parameters of the full model is shown in Table 5 and the results on NLU tasks are shown in Table 7."
    }, {
      "heading" : "D Ablation Study Results",
      "text" : "We report the performance of small tasks while using different loss functions. The results are shown in Table 8, where MLM refers to the joint MLM, MATCH refers to cross-modal matching, and VC (visual classification) refers to the CLIP token classification. Other attempts include changing the number of layers in the cross-modal encoder, training for longer, and swapping to a much larger wiki (14G).\nBesides experimental evidence, we also justify the VC loss via further analysis because VC loss can theoretically be trivially solved by identity mapping. The importance of this loss is to balances out the cross-modal matching loss. Without VC loss, the cross-modal matching is too easy, because relying on only a few common words is sufficient, even when the word is trivial to the meaning of the sentence. Driven by the loss function of cross-modal matching, the model sacrifices rare word representations to better match common words. By adding VC loss, each VC token must keep its representation, which makes the cross-modal attention more robust. We show the attention of the cross-modal matching with a random sequence from RTE in Table 9."
    }, {
      "heading" : "E RTE Examples",
      "text" : "We provide three RTE example of each type in Figure3, and we choose extreme examples where performance difference is huge over 5 runs for both \"Improved\" and \"Worsened\" categories. We follow Tan and Bansal (2020) to classify tokens as visually-grounded if it is not a stopword and has more than 100 occurrences in MSCOCO. In the following examples, Bold words are visuallygrounded, while normal words are non-visuallygrounded. Words in brackets are stopwords and does not count towards either category.\nE.1 Improved : XDBERT outperforms BERT Example1 : Visually-grounded ratio : 11/(11+16) = 0.4074 BERT answered correctly : 0/5 XDBERT answered correctly : 5/5\nhands across (the) divide (was) formed (in) march 2001 (,) (and) one (of) (its) immediate aims (was) (to) press (for) (more) freedom (of) contact (and) communication right away (between) (the) two parts (of) cyprus (,) (and) (for) early progress towards (a) solution (to) (’) (the) cyprus problem (’) (.)\ncyprus (was) divided (into) two parts (in) march 2001 (.)\nExample2 : Visually-grounded ratio : 4/(10+4) = 0.2857 BERT answered correctly : 0/5 XDBERT answered correctly : 5/5\n(it) (is) hoped (that) women (,) (who) constitute (more) (than) half (of) (the) population (,) (will) vote (for) (other) women (and) ensure (that) (their) issues (are) represented (in) parliament (.)\nwomen (are) poorly represented (in) parliament (.)\nExample3 : Visually-grounded ratio : 13/(13+17) = 0.4333 BERT answered correctly : 0/5 XDBERT answered correctly : 5/5\nho ##dler claimed (there) (were) also irregu-\nlarities (in) (the) campaigns organized (by) atlanta (for) (the) 1996 summer games (,) sydney (for) (the) summer olympics (in) 2000 (and) salt lake city (for) (the) 2002 winter games (.)\n(before) salt lake city (,) winter olympic games took place (in) naga ##no (.)\nE.2 On Par : XDBERT and BERT perform equally\nExample1 : Visually-grounded ratio : 6/(6+32) = 0.1375 BERT answered correctly : 0/5 XDBERT answered correctly : 0/5\n(on) october 1 2001 (,) eu (and) (other) countries introduced (the) option (for) domestic animal owners (to) apply (for) pet passports (under) (the) pets travel scheme (() pets (for) short ()) (,) (for) pets returning (from) abroad (to) (the) united kingdom (.) (this) replaced (the) old system(of) 6 months compulsory qu ##aran ##tine (for) (all) domestic pets (.)\n(in) 2001 (,) (the) eu introduced (a) passport(for) pets (.)\nExample2 : Visually-grounded ratio : 5/(5+16) = 0.2381 BERT answered correctly : 5/5 XDBERT answered correctly : 5/5\nsecurity forces (were) (on) high alert (after) (an) election campaign (in) (which) (more) (than) 1 (,) 000 people (,) including seven election candidates (,) (have) (been) killed (.)\nsecurity forces (were) (on) high alert (after) (a) campaign marred (by) violence (.)\nExample3 : Visually-grounded ratio : 8/(8+16) = 0.3333 BERT answered correctly : 5/5 XDBERT answered correctly : 5/5\n(in) 1979 (,) (the) leaders signed (the) egypt (-) israel peace treaty (on) (the) white house lawn (.) (both) president begin (and) sad ##at received (the) nobel peace prize (for) (their) work (.) (the) two nations (have) enjoyed peaceful relations (to) (this) day (.)\n(the) israel (-) egypt peace agreement (was) signed (in) 1979 (.)\nE.3 Worsened : XDBERT underperforms BERT\nExample1 : Visually-grounded ratio : 11/(11+29) = 0.2750 BERT answered correctly : 5/5 XDBERT answered correctly : 0/5\njean (-) claude tri ##chet (,) (the) european central bank president (,) made (it) clear (,) (on) wednesday (,) (that) (he) would oppose un ##war ##rant ##ed political attempts (to) remove antonio fa ##zio (:) (the) bank (of) italy governor (,) engulfed (in) controversy (over) (his) handling (of) bank takeover bids (.)\nantonio fa ##zio (is) subordinate (to) jean (-) claude tri ##chet (.)\nExample2 : Visually-grounded ratio : 11/(11+29) = 0.4167 BERT answered correctly : 5/5 XDBERT answered correctly : 0/5\n(about) half (were) along (a) 20 (-) mile stretch (of) santa monica bay (from) top anga canyon boulevard (to) (the) palo s verde s peninsula (.)\n(the) coastline (of) santa monica bay (is) 50 miles long (.)\nExample3 : Visually-grounded ratio : 32/(32+55) = 0.3678 BERT answered correctly : 5/5 XDBERT answered correctly : 0/5\ncairo (is) (now) home (to) (some) 15 mil-\nlion people (-) (a) bu ##rgeon ##ing population (that) produces approximately 10 (,) 000 tonnes (of) rubbish per day (,) putting (an) enormous strain (on) public services (.) (in) (the) past 10 years (,) (the) government (has) tried hard (to) encourage private investment (in) (the) refuse sector (,) (but) (some) estimate 4 (,) 000 tonnes (of) waste (is) left behind every day (,) fest ##ering (in) (the) heat (as) (it) waits (for) someone (to) clear (it) (up) (.) (it) (is) often (the) people (in) (the) poor ##est neighbourhoods (that) (are) worst affected (.) (but) (in) (some) areas (they) (are) fighting back (.) (in) shu ##bra (,) one (of) (the) northern districts (of) (the) city (,) (the) residents (have) taken (to) (the) streets armed (with) dust ##pan ##s (and) brushes (to) clean (up) public areas (which) (have) (been) used (as) public dump ##s (.)\n15 million tonnes (of) rubbish (are) produced daily (in) cairo (.)"
    } ],
    "references" : [ {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Vico: Word embeddings from visual co-occurrences",
      "author" : [ "Tanmay Gupta", "Alexander G. Schwing", "Derek Hoiem." ],
      "venue" : "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised multimodal neural machine translation with pseudo visual pivoting",
      "author" : [ "Po-Yao Huang", "Junjie Hu", "Xiaojun Chang", "Alexander Hauptmann." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "CoRR, abs/1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Oscar: Object-semantics aligned pretraining for vision-language tasks",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Pengchuan Zhang", "Xiaowei Hu", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "Computer Vi-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Pro-",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning transferable visual models from natural language",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "VL-BERT: pretraining of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April",
      "citeRegEx" : "Su et al\\.,? 2020",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2020
    }, {
      "title" : "LXMERT: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Vokenization: Improving language understanding with contextualized, visual-grounded supervision",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2066–2080,",
      "citeRegEx" : "Tan and Bansal.,? 2020",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Swag: A large-scale adversarial dataset for grounded commonsense inference",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:1808.05326.",
      "citeRegEx" : "Zellers et al\\.,? 2018",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2018
    }, {
      "title" : "2020) to classify tokens as visually-grounded if it is not a stopword and has more than 100 occurrences in MSCOCO. In the following examples, Bold words are visuallygrounded, while normal words are non-visually",
      "author" : [ "Tan", "Bansal" ],
      "venue" : null,
      "citeRegEx" : "Tan and Bansal,? \\Q2020\\E",
      "shortCiteRegEx" : "Tan and Bansal",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Transformer-based models are extensively used in natural language understanding (NLU) tasks, and some prominent pretraining strategies include BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : ", 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : ", 2019), ALBERT (Lan et al., 2020), and ELECTRA (Clark et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "ViCo (Gupta et al., 2019) learned visual co-occurrences in text and reported superior performance to GloVe in word analogy problems.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "ViCo (Gupta et al., 2019) learned visual co-occurrences in text and reported superior performance to GloVe in word analogy problems. Zhang et al. (2020) and Huang et al. (2020) used images to boost translation performance in supervised and unsupervised settings.",
      "startOffset" : 6,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : "ViCo (Gupta et al., 2019) learned visual co-occurrences in text and reported superior performance to GloVe in word analogy problems. Zhang et al. (2020) and Huang et al. (2020) used images to boost translation performance in supervised and unsupervised settings. Tan and Bansal (2020) reported improvements over BERT on NLU by proposing the concept of vokenization.",
      "startOffset" : 6,
      "endOffset" : 285
    }, {
      "referenceID" : 5,
      "context" : "Another branch of research focuses on solving multimodal downstream tasks such as visual question answering and image retrieval Li et al. (2019) ,",
      "startOffset" : 128,
      "endOffset" : 145
    }, {
      "referenceID" : 11,
      "context" : "(2020) trained visual-text transformers, while LXMERT (Tan and Bansal, 2019) uses different encoders for text and image and a cross-modal encoder.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "CLIP (Radford et al., 2021) utilizes contrastive loss to reach SOTA on zero-shot image classification in a retrieval fashion.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "(2020) and Li et al. (2020) trained visual-text transformers, while LXMERT (Tan and Bansal, 2019) uses different encoders for text and image and a cross-modal encoder.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "(2020) and Li et al. (2020) trained visual-text transformers, while LXMERT (Tan and Bansal, 2019) uses different encoders for text and image and a cross-modal encoder. Tan and Bansal (2020) tested these models with general language understanding evaluation (GLUE Wang et al.",
      "startOffset" : 11,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "(2020) and Li et al. (2020) trained visual-text transformers, while LXMERT (Tan and Bansal, 2019) uses different encoders for text and image and a cross-modal encoder. Tan and Bansal (2020) tested these models with general language understanding evaluation (GLUE Wang et al. (2018)) and found that the performance does not exceed using BERT (Appendix A), drawing the conclusion that vision-and-language pretraining on visually-grounded language dataset failed to distill useful information for general NLU.",
      "startOffset" : 11,
      "endOffset" : 282
    }, {
      "referenceID" : 11,
      "context" : "Methodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "While adapting pretrained-BERT, we favor a document-level corpus (wiki103) over a vision-language corpus (MSCOCO) due to claims from Devlin et al. (2019)1 and results from Tan and Bansal (2020).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "While adapting pretrained-BERT, we favor a document-level corpus (wiki103) over a vision-language corpus (MSCOCO) due to claims from Devlin et al. (2019)1 and results from Tan and Bansal (2020). The adapting tasks are joint masked language modeling (MLM), same sentence prediction, and CLIP token classification tasks, which are resemblant of BERT pretraining tasks to cater to the languageheavy characteristics of NLU.",
      "startOffset" : 133,
      "endOffset" : 194
    }, {
      "referenceID" : 15,
      "context" : "We evaluate the linguistic capabilities of the model by finetuning on GLUE, situations with adversarial generations (SWAG (Zellers et al., 2018)) benchmarks, and readability benchmarks2.",
      "startOffset" : 122,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "The masked ratio and masked token replacement probabilities follow Devlin et al. (2019). Since there is no equivalent of a [MASK] token in CLIP, we leave the sentence as is.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "Finetuning follows the methods described in Devlin et al. (2019), and is applied to the language encoder only (XDBERT), therefore the number of parameters are kept equal to pretrained-BERT.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "BERT-b is the BERT-base-uncased model from Devlin et al. (2019), while XDBERT-b is the proposed models shown in the right part of Figure 1.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "ELECTRA-b and ELECTRA-l refer to the ELECTRA-base model and the ELECTRA-large model from Clark et al. (2020) respectively.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "RTE consists of two sentences, so we report them separately, and the visually grounded ratio estimation follows Tan and Bansal (2020).",
      "startOffset" : 112,
      "endOffset" : 134
    } ],
    "year" : 0,
    "abstractText" : "Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders’ success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of NLU. After training with a small number of extra adapting steps and finetuned, the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in general language understanding evaluation (GLUE), situations with adversarial generations (SWAG) benchmarks, and readability benchmarks. We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded.",
    "creator" : null
  }
}
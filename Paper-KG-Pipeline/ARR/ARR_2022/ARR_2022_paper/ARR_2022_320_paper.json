{
  "name" : "ARR_2022_320_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) is a fundamental natural language processing task that extracts entities from texts. Flat NER has been well studied and is usually viewed as a sequence labeling problem (Lample et al., 2016). However, nested entities also widely exist in real-world applications due to their multi-granularity semantic meaning (Alex et al., 2007; Yuan et al., 2020), which cannot be solved by the sequence labeling framework since tokens have multiple labels (Finkel and Manning, 2009).\nVarious paradigms for nested NER have been proposed in recent years. A representative direction is the span-based approach that learns deep representation for every possible span and then classifies it to the corresponding type (Zheng et al., 2019; Xia et al., 2019; Wadden et al., 2019; Tan et al., 2020; Wang et al., 2020; Yu et al., 2020). By leveraging the large-scale pretrained language\nmodel, several works show that the simple model structure for span representation and classification can achieve satisfactory results (Luan et al., 2019; Zhong and Chen, 2021). However, we still believe that explicit modeling of some relevant features will further benefit the span representation and classification under the complex nested setting. Taking Figure 1 as an example, we claim that the following factors are critical for recognizing whether a span is an entity. (1) Tokens: It is obvious that tokens of the given span contribute to the recognition. (2) Boundaries: We emphasize boundaries (or boundary tokens) because they are special tokens with rich semantics. Works with simple structure may just produce the span representation based on the concatenation or biaffine transformation of boundary representation (Yu et al., 2020; Fu et al., 2021). Some other works take boundary detection as additional supervision for better representation learning (Zheng et al., 2019; Tan et al., 2020). More importantly, a unilateral boundary cannot determine the entity type since it can exist in multiple entities with different labels (e.g., “NF”, “B”, and “cells”) under the nested setting. (3) Labels: As mentioned above, tokens could belong to entities with different labels. Therefore, we propose that the model should learn label-aware span representation to take into consideration of the different token contributions at the label level.1 For exam-\n1Label is the perdition object that we cannot touch in representation learning. Here, leveraging label information only means we need label-aware representation learning.\nple, “NF” may contribute more to “protein” type when classifying the span “NF - chi B”, as well as “chi B” and “site” contribute more to “DNA” type when classifying the span “NF - chi B site”. (4) Related spans: Interactions among spans are important in nested entities (Luo and Zhao, 2020; Wang et al., 2020; Fu et al., 2021). The insider and outsider entities may hint at each other’s types. For example, entities inside “EBV-transformed B cells” have more possibilities to be cell-related entities. Interactions can also help the non-entity span like “transformed B cells” to validate its partialness by looking at outer entity “EBV - transformed B cells”.\nAlthough some of the factors may be explored in previous works, to the best of our knowledge, it is the first work to fuse all these heterogeneous factors into a unified network. As the traditional additive, multiplicative attention, or biaffine transformation cannot interact with such multiple heterogeneous factors simultaneously, we propose a novel triaffine mechanism as the tensor multiplication with three rank-1 tensors (vectors) and a rank-3 tensor, which makes it possible to jointly consider high-order interactions among multiple factors. Specifically, our method follows the pipeline of span representation learning and classification. At the stage of span representation learning, we apply the triaffine attention to aggregate the label-wise span representations by considering boundaries and labels as queries as well as inside tokens as keys and values. Then, a similar triaffine attention is applied to produce the label-wise cross-span representations by querying boundaries and labels with related spans. At the stage of span classification, we fuse the span representations and boundaries for label-wise classification with a triaffine score function. In practice, we add an auxiliary object function to classify spans without the cross-span interaction, which benefits learning robust span representation and can be used as a span filter to speed up both training and inference without performance degradation.\nWe conduct experiments on four nested NER datasets: ACE2004, ACE2005, GENIA, and KBP2017. Our model achieves 88.56, 88.83, 81.23, and 87.27 scores in terms of F1, respectively, outperforming state-of-the-art methods. Ablation studies show the effectiveness of each factor and the superiority of the triaffine mechanism. We will release our codes and models for further research.\nOur contributions are summarized as:\n• We propose that heterogeneous factors (i.e.,\ntokens, boundaries, labels, related spans) should be taken into consideration in the spanbased methods for nested NER.\n• We propose a span-based method with a novel triaffine mechanism including triaffine attention and scoring to fuse the above-mentioned heterogeneous factors for span representations and classification.\n• Experiments show that our proposed method performs better than existing span-based methods and achieves state-of-the-arts performances on four nested NER datasets."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Nested NER",
      "text" : "Nested NER approaches do not have a unified paradigm. Here we mainly focus on span-based methods since they are close to our work.\nThe span-based methods are one of the most mainstream ways for the nested NER. With the development of pre-training, it is easy to obtain the span representation by the concatenation of boundary representation (Luan et al., 2019; Zhong and Chen, 2021) or the aggregated representation of tokens (Zheng et al., 2019; Wadden et al., 2019), and then follow a linear layer (Xia et al., 2019) or biaffine transformation (Yu et al., 2020) for classification. Several works improve the span-based methods with additional features or supervision. Zheng et al. (2019); Tan et al. (2020) point out the importance of boundaries and therefore introduce the boundary detection task. Wang et al. (2020) propose Pyramid to allow interactions between spans from different layers. Fu et al. (2021) adopt TreeCRF to model interactions between nested spans. Compared with previous methods, our method can jointly fuse multiple heterogeneous factors with the proposed triaffine mechanism.\nOther methods for nested NER vary greatly. Earlier research on nested NER is rule-based (Zhang et al., 2004). Lu and Roth (2015); Katiyar and Cardie (2018); Wang and Lu (2018) leverage the hypergraph to represent all possible nested structures, which needs to be carefully designed to avoid spurious structures and structural ambiguities. Wang et al. (2018); Fisher and Vlachos (2019) predict the transition actions to construct nested entities. Lin et al. (2019) propose an anchor-based method to recognize entities. There are other works that recognize entities in a generative fashion (Yan\net al., 2021; Shen et al., 2021; Tan et al., 2021). Generally, it is not a unified framework for nested NER, and we model it with a span-based method since it is most straightforward."
    }, {
      "heading" : "2.2 Affine Transformations in NLP",
      "text" : "Dozat and Manning (2017) introduce the biaffine transformation in the dependency parsing task for arc classification. Later, it is widely used in many tasks that need to model bilateral representations (Li et al., 2019; Yu et al., 2020). The triaffine transformation is further introduced to extend biaffine transformation for high-order interaction in the field of dependency parsing (Wang et al., 2019; Zhang et al., 2020) and semantic role labeling (Li et al., 2020b). There are two key differences between our triaffine transformation and theirs. Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors. Secondly, they usually leverage triaffine transformation to obtain log potentials for CRFs, but we apply it for span representation and classification."
    }, {
      "heading" : "3 Approach",
      "text" : "Figure 2 shows an overview of our method. We will first introduce the triaffine transformations, which lie in the heart of our model to fuse heterogeneous factors. Then, we will introduce our model based on the proposed triaffine transformations."
    }, {
      "heading" : "3.1 Deep Triaffine Transformation",
      "text" : "We define the deep triaffine transformation with vectors u,v,w ∈ Rd and a tensor W ∈ Rd+1 × Rd × Rd+1 which outputs a scalar by applying distinct MLP transformations on input vectors and calculating tensor vector multiplications.\nu′ =\n[ MLP(u)\n1\n] ,v′ = [ MLP(v)\n1\n] (1)\nw′ =MLP(w) (2)\nTriAff(u,v,w,W) =W ×1 u′ ×2 w′ ×3 v′ (3)\nwhere ×n is the mode-n tensor vector multiplication. A constant 1 is concatenated with inputs to retain the biaffine transformation. The tensor W is initialized using N (0, σ2). In our approach, we use boundary representations as u and v. Inside tokens or span representations are used as w. We denote\nthe tensors in the triaffine attention as {Wr} and triaffine scoring as {Vr}, which decouples attention weights and scores for different labels."
    }, {
      "heading" : "3.2 Text Encoding",
      "text" : "We follow Shen et al. (2021) and Tan et al. (2021) to encode the text. For text X = [x1, x2, ..., xN ] with N tokens, we first generate the contextual embedding xci with the pre-trained language model,\nxc1,x c 2, ...,x c N = PLM(x1, x2, ..., xN ) (4)\nThen, we concatenate xci with word embedding x w i , part-of-speech embedding xpi and character embedding xchi , and feed the concatenated embedding xi into a BiLSTM (Hochreiter and Schmidhuber, 1997) to obtain the token representations {hi}."
    }, {
      "heading" : "3.3 Triaffine Attention for Span Representations",
      "text" : "To fuse heterogeneous factors for better span representation, we propose a triaffine attention mechanism shown in Figure 3a. To interact tokens with labels and boundaries, we learn the label-wise span representation hi,j,r with the triaffine attention αi,j,k,r for the span (i, j):\nsi,j,k,r = TriAff(hi,hj ,hk,Wr) (5) αi,j,k,r = exp(si,j,k,r)∑j\nk′=i exp(si,j,k′,r) (6)\nhi,j,r = j∑ k=i αi,j,k,rMLP(hk) (7)\nBoundary representations (hi, hj) and the labelwise parameters (Wr) can be viewed as attention queries, and tokens (hk) can be viewed as keys and values. Compared with the general attention framework (additive or multiplicative attention), our triaffine attention permits all high-order interactions between heterogeneous queries and keys."
    }, {
      "heading" : "3.4 Triaffine Attention for Cross-span Representations",
      "text" : "Motivated by the span-level interactions in the nested setting, we fuse related spans information into cross-span representations. We view the boundaries of the span and labels as attention queries, related spans (containing the span itself) as attention keys and values to obtain cross-span representations. Similar to the Equation 7, we obtain label-wise cross-span representations hci,j,r for\nthe span (i, j) based on triaffine attention βi,j,g,r.\nqi,j,g,r = TriAff(hi,hj ,hig ,jg ,r,Wr) (8) βi,j,g,r = exp(qi,j,g,r)∑ g′ exp(qi,j,g′,r) (9)\nhci,j,r = ∑ g βi,j,g,rMLP(hig ,jg ,r) (10)\nwhere {(ig, jg)} are the related spans. One can treat all enumerated spans as related spans, and we will introduce how we select them in Section 3.6."
    }, {
      "heading" : "3.5 Triaffine Scoring for Span Classification",
      "text" : "To classify the entity type of the span, we calculate label-wise scores based on cross-span representations. Since boundary information has been proved effective in previous works (Yu et al., 2020; Fu et al., 2021), we leverage the boundaries information and cross-span representations for span classification via triaffine scoring. Specifically, we estimate the log probabilities pci,j,r of the span (i, j) for label r using boundaries hi,hj and cross-span representations hci,j,r.\npci,j,r = TriAff(hi,hj ,h c i,j,r,Vr) (11)\nSince hci,j,r are composed by hig ,jg ,r, we can decompose Equation 11 into:\nti,j,g,r = TriAff(hi,hj ,hig ,jg ,r,Vr) (12) pci,j,r = ∑ g βi,j,g,rti,j,g,r (13)\nFigure 3b and 3c show the mechanism of triaffine scoring and the decomposition. We also apply the similar decomposition functions in the auxiliary span classification task, which applies the triaffine scoring on boundary representations and intermediate span representations hi,j,r to estimate log probabilities pi,j,r as intermediate predictions."
    }, {
      "heading" : "3.6 Training and Inference",
      "text" : "In practice, it is expensive and non-informative to consider interactions between all spans. Therefore, we propose an auxiliary task to classify spans with intermediate span representations. Then, we can rank all spans based on the maximum of log probabilities (except None) from the intermediate predictions pi,j = maxRr=1 pi,j,r, and retain top-m spans {(il, jl)}ml=1 as candidates. We calculate cross-span representations hcil,jl,r for re-\ntained spans by considering the full interactions among them, and estimate the classification logits pcil,jl,r. Thus, we have two groups of predictions in our model {pi,j,r}1≤i≤j≤N and {pcil,jl,r}1≤l≤m. {pi,j,r} are calculated for every possible span, and {pcil,jl,r} are calculated only on top-m spans.\nIn the training phase, we jointly minimize two groups of cross-entropy losses:\nLaux =− 2\nN(N + 1) ∑ i,j log exp(pi,j,rij )∑ r exp(pi,j,r)\n(14)\nLmain =− 1\nm ∑ 1≤l≤m log exp(pcil,jl,ril,jl )∑ r exp(p c il,jl,r ) (15)\nL =µauxLaux + Lmain (16)\nwhere rij is the label of span (i, j). In both the training and inference phase, {pi,j,r} are used to select spans with high possibilities based on the supervision from Laux. We inference the labels of selected spans using {pcil,jl,r} by assigning label r̃il,jl = argr max p c il,jl,r\n, and we assign None class for others."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conduct our experiments on the ACE20042, ACE20053 (Doddington et al., 2004), GENIA (Kim et al., 2003) and KBP20174 (Ji et al., 2017) datasets. To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 datasets and use the split from Lin et al. (2019) for GENIA and KBP2017 datasets. The statistics of all datasets are listed in Table 1. Following previous work, we measure the results using span-level precision, recall, and F1 scores."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We use BERT-large-cased (Devlin et al., 2019) and albert-xxlarge-v2 (Lan et al., 2020) as the contextual embedding, fastText (Bojanowski et al., 2017) as the word embedding in ACE2004, ACE2005 and KBP2017 dataset. We use BioBERT-v1.1 (Lee et al., 2020) and\n2https://catalog.ldc.upenn.edu/ LDC2005T09\n3https://catalog.ldc.upenn.edu/ LDC2006T06\n4https://catalog.ldc.upenn.edu/ LDC2019T12\nBioWordVec (Zhang et al., 2019) as the contextual and word embedding in the GENIA dataset respectively. We truncate the input texts with context at length 192. The part-of-speech embeddings are initialized with dimension 50. The char embeddings are generated by a one-layer BiLSTM with hidden size 50. The two-layers BiLSTM with a hidden size of 1,024 is used for the token representations. For triaffine transformations, we use d = 256 for the ACE2004, ACE2005, and KBP2017 dataset, and d = 320 for the GENIA dataset, respectively. We set µaux to 1.0, and select m = 30 in both training and inference. We use AdamW (Loshchilov and Hutter, 2019) to optimize our models with a linear learning rate decay. Detailed training parameters are presented in Appendix A."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "DYGIE (Luan et al., 2019) uses multi-task learning to extract entities, relations, and coreferences. MGNER (Xia et al., 2019) uses a detector to find span candidates and a classifier for categorization. BENSC (Tan et al., 2020) trains the boundary detection and span classification tasks jointly. TreeCRF (Fu et al., 2021) views entities as nodes in a constituency tree and decodes them with a Masked Inside algorithm. Biaffine (Yu et al., 2020) classifies spans by a biaffine function between boundary representations. Pyramid (Wang et al., 2020) designs pyramid layer and inverse pyramid layer to decode nested entities.\nWe also report the results of models with other paradigms, including hypergraph-based methods (Wang and Lu, 2018), transition-based methods (Fisher and Vlachos, 2019), generative methods (Yan et al., 2021; Tan et al., 2021; Shen et al., 2021), and so on. We do not compare to BERT-MRC (Li et al., 2020a) since they use additional resources as queries. DYGIE++ (Wadden et al., 2019) and PURE (Zhong and Chen, 2021) use different splits of the ACE datasets which are not comparable."
    }, {
      "heading" : "4.4 Results",
      "text" : "We compare our method with baseline methods in Table 2 for the ACE2004, ACE2005, and GENIA datasets and Table 3 for the KBP2017 dataset, respectively. With BERT as the encoder, our model achieves 87.40, 86.82, 81.23, and 85.05 scores in terms of F1, outperforming all other span-based methods such as BENSC, Pyramid, TreeCRF, and Biaffine (+0.70 on ACE2004, +1.42 on ACE2005, +0.73 on GENIA). Compared with methods in other\nparadigms, our model also achieves the state-of-theart results on the GENIA (+0.69 vs. Locate and Label) and KBP2017 dataset (+1.00 vs. Locate and Label) and shows comparable performances on ACE2004 (-0.01 vs. Locate and Label) and ACE2005 (-0.23 vs. Sequence to Set). With a stronger encoder ALBERT, our model achieves 88.56, 88.83, and 87.27 scores in terms of F1 on ACE2004, ACE2005, and KBP2017 respectively, which exceeds all existing baselines including the Pyramid model with ALBERT (+0.82 on ACE2004,\n+2.49 on ACE2005) and the previous state-of-theart method on KBP2017 dataset (+3.22 vs. Locate and Label)."
    }, {
      "heading" : "4.5 Ablation Study",
      "text" : "Considering we leverage multiple factors in multiple parts of the model, we design the following ablation settings to validate the effectiveness of each factor and the proposed triaffine mechanism. (a) To show the effectiveness of triaffine mechanism, we use a baseline biaffine model with the combination of boundary representations:\npi,j,r =\n[ hi\n1\n]T Vr [ hj\n1\n] (17)\n(b) To show the effectiveness of boundaries in scoring, we remove boundaries factor from scoring:\npi,j,r = Vrhi,j,r + br (18)\n(c) To show the effectiveness of labels in representation, we remove label factor in attention:\nsi,j,k,r = TriAff(hi,hj ,hk,W) (19)\n(d) To show the effectiveness of boundaries in representation, we remove boundaries factor in attention:\nsi,j,k,r = sk,r = qr · hk (20)\n(e) To show the effectiveness of the triaffine mechanism in representations, we replace triaffine attention with linear attention:\nsi,j,k,r = Wr(hi ∥ hj ∥ hk) + cr (21)\n(f) To show the effectiveness of triaffine scoring, we replace triaffine scoring to linear scoring:\npi,j,r = Vr(hi ∥ hj ∥ hi,j,r) + br (22)\n(g) To show the effectiveness of cross-span interactions, we use our partial model with intermediate predictions (model (a)-(g) use pi,j,r). (h) Our full model (i.e, use pcil,jl,r as predictions).\nTable 4 shows the results of ablation studies on ACE2004 and GENIA datasets. We use BERT-large-cased as the backbone encoder on ACE2004 and BioBERT-v1.1 on GENIA, respectively. By comparing (a) with (g), we observe significant performances drop (-0.87 on ACE2004, - 1.87 on GENIA), which indicates that our proposed triaffine mechanism with multiple heterogeneous factors performs better than the biaffine baseline. Comparing (b) with (g), we find that the boundary information contributes to span classification. Comparing (c) and (d) with (g) supports that either label or boundary in the triaffine attention improves the performance. The setting (g) performs better than (e) and (f), which shows the superiority of the triaffine transformation over the linear function. We observe that (h) performs better than (g) (+0.28 on ACE2004, +0.39 on GENIA), proving the strength of triaffine attention with interactions among related spans. The above studies support\nthat our proposed triaffine mechanism with associated heterogeneous factors is effective for span representation and classification."
    }, {
      "heading" : "4.6 Discussion",
      "text" : "We compare the F1 scores of GENIA between triaffine model (g) and biaffine model (a) grouped by entity lengths in Figure 4. In all columns, the F1 score of our method is better than the baseline. Furthermore, the right columns show that the F1 score of the baseline gradually decreases with the incremental entity lengths. However, our method based on the triaffine mechanism with heterogeneous factors takes advantage of the interaction from boundaries and related spans, which keeps consistent results and outperforms the baseline.\nThe results grouped by flat or nested entities are shown in Table 6. Our method has consistent improvements than the baseline, especially for the nested setting. Based on the above observations, our method is good at solving long entities that are more likely to be nested, which supports our model is built upon the characteristics of nested NER.\nAt the stage of cross-span interactions, we only select top-m spans in practice. In Figure 5, we analyze the number m in two aspects. Firstly, we check\nthe recall of entity spans. We observe that taking top-30 spans achieves a recall of 99.89, which means it covers almost all entities. As the maximum number of entities is 25, we believe it is enough to select top-30 spans. Secondly, we check the model performance. With top-30 spans, the model achieves 81.23 scores in terms of F1 and there is no obvious performance improvement with more candidates. Based on two above observations, we choose m = 30, which can well balance the performance and efficiency.\nFinally, we test the efficiency of the decomposition. Compared with the naive triaffine scoring that takes 638.1ms (509.4ms in GPU + 128.7ms in CPU), the decomposed triaffine scoring takes 432.7ms (330.5ms in GPU + 102.2ms in CPU) for 10 iterations, which leads to approximately 32% speedup (details are shown in Appendix B)."
    }, {
      "heading" : "4.7 Case Study",
      "text" : "To analyze the effect of fusing information from related spans with the cross-span interaction, we show two examples from ACE2004 and GENIA datasets in Table 5. In the first example, the model\nfirst predicts “the trading population” as “GPE”, however, it revises to “PER” correctly by considering span interactions with the outer span “the rest of the trading population”. In the second example, it first predicts “MnlI-AluI” as “protein”. By interacting with surrounding entities “MnlI-AluI fragment”, the model corrects its label to None."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a span-based method for nested NER. Heterogeneous factors including tokens, boundaries, labels, and related spans are introduced to improve span classification with a novel triaffine mechanism. Experiments show our method outperforms all span-based methods and achieves state-of-the-art performance on four nested NER datasets. Ablation studies show the introduced heterogeneous factors and triaffine mechanism are helpful for nested setting. Despite that large-scale pretrained language models have shown consistent improvement over many NLP tasks, we argue that the well-designed features and model structures are still useful for complex tasks like nested NER. Furthermore, although we only verify our triaffine mechanism in nested NER, we believe it can also be useful in tasks requiring high order interactions like parsing and semantic role labeling."
    }, {
      "heading" : "A Reproducibility Checklist",
      "text" : "We specific seeds of torch, torch.cuda, numpy, and random in Python to ensure reproducibility. We use a grid search to find the best hyperparameters depending on development set performances. We search contextual embedding learning rate among {1e-5,3e-5}. If the contextual embedding learning rate is 1e-5, we use static embedding learning rate and task learning rate as 1e-4 and 1e-5. If the contextual embedding learning rate is 3e-5, we use static embedding learning rate and task learning rate as 5e-4 and 3e-5. We search batch size among {8,48,72}. We search MLP dropout ratio among {0.1,0.2}. The final hyperparameters we used for four datasets are listed in Table 7 and Table 8."
    }, {
      "heading" : "B The Decomposition of Triaffine Scoring",
      "text" : "We introduce the decomposition of triaffine scoring in calculating pi,j,r and pci,j,r.\nThe naive calculation procedure of pi,j,r is:\nsi,j,k,r = TriAff(hi,hj ,hk,Wr) (23) αi,j,k,r = exp(si,j,k,r)∑j\nk′=i exp(si,j,k′,r) (24)\nhi,j,r = j∑ k=i αi,j,k,rMLP(hk) (25) pi,j,r = TriAff(hi,hj ,hi,j,r,Vr) (26)\nFor our proposed decomposition of pi,j,r, we first calculate αi,j,k,r as equations 23 and 24. And we calculate:\noi,j,k,r = TriAff(hi,hj ,hk,Vr) (27)\npi,j,r = j∑ k=i αi,j,k,roi,j,k,r (28)\nThe main difference between naive calculation and decomposition calculation is between Equation 26 and Equation 27.\nWe suppose our batch size as B, sequence count as N , output dimensions of MLP layers as d, the count of spans for calculating cross span representations as m, and label count as R (including None class). The shapes of tensors [hi], [hj ], [hk] are B × N × d. The shape of tensor [hi,j,r] is B ×N ×N ×R× d.\nWe benchmark the performances of Equation 26 and Equation 27 in PyTorch for 10 iterations. We use the same hyper-parameters and devices as our main experiments. We levearge opt_einsum5 to calculate triaffine transformations in both equations.\nTable 9 shows the time usage comparison between Equation 26 and Equation 27. Equation 26\n5https://github.com/dgasmith/opt_ einsum\nuses 309.7ms (300.5ms in GPU + 9.2ms in CPU) and Equation 27 uses 150.1ms (145.6ms in GPU + 4.4ms in CPU). The larger tensor size and higher rank of [hi,j,r] results in slower calculations of aten::bmm, aten::copy_ and aten::permute in Equation 26. The time usage differences are clearly dominated by the function aten::copy_, which is optimized by our decomposition.\nWe also compare the time usage between the naive triaffine scoring and the decomposed triaffine scoring in Table 9. The naive triaffine scoring takes 638.1ms (509.4ms in GPU + 128.7ms in CPU), and the decomposed triaffine scoring takes 432.7ms (330.5ms in GPU + 102.2ms in CPU) for 10 iterations, which leads to approximately 32% speedup. The GPU time usages are reasonable since they both need to calculate two triaffine transformations. The CPU time usages increase for both naive and decomposition triaffine scoring. Additional CPU time usages come from function aten::einsum, aten::permute, and aten::reshape, and the naive calculation increases more due to slower aten::einsum. Overall, the decomposition triaffine scoring uses less time on both GPU and CPU than the naive triaffine scoring.\nFuthermore, we also test the time usage of pci,j,r using two calculation procedures. We find using the decomposition triaffine scoring still has about 6% speed up (naive:125.8ms in GPU + 15.0ms in\nCPU vs. decomposition:115.5ms in GPU + 16.8ms in CPU) regardless the relatively small size of hci,j,r (The shape of tensor [hci,j,r] is B ×m×R× d)."
    } ],
    "references" : [ {
      "title" : "Recognising nested named entities in biomedical text",
      "author" : [ "Beatrice Alex", "Barry Haddow", "Claire Grover." ],
      "venue" : "Biological, translational, and clinical language processing, pages 65–72.",
      "citeRegEx" : "Alex et al\\.,? 2007",
      "shortCiteRegEx" : "Alex et al\\.",
      "year" : 2007
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The automatic content extraction (ace) program-tasks, data, and evaluation",
      "author" : [ "George R Doddington", "Alexis Mitchell", "Mark A Przybocki", "Lance A Ramshaw", "Stephanie M Strassel", "Ralph M Weischedel." ],
      "venue" : "Lrec, volume 2, pages 837–840. Lisbon.",
      "citeRegEx" : "Doddington et al\\.,? 2004",
      "shortCiteRegEx" : "Doddington et al\\.",
      "year" : 2004
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Nested named entity recognition",
      "author" : [ "Jenny Rose Finkel", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2009 conference on empirical methods in natural language processing, pages 141–150.",
      "citeRegEx" : "Finkel and Manning.,? 2009",
      "shortCiteRegEx" : "Finkel and Manning.",
      "year" : 2009
    }, {
      "title" : "Merge and label: A novel neural network architecture for nested NER",
      "author" : [ "Joseph Fisher", "Andreas Vlachos." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5840–5850, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Fisher and Vlachos.,? 2019",
      "shortCiteRegEx" : "Fisher and Vlachos.",
      "year" : 2019
    }, {
      "title" : "Nested named entity recognition with partially-observed treecrfs",
      "author" : [ "Yao Fu", "Chuanqi Tan", "Mosha Chen", "Songfang Huang", "Fei Huang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12839–12847.",
      "citeRegEx" : "Fu et al\\.,? 2021",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2021
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Overview of tac-kbp2017 13 languages entity discovery and linking",
      "author" : [ "Heng Ji", "Xiaoman Pan", "Boliang Zhang", "Joel Nothman", "James Mayfield", "Paul McNamee", "Cash Costello." ],
      "venue" : "Theory and Applications of Categories.",
      "citeRegEx" : "Ji et al\\.,? 2017",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2017
    }, {
      "title" : "Nested named entity recognition revisited",
      "author" : [ "Arzoo Katiyar", "Claire Cardie." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of",
      "citeRegEx" : "Katiyar and Cardie.,? 2018",
      "shortCiteRegEx" : "Katiyar and Cardie.",
      "year" : 2018
    }, {
      "title" : "Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl_1):i180–i182",
      "author" : [ "J-D Kim", "Tomoko Ohta", "Yuka Tateisi", "Jun’ichi Tsujii" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2003
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "A unified MRC framework for named entity recognition",
      "author" : [ "Xiaoya Li", "Jingrong Feng", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Jiwei Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849–5859, On-",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Dependency or span, end-to-end uniform semantic role labeling",
      "author" : [ "Zuchao Li", "Shexia He", "Hai Zhao", "Yiqing Zhang", "Zhuosheng Zhang", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6730–6737.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "High-order semantic role labeling",
      "author" : [ "Zuchao Li", "Hai Zhao", "Rui Wang", "Kevin Parnow." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1134–1151, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-to-nuggets: Nested entity mention detection via anchor-region networks",
      "author" : [ "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5182–5192, Florence,",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Joint mention extraction and classification with mention hypergraphs",
      "author" : [ "Wei Lu", "Dan Roth." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical 9",
      "citeRegEx" : "Lu and Roth.,? 2015",
      "shortCiteRegEx" : "Lu and Roth.",
      "year" : 2015
    }, {
      "title" : "A general framework for information extraction using dynamic span graphs",
      "author" : [ "Yi Luan", "Dave Wadden", "Luheng He", "Amy Shah", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Luan et al\\.,? 2019",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2019
    }, {
      "title" : "Bipartite flat-graph network for nested named entity recognition",
      "author" : [ "Ying Luo", "Hai Zhao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6408– 6418, Online. Association for Computational Lin-",
      "citeRegEx" : "Luo and Zhao.,? 2020",
      "shortCiteRegEx" : "Luo and Zhao.",
      "year" : 2020
    }, {
      "title" : "Locate and label: A two-stage identifier for nested named entity recognition",
      "author" : [ "Yongliang Shen", "Xinyin Ma", "Zeqi Tan", "Shuai Zhang", "Wen Wang", "Weiming Lu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Nested named entity recognition via second-best sequence learning and decoding",
      "author" : [ "Takashi Shibuya", "Eduard Hovy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:605–620.",
      "citeRegEx" : "Shibuya and Hovy.,? 2020",
      "shortCiteRegEx" : "Shibuya and Hovy.",
      "year" : 2020
    }, {
      "title" : "Neural architectures for nested NER through linearization",
      "author" : [ "Jana Straková", "Milan Straka", "Jan Hajic." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5326–5331, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Straková et al\\.,? 2019",
      "shortCiteRegEx" : "Straková et al\\.",
      "year" : 2019
    }, {
      "title" : "Boundary enhanced neural span classification for nested named entity recognition",
      "author" : [ "Chuanqi Tan", "Wei Qiu", "Mosha Chen", "Rui Wang", "Fei Huang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9016–9023.",
      "citeRegEx" : "Tan et al\\.,? 2020",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2020
    }, {
      "title" : "A sequence-to-set network for nested named entity recognition",
      "author" : [ "Zeqi Tan", "Yongliang Shen", "Shuai Zhang", "Weiming Lu", "Yueting Zhuang." ],
      "venue" : "Proceedings of the 30th International Joint Conference on Artificial Intelligence, IJCAI-21.",
      "citeRegEx" : "Tan et al\\.,? 2021",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2021
    }, {
      "title" : "Entity, relation, and event extraction with contextualized span representations",
      "author" : [ "David Wadden", "Ulme Wennberg", "Yi Luan", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Wadden et al\\.,? 2019",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural segmental hypergraphs for overlapping mention recognition",
      "author" : [ "Bailin Wang", "Wei Lu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204– 214, Brussels, Belgium. Association for Computa-",
      "citeRegEx" : "Wang and Lu.,? 2018",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2018
    }, {
      "title" : "A neural transition-based model for nested mention recognition",
      "author" : [ "Bailin Wang", "Wei Lu", "Yu Wang", "Hongxia Jin." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1011–1017, Brussels, Belgium. Associa-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Pyramid: A layered model for nested named entity recognition",
      "author" : [ "Jue Wang", "Lidan Shou", "Ke Chen", "Gang Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5918–5928.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Second-order semantic dependency parsing with endto-end neural networks",
      "author" : [ "Xinyu Wang", "Jingxian Huang", "Kewei Tu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4609–4618, Florence, Italy. Asso-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-grained named entity recognition",
      "author" : [ "Congying Xia", "Chenwei Zhang", "Tao Yang", "Yaliang Li", "Nan Du", "Xian Wu", "Wei Fan", "Fenglong Ma", "Philip Yu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1430–",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "A unified generative framework for various NER subtasks",
      "author" : [ "Hang Yan", "Tao Gui", "Junqi Dai", "Qipeng Guo", "Zheng Zhang", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "Named entity recognition as dependency parsing",
      "author" : [ "Juntao Yu", "Bernd Bohnet", "Massimo Poesio." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470– 6476, Online. Association for Computational Lin-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised multi-granular chinese word segmentation and term discovery via graph partition",
      "author" : [ "Zheng Yuan", "Yuanhao Liu", "Qiuyang Yin", "Boyao Li", "Xiaobin Feng", "Guoming Zhang", "Sheng Yu." ],
      "venue" : "Journal of Biomedical Informatics, 110:103542.",
      "citeRegEx" : "Yuan et al\\.,? 2020",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing hmm-based biomedical named entity recognition by studying special phenomena",
      "author" : [ "Jie Zhang", "Dan Shen", "Guodong Zhou", "Jian Su", "Chew-Lim Tan." ],
      "venue" : "Journal of biomedical informatics, 37(6):411–422.",
      "citeRegEx" : "Zhang et al\\.,? 2004",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2004
    }, {
      "title" : "Biowordvec, improving biomedical word embeddings with subword information and mesh",
      "author" : [ "Yijia Zhang", "Qingyu Chen", "Zhihao Yang", "Hongfei Lin", "Zhiyong Lu." ],
      "venue" : "Scientific data, 6(1):1–9.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient second-order TreeCRF for neural dependency parsing",
      "author" : [ "Yu Zhang", "Zhenghua Li", "Min Zhang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 10",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "A boundary-aware neural model for nested named entity recognition",
      "author" : [ "Changmeng Zheng", "Yi Cai", "Jingyun Xu", "HF Leung", "Guandong Xu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    }, {
      "title" : "A frustratingly easy approach for entity and relation extraction",
      "author" : [ "Zexuan Zhong", "Danqi Chen." ],
      "venue" : "North American Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Zhong and Chen.,? 2021",
      "shortCiteRegEx" : "Zhong and Chen.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Flat NER has been well studied and is usually viewed as a sequence labeling problem (Lample et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "However, nested entities also widely exist in real-world applications due to their multi-granularity semantic meaning (Alex et al., 2007; Yuan et al., 2020), which cannot be solved by the sequence labeling framework since tokens have multiple labels (Finkel and Manning, 2009).",
      "startOffset" : 118,
      "endOffset" : 156
    }, {
      "referenceID" : 36,
      "context" : "However, nested entities also widely exist in real-world applications due to their multi-granularity semantic meaning (Alex et al., 2007; Yuan et al., 2020), which cannot be solved by the sequence labeling framework since tokens have multiple labels (Finkel and Manning, 2009).",
      "startOffset" : 118,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : ", 2020), which cannot be solved by the sequence labeling framework since tokens have multiple labels (Finkel and Manning, 2009).",
      "startOffset" : 101,
      "endOffset" : 127
    }, {
      "referenceID" : 40,
      "context" : "A representative direction is the span-based approach that learns deep representation for every possible span and then classifies it to the corresponding type (Zheng et al., 2019; Xia et al., 2019; Wadden et al., 2019; Tan et al., 2020; Wang et al., 2020; Yu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 272
    }, {
      "referenceID" : 33,
      "context" : "A representative direction is the span-based approach that learns deep representation for every possible span and then classifies it to the corresponding type (Zheng et al., 2019; Xia et al., 2019; Wadden et al., 2019; Tan et al., 2020; Wang et al., 2020; Yu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 272
    }, {
      "referenceID" : 28,
      "context" : "A representative direction is the span-based approach that learns deep representation for every possible span and then classifies it to the corresponding type (Zheng et al., 2019; Xia et al., 2019; Wadden et al., 2019; Tan et al., 2020; Wang et al., 2020; Yu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 272
    }, {
      "referenceID" : 26,
      "context" : "A representative direction is the span-based approach that learns deep representation for every possible span and then classifies it to the corresponding type (Zheng et al., 2019; Xia et al., 2019; Wadden et al., 2019; Tan et al., 2020; Wang et al., 2020; Yu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 272
    }, {
      "referenceID" : 31,
      "context" : "A representative direction is the span-based approach that learns deep representation for every possible span and then classifies it to the corresponding type (Zheng et al., 2019; Xia et al., 2019; Wadden et al., 2019; Tan et al., 2020; Wang et al., 2020; Yu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 272
    }, {
      "referenceID" : 35,
      "context" : "A representative direction is the span-based approach that learns deep representation for every possible span and then classifies it to the corresponding type (Zheng et al., 2019; Xia et al., 2019; Wadden et al., 2019; Tan et al., 2020; Wang et al., 2020; Yu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 272
    }, {
      "referenceID" : 21,
      "context" : "model, several works show that the simple model structure for span representation and classification can achieve satisfactory results (Luan et al., 2019; Zhong and Chen, 2021).",
      "startOffset" : 134,
      "endOffset" : 175
    }, {
      "referenceID" : 41,
      "context" : "model, several works show that the simple model structure for span representation and classification can achieve satisfactory results (Luan et al., 2019; Zhong and Chen, 2021).",
      "startOffset" : 134,
      "endOffset" : 175
    }, {
      "referenceID" : 35,
      "context" : "Works with simple structure may just produce the span representation based on the concatenation or biaffine transformation of boundary representation (Yu et al., 2020; Fu et al., 2021).",
      "startOffset" : 150,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "Works with simple structure may just produce the span representation based on the concatenation or biaffine transformation of boundary representation (Yu et al., 2020; Fu et al., 2021).",
      "startOffset" : 150,
      "endOffset" : 184
    }, {
      "referenceID" : 40,
      "context" : "Some other works take boundary detection as additional supervision for better representation learning (Zheng et al., 2019; Tan et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : "Some other works take boundary detection as additional supervision for better representation learning (Zheng et al., 2019; Tan et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : "(4) Related spans: Interactions among spans are important in nested entities (Luo and Zhao, 2020; Wang et al., 2020; Fu et al., 2021).",
      "startOffset" : 77,
      "endOffset" : 133
    }, {
      "referenceID" : 31,
      "context" : "(4) Related spans: Interactions among spans are important in nested entities (Luo and Zhao, 2020; Wang et al., 2020; Fu et al., 2021).",
      "startOffset" : 77,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "(4) Related spans: Interactions among spans are important in nested entities (Luo and Zhao, 2020; Wang et al., 2020; Fu et al., 2021).",
      "startOffset" : 77,
      "endOffset" : 133
    }, {
      "referenceID" : 21,
      "context" : "With the development of pre-training, it is easy to obtain the span representation by the concatenation of boundary representation (Luan et al., 2019; Zhong and Chen, 2021) or the aggregated representation of tokens (Zheng et al.",
      "startOffset" : 131,
      "endOffset" : 172
    }, {
      "referenceID" : 41,
      "context" : "With the development of pre-training, it is easy to obtain the span representation by the concatenation of boundary representation (Luan et al., 2019; Zhong and Chen, 2021) or the aggregated representation of tokens (Zheng et al.",
      "startOffset" : 131,
      "endOffset" : 172
    }, {
      "referenceID" : 40,
      "context" : ", 2019; Zhong and Chen, 2021) or the aggregated representation of tokens (Zheng et al., 2019; Wadden et al., 2019), and then follow a linear layer (Xia et al.",
      "startOffset" : 73,
      "endOffset" : 114
    }, {
      "referenceID" : 28,
      "context" : ", 2019; Zhong and Chen, 2021) or the aggregated representation of tokens (Zheng et al., 2019; Wadden et al., 2019), and then follow a linear layer (Xia et al.",
      "startOffset" : 73,
      "endOffset" : 114
    }, {
      "referenceID" : 33,
      "context" : ", 2019), and then follow a linear layer (Xia et al., 2019) or biaffine transformation (Yu et al.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 35,
      "context" : ", 2019) or biaffine transformation (Yu et al., 2020) for classification.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 37,
      "context" : "Earlier research on nested NER is rule-based (Zhang et al., 2004).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Later, it is widely used in many tasks that need to model bilateral representations (Li et al., 2019; Yu et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 118
    }, {
      "referenceID" : 35,
      "context" : "Later, it is widely used in many tasks that need to model bilateral representations (Li et al., 2019; Yu et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 118
    }, {
      "referenceID" : 32,
      "context" : "The triaffine transformation is further introduced to extend biaffine transformation for high-order interaction in the field of dependency parsing (Wang et al., 2019; Zhang et al., 2020) and semantic role labeling (Li et al.",
      "startOffset" : 147,
      "endOffset" : 186
    }, {
      "referenceID" : 39,
      "context" : "The triaffine transformation is further introduced to extend biaffine transformation for high-order interaction in the field of dependency parsing (Wang et al., 2019; Zhang et al., 2020) and semantic role labeling (Li et al.",
      "startOffset" : 147,
      "endOffset" : 186
    }, {
      "referenceID" : 17,
      "context" : ", 2020) and semantic role labeling (Li et al., 2020b).",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "Then, we concatenate xi with word embedding x w i , part-of-speech embedding xpi and character embedding xch i , and feed the concatenated embedding xi into a BiLSTM (Hochreiter and Schmidhuber, 1997) to obtain the token representations {hi}.",
      "startOffset" : 166,
      "endOffset" : 200
    }, {
      "referenceID" : 35,
      "context" : "Since boundary information has been proved effective in previous works (Yu et al., 2020; Fu et al., 2021), we leverage the boundaries information and cross-span representations for span classification via triaffine scoring.",
      "startOffset" : 71,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "Since boundary information has been proved effective in previous works (Yu et al., 2020; Fu et al., 2021), we leverage the boundaries information and cross-span representations for span classification via triaffine scoring.",
      "startOffset" : 71,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "We conduct our experiments on the ACE20042, ACE20053 (Doddington et al., 2004), GENIA (Kim et al.",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : ", 2004), GENIA (Kim et al., 2003) and KBP20174 (Ji et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "We use BERT-large-cased (Devlin et al., 2019) and albert-xxlarge-v2 (Lan et al.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and albert-xxlarge-v2 (Lan et al., 2020) as the contextual embedding, fastText (Bojanowski et al.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : ", 2020) as the contextual embedding, fastText (Bojanowski et al., 2017) as the word embedding in ACE2004, ACE2005 and KBP2017 dataset.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 38,
      "context" : "edu/ LDC2019T12 BioWordVec (Zhang et al., 2019) as the contextual and word embedding in the GENIA dataset respectively.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "We use AdamW (Loshchilov and Hutter, 2019) to optimize our models with a linear learning rate decay.",
      "startOffset" : 13,
      "endOffset" : 42
    }, {
      "referenceID" : 21,
      "context" : "DYGIE (Luan et al., 2019) uses multi-task learning to extract entities, relations, and coreferences.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 33,
      "context" : "MGNER (Xia et al., 2019) uses a detector to find span candidates and a classifier for categorization.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 26,
      "context" : "BENSC (Tan et al., 2020) trains the boundary detection and span classification tasks jointly.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "TreeCRF (Fu et al., 2021) views entities as nodes in a constituency tree and decodes them with a Masked Inside algorithm.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 35,
      "context" : "Biaffine (Yu et al., 2020) classifies spans by a biaffine function between boundary representations.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 31,
      "context" : "Pyramid (Wang et al., 2020) designs pyramid layer and inverse pyramid layer to decode nested entities.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 29,
      "context" : "We also report the results of models with other paradigms, including hypergraph-based methods (Wang and Lu, 2018), transition-based methods (Fisher and Vlachos, 2019), generative methods (Yan et al.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "We also report the results of models with other paradigms, including hypergraph-based methods (Wang and Lu, 2018), transition-based methods (Fisher and Vlachos, 2019), generative methods (Yan et al.",
      "startOffset" : 140,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : "We also report the results of models with other paradigms, including hypergraph-based methods (Wang and Lu, 2018), transition-based methods (Fisher and Vlachos, 2019), generative methods (Yan et al., 2021; Tan et al., 2021; Shen et al., 2021), and so on.",
      "startOffset" : 187,
      "endOffset" : 242
    }, {
      "referenceID" : 27,
      "context" : "We also report the results of models with other paradigms, including hypergraph-based methods (Wang and Lu, 2018), transition-based methods (Fisher and Vlachos, 2019), generative methods (Yan et al., 2021; Tan et al., 2021; Shen et al., 2021), and so on.",
      "startOffset" : 187,
      "endOffset" : 242
    }, {
      "referenceID" : 23,
      "context" : "We also report the results of models with other paradigms, including hypergraph-based methods (Wang and Lu, 2018), transition-based methods (Fisher and Vlachos, 2019), generative methods (Yan et al., 2021; Tan et al., 2021; Shen et al., 2021), and so on.",
      "startOffset" : 187,
      "endOffset" : 242
    }, {
      "referenceID" : 15,
      "context" : "We do not compare to BERT-MRC (Li et al., 2020a) since they use additional resources as queries.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "DYGIE++ (Wadden et al., 2019) and PURE (Zhong and Chen, 2021) use different splits of the ACE datasets which are not comparable.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 41,
      "context" : ", 2019) and PURE (Zhong and Chen, 2021) use different splits of the ACE datasets which are not comparable.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : "Span-based Methods DYGIE (Luan et al., 2019) + LSTM - - 84.",
      "startOffset" : 25,
      "endOffset" : 44
    } ],
    "year" : 0,
    "abstractText" : "Nested entities are observed in many domains due to their compositionality, which cannot be easily recognized by the widely-used sequence labeling framework. A natural solution is to treat the task as a span classification problem. To learn better span representation and increase classification performance, it is crucial to effectively integrate heterogeneous factors including inside tokens, boundaries, labels, and related spans which could be contributing to nested entities recognition. To fuse these heterogeneous factors, we propose a novel triaffine mechanism including triaffine attention and scoring. Triaffine attention uses boundaries and labels as queries, and uses inside tokens and related spans as keys and values for span representations. Triaffine scoring interacts with boundaries and span representations for classification. Experiments show that our proposed method achieves the state-of-the-art F1 scores on four nested NER datasets: ACE2004, ACE2005, GENIA, and KBP2017.",
    "creator" : null
  }
}
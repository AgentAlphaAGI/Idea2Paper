{
  "name" : "ARR_2022_258_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Understanding Iterative Revision from Human-Written Text",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions."
    }, {
      "heading" : "1 Introduction",
      "text" : "Writing is a complex and effortful cognitive task, where writers balance and orchestrate three distinct cognitive processes: planning, translation, and revising (Flower and Hayes, 1980). These processes can be hierarchical and recursive and can occur at any moment during writing. This work focuses on text revision as an essential part of writing (Scardamalia, 1986). Revising text is a strategic, and adaptive process. It enables writers to deliberate over and organize their thoughts, find a better line of argument, learn afresh, and discover what was not known before (Sommers, 1980). Specifically,\n1Code and dataset to be publicly released upon acceptance.\nEDIT-INTENTION in ITERATER.\ntext revision involves identifying discrepancies between intended and instantiated text, deciding what edits to make, and how to make those desired edits (Faigley and Witte, 1981; Fitzgerald, 1987; Bridwell, 1980).\nText revision is an iterative process. Human writers are unable to simultaneously comprehend multiple demands and constraints of the task when producing well-written texts (Flower, 1980; Collins and Gentner, 1980; Vaughan and McDonald, 1986) – for instance, expressing ideas, covering the content, following linguistic norms and discourse conventions of written prose, etc. Thus, they turn towards making successive iterations of revisions to reduce the number of considerations at each time.\nPrevious works on iterative text revision have three major limitations: (1) simplifying the task to a single-pass \"original-to-final\" text paraphrasing, (2) focusing largely on sentence-level edit-\ning (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021); (3) developing editing taxonomies within singular domains (e.g. Wikipedia articles, academic writings) (Yang et al., 2017; Zhang et al., 2017; Anthonio et al., 2020). These limitations make their proposed text editing taxonomies, datasets, and models, and lose their generalizability and practicality.\nWe present ITERATER— an annotated dataset for ITERAtive TExt Revision that consists of 31,631 iterative document revisions with sentence-level and paragraph-level edits across multiple domains, including Wikipedia2, ArXiv3 and Wikinews.4 Table 1 shows a sample ArXiv document in ITERATER, that underwent iterative revisions. Our dataset includes 4K human-annotated and 196K automatically-annotated edit intentions based on a sound taxonomy we developed, and is generally applicable across multiple domains and granularities (See Table 2). Our contributions are as follows: • formulate the iterative text revision task in a more\ncomprehensive way, capturing greater real-world challenges such as successive revisions, multigranularity edits, and domain shifts. • collect and release a large, multi-domain Iterative Text Revision dataset: ITERATER, which contains 31K document revisions from Wikipedia, ArXiv and Wikinews, and 4K edit actions with high-quality edit intention annotations. • analyze how text quality evolves across iterations and how text quality are affected by different kinds of edits. • find that incorporating the annotated editintentions is advantageous for text revision systems to generate better-revised documents."
    }, {
      "heading" : "2 Related Work",
      "text" : "Edit Intention Identification. Identification of edit intentions is an integral part of the iterative text revision task. Prior works have studied the categorization of different types of edit actions to help understand why editors do what they do and how effective their actions are (Yang et al., 2017; Zhang et al., 2017; Ito et al., 2019). However, these works do not further explore how to leverage edit intentions to generate better-revised documents. Moreover, some of their proposed edit intention taxonomies are constructed with a focus\n2https://www.wikipedia.org/ 3https://arxiv.org/ 4https://www.wikinews.org/\nDataset Size Domain Gran. Hist. Ann. Yang et al. (2017) 5K Wiki P × √ Anthonio et al. (2020) 2.7M Wiki S √\n× Zhang et al. (2017) 180 Academic S √ √ Spangher and May (2021) 4.6M News S √\n× ITERATER (Ours) 31K All S&P √ √\nTable 2: Comparisons with previous related works. Gran. for Granularity: S for sentence-level and P for paragraph-level. Hist. for Revision History. Ann. for Edit Intention Annotations.\non specific domains of writing, such as Wikipedia articles (Anthonio et al., 2020; Bhat et al., 2020; Faltings et al., 2021) or academic essays (Zhang et al., 2017). As a result, their ability to generalize to other domains remains an open question.\nSingle-Pass Text Revision Models. Some prior works (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021) simplify the text revision task to a single-pass \"original-to-final\" sentence-to-sentence generation task. However, it is very challenging to conduct multiple perfect edits at once. For example, adding transition words or reordering the sentences are required to further improve the document quality. Therefore, singlepass sentence-to-sentence text revision models are not sufficient to deal with real-world challenges of text revision tasks. In this work, we explore the performance of text revision models in multiple iterations and multiple granularities.\nIterative Text Revision Datasets. While some prior works have constructed iterative text revision datasets, they are limited to singular writing domains, such as Wikipedia-style articles (Anthonio et al., 2020), academic essays (Zhang et al., 2017) or news articles (Spangher and May, 2021). In this work, we develop a unified taxonomy to analyze the characteristics of iterative text revision behaviors across different domains and collect large scale text revisions of human writings from multiple domains. The differences between ITERATER and the prior datasets are summarized in Table 2."
    }, {
      "heading" : "3 Formulation: Iterative Text Revision",
      "text" : "We provide formal definitions of the Iterative Text Revision task, and its building blocks.\nEdit-Action. An edit-action ak is a local change applied on a certain text object, where k is the index of the current edit-action. The local changes include: insert, delete and modify. The text objects\ninclude: token, phrase, sentence, and paragraph. This work defines local changes applied on tokens or phrases as sentence-level edits, local changes applied on sentences as paragraph-level edits and local changes applied on paragraphs as documentlevel edits.\nEdit-Intention. An edit-intention ek reflects the revising goal of the editor when making a certain edit-action. In this work, we assume each edit-action ak will only be labeled with one editintention ek. We further describe our edit-intention taxonomy in Table 4 and §4.2.1.\nDocument-Revision. A document-revision is created when an editor saves changes for the current document (Yang et al., 2016, 2017). One revision Rt is aligned with a pair of documents (Dt−1,Dt) and containsK edit-actions, where t indicates the version of the document and K ≥ 1. A revision with K edit-actions will correspondingly have K edit-intentions:\n(Dt−1,Dt)→ Rt = {(atk, etk)}Kk=1 (1)\nWe define t as the document version or revision depth, which we use interchangeably.\nIterative Text Revision. Given a source text Dt−1, iterative text revision is the task of generating revisions of textDt at depth t until the quality of the text in the final revision satisfies a set of pre-defined stopping criteria {s0, ..., sM}:\nDt−1 g(D)−−−→ Dt, if f(Dt) < {s0, ..., sM} (2)\nwhere g(D) is a text revision system and f(D) is a quality evaluator of the revised text."
    }, {
      "heading" : "4 ITERATER Dataset",
      "text" : ""
    }, {
      "heading" : "4.1 Raw Data Collection",
      "text" : "Domains. We select three domains – Wikipedia articles, academic papers, and news articles – to cover different human writing goals, formats, revision patterns, and quality standards. The three domains consist of formally written texts, typically edited by multiple authors. We describe how we collect revision text from each domain below: • Scientific Papers. Scientific articles are written\nin a rigorous, logical manner. Authors generally highlight and revise their hypotheses, experimental results, and research insights in this domain. We collect paper abstracts submitted at different timestamps (i.e., version labels) from ArXiv. • Wikipedia Articles. Encyclopedic articles are written in a formal, coherent manner, where editors typically focus on improving the clarity and structure of articles to make people easily understand all kinds of factual and abstract encyclopedic information. We collect revision histories of the main contents of Wikipedia articles. • News Articles. News articles are generally written in a precise and condensed way. News editors emphasize improving the clarity and readability of news articles to keep people updated on rapidly changing news events. We collect revision histories of news content from Wikinews.\nRaw Data Processing. We first collect all raw documents, then sort each document version according to its timestamp in ascending order. For each document D, we pair two consecutive versions as one revision (Dt−1,Dt) → Rt, where t is the revision depth. For each sampled documentrevision Rt, we extract its full edit-actions using latexdiff.5 We provide both the paragraph-level\n5https://www.ctan.org/pkg/latexdiff\nand sentence-level revisions where the latter is constructed by applying a sentence segmentation tool,6 and aligning each sentence to each revision. For each revision pair, we have: the revision type, the document id, the revision depth, an original phrase and a revised phrase, respectively.7 The detailed processing of raw text is described in Appendix A.\nIn summary, we collect 31,631 documentrevisions with 196,987 edit-actions, and maintain a relatively balanced distribution across three domains, as shown in Table 3. We call this large-scale dataset as ITERATER-FULL-RAW."
    }, {
      "heading" : "4.2 Data Annotation",
      "text" : "To better understand the human revision process, we sample 559 document-revisions from ITERATER-FULL-RAW, consisting of 4,018 human edit-actions. We refer to this small-scale unannotated dataset as ITERATER-HUMAN-RAW. In §4.2.2, we then use Amazon Mechanical Turk (AMT) to crowdsource edit-intention annotations for each edit-action according to our proposed editintention taxonomy (§4.2.1). To further improve the annotation agreement, we ask expert linguists to re-annotate the edits that do not have a majority vote among AMT workers. We refer to this smallscale annotated dataset as ITERATER-HUMAN.8\nWe then scale these human annotations to ITERATER-FULL-RAW by training edit-intention prediction models on ITERATER-HUMAN, and automatically-label ITERATER-FULL-RAW to construct ITERATER-FULL. (§4.2.3)\n6https://github.com/zaemyung/sentsplit 7We also record character-level indices of their positions\nwithin the original sentence and the paragraph. 8We provide our annotation instruction in Appendix C."
    }, {
      "heading" : "4.2.1 Edit-Intention Taxonomy",
      "text" : "For human annotation, we propose a new editintention taxonomy in ITERATER (Table 4), in order to comprehensively model the iterative text revision process. Our taxonomy focuses on categorizing edit actions that do not change the meaning of the text.9 Our proposed taxonomy of edit intentions is generally applicable to multiple domains, edit-action granularities (sentence-level and paragraph-level), and revision depths."
    }, {
      "heading" : "4.2.2 Human Annotation",
      "text" : "Since edit-intention annotation is a challenging task, we design strict qualification tests to select 11 qualified AMT annotators (details in Appendix B). To further improve the annotation quality, we ask another group of expert linguists (English L1, bachelor’s or higher degree in Linguistics) to reannotate the edits which do not have a majority vote among the AMT workers. Finally, we take the majority vote among 3 human annotations (either from AMT workers or from expert linguists) as the final edit-intention labels. This represents the ITERATER-HUMAN dataset. We will release both the final majority vote and the three raw human annotations per edit-action as part of the dataset."
    }, {
      "heading" : "4.2.3 Automatic Annotation",
      "text" : "To scale up the annotation, we train an editintention classifier to annotate ITERATER-FULLRAW and construct the ITERATER-FULL dataset. We split the ITERATER-HUMAN dataset into 3,254/400/364 train, validation and testing pairs.\n9In practice, we observe that many edit-actions change the meaning of the text (e.g. add or update new information) which cannot be simply discarded, therefore we also include a \"Meaning-Changed\" intention to capture these types of edits.\nThe edit-intention classifier is a RoBERTa-based (Liu et al., 2019) multi-class classifier that predicts an intent given the original and the revised text for each edit-action. Table 5 shows its performance on the test set. The Fluency and Clarity edit-intentions are easy to predict with F1 scores of 0.8 and 0.69, respectively, while Style and Coherence edit-intentions are harder to predict with F1 scores of 0.13 and 0.32, respectively, largely due to the limited occurrence of Style and Coherence intents in the training data (Table 4)."
    }, {
      "heading" : "4.3 Data Analysis",
      "text" : "Edit-Intention Distributions. The iterative editintention distributions under three domains are demonstrated in Figure 1. Across all three domains, authors tend to make the majority of edits at revision depth 1. However, the number of edits rapidly decreases at revision depth 2, and few edits are made at revision depth 3 and 4.\nWe find that Clarity is one of the most frequent edit-intention across all domains, indicating that authors focus on improving readability across all domains. For ArXiv, Meaning-changed edits are also among the most frequent edits, which indicates that authors also focus on updating the contents of their abstracts to share new research insights or update existing ones. For Wikipedia, we also find that Fluency, Coherence, and Meaning-changed edits roughly share a similar frequency, which indicates Wikipedia articles have more complex revision pat-\nterns than ArXiv and news articles. For Wikinews, Fluency edits are equally emphasized, indicating that improving grammatical correctness of the news articles is just as important.\nInter-Annotator Agreement. We measure interannotator agreement (IAA) using the Fleiss’ κ (Fleiss, 1971). Table 6 shows the IAA across three domains. After the first round of annotation on AMT, Fleiss’ κ is 0.3628, which indicates fair agreement among annotators. After the second round of re-annotation by proficient linguists, the Fleiss’ κ increases to 0.5014, which indicates moderate agreement among annotators.\nWe further look at the raw annotations where at least 1 out of 3 annotators assign a different edit-intention label. We find that the Coherence intention is the one that is the most likely to have a disagreement: 312 out of 393 Coherence annotations do not have consensus. Within those disagreements of the Coherence intention, 68.77% are considered to be Clarity, and 11.96% are considered to be the Fluency intention. Annotators also often disagree on the Clarity intention, where 1023 out of 1601 Clarity intentions do not have a consensus. Among those disagreements of the Clarity intention, 30.33% are considered to be Coherence, and 30.23% are considered to be Style.\nThe above findings explain why the interannotator agreement scores are lower in Wikipedia and ArXiv. As shown in Figure 1, Wikipedia has many coherence edits while ArXiv has many clar-\nity edits. This explains the difficulty of the editintention annotation task: it not only asks annotators to infer the edit-intention from the full document context, but also requires annotators to have a wide range of domain-specific knowledge in scientific writings."
    }, {
      "heading" : "5 Understanding Iterative Text Revisions",
      "text" : "To better understand how human revisions affect the overall quality of documents, we conduct both human and automatic evaluations on a sampled set of document revisions."
    }, {
      "heading" : "5.1 Experiment Setups",
      "text" : "Evaluation Data. We sample two sets of text revisions for different evaluation purposes. The first set contains 21 iterative document-revisions, consisting of 7 unique documents, each document having 3 document-revisions from revision depth 1 to 3. The second set contains 120 text pairs, each associated with exactly one edit-intention of Fluency, Coherence, Clarity or Style. We validate the following research questions: RQ1 How do human revisions affect the text quality across revision depths? RQ2 How does text quality vary across edit-\nintentions?\nHuman Evaluation Configuration. We hire a group of proficient linguists to evaluate the overall quality of the documents/sentences, where each revision is annotated by 3 linguists. For each revision, we randomly shuffle the original and human revised texts, and ask the evaluators to select which one has better overall quality. They can choose one of the two texts, or neither. Then, we calculate the score for the overall quality of the human revisions as follows: -1 means the human revision has worse overall quality than the original text; 0 means the human revision do not show a better overall quality than the original text, or cannot reach agreement among 3 annotators; 1 means the human revision has better overall quality than the original text.\nAutomatic Evaluation Configuration. We select four automatic metrics to evaluate the document quality on four different aspects: Syntactic Log-Odds Ratio (SLOR) (Kann et al., 2018) for Fluency, Entity Grid (EG) score (Lapata et al., 2005) for Coherence, Flesch–Kincaid Grade Level (FKGL) (Kincaid et al., 1975) for Readability and BLEURT score (Sellam et al., 2020) for Content\nPreservation. We describe the detailed justification of our metric selection in Appendix D."
    }, {
      "heading" : "5.2 Quality Analyses on Revised Texts",
      "text" : "RQ1: Iterative Revisions vs. Quality. Table 7 shows the document quality changes at different revision depths. Generally, human revisions improve the overall quality of original documents, as indicated by the overall score at each revision depth.10 However, the overall quality keeps decreasing as the revision depth increases from 1 to 3, likely because it is more difficult for human evaluators to grasp the overall quality in the deeper revision depths in the pair-wise comparisons between the original and revised documents.\nFor automatic metrics, BLEURT indicates that much content information has been preserved in depths 1 and 2, compared to depth 3. ∆FKGL demonstrates that the readability of the document has some improvement in the first 2 depths, but degrades in the last one. Moreover, since many content updates are applied in depth 3, we guess that they may influence the readability. However, ∆SLOR and ∆EG are not well-aligned with human overall score, we further examine whether human revisions makes original documents less fluent and less coherent in the analysis of RQ2.\nRQ2: Edit-Intentions vs. Quality. Table 8 shows how text quality varies across edit-intentions. We find that Fluency and Coherence edits indeed improve the overall quality of original sentences according to human judgments. This finding suggests that ∆SLOR and ∆EG are not well-aligned with human judgements, and calls for the need to ex-\n10We further validate this observation in an another set of 50 single document-revisions in Appendix E.\nplore other effective automatic metrics to evaluate Fluency and Coherence in documents. Besides, we observe that Style edits degrade the overall quality of original sentences. This observation also makes sense since Style edits reflect the writer’s personal manner (according to our edit-intention taxonomy in Table 4), which not necessarily improve the readability, fluency or coherence of the text."
    }, {
      "heading" : "6 Modeling Iterative Text Revisions",
      "text" : "To better understand the challenges of modeling the task of iterative text revisions, we train different types of text revision models using ITERATER."
    }, {
      "heading" : "6.1 Experiment Setups",
      "text" : "Text Revision Models. For training the text revision models, we experiment with both edit-based and generative models. For the edit-based model, we use FELIX (Mallinson et al., 2020), and for the generative model, we use PEGASUS (Zhang et al., 2020). FELIX decomposes text revision into two sub-tasks: Tagging, which uses a pointer mechanism to select the subset of input tokens and their order; and Insertion, which uses a masked language model to in-fill missing tokens in the output not present in the input. PEGASUS, on the other hand, is a large transformer-based encoder-decoder model pre-trained for abstractive summarization.\nTraining. We use four training configurations to evaluate whether edit-intention information can help better model text revisions. The first configuration uses the pure revision pairs without edit-intention annotations (ITERATER-HUMANRAW dataset). In the second configuration, we include the human-annotated edit-intentions to the source text (ITERATER-HUMAN dataset). Similarly, for the third and fourth training configurations, we use ITERATER-FULL-RAW dataset (no edit-intention information) and ITERATERFULL dataset (automatically-annotated labels, as described in §4.2.3, simply appended to the input text). We use these four configurations for both FELIX and PEGASUS models.\nAutomatic Evaluation. Following prior works (Malmi et al., 2019; Dong et al., 2019; Mallinson et al., 2020), we use SARI as our main automated metric, but include other automatic metrics in Appendix G. SARI calculates how often a generated output text correctly keeps, inserts, and deletes\nn-grams from the source text, compared to the reference text, where 1 ≤ n ≤ 4.\nHuman Evaluation. Similar to §5.1, we hire a group of linguists to evaluate the quality of model generated document-revisions, where each data pair is evaluated by 3 linguists. We choose the bestperforming model according to Table 9 (which is PEGASUS trained on ITERATER-FULL), and compare its generated revisions with human revisions. We randomly shuffle model revisions and human revisions, and ask linguists to select which one leads to better document quality. We provide detailed human evaluation configuration in Appendix F."
    }, {
      "heading" : "6.2 Results Analysis",
      "text" : "Automatic Evaluation. Table 9 shows the results of the two models for our different training configurations. It is noteworthy that the SARI score on the no-edit baseline is the lowest, which indicates the positive impact of revisions on document quality, as also corroborated by the human evaluations in §5. For both ITERATER-HUMAN and ITERATER-FULL datasets, we see that editintention annotations help to improve the performance of both FELIX and PEGASUS. Also, both models perform better on the larger ITERATERFULL dataset compared to the ITERATER-HUMAN dataset, showing that the additional data (and automatically-annotated annotations) are helpful.\nHuman Evaluation. Table 10 shows how the model revision affects the quality of the original document. There exists a big gap between the bestperforming model revisions and human revisions, indicating the challenging nature of the modeling problem. We also observe that the models tend to make non-meaning-changing edits, whereas there are many more meaning-changing edits in human revisions. Thus, while model revisions can achieve comparable performance with human revisions on Fluency, Coherence and Meaning Preservation, human revisions still outperform in terms of Readability and Overall Quality.\nTable 11 demonstrates how model-generated text quality varies across revision depths. In the first two depths, human revisions win over model revisions with a ratio of 57.14%. However, in the last depth, model revisions tie with human revisions with a similar percentage. Upon reviewing revisions in the last depth, we find a lot of content updates in human revisions. At the same time, the model revisions only made a few fluency or clarity edits, which the human evaluators tend to judge as “tie”.\nIterativeness. We also compare the iterative ability between the two kinds of text revision models (best performing versions of both FELIX and PEGASUS: trained on ITERATER-FULL), against human’s iterative revisions. Figure 2 shows that while PEGASUS is able to finish iterating after 2.57 revisions on average, FELIX continues to make it-\nerations until the maximum cutoff of 10 that we set for the experiment. In contrast, humans on average make 1.61 iterations per document. While FELIX is able to make meaningful revisions (as evidenced by the improvements in the SARI metric in Table 14), it lacks the ability to effectively evaluate the text quality at a given revision, and decide whether or not to make further changes. PEGASUS, on the other hand, is able to pick up on these nuances of iterative revision, and learns to stop revising after a certain level of quality has been reached."
    }, {
      "heading" : "7 Discussions",
      "text" : "Our study shows that existing text revision models fail to perform effective iterative text revision as humans do. We aim to dive deeper into these limitations and develop a system that can consider previous revision history and perform end-to-end iterative revisions using ITERATER. Incorporating human writers into such end-to-end systems and revising content together would be an exciting direction towards building intelligent writing assistants.\nDespite the deliberate design of our dataset collection, ITERATER only includes formally written texts. We plan to extend it to diverse sets of revision texts, such as informally written blogs and less informal but communicative texts like emails, as well as increase the size of the current dataset.\nInterestingly, our analysis shows that human judgments differ from automatic evaluation metrics on human revised text. It remains an important next step to develop more accurate metrics for coherence, fluency, and general text quality."
    }, {
      "heading" : "8 Conclusion",
      "text" : "Our work is a step toward understanding the complex process of iterative text revision from humanwritten texts. We collect, annotate and release ITERATER: a novel, large-scale, domain-diverse, annotated dataset of human edit actions. Our research showed that different domains of text have different distributions of edit intentions, and the general quality of the text has improved over time. Computationally modeling the human’s revision process is still under-explored, yet our results indicate some interesting findings and potential directions. For future research, we believe ITERATER can serve as a basis for future corpus development and computationally modeling iterativeness of text revision."
    }, {
      "heading" : "9 Ethical Considerations",
      "text" : "We collect all data from publicly available sources, and respect copyrights for original document authors. During the data annotation process, all human annotators are anonymized to respect their privacy rights. We provide fair compensation to all human annotators, where each annotator gets paid more than the minimum wage and based on the number of annotations they conducted.\nWe provide detailed descriptions of dataset characteristics in §4 and §5 to help readers understand which technology could be expected to work for this dataset. Our experimental results in §5 and §6 are aligned with our claims. We conduct evaluation for the quality of our dataset in §5.\nOur work has no possible harms to fall disproportionately on marginalized or vulnerable populations. Our dataset does not contain any identity characteristics (e.g. gender, race, ethnicity), and will not have ethical implications of categorizing people."
    }, {
      "heading" : "A Details on Text Processing in",
      "text" : "ITERATER\nFor Wikipedia and Wikinews, we use the MediaWiki Action API11 to retrieve raw pages updated at different timestamps. For each article, we start from July 2021 and trace back to its five most recent updated versions. Then, we parse12 plain texts from raw wiki-texts and filter out all references and external links.For Wikipedia, we retrieve pages under the categories listed on the main category page 13. For Wikinews, we retrieve pages listed on the published articles page14.\nFor ArXiv, we use the ArXiv API15 to retrieve paper abstracts. Note that we do not retrieve the full paper for two reasons: (1) some paper reserved their copyright for distribution, (2) parsing and aligning editing actions in different document types (e.g. pdf, tex) is challenging. For each paper, we start from July 2021 and retrieve all its previous submissions. We collect papers in the fields of Computer Science, Quantitative Biology, Quantitative Finance, and Economics."
    }, {
      "heading" : "B Details on Qualificiation Tests for Human Annotation",
      "text" : "First, we prepare a small test set with 67 editactions and deploy parallel test runs on AMT to get more workers participate in this task. Before starting the annotation, workers are required to pass a qualification test which has 5 test questions to get familiar with our edit-intention taxonomy. Second, we compare workers’ annotations with our golden annotations, and select workers who have an accuracy over 0.4. After 5 test runs, we select 11 AMT workers who are qualified to participate in this task. Then, we deploy the full 4K edit-actions on AMT, and collect 3 human annotations per edit-action."
    }, {
      "heading" : "C Human Annotation Instruction and Interface",
      "text" : "To guide human annotators make accurate editintention annotation, we provide them with a short\n11https://www.mediawiki.org/wiki/API: Main_page\n12https://github.com/earwig/ mwparserfromhell\n13https://en.wikipedia.org/wiki/ Wikipedia:Contents/Categories\n14https://en.wikinews.org/wiki/Category: Published\n15https://arxiv.org/help/api/\ntask instruction (Figure 3) followed by some concrete edit-intention examples (Figure 4). Then, we highlight the edit-action within the documentrevision and ask human annotators three questions to obtain the accurate edit-intention of the current edit-action, as illustrated in Figure 5. Note that in our previous test runs on AMT, we find that AMT workers can hardly have a consensus on Clarity and Style edits, which give a very low IAA score. Therefore, in the annotation interface, we include Clarity and Style edits under the category of \"Rephrasing\", and further ask the annotators to judge whether the current \"Rephrasing\" edit is making the text more clearer and understandable. If yes, we convert this edit to Clarity, otherwise we convert this edit to Style. This interface configuration gives us the best IAA score among our 5 test runs."
    }, {
      "heading" : "D Justification of Automatic Evaluation Metrics",
      "text" : "For Fluency, we use the Syntactic Log-Odds Ratio (SLOR) (Kann et al., 2018) to evaluate the naturalness and grammaticality of the current revised document, where a higher SLOR score indicates a more fluent document. Prior works (Pauls and Klein, 2012; Kann et al., 2018) found word-piece log-probability correlates well with human fluency ratings. For Coherence, we use the Entity Grid (EG) score (Lapata et al., 2005) to evaluate the local coherence of the current revised document, where a higher EG score indicates a more coherent document. EG is a widely adopted (Soricut and Marcu, 2006; Elsner and Charniak, 2008; Louis and Nenkova, 2012) metric for measuring document coherence. For Readability, we use the the Flesch–Kincaid Grade Level (FKGL) (Kincaid et al., 1975) to evaluate how easy the current revised document is for the readers to understand, where a lower FKGL indicates a more readable document. FKGL is a popular metric that has been used by many prior works (Solnyshkina et al., 2017; Xu et al., 2016; Guo et al., 2018; Nassar et al., 2019; Nishihara et al., 2019) to measure the readability of documents. For Content Preservation, we use the BLEURT score (Sellam et al., 2020) to measure how much content has been changed from the previous document to the current revised one, where a higher BLEURT score indicates more content has been preserved. BLEURT has been shown to correlate better with human judgments\nthan other metrics that take semantic information into account, e.g. METEOR (Banerjee and Lavie, 2005) or BERTScore (Zhang* et al., 2020)."
    }, {
      "heading" : "E Details on Human Evaluation for Single Human Revision Quality",
      "text" : "Evaluation Data. To evaluate how do human revisions affect the text quality, we sample 50 single document-revisions, which contains 50 randomly sampled documents and each document has 1 document-revision.16\nResult Analysis. In Table 12, we observe that human revised documents generally improve the overall quality of original documents. As for the automatic metrics, BLEURT indicates that human revisions preserve much of the content, and ∆FKGL shows that the readability of original documents\n16We exclude documents including Meaning-changed edits\nimproves by human revisions. However, ∆SLOR and ∆EG show a slight drop in performance. We conjecture this is because (1) ∆SLOR and ∆EG are not well-aligned with human judgements, or (2) human revisions make original documents less fluent and less coherent.\nCorrelation Analysis. To analyze how automatic metrics are correlated with human overall quality score, we compute the Pearson (Kowalski, 1972) and Spearman (Zwillinger and Kokoska, 1999) correlation coefficients between the automatic metrics and the human overall quality scores based on 50 single document-revisions and 21 iterative document-revisions. Table 13 shows that BLEURT and ∆FKGL are positively correlated with human overall quality score, while ∆SLOR and ∆EG are negatively correlated with human overall quality score."
    }, {
      "heading" : "F Details on Human Evaluation Configuration for Model Revisions",
      "text" : "First, we evaluate how do model revisions affect the quality of the document. We randomly sample 30 single document-revisions which do not contain Meaning-changed edits, and input the original documents to the best-performing model to get the model-revised documents. Then, for each data pair, we randomly shuffle model revisions and human revisions, and ask human evaluators to select which revision leads to better document quality in terms of: • Content Preservation: keeping more content\ninformation unchanged; • Fluency: fixing more grammatical errors or syn-\ntactic errors; • Coherence: making the sentences more logically\nlinked and organized; • Readability: making the text easier to read and\nunderstand; • Overall Quality: better improving the overall\nquality of the document. We provide the evaluation interface in Figure 6.\nSecondly, we evaluate how does model generated text quality vary across revision depths. We use the same set of 21 iterative document-revisions in §5.1. We feed the original documents into the best-performing model to obtain the model revised documents at each revision depth. For each data\npair, we randomly shuffle model revisions and human revisions, and ask human evaluators to judge which one gives better overall text quality. We provide the evaluation interface in Figure 7."
    }, {
      "heading" : "G Details on Automatic Evaluation for Model Revisions",
      "text" : "Table 14 provides detailed automatic evaluation results for FELIX and PEGASUS, including SARI, BLEU, and ROUGE. We choose these automatic metrics following prior text revision works (Malmi et al., 2019; Dong et al., 2019; Mallinson et al., 2020).\nTable 15 further provides SARI score under different revision depths as well as different editintentions. We find that PEGASUS only conduct deletions in the revision depth 3, and the SARI score for each edit-intention varies a lot across different revision depths.\nTable 16 and Table 17 are some examples of iterative text revisions generated by FELIX and PEGASUS trained on ITERATER-FULL. We observe that while FELIX can make more edits with more iterations than PEGASUS, it cannot ensure the quality of its generated edits. FELIX often insert some random out-of-context tokens into the original text, and distort the semantic meaning of the original text. PEGASUS is better at preserving the semantic meaning of the original text, but it is more likely to delete phrases or tokens in deeper revision depth."
    } ],
    "references" : [ {
      "title" : "wikiHowToImprove: A resource and analyses on edits in instructional texts",
      "author" : [ "Talita Anthonio", "Irshad Bhat", "Michael Roth." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 5721–5729, Marseille, France. Euro-",
      "citeRegEx" : "Anthonio et al\\.,? 2020",
      "shortCiteRegEx" : "Anthonio et al\\.",
      "year" : 2020
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Towards modeling revision requirements in wikiHow instructions",
      "author" : [ "Irshad Bhat", "Talita Anthonio", "Michael Roth." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8407–8414, Online. As-",
      "citeRegEx" : "Bhat et al\\.,? 2020",
      "shortCiteRegEx" : "Bhat et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to split and rephrase from Wikipedia edit history",
      "author" : [ "Jan A. Botha", "Manaal Faruqui", "John Alex", "Jason Baldridge", "Dipanjan Das." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Botha et al\\.,? 2018",
      "shortCiteRegEx" : "Botha et al\\.",
      "year" : 2018
    }, {
      "title" : "Revising strategies in twelfth grade students’ transactional writing",
      "author" : [ "Lillian S. Bridwell." ],
      "venue" : "Research in the Teaching of English, 14(3):197–222.",
      "citeRegEx" : "Bridwell.,? 1980",
      "shortCiteRegEx" : "Bridwell.",
      "year" : 1980
    }, {
      "title" : "A framework for a cognitive theory of writing",
      "author" : [ "Allan Collins", "Dedre Gentner." ],
      "venue" : "Cognitive processes in writing, pages 51–72. Erlbaum.",
      "citeRegEx" : "Collins and Gentner.,? 1980",
      "shortCiteRegEx" : "Collins and Gentner.",
      "year" : 1980
    }, {
      "title" : "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
      "author" : [ "Yue Dong", "Zichao Li", "Mehdi Rezagholizadeh", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Coreference-inspired coherence modeling",
      "author" : [ "Micha Elsner", "Eugene Charniak." ],
      "venue" : "Proceedings of ACL-08: HLT, Short Papers, pages 41–44, Columbus, Ohio. Association for Computational Linguistics.",
      "citeRegEx" : "Elsner and Charniak.,? 2008",
      "shortCiteRegEx" : "Elsner and Charniak.",
      "year" : 2008
    }, {
      "title" : "Analyzing revision",
      "author" : [ "Lester Faigley", "Stephen Witte." ],
      "venue" : "College composition and communication, 32(4):400–414.",
      "citeRegEx" : "Faigley and Witte.,? 1981",
      "shortCiteRegEx" : "Faigley and Witte.",
      "year" : 1981
    }, {
      "title" : "Text editing by command",
      "author" : [ "Felix Faltings", "Michel Galley", "Gerold Hintz", "Chris Brockett", "Chris Quirk", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Faltings et al\\.,? 2021",
      "shortCiteRegEx" : "Faltings et al\\.",
      "year" : 2021
    }, {
      "title" : "WikiAtomicEdits: A multilingual corpus of Wikipedia edits for modeling language and discourse",
      "author" : [ "Manaal Faruqui", "Ellie Pavlick", "Ian Tenney", "Dipanjan Das." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Faruqui et al\\.,? 2018",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2018
    }, {
      "title" : "Research on revision in writing",
      "author" : [ "Jill Fitzgerald." ],
      "venue" : "Review of educational research, 57(4):481–506.",
      "citeRegEx" : "Fitzgerald.,? 1987",
      "shortCiteRegEx" : "Fitzgerald.",
      "year" : 1987
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin, 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "The dynamics of composing: Making plans and juggling constraints",
      "author" : [ "Linda Flower." ],
      "venue" : "Cognitive processes in writing, pages 31–50.",
      "citeRegEx" : "Flower.,? 1980",
      "shortCiteRegEx" : "Flower.",
      "year" : 1980
    }, {
      "title" : "The cognition of discovery: Defining a rhetorical problem",
      "author" : [ "Linda Flower", "John R. Hayes." ],
      "venue" : "College Composition and Communication, 31(1):21–32.",
      "citeRegEx" : "Flower and Hayes.,? 1980",
      "shortCiteRegEx" : "Flower and Hayes.",
      "year" : 1980
    }, {
      "title" : "Dynamic multi-level multi-task learning for sentence simplification",
      "author" : [ "Han Guo", "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 462–476, Santa Fe, New Mexico, USA.",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "Diamonds in the rough: Generating fluent sentences from early-stage drafts for academic writing assistance",
      "author" : [ "Takumi Ito", "Tatsuki Kuribayashi", "Hayato Kobayashi", "Ana Brassard", "Masato Hagiwara", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Ito et al\\.,? 2019",
      "shortCiteRegEx" : "Ito et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentence-level fluency evaluation: References help, but can be spared",
      "author" : [ "Katharina Kann", "Sascha Rothe", "Katja Filippova" ],
      "venue" : "In Proceedings of the 22nd Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Kann et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2018
    }, {
      "title" : "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel",
      "author" : [ "J Peter Kincaid", "Robert P Fishburne Jr", "Richard L Rogers", "Brad S Chissom." ],
      "venue" : "Technical report, Naval",
      "citeRegEx" : "Kincaid et al\\.,? 1975",
      "shortCiteRegEx" : "Kincaid et al\\.",
      "year" : 1975
    }, {
      "title" : "On the effects of nonnormality on the distribution of the sample productmoment correlation coefficient",
      "author" : [ "Charles J Kowalski." ],
      "venue" : "Journal of the Royal Statistical Society: Series C (Applied Statistics), 21(1):1–12.",
      "citeRegEx" : "Kowalski.,? 1972",
      "shortCiteRegEx" : "Kowalski.",
      "year" : 1972
    }, {
      "title" : "Automatic evaluation of text coherence: Models and representations",
      "author" : [ "Mirella Lapata", "Regina Barzilay" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Lapata and Barzilay,? \\Q2005\\E",
      "shortCiteRegEx" : "Lapata and Barzilay",
      "year" : 2005
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A coherence model based on syntactic patterns",
      "author" : [ "Annie Louis", "Ani Nenkova." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1157–1168, Jeju",
      "citeRegEx" : "Louis and Nenkova.,? 2012",
      "shortCiteRegEx" : "Louis and Nenkova.",
      "year" : 2012
    }, {
      "title" : "FELIX: Flexible text editing through tagging and insertion",
      "author" : [ "Jonathan Mallinson", "Aliaksei Severyn", "Eric Malmi", "Guillermo Garrido." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1244–1255, Online. Association for",
      "citeRegEx" : "Mallinson et al\\.,? 2020",
      "shortCiteRegEx" : "Mallinson et al\\.",
      "year" : 2020
    }, {
      "title" : "Encode, tag, realize: High-precision text editing",
      "author" : [ "Eric Malmi", "Sebastian Krause", "Sascha Rothe", "Daniil Mirylenka", "Aliaksei Severyn." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Malmi et al\\.,? 2019",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural versus non-neural text simplification: A case study",
      "author" : [ "Islam Nassar", "Michelle Ananda-Rajah", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association, pages 172–177, Syd-",
      "citeRegEx" : "Nassar et al\\.,? 2019",
      "shortCiteRegEx" : "Nassar et al\\.",
      "year" : 2019
    }, {
      "title" : "Controllable text simplification with lexical constraint loss",
      "author" : [ "Daiki Nishihara", "Tomoyuki Kajiwara", "Yuki Arase." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 260–",
      "citeRegEx" : "Nishihara et al\\.,? 2019",
      "shortCiteRegEx" : "Nishihara et al\\.",
      "year" : 2019
    }, {
      "title" : "Large-scale syntactic language modeling with treelets",
      "author" : [ "Adam Pauls", "Dan Klein." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 959–968, Jeju Island, Korea. Association for",
      "citeRegEx" : "Pauls and Klein.,? 2012",
      "shortCiteRegEx" : "Pauls and Klein.",
      "year" : 2012
    }, {
      "title" : "Research on written composition",
      "author" : [ "M. Scardamalia." ],
      "venue" : "Handbook of reserch on teaching.",
      "citeRegEx" : "Scardamalia.,? 1986",
      "shortCiteRegEx" : "Scardamalia.",
      "year" : 1986
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating text complexity and flesch-kincaid grade level",
      "author" : [ "Marina Solnyshkina", "Radif Zamaletdinov", "Ludmila Gorodetskaya", "Azat Gabitov." ],
      "venue" : "Journal of Social Studies Education Research, 8(3):238– 248.",
      "citeRegEx" : "Solnyshkina et al\\.,? 2017",
      "shortCiteRegEx" : "Solnyshkina et al\\.",
      "year" : 2017
    }, {
      "title" : "Revision strategies of student writers and experienced adult writers",
      "author" : [ "Nancy Sommers." ],
      "venue" : "College composition and communication, 31(4):378–388.",
      "citeRegEx" : "Sommers.,? 1980",
      "shortCiteRegEx" : "Sommers.",
      "year" : 1980
    }, {
      "title" : "Discourse generation using utility-trained coherence models",
      "author" : [ "Radu Soricut", "Daniel Marcu." ],
      "venue" : "Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 803–810, Sydney, Australia. Association for Computational Linguis-",
      "citeRegEx" : "Soricut and Marcu.,? 2006",
      "shortCiteRegEx" : "Soricut and Marcu.",
      "year" : 2006
    }, {
      "title" : "textit{NewsEdits}: A dataset of revision histories for news articles (technical report: Data processing)",
      "author" : [ "Alexander Spangher", "Jonathan May." ],
      "venue" : "CoRR, abs/2104.09647.",
      "citeRegEx" : "Spangher and May.,? 2021",
      "shortCiteRegEx" : "Spangher and May.",
      "year" : 2021
    }, {
      "title" : "A model of revision in natural language generation",
      "author" : [ "Marie M. Vaughan", "David D. McDonald." ],
      "venue" : "24th Annual Meeting of the Association for Computational Linguistics, pages 90–96, New York, New York, USA. Association for Computational Linguis-",
      "citeRegEx" : "Vaughan and McDonald.,? 1986",
      "shortCiteRegEx" : "Vaughan and McDonald.",
      "year" : 1986
    }, {
      "title" : "Optimizing Statistical Machine Translation for Text Simplification",
      "author" : [ "Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:401–415.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Who did what: Editor role identification in wikipedia",
      "author" : [ "Diyi Yang", "Aaron Halfaker", "Robert Kraut", "Eduard Hovy." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media, volume 10.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Identifying semantic edit intentions from revisions in Wikipedia",
      "author" : [ "Diyi Yang", "Aaron Halfaker", "Robert Kraut", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2000–2010,",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "A corpus of annotated revisions for studying argumentative writing",
      "author" : [ "Fan Zhang", "Homa B. Hashemi", "Rebecca Hwa", "Diane Litman." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter Liu." ],
      "venue" : "International Conference on Machine Learning, pages 11328–11339. PMLR.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Zhang. et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang. et al\\.",
      "year" : 2020
    }, {
      "title" : "CRC standard probability and statistics tables and formulae",
      "author" : [ "Daniel Zwillinger", "Stephen Kokoska." ],
      "venue" : "Crc Press. 11",
      "citeRegEx" : "Zwillinger and Kokoska.,? 1999",
      "shortCiteRegEx" : "Zwillinger and Kokoska.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Writing is a complex and effortful cognitive task, where writers balance and orchestrate three distinct cognitive processes: planning, translation, and revising (Flower and Hayes, 1980).",
      "startOffset" : 161,
      "endOffset" : 185
    }, {
      "referenceID" : 28,
      "context" : "This work focuses on text revision as an essential part of writing (Scardamalia, 1986).",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : "It enables writers to deliberate over and organize their thoughts, find a better line of argument, learn afresh, and discover what was not known before (Sommers, 1980).",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "text revision involves identifying discrepancies between intended and instantiated text, deciding what edits to make, and how to make those desired edits (Faigley and Witte, 1981; Fitzgerald, 1987; Bridwell, 1980).",
      "startOffset" : 154,
      "endOffset" : 213
    }, {
      "referenceID" : 11,
      "context" : "text revision involves identifying discrepancies between intended and instantiated text, deciding what edits to make, and how to make those desired edits (Faigley and Witte, 1981; Fitzgerald, 1987; Bridwell, 1980).",
      "startOffset" : 154,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : "text revision involves identifying discrepancies between intended and instantiated text, deciding what edits to make, and how to make those desired edits (Faigley and Witte, 1981; Fitzgerald, 1987; Bridwell, 1980).",
      "startOffset" : 154,
      "endOffset" : 213
    }, {
      "referenceID" : 13,
      "context" : "Human writers are unable to simultaneously comprehend multiple demands and constraints of the task when producing well-written texts (Flower, 1980; Collins and Gentner, 1980; Vaughan and McDonald, 1986) – for instance, expressing ideas, covering the content, following linguistic norms and discourse conventions of written prose, etc.",
      "startOffset" : 133,
      "endOffset" : 202
    }, {
      "referenceID" : 5,
      "context" : "Human writers are unable to simultaneously comprehend multiple demands and constraints of the task when producing well-written texts (Flower, 1980; Collins and Gentner, 1980; Vaughan and McDonald, 1986) – for instance, expressing ideas, covering the content, following linguistic norms and discourse conventions of written prose, etc.",
      "startOffset" : 133,
      "endOffset" : 202
    }, {
      "referenceID" : 34,
      "context" : "Human writers are unable to simultaneously comprehend multiple demands and constraints of the task when producing well-written texts (Flower, 1980; Collins and Gentner, 1980; Vaughan and McDonald, 1986) – for instance, expressing ideas, covering the content, following linguistic norms and discourse conventions of written prose, etc.",
      "startOffset" : 133,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "ing (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021); (3) developing editing taxonomies within singular domains (e.",
      "startOffset" : 4,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "ing (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021); (3) developing editing taxonomies within singular domains (e.",
      "startOffset" : 4,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "ing (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021); (3) developing editing taxonomies within singular domains (e.",
      "startOffset" : 4,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "ing (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021); (3) developing editing taxonomies within singular domains (e.",
      "startOffset" : 4,
      "endOffset" : 87
    }, {
      "referenceID" : 37,
      "context" : "Wikipedia articles, academic writings) (Yang et al., 2017; Zhang et al., 2017; Anthonio et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 38,
      "context" : "Wikipedia articles, academic writings) (Yang et al., 2017; Zhang et al., 2017; Anthonio et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "Wikipedia articles, academic writings) (Yang et al., 2017; Zhang et al., 2017; Anthonio et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 37,
      "context" : "Prior works have studied the categorization of different types of edit actions to help understand why editors do what they do and how effective their actions are (Yang et al., 2017; Zhang et al., 2017; Ito et al., 2019).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 38,
      "context" : "Prior works have studied the categorization of different types of edit actions to help understand why editors do what they do and how effective their actions are (Yang et al., 2017; Zhang et al., 2017; Ito et al., 2019).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 16,
      "context" : "Prior works have studied the categorization of different types of edit actions to help understand why editors do what they do and how effective their actions are (Yang et al., 2017; Zhang et al., 2017; Ito et al., 2019).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 0,
      "context" : "on specific domains of writing, such as Wikipedia articles (Anthonio et al., 2020; Bhat et al., 2020; Faltings et al., 2021) or academic essays (Zhang et al.",
      "startOffset" : 59,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "on specific domains of writing, such as Wikipedia articles (Anthonio et al., 2020; Bhat et al., 2020; Faltings et al., 2021) or academic essays (Zhang et al.",
      "startOffset" : 59,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "on specific domains of writing, such as Wikipedia articles (Anthonio et al., 2020; Bhat et al., 2020; Faltings et al., 2021) or academic essays (Zhang et al.",
      "startOffset" : 59,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "Some prior works (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021) simplify the text revision task to a single-pass \"original-to-final\" sentence-to-sentence generation task.",
      "startOffset" : 17,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Some prior works (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021) simplify the text revision task to a single-pass \"original-to-final\" sentence-to-sentence generation task.",
      "startOffset" : 17,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "Some prior works (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021) simplify the text revision task to a single-pass \"original-to-final\" sentence-to-sentence generation task.",
      "startOffset" : 17,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "Some prior works (Faruqui et al., 2018; Botha et al., 2018; Ito et al., 2019; Faltings et al., 2021) simplify the text revision task to a single-pass \"original-to-final\" sentence-to-sentence generation task.",
      "startOffset" : 17,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "prior works have constructed iterative text revision datasets, they are limited to singular writing domains, such as Wikipedia-style articles (Anthonio et al., 2020), academic essays (Zhang et al.",
      "startOffset" : 142,
      "endOffset" : 165
    }, {
      "referenceID" : 38,
      "context" : ", 2020), academic essays (Zhang et al., 2017) or news articles (Spangher and May, 2021).",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : "The edit-intention classifier is a RoBERTa-based (Liu et al., 2019) multi-class classifier that pre-",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Table 6: Inter-annotator agreement (Fleiss’ κ (Fleiss, 1971)) across two rounds of annotations, where the 1stround only contains annotations from qualified AMT workers, and the 2nd-round contains annotations from both qualified AMT workers and expert linguists.",
      "startOffset" : 46,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "We measure interannotator agreement (IAA) using the Fleiss’ κ (Fleiss, 1971).",
      "startOffset" : 62,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "We select four automatic metrics to evaluate the document quality on four different aspects: Syntactic Log-Odds Ratio (SLOR) (Kann et al., 2018) for Fluency, Entity Grid (EG) score (Lapata et al.",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : ", 2005) for Coherence, Flesch–Kincaid Grade Level (FKGL) (Kincaid et al., 1975) for Readability and BLEURT score (Sellam et al.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : ", 1975) for Readability and BLEURT score (Sellam et al., 2020) for Content t Overall ↑ BLEURT↑ ∆SLOR ↑ ∆EG ↑ ∆FKGL ↓",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "For the edit-based model, we use FELIX (Mallinson et al., 2020), and for the generative model, we use PEGASUS (Zhang et al.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 39,
      "context" : ", 2020), and for the generative model, we use PEGASUS (Zhang et al., 2020).",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "Following prior works (Malmi et al., 2019; Dong et al., 2019; Mallinson et al., 2020), we use SARI as our main automated metric, but include other automatic metrics in Appendix G.",
      "startOffset" : 22,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "Following prior works (Malmi et al., 2019; Dong et al., 2019; Mallinson et al., 2020), we use SARI as our main automated metric, but include other automatic metrics in Appendix G.",
      "startOffset" : 22,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "Following prior works (Malmi et al., 2019; Dong et al., 2019; Mallinson et al., 2020), we use SARI as our main automated metric, but include other automatic metrics in Appendix G.",
      "startOffset" : 22,
      "endOffset" : 85
    } ],
    "year" : 0,
    "abstractText" : "Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human’s revision cycles. This work describes ITERATER: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text. In particular, ITERATER is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities. When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations.1 Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions.",
    "creator" : null
  }
}
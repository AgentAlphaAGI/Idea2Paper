{
  "name" : "ARR_2022_298_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Few-Shot Learning with Siamese Networks and Label Tuning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Few-shot learning is the problem of learning classifiers with only a few training examples. Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used. For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020; Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.\nThe entailment approach for text classification sets the input text as the premise and the text repre-\nsenting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.\nIn this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993), also known as dual encoders. These models embed both input and label texts into a common vector space. The similarity of the two items can then be computed using a similarity function such as the dot product. The advantage is that input and label text are encoded independently, which means that the label embeddings can be pre-computed. Therefore, at inference time, only a single call to the model per input is needed. In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.\nBoth CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks. In this work, we show that when using a SN, one can decide to only fine-tune the label embeddings. We call this Label Tuning (LT). With LT the encoder can be shared between different tasks, which greatly eases the deployment of this approach in a production setup. LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).\nOur contributions are as follows: We perform a large study on a diverse set of tasks showing that CA models and SN yield similar performance for both zero-shot and few-shot text classification.\nIn contrast to most prior work, we also show that these results can also be achieved for languages other than English. We compare the hypothesis patterns commonly used in the literature and using the plain label name (null hypothesis) and find that on average there is no significant difference in performance. Finally, we present LT as an alternative to full fine-tuning that allows using the same model for many tasks and thus greatly increases the scalability of the method. We will release the code and trained models used in our experiments."
    }, {
      "heading" : "2 Methodology",
      "text" : "Figure 1 explains the overall system. We follow Reimers and Gurevych (2019) and apply symmetric Siamese Networks that embed both input texts using a single encoder. The encoder consists of a transformer (Vaswani et al., 2017) that produces contextual token embeddings and a mean pooler that combines the token embeddings into a single text embedding. We use the dot product as the similarity function. We experimented with cosine similarity but did not find it to yield significantly better results.\nAs discussed, we can directly apply this model to zero-shot text classification by embedding the input text and a textual representation of the label. For the label representation we experiment with a\nplain verbalization of the label, or null hypothesis, as well as the hypotheses or prompts used in the related work.\nFine-Tuning In the case of few-shot learning, we need to adapt the model based on a small set of examples. In gradient-based few-shot learning we attempt to improve the similarity scores for a small set of labeled examples. Conceptually, we want to increase the similarity between every text and its correct label and decrease the similarity for every other label. As the objective we use the so called batch softmax (Henderson et al., 2017):\nJ = − 1 B B∑ i=1\n[ S(xi, yi)− log\nB∑ j=1\neS(xi,yj) ] (1)\nWhere B is the batch size and S(x, y) = f(x)·f(y) the similarity between input x and label text y under the current model f . All other elements of the batch are used as in-batch negatives. To this end, we construct the batches so that every batch contains exactly one example of each label. Note that this is similar to a typical softmax classification objective. The only difference is that f(yi) is computed during the forward pass and not as a simple parameter look-up.\nLabel Tuning Regular fine-tuning has the drawback of requiring to update the weights of the complete network. This results in slow training and\nlarge memory requirements for every new task, which in turn makes it challenging to deploy new models at scale. As an alternative, we introduce label tuning, which does not change the weights of the encoder. The main idea is to first pre-compute label embeddings for each class and later tune them using a small set of labeled examples. Formally, we have a training set containing N pairs of an input text xi and its reference label index zi. We pre-compute a matrix of the embedded input texts and embedded labels, X∈RN×d and Y ∈RK×d, respectively. d is the embedding dimension and K the size of the label set. We now define the score for every input and label combination as S = X ×Y T (S∈RN×K) and tune it using cross entropy:\nJ ′ = − 1 N N∑ i=1 Si,zi − log K∑ j=1 eSi,j  (2) To avoid overfitting, we add a regularizer that penalizes moving too far from the initial label embeddings Y0 as ‖Y0 − Y ‖F , where ‖.‖F is the Frobenius norm.1 Additionally, we also implement a version of dropout by masking some of the entries in the label embedding matrix at each gradient step. To this end, we sample a random vector ~r of dimension d whose components are 0 with probability dropout and 1 otherwise. We then multiply this vector component-wise with each row in the label embedding matrix Y . The dropout rate and the strength of the regularizer are two hyper-parameters of the method. The other hyperparameters are the learning rate for the stochastic gradient descent as well as the number of steps. Following Logan IV et al. (2021), we tune them using 4-fold cross-validation on the few-shot training set. Note that the only information to be stored for each tuned model are the d-dimensional label embeddings.\nKnowledge Distillation As mentioned, label tuning produces less accurate models than real finetuning. We find that this can be compensated by a form of knowledge distillation (Hinton et al., 2014). We first train a normal fine-tuned model and use that to produce label distributions for a set of unlabeled examples. Later, this silver set is used to train the new label embeddings for the untuned model. This increases the training cost of the approach and adds an additional requirement of unlabeled data\n1https://en.wikipedia.org/wiki/Matrix_ norm#Frobenius_norm\nbut keeps the advantages that at inference time we can share one model across multiple tasks."
    }, {
      "heading" : "3 Related Work",
      "text" : "Pre-trained Language Models (LMs) have been proved to encode knowledge that, with taskspecific guidance, can solve natural language understanding tasks (Petroni et al., 2019). Leveraging that, Le Scao and Rush (2021) quantified a reduction in the need of labeled data of hundreds of instances with respect to traditional fine-tuning approaches (Devlin et al., 2019; Liu et al., 2019). This has led to quality improvements in zero and few-shot learning.\nSemantic Similarity methods Gabrilovich and Markovitch (2007) and Chang et al. (2008) use the explicit meaning of the label names to compute the similarity with the input text. Recent advances in pre-trained LMs and their application to semantic textual similarity tasks, such as Sentence-BERT (Reimers and Gurevych, 2019), have shown a new opportunity to increase the quality of these methods and set the stage for this work. Chu et al. (2020) employ a technique called unsupervised label-refinement (LR). They incorporated a modified k-means clustering algorithm for refining the outputs of cross attention and Siamese Networks. We incorporate LR into our experiments and extend the analysis of their work. We evaluate it against more extensive and diverse benchmarks. In addition, we show that pre-training few-shot learners on their proposed textual similarity task NatCat underperforms pre-training on NLI datsets.\nPrompt-based methods GPT-3 (Brown et al., 2020), a 175 billion parameter LM, has been shown to give good quality on few-shot learning tasks. Pattern-Exploiting Training (PET) (Schick and Schütze, 2021) is a more computational and memory efficient alternative. It is based on ensembles of smaller masked language models (MLMs) and was found to give few-shot results similar to GPT-3. Logan IV et al. (2021) reduced the complexity of finding optimal templates in PET by using nullprompts and achieved competitive performance. They incorporated BitFit (Ben-Zaken et al., 2021) and thus reached comparable accuracy fine-tuning only 0.1% of the parameters of the LMs. Hambardzumyan et al. (2021) present a contemporary approach with a similar idea to label tuning. As in our work, they use label embeddings initialized\nas the verbalization of the label names. These taskspecific embeddings, along with additional ones that are inserted into the input sequence, are the only learnable parameters during model training. They optimize a cross entropy loss between the label embeddings and the output head of a MLM. The major difference is that they employ a promptbased approach while our method relies on embedding models.\nEntailment methods The entailment approach (Yin et al., 2019; Halder et al., 2020) uses the label description to reformulate text classification as textual entailment. The model predicts the entailment probability of every label description . Wang et al. (2021) report results outperforming LM-BFF (Gao et al., 2021), an approach similar to PET.\nTrue Few-Shot Learning Setting Perez et al. (2021) argue that for true few-shot learning, one should not tune parameters on large validation sets or use parameters or prompts that might have been tuned by others. We follow their recommendation and rely on default parameters and some hyperparameters and prompts recommended by Wang et al. (2021), which according to the authors, were not tuned on the few-shot datasets. For label tuning, we follow Logan IV et al. (2021) and tune parameters with cross-validation on the few-shot training set."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Random The theoretical performance of a random model that uniformly samples labels from the label set.\nWord embeddings For the English experiments, we use Word2Vec (Mikolov et al., 2013) embeddings2. For the multi-lingual experiments, we use FastText (Grave et al., 2018). In all cases we preprocess using the NLTK tokenizer (Bird et al., 2009) and stop-words list and by filtering non-alphabetic tokens. Sentence embeddings are computed by averaging the token embeddings.\nChar-SVM For the few-shot experiments we implemented a Support Vector Machines (SVM) (Hearst et al., 1998) based on character n-grams. The model was implemented with scikit-learn (Pedregosa et al., 2011) and uses bigrams to fivegrams.\n2https://code.google.com/archive/p/ word2vec\nCross Attention For our experiments we use pretrained models from HuggingFace (Wolf et al., 2020). As the cross attention baseline, we trained a version of MPNET (Song et al., 2020) on MultiGenre (MNLI, Williams et al. (2018)) and Stanford NLI (SNLI, Bowman et al. (2015)) using the parameters and code of Nie et al. (2020). This model has approx. 110M parameters. For the multilingual experiments, we trained – the cross-lingual language model – XLM roberta-base (Liu et al., 2019) on SNLI, MNLI, adversarial NLI (ANLI, Nie et al. (2020)) and cross-lingual NLI (XNLI, Conneau et al. (2018)), using the same code and parameters as above. The model has approx. 280M parameters. We give more details on the NLI datasets in Appendix G.\nSiamese Network We also use models based on MPNET for the experiments with the Siamese Networks. paraphrase-mpnet-base-v23 is a sentence transformer model (Reimers and Gurevych, 2019) trained on a variety of paraphrasing datasets as well as SNLI and MNLI using a batch softmax loss (Henderson et al., 2017). nli-mpnet-base-v24 is identical to the previous model but trained exclusively on MNLI and SNLI and thus comparable to the cross attention model. For the multilingual experiments, we trained a model using the code of the sentence transformers with the same batch softmax objective used for fine-tuning the few-shot models and on the same data we used for training the cross attention model.\nRoberta-NatCat For comparison with the related work, we also trained a model based on RoBERTa (Liu et al., 2019) and fine-tuned on the NatCat dataset as discussed in Chu et al. (2020) using the code5 and parameters of the authors.\nDatasets We use a number of English text classification datasets used in the zero-shot and the fewshot literature (Yin et al., 2019; Gao et al., 2021; Wang et al., 2021). In addition, we use several German and Spanish datasets for the multilingual experiments. Table 1 provides more details.\nThese datasets are of a number of common text classification tasks such as topic classification, sentiment and emotion detection, and review rating. However, we also included some less well-known tasks such as acceptability, whether an English sen-\n3https://tinyurl.com/pp-mpnet 4https://tinyurl.com/nli-mpnet 5https://github.com/ZeweiChu/ULR\ntence is deemed acceptable by a native speaker, and subjectivity, whether a statement is subjective or objective. As some datasets do not have a standard split we split them randomly using a 9/1 ratio.\nHypotheses We use the same hypotheses for the cross attention model and for the Siamese network. For Yahoo and Unified we use the hypotheses from Yin et al. (2019). For SUBJ, COLA, TREC, Yelp, AG News and IMDB we use the same hypotheses as Wang et al. (2021). For the remaining datasets we designed our own hypotheses. These were written in an attempt to mirror what has been done for other datasets and they have not been tuned in any way. Appendix B shows the patterns used. We also explored using a null hypothesis, that is the raw label names as the label representation and found this to give similar results.\nFine-Tuning Inspired by Wang et al. (2021), we investigate fine-tuning the models with 8, 64 and 512 examples per label. For fine-tuning the cross attention models we follow the literature (Wang et al., 2021) and create examples of every possible combination of input text and label. The example corresponding to the correct label is labeled as entailed while all other examples are labeled as refuted. We then fine-tune the model using stochastic gradient descent and a cross-entropy loss. We use a learning rate of 1e-5, a batch size of 8 and run the training for 10 epochs. As discussed in the methodology Section 2, for the Siamese Networks every batch contains exactly one example of every label and therefore the batch size equals the number of labels of the task. We use a learning\nrate of 2e-5 and of 2e-4 for the BitFit experiments. Appendix D contains additional information on the hyper-parameters used.\nWe use macro F1-score as the evaluation metric. We run all experiments with 5 different training sets and report the mean and standard deviation. For the zero-shot experiments, we estimate the standard deviation using bootstrapping (Koehn, 2004). In all cases, we use Welch’s t-test6 with a p-value of 0.05 to establish significance (following Logan IV et al. (2021)). For the experiments with label refinement (Chu et al., 2020) and distillation, we use up to 10,000 unlabeled examples from the training set."
    }, {
      "heading" : "5 Results",
      "text" : "Table 2 shows results comparing Siamese Networks (SN) with cross attention models (CA) and various baselines. As discussed above, SN and CA models are based on the MPNET architecture and trained on SNLI and MNLI.\nFor the zero-shot setup (n=0) we see that all models out-perform the random baseline on average. The word embedding baselines and RoBERTaNatCat perform significantly worse than random on several of the datasets. In contrast the SN and CA models only perform worse than random on COLA. The SN outperforms the CA on average, but the results for the individual datasets are mixed. The SN is significantly better for 4, significantly worse for 4 and on par for the remaining 3 datasets. Regarding the use of a hypothesis pattern from the literature or just a null hypothesis (NH), we find\n6https://en.wikipedia.org/wiki/Welch% 27s_t-test\nthat, while there are significant differences on individual datasets, the NH setup shows higher but still comparable (within 1 point) average performance.\nFor the few-shots setup (n={8, 64, 512}), we find that all models out-perform a Char-SVM trained with the same number of instances by a large margin. Comparing SN and CA, we see that CA outperforms the SN on average but with a difference with-in the confidence interval. For n=8 and n=64, CA significantly outperforms SN on 3 datasets and performs comparably on the remaining 8. For n=512, we see an even more mixed picture. CA is on par with SN on 6 datasets, outperforms it on 3 and is out-performed on 2. We can conclude that for the English datasets, SN is more accurate for zero-shot while CA is more accurate for fewshot. The average difference is small in both setups and we do not see a significant difference for most datasets.\nTable 3 shows the multi-lingual experiments. The RoBERTa XLM models were pre-trained on data from more than 100 languages and fine-tuned on an NLI data of 15 languages. The cross-lingual data and the fact that there is only 7500 examples for the languages other than English, explains why quality is lower than for the English-only experiments. For the zero-shot scenario, all models outperform the random baseline on average, but with a smaller margin than for the English-only models. The FastText baseline performs comparable to CA\non average (26.0 vs 27.2), while SN is ahead by a large margin (27.2 vs 32.4). The differences between models with hypotheses and null hypothesis (NH) are smaller than for the English experiments.\nLooking at the few-shot scenarios, we see that both models out-perform the Char-SVM by a large margin. In general, the results are closer than for the English experiments, as well as in the number of datasets with significant differences (only 2-4 of datasets). Similarly to English, we can conclude that at multilingual level, SN is more accurate in the zero-shot scenario whereas CA performs better in the few-shot one. However, for few-shot we see only small average differences (less than 1 point except for n=64).\nTable 4 shows a comparison of different finetuning approaches on the English datasets. Appendix H contains the multi-lingual results and gives a similar picture. We first compare Label Refinement (LR) as discussed in Chu et al. (2020) (see Section 3). Recall that this approach makes use of unlabeled data. We find that in the zero-shot scenario LR gives an average improvement of more than 2 points and significantly out-performing the baseline (mpnet) for 7 of the 11 datasets. When combining LR with labeled data as discussed in Chu et al. (2020) we find this to only give modest improvements over the zero-shot model (e.g., 54.0 (zero-shot) vs 55.8 (n=8)). Note that we apply LR to the untuned model, while Chu et al. (2020)\nproposed to apply it to a tuned model. However, we find that to only give small improvements over an already tuned model (mpnet (FT) vs. mpnet (FT+LR)). Also, in this work we are interested in approaches that do not change the initial model so that it can be shared between tasks to improve scalability. Label Tuning (LT) improves results as n grows and out-performs LR and the Char-SVM baseline from Table 2.\nComparing regular Fine-Tuning (FT) and BitFit, we find them to perform quite similarly both on average and on individual datasets, with only few exceptions, such as the performance difference on TREC for the n=8 setup. In comparison with FT and BitFit, LT is significantly out-performed on most datasets. The average difference in performance is around 5 points, which is comparable to using 8 times less training data.\nUsing the knowledge distillation approach discussed before (LT-DIST), we find that for 8 and 64 examples, most of the difference in performance can be recovered while still keeping the high scalability. For n=8, we only find a significant difference to mpnet (FT) for Yelp full."
    }, {
      "heading" : "6 Analysis",
      "text" : "We analyze the performance of the Cross Attention (CA) and Siamese Network-based (SN) models. Unless otherwise noted, the analysis was run over all datasets and languages. Table 5, gives a comparison of the processing speed of different models.\nDetails on the hardware used is given in Appendix F. As expected, the performance of the cross attention model halves when the label size doubles. The performance of the Siamese network is independent of the number of labels. This shows that Siamese Networks have a huge advantage at inference time – especially for tasks with many labels.\nTable 6 shows the average F1 scores for different token lengths. To this end the data was grouped in bins of roughly equal size. SN has an advantage for shorter sequences (≤ 44 tokens), while CA performs better for longer texts (> 160 tokens).\nTable 7 shows an analysis based on whether the text does or does not contain negation markers7. For emotion detection and review tasks, both models perform better on the subset without negations. However, while SN outperforms CA on the data without negations, CA performs better on the data with negations. The same trend does not hold for the sentiment datasets. These are based on Twitter and thus contain shorter and simpler sentences. For the sentiment datasets based on Twitter we also found that both models struggle to predict the neutral class. CA classifies almost everything neutral tweet as positive or negative. SN predicts the neutral class regularly but still with a relative high error rate. Appendix E contains further analysis showing that label set size, language and task do not have a visible effect on the difference in accuracy of the\n7We used an in-house list of 23 phrases for German and Spanish and 126 for English\ntwo models."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have shown that Cross Attention (CA) and Siamese Networks (SN) for zero-shot and few-shot text classification give comparable results across a diverse set of tasks and multiple languages. The inference cost of SNs is low as label embeddings can be pre-computed and, in contrast to CA, does not scale with the number of labels. We also showed that tuning only these label embeddings (Label Tuning (LT)) is an interesting alternative to regular Fine-Tuning (FT). LT gets close to FT performance when combined with knowledge distillation and when the number of training samples is low, i.e.,\nfor realistic few-shot learning. This is relevant for production scenarios, as it allows to share the same model among tasks. BitFit achieves better accuracy and also allows tuning relatively few parameters. However, it will require 60 times more memory to add a new task than for LT8. The main disadvantage of BitFit, however, is that the weight sharing it requires is much harder to implement, especially in highly optimized environments such as NVIDIA Triton. Therefore we think that LT is an interesting alternative for fast and scalable few-shot learning."
    }, {
      "heading" : "A Unified Emotions",
      "text" : "Unified Emotions is a meta-dataset comprised of the following datasets: DailyDialog (Li et al., 2017), CrowdFlower (Crowdflower, 2016), TEC (Mohammad, 2012), Tales (Alm et al., 2005; Alm and Sproat, 2005; Alm, 2008), ISEAR (Scherer and Wallbott, 1994), Emoint (Mohammad et al., 2017), ElectoralTweets (Mohammad et al., 2015), GroundedEmotions (Liu et al., 2007) and EmotionCause (Ghazi et al., 2015)."
    }, {
      "heading" : "B Hypotheses",
      "text" : "Table 9 lists all the hypothesis patterns used in our experiments.\nC Paraphrase datasets\nparaphrase-mpnet-base-v2 has been trained on these datasets: AllNLI, sentencecompression, SimpleWiki, altlex, msmarcotriplets, quora duplicates, coco captions, yahoo answers title question, S2ORC citation pairs, stackexchange duplicate questions and wikiatomic-edits. Details on these dataset are provided here."
    }, {
      "heading" : "D Hyperparameters",
      "text" : "For the label tuning experiments we used the following hyper-parameters:\n• learning rate ∈ {0.01, 0.1} • number of epochs ∈ {1000, 2000} • regularizer coefficient ∈ {0.01, 0.1} • dropout rate ∈ {0.01, 0.1}"
    }, {
      "heading" : "E Additional Analysis",
      "text" : "The following table shows the F1-score breakdown by hypothesis length. One could think that the CA model performs better for longer hypothesis but this cannot be observed. Potentially because all hypotheses are relatively short.\nFor completeness, we also add similar breakdowns by task type, label set size, and language.\nNone of them indicate an effect on the difference between SN and CA model performance."
    }, {
      "heading" : "F Computing Requirements",
      "text" : "All experiments were run on a system with an AMD Ryzen Threadripper 1950X CPU and a Nvidia GeForce GTX 1080 Ti GPU. Most of the computing time was spent training the NLI models used in our experiments. Training the CA models took approx. 20h while training the SN models took approx. 10h.\nG NLI Training sets"
    }, {
      "heading" : "H Multilingual Label Tuning Results",
      "text" : "Table 8 multilingual results for label tuninig and fine-tuning."
    } ],
    "references" : [ {
      "title" : "Emotions from text: machine learning for text-based emotion prediction",
      "author" : [ "Cecilia Ovesdotter Alm", "Dan Roth", "Richard Sproat." ],
      "venue" : "Proceedings of the conference on human language technology and empirical methods in natural language processing,",
      "citeRegEx" : "Alm et al\\.,? 2005",
      "shortCiteRegEx" : "Alm et al\\.",
      "year" : 2005
    }, {
      "title" : "Perceptions of emotions in expressive storytelling",
      "author" : [ "Cecilia Ovesdotter Alm", "Richard Sproat." ],
      "venue" : "Ninth European Conference on Speech Communication and Technology.",
      "citeRegEx" : "Alm and Sproat.,? 2005",
      "shortCiteRegEx" : "Alm and Sproat.",
      "year" : 2005
    }, {
      "title" : "Affect in* text and speech",
      "author" : [ "Ebba Cecilia Ovesdotter Alm." ],
      "venue" : "University of Illinois at Urbana-Champaign.",
      "citeRegEx" : "Alm.,? 2008",
      "shortCiteRegEx" : "Alm.",
      "year" : 2008
    }, {
      "title" : "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels",
      "author" : [ "Elad Ben-Zaken", "Shauli Ravfogel", "Yoav Goldberg." ],
      "venue" : "ArXiv, abs/2106.10199.",
      "citeRegEx" : "Ben.Zaken et al\\.,? 2021",
      "shortCiteRegEx" : "Ben.Zaken et al\\.",
      "year" : 2021
    }, {
      "title" : "Natural Language Processing with Python, 1st edition",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "O’Reilly Media, Inc.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Ten thousand german news articles dataset",
      "author" : [ "Timo Block." ],
      "venue" : "https://tblock.github.io/ 10kGNAD/. Accessed: 2021-08-25.",
      "citeRegEx" : "Block.,? 2019",
      "shortCiteRegEx" : "Block.",
      "year" : 2019
    }, {
      "title" : "An analysis of annotated corpora for emotion classification in text",
      "author" : [ "Laura Ana Maria Bostan", "Roman Klinger." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 2104–2119. Association for Computational",
      "citeRegEx" : "Bostan and Klinger.,? 2018",
      "shortCiteRegEx" : "Bostan and Klinger.",
      "year" : 2018
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Signature verification using a ”siamese” time delay neural network",
      "author" : [ "Jane Bromley", "Isabelle Guyon", "Yann LeCun", "Eduard Säckinger", "Roopak Shah." ],
      "venue" : "Proceedings of the 6th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Bromley et al\\.,? 1993",
      "shortCiteRegEx" : "Bromley et al\\.",
      "year" : 1993
    }, {
      "title" : "Importance of semantic representation: Dataless classification",
      "author" : [ "Ming-Wei Chang", "Lev-Arie Ratinov", "D. Roth", "Vivek Srikumar." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Chang et al\\.,? 2008",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2008
    }, {
      "title" : "Unsupervised label refinement improves dataless text classification",
      "author" : [ "Zewei Chu", "Karl Stratos", "Kevin Gimpel." ],
      "venue" : "CoRR, abs/2012.04194.",
      "citeRegEx" : "Chu et al\\.,? 2020",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2020
    }, {
      "title" : "A Twitter corpus and benchmark resources for German sentiment analysis",
      "author" : [ "Mark Cieliebak", "Jan Milan Deriu", "Dominic Egger", "Fatih Uzdilli." ],
      "venue" : "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media,",
      "citeRegEx" : "Cieliebak et al\\.,? 2017",
      "shortCiteRegEx" : "Cieliebak et al\\.",
      "year" : 2017
    }, {
      "title" : "Xnli: Evaluating crosslingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel R. Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Computing semantic relatedness using wikipediabased explicit semantic analysis",
      "author" : [ "Evgeniy Gabrilovich", "Shaul Markovitch." ],
      "venue" : "Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07, page 1606–1611, San Fran-",
      "citeRegEx" : "Gabrilovich and Markovitch.,? 2007",
      "shortCiteRegEx" : "Gabrilovich and Markovitch.",
      "year" : 2007
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Detecting emotion stimuli in emotion-bearing sentences",
      "author" : [ "Diman Ghazi", "Diana Inkpen", "Stan Szpakowicz." ],
      "venue" : "CICLing (2), pages 152–165.",
      "citeRegEx" : "Ghazi et al\\.,? 2015",
      "shortCiteRegEx" : "Ghazi et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning word vectors for 157 languages",
      "author" : [ "Edouard Grave", "Piotr Bojanowski", "Prakhar Gupta", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).",
      "citeRegEx" : "Grave et al\\.,? 2018",
      "shortCiteRegEx" : "Grave et al\\.",
      "year" : 2018
    }, {
      "title" : "AG’s corpus of news articles",
      "author" : [ "Antonio Gulli." ],
      "venue" : "http://groups.di.unipi.it/ ̃gulli/ AG_corpus_of_news_articles.html. Accessed: 2021-07-08.",
      "citeRegEx" : "Gulli.,? 2005",
      "shortCiteRegEx" : "Gulli.",
      "year" : 2005
    }, {
      "title" : "Task-aware representation of sentences for generic text classification",
      "author" : [ "Kishaloy Halder", "Alan Akbik", "Josip Krapac", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3202–3213,",
      "citeRegEx" : "Halder et al\\.,? 2020",
      "shortCiteRegEx" : "Halder et al\\.",
      "year" : 2020
    }, {
      "title" : "WARP: Word-level Adversarial ReProgramming",
      "author" : [ "Karen Hambardzumyan", "Hrant Khachatrian", "Jonathan May." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
      "citeRegEx" : "Hambardzumyan et al\\.,? 2021",
      "shortCiteRegEx" : "Hambardzumyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Support vector machines",
      "author" : [ "Marti A. Hearst", "Susan T Dumais", "Edgar Osuna", "John Platt", "Bernhard Scholkopf." ],
      "venue" : "IEEE Intelligent Systems and their applications, 13(4):18–28.",
      "citeRegEx" : "Hearst et al\\.,? 1998",
      "shortCiteRegEx" : "Hearst et al\\.",
      "year" : 1998
    }, {
      "title" : "Efficient natural language response suggestion for smart reply",
      "author" : [ "Matthew L. Henderson", "Rami Al-Rfou", "Brian Strope", "Yun-Hsuan Sung", "László Lukács", "Ruiqi Guo", "Sanjiv Kumar", "Balint Miklos", "Ray Kurzweil." ],
      "venue" : "CoRR, abs/1705.00652.",
      "citeRegEx" : "Henderson et al\\.,? 2017",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2017
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "J. Dean." ],
      "venue" : "The NIPS 2014 Learning Semantics Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2014",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2014
    }, {
      "title" : "The multilingual amazon reviews corpus",
      "author" : [ "Phillip Keung", "Yichao Lu", "György Szarvas", "Noah A. Smith." ],
      "venue" : "CoRR, abs/2010.02573.",
      "citeRegEx" : "Keung et al\\.,? 2020",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2020
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computa-",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Zero-data learning of new tasks",
      "author" : [ "Hugo Larochelle", "Dumitru Erhan", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2, AAAI’08, page 646–651. AAAI Press.",
      "citeRegEx" : "Larochelle et al\\.,? 2008",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2008
    }, {
      "title" : "How many data points is a prompt worth",
      "author" : [ "Teven Le Scao", "Alexander Rush" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Scao and Rush.,? \\Q2021\\E",
      "shortCiteRegEx" : "Scao and Rush.",
      "year" : 2021
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Xin Li", "Dan Roth." ],
      "venue" : "COLING 2002: The 19th International Conference on Computational Linguistics.",
      "citeRegEx" : "Li and Roth.,? 2002",
      "shortCiteRegEx" : "Li and Roth.",
      "year" : 2002
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "arXiv preprint arXiv:1710.03957.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Grounded emotions",
      "author" : [ "V. Liu", "C. Banea", "R. Mihalcea." ],
      "venue" : "International Conference on Affective Computing and Intelligent Interaction (ACII 2017), San Antonio, Texas.",
      "citeRegEx" : "Liu et al\\.,? 2007",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2007
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Cutting down on prompts and parameters: Simple few-shot learning with language models",
      "author" : [ "Robert L. Logan IV", "Ivana Balazevic", "Eric Wallace", "Fabio Petroni", "Sameer Singh", "Sebastian Riedel." ],
      "venue" : "CoRR, abs/2106.13353.",
      "citeRegEx" : "IV et al\\.,? 2021",
      "shortCiteRegEx" : "IV et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomás Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Emotional Tweets",
      "author" : [ "Saif Mohammad." ],
      "venue" : "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Work-",
      "citeRegEx" : "Mohammad.,? 2012",
      "shortCiteRegEx" : "Mohammad.",
      "year" : 2012
    }, {
      "title" : "Stance and sentiment in tweets",
      "author" : [ "Saif M Mohammad", "Parinaz Sobhani", "Svetlana Kiritchenko." ],
      "venue" : "ACM Transactions on Internet Technology (TOIT), 17(3):1–23.",
      "citeRegEx" : "Mohammad et al\\.,? 2017",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2017
    }, {
      "title" : "Sentiment, emotion, purpose, and style in electoral tweets",
      "author" : [ "Saif M Mohammad", "Xiaodan Zhu", "Svetlana Kiritchenko", "Joel Martin." ],
      "venue" : "Information Processing & Management, 51(4):480–499.",
      "citeRegEx" : "Mohammad et al\\.,? 2015",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval2016 task 4: Sentiment analysis in Twitter",
      "author" : [ "Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanov", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval ’16, San Diego, Cali-",
      "citeRegEx" : "Nakov et al\\.,? 2016",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2016
    }, {
      "title" : "Spanish corpus for sentiment analysis towards brands",
      "author" : [ "Marı́a Navas-Loro", "Vı́ctor Rodrı́guez-Doncel", "Idafen Santana-Perez", "Alberto Sánchez" ],
      "venue" : "In Speech and Computer,",
      "citeRegEx" : "Navas.Loro et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Navas.Loro et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271–",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "True few-shot learning with language models",
      "author" : [ "Ethan Perez", "Douwe Kiela", "Kyunghyun Cho." ],
      "venue" : "CoRR, abs/2105.11447.",
      "citeRegEx" : "Perez et al\\.,? 2021",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Evidence for universality and cultural variation of differential emotion response patterning",
      "author" : [ "K. Scherer", "H.G. Wallbott." ],
      "venue" : "Journal of personality and social psychology, 66 2:310–28.",
      "citeRegEx" : "Scherer and Wallbott.,? 1994",
      "shortCiteRegEx" : "Scherer and Wallbott.",
      "year" : 1994
    }, {
      "title" : "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
      "citeRegEx" : "Schick and Schütze.,? 2021",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "Mpnet: Masked and permuted pretraining for language understanding",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Advances in",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Crowdsourcing and validating event-focused emotion corpora for German and English",
      "author" : [ "Enrica Troiano", "Sebastian Padó", "Roman Klinger." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4005–",
      "citeRegEx" : "Troiano et al\\.,? 2019",
      "shortCiteRegEx" : "Troiano et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "HEAD-QA: A healthcare dataset for complex reasoning",
      "author" : [ "David Vilares", "Carlos Gómez-Rodrı́guez" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Vilares and Gómez.Rodrı́guez.,? \\Q2019\\E",
      "shortCiteRegEx" : "Vilares and Gómez.Rodrı́guez.",
      "year" : 2019
    }, {
      "title" : "Entailment as few-shot learner",
      "author" : [ "Sinong Wang", "Han Fang", "Madian Khabsa", "Hanzi Mao", "Hao Ma." ],
      "venue" : "ArXiv, abs/2104.14690.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach",
      "author" : [ "Wenpeng Yin", "Jamaal Hay", "Dan Roth." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Yin et al\\.,? 2019",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal natural language processing with limited annotations: Try few-shot textual entailment as a start",
      "author" : [ "Wenpeng Yin", "Nazneen Fatema Rajani", "Dragomir Radev", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc. 12",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : ", 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "One approach (Yin et al., 2019, 2020; Halder et al., 2020; Wang et al., 2021) makes use of entailment models.",
      "startOffset" : 13,
      "endOffset" : 77
    }, {
      "referenceID" : 53,
      "context" : "One approach (Yin et al., 2019, 2020; Halder et al., 2020; Wang et al., 2021) makes use of entailment models.",
      "startOffset" : 13,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : ", 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993),",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : "LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 51,
      "context" : "The encoder consists of a transformer (Vaswani et al., 2017) that produces contextual token embeddings and a mean pooler that combines the token embeddings into a single text embedding.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : "As the objective we use the so called batch softmax (Henderson et al., 2017):",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : "We find that this can be compensated by a form of knowledge distillation (Hinton et al., 2014).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 45,
      "context" : "Pre-trained Language Models (LMs) have been proved to encode knowledge that, with taskspecific guidance, can solve natural language understanding tasks (Petroni et al., 2019).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 14,
      "context" : "Leveraging that, Le Scao and Rush (2021) quantified a reduction in the need of labeled data of hundreds of instances with respect to traditional fine-tuning approaches (Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 168,
      "endOffset" : 207
    }, {
      "referenceID" : 32,
      "context" : "Leveraging that, Le Scao and Rush (2021) quantified a reduction in the need of labeled data of hundreds of instances with respect to traditional fine-tuning approaches (Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 168,
      "endOffset" : 207
    }, {
      "referenceID" : 46,
      "context" : "Recent advances in pre-trained LMs and their application to semantic textual similarity tasks, such as Sentence-BERT (Reimers and Gurevych, 2019), have shown a new opportunity to increase the quality of these methods and set the stage for this work.",
      "startOffset" : 117,
      "endOffset" : 145
    }, {
      "referenceID" : 48,
      "context" : "Pattern-Exploiting Training (PET) (Schick and Schütze, 2021) is a more computational and memory efficient alternative.",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "They incorporated BitFit (Ben-Zaken et al., 2021) and thus reached comparable accuracy fine-tuning only 0.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 57,
      "context" : "Entailment methods The entailment approach (Yin et al., 2019; Halder et al., 2020) uses the label description to reformulate text classification as textual entailment.",
      "startOffset" : 43,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "Entailment methods The entailment approach (Yin et al., 2019; Halder et al., 2020) uses the label description to reformulate text classification as textual entailment.",
      "startOffset" : 43,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "(2021) report results outperforming LM-BFF (Gao et al., 2021), an approach similar to PET.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 35,
      "context" : "Word embeddings For the English experiments, we use Word2Vec (Mikolov et al., 2013) embeddings2.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "For the multi-lingual experiments, we use FastText (Grave et al., 2018).",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "In all cases we preprocess using the NLTK tokenizer (Bird et al., 2009) and stop-words list and by filtering non-alphabetic tokens.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "Char-SVM For the few-shot experiments we implemented a Support Vector Machines (SVM) (Hearst et al., 1998) based on character n-grams.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 43,
      "context" : "The model was implemented with scikit-learn (Pedregosa et al., 2011) and uses bigrams to fivegrams.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 49,
      "context" : "As the cross attention baseline, we trained a version of MPNET (Song et al., 2020) on MultiGenre (MNLI, Williams et al.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 32,
      "context" : "For the multilingual experiments, we trained – the cross-lingual language model – XLM roberta-base (Liu et al., 2019) on SNLI, MNLI, adversarial NLI (ANLI, Nie et al.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 46,
      "context" : "paraphrase-mpnet-base-v23 is a sentence transformer model (Reimers and Gurevych, 2019) trained on a variety of paraphrasing datasets as well as SNLI and MNLI using a batch softmax loss (Henderson et al.",
      "startOffset" : 58,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "paraphrase-mpnet-base-v23 is a sentence transformer model (Reimers and Gurevych, 2019) trained on a variety of paraphrasing datasets as well as SNLI and MNLI using a batch softmax loss (Henderson et al., 2017).",
      "startOffset" : 185,
      "endOffset" : 209
    }, {
      "referenceID" : 32,
      "context" : "Roberta-NatCat For comparison with the related work, we also trained a model based on RoBERTa (Liu et al., 2019) and fine-tuned on the NatCat dataset as discussed in Chu et al.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 57,
      "context" : "Datasets We use a number of English text classification datasets used in the zero-shot and the fewshot literature (Yin et al., 2019; Gao et al., 2021; Wang et al., 2021).",
      "startOffset" : 114,
      "endOffset" : 169
    }, {
      "referenceID" : 16,
      "context" : "Datasets We use a number of English text classification datasets used in the zero-shot and the fewshot literature (Yin et al., 2019; Gao et al., 2021; Wang et al., 2021).",
      "startOffset" : 114,
      "endOffset" : 169
    }, {
      "referenceID" : 53,
      "context" : "Datasets We use a number of English text classification datasets used in the zero-shot and the fewshot literature (Yin et al., 2019; Gao et al., 2021; Wang et al., 2021).",
      "startOffset" : 114,
      "endOffset" : 169
    }, {
      "referenceID" : 5,
      "context" : "GNAD (Block, 2019) topic de 9,245 1,028 9 279 AG News (Gulli, 2005) en 120,000 7,600 4 37 HeadQA (Vilares and Gómez-Rodrı́guez, 2019) es 4,023 2,742 6 15 Yahoo (Zhang et al.",
      "startOffset" : 5,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "GNAD (Block, 2019) topic de 9,245 1,028 9 279 AG News (Gulli, 2005) en 120,000 7,600 4 37 HeadQA (Vilares and Gómez-Rodrı́guez, 2019) es 4,023 2,742 6 15 Yahoo (Zhang et al.",
      "startOffset" : 54,
      "endOffset" : 67
    }, {
      "referenceID" : 52,
      "context" : "GNAD (Block, 2019) topic de 9,245 1,028 9 279 AG News (Gulli, 2005) en 120,000 7,600 4 37 HeadQA (Vilares and Gómez-Rodrı́guez, 2019) es 4,023 2,742 6 15 Yahoo (Zhang et al.",
      "startOffset" : 97,
      "endOffset" : 133
    }, {
      "referenceID" : 59,
      "context" : "GNAD (Block, 2019) topic de 9,245 1,028 9 279 AG News (Gulli, 2005) en 120,000 7,600 4 37 HeadQA (Vilares and Gómez-Rodrı́guez, 2019) es 4,023 2,742 6 15 Yahoo (Zhang et al., 2015) en 1,360,000 100,000 10 71",
      "startOffset" : 160,
      "endOffset" : 180
    }, {
      "referenceID" : 25,
      "context" : "Amazon Reviews (Keung et al., 2020) reviews de, en, es 205,000 5,000 5 25-29 IMDB (Maas et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 34,
      "context" : ", 2020) reviews de, en, es 205,000 5,000 5 25-29 IMDB (Maas et al., 2011) en 25,000 25,000 2 173 Yelp full (Zhang et al.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 59,
      "context" : ", 2011) en 25,000 25,000 2 173 Yelp full (Zhang et al., 2015) en 650,000 50,000 5 99 Yelp polarity (Zhang et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 59,
      "context" : ", 2015) en 650,000 50,000 5 99 Yelp polarity (Zhang et al., 2015) en 560,000 38,000 2 97",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 40,
      "context" : "SAB (Navas-Loro et al., 2017) sentiment es 3,979 459 3 13 SemEval (Nakov et al.",
      "startOffset" : 4,
      "endOffset" : 29
    }, {
      "referenceID" : 39,
      "context" : ", 2017) sentiment es 3,979 459 3 13 SemEval (Nakov et al., 2016) en 9,834 20,632 3 20 sb10k (Cieliebak et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : ", 2016) en 9,834 20,632 3 20 sb10k (Cieliebak et al., 2017) de 8,955 994 3 11",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Unified (Bostan and Klinger, 2018) emotions en 42,145 15,689 10 15 deISEAR (Troiano et al.",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 50,
      "context" : "Unified (Bostan and Klinger, 2018) emotions en 42,145 15,689 10 15 deISEAR (Troiano et al., 2019) de 643 340 7 9",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 54,
      "context" : "COLA (Warstadt et al., 2019) acceptability en 8,551 1,043 2 7 SUBJ (Pang and Lee, 2004) subjectivity en 8,019 1,981 2 22 TREC (Li and Roth, 2002) entity type en 5,452 500 6 10",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 42,
      "context" : ", 2019) acceptability en 8,551 1,043 2 7 SUBJ (Pang and Lee, 2004) subjectivity en 8,019 1,981 2 22 TREC (Li and Roth, 2002) entity type en 5,452 500 6 10",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : ", 2019) acceptability en 8,551 1,043 2 7 SUBJ (Pang and Lee, 2004) subjectivity en 8,019 1,981 2 22 TREC (Li and Roth, 2002) entity type en 5,452 500 6 10",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 53,
      "context" : "For fine-tuning the cross attention models we follow the literature (Wang et al., 2021) and create examples of every possible combination of input text and label.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 26,
      "context" : "For the zero-shot experiments, we estimate the standard deviation using bootstrapping (Koehn, 2004).",
      "startOffset" : 86,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "For the experiments with label refinement (Chu et al., 2020) and distillation, we use up to 10,000 unlabeled examples from the training set.",
      "startOffset" : 42,
      "endOffset" : 60
    } ],
    "year" : 0,
    "abstractText" : "We study the problem of building text classifiers with little or no training data, commonly known as zero and few-shot text classification. In recent years, an approach based on neural textual entailment models has been found to give strong results on a diverse range of tasks. In this work, we show that with proper pre-training, Siamese Networks that embed texts and labels offer a competitive alternative. These models allow for a large reduction in inference cost: constant in the number of labels rather than linear. Furthermore, we introduce label tuning, a simple and computationally efficient approach that allows to adapt the models in a few-shot setup by only changing the label embeddings. While giving lower performance than model fine-tuning, this approach has the architectural advantage that a single encoder can be shared by many different tasks.",
    "creator" : null
  }
}
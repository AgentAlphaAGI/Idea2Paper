{
  "name" : "ARR_2022_87_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Large pre-trained language models (LMs) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020) have become the backbone of natural language processing in recent years. However, recent work has shown that they struggle in performing symbolic reasoning operations, such as composition or conjunction of facts (Talmor et al., 2019, 2020), numerical operations (Wallace et al., 2019; Hidey et al., 2020), and quantification (Warstadt et al., 2019), without substantial amounts of additional data.\nPast work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019; Gupta et al., 2020a; Khot et al., 2021; Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al., 2019; Zhao et al., 2019; Andreas, 2020; Asai and Hajishirzi, 2020; Campagna et al., 2020), and\nquestion generation models (Alberti et al., 2019; Puri et al., 2020; Bartolo et al., 2021).\nIn this work, we take the latter approach and argue that semi-structured tables are a valuable resource for automatic generation of training data that can endow LMs with reasoning skills. Tables can be crawled from the web at scale, and cover a wide range of domains and topics. Moreover, their structured nature makes them amenable to automatic processes of data generation. Specifically, given a table, we use templates to generate reading comprehension (RC) examples, that is, questioncontext-answer triplets, where answering the question requires diverse types of reasoning over facts mentioned in the context. Fig. 1 shows an example table, and three generated question-context-answer examples, which require fact composition, number comparison, and computing a date difference. Unlike prior work where semi-structured data was used for reasoning over tables or knowledge-bases\n(Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al., 2020; Yu et al., 2021), here we harness tables to allow LMs to reason over text directly.\nFig. 2 provides an overview of our approach. We generate data by crawling tables from Wikipedia, and applying 16 different example generators (EGs) on each table. Each EG corresponds to a particular reasoning skill (composition, numerical comparison, see Table 1 for full list), and comprises a small set of question templates. Variables in the templates are filled with content from the table, and the structure of the table allows to compute the answer automatically. The context is a list of facts generated from the table that contain facts required for answering the question as well as distractor facts.\nWe add a pre-training step over this generated data, where we perform multi-task training over the 16 task corresponding to the EGs. Since each EG can generate vast numbers of examples, it is important to focus training on reasoning skills that the model lacks. Thus, we use error-driven sampling (Gottumukkala et al., 2020) to construct training batches, where most examples are sampled from EGs that the model currently struggles with.\nWe fine tune our Pre-traind for Reasoning Model, PReasM, on three RC datasets that require reasoning: DROP (Dua et al., 2019), IIRC (Ferguson et al., 2020), and MMQA (Talmor et al., 2021). PReasM outperforms the original pre-trained T5 (Raffel et al., 2020) model by significant margins: 7.6, 4.1, and 1.2 F1 points, respectively. Our results set a new state-of-the-art on MMQA and are the best results on IIRC for models where the retriever and reader are trained separately. Our analysis shows that PReasM leads to improvements of up to 40 F1 points on specific question types, such as computing the difference between two dates, without causing a drop in other question types.\nIn conclusion, our results suggest that tables are a viable and untapped source of information for automatically generating large amounts of data that can be used to endow LMs with skills that are not captured using current pre-training approaches.1"
    }, {
      "heading" : "2 Data Generation",
      "text" : "Our goal is to train a RC model that given a question q and textual context c returns an answer a, given a training set D = {(qi, ci, ai)}Ni=1. We focus on questions that require reasoning over the context, e.g., composing two facts. To endow LMs with reasoning skills, we want to generate a large synthetic training set Dsyn = {(qj , cj , aj)}Mj=1 (M ≫ N ) from semi-structured tables, before finetuning on a target dataset."
    }, {
      "heading" : "2.1 Generating Examples from Tables",
      "text" : "We use tables from English Wikipedia2 to generate Dsyn. English Wikipedia includes millions of tables with high lexical and domain diversity (Fetahu et al., 2019; Chen et al., 2020b; Gupta et al., 2020b; Talmor et al., 2021; Nan et al., 2021; Neeraja et al., 2021a). We extract from Wikipedia all tables T that have at least two columns and 10-25 rows, resulting in more than 700K tables. Then, we annotate all table columns with their semantic type (STRING, NUMBER, or DATE), which allows us to generate questions that involve manipulating numbers and dates. Details on the process of column annotation are in §A.1.\nThe core of the generation process are the example generators (EGs), each corresponding to a reasoning skill (Table 1). Each example generator g ∈ G is a function that takes a table t ∈ T\n1Code, data, and models will be made publicly available. 2We use the 01-01-2020 Wikipedia dump.\nand randomly samples at most ten (q, c, a) triplets from the set of all possible triplets, where (i) q is a question is pseudo-language, (ii) c is the context, i.e., a list of facts extracted from t that includes the gold facts necessary for answering q and distractor facts, all phrased in pseudo-language, and (iii) a is the answer. Overall, the synthetic training set is Dsyn = ⋃ t∈T ⋃ g∈G g(t).\nEGs generate examples in the following way. Each EG is associated with one or more question templates, which differ in their surface phrasing.3 Templates contain typed variables that are instantiated with content from the table (see all variables in Table 1). Column and value variables are indexed to specify that the variable val:i must be instantiated by a value from the column col:i. Instantiating all variables results in the question q and the template allows us to programmatically compute the answer a. E.g., in the question from Fig. 1: “In League Cup of 1990–91 Chelsea F.C. season, Which Round had a higher Attendance: QF\n3We also experimented with using just one question template per EG and observed very similar downstream results.\nor QFR?” the answer a can be found by finding the rows with the values “QF” and “QFR” in the column “Round”, and returning the value that has a higher number in the column “Attendance”.\nThe context c is generated from the content necessary for answering the question, which can be identified using the instantiated question template. Facts generally have the form “The col:1 when the col:2 was val:2 was val:1”. E.g., to answer the question above, we generate the gold facts “The Attendance when the Round was QF was 34,178”, and “The Attendance when the Round was QFR was 33,861”. We also generate distractors by generating facts from rows or columns that are not relevant for the question, e.g., “The Attendance when the Round was R4 was 9,789”. Overall, our process results in a large set Dsyn, which includes examples from 16 EGs (all shown in Table 1)."
    }, {
      "heading" : "2.2 Data Analysis",
      "text" : "Data generation yields 4.8M questions from over 176K tables and 130K pages. The number of distinct words is large (850K), illustrating the wide\ncoverage and high lexical diversity of our approach. Moreover, generated examples have diverse answer types, which include text spans (43.2%), yes/no questions (31.6%), numeric (15.8%), and date answers (9.4%). In addition, our questions cover a wide range of domains including popular culture, politics and science. Tables cover more than 2,500 different Wikipedia categories, with 150 covering 80% of the data. We show the most frequent categories and more examples in §A.1."
    }, {
      "heading" : "3 Training",
      "text" : "Since our EGs generate large quantities of examples, one can think of each EG as providing an infinite stream of examples. In this setup, a natural question is how to construct training batches such that the model learns the required skills as quickly as possible. After briefly describing our model, we will detail our training framework, where we sample examples from EGs in an error-driven manner.\nModel We use a standard encoder-decoder architecture (Raffel et al., 2020; Lewis et al., 2020). Given a training example (q, c, a), the model takes as input the sequence of tokens ‘q [SEP] c’, and the task is to autoregressively decode the answer a token-by-token. We train to maximize the maximum likelihood objective logP (a | q, c)."
    }, {
      "heading" : "3.1 Multi-task Training over EGs",
      "text" : "Given a pre-trained LM, we add another pretraining step, where we multi-task over a set of tasks S , each task corresponding to examples generated from one EG. Similar to past work (Yogatama et al., 2019; Geva et al., 2020), to avoid “catastrophic forgetting” (Kirkpatrick et al., 2016) of the language skills, we sample batches from the original pre-training task with probability λ = 0.5.\nPast work (Gottumukkala et al., 2020) has shown that heterogeneous batching, i.e., having examples from all tasks in each batch, leads to better performance compared to having entire batches from a single task. We follow this practice, and in every batch sample examples from every task according to a probability distribution Ptasks ∈ R\n|S|. The main question is how to determine the distribution Ptasks, which we turn to next."
    }, {
      "heading" : "3.2 Sampling Strategies",
      "text" : "We describe strategies for computing Ptasks, starting with the commonly-used uniform sampling approach, and then turn to error-driven approaches.\nUniform sampling Past work (Khashabi et al., 2020; Raffel et al., 2020; Wang et al., 2020) used uniform sampling, where the probability to sample from a task s is Ptasks(s) = 1 |S| , as a-priori all tasks are equally important. Some approaches also sample examples in proportion to the size of the training set (Raffel et al., 2020; Wang et al., 2020). This is not applicable in our case, where we assume an infinite stream of examples for every task, and make no assumptions on the distribution over reasoning skills in the downstream test set.\nError sampling Recent work (Sharma et al., 2018; Gottumukkala et al., 2020) proposed to construct Ptasks based on model errors, where one over-samples tasks with higher errors. More formally, let Ceil(s) be an estimate of the maximum accuracy achievable on a task s, and Acc(s) be the current model accuracy for task s on an heldout set. We define ∆(s) = Ceil(s) − Acc(s) and Ptasks(s) = ∆(s)∑ s′∈S ∆(s\n′) . The distribution Ptasks of a task is updated every time we evaluate the current model on the held-out data. In our setup, since the data is synthetic and abundant, we assume that the ceiling accuracy for all tasks is 1.0, and hence: ∆(s) = 1.0 − Acc(s). Similar to Gottumukkala et al. (2020), we use accuracy over a held-out set rather than the training loss, as this corresponds directly to our target metric.\nMomentum sampling A potential issue with error sampling, is that if the error rate on a task is high, the model will spend most of its time on that task at the expense of other tasks, which may lead to low data efficiency. To remedy this, we introduce momentum sampling, a sampling strategy that samples from a task in proportion to its rate of improvement, putting most probability mass on skills that are improving rapidly.\nAlg. 1 provides the details of momentum sampling. Let t denote the index of a checkpoint evaluated on the held-out set, let w be a window size, and Accs(i) be the held-out accuracy of checkpoint i on task s. We estimate model accuracy on a task s at the beginning and end of the window, and sample examples in proportion to the difference4 in accuracy during that window. To smooth out accuracy fluctuations in adjacent checkpoints, we estimate accuracy as an average of k model checkpoints. During the first w checkpoint evaluations,\n4We use the difference in performance and not the gain to account for cases of sudden drops in performance.\nAlgorithm 1 Momentum Sampling(w, t, ϵ, k) Input: windows size w, training time t, minimum share of examples per task ϵ, smoothing factor k. 1: for s ∈ S do 2: if t ≥ w then 3: Acchead← 1 k ∑t i=t−k Accs(i)\n4: Acctail← 1 k ∑t−w+k i=t−w Accs(i) 5: Ptasks[s]← max(|Acchead − Acctail|, ϵ) 6: else 7: Ptasks[s]← 1/|S| 8: Ptasks ← Ptasks/∥Ptasks∥1\nwe simply use uniform sampling. Momentum sampling has several theoretical benefits over error sampling. First, it does not assume anything on the ceiling accuracy of a task. Second, when all tasks converge to their ceiling accuracy, momentum sampling converges to uniform sampling, unlike error sampling, which will oversample from tasks for which Ceil(s) is low. This is useful in cases where the variance of Ceil(s) is high across tasks. On the other hand, momentum sampling requires a warm-up of w steps, and might under-sample from tasks that train slowly. In §A.2., we describe two synthetic experiments where momentum sampling clearly outperforms error sampling. However, we do not observe an advantage for momentum sampling in our experiments in §5, and leave further investigation of momentum sampling to future work."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Models",
      "text" : "Baselines Our main baseline is T5 (Raffel et al., 2020), a popular pre-trained encoder-decoder model, which we fine-tune on the downstream datasets. We experiment with Base and Large size models. For each dataset, we compare to the relevant state-of-the-art model.\nOur pre-trained for reasoning model, PReasM, is a T5 model with another pre-training step on Dsyn. We experiment with uniform sampling (PReasMUni), error sampling (PReasM-Err), and momentum sampling (PReasM-Moment) strategies."
    }, {
      "heading" : "4.2 Datasets",
      "text" : "DROP (Dua et al., 2019) is a RC dataset with questions that require numeric reasoning. As an additional baseline, we also compare to GenBERT (Geva et al., 2020), which similar to our approach injects numerical skills by automatically generating synthetic data from a grammar.\nIIRC (Ferguson et al., 2020) is a QA dataset, where annotators were given a single Wikipedia paragraph, and were asked to author questions that depend on that paragraph, but also on other paragraphs linked from the input paragraph. This resulted in questions that require discrete temporal (28%) or numeric (11%) reasoning. In addition, 30% of the questions are unanswerable.\nWe experiment with IIRC in two settings: (a) Oracle, where the model is given the gold context, reducing the problem to RC, where we can apply our models. (b) Retrieval, where we use the “improved pipeline”introduced by Ni et al. (2021) to retrieve the context, and replace the NumNet+ (Base) reader (Ran et al., 2019) used by the authors (which has specialized architecture) with T5/PReasM.\nMMQA (Talmor et al., 2021) is a QA dataset, where the input is a question and a context that consists of a table, multiple text paragraphs, and multiple images, and the model must reason over a subset of the input modalities to answer the question.5 We chose to use MMQA as it has many questions that involve a conjunction of facts, an operation that is largely missing from other datasets. Moreover, a large fractions of the questions can be answered by reasoning over the text and table only.\nSince T5/PReasM cannot handle images or very long contexts, we construct a pipeline that automatically directs some MMQA questions to T5/PReasM, and uses the original Implicit-Decomp baseline from Talmor et al. (2021) elsewhere. Our pipeline includes two classifiers, the first determines whether a question requires reasoning over an image, and the second classifies whether a text paragraph is necessary to answer a question. Again, we experiment with an oracle and retrieval setting, such that in the oracle setting our model is presented with the gold paragraphs. We provide the full description of this pipeline in §A.4.\nEvaluation metrics For all datasets, we use the official scripts for computing F1 and EM, which compare the gold and predicted list of answers."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We present results on the downstream RC datasets (§5.1) and on the synthetic data (§5.2).\n5We removed tables that appear in the MMQA development and test sets from Dsyn."
    }, {
      "heading" : "5.1 Performance on RC Datasets",
      "text" : "Table 2 presents the results of our large models over all datasets, also in comparison to current state-of-the-art. We observe that PReasM substantially improves performance compared to T5 in all conditions, improving on the test set by 7.6, 7.9, 4.1, and 1.2 F1 points on DROP, IIRCoracle, IIRC, and MMQA respectively.6 We obtain new state-of-the-art results on MMQA and IIRCoracle. On IIRC, we improve performance when using the same retriever (Pipeline) and replacing the NumNet+ reader with PReasM.7 On DROP, specialized architectures for handling numbers still substantially outperform both T5 and PReasM.\nTable 3 shows the effect of different sampling strategies. Error sampling and momentum sampling generally outperform uniform sampling, but there is no clear advantage to momentum sampling compared to error sampling. We further analyze the effect of sampling methods in §5.2.\nWe now look at performance on different answer types across datasets, where PReasM leads to dramatic improvements on some types, while maintaining similar performance on other types.\n6To verify that the gains of PReasM over T5 are not due to knowledge memorized from Dsyn, we trained T5 and PReasM to generate the answer given the question only (without context). We found that the performance of T5 and PReasM is nearly identical in this setup.\n7We report the numbers from Ni et al. (2021) (45.8/44.3 F1 on the development/test sets). To fairly compare with the NumNet+ reader, we got the retrieved paragraphs for the Pipeline model through personal communication. However, results on these paragraphs was lower than reported in the paper: 45.5/42.8 F1. The reported results of our models are with this slightly worse retriever, but still outperform the performance of NumNet+ (Pipeline) from the original paper.\nDROP Table 4 breaks down performance based on answer types: PReasM outperforms T5 across the board for all model sizes and answer types. PReasM-Base outperforms GenBERT on 3 of 4 answer types. The high performance of GenBERT on Number questions can be explained by: (a) GenBERT uses digit tokenization which improves arithmetic reasoning (Thawani et al., 2021), and (b) training on multiple numerical reasoning templates.\nIIRC Table 5 breaks down performance based on answer types. PReasM outperforms T5 in the oracle setup by roughly 8 points for both Base and Large models, and by 2.6-4 points in the retrieval setup. Improvements are mostly due to cases when the answer is a numerical Value, where PReasM outperforms T5 by 39.1 and 40.3 F1 points in Base and Large models (oracle setup).\nComparing PReasM-Base to NumNet+, PReasM outperforms NumNet+ on None, Span and Binary questions, but lags behind on Value questions, where NumNet+ uses specialized architecture.\nOverall, PReasM-Large improves state-of-theart in the oracle setup by 4.7 F1 points. In the retrieval setting, PReasM outperforms NumNet+ (Pipeline) by 4.2 and 0.8 F1 points on the development and test sets, respectively (see Table 2).\nMMQA Table 6 breaks down performance based on reasoning skills (annotated per example in MMQA). PReasM outperforms T5 in both the oracle and retrieval setting, and for both model sizes.\nThe main cause for improvement are comparison questions, where PReasM outperforms T5 by 19 and 11.7 F1 points on Base and Large models.\nPReasM outperforms T5 on conjunction questions in Base models, and yes/no questions in all settings. Interestingly, T5 is equipped with decent composition skills, without any specialized pre-training.\nCompared to Implicit-Decomp, although Implicit-Decomp outperforms our models on questions that require hopping between two table columns and aggregations, PReasM outperforms Implicit-Decomp in all other cases. When considering only questions that require reasoning over text and tables, PReasM-Large improves F1 by 16.1 points, from 62.3 to 78.4.\n5.2 Performance on Dsyn Fig. 3 shows statistics on the performance of PReasM on different tasks in Dsyn during training. The average accuracy across all tasks at the end of training is high – almost 98.0 F1. PReasM reaches high performance on all tasks, where the lowest-performing tasks are ‘arithmetic addition’ (91.1) and ‘date difference’ (94.7). On those tasks, the advantage of error-driven sampling is evident, and it outperforms uniform sampling by as much as 4 points. Full results over Dsyn are in §A.5.\nZooming-in on the learning curve, momentum and error sampling learn reasoning skills a lot faster than uniform sampling. Looking at the entropy of Ptasks sheds light on the difference between error sampling and momentum sampling. Error sampling puts most probability mass on the lowestperforming task (arithmetic addition), and thus its entropy over tasks is roughly constant from a certain point. Conversely, momentum sampling puts a lot of probability mass on tasks that are improving quickly at the beginning, but as improvements plateau, it converges towards uniform sampling."
    }, {
      "heading" : "6 Analysis",
      "text" : "Reasoning skills in DROP To check which reasoning skills PReasM has, we use a proposed split of a subset of DROP to reasoning skills (Gupta\net al., 2020a). Table 7 presents the F1 for our best PReasM and T5 models, as well as the F1 from the neural module network (NMN) used in Gupta et al. (2020a). NMN was trained only on a subset of the original DROP dataset. When comparing to T5, PReasM dramatically improves performance on Date-Difference, and also leads to sizable gains in Number-Compare, Extract-Number and Count.\nAccuracy vs. training cost trade-off We evaluate PReasM-Base models on DROP and IIRCoracle as we vary the number of pre-training steps on Dsyn (Fig. 4). Most of the improvement happens in the first 100K steps, and error-driven sampling outperforms uniform sampling throughout training. Error sampling outperforms momentum sampling in the latter part of training. A possible reason is that the reasoning skills in the downstream tasks are correlated with the harder tasks during pre-training (arithmetic addition and date difference). This provides an advantage for error sampling, since it will focus on these tasks even if the improvement during pre-training is small."
    }, {
      "heading" : "7 Related Work",
      "text" : "Template-based data generation has been previously used for data augmentation, for example to inject numerical skills (Geva et al., 2020), and to improve consistency (Asai and Hajishirzi, 2020), and zero-shot accuracy (Zhao et al., 2019). In addition, templates were used for dataset construction (Talmor and Berant, 2018; Clark et al., 2020; Thorne et al., 2021), and to analyse model generalization (Rozen et al., 2019). In this work, we automatically generate examples by instantiating templates using structured data. Since our method relies solely on tables as input, it is highly scalable, has rich lexical diversity, and can be easily extended to new skills and domains.\nRecently, Thorne et al. (2021) introduced the WIKINLDB dataset, which includes queries that require reasoning over a set of textual facts. Queries are instantiated with values from a knowledge graph (KG), and facts are generated by a LM. Unlike this work, WIKINLDB is focused on evaluating reasoning skills. We, on the other hand, show that generated examples can be used to endow a pretrained LM with new reasoning skills. Moreover, tables are much easier to collect at scale compared to KGs, which tend to have limited coverage. Data augmenatation techniques have been extensively explored in RC, QA, and dialogue (Feng\net al., 2021; Talmor and Berant, 2019; Khashabi et al., 2020; Alberti et al., 2019; Puri et al., 2020; Bartolo et al., 2021). Here, we focus on tables as a valuable source for data generation. Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases (Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al.,\n2020; Müller et al., 2021; Yu et al., 2021; Neeraja et al., 2021b). Here, we use pre-training over tables to improve reasoning over text. We leave evaluation on tasks beyond RC to future work. Error-driven sampling has been considered in the past in the context of active learning (Sharma et al., 2018), reinforcement learning (Graves et al., 2017; Glover and Hokamp, 2019; Xu et al., 2019), transfer learning (Zhang et al., 2020; Pilault et al., 2021), and distributionally robust optimization (Oren et al., 2019; Sagawa et al., 2020), where the goal is to perform well over a family of distributions. Similar to Gottumukkala et al. (2020), we compute heterogeneous batches based on error rates, and show that this improves efficiency and performance."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We propose semi-structured tables as a valuable resource for generating examples that can endow pretrained language models with reasoning skills. We generate 5M examples that correspond to 16 reasoning skills from Wikipedia tables and add a pretraining step over this data. To improve data efficiency we use error-driven sampling, which focuses training on reasoning skills that the model currently lacks. We evaluate our model, PReasM, on three reasoning-focused RC datasets and show that it leads to substantial improvements in all cases."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "A.1 Data Generation In this section, we provide details about our data generation process. We describe how we classify table columns and provide examples and analysis.\nClassifying table columns When annotating the semantic types of columns, a column will be of type NUMBER or DATE if all values in the column can be parsed with standard tools for parsing numbers and dates,8 accordingly. Otherwise, we annotate the column as type STRING.\nInformation in tables is usually aggregated such that certain columns serve as the semantic index of the table. For example, the table in Fig. 1 provides information about each round in a tournament. In order for our examples to be semantically meaningful, we generate questions about columns that serve as the semantic index of their table.\nSince the semantic index is not provided, we use a linear decision rule to find such columns. The features to our classifier include the column’s distance from the leftmost column, the percentage of unique cells in the column, the percentage of cells whose values are links to Wikipedia articles, the percentage of cells with short text (at most 2 characters), the percentage of cells with numbers, and the column’s header. We allow more than one semantic index per table, such that both the Round and Opponent columns can serve as a semantic index in the table in Fig. 1.\nData analysis Table 8 contains the number of generated examples for every EG. During data generation, we randomly generate at most 10 examples for each EG and table. Table 9 contains examples for generated (q, c, a) triplets, including the full context c. Fig. 5 presents the most common categories of the Wikipedia pages from which we scraped our tables.\nA.2 Advantages of Momentum Sampling To highlight the theoretical benefits of momentum sampling, we construct synthetic experiments where there is high variance in the ceiling accuracy between different tasks. As we show in §5.2, our models are able to achieve near perfect accuracy on our tasks when provided with enough training examples. Hence, we create settings where the ceiling accuracy for a task is lower than 1.0, either\n8https://pypi.org/project/ python-dateutil/\nby adding noise or by down-sampling the number of training examples. More specifically, we train on two tasks: an arithmetic addition task that trains slowly and has a high ceiling accuracy, and a second task that trains quickly, and evaluate the performance on a held-out set of arithmetic addition examples.\nFirst, we train on arithmetic addition and 2-hop composition, which is faster to train. We conduct two experiments, in which we add noise to the 2- hop composition task by randomly sampling the label from the vocabulary in order to force the ceiling accuracy to be lower than 1.0. To check the performance of sampling strategies in varying levels of noise, we conduct two experiments where we add noise to 30% or 100% of the examples (in the latter case the label of 2-hop composition is random). We expect that this will lead to slower\nlearning of arithmetic addition for error sampling, since more probability mass will be allocated to the noisy task (since its ceiling accuracy is low), despite the fact that it is easier.\nNext, we train on arithmetic addition and date difference, both of which train slowly. To force the ceiling accuracy of the date difference task to be lower than 1.0, our training set contains only 1,000 examples. This emulates settings where the data is not generated automatically and the cost of generating examples is higher. Again, we expect error sampling to over-sample from the date difference task even when this would not lead to gains in performance, due to the small training set.\nFig. 6 illustrates the advantage of momentum sampling in these settings. Without noise (top left), both momentum sampling and error sampling learn faster than uniform sampling. Momentum sampling learns more slowly than error sampling, due to the warm-start period in the first w evaluation checkpoints. As we add noise to 30% of the examples (top right), error sampling focuses on the noisy task once accuracy approaches a certain level (0.7 F1). When we add noise to all of the 2-hop composition examples (bottom left), uniform sampling outperforms error sampling, while momentum sampling still performs well. This phenomenon repeats when we switch the 2-hop composition task with the date difference task and down-sample the number of training examples (bottom right).\nA.3 Implementation Details\nThe following section includes implementation details for our experiments, including: hyperparameters for the momentum sampling algorithm, the original pre-training task, and technical details.\nMomentum sampling For momentum sampling we use a window size of w = 4, a smoothing factor of k = 2, and sample at least ϵ = 0.002 examples from every task in Dsyn.\nOriginal pre-training task In order to avoid catastrophic forgetting (Kirkpatrick et al., 2016), we continue training with the span-corruption objective introduced in (Raffel et al., 2020), over sequences of length 256 from the English Wikipedia.\nTechnical details We train all our experiments on one RTX8000 (48GB) or RTX3090 (24GB) GPUs. Our PReasM-Base and PReasM-Large models training time was 5-6 and 8-9 days on one RTX8000 GPU, respectively. We use the T5 model from https://huggingface. co/transformers/model_doc/t5.html (Wolf et al., 2020). Table 10 contains the hyper-parameters used in our experiments.\nA.4 MMQA Pipeline\nThe first classifier in our pipeline is a T5-large model fine-tuned on the MMQA training set to determine if a question is likely to require an image or not. When the classifier determines a question requires an image, the example is directed to Implicit-Decomp. The accuracy of this classifier on the MMQA development set is 99.2%.\nThe second classifier in the pipeline is a T5-3B model, fine-tuned on the MMQA training set to determine given a question and one of the textual paragraphs if that paragraph is required for answering the question. Then, for every question that does not require an image, we classify each of the textual paragraphs and only use the ones classified as relevant. This process identifies all gold paragraphs in 95.8% of the examples.\nLast, we convert the table into text by linearizing the table as described in Talmor et al. (2021). The model is presented with multiple paragraphs and the linearized table, and can answer questions that require any reasoning across them. Since the context is long, we present the model with contexts of size 1,536 word-pieces (without any change to the original T5 model).\nA.5 Performance on Dsyn Fig. 7 and Table 11 show the results for T5 and PReasM on Dsyn for both model sizes. The results for T5 were obtained by training in a few-shot manner on 32 examples for 200 steps, as suggested in (Ram et al., 2021). T5-Large outperforms T5-Base on most tasks, suggesting that larger models are able to learn reasoning skills faster. On tasks such as date difference and arithmetic addition, the results for T5-Large are low, at around 10 F1. Our PReasM models significantly outperforms T5 on all tasks."
    } ],
    "references" : [ {
      "title" : "Synthetic QA corpora generation with roundtrip consistency",
      "author" : [ "Chris Alberti", "Daniel Andor", "Emily Pitler", "Jacob Devlin", "Michael Collins." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173, Flo-",
      "citeRegEx" : "Alberti et al\\.,? 2019",
      "shortCiteRegEx" : "Alberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Good-enough compositional data augmentation",
      "author" : [ "Jacob Andreas." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556–7566, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Andreas.,? 2020",
      "shortCiteRegEx" : "Andreas.",
      "year" : 2020
    }, {
      "title" : "Logicguided data augmentation and regularization for consistent question answering",
      "author" : [ "Akari Asai", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5642–5650, Online. Asso-",
      "citeRegEx" : "Asai and Hajishirzi.,? 2020",
      "shortCiteRegEx" : "Asai and Hajishirzi.",
      "year" : 2020
    }, {
      "title" : "Improving question answering model robustness with synthetic adversarial data generation",
      "author" : [ "Max Bartolo", "Tristan Thrush", "Robin Jia", "Sebastian Riedel", "Pontus Stenetorp", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Bartolo et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bartolo et al\\.",
      "year" : 2021
    }, {
      "title" : "Zero-shot transfer learning with synthesized data for multi-domain dialogue state tracking",
      "author" : [ "Giovanni Campagna", "Agata Foryciarz", "Mehrad Moradshahi", "Monica Lam." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Campagna et al\\.,? 2020",
      "shortCiteRegEx" : "Campagna et al\\.",
      "year" : 2020
    }, {
      "title" : "Question directed graph attention network for numerical reasoning over text",
      "author" : [ "Kunlong Chen", "Weidi Xu", "Xingyi Cheng", "Zou Xiaochuan", "Yuyu Zhang", "Le Song", "Taifeng Wang", "Yuan Qi", "Wei Chu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Tabfact: A large-scale dataset for table-based fact verification",
      "author" : [ "Wenhu Chen", "Hongmin Wang", "Jianshu Chen", "Yunkai Zhang", "Hong Wang", "Shiyang Li", "Xiyou Zhou", "William Yang Wang." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers as soft reasoners over language",
      "author" : [ "Peter Clark", "Oyvind Tafjord", "Kyle Richardson." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3882–3890. International Joint Conferences on Arti-",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "An Introduction to the Bootstrap",
      "author" : [ "Bradley Efron", "Robert J. Tibshirani." ],
      "venue" : "Number 57 in Monographs on Statistics and Applied Probability. Chapman & Hall/CRC, Boca Raton, Florida, USA.",
      "citeRegEx" : "Efron and Tibshirani.,? 1993",
      "shortCiteRegEx" : "Efron and Tibshirani.",
      "year" : 1993
    }, {
      "title" : "Understanding tables with intermediate pre-training",
      "author" : [ "Julian Eisenschlos", "Syrine Krichene", "Thomas Müller." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 281–296, Online. Association for Computational Lin-",
      "citeRegEx" : "Eisenschlos et al\\.,? 2020",
      "shortCiteRegEx" : "Eisenschlos et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey of data augmentation approaches for NLP",
      "author" : [ "Steven Y. Feng", "Varun Gangal", "Jason Wei", "Sarath Chandar", "Soroush Vosoughi", "Teruko Mitamura", "Eduard Hovy" ],
      "venue" : null,
      "citeRegEx" : "Feng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "IIRC: A dataset of incomplete information reading comprehension questions",
      "author" : [ "James Ferguson", "Matt Gardner", "Hannaneh Hajishirzi", "Tushar Khot", "Pradeep Dasigi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Ferguson et al\\.,? 2020",
      "shortCiteRegEx" : "Ferguson et al\\.",
      "year" : 2020
    }, {
      "title" : "Tablenet: An approach for determining finegrained relations for wikipedia tables",
      "author" : [ "Besnik Fetahu", "Avishek Anand", "Maria Koutraki." ],
      "venue" : "The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pages 2736–2742. ACM.",
      "citeRegEx" : "Fetahu et al\\.,? 2019",
      "shortCiteRegEx" : "Fetahu et al\\.",
      "year" : 2019
    }, {
      "title" : "Injecting numerical reasoning skills into language models",
      "author" : [ "Mor Geva", "Ankit Gupta", "Jonathan Berant." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 9",
      "citeRegEx" : "Geva et al\\.,? 2020",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2020
    }, {
      "title" : "Task selection policies for multitask learning",
      "author" : [ "John Glover", "Chris Hokamp" ],
      "venue" : null,
      "citeRegEx" : "Glover and Hokamp.,? \\Q2019\\E",
      "shortCiteRegEx" : "Glover and Hokamp.",
      "year" : 2019
    }, {
      "title" : "Dynamic sampling strategies for multi-task reading comprehension",
      "author" : [ "Ananth Gottumukkala", "Dheeru Dua", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 920–924, Online.",
      "citeRegEx" : "Gottumukkala et al\\.,? 2020",
      "shortCiteRegEx" : "Gottumukkala et al\\.",
      "year" : 2020
    }, {
      "title" : "Automated curriculum learning for neural networks",
      "author" : [ "Alex Graves", "Marc G. Bellemare", "Jacob Menick", "Rémi Munos", "Koray Kavukcuoglu." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11",
      "citeRegEx" : "Graves et al\\.,? 2017",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural module networks for reasoning over text",
      "author" : [ "Nitish Gupta", "Kevin Lin", "Dan Roth", "Sameer Singh", "Matt Gardner." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
      "citeRegEx" : "Gupta et al\\.,? 2020a",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "INFOTABS: Inference on tables as semi-structured data",
      "author" : [ "Vivek Gupta", "Maitrey Mehta", "Pegah Nokhiz", "Vivek Srikumar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2309–2324, Online. Association",
      "citeRegEx" : "Gupta et al\\.,? 2020b",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "TaPas: Weakly supervised table parsing via pre-training",
      "author" : [ "Jonathan Herzig", "Pawel Krzysztof Nowak", "Thomas Müller", "Francesco Piccinno", "Julian Eisenschlos." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Herzig et al\\.,? 2020",
      "shortCiteRegEx" : "Herzig et al\\.",
      "year" : 2020
    }, {
      "title" : "DeSePtion: Dual sequence prediction and adversarial examples for improved fact-checking",
      "author" : [ "Christopher Hidey", "Tuhin Chakrabarty", "Tariq Alhindi", "Siddharth Varia", "Kriste Krstovski", "Mona Diab", "Smaranda Muresan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Hidey et al\\.,? 2020",
      "shortCiteRegEx" : "Hidey et al\\.",
      "year" : 2020
    }, {
      "title" : "UNIFIEDQA: Crossing format boundaries with a single QA system",
      "author" : [ "Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Clark", "Hannaneh Hajishirzi." ],
      "venue" : "Findings of the Association for Computational Linguistics:",
      "citeRegEx" : "Khashabi et al\\.,? 2020",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "Text modular networks: Learning to decompose tasks in the language of existing models",
      "author" : [ "Tushar Khot", "Daniel Khashabi", "Kyle Richardson", "Peter Clark", "Ashish Sabharwal." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the",
      "citeRegEx" : "Khot et al\\.,? 2021",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2021
    }, {
      "title" : "BART: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Tapas at semeval-2021 task 9: Reasoning over tables with intermediate pre-training",
      "author" : [ "Thomas Müller", "Julian Martin Eisenschlos", "Syrine Krichene" ],
      "venue" : null,
      "citeRegEx" : "Müller et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2021
    }, {
      "title" : "Fetaqa: Free-form table question answering",
      "author" : [ "Dragomir R. Radev." ],
      "venue" : "CoRR, abs/2104.00369.",
      "citeRegEx" : "Radev.,? 2021",
      "shortCiteRegEx" : "Radev.",
      "year" : 2021
    }, {
      "title" : "Incorporating external knowledge to enhance tabular reasoning",
      "author" : [ "J. Neeraja", "Vivek Gupta", "Vivek Srikumar." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Neeraja et al\\.,? 2021a",
      "shortCiteRegEx" : "Neeraja et al\\.",
      "year" : 2021
    }, {
      "title" : "Incorporating external knowledge to enhance tabular reasoning",
      "author" : [ "J. Neeraja", "Vivek Gupta", "Vivek Srikumar." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Neeraja et al\\.,? 2021b",
      "shortCiteRegEx" : "Neeraja et al\\.",
      "year" : 2021
    }, {
      "title" : "Mitigating false-negative contexts in multi-document questionanswering with retrieval marginalization",
      "author" : [ "Ansong Ni", "Matt Gardner", "Pradeep Dasigi." ],
      "venue" : "CoRR, abs/2103.12235.",
      "citeRegEx" : "Ni et al\\.,? 2021",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2021
    }, {
      "title" : "Distributionally robust language modeling",
      "author" : [ "Yonatan Oren", "Shiori Sagawa", "Tatsunori Hashimoto", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Oren et al\\.,? 2019",
      "shortCiteRegEx" : "Oren et al\\.",
      "year" : 2019
    }, {
      "title" : "Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters & less data",
      "author" : [ "Jonathan Pilault", "Amine El hattami", "Christopher Pal." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Pilault et al\\.,? 2021",
      "shortCiteRegEx" : "Pilault et al\\.",
      "year" : 2021
    }, {
      "title" : "Training question answering models from synthetic data",
      "author" : [ "Raul Puri", "Ryan Spring", "Mohammad Shoeybi", "Mostofa Patwary", "Bryan Catanzaro." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Puri et al\\.,? 2020",
      "shortCiteRegEx" : "Puri et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Few-shot question answering by pretraining span selection",
      "author" : [ "Ori Ram", "Yuval Kirstain", "Jonathan Berant", "Amir Globerson", "Omer Levy." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ram et al\\.,? 2021",
      "shortCiteRegEx" : "Ram et al\\.",
      "year" : 2021
    }, {
      "title" : "NumNet: Machine reading comprehension with numerical reasoning",
      "author" : [ "Qiu Ran", "Yankai Lin", "Peng Li", "Jie Zhou", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Ran et al\\.,? 2019",
      "shortCiteRegEx" : "Ran et al\\.",
      "year" : 2019
    }, {
      "title" : "Diversify your datasets: Analyzing generalization via controlled variance in adversarial datasets",
      "author" : [ "Ohad Rozen", "Vered Shwartz", "Roee Aharoni", "Ido Dagan." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),",
      "citeRegEx" : "Rozen et al\\.,? 2019",
      "shortCiteRegEx" : "Rozen et al\\.",
      "year" : 2019
    }, {
      "title" : "Distributionally robust neural networks",
      "author" : [ "Shiori Sagawa", "Pang Wei Koh", "Tatsunori B. Hashimoto", "Percy Liang." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
      "citeRegEx" : "Sagawa et al\\.,? 2020",
      "shortCiteRegEx" : "Sagawa et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to multitask by active sampling",
      "author" : [ "Sahil Sharma", "Ashutosh Kumar Jha", "Parikshit Hegde", "Balaraman Ravindran." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "The web as a knowledge-base for answering complex questions",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Talmor and Berant.,? 2018",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2018
    }, {
      "title" : "MultiQA: An empirical investigation of generalization and transfer in reading comprehension",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4911–4921, Florence, Italy.",
      "citeRegEx" : "Talmor and Berant.,? 2019",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2019
    }, {
      "title" : "oLMpics-on what language model pre-training captures",
      "author" : [ "Alon Talmor", "Yanai Elazar", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:743–758.",
      "citeRegEx" : "Talmor et al\\.,? 2020",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2020
    }, {
      "title" : "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "MultimodalQA: complex question answering over text, tables and images",
      "author" : [ "Alon Talmor", "Ori Yoran", "Amnon Catav", "Dan Lahav", "Yizhong Wang", "Akari Asai", "Gabriel Ilharco", "Hannaneh Hajishirzi", "Jonathan Berant." ],
      "venue" : "International Conference on",
      "citeRegEx" : "Talmor et al\\.,? 2021",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2021
    }, {
      "title" : "Representing numbers in NLP: a survey and a vision",
      "author" : [ "Avijit Thawani", "Jay Pujara", "Filip Ilievski", "Pedro Szekely." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Thawani et al\\.,? 2021",
      "shortCiteRegEx" : "Thawani et al\\.",
      "year" : 2021
    }, {
      "title" : "Database reasoning over text",
      "author" : [ "James Thorne", "Majid Yazdani", "Marzieh Saeidi", "Fabrizio Silvestri", "Sebastian Riedel", "Alon Y. Halevy." ],
      "venue" : "CoRR, abs/2106.01074.",
      "citeRegEx" : "Thorne et al\\.,? 2021",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2021
    }, {
      "title" : "Do NLP models know numbers? probing numeracy in embeddings",
      "author" : [ "Eric Wallace", "Yizhong Wang", "Sujian Li", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Balancing training for multilingual neural machine translation",
      "author" : [ "Xinyi Wang", "Yulia Tsvetkov", "Graham Neubig." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8526–8537, Online. Association for Computa-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Investigating BERT’s knowledge of language: Five analysis methods with NPIs",
      "author" : [ "Haokun Liu", "Alicia Parrish", "Sheng-Fu Wang", "Jason Phang", "Anhad Mohananey", "Phu Mon Htut", "Paloma Jeretic", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2019 Confer-",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush" ],
      "venue" : null,
      "citeRegEx" : "Scao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task learning with sample re-weighting for machine reading comprehension",
      "author" : [ "Yichong Xu", "Xiaodong Liu", "Yelong Shen", "Jingjing Liu", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "TaBERT: Pretraining for joint understanding of textual and tabular data",
      "author" : [ "Pengcheng Yin", "Graham Neubig", "Wen-tau Yih", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426, On-",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning and evaluating general linguistic intelligence",
      "author" : [ "Dani Yogatama", "Cyprien de Masson d’Autume", "Jerome Connor", "Tomas Kocisky", "Mike Chrzanowski", "Lingpeng Kong", "Angeliki Lazaridou", "Wang Ling", "Lei Yu", "Chris Dyer", "Phil Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Yogatama et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yogatama et al\\.",
      "year" : 2019
    }, {
      "title" : "GraPPa: Grammar-augmented pre-training for table semantic parsing",
      "author" : [ "Tao Yu", "Chien-Sheng Wu", "Xi Victoria Lin", "bailin wang", "Yi Chern Tan", "Xinyi Yang", "Dragomir Radev", "richard socher", "Caiming Xiong" ],
      "venue" : "In International Conference on Learning",
      "citeRegEx" : "Yu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2021
    }, {
      "title" : "Worst-case-aware curriculum learning for zero and few shot transfer",
      "author" : [ "Sheng Zhang", "Xin Zhang", "Weiming Zhang", "Anders Søgaard" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Data augmentation with atomic templates for spoken language understanding",
      "author" : [ "Zijian Zhao", "Su Zhu", "Kai Yu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "The attendance when the opponent was Oxford United was 9,789. The attendances when the opponent was Portsmouth were 16,699 and 16,085. The attendances when the opponent was Walsall",
      "author" : [ "F.C. Chelsea" ],
      "venue" : "Counting",
      "citeRegEx" : "Chelsea,? \\Q1990\\E",
      "shortCiteRegEx" : "Chelsea",
      "year" : 1990
    }, {
      "title" : "Nana Akufo-Addo. The candidate when Elections have Candidate John the election",
      "author" : [ "Boahen" ],
      "venue" : "Kufuor?",
      "citeRegEx" : "Boahen.,? \\Q2008\\E",
      "shortCiteRegEx" : "Boahen.",
      "year" : 2008
    }, {
      "title" : "Kufuor. The candidate when the election",
      "author" : [ "Albert Adu Boahen" ],
      "venue" : "Nana Akufo-Addo",
      "citeRegEx" : "Boahen.,? \\Q1992\\E",
      "shortCiteRegEx" : "Boahen.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Large pre-trained language models (LMs) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020) have become the backbone of natural language processing in recent years.",
      "startOffset" : 40,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "Large pre-trained language models (LMs) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020) have become the backbone of natural language processing in recent years.",
      "startOffset" : 40,
      "endOffset" : 99
    }, {
      "referenceID" : 48,
      "context" : ", 2019, 2020), numerical operations (Wallace et al., 2019; Hidey et al., 2020), and quantification (Warstadt et al.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : ", 2019, 2020), numerical operations (Wallace et al., 2019; Hidey et al., 2020), and quantification (Warstadt et al.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 37,
      "context" : "Past work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019; Gupta et al., 2020a; Khot et al., 2021; Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al.",
      "startOffset" : 171,
      "endOffset" : 249
    }, {
      "referenceID" : 19,
      "context" : "Past work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019; Gupta et al., 2020a; Khot et al., 2021; Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al.",
      "startOffset" : 171,
      "endOffset" : 249
    }, {
      "referenceID" : 24,
      "context" : "Past work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019; Gupta et al., 2020a; Khot et al., 2021; Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al.",
      "startOffset" : 171,
      "endOffset" : 249
    }, {
      "referenceID" : 5,
      "context" : "Past work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019; Gupta et al., 2020a; Khot et al., 2021; Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al.",
      "startOffset" : 171,
      "endOffset" : 249
    }, {
      "referenceID" : 38,
      "context" : ", 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al., 2019; Zhao et al., 2019; Andreas, 2020; Asai and Hajishirzi, 2020; Campagna et al., 2020), and Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data.",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 57,
      "context" : ", 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al., 2019; Zhao et al., 2019; Andreas, 2020; Asai and Hajishirzi, 2020; Campagna et al., 2020), and Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data.",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : ", 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al., 2019; Zhao et al., 2019; Andreas, 2020; Asai and Hajishirzi, 2020; Campagna et al., 2020), and Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data.",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 2,
      "context" : ", 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al., 2019; Zhao et al., 2019; Andreas, 2020; Asai and Hajishirzi, 2020; Campagna et al., 2020), and Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data.",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 4,
      "context" : ", 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen et al., 2019; Zhao et al., 2019; Andreas, 2020; Asai and Hajishirzi, 2020; Campagna et al., 2020), and Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data.",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "(Gottumukkala et al., 2020) to construct training batches, where most examples are sampled from EGs that the model currently struggles with.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "We fine tune our Pre-traind for Reasoning Model, PReasM, on three RC datasets that require reasoning: DROP (Dua et al., 2019), IIRC (Ferguson et al.",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : ", 2019), IIRC (Ferguson et al., 2020), and MMQA (Talmor et al.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 35,
      "context" : "PReasM outperforms the original pre-trained T5 (Raffel et al., 2020) model by significant margins: 7.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "English Wikipedia includes millions of tables with high lexical and domain diversity (Fetahu et al., 2019; Chen et al., 2020b; Gupta et al., 2020b; Talmor et al., 2021; Nan et al., 2021; Neeraja et al., 2021a).",
      "startOffset" : 85,
      "endOffset" : 209
    }, {
      "referenceID" : 6,
      "context" : "English Wikipedia includes millions of tables with high lexical and domain diversity (Fetahu et al., 2019; Chen et al., 2020b; Gupta et al., 2020b; Talmor et al., 2021; Nan et al., 2021; Neeraja et al., 2021a).",
      "startOffset" : 85,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "English Wikipedia includes millions of tables with high lexical and domain diversity (Fetahu et al., 2019; Chen et al., 2020b; Gupta et al., 2020b; Talmor et al., 2021; Nan et al., 2021; Neeraja et al., 2021a).",
      "startOffset" : 85,
      "endOffset" : 209
    }, {
      "referenceID" : 45,
      "context" : "English Wikipedia includes millions of tables with high lexical and domain diversity (Fetahu et al., 2019; Chen et al., 2020b; Gupta et al., 2020b; Talmor et al., 2021; Nan et al., 2021; Neeraja et al., 2021a).",
      "startOffset" : 85,
      "endOffset" : 209
    }, {
      "referenceID" : 29,
      "context" : "English Wikipedia includes millions of tables with high lexical and domain diversity (Fetahu et al., 2019; Chen et al., 2020b; Gupta et al., 2020b; Talmor et al., 2021; Nan et al., 2021; Neeraja et al., 2021a).",
      "startOffset" : 85,
      "endOffset" : 209
    }, {
      "referenceID" : 54,
      "context" : "Similar to past work (Yogatama et al., 2019; Geva et al., 2020), to avoid “catastrophic forgetting” (Kirkpatrick et al.",
      "startOffset" : 21,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "Similar to past work (Yogatama et al., 2019; Geva et al., 2020), to avoid “catastrophic forgetting” (Kirkpatrick et al.",
      "startOffset" : 21,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "Past work (Gottumukkala et al., 2020) has shown that heterogeneous batching, i.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 40,
      "context" : "Error sampling Recent work (Sharma et al., 2018; Gottumukkala et al., 2020) proposed to construct Ptasks based on model errors, where one over-samples tasks with higher errors.",
      "startOffset" : 27,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Error sampling Recent work (Sharma et al., 2018; Gottumukkala et al., 2020) proposed to construct Ptasks based on model errors, where one over-samples tasks with higher errors.",
      "startOffset" : 27,
      "endOffset" : 75
    }, {
      "referenceID" : 35,
      "context" : "Baselines Our main baseline is T5 (Raffel et al., 2020), a popular pre-trained encoder-decoder model, which we fine-tune on the downstream",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "DROP (Dua et al., 2019) is a RC dataset with questions that require numeric reasoning.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "As an additional baseline, we also compare to GenBERT (Geva et al., 2020), which similar to our approach injects numerical skills by automatically generating synthetic data from a grammar.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "IIRC (Ferguson et al., 2020) is a QA dataset, where annotators were given a single Wikipedia",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 37,
      "context" : "(2021) to retrieve the context, and replace the NumNet+ (Base) reader (Ran et al., 2019) used by the authors (which",
      "startOffset" : 70,
      "endOffset" : 88
    }, {
      "referenceID" : 45,
      "context" : "MMQA (Talmor et al., 2021) is a QA dataset, where the input is a question and a context that",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "05) according to the paired bootstrap test (Efron and Tibshirani, 1993).",
      "startOffset" : 43,
      "endOffset" : 71
    }, {
      "referenceID" : 46,
      "context" : "arithmetic reasoning (Thawani et al., 2021), and (b) training on multiple numerical reasoning templates.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "Reasoning skills in DROP To check which reasoning skills PReasM has, we use a proposed split of a subset of DROP to reasoning skills (Gupta et al., 2020a).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : "ously used for data augmentation, for example to inject numerical skills (Geva et al., 2020), and to improve consistency (Asai and Hajishirzi, 2020), and zero-shot accuracy (Zhao et al.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : ", 2020), and to improve consistency (Asai and Hajishirzi, 2020), and zero-shot accuracy (Zhao et al.",
      "startOffset" : 36,
      "endOffset" : 63
    }, {
      "referenceID" : 57,
      "context" : ", 2020), and to improve consistency (Asai and Hajishirzi, 2020), and zero-shot accuracy (Zhao et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 41,
      "context" : "tion (Talmor and Berant, 2018; Clark et al., 2020; Thorne et al., 2021), and to analyse model generalization (Rozen et al.",
      "startOffset" : 5,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "tion (Talmor and Berant, 2018; Clark et al., 2020; Thorne et al., 2021), and to analyse model generalization (Rozen et al.",
      "startOffset" : 5,
      "endOffset" : 71
    }, {
      "referenceID" : 47,
      "context" : "tion (Talmor and Berant, 2018; Clark et al., 2020; Thorne et al., 2021), and to analyse model generalization (Rozen et al.",
      "startOffset" : 5,
      "endOffset" : 71
    }, {
      "referenceID" : 38,
      "context" : ", 2021), and to analyse model generalization (Rozen et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases (Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al., 2020; Müller et al., 2021; Yu et al., 2021; Neeraja et al., 2021b).",
      "startOffset" : 94,
      "endOffset" : 220
    }, {
      "referenceID" : 53,
      "context" : "Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases (Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al., 2020; Müller et al., 2021; Yu et al., 2021; Neeraja et al., 2021b).",
      "startOffset" : 94,
      "endOffset" : 220
    }, {
      "referenceID" : 21,
      "context" : "Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases (Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al., 2020; Müller et al., 2021; Yu et al., 2021; Neeraja et al., 2021b).",
      "startOffset" : 94,
      "endOffset" : 220
    }, {
      "referenceID" : 27,
      "context" : "Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases (Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al., 2020; Müller et al., 2021; Yu et al., 2021; Neeraja et al., 2021b).",
      "startOffset" : 94,
      "endOffset" : 220
    }, {
      "referenceID" : 55,
      "context" : "Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases (Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al., 2020; Müller et al., 2021; Yu et al., 2021; Neeraja et al., 2021b).",
      "startOffset" : 94,
      "endOffset" : 220
    }, {
      "referenceID" : 30,
      "context" : "Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases (Eisenschlos et al., 2020; Yin et al., 2020; Herzig et al., 2020; Müller et al., 2021; Yu et al., 2021; Neeraja et al., 2021b).",
      "startOffset" : 94,
      "endOffset" : 220
    }, {
      "referenceID" : 40,
      "context" : "past in the context of active learning (Sharma et al., 2018), reinforcement learning (Graves et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : ", 2018), reinforcement learning (Graves et al., 2017; Glover and Hokamp, 2019; Xu et al., 2019), transfer learning (Zhang et al.",
      "startOffset" : 32,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : ", 2018), reinforcement learning (Graves et al., 2017; Glover and Hokamp, 2019; Xu et al., 2019), transfer learning (Zhang et al.",
      "startOffset" : 32,
      "endOffset" : 95
    }, {
      "referenceID" : 52,
      "context" : ", 2018), reinforcement learning (Graves et al., 2017; Glover and Hokamp, 2019; Xu et al., 2019), transfer learning (Zhang et al.",
      "startOffset" : 32,
      "endOffset" : 95
    }, {
      "referenceID" : 56,
      "context" : ", 2019), transfer learning (Zhang et al., 2020; Pilault et al., 2021), and distributionally robust optimization (Oren et al.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 33,
      "context" : ", 2019), transfer learning (Zhang et al., 2020; Pilault et al., 2021), and distributionally robust optimization (Oren et al.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 32,
      "context" : ", 2021), and distributionally robust optimization (Oren et al., 2019; Sagawa et al., 2020), where the goal is to perform well over a family of distributions.",
      "startOffset" : 50,
      "endOffset" : 90
    }, {
      "referenceID" : 39,
      "context" : ", 2021), and distributionally robust optimization (Oren et al., 2019; Sagawa et al., 2020), where the goal is to perform well over a family of distributions.",
      "startOffset" : 50,
      "endOffset" : 90
    } ],
    "year" : 0,
    "abstractText" : "Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale questionparagraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs. We evaluate our approach on three reasoning-focused reading comprehension datasets, and show that our model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling examples based on model errors leads to faster training and higher performance.",
    "creator" : null
  }
}
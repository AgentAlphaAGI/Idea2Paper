{
  "name" : "ARR_2022_167_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Question Answering Infused Pre-training of General-Purpose Contextualized Representations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Although masked language models build contextualized word representations, they are pre-trained with losses that minimize distance to uncontextualized word embeddings (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019). In this paper, we introduce Question Answering Infused Pre-training (QUIP), a new pre-training loss based on question answering (QA) that depends much more directly on context, and learns improved token-level representations for a range of zero- and few-shot tasks.\nOur intuition for QUIP is that the contextualized representation for a phrase in a passage should contain enough information to identify all the questions that the phrase could answer in context. For example, in Figure 1, the representation for Johannes Brahms should be similar to the representation of all questions it can answer, such as “Who wrote the violin concerto?” We anticipate that optimizing passage representations for QA should benefit many downstream tasks, as question-answer pairs\nhave been used as broad-coverage meaning representations (He et al., 2015; Michael et al., 2018), and a wide range of NLP tasks can be cast as QA problems (Levy et al., 2017; McCann et al., 2018; Gardner et al., 2019), For instance, our learned representations should encode whether a phrase answers a question like “Why was the movie considered good?”, which corresponds to identifying rationales for sentiment analysis.\nWe train QUIP with a bi-encoder extractive QA objective. The bi-encoder model independently encodes passages and questions such that the representation of each phrase in a passage is similar to the representation of reading comprehension questions answered by that phrase. To train this model, we use a question generation model to synthesize 80 million QA examples, then train the bi-encoder to match the predictions of a cross-encoder QA model, which processes the passage and question\ntogether, on these examples. Bi-encoder QA has been used before for efficient open-domain QA via phrase retrieval (Seo et al., 2018, 2019; Lee et al., 2020, 2021), but its lower accuracy compared to cross-encoder QA has previously been viewed as a drawback. We instead view the relative weakness of bi-encoder QA as an opportunity to improve contextual representations via knowledge distillation, as self-training can be effective when the student model must solve a harder problem than the teacher (Xie et al., 2020). In particular, since the bi-encoder does not know the question when encoding the passage, it must produce a single passage representation that simultaneously encodes the answers to all possible questions. In contrast, while cross-encoder QA models are more accurate, they depend on a specific question when encoding a passage; thus, they are less suited to downstream use cases that require contextualized representations of passages in isolation.\nWe show that QUIP token-level representations are useful in a variety of zero-shot and few-shot learning settings, both because the representations directly encode useful contextual information, and because we can often reduce downstream tasks to QA. For few-shot paraphrase detection, QUIP with BERTScore-based features (Zhang et al., 2020) outperforms prior work by 9 F1 points across four datasets. For few-shot named entity recognition (NER), QUIP combined with an initialization scheme that uses question embeddings improves over RoBERTa-large by 14 F1 across two datasets. Finally, for zero-shot sentiment analysis, QUIP with question prompts improves over RoBERTa-large with MLM-style prompts by 5 accuracy points across three datasets, and extracts interpretable rationales as a side effect. Through ablations, we show that using real questions, a strong teacher model, and the bi-encoder architecture are all crucial to the success of QUIP. We will release code to reproduce all results upon publication."
    }, {
      "heading" : "2 QA Infused Pre-training",
      "text" : "QA Infused Pre-training (QUIP) involves pretraining contextual representations with a biencoder extractive QA objective. In this section, we introduce some notation (§2.1), then describe our QUIP pipeline, which consists of three steps: question generation (§2.2), cross-encoder teacher re-labeling (§2.3), and bi-encoder training (§2.4)."
    }, {
      "heading" : "2.1 Notation",
      "text" : "All models operate on sequences of tokens x = [x1, . . . , xL] of length L, where x1 is the special beginning-of-sequence token. We learn an encoder r that maps inputs x to outputs r(x) = [r(x)1, . . . , r(x)L] where each r(x)i ∈ Rd for some fixed dimension d. We call r(x)i the contextual representation of the i-th token in x.\nIn extractive question answering, a model is given a context passage c and question q, and must output a span of c that answers the question. Typically, models independently predict probability distributions p(astart | c, q) and p(aend | c, q) over the answer start index astart and end index aend."
    }, {
      "heading" : "2.2 Question Generation",
      "text" : "Question generation model. We train a BARTlarge model (Lewis et al., 2020) to generate question-answer pairs given context passages. The model receives the passage as context and must generate the answer text, then a special separator token, then the question; this approach is simpler than prior approaches that use separate models for answer and question generation (Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020), and works well in practice.\nTraining data. We train on data from the MRQA 2019 Shared Task (Fisch et al., 2019), which includes six datasets: HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), and TriviaQA (Joshi et al., 2017). These datasets cover many of the text sources commonly used for pre-training (Liu et al., 2019; Lewis et al., 2020), namely Wikipedia (HotpotQA, NaturalQuestions, SQuAD), News articles (NewsQA), and general web text (SearchQA, TriviaQA).\nGenerating questions. We run our question generation model over a large set of passages to generate a large dataset of question-answer pairs. We decode using nucleus sampling (Holtzman et al., 2020) with p = 0.6, which was chosen by manual inspection to balance diversity with quality of generated questions. We do not filter questions in any way. While we observed some flaws related to question quality (questions were not always well-formed) and diversity (for some passages, the same or very similar questions were asked multiple times), this approach nonetheless yielded good\ndownstream results. Attempts to mitigate these issues, such as using a two-stage beam search to ensure that questions for the same passage have different answers, did not noticeably change our downstream results (see §4.8). We obtain passages from the same training corpus as RoBERTa (Liu et al., 2019), which uses four sub-domains: BOOKCORPUS plus Wikipedia, CC-NEWS, OPENWEBTEXT, and STORIES. For each domain, we sample 2 million passages and generate 10 questions per passage, for a total of 80 million questions.1"
    }, {
      "heading" : "2.3 Teacher Re-labeling",
      "text" : "The answers generated by our BART model are not always accurate, nor are they always spans in the context passage. To improve the training signal, we relabel examples with a teacher model, as is common in knowledge distillation (Hinton et al., 2015). We use a standard cross-encoder RoBERTalarge model trained on the MRQA training data as our teacher. The model takes in the concatenation of the context passage c and question q and predicts astart and aend with two independent 2-layer multilayer perceptron (MLP) heads. Let Tstart(c, q) and Tend(c, q) denote the teacher’s predicted probability distribution over astart and aend, respectively."
    }, {
      "heading" : "2.4 Bi-encoder Training",
      "text" : "Finally, we train a bi-encoder model to match the cross-encoder predictions on the generated questions. This objective encourages the contextual representation for a token to have high similarity (in inner product space) with the representation of every question that is answered by that token.\nModel. The bi-encoder model with parameters θ consists of of three components: an encoder r and two question embedding heads hstart and hend that map Rd → Rd. These heads will only be applied to beginning-of-sequence (i.e., CLS) representations; as shorthand, define fstart(x) = hstart(r(x)1) and likewise for fend. Given a context passage c and question q, the model predicts\npθ(astart = i | c, q) ∝ er(c) ⊤ i fstart(q) (1)\npθ(aend = i | c, q) ∝ er(c) ⊤ i fend(q) (2)\nIn other words, the model independently encodes the passage and question with r, applies the start\n1We estimate that using the entire corpus with these settings would generate around 900 million questions. We leave investigation of further scaling to future work.\nand end heads to the CLS token embedding for q, then predicts the answer start (end) index with a softmax over the dot product between the passage representation at that index and the output of the start (end) head. We initialize r to be the pretrained RoBERTa-large model (Liu et al., 2019), which uses d = 1024. hstart and hend are randomlyinitialized 2-layer MLPs with hidden dimension 1024, matching the default initialization of classification heads in RoBERTa.\nTraining. For an input consisting of context c of length L and question q, we train θ to minimize the KL-divergence between the student and teacher predictions, which is equivalent to the objective\n− L∑ i=1 Tstart(c, q)i log pθ(astart = i | c, q)\n+ Tend(c, q)i log pθ(aend = i | c, q) (3)\nup to constants that do not depend on θ. We train for two epochs on the 80 million generated questions, which takes roughly 56 hours on 8 V100 GPUs, or roughly 19 GPU-days.2 For details about training, see Appendix A.1."
    }, {
      "heading" : "3 Downstream Tasks",
      "text" : "We evaluate QUIP on zero-shot paraphrase ranking, few-shot paraphrase classification, few-shot NER, and zero-shot sentiment analysis. We leverage the improved token-level representations afforded by QUIP, and in many cases also directly use QUIP’s question-answering abilities."
    }, {
      "heading" : "3.1 Paraphrase Ranking",
      "text" : "We first evaluate QUIP token-level representations by measuring their usefulness for zero-shot paraphrase ranking. In this task, systems must rank sentence pairs that are paraphrases above pairs that are non-paraphrases, without any task-specific training data. We compute similarity scores using the FBERT variant of BERTScore (Zhang et al., 2020), which measures cosine similarities between the representation of each token in one sentence and its most similar token in the other sentence. Given sentences x1 and x2 of lengths L1 and L2, define\nB(x1, x2) = 1\nL1 L1∑ i=1 max 1≤j≤L2 r(x1) ⊤ i r(x2)j ∥r(x1)i∥∥r(x2)j∥ .\n2For comparison, pre-training RoBERTa-large from scratch took roughly 5000 GPU-days (Liu et al., 2019)\nThe FBERT BERTScore is defined as the harmonic mean of B(x1, x2) and B(x2, x1). Zhang et al. (2020) showed that BERTScore with RoBERTa is useful for both natural language generation evaluation and paraphrase ranking. Since BERTScore uses token-level representations, we hypothesize that it should pair well with QUIP. As in Zhang et al. (2020), we use representations from the layer of the network that maximizes Pearson correlation between BERTScore and human judgments on the WMT16 metrics shared task (Bojar et al., 2016)."
    }, {
      "heading" : "3.2 Paraphrase Classification",
      "text" : "We next use either frozen or fine-tuned QUIP representations for few-shot paraphrase classification, rather than ranking. Through these experiments, we can compare QUIP with existing work on fewshot paraphrase classification.\nFrozen model. We train a logistic regression model that uses BERTScore with frozen representations as features. For a given pair of sentences, we extract eight features, corresponding to BERTScore computed with the final eight layers (i.e., layers 17- 24) of the network. These layers encompass the optimal layers for both RoBERTa-large and QUIP (see §4.4). Freezing the encoder is often useful in practice, particularly for large models, as the same model can be reused for many tasks (Brown et al., 2020; Du et al., 2020).\nFine-tuning. For fine-tuning, we use the same computation graph and logistic loss function, but now backpropagate through the parameters of our encoder. For details, see Appendix A.3."
    }, {
      "heading" : "3.3 Named Entity Recognition",
      "text" : "We also use QUIP for few-shot3 named entity recognition, which we frame as a BIO tagging task. Since questions in QA often ask for entities of a specific type, we expect QUIP representations to contain rich entity type information. We add a linear layer that takes in token-level representations and predicts the tag for each token, and backpropagate log loss through the entire network. By default, the output layer is initialized randomly.\nAs a refinement, we propose using question prompts to initialize this model. The output layer is parameterized by a T ×d matrix M , where T is the number of distinct BIO tags. The log-probability\n3Yang and Katiyar (2020) study few-shot NER assuming data from other NER datasets is available; we assume no such data is available, matching Huang et al. (2020).\nof predicting the j-th tag for token i is proportional to the dot product between the representation for token i and the j-th row of M ; this resembles how the bi-encoder predicts answers. Thus, we initialize each row of M with the start head embedding of a question related to that row’s corresponding entity tag. For instance, we initialize the parameters for the B-location and I-location tags with the embedding for “What is a location ?” We normalize the question embeddings to have unit L2 norm. This style of initialization is uniquely enabled by our bi-encoder QA model; it would be unclear how to use a language model or a crossencoder QA model similarly."
    }, {
      "heading" : "3.4 Zero-shot Sentiment Analysis",
      "text" : "Finally, we use QUIP for zero-shot binary sentiment analysis. We reduce sentiment analysis to QA by writing a pair of questions that ask for a reason why an item is good or bad (e.g., “Why is this movie [good/bad]?”). We predict the label whose corresponding question has higher similarity with the QUIP representation of some token in the input. This prompting strategy has the additional benefit of extracting rationales, namely the span that the QUIP model predicts as the answer to the question. While we focus on sentiment analysis, extractive rationales have been used for a wide range of NLP tasks (DeYoung et al., 2020), suggesting that this method could be applied more broadly.\nMore formally, let x be an input sentence and (q0, q1) be a pair of questions (i.e., a prompt). For label y ∈ {0, 1}, we compute a score for y as\nS(x, y) =max i\nr(x)⊤i fstart(qy)+\nmax i\nr(x)⊤i fend(qy). (4)\nThis formula is a straightforward way to measure the extent to which some span in x looks like the answer to the question qy, based on the model’s pre-trained ability to perform QA. We predict whichever y has the higher value of S(x, y)− Cy, where Cy is a calibration constant that offsets the model’s bias towards answering q0 or q1. Our inclusion of Cy is inspired by Zhao et al. (2021), who recommend calibrating zero-shot and few-shot models with a baseline derived from content-free inputs to account for biases towards a particular label. To choose Cy, we obtain a list W of the ten most frequent English words, all of which convey no sentiment, and define Cy as the mean over\nw ∈ W of S(w, y), i.e., the score when using w as the input sentence (see Appendix A.5)."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental details",
      "text" : "Datasets. For paraphrasing, we use four datasets: QQP (Iyer et al., 2017), MRPC (Dolan and Brockett, 2005), PAWS-Wiki, and PAWS-QQP (Zhang et al., 2019). The PAWS datasets were designed to be challenging for bag-of-words models, and thus test whether our representations are truly contextual or mostly lexical. For QQP and MRPC, we use the few-shot splits from Gao et al. (2021) that include 16 examples per class; for the PAWS datasets, we create new few-shot splits in the same manner. We report results on the development sets of QQP and MRPC (as test labels were not available), the test set of PAWS-Wiki, and the “dev-and-test” set of PAWS-QQP. For NER, we use two datasets: CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and WNUT-17 (Derczynski et al., 2017). We use the few-shot splits from Huang et al. (2020) that include 5 examples per entity type. All few-shot experiments report an average over five random splits and seeds, following both Gao et al. (2021) and Huang et al. (2020). For sentiment analysis, we use two movie review datasets, SST-2 (Socher et al., 2013) and Movie Reviews (MR; Pang and Lee, 2005), as well as the Customer Reviews (CR) dataset (Hu and Liu, 2004). We evaluate on the SST-2 development set and the MR and CR test sets made by Gao et al. (2021).\nHyperparameter and prompt selection. Due to the nature of zero-shot and few-shot experiments, we minimize the extent to which we tune hyperparameters, relying on existing defaults and previously published hyperparameters. For few-shot paraphrase classification, NER, and sentiment analysis, we developed our final method only using QQP, CoNLL, and SST-2, respectively, and directly applied it to the other datasets with no further tuning. We did measure zero-shot paraphrase ranking accuracy on all datasets during development of QUIP. For more details, see Appendix A.4.\nFor NER, we used the first question prompts we wrote for both CoNLL and WNUT, which all follow the same format, “Who/What is a/an [entity type] ?” (see Appendix A.8 for all prompts). For sentiment analysis, we wrote six prompts (shown in Appendix A.10) and report mean accuracy over\nthese prompts, to avoid pitfalls associated with prompt tuning (Perez et al., 2021). We use the same prompts for SST-2 and MR; for CR, the only change we make is replacing occurrences of the word “movie” with “product” to reflect the change in domain between these datasets."
    }, {
      "heading" : "4.2 Baselines and Ablations",
      "text" : "To confirm the importance of all three stages of our pre-training pipeline, we compare with a number of baselines and ablations.\nNo question generation. We train the bi-encoder model directly on the MRQA training data (“Biencoder + MRQA”). We also include the crossencoder teacher model trained on MRQA as a baseline (“Cross-encoder + MRQA”). These settings mirror standard intermediate task training (Phang et al., 2018; Pruksachatkun et al., 2020).\nNo teacher. We train the bi-encoder using the answer generated by the question generation model (“QUIP, no teacher”). If the generated answer is not a span in the passage, we consider the question unanswerable and treat the span containing the CLS token as the answer, as in Devlin et al. (2019).\nCross-encoder self-training. To test whether the bottleneck imposed by the bi-encoder architecture is crucial for QUIP, we also train a crossencoder model on our generated data (“QUIP, cross-encoder student”). Since this student model has the same architecture as the teacher model, we train it to match the teacher’s argmax predictions, a standard self-training objective (Lee, 2013). We train for a comparable number of GPU-hours as QUIP (see Appendix A.2 for details).\nUnsupervised QA. We test whether QUIP requires real QA data, or if a rough approximation suffices. We thus train a bi-encoder on 80 million pseudo-questions generated by applying noise to sentences (“Bi-encoder + UnsupervisedQA”), as in Lewis et al. (2019)."
    }, {
      "heading" : "4.3 Bi-encoder Question Answering",
      "text" : "While not our main focus, we first check that QUIP improves bi-encoder QA accuracy, as shown in Table 1. QUIP improves over Lee et al. (2021) by 5.4 F1 on the SQuAD development set. It also surpasses the reported human accuracy of 91.2 F1 on the SQuAD test set, as well as the best crossencoder BERT-large single model from Devlin et al. (2019). QUIP greatly improves over baselines that\ndirectly train on MRQA data or do not use the teacher model. As expected, our cross-encoder models are more accurate at QA; we will show that their representations are less useful for non-QA tasks than the QUIP bi-encoder. Appendix A.6 shows results on all MRQA development datasets."
    }, {
      "heading" : "4.4 Zero-shot Paraphrase Ranking",
      "text" : "We validate our approach and study the effects of various ablations on zero-shot paraphrase ranking. The first half of Table 2 shows WMT development set Pearson correlations averaged across six to-English datasets, as in Zhang et al. (2020), along with the best layer for each model. QUIP reaches its optimal score at a later layer (20) than RoBERTa-large (17), which may suggest that the QUIP training objective is more closely aligned with learning better representations than MLM.\nThe rest of Table 2 shows zero-shot paraphrase ranking results using BERTScore. QUIP improves substantially over RoBERTa on all four datasets, with an average improvement of .076 AUROC. The improvement is greatest on the PAWS datasets; since these datasets cannot be solved by lexical features alone, QUIP representations must be much more contextualized than RoBERTa representations. Training on Unsupervised QA data degrades performance compared to RoBERTa, showing that QUIP does not merely make word representations encode local context in a simple way. Training the bi-encoder directly on the MRQA dataset or without the teacher improves on average over RoBERTa, but QUIP greatly outperforms both baselines. The cross-encoder models also lag behind QUIP at paraphrase ranking, despite their higher QA accuracy. Thus, we conclude that having real questions, accu-\nrate answer supervision, and a bi-encoder student model are all crucial to the success of QUIP."
    }, {
      "heading" : "4.5 Paraphrase Classification",
      "text" : "Table 3 shows few-shot paraphrase classification results. As we studied ablations in the previous section, we focus on the comparison between QUIP and non-QUIP baselines. First, we use RoBERTalarge embeddings in place of QUIP in our method. Second, we compare with LM-BFF (Gao et al., 2021), which pairs RoBERTa-large with MLMstyle prompts. We use LM-BFF with manually written prompts and demonstrations, which was their best method on QQP by 2.1 F1 and was 0.3 F1 worse than their best method on MRPC. QUIP used as a frozen encoder is competitive with LM-BFF on QQP and outperforms it by 6.1 F1 on MRPC, 11.2 F1 on PAWS-Wiki, and 12.1 F1 on PAWS-QQP. Fine-tuning QUIP gives additional improvements on three of the four datasets, and outperforms finetuning RoBERTa by an average of 6.9 F1."
    }, {
      "heading" : "4.6 Named Entity Recognition",
      "text" : "Table 4 shows few-shot NER results on the CoNLL and WNUT datasets. QUIP improves over RoBERTa-large by 11 F1 on CoNLL and 2.9 F1 on WNUT when used with a randomly initialized output layer. We see a further improvement of 4 F1 on CoNLL and 7.4 F1 on WNUT when using question embeddings to initialize the output layer. Using the cross-encoder trained directly on QA data is roughly as good as QUIP when using randomly initialized output layers, but it is incompatible with question embedding initialization."
    }, {
      "heading" : "4.7 Sentiment Analysis",
      "text" : "Table 5 shows zero-shot accuracy on our three sentiment analysis datasets. We compare with zeroshot results for LM-BFF (Gao et al., 2021)4 and reported zero-shot results from Zhao et al. (2021) using GPT-3 with Contextual Calibration (CC) on SST-2. QUIP using an average prompt outperforms zero-shot LM-BFF by 5.4 points, averaged across the three datasets. Choosing the best prompt on SST-2 and using that for all datasets improves results not only on SST-2 but also MR, and maintains average accuracy on CR. Using the cross-encoder student QA model with the same prompts leads to worse performance: we hypothesize that the biencoder bottleneck encourages QUIP to make each\n4We tried applying our calibration strategy to LM-BFF as well, but found that it did not improve accuracy.\nspan’s representation dissimilar to those of questions that it does not answer, whereas the crossencoder model trained only on answerable questions handles unanswerable questions poorly (e.g., “Why is it bad?” asked about a positive review).\nTable 6 shows rationales extracted from random SST-2 examples for which QUIP was correct with the best prompt for SST-2 (“What is the reason this movie is [good/bad]?”). To prefer shorter rationales, we extract the highest-scoring span of five BPE tokens or less. The model often identifies phrases that convey clear sentiment. Appendix A.11 shows full examples and rationales."
    }, {
      "heading" : "4.8 Stability Analysis",
      "text" : "We experimented with some design decisions that did not materially affect our results. Appendix A.7 shows results for three such choices: including inbatch negative passages (Lee et al., 2021), using the argmax prediction of the teacher rather than soft labels, and using beam search to generate a diverse set of answers followed by one high-likelihood\nquestion per answer. We take these findings as evidence that our basic recipe is stable to many small changes. For question generation, we hypothesize that the objective of matching the cross-encoder teacher model encourages the bi-encoder to learn important features identified by the cross-encoder, even on questions that are not entirely well-formed."
    }, {
      "heading" : "5 Discussion and Related Work",
      "text" : "We build on work in question generation and answering, pre-training, and few-shot learning.\nQuestion Generation. Neural question generation has been well-studied for different purposes (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021; Bartolo et al., 2021). We use generated questions to learn generalpurpose representations. We also show that a relatively simple strategy of generating the answer and question together with a single model can be effective; most prior work uses separate answer selection and question generation models.\nPhrase-indexed Question Answering. Phraseindexed question answering is a paradigm for opendomain QA that retrieves answers by embedding questions and candidate answers in a shared embedding space (Seo et al., 2018, 2019; Lee et al., 2020). It requires using a bi-encoder architecture for efficient phrase retrieval. Especially related is Lee et al. (2021), which also uses question generation and a cross-encoder teacher model to improve phraseindexed QA, though they focus on improving QA accuracy rather than transfer to other tasks. Our results reinforce prior observations that bi-encoder models are usually less accurate at QA than crossencoders (see Table 1). However, the bi-encoder model transfers better to settings that require a contextualized representation of a single passage; the cross-encoder instead optimizes for producing representations of passage-question pairs.\nImproving question answering. While we use QA to aid pre-training, related work aims to improve accuracy on QA. Ram et al. (2021) propose a span extraction pre-training objective that enables few-shot QA. Khashabi et al. (2020) run multi-task training on many QA datasets, both extractive and non-extractive, to improve QA accuracy.\nLearning contextual representations. Pretraining on unlabeled data has yields useful\ncontextual representations (Peters et al., 2018; Devlin et al., 2019), but further improvements are possible using labeled data. Intermediate task training (Phang et al., 2018) improves representations by training directly on large labeled datasets. Muppet (Aghajanyan et al., 2021) improves models by multi-task pre-finetuning on many labeled datasets. Most similar to our work, He et al. (2020) uses extractive QA to pre-train a BERT paragraph encoder. We use question generation and knowledge distillation to improve over directly training on labeled data, and focus on zero- and few-shot settings. Other work has used similar methods to learn sentence embeddings. Reimers and Gurevych (2019) train sentence embeddings for sentence similarity tasks using natural language inference data. Thakur et al. (2021) train a sentence embedding bi-encoder to mimic the predictions of a cross-encoder model. We learn token-level representations, rather than a single vector for a sentence, and thus use token-level supervision from extractive QA.\nFew-shot learning. We study few-shot learning without access to unlabeled data, following most recent work (Brown et al., 2020; Gao et al., 2021; Zhao et al., 2021). Schick and Schütze (2021) notably propose a semi-supervised approach that uses unlabeled data for knowledge distillation; this process does not improve accuracy, but mainly improves efficiency. Moreover, large-scale unlabeled data may not be easily obtainable for all tasks, and utilizing such data increase computation time in the fine-tuning stage, so we focus on the setting without unlabeled data. The aforementioned work uses language models for few-shot learning by converting tasks to language modeling problems; we develop alternative methods for few-shot learning that use token-level representations and questionbased prompts."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we pre-trained token-level contextual representations that are useful for downstream fewshot learning. Our key idea was to use questionanswer pairs to define what information should be encoded in bi-encoder passage representations. We showed that passage representations learned in this way are useful for a variety of standard NLP tasks in zero- and few-shot settings, namely paraphrase detection, named entity recognition, and sentiment analysis, across nine total datasets."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 QUIP Details",
      "text" : "For efficiency, we process all questions for the same passage in the same batch, as encoding passages dominates runtime. We limit passages to 456 bytepair encoding (BPE) tokens and questions to 50 so\nthat the concatenation can fit comfortably within the 512 token context usable by the cross-encoder teacher. We create passages from our unlabeled text corpus by greedily selecting maximal chunks of contiguous sentences that fit within the BPE token limit. We pre-compute the teacher predictions Tstart and Tend before bi-encoder training. To save space, we sparsify these vectors by only storing the eight largest predicted probabilities, treating all others as 0.\nWe conducted minimal hyperparameter tuning for QUIP. We used a learning rate of 1 · 10−5 (default for most RoBERTa fine-tuning experiments5) and no gradient accumulation, which we found led to faster training."
    }, {
      "heading" : "A.2 Cross-encoder Self-training Details",
      "text" : "Training is much less efficient for the cross-encoder than the bi-encoder, since batching questions about the same passage together does not speed up training, so we train for a comparable number of GPUhours (60 hours on 8 V100 GPUs)."
    }, {
      "heading" : "A.3 Paraphrase Fine-tuning Details",
      "text" : "To fine-tune our model for paraphrase classification, we use two practices recommended by Mussmann et al. (2020), who also train a binary classification model that uses cosine similarity-based features derived from fine-tuned BERT embeddings. First, we disable dropout during training, as dropout artificially lowers all cosine similarities. Second, we use a larger learning rate on the final output layer than the Transformer parameters, by a factor of 103."
    }, {
      "heading" : "A.4 Downstream Task Hyperparameter Details",
      "text" : "For few-shot paraphrase detection with the frozen model, we use Scikit-learn’s logistic regression implementation with default settings (Pedregosa et al., 2011). For fine-tuned paraphrase detection, we again use a learning rate of 1 ·10−5 and train for 20 epochs, which we found to usually be sufficient for convergence on the training data. For NER, we use the default hyperparameters from the Huggingface transformers repository (Wolf et al., 2020), with the exception of decreasing the learning rate from 5 ·10−5 to 2 ·10−5, which we found improved the RoBERTa baseline on CoNLL.\n5https://github.com/pytorch/fairseq/ tree/master/examples/roberta"
    }, {
      "heading" : "A.5 Sentiment Analysis Calibration",
      "text" : "To calibrate the zero-shot sentiment analysis model, we use ten content-free inputs: “the”, “be”, “to”, “of ”, “and”, “a”, “in”, “that”, “have”, and “I”. These were the top ten words listed on https://en.wikipedia.org/wiki/ Most_common_words_in_English. We only applied calibration for the main QUIP model, as we did not find calibration to improve results for either LM-BFF or the cross-encoder QA student model."
    }, {
      "heading" : "A.6 Full QA results",
      "text" : "Table 7 shows EM and F1 scores on the 12 development sets from the MRQA 2019 Shared Task (Fisch et al., 2019). These are divided into 6 in-domain datasets—HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), and TriviaQA (Joshi et al., 2017)—for which corresponding training data was used to train the question generation model and teacher, and 6 out-of-domain datasets—BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RACE (Lai et al., 2017), RelationExtraction (Levy et al., 2017), and TextbookQA (Kembhavi et al., 2017)—for which no training data was used in the QUIP pipeline. QUIP improves over training the bi-encoder directly on the MRQA data by an average of 4.4 F1 on the in-domain datasets and 12.7 F1 on the outof-domain datasets. It underperforms the crossencoder teacher by about 5 F1 on both the indomain and out-of-domain datasets on average."
    }, {
      "heading" : "A.7 Stability Analysis",
      "text" : "We experimented with some design decisions that did not materially affect our results. Here, we report these findings as evidence that our basic recipe is stable to many small changes. First, we concatenated the representations of all passages in the same batch and on the same GPU together (9 passages on average), and trained the model to extract answers from this larger pseudo-document; this effectively adds in-batch negative passages, as in Lee et al. (2021). Second, we trained the model to match the argmax prediction of the teacher, rather than its soft distribution over start and end indices. Finally, we used a two-stage beam search to generate questions. For a given passage, we generated\n20 possible answers via beam search, chose 10 of these to maximize answer diversity, then generated one question for each answer with another beam search. Our goal was to ensure diversity by forcing questions to be about different answers, while also maintaining high question quality. As shown in Table 8, these choices have a relatively minor impact on the results (within .007 AUROC and 1 F1 on NER)."
    }, {
      "heading" : "A.8 QA Prompts for NER",
      "text" : "Table 9 shows the question prompts we use to initialize the NER model for CoNLL and WNUT. For entity types that occur in both datasets, and for the O tag, we always use the same question. We used the English description of the entity type provided by the dataset."
    }, {
      "heading" : "A.9 Full training set NER",
      "text" : "Table 10 shows NER results when training on the full training dataset. QUIP gives a 0.6 F1 improvement on WNUT, but has effectively the same accuracy on CoNLL."
    }, {
      "heading" : "A.10 Sentiment Analysis QA Prompts",
      "text" : "Table 11 shows the six prompts we use for sentiment analysis for the movie review datasets (SST-2 and MR). Each prompt consists of one question for the positive label and one for the negative label. For CR, we use the same prompts except that we replace all instances of the word “movie” with “product”."
    }, {
      "heading" : "A.11 Sentiment Analysis Rationales",
      "text" : "Tables 12, 13, and 14 show full examples and rationales extracted by our zero-shot sentiment analysis\nmethod for SST-2, MR, and CR, respectively. In all cases, we use the prompt that led to the highest accuracy on SST-2. For each dataset, we randomly sample ten examples of each label for which the model predicted the correct answer. We highlight in bold the span of ≤ 5 BPE tokens that the model predicts best answers the question associated with the correct label. In some cases, the rationales correspond to clear sentiment markers. In other cases, they highlight an aspect of a movie or product that is criticized or praised in the review; these could be considered reasonable answers to a question like “Why is this movie bad?” even if the sentiment associated with them is unclear without the surrounding context. In future work, it would be interesting to find better ways to align the task of extractive QA and with the goal of producing rationales that are human-interpretable in isolation."
    } ],
    "references" : [ {
      "title" : "Muppet: Massive multi-task representations with pre-finetuning",
      "author" : [ "Armen Aghajanyan", "Anchit Gupta", "Akshat Shrivastava", "Xilun Chen", "Luke Zettlemoyer", "Sonal Gupta." ],
      "venue" : "arXiv preprint arXiv:2101.11038.",
      "citeRegEx" : "Aghajanyan et al\\.,? 2021",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Synthetic QA corpora generation with roundtrip consistency",
      "author" : [ "Chris Alberti", "Daniel Andor", "Emily Pitler", "Jacob Devlin", "Michael Collins." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173, Flo-",
      "citeRegEx" : "Alberti et al\\.,? 2019",
      "shortCiteRegEx" : "Alberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving question answering model robustness with synthetic adversarial data generation",
      "author" : [ "Max Bartolo", "Tristan Thrush", "Robin Jia", "Sebastian Riedel", "Pontus Stenetorp", "Douwe Kiela." ],
      "venue" : "arXiv preprint arXiv:2104.08678.",
      "citeRegEx" : "Bartolo et al\\.,? 2021",
      "shortCiteRegEx" : "Bartolo et al\\.",
      "year" : 2021
    }, {
      "title" : "Results of the WMT16 metrics shared task",
      "author" : [ "Ondřej Bojar", "Yvette Graham", "Amir Kamran", "Miloš Stanojević." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 199–231, Berlin, Germany. Association",
      "citeRegEx" : "Bojar et al\\.,? 2016",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2016
    }, {
      "title" : "Results of the WNUT2017 shared task on novel and emerging entity recognition",
      "author" : [ "Leon Derczynski", "Eric Nichols", "Marieke van Erp", "Nut Limsopatham." ],
      "venue" : "Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 140–147, Copenhagen,",
      "citeRegEx" : "Derczynski et al\\.,? 2017",
      "shortCiteRegEx" : "Derczynski et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "ERASER: A benchmark",
      "author" : [ "Jay DeYoung", "Sarthak Jain", "Nazneen Fatema Rajani", "Eric Lehman", "Caiming Xiong", "Richard Socher", "Byron C. Wallace" ],
      "venue" : null,
      "citeRegEx" : "DeYoung et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "DeYoung et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B. Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "General purpose text embeddings from pre-trained language models for scalable inference",
      "author" : [ "Jingfei Du", "Myle Ott", "Haoran Li", "Xing Zhou", "Veselin Stoyanov." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3018–",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Harvesting paragraph-level question-answer pairs from Wikipedia",
      "author" : [ "Xinya Du", "Claire Cardie." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1907–1917, Mel-",
      "citeRegEx" : "Du and Cardie.,? 2018",
      "shortCiteRegEx" : "Du and Cardie.",
      "year" : 2018
    }, {
      "title" : "Learning to ask: Neural question generation for reading comprehension",
      "author" : [ "Xinya Du", "Junru Shao", "Claire Cardie." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342–1352,",
      "citeRegEx" : "Du et al\\.,? 2017",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2017
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "SearchQA: A new Q&A dataset augmented with context from a search engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V. Ugur Guney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1704.05179.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
      "author" : [ "Adam Fisch", "Alon Talmor", "Robin Jia", "Minjoon Seo", "Eunsol Choi", "Danqi Chen." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages",
      "citeRegEx" : "Fisch et al\\.,? 2019",
      "shortCiteRegEx" : "Fisch et al\\.",
      "year" : 2019
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "Association for Computational Linguistics (ACL). 9",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Question answering is a format; when is it useful? arXiv preprint arXiv:1909.11291",
      "author" : [ "Matt Gardner", "Jonathan Berant", "Hannaneh Hajishirzi", "Alon Talmor", "Sewon Min" ],
      "venue" : null,
      "citeRegEx" : "Gardner et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2019
    }, {
      "title" : "QuASE: Question-answer driven sentence encoding",
      "author" : [ "Hangfeng He", "Qiang Ning", "Dan Roth." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8743– 8758, Online. Association for Computational Lin-",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
      "author" : [ "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NeurIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Few-shot named entity recognition: A comprehensive study",
      "author" : [ "Jiaxin Huang", "Chunyuan Li", "Krishan Subudhi", "Damien Jose", "Shobana Balakrishnan", "Weizhu Chen", "Baolin Peng", "Jianfeng Gao", "Jiawei Han." ],
      "venue" : "arXiv preprint arXiv:2012.14978.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "First quora dataset release: Question pairs",
      "author" : [ "Shankar Iyer", "Nikhil Dandekar", "Kornél Csernai." ],
      "venue" : "https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs.",
      "citeRegEx" : "Iyer et al\\.,? 2017",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2017
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
      "author" : [ "Aniruddha Kembhavi", "Minjoon Seo", "Dustin Schwenk", "Jonghyun Choi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "2017 IEEE Conference on Com-",
      "citeRegEx" : "Kembhavi et al\\.,? 2017",
      "shortCiteRegEx" : "Kembhavi et al\\.",
      "year" : 2017
    }, {
      "title" : "UNIFIEDQA: Crossing format boundaries with a single QA system",
      "author" : [ "Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Clark", "Hannaneh Hajishirzi." ],
      "venue" : "Findings of the Association for Computational Linguistics:",
      "citeRegEx" : "Khashabi et al\\.,? 2020",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:452–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "RACE: Large-scale ReAding comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785–",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "author" : [ "Dong-Hyun Lee." ],
      "venue" : "ICML 2013 Workshop on Challenges in Representation Learning (WREPL).",
      "citeRegEx" : "Lee.,? 2013",
      "shortCiteRegEx" : "Lee.",
      "year" : 2013
    }, {
      "title" : "Contextualized sparse representations for real-time open-domain question answering",
      "author" : [ "Jinhyuk Lee", "Minjoon Seo", "Hannaneh Hajishirzi", "Jaewoo Kang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning dense representations of phrases at scale",
      "author" : [ "Jinhyuk Lee", "Mujeen Sung", "Jaewoo Kang", "Danqi Chen." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Lee et al\\.,? 2021",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "Zero-shot relation extraction via reading comprehension",
      "author" : [ "Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342, Vancouver,",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "Generative question answering: Learning to answer the whole question",
      "author" : [ "Mike Lewis", "Angela Fan." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lewis and Fan.,? 2019",
      "shortCiteRegEx" : "Lewis and Fan.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised question answering by cloze translation",
      "author" : [ "Patrick Lewis", "Ludovic Denoyer", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 10",
      "citeRegEx" : "Lewis et al\\.,? 2019",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Paq: 65 million probably-asked questions and what you can do with them",
      "author" : [ "Patrick Lewis", "Yuxiang Wu", "Linqing Liu", "Pasquale Minervini", "Heinrich Küttler", "Aleksandra Piktus", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "arXiv preprint arXiv:2102.07033.",
      "citeRegEx" : "Lewis et al\\.,? 2021",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2021
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The natural language decathlon: Multitask learning as question answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "Crowdsourcing question-answer meaning representations",
      "author" : [ "Julian Michael", "Gabriel Stanovsky", "Luheng He", "Ido Dagan", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Michael et al\\.,? 2018",
      "shortCiteRegEx" : "Michael et al\\.",
      "year" : 2018
    }, {
      "title" : "On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks",
      "author" : [ "Stephen Mussmann", "Robin Jia", "Percy Liang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3400–3413, Online. Association",
      "citeRegEx" : "Mussmann et al\\.,? 2020",
      "shortCiteRegEx" : "Mussmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 115–124, Ann",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "True few-shot learning with language models",
      "author" : [ "Ethan Perez", "Douwe Kiela", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:2105.11447.",
      "citeRegEx" : "Perez et al\\.,? 2021",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Intermediate-task transfer learning with pretrained language models: When and why does it",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Training question answering models from synthetic data",
      "author" : [ "Raul Puri", "Ryan Spring", "Mohammad Shoeybi", "Mostofa Patwary", "Bryan Catanzaro." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Puri et al\\.,? 2020",
      "shortCiteRegEx" : "Puri et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Few-shot question answering by pretraining span selection",
      "author" : [ "Ori Ram", "Yuval Kirstain", "Jonathan Berant", "Amir Globerson", "Omer Levy." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ram et al\\.,? 2021",
      "shortCiteRegEx" : "Ram et al\\.",
      "year" : 2021
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "DuoRC: Towards complex language understanding with paraphrased reading comprehension",
      "author" : [ "Amrita Saha", "Rahul Aralikatte", "Mitesh M. Khapra", "Karthik Sankaranarayanan." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Saha et al\\.,? 2018",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2018
    }, {
      "title" : "It’s not just size that matters: Small language models are also fewshot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Schick and Schütze.,? 2021",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "Phraseindexed question answering: A new challenge for scalable document comprehension",
      "author" : [ "Minjoon Seo", "Tom Kwiatkowski", "Ankur Parikh", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings 11",
      "citeRegEx" : "Seo et al\\.,? 2018",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2018
    }, {
      "title" : "Real-time open-domain question answering with dense-sparse phrase index",
      "author" : [ "Minjoon Seo", "Jinhyuk Lee", "Tom Kwiatkowski", "Ankur Parikh", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Seo et al\\.,? 2019",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on Empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks",
      "author" : [ "Nandan Thakur", "Nils Reimers", "Johannes Daxenberger", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter",
      "citeRegEx" : "Thakur et al\\.,? 2021",
      "shortCiteRegEx" : "Thakur et al\\.",
      "year" : 2021
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "NewsQA: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191–200,",
      "citeRegEx" : "Trischler et al\\.,? 2017",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2017
    }, {
      "title" : "An overview of the bioasq large-scale biomedical",
      "author" : [ "las Baskiotis", "Patrick Gallinari", "Thierry Artieres", "Axel Ngonga", "Norman Heino", "Eric Gaussier", "Liliana Barrio-Alvers", "Michael Schroeder", "Ion Androutsopoulos", "Georgios Paliouras" ],
      "venue" : null,
      "citeRegEx" : "Baskiotis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Baskiotis et al\\.",
      "year" : 2015
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association",
      "citeRegEx" : "Lhoest and Rush.,? 2020",
      "shortCiteRegEx" : "Lhoest and Rush.",
      "year" : 2020
    }, {
      "title" : "Self-training with noisy student improves imagenet classification",
      "author" : [ "Qizhe Xie", "Minh-Thang Luong", "Eduard Hovy", "Quoc V. Le." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple and effective few-shot named entity recognition with structured nearest neighbor learning",
      "author" : [ "Yi Yang", "Arzoo Katiyar." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6365–6375,",
      "citeRegEx" : "Yang and Katiyar.,? 2020",
      "shortCiteRegEx" : "Yang and Katiyar.",
      "year" : 2020
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empiri-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "BERTScore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "PAWS: Paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Calibrate before use: Improving few-shot performance of language models",
      "author" : [ "Tony Z. Zhao", "Eric Wallace", "Shi Feng", "Dan Klein", "Sameer Singh." ],
      "venue" : "arXiv preprint arXiv:2102.09690.",
      "citeRegEx" : "Zhao et al\\.,? 2021",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "Paragraph-level neural question generation with maxout pointer and gated self-attention networks",
      "author" : [ "Yao Zhao", "Xiaochuan Ni", "Yuanyuan Ding", "Qifa Ke." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 43,
      "context" : "Although masked language models build contextualized word representations, they are pre-trained with losses that minimize distance to uncontextualized word embeddings (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 5,
      "context" : "Although masked language models build contextualized word representations, they are pre-trained with losses that minimize distance to uncontextualized word embeddings (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 36,
      "context" : "Although masked language models build contextualized word representations, they are pre-trained with losses that minimize distance to uncontextualized word embeddings (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 17,
      "context" : "have been used as broad-coverage meaning representations (He et al., 2015; Michael et al., 2018), and a wide range of NLP tasks can be cast as QA problems (Levy et al.",
      "startOffset" : 57,
      "endOffset" : 96
    }, {
      "referenceID" : 38,
      "context" : "have been used as broad-coverage meaning representations (He et al., 2015; Michael et al., 2018), and a wide range of NLP tasks can be cast as QA problems (Levy et al.",
      "startOffset" : 57,
      "endOffset" : 96
    }, {
      "referenceID" : 60,
      "context" : "contextual representations via knowledge distillation, as self-training can be effective when the student model must solve a harder problem than the teacher (Xie et al., 2020).",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 63,
      "context" : "For few-shot paraphrase detection, QUIP with BERTScore-based features (Zhang et al., 2020)",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 33,
      "context" : "We train a BARTlarge model (Lewis et al., 2020) to generate question-answer pairs given context passages.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 32,
      "context" : "token, then the question; this approach is simpler than prior approaches that use separate models for answer and question generation (Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020), and works well in practice.",
      "startOffset" : 133,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "token, then the question; this approach is simpler than prior approaches that use separate models for answer and question generation (Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020), and works well in practice.",
      "startOffset" : 133,
      "endOffset" : 195
    }, {
      "referenceID" : 46,
      "context" : "token, then the question; this approach is simpler than prior approaches that use separate models for answer and question generation (Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020), and works well in practice.",
      "startOffset" : 133,
      "endOffset" : 195
    }, {
      "referenceID" : 13,
      "context" : "We train on data from the MRQA 2019 Shared Task (Fisch et al., 2019), which includes six datasets: HotpotQA (Yang et al.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 62,
      "context" : ", 2019), which includes six datasets: HotpotQA (Yang et al., 2018),",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 57,
      "context" : ", 2019), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : ", 2017), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 47,
      "context" : ", 2017), SQuAD (Rajpurkar et al., 2016), and TriviaQA (Joshi et al.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 36,
      "context" : "These datasets cover many of the text sources commonly used for pre-training (Liu et al., 2019; Lewis et al., 2020), namely Wikipedia (HotpotQA, NaturalQuestions, SQuAD), News articles (NewsQA), and general web text (SearchQA, TriviaQA).",
      "startOffset" : 77,
      "endOffset" : 115
    }, {
      "referenceID" : 33,
      "context" : "These datasets cover many of the text sources commonly used for pre-training (Liu et al., 2019; Lewis et al., 2020), namely Wikipedia (HotpotQA, NaturalQuestions, SQuAD), News articles (NewsQA), and general web text (SearchQA, TriviaQA).",
      "startOffset" : 77,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "We decode using nucleus sampling (Holtzman et al., 2020) with p = 0.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 36,
      "context" : "We obtain passages from the same training corpus as RoBERTa (Liu et al., 2019), which uses four sub-domains: BOOKCORPUS plus Wikipedia, CC-NEWS, OPENWEBTEXT, and STORIES.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "To improve the training signal, we relabel examples with a teacher model, as is common in knowledge distillation (Hinton et al., 2015).",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 36,
      "context" : "We initialize r to be the pretrained RoBERTa-large model (Liu et al., 2019), which uses d = 1024.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 63,
      "context" : "We compute similarity scores using the FBERT variant of BERTScore (Zhang et al., 2020), which measures cosine similarities between the representation of each token in one sentence and its most similar token in the other sentence.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 36,
      "context" : "For comparison, pre-training RoBERTa-large from scratch took roughly 5000 GPU-days (Liu et al., 2019)",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "(2020), we use representations from the layer of the network that maximizes Pearson correlation between BERTScore and human judgments on the WMT16 metrics shared task (Bojar et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 187
    }, {
      "referenceID" : 8,
      "context" : "model can be reused for many tasks (Brown et al., 2020; Du et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "While we focus on sentiment analysis, extractive rationales have been used for a wide range of NLP tasks (DeYoung et al., 2020), suggesting that this method could be applied more broadly.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "For paraphrasing, we use four datasets: QQP (Iyer et al., 2017), MRPC (Dolan and Brockett, 2005), PAWS-Wiki, and PAWS-QQP (Zhang et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : ", 2017), MRPC (Dolan and Brockett, 2005), PAWS-Wiki, and PAWS-QQP (Zhang et al.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 64,
      "context" : ", 2017), MRPC (Dolan and Brockett, 2005), PAWS-Wiki, and PAWS-QQP (Zhang et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "For NER, we use two datasets: CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and WNUT-17 (Derczynski et al., 2017).",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 54,
      "context" : "we use two movie review datasets, SST-2 (Socher et al., 2013) and Movie Reviews (MR; Pang and Lee, 2005), as well as the Customer Reviews (CR) dataset (Hu and Liu, 2004).",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 40,
      "context" : ", 2013) and Movie Reviews (MR; Pang and Lee, 2005), as well as the Customer Reviews (CR) dataset (Hu and Liu, 2004).",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 20,
      "context" : ", 2013) and Movie Reviews (MR; Pang and Lee, 2005), as well as the Customer Reviews (CR) dataset (Hu and Liu, 2004).",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 42,
      "context" : "10) and report mean accuracy over these prompts, to avoid pitfalls associated with prompt tuning (Perez et al., 2021).",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 28,
      "context" : "Since this student model has the same architecture as the teacher model, we train it to match the teacher’s argmax predictions, a standard self-training objective (Lee, 2013).",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 14,
      "context" : "Second, we compare with LM-BFF (Gao et al., 2021), which pairs RoBERTa-large with MLMstyle prompts.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "We compare with zeroshot results for LM-BFF (Gao et al., 2021)4 and reported zero-shot results from Zhao et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "QUIP outperforms prior work (LM-BFF; Gao et al., 2021) as well as our own RoBERTa baselines.",
      "startOffset" : 28,
      "endOffset" : 54
    }, {
      "referenceID" : 30,
      "context" : "7 shows results for three such choices: including inbatch negative passages (Lee et al., 2021), using the argmax prediction of the teacher rather than soft labels, and using beam search to generate a diverse set of answers followed by one high-likelihood",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 43,
      "context" : "Pretraining on unlabeled data has yields useful contextual representations (Peters et al., 2018; Devlin et al., 2019), but further improvements",
      "startOffset" : 75,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "Pretraining on unlabeled data has yields useful contextual representations (Peters et al., 2018; Devlin et al., 2019), but further improvements",
      "startOffset" : 75,
      "endOffset" : 117
    }, {
      "referenceID" : 44,
      "context" : "Intermediate task training (Phang et al., 2018) improves representations by training directly on large labeled datasets.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Muppet (Aghajanyan et al., 2021) improves models by multi-task pre-finetuning",
      "startOffset" : 7,
      "endOffset" : 32
    } ],
    "year" : 0,
    "abstractText" : "We propose a pre-training objective based on question answering (QA) for learning generalpurpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. To this end, we train a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder’s token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) finetuning. We show large improvements over both RoBERTa-large and previous state-of-the-art results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_350_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Cross-Lingual IR from an English Retriever",
    "authors" : [ ],
    "emails" : [ "Recall@5kt" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multilingual models are critical for the democratization of AI. Cross-lingual information retrieval (CLIR) (Braschler et al., 1999; Shakery and Zhai, 2013; Jiang et al., 2020; Asai et al., 2021a), for example, can find relevant text in a high-resource language such as English even when the query is posed in a different, possibly low-resource, language. In this work, we develop useful CLIR models for this constrained, yet important, setting where a retrieval corpus is available only in a single high-resource language (English in our experiments).\nA straightforward solution to this problem can be based on machine translation (MT) of the query into English, followed by English IR (Asai et al., 2021a). While this two-stage process is capable of providing accurate predictions, an alternative end-to-end approach that can tackle the problem purely cross-lingually, i.e., without involving MT, would clearly be more efficient and cost-effective. Pre-trained multilingual masked language models\n(PLMs) such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (XLM-R) (Conneau et al., 2020) can provide the foundation for such an approach, as one can simply fine-tune a PLM with labeled CLIR data (Asai et al., 2021b).\nHere we first run an empirical evaluation of these two approaches on a public CLIR benchmark (Asai et al., 2021a), which includes both in-domain and zero-shot out-of-domain tests. We use ColBERT (Khattab and Zaharia, 2020; Khattab et al., 2021) as our IR architecture1 and XLM-R as the underlying PLM for both methods (§2). Results indicate that the MT-based solution can be vastly more effective than CLIR fine-tuning, with observed differences in Recall@5kt of 22.2–28.6 points (§3). Crucially, the modular design of the former allows it to leverage additional English-only training data for its IR component, providing significant boosts to its results.\nThe above findings lead naturally to the central research question of this paper: Can a highperformance CLIR model be trained that can operate without having to rely on MT? To answer the question, instead of viewing the MT-based approach as a competing one, we propose to leverage its strength via knowledge distillation (KD) into an end-to-end CLIR model. KD (Hinton et al., 2014) is a powerful supervision technique typically used to distill the knowledge of a large teacher model about some task into a smaller student model (Mukherjee and Awadallah, 2020; Turc et al., 2020). Here we propose to use it in a slightly different context, where the teacher and the student retriever are identical in size, but the former has superior performance simply due to utilizing MT output and consequently operating in a high-resource and lowdifficulty monolingual environment.\nWe run two independent KD operations (§2.2). One directly optimizes an IR objective by utiliz-\n1Due to its state-of-the-art (SOTA) performance, outperforming recent models such as DPR (Karpukhin et al., 2020).\ning labeled CLIR data: parallel questions (English and non-English) and corresponding relevant and non-relevant English passages. The teacher and the student are shown the English and non-English versions of the questions, respectively; the training objective is for the student to match the soft query-passage relevance predictions of the teacher. The second KD task is representation learning from parallel text, where the student learns to encode a non-English text in a way that matches the teacher’s encoding of the aligned English text, at the token level. The cross-lingual token alignment needed to create the training data for this task is generated using a greedy alignment process that exploits the PLM’s multilingual representations.\nIn our experiments on the XOR-TyDi dataset (Asai et al., 2021a), the KD student outperforms the fine-tuned ColBERT baseline by 25.4 (in-domain) and 14.9 (zero-shot) Recall@5kt, recovering much of the performance loss from the MT-based solution. It is also the best single-model system on the XOR-TyDi leaderboard2 at the time of this writing. Ablation studies show that each of our two KD processes contribute significantly towards the final performance of the student model.\nOur contributions can be summarized as follows: (1) We present an empirical study of the effectiveness of a SOTA IR method (ColBERT) on crosslingual IR with and without MT. (2) We propose a novel end-to-end cross-lingual solution that uses knowledge distillation to learn both improved text representation and retrieval. (3) We demonstrate with a new cross-lingual alignment algorithm that distillation using parallel text can strongly augment cross-lingual IR training. (4) We achieve new single-model SOTA results on XOR-TyDi.\n2https://nlp.cs.washington.edu/xorqa/"
    }, {
      "heading" : "2 Method",
      "text" : "Here we first describe our base IR architecture (ColBERT) and then the proposed KD-based crosslingual training algorithms."
    }, {
      "heading" : "2.1 The ColBERT Model",
      "text" : "ColBERT (Khattab and Zaharia, 2020) employs a transformer-based encoder to separately encode the input query and document, followed by a linear compression layer. Each training instance is a <q, d+, d−> triple, where q is a query, d+ is a positive (relevant) document and d− is a negative (non-relevant) document. A relevance score Sq,d for the pair (q, d) is first computed using Eq. 1, where d ∈ {d+, d−} and Eqi and Edj are the output embeddings of query token qi and document token dj , respectively. For a given training triple, a cross-entropy loss is minimized for the softmax over Sq,d+ and Sq,d− .\nSq,d := ∑ i∈[|q|] maxj∈[|d|]Eqi · ETdj (1)\nFor inference, the embeddings of all documents are calculated a priori, while the query embeddings and the relevance score are computed at runtime."
    }, {
      "heading" : "2.2 Knowledge Distillation",
      "text" : "Our teacher and student are both ColBERT models that fine-tune the same underlying multilingual PLM for IR. The teacher is first trained with all-English triples using the procedure of §2.1. The goal of the subsequent KD training is to teach the student how to reproduce the behavior of this teacher, provided that it sees non-English questions while the teacher sees English questions.\nWe apply KD at two different stages of the ColBERT workflow: (1) relevance score computa-\nInput: vT : Teacher’s output representation of tokenized\nEnglish (EN) text. vS : Student’s representation of parallel non-EN text. Output: v (a) T : Reordered teacher output embeddings to reflect position-wise alignment with vS . Procedure: DM ← cosine_distance(vT , vS) //matrix //get index pairs to swap in vT swaps← [ ] for row in rows(DM) do\n//loop runs |vT | times minV alue← min(DM) i, j ← index_of(minV alue) //swap rows i and j DM [[i, j], :] = DM [[j, i], :] //set row j and column j to +∞ DM [j, :]← +∞ DM [:, j]← +∞ swaps.append((i, j))\nend //swap teacher’s output tokens v (a) T ← vT for s in swaps do v (a) T [s[0], s[1]]← vT [s[1], s[0]] end\nAlgorithm 1: Cross-lingual alignment.\ntion (Sq,d in Eq. 1), and (2) encoding (e.g., Eqi). Figure 1 depicts the former in detail, where training minimizes the KL divergence between the student’s and the teacher’s output softmax distributions (with temperature) over Sq,d+ and Sq,d− .\nLabeled training data for CLIR are of limited availability, whereas MT, being a more established area of research, has produced a large amount of parallel text over the years. We seek to exploit parallel corpora in our second KD training stage, where we train the student to compute representations for non-English texts that closely match the teacher’s representations of aligned English texts. Crucially, since ColBERT computes a single vector for each individual input token (i.e., a PLM vocabulary item) and not for the entire input text, our algorithm must support distillation at the token level.\nTo achieve this, we apply an unsupervised cross-lingual alignment algorithm. Assuming (ne1, ..., neS) to be the ordered tuple of tokens in a non-English text and (e1, ..., eT ) the corresponding tuple from the aligned English text, each iteration of this algorithm greedily aligns the next (nei, ej) pair with the highest cosine similarity of their output embeddings. Algorithm 1 implements this idea by repositioning the teacher’s tokens so that they are position-wise aligned with the corresponding student tokens. Note that the design choice of us-"
    }, {
      "heading" : "Linear Trans.",
      "text" : "ing a common multilingual PLM in the teacher and the student, even though the former is tasked only with handling English content, is key for the operation of this algorithm as it relies on the PLM’s multilingual representation capabilities.\nIn addition to cross-lingual alignment, we also perform a similar KD procedure in which both the teacher and the student are shown the same English text. This step is useful because ColBERT uses a shared encoder for the query and the document, necessitating a student that is able to effectively encode text from both English documents and nonEnglish queries.\nUsing the alignment information, we train the student by minimizing the Euclidean distance between its representation of a token (English or nonEnglish) and the teacher’s representation of the corresponding English token. Figure 2 shows the KD process for representation learning."
    }, {
      "heading" : "3 Experiments",
      "text" : "Setup. Our primary CLIR dataset is XOR-TyDi (Asai et al., 2021a), which contains examples in seven typologically diverse languages: Arabic (Ar), Bengali (Bn), Finnish (Fi), Japanese (Ja), Korean (Ko), Russian (Ru) and Telugu (Te). For standard in-domain experiments, we use a train-dev-test split of this dataset. There are 2,113 questions in the test set. For zero-shot experiments, we use the MKQA (Longpre et al., 2020) dataset for training and validation, and the following shared languages in the XOR-TyDi test set for evaluation: Ar, Fi, Ja, Ko and Ru. Both training sets contain English questions and their human translations in the other languages, their short answers and corresponding relevant (positive) and non-relevant (negative) Wikipedia snippets. Additionally, we use the\nOpenNQ (Kwiatkowski et al., 2019) training set for English pre-training of the baseline model. Further details on data pre-processing, the final training sets and the optimal hyperparameter configurations are provided in Appendix A.1 and A.2.\nThe CLIR baseline used in our experiments is a ColBERT model with an XLM-R PLM, which we iteratively fine-tune first on English and then on cross-lingual IR triples for optimal performance. Our student model is initialized with the parameter weights of this baseline, and is further fine-tuned using the two KD objectives. As the KD teacher, we train a model on only English triples, as stated before. During evaluation, it is given machinetranslated questions that come with the XOR-TyDi dataset. Appendix A.1 contains more details on the supervision of these models.\nOur evaluation metric is Recall at 5000 tokens (R@5kt) (Asai et al., 2021a), which computes the fraction of questions for which the ground truth short answer is contained within the top 5,000 tokens of the retrieved passages. Appendix A.3 also presents R@2kt results.\nEvaluation. Table 1 compares the performance of our models measured by their R@5kt scores. First, we observe that pre-training the baseline model on English IR triples from the OpenNQ dataset (Kwiatkowski et al., 2019) substantially boosts its performance in both in-domain and zeroshot settings. However, it still underperforms the MT + English IR pipeline by 28.6 and 26.5 points, respectively. The proposed KD-based procedure, first with the parallel corpus (for representation learning) and then with the IR triples (for CLIR), achieves an overall improvement of 25.4 points for the baseline model in in-domain evaluation, which, quite impressively, is only 3.2 points be-\nhind the teacher’s score. In the zero-shot setting, we again observe a sizable gain of 14.9 points. We present more fine-grained per-language results in Appendix A.4.\nLeaderboard Submission. Our KD student of Table 4 row 4, trained on XOR-TyDi and submitted under the name CHIMAERAS B, is the best singlemodel system on the official XOR-TyDi leaderboard at the time of this writing. Please see Appendix A.5 for further details.\nAblation Study. We train two more student models, each of which goes through only one of the two KD training steps. Table 2 summarizes the results: KD with only CLIR examples and with only the parallel corpus improves the system’s score by 15.9 and 20.9 points, respectively with target-domain supervision. Interestingly, although the parallel corpus does not provide any IR signal, it contributes more to the final performance of the model. These results also confirm that our cross-lingual alignment algorithm indeed produces useful alignments."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We train highly effective end-to-end cross-lingual IR models by distilling the knowledge of an English retriever. We propose separate processes to teach IR and multilingual text representations, and present for the latter a cross-lingual alignment algorithm that only relies on the underlying masked language model’s multilingual representation capabilities. Supervised and zero-shot evaluations show that our model recovers much of the performance lost due to operating in an efficient crosslingual mode. Our KD-based method also yields new single-model SOTA results on the XOR-TyDi benchmark. Future work will explore IR on unseen languages and evaluation on additional datasets."
    }, {
      "heading" : "5 Ethics",
      "text" : ""
    }, {
      "heading" : "5.1 Limitations",
      "text" : "We show the effectiveness of multi-stage knowledge distillation and cross-lingual token alignment in training a cross-lingual information retrieval system. We believe that it can be transferred to more datasets and languages, but here we only show proof of concept for the XOR-TyDi and MKQA datasets and the seven languages mentioned in the manuscript."
    }, {
      "heading" : "5.2 Risks",
      "text" : "The intent of this work is to develop a new method for high-performance cross-lingual information retrieval. It is possible that a malicious user could try to attack the system by providing poor or offensive training data. We do not support it being used in such a manner. The risks of our system are the same as other NLP systems and we do not believe we introduce any additional risk."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Data Pre-processing",
      "text" : "The official XOR-TyDi training set consists of 15,221 natural language queries, their short answers, and examples of corresponding relevant (positive) and non-relevant (negative) Wikipedia snippets. For most queries, there are one positive and three negative examples. We remove the 1,699 (11%) questions that have no answers in the dataset. A random selection of 90% of the remaining examples is used for training and the rest for validation.\nFollowing the original XOR-TyDi process, we also obtain additional training examples by running BM25-based retrieval against a Wikipedia corpus and using answer string match as the relevance criterion. These examples are added to the original set to obtain three positive and 100 negative examples per query. As the blind test set for final evaluation, we use the 2,113 questions in the official XORTyDi dev set.\nOur monolingual (English) training data of about 17.5M triples are derived from the third fine-tuning round (ColBERT-QA3) of ColBERT relevance-guided supervision (Khattab et al., 2021) with OpenNQ data (Kwiatkowski et al., 2019). The parallel corpus used in our KD experiments for representation learning is constructed from three different sources: (1) an in-house crawl of Korean, (2) LDC releases (Arabic), and (3) OPUS3. The corpus has a total of 6.9M passage pairs which include .9M pairs in Telugu and 1M pairs in each of the other six languages.\nFor zero-shot experiments, the training examples are derived from MKQA (Longpre et al., 2020), which consists of 10k queries selected from NQ, human translated into 25 additional languages, five of which overlap with XOR-TyDI: Ar, Fi, Ja, Ko and Ru. We construct training data (triples) from 2,037 queries translated into these five languages for which there are corresponding positive and negative passages in the OpenNQ dataset. For each of the five languages, there are 519k triples for a total of 2.6M triples. We set aside 200 queries translated into the 5 languages for a total of 1,000 queries as a development set. We remove all MKQA queries from the OpenNQ training data for these experiments.\nThe CLIR baseline for our experiments is a ColBERT model with an XLM-R PLM, which we\n3https://opus.nlpl.eu\nfirst fine-tune with 17.5M NQ examples for one epoch and then 2.9M XOR-TyDi triples for five epochs. Our student model is initialized with the parameter weights of the baseline, and is further fine-tuned using the two KD objectives. The monolingual teacher model—also a ColBERT model running on top of the pre-trained XLM-R—is trained with only the 17.5M NQ triples for one epoch."
    }, {
      "heading" : "A.2 Model Selection",
      "text" : "All the models were trained with single Nvidia A100 GPU. The longest training time for a single model was less than 200 hours. Following are the final hyperparameter configurations of our different models. They were selected based on the respective validation sets performance."
    }, {
      "heading" : "A.3 R@2kt Results",
      "text" : "In Table 4, we present results for R@2kt which evaluates a smaller subset of the top retrievals than R@5kt. The general pattern in these results is very\nsimilar to that for R@5kt: the student model closes much of the performance gap between the teacher and the baseline in both settings. Table 5 also shows the R@2kt results of the ablation study, which is again similar to the R@5kt results of Table 2."
    }, {
      "heading" : "A.4 Results on Individual Languages",
      "text" : "Table 6 compares the performance (R@5kt) of our KD student with that of the baseline CLIR model on each individual language. In both settings, i.e., with and without target-domain supervision, the student consistently outperforms the baseline, with large gains across all languages. These results point to the robustness of our approach, stemming from the individual strengths of MT, English IR and KD."
    }, {
      "heading" : "A.5 Leaderboard Submission",
      "text" : "We anonymously submitted the official XOR-TyDi test set predictions of the KD student of Table 1 row 4 to the XOR-TyDi leaderboard4, under the model name CHIMAERAS B. Since our parallel corpus extraction process relies on in-house source code that is not publicly available, we submitted to the “Systems using External APIs” category. It should be noted here though that our approach is not dependent on any specific extraction algorithm;\n4https://nlp.cs.washington.edu/xorqa/"
    }, {
      "heading" : "Language Baseline KD Student",
      "text" : "we used an existing parallel corpus originally created for a different purpose simply for convenience. CHIMAERAS B, at the time of this writing, is the best single-model system on the leaderboard, outperformed only by a ColBERT ensemble that also uses MT. Crucially, all other submitted systems under the External APIs category rely on MT at decoding time, avoiding which is one of the primary goals of our work."
    }, {
      "heading" : "A.6 Qualitative Analysis",
      "text" : "To find out what exact weaknesses of the baseline model the proposed method helps to address, we examine thirty random zero-shot test examples where the baseline fails to retrieve the correct answer in the top 5k tokens, but the KD student succeeds within the top 3 passages. We show four examples in Table 7 with human translations of the original non-English questions. The vast majority of our observed cases are related to weak cross-lingual encoding on the baseline model’s part, where at least one important non-English word/entity in the question seems to be incorrectly matched with a similar but different English entity in the passage (e.g., the name of a different place). For the Korean, Russian and Arabic queries in the table, we observe the presence of such topically similar entities (e.g., microwave↔ gamma-ray, Germany↔ places in North America). Much more rarely, we see cases similar to the Japanese query where the retrieved passage is completely off-topic.\nKorean Query:우주마이크로파배경은누가처음발견했나요? English Translation: Who first discovered the cosmic microwave background? Answer: Arno Penzias and Robert Wilson Baseline Top Passage: History of gamma-ray burst research The history of gamma-ray began with the serendipitous detection of a gamma-ray burst (GRB) on July 2, 1967, by the U.S. Vela satellites. After these satellites detected fifteen other GRBs, Ray Klebesadel of the Los Alamos National Laboratory published the first paper on the subject, \"Observations of Gamma-Ray Bursts of Cosmic Origin\". As more and more research was done on these mysterious events, hundreds of models were developed in an attempt to explain their origins. Gamma-ray bursts were discovered in the late 1960s by the U.S. Vela nuclear test detection satellites. The Velas were built KD Student Top Passage: project to date, it consists of more than 66 dishes and was built in an international collaboration by Europe, North America, East Asia and Chile. A major recent focus of microwave radio astronomy has been mapping the cosmic microwave background radiation (CMBR) discovered in 1964 by radio astronomers Arno Penzias and Robert Wilson. This faint background radiation, which fills the universe and is almost the same in all directions, is \"relic radiation\" from the Big Bang, and is one of the few sources of information about conditions in the early universe. Due to the expansion and thus cooling of the Russian Query: В каком сражении участвовал крейсер \"Аврора\"? English Translation: What battle did the cruiser Aurora take part in? Answer: Battle of Tsushima Baseline Top Passage: Battle of the Arar The Battle of the Arar was fought between the migrating tribes of the Helvetii, and four Roman legions (Legions VII, VIII, IX \"Hispana\" and X \"Equestris\"), under the command of Gaius Julius Caesar, in 58 BC. This was the first major battle of the Gallic Wars. The Helvetii were a tribe that originated from what is now Switzerland. Just prior to the battle with Caesar, they had commenced on a mass migration through Roman Gaul towards the Atlantic coast. At Geneva, the Romans destroyed the wooden bridge across the Rhone and constructed 19 mi of fortifications. KD Student Top Passage: ordered back to Asia as part of the Russian 2nd Pacific Squadron, a collection formed from the Russian Baltic Fleet, under the command of Vice-Admiral Zinovy Rozhestvensky. On the way to the Far East, \"Aurora\" sustained light damage from confused friendly fire, which killed the ship’s chaplain and a sailor, in the Dogger Bank incident. On 27 and 28 May 1905 \"Aurora\" took part in the Battle of Tsushima, along with the rest of the Russian squadron. During the battle her captain, Captain 1st rank Eugene R. Yegoryev, and 14 crewmen were killed. The executive officer, Captain of 2nd rank Japanese Query:抑された記憶を提唱したのは誰 English Translation: Who proposed repressed memory Answer: Sigmund Freud Baseline Top Passage: 14 on the charts on another reissue at the end of 1979, and is now regarded as the Moody Blues signature song by British audiences. In the US, \"Nights in White Satin\" did not make the \"Billboard\" Hot 100 in 1968, although it reached No. 2 on re-release in 1972; \"Tuesday Afternoon\" was more successful on initial release stateside, peaking at No. 24 on the Billboard Hot 100. The 1968 follow-up LP, \"In Search of the Lost Chord\" included \"Legend of a Mind\", a song written by Ray Thomas in tribute to LSD guru Timothy Leary which encompassed a flute KD Student Top Passage: literary example of traumatic memory that has been repressed by an otherwise healthy individual, and then recovered.\" Pope claimed that no entrant had satisfied the criteria. Ross Cheit, a political scientist at Brown University, cited Nina, a 1786 opera by the French composer Nicolas Dalayrac. The concept of repressed memory originated with Sigmund Freud in his 1896 essay \"Zur Ätiologie der Hysterie\" (\"On the etiology of hysteria\"). One of the studies published in his essay involved a young woman by the name of Anna O. Among her many ailments, she suffered from stiff paralysis on the right side of her Arabic Query: ? AJ K AÖÏ @ Õæ ËA\n¯ @ Q .» @ ñë AÓ\nEnglish Translation: What is the largest region of Germany? Answer: Bavaria Baseline Top Passage: the original name of Montana was adopted. Montana is one of the nine Mountain States, located in the north of the region known as the Western United States. It borders North Dakota and South Dakota to the east. Wyoming is to the south, Idaho is to the west and southwest, and three Canadian provinces, British Columbia, Alberta, and Saskatchewan, are to the north. With an area of , Montana is slightly larger than Japan. It is the fourth largest state in the United States after Alaska, Texas, and California; it is the largest landlocked U.S. state. The state’s topography is KD Student Top Passage: Bavaria (; German and Bavarian: \"Bayern\" ; ), officially the Free State of Bavaria (German and Bavarian: \"Freistaat Bayern\" ), is a landlocked federal state of Germany, occupying its southeastern corner. With an area of 70,550.19 square kilometres (27,200 sq mi), Bavaria is the largest German state by land area. Its territory comprises roughly a fifth of the total land area of Germany. With 13 million inhabitants, it is Germany’s second-most-populous state after North Rhine-Westphalia. Bavaria’s capital and largest city, Munich, is the third-largest city in Germany. The history of Bavaria stretches from its earliest settlement and formation as\nTable 7: Examples of cases where the baseline model fails to retrieve a relevant passage but the KD student succeeds within top 3. We only show the top retrieval for each system. Most errors are related to potential word/entity mistranslations, the only exception being the Japanese query where the issue is a weaker understanding of the passage content."
    } ],
    "references" : [ {
      "title" : "XOR QA: Cross-lingual Open-Retrieval Question Answering",
      "author" : [ "Akari Asai", "Jungo Kasai", "Jonathan Clark", "Kenton Lee", "Eunsol Choi", "Hannaneh Hajishirzi." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Asai et al\\.,? 2021a",
      "shortCiteRegEx" : "Asai et al\\.",
      "year" : 2021
    }, {
      "title" : "One question answering model for many languages with cross-lingual dense passage retrieval",
      "author" : [ "Akari Asai", "Xinyan Yu", "Jungo Kasai", "Hannaneh Hajishirzi." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Asai et al\\.,? 2021b",
      "shortCiteRegEx" : "Asai et al\\.",
      "year" : 2021
    }, {
      "title" : "Cross-language information retrieval (clir) track overview",
      "author" : [ "Martin Braschler", "Jürgen Krause", "Carol Peters", "Peter Schäuble." ],
      "venue" : "TREC.",
      "citeRegEx" : "Braschler et al\\.,? 1999",
      "shortCiteRegEx" : "Braschler et al\\.",
      "year" : 1999
    }, {
      "title" : "Unsupervised Cross-lingual Representation Learning at Scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "NeurIPS Deep Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2014",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2014
    }, {
      "title" : "Crosslingual information retrieval with bert",
      "author" : [ "Zhuolin Jiang", "Amro El-Jaroudi", "William Hartmann", "Damianos Karakos", "Lingjun Zhao." ],
      "venue" : "arXiv preprint arXiv:2004.13005.",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Relevance-guided supervision for openqa with colbert",
      "author" : [ "Omar Khattab", "Christopher Potts", "Matei Zaharia." ],
      "venue" : "Transactions of the ACL.",
      "citeRegEx" : "Khattab et al\\.,? 2021",
      "shortCiteRegEx" : "Khattab et al\\.",
      "year" : 2021
    }, {
      "title" : "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
      "author" : [ "Omar Khattab", "Matei Zaharia." ],
      "venue" : "SIGIR.",
      "citeRegEx" : "Khattab and Zaharia.,? 2020",
      "shortCiteRegEx" : "Khattab and Zaharia.",
      "year" : 2020
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association of Computational Linguistics.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
      "author" : [ "Shayne Longpre", "Yi Lu", "Joachim Daiber." ],
      "venue" : "arXiv preprint arXiv:2007.15207.",
      "citeRegEx" : "Longpre et al\\.,? 2020",
      "shortCiteRegEx" : "Longpre et al\\.",
      "year" : 2020
    }, {
      "title" : "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models",
      "author" : [ "Subhabrata Mukherjee", "Ahmed Awadallah." ],
      "venue" : "ACL.",
      "citeRegEx" : "Mukherjee and Awadallah.,? 2020",
      "shortCiteRegEx" : "Mukherjee and Awadallah.",
      "year" : 2020
    }, {
      "title" : "Leveraging comparable corpora for cross-lingual information retrieval in resource-lean language pairs",
      "author" : [ "Azadeh Shakery", "ChengXiang Zhai." ],
      "venue" : "Information retrieval, 16(1):1–29.",
      "citeRegEx" : "Shakery and Zhai.,? 2013",
      "shortCiteRegEx" : "Shakery and Zhai.",
      "year" : 2013
    }, {
      "title" : "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models",
      "author" : [ "Iulia Turc", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Turc et al\\.,? 2020",
      "shortCiteRegEx" : "Turc et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Cross-lingual information retrieval (CLIR) (Braschler et al., 1999; Shakery and Zhai, 2013; Jiang et al., 2020; Asai et al., 2021a), for example, can find relevant text in a high-resource language such as English even when the query is posed in a different, possibly low-resource, language.",
      "startOffset" : 43,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Cross-lingual information retrieval (CLIR) (Braschler et al., 1999; Shakery and Zhai, 2013; Jiang et al., 2020; Asai et al., 2021a), for example, can find relevant text in a high-resource language such as English even when the query is posed in a different, possibly low-resource, language.",
      "startOffset" : 43,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Cross-lingual information retrieval (CLIR) (Braschler et al., 1999; Shakery and Zhai, 2013; Jiang et al., 2020; Asai et al., 2021a), for example, can find relevant text in a high-resource language such as English even when the query is posed in a different, possibly low-resource, language.",
      "startOffset" : 43,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Cross-lingual information retrieval (CLIR) (Braschler et al., 1999; Shakery and Zhai, 2013; Jiang et al., 2020; Asai et al., 2021a), for example, can find relevant text in a high-resource language such as English even when the query is posed in a different, possibly low-resource, language.",
      "startOffset" : 43,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "A straightforward solution to this problem can be based on machine translation (MT) of the query into English, followed by English IR (Asai et al., 2021a).",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "Pre-trained multilingual masked language models (PLMs) such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (XLM-R) (Conneau et al.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : ", 2019) or XLM-RoBERTa (XLM-R) (Conneau et al., 2020) can provide the foundation for such an approach, as one can simply fine-tune a PLM with labeled CLIR data (Asai et al.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : ", 2020) can provide the foundation for such an approach, as one can simply fine-tune a PLM with labeled CLIR data (Asai et al., 2021b).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "Here we first run an empirical evaluation of these two approaches on a public CLIR benchmark (Asai et al., 2021a), which includes both in-domain and zero-shot out-of-domain tests.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "We use ColBERT (Khattab and Zaharia, 2020; Khattab et al., 2021) as our IR architecture1 and XLM-R as the underlying PLM for both methods (§2).",
      "startOffset" : 15,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "We use ColBERT (Khattab and Zaharia, 2020; Khattab et al., 2021) as our IR architecture1 and XLM-R as the underlying PLM for both methods (§2).",
      "startOffset" : 15,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "KD (Hinton et al., 2014) is a powerful supervision technique typically used to distill the knowledge of a large teacher model about some task into a smaller student model (Mukherjee and Awadallah, 2020; Turc et al.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : ", 2014) is a powerful supervision technique typically used to distill the knowledge of a large teacher model about some task into a smaller student model (Mukherjee and Awadallah, 2020; Turc et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : ", 2014) is a powerful supervision technique typically used to distill the knowledge of a large teacher model about some task into a smaller student model (Mukherjee and Awadallah, 2020; Turc et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 204
    }, {
      "referenceID" : 7,
      "context" : "Due to its state-of-the-art (SOTA) performance, outperforming recent models such as DPR (Karpukhin et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "In our experiments on the XOR-TyDi dataset (Asai et al., 2021a), the KD student outperforms the fine-tuned ColBERT baseline by 25.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "ColBERT (Khattab and Zaharia, 2020) employs a transformer-based encoder to separately encode the input query and document, followed by a linear compression layer.",
      "startOffset" : 8,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Our primary CLIR dataset is XOR-TyDi (Asai et al., 2021a), which contains examples in seven typologically diverse languages: Arabic (Ar), Bengali (Bn), Finnish (Fi), Japanese (Ja), Korean (Ko), Russian (Ru) and Telugu (Te).",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "For zero-shot experiments, we use the MKQA (Longpre et al., 2020) dataset for training and validation, and the following shared languages in the XOR-TyDi test set for evaluation: Ar, Fi, Ja, Ko and Ru.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "(R@5kt) (Asai et al., 2021a), which computes the fraction of questions for which the ground truth short answer is contained within the top 5,000 tokens of the retrieved passages.",
      "startOffset" : 8,
      "endOffset" : 28
    } ],
    "year" : 0,
    "abstractText" : "We present a new cross-lingual information retrieval (CLIR) system trained using multi-stage knowledge distillation (KD). The teacher relies on a highly effective but expensive twostage process consisting of query translation and monolingual IR, while the student executes a single CLIR step. We teach the student powerful multilingual encoding as well as CLIR by optimizing two corresponding KD objectives. Learning useful non-English representations from an English-only retriever is accomplished through a cross-lingual token alignment algorithm that relies on the representation capabilities of the underlying multilingual language model. In both in-domain and zero-shot evaluation, the proposed method demonstrates far superior accuracy over direct fine-tuning with labeled CLIR data. One of our systems is also the current best single-model system on the XOR-TyDi leaderboard.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_224_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964). In coordinate compounds (CCs), twowords are combined to form a compoundword whose semantics are often a generalization of those of the two conjoined words. Elaborate expressions\n(EEs) are similar, except that they can consist of two phrases (rather than words) and include a repeated word. Take the following examples:\n(1) Chinese coordinate compounds (CCs) 父母 fùmǔ father-mother ‘parents’ 花木 huāmù flower-tree ‘vegetation’ 天地 tiāndì heaven-earth ‘universe’ 国家 guójiā country-home ‘nation’ 风水 fēngshuǐ wind-water ‘geomancy’\n(2) Lahu elaborate expressions (EEs) a. ɔ̂\nfour cē corner ɔ̂ four phɔ̂ side\n‘at every corner’ b. chɔ\npeople phôʔ pile chɔ people dì lump\n‘a throng of people’ c. câ\neat cûʔ scarce dɔ̀ drink cûʔ scarce\n‘have nothing to eat or drink’\nCoordinating compounds are found throughout the world, with varying semantic relationships between the whole and the parts (Obermüller, 2015). Elaborate expressions are most common in mainland Southeast Asia, where they occupy a position of great prominence. They are often associated with elevated styles of discourse, but they occur in all genres and registers. Earlier investigators have claimed the order of the constituent words in CCs and EEs—in some languages—is predictable by rule. Many of the proposed ordering hierarchies are based on phonology (Ting, 1975; Dai, 1986; Mortensen, 2006). Building on this earlier work, Mortensen (2006) posited that Lahu EE orders could be predicted based on vowel quality—like Jingpho (Dai, 1986)—and that Hmong EE orders could be predicted based on tone, echoing earlier claims for Chinese and Qe-Nao (Ting, 1975; Pan and Cao, 1972). These tone and vowel scales were, however, not easy to rationalize in phonetic terms and\nwere used by Mortensen to argue for a phonology in which structure reigns supreme and in which phonetic substance plays only an epiphenomenal role. These claims have been viewed with skepticism for two reasons: morphosyntax has been widely seen as providing the inputs to phonology, not being driven by phonology; and phonology, since Jakobson et al. (1951) and Chomsky and Halle (1968) has usually been seen as grounded in phonetic categories. Some investigators have claimed that sound patterns that are not phonetically natural are inherently unlearnable. They can exist only as linguistic fossils deposited by a history of language change. In this paper, we undertake to investigate what kind of data is needed for (computational) learners to acquire these patterns. We report the following findings:\n• Even rather simple classifiers like decision trees can learn to predict the order of EEs in isolation in over 96% of cases (Hmong) and 79% of cases (Lahu) using only phonological information.\n• The decision trees for Hmong, Lahu, and Chinese mirror the phonological hierarchies proposed for these languages, suggesting that these hierarchies are empirically robust and learnable from the available evidence.\n• However, correct and incorrect orderings of Hmong EEs can be effectively distinguished in context by a neural sequence labeling model without any phonological information, suggesting that learners would not have to acquire the phonological generalization directly in order to produce well-formed EEs."
    }, {
      "heading" : "2 Theoretical Significance",
      "text" : "The experiments reported in this paper have a bearing on two assumptions widely held in phonological theory:\n1. True phonological generalizations are always grounded in phonetic realities (phonology is natural)\n2. Phonology operates on the outputs of syntax and morphology (grammar is serial)\nBoth of these assumptions have been contested. If Mortensen’s (2006) analysis of EE and CC ordering is sound, neither of these assumptions can be entirely correct."
    }, {
      "heading" : "2.1 Phonological patterns and phonetic substance",
      "text" : "Starting even before Prague School phonology, it was widely assumed that the grammatical categories and patterns making reference to sound are coherent in terms of physical (articulatory, acoustic, and psychophysical) dimensions. For phonologically distinctive features, this was codified by Jakobson et al. (1951) and injected into generative phonology by Chomsky and Halle (1968). Even more radical statements about the relationship between phonological form and substance have been made since then (Donegan and Stampe, 1979; Flemming, 2013; Hayes, 2011; Donegan and Stampe, 2009; Steriade et al., 2001). While there has never been a complete consensus on the matter (Fudge, 1967; Hyman, 1970; Hale and Reiss, 2000, 2008), it has been widely assumed that phonological patterns that are phonetically incoherent cannot be learned by humans or can be learned only with difficulty (Hayes and White, 2013). More recently, artificial grammar learning experiments have been inconclusive but have suggested that the difficult-to-learn phonological patterns are structurally complex, not phonetically unnatural (Moreton and Pater, 2012a,b). The phonological ordering generalizations proposed by Mortensen (2006) are structurally quite simple, but often phonetically incoherent. For Hmong EEs, ordering follows the hierarchy presented in Table 1; an EE with an \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 form1 is ordered such that, if \uD835\uDC351 and \uD835\uDC352 differ in tone, the tone of \uD835\uDC351 is higher on the hierarchy than the tone of \uD835\uDC352. This hierarchy has one phonetically reason-\n1\uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 (as in Lahu chɔ phôʔ chɔ dì) is also denoted as \uD835\uDC34\uD835\uDC35\uD835\uDC34\uD835\uDC36 in the literature. We use \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 in this paper to indicate that the second and fourth words are closely related as they form a potential coordinate compound.\nable aspect—the first two tones start high (though their relative rank seems arbitrary). The rest of the hierarchy is puzzling: it goes from lowest to low to rising to falling to neutral. Mortensen’s generalization for Lahu elaborate expressions would be easier to reconcile with phonetic substance (the higher in vowel space a vowel is, the better a candidate it is for the first position) were it not that the best first-position vowel is /o/, a mid, back, rounded vowel. The ordering generalizations that have been proposed for Chinese are similarly arbitrarylooking—they can be stated in terms of historical tonal categories (like the Middle Chinese tones) but appear incoherent in modern lects, in which the tones have “wandered” phonetically to a dramatic degree. If it can be shown that these patterns can be learned from naturalistic data, that they are robust predictors of EE and CC ordering, and that models trained to detect correctly ordered EEs and CCs in running text learn to use this kind of phonological evidence to assign labels, it would be suggestive, though not definitive, evidence against the position that phonological constraints must be grounded in phonetic substance."
    }, {
      "heading" : "2.2 Word order conditioned on phonology",
      "text" : "In mainstream generative linguistics, grammar has usually been viewed as a feed-forward production system. While the nature of this pipeline has changed over various revisions of the theory, a consistent theme is that phonology operates on the output of syntax (Chomsky, 1981, 1995) and that, therefore, syntax should not be sensitive to phonology.2 If phonology can determine word order, this has some specific implications for the phonologysyntax interface. In fact, there is mounting evidence that word order can be sensitive to phonology. It has long been suggested that dative shift in English is sensitive to phonological weight (Ross, 1967) although this claim has also been long contested (Wasow and Arnold, 2003). Some newer evidence comes from coordinate compound and echo reduplication constructions in Japanese, Korean, and Jingpho (Kwon and Masuda, 2019; Dai, 1986). An even\n2An important caveat is that—in some versions of generative grammar—syntactic structures are pure hierarchy and are not linearized until PF (phonetic form), when abstract lexical and functional categories are “spelled-out” (Fox and Pesetsky, 2005). This potentially opens the door for interaction between phonology and word order.\nmore interesting case comes from Tagalog nounadjective order, which is sometimes viewed as being free but which is actually sensitive to a set of phonological constraints (Shih and Zuraw, 2017). The current case would enrich the body of relevant evidence in part because, while these cases are all instances of “soft” statistical tendencies, the Hmong ordering generalization is claimed to be nearly categorical (with a few, principled, exceptions)."
    }, {
      "heading" : "3 Hypotheses",
      "text" : "Based on the existing volume of work, we propose the following hypotheses:\n1. The order of Hmong and Lahu EEs and Chinese CCs can be predicted phonologically (out of context).\n2. The “phonetically unnatural” phonological scales proposed by Mortensen (2006) and Ting (1975) predict the ordering of EEs in Hmong and Lahu and CCs in Chinese (out of context).\n3. These scales can be learned by decision tree classifiers (out of context).\n4. Phonological information facilitates the recognition of correctly and incorrectly ordered Hmong EEs in context."
    }, {
      "heading" : "4 Data",
      "text" : "We examine the ordering effects across three languages: Hmong, Lahu, and Chinese (with Middle Chinese and Mandarin pronunciations). For Hmong, we use a list of 3253 unique elaborate expressions extracted from a 12 million-word Hmong corpus,3 which was manually annotated and validated by human experts. All of the EEs are of the form \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 where \uD835\uDC351\uD835\uDC352 forms a coordinate compound. We also use the entire corpus for the EE tagging task described in Section 5.2. For Lahu, we use a list of 1400 EEs compiled by Matisoff (1989, 2006),4 which contains both \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 and \uD835\uDC351\uD835\uDC34\uD835\uDC352\uD835\uDC34 forms. For Chinese, we use a list of 254 antonymic coordinate compounds \uD835\uDC351\uD835\uDC352 recorded in the Modern Chinese Dictionary\n3From the soc.culture.hmong Usenet group and available at https://github.com/released/on/ acceptance.\n4Available at https://github.com/released/on/ acceptance.\n(Anonymous, 2016). Middle Chinese pronunciations are retrieved from Wiktionary.5"
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Learning Hmong, Lahu, and Chinese CC and EE Ordering with Classifiers over Phonological Features",
      "text" : "Experiment We first examine whether the orders in elaborate expressions and coordinate compounds can be learned by a classifier. We use the EE lists described in Section 4 as phrases with the attested ordering, and create an unattested list of EEs by switching the order of \uD835\uDC351 and \uD835\uDC352 (occasionally both orders are attested). We then formulate the task as a binary classification problem to predict whether a given ordering is attested or unattested. To examine the degree to which the order can be predicted by phonology only, we use one-hot features of the onset, rhyme (vowel) and tone constituents in each syllable as classification features. We found that one-hot features were sufficiently expressive, and that using articulatory features (Mortensen et al., 2016) did not further improve the performance. In Section 5.3 we analyze the effect of adding word embeddings to the feature set. For all classification experiments, we compute the \uD835\uDF122 statistic on all input features and select the top \uD835\uDC3E features that most correlate with the class label, where \uD835\uDC3E is determined by a development set. We report the result on two types of classifiers: a decision tree (DT) classifier for maximal interpretability, and a support vector machine (SVM) with RBF kernel for the best classification performance.6 We also experimented with multilayer perceptron classifiers of varying widths and depths, but they did not outperform SVM on this dataset. Since other classifiers do not offer the explainability of DT or the performance of SVM, we only report results on these two models. We split the attested word list into 70%/30% train/test sets before augmenting it with unattested data in order to prevent the same EE from appearing in both the train and test sets. However, it would still be possible for the same (\uD835\uDC351, \uD835\uDC352) to appear in both train and test sets with different \uD835\uDC34 words (repeated words). To eliminate this pos-\n5Reconstruction from (Li, 1952) 6Classification models are trained using scikit-learn (Pe-\ndregosa et al., 2011)\nsibility, we also report results on randomly sampled subsets of EEs wherein all (\uD835\uDC351, \uD835\uDC352) pairs are unique (so that there is no contamination across the train and test sets).\nRule-Based Classification We also test how well the ordering scales proposed in Mortensen (2006) perform as a rule-based classifier, compared to a DT and SVM trained on the dataset. Table 2 shows the orders in Hmong, Lahu and Middle Chinese used in the rule-based classifier. When there is a tie, the order is determined randomly.\nResults Table 3 shows the classification accuracies for all languages. We report results on two classifiers, using two different sets of features: focal constituent for the group of phonemes corresponding to the ordering rules (Rhyme for Lahu and Tone for Hmong and Chinese), and all constituents for all the onset, rhyme, and tone phonemes. We observe a robust correlation between phonology and attested orders in all four languages, as seen by the high accuracy a classifier can attain. Even on unique (\uD835\uDC351, \uD835\uDC352) pairs, the best classifier and feature set achieves 71%–88% accuracy. This means that the ordering effects are not simply due to frequent (\uD835\uDC351, \uD835\uDC352) pairs skewing the statistics; rather, the ordering effect is robust across many (\uD835\uDC351, \uD835\uDC352) pairs in the four languages.\nWith only the focal constituent feature set, we observe comparable accuracy between the rulebased classification and either statistical classifier. This suggests that the degree to which the focal constituent alone determines EE ordering is no more than the linear ordering scale proposed by Mortensen (2006).7 However, when phonemes from other constituents are included in the feature set and an SVM is used, we observe an increase of 3–11% in accuracy. This suggests the existence\n7We ran an exhaustive search on all permutations of the tones/vowels, and found the one presented here performs the best as a rule-based classifier.\nofmore complex phonological interactions beyond the linear scale over the focal constituent.\nVisualization of Learned Decision Tree By examining the learned decision tree, one can derive a linear hierarchy based on the order of features on the no branch, and whether each branching action leads to majority attested words or majority unattested words. We find that phonemes that appear topmost in the tree (the most order-defining phonemes) are exactly those at the two ends of the scales proposed by Mortensen (2006), and a decision tree classifier can learn a strikingly similar hierarchy, as shown in Table 4. Details on the derivation and the learned tree are shown in Appendix A.\nLanguage Order\nHmong Ling. j ≺ b ≺ m ≺ s ≺ v ≺ g ≺ ∅Tree j ≺ b ≺ m ≺ v ≺ s ≺ g ≺ ∅"
    }, {
      "heading" : "5.2 Learning Hmong EE Ordering as Sequence Labeling",
      "text" : "Experiment Now we investigate whether models can learn to recognize elaborate expressions and their ordering effects in context in a naturalistic corpus. We limit our experiments to Hmong in this section due to the unavailability of EEannotated corpora in other languages. The Hmong dataset is annotated with BIO tags, where a BIII\nsequence represents a labeled EE. We train a neural sequence labeling model to predict the BIO tag of each word in a sentence. We experiment with two types of feature extractors: a bidirectional LSTM and a CNN. We use both word-level and phoneme-level embeddings, following the intuition that the phonologically conditioned ordering helps speakers recognize an EE structure in context. Implementation details and hyperparameters are described in Appendix B. In addition to the vanilla tagging task, to investigate whether the models can learn the ordering of EEs in context, we perform an experiment where the orders of \uD835\uDC351 and \uD835\uDC352 are swapped for half of the EEs, and the tags for the swapped EEs are changed to B-fake and I-fake. This renders the task more difficult as the model needs to both identify an EE in context and classify whether the order has been changed. To prevent the model from memorizing certain EEs, we split the data into train/val/test sets by partitioning the list of EEs into disjoint sets, so that EEs in the validation and test sets do not appear in the training split. This way, the model is only given unseen EEs at test time. Furthermore, we partition the EEs into swap/no-swap so that occurrences of each EE are either all swapped or all kept unchanged.\nBaseline The simplest baseline model would be to tag every occurrence of \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 (a 4-gram where the first and third words are identical) in the corpus as an EE without any consideration of the word or its phonology. Doing so yields 100% recall but very poor precision, since most occurrences of \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 are not elaborate expressions. Three strategies are employed to improve the performance of this baseline: (1) ensure that\n(\uD835\uDC34, \uD835\uDC351, \uD835\uDC352) are proper Hmong syllables parsable by a regular expression classifier; (2) set a word vector similarity threshold between the two CC words (\uD835\uDC351 and \uD835\uDC352) so that \uD835\uDC50\uD835\uDC5C\uD835\uDC60\uD835\uDC56\uD835\uDC5B\uD835\uDC52(\uD835\uDC63\uD835\uDC351 , \uD835\uDC63\uD835\uDC352) > \uD835\uDEFC, since many Hmong EEs have the two CC words of similar meanings (Mortensen, 2006);8 (3) ensure the tonal scale in Table 2 is followed between \uD835\uDC351 and \uD835\uDC352\nResults We report the F1 score of predicted tags on different models in Table 5. For the baseline model, all three strategies improve the tagging performance, suggesting that both semantic similarity and adherence to the tonal scale are indicators of being an EE. Despite the reasonable performance of the baseline, a neural sequence labeling model is able to beat it substantially, achieving a high F1 score in the EE tagging task. In particular, a CNN feature extractor outperforms an LSTM feature extractor. We hypothesize that this is due to a convolution kernel being able to capture nonlocal interactions in an EE (i.e., identical first and third words, and similar second and fourth words), whereas the linear nature of an LSTM encoder becomes restrictive in this task. When half of the EEs in the form of\uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 are changed to \uD835\uDC34\uD835\uDC352\uD835\uDC34\uD835\uDC351 and their tags are modified to B-fake and I-fake (swap clf. rows in the table),\n8Word vectors are trained usingWord2Vec (Mikolov et al., 2013). We find that SkipGram outperforms CBOW on this task, hence all results reported are SkipGram embeddings. \uD835\uDEFC is determined by grid search and we find that \uD835\uDEFC = 0.4 works best.\nthemodel is still able to achieve high F1 scores that are only slightly lower than the unswapped counterpart, even though the B and I tags have split into two types. The fact that increasing the number of classes does not degrade the performance very much suggests that the model can learn to distinguish attested and unattested orderings very well. To quantify the model’s ability to learn Hmong EE ordering, we calculate an in-context classification accuracy by examining how many correctly identified EEs also have a correct prediction in whether the order has been swapped. We find that the incontext classification accuracy is 99.1% for LSTM and 99.5% for CNN, which are both exceptionally high. Note that this analysis excludes EEs that are not correct identified (both false positives and false negatives). Full confusion matrices are shown in Appendix C.\nInterestingly, we find that adding phoneme level features to the input of either LSTM or CNN does not improve the performance in both the swapped and unswapped cases.9 This result is in contrast with other similar sequence tagging tasks (e.g., NER), where character level features are found to improve performance (Yang et al., 2018; Kuru et al., 2016). More importantly, this result presents a contrast to the robust phonological patterns found in the previous section, as it demonstrates that the model is able to tag elaborate expressions and classify their orders successfully without any reference to phonology. This suggests that the ordering (\uD835\uDC351, \uD835\uDC352) can be predicted not only via phonology, but also via word-level features through the embeddings trained with the tagging model.\nVisualization of Word Embeddings It is a rather perplexing result that a tagging model can learn the ordering of EEs via word embeddings only. Figure 1 shows the UMAP projection (McInnes et al., 2018) of two types of learned embeddings into 2D space. Embeddings from the tagging model show clear separation between words that tend to occur first in an EE (in the \uD835\uDC351 position) and words that tend to occur second, where as embeddings trained separately on the SkipGram algorithm (Mikolov et al., 2013) show no separation. This suggests that the learned separation is unique to the tagging model. However, there is no way for the model to memorize EEs from the train-\n9We also tried using character features or using only tones (the focal constituent for Hmong), but they were equally ineffective.\ning set, since the test set contains non-overlapping EEs. How, then, would the tagging model learn what words tend to occur first and what words tend to occur second in an EE? It appears that the model is able to learn the order of \uD835\uDC351 and \uD835\uDC352 from other occurrences of these words in the training set. For an EE \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 in the test set, although \uD835\uDC34\uD835\uDC351\uD835\uDC34\uD835\uDC352 never appears in the training set, \uD835\uDC351 and \uD835\uDC352 do appear either as a coordinate compound \uD835\uDC351\uD835\uDC352/\uD835\uDC352\uD835\uDC351, or as parts of another EE \uD835\uDC4B\uD835\uDC351\uD835\uDC4B\uD835\uDC352/\uD835\uDC4B\uD835\uDC352\uD835\uDC4B\uD835\uDC351 in the training set. As shown in Figure 2, appearances of the same order greatly outnumbers those of the reversed order. As a result, the model may be able to learn which words tend to be \uD835\uDC351 or \uD835\uDC352 from these distributional properties of the EE words."
    }, {
      "heading" : "5.3 Learning Hmong EE Ordering with Classifiers over Word Vectors",
      "text" : "Experiment To further investigate to what extent word embeddings determine the order of (\uD835\uDC351, \uD835\uDC352) in Hmong EEs, we revisit the out-ofcontext classification experiment presented in section 5.1, this time adding word vector features. We experiment with both SkipGram embeddings and embeddings extracted from the CNN sequence taggingmodel (without swapping). Embeddings from the tagging model are expected to perform better on the classification task, since they are optimized to detect words contained in EEs in the attested order. On the other hand, embeddings separately trained via SkipGram aremore “pure,” as they only capture the distributional semantics of the words without additional information.\nResults Table 6 shows the classification accuracies using word embedding features, as well as word embedding combined with one-hot phoneme features. We observe that embeddings trained with the tagger indeed perform better than those trained via SkipGram. What is surprising is that using embedding features from the tagger alone produces a classification accuracy comparable to using all phonemes (88%). Moreover, an even higher accuracy can be achieved by combining phoneme features with embeddings from the tagger. This suggests that EE ordering in Hmong can be predicted from two independent butmutually reinforcing routes, namely phonology and lexical distribution. Either method alone is a good predictor of the ordering, but combining the two achieves the best accuracy, because the two routes each offer additional information that are important in predicting the ordering of Hmong EEs."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, we set out to explore the ways that the order of words in EEs and CCs in Hmong, Lahu, and Chinese can be learned by computational models. Motivated by earlier linguists’ findings, we first use phonological features alone to discriminate between attested and unattested orders of words. We find that in the case of all three languages, the order of words can indeed be predicted phonologically, and the “phonetically unnatural” hierarchies do predict the ordering of EEs and CCs. Furthermore, a decision tree classifier is able to learn more-or-less the same hierarchies, suggesting that speakers of those languages could in principle learn the linear hierarchies through exposure to the language, and use these hierarchies to decide on the correct order of words in EEs and CCs. These findings provide positive evidence for hypotheses 1–3 from Section 3. We then explored the ways models can utilize context and distributional patterns of words to learn the orders in the sequence tagging experiments, and we were not able to find evidence for hypothesis 4. We were surprised to find that models can perform well using only word features, and that adding phonemes to the feature set does not help at all. The seemingly contradictory results of our investigation point in an interesting direction. Information on which a model could rely to learn the ordering of these constructions is present redundantly in phonology (on the one hand) and in lexi-\ncal and distributional patterns (on the other). When allowed to cooperate on a level playing field, embeddings and phonology-based features both contribute to the identification of well-formed EEs at a similar level. In other words, while it is possible that language users may use phonological hierarchies like those proposed in Mortensen (2006) to select appropriate orders for EEs and CCs, it is clearly not the case that they must (though they will perform a bit better if they do). These phonological hierarchies may have been more orderdefining in the history of the languages, but as the sequence tagging experiments have suggested, they may also have become fossilized in the lexicon and in distributional patterns in the modern form. Many times, a (\uD835\uDC351, \uD835\uDC352) pair appears abundantly in multiples EEs (as \uD835\uDC4B\uD835\uDC351\uD835\uDC4B\uD835\uDC352), as a CC (as \uD835\uDC351\uD835\uDC352), or in other—more complicated—discourse patterns in the same order, so that language users could learnwhether a givenword tends to appear in the \uD835\uDC351 or \uD835\uDC352 position. If a tagging model can learn a word representation that distinguishes between \uD835\uDC351 and \uD835\uDC352, language users may do the same. In a sense, these results should be pleasing to both the “structure” (Mortensen, Hale, Reiss) and the “substance” (Hayes, Flemming, Steriade) camps. They show, once again, that generalizations about sounds can be robust but phonetically arbitrary. However, they leave open the possibility that the relevant synchronic generalizations are not actually phonological."
    }, {
      "heading" : "7 Future Directions",
      "text" : "We have shown two independent routes, namely phonology and lexical distribution, by which computational methods can predict the order of words in Hmong EEs. A language user could probably do the same, relying on both routes to some degree when they need to select the order of words in EEs. However, there is no way to know for sure without conducting a psycholinguistic experiment with native speakers, which would shed light on whether any of the modeling actually translates to human cognition. The Chinese and Lahu cases also raise interesting questions for future work: does the same two-route mechanism work for EEs and CCs in these languages as well? Answering this question will require additional data collection and annotation, but will shed significant light on this theoretically important issue."
    }, {
      "heading" : "A Trained Decision Trees",
      "text" : "Figures 3, 4, 5 show what the trained decision tree looks like in the three languages. In each tree node, the top half of the box show the current majority class, attested (ATT) or unattested (FAKE), as well\nas the number of votes. The bottom half of the box shows the variable to branch on. As noted in the main text, a linear ordering can be induced from the tree by following the branches. Take the Hmong tree (Figure 3) as an example. The first factor to split on is whether \uD835\uDC351 has the j tone, and if the answer is yes, the majority of words are attested (255 attested vs 15 unattested/fake). This suggests that j has a strong tendency to occur in the \uD835\uDC351 position, since it is the most distinguishing factor to split on. Hence j can be placed as the first tone on the scale. If\uD835\uDC351 does not have the j tone, the next question to ask is whether \uD835\uDC351 has the b tone. Since a yes answer again leads to majority attested words (273 attested vs 61 unattested/fake), b can be placed second on the scale. The next three questions to ask concern with the \uD835\uDC352 word. Since a yes answer leads to attested words in all three cases, it suggests that ∅, g and s have a tendency to appear in the \uD835\uDC352 position, hence they can be placed on the end of the scale in that order. The next two factors concern with the j and b tones, which have already been placed on the scale, so we skip them. This process of following the left child (the no branch) and placing tones at either end of the scale is repeatedly applied, yielding the induced linear scales shown in Table 4.\nB Implementation Details\nB.1 Data\nTheHmong corpus consists of 740k sentences with a positive rate of around 3.1% (i.e. 96.9% of sentences contain no EEs). The EEs are randomly split into disjoint train and val/test sets with approximate ratios of 91%/4.5%/4.5%. To reduce the possibility that certain splits are easier than others, three such splits are independently produced. The positive sentences are split into train and val/test sets according to the EE partitions, and the negative sentences are split with approximate ratios of 91%/4.5%/4.5%.\nB.2 Models\nThe sequence tagging model consists of a feature extractor followed by a fully connected layer to predict the tags: {B,I,O} in the unswapped case and {B,B-fake,I,I-fake,O} in the swapped classification experiments. Two feature extractors are used: 1) an LSTM with bidirectional encoding, and 2) a CNN, consisting of four layers of 1D\nconvolution, ReLU, Dropout, and BatchNorm.10 When character or phoneme level features are used, the character embeddings go through a CharCNN before being concatenated with the word embedding. Details on model configuration is shown in Table 7. The LSTMmodel contains approximately 1.4M parameters and the CNN contains approximately 1.7M parameters. Our code is based on NCRF++ (Yang and Zhang, 2018).11\nB.3 Training and Decoding\nThe model is trained with cross entropy loss using an SGD optimizer with momentum. Early stopping is used on the F1 score of the validation set, with a patience of 10 epochs. During training, negative sentences in the training set are downsampled to 90% (resampled every epoch) instead of 97%, which leads to 3x faster training time but minimal impact on performance. Validation and test sets are used in their entirety. Training hyperparameters are shown in Table 8. Training typically takes less than 2 hours to complete on a single GeForce RTX 2080 Ti GPU ."
    }, {
      "heading" : "C Confusion Matrices",
      "text" : "Figure 6 shows the confusionmatrices for the swap classification experiments. As mentioned in the main text, an in-context classification accuracy can be calculated from the tokens that are correctly identified as part of an EE but may or may not have a correct prediction of the orders (i.e. confuses B with B-fake). For example, the in-context classification accuracy for the CNN confusion matrix is\n\uD835\uDC4E\uD835\uDC50\uD835\uDC50\uD835\uDC36\uD835\uDC41\uD835\uDC41 = 439 + 447\n439 + 447 + 4 = 99.55% 10An extensive architecture search was not performed, because the purpose of the experiments is not to achieve the best performing model.\n11 https://github.com/jiesutd/NCRFpp, under Apache 2.0 License which permits use for research purposes."
    } ],
    "references" : [ {
      "title" : "Xiandai Hanyu Cidian",
      "author" : [ "Anonymous." ],
      "venue" : "Commercial Press, Beijing.",
      "citeRegEx" : "Anonymous.,? 2016",
      "shortCiteRegEx" : "Anonymous.",
      "year" : 2016
    }, {
      "title" : "Bahnar reduplication",
      "author" : [ "Elizabeth M. Banker." ],
      "venue" : "Mon-Khmer Studies, 1:119–134.",
      "citeRegEx" : "Banker.,? 1964",
      "shortCiteRegEx" : "Banker.",
      "year" : 1964
    }, {
      "title" : "Lectures on Government and Binding: The Pisa Lectures",
      "author" : [ "Noam Chomsky." ],
      "venue" : "Studies in Generative Grammar. de Gruyter.",
      "citeRegEx" : "Chomsky.,? 1981",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1981
    }, {
      "title" : "The Minimalist Program",
      "author" : [ "Noam Chomsky." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Chomsky.,? 1995",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1995
    }, {
      "title" : "The Sound Pattern of English",
      "author" : [ "Noam Chomsky", "Morris Halle." ],
      "venue" : "ERIC.",
      "citeRegEx" : "Chomsky and Halle.,? 1968",
      "shortCiteRegEx" : "Chomsky and Halle.",
      "year" : 1968
    }, {
      "title" : "Jingpo yu binglie jiegou fuheci de yuanyin hexie",
      "author" : [ "Qingxia Dai." ],
      "venue" : "Minzu Yuwen, 1986(5):23–29.",
      "citeRegEx" : "Dai.,? 1986",
      "shortCiteRegEx" : "Dai.",
      "year" : 1986
    }, {
      "title" : "Hypotheses of natural phonology",
      "author" : [ "Patricia Donegan andDavid Stampe." ],
      "venue" : "Poznań Studies in Contemporary Linguistics, 45(1):1–31.",
      "citeRegEx" : "Stampe.,? 2009",
      "shortCiteRegEx" : "Stampe.",
      "year" : 2009
    }, {
      "title" : "The study of natural phonology",
      "author" : [ "Patricia J Donegan", "David Stampe." ],
      "venue" : "Current approaches to phonological theory, 126173.",
      "citeRegEx" : "Donegan and Stampe.,? 1979",
      "shortCiteRegEx" : "Donegan and Stampe.",
      "year" : 1979
    }, {
      "title" : "Couplets and duplication in Mal",
      "author" : [ "David Filbeck." ],
      "venue" : "Mon-Khmer Studies, 26:91–106.",
      "citeRegEx" : "Filbeck.,? 1996",
      "shortCiteRegEx" : "Filbeck.",
      "year" : 1996
    }, {
      "title" : "Auditory representations in phonology",
      "author" : [ "Edward S Flemming." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Flemming.,? 2013",
      "shortCiteRegEx" : "Flemming.",
      "year" : 2013
    }, {
      "title" : "Cyclic linearization of syntactic structure",
      "author" : [ "Danny Fox", "David Pesetsky." ],
      "venue" : "Theoretical Linguistics, 31(1-2):1–45.",
      "citeRegEx" : "Fox and Pesetsky.,? 2005",
      "shortCiteRegEx" : "Fox and Pesetsky.",
      "year" : 2005
    }, {
      "title" : "The nature of phonological primes",
      "author" : [ "Eric C Fudge." ],
      "venue" : "Journal of Linguistics, 3(1):1–36.",
      "citeRegEx" : "Fudge.,? 1967",
      "shortCiteRegEx" : "Fudge.",
      "year" : 1967
    }, {
      "title" : "substance abuse” and “dysfunctionalism”: current trends in phonology",
      "author" : [ "Mark Hale", "Charles Reiss." ],
      "venue" : "Linguistic inquiry, 31(1):157–169.",
      "citeRegEx" : "Hale and Reiss.,? 2000",
      "shortCiteRegEx" : "Hale and Reiss.",
      "year" : 2000
    }, {
      "title" : "The phonological enterprise",
      "author" : [ "Mark Hale", "Charles Reiss." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Hale and Reiss.,? 2008",
      "shortCiteRegEx" : "Hale and Reiss.",
      "year" : 2008
    }, {
      "title" : "Elaborate expressions in Dai Lue",
      "author" : [ "William J. Hanna." ],
      "venue" : "Linguistics of the Tibeto-Burman Area, 36(1):33–56.",
      "citeRegEx" : "Hanna.,? 2013",
      "shortCiteRegEx" : "Hanna.",
      "year" : 2013
    }, {
      "title" : "Introductory phonology, volume 32",
      "author" : [ "Bruce Hayes." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Hayes.,? 2011",
      "shortCiteRegEx" : "Hayes.",
      "year" : 2011
    }, {
      "title" : "Phonological Naturalness and Phonotactic Learning",
      "author" : [ "Bruce Hayes", "James White." ],
      "venue" : "Linguistic Inquiry, 44(1):45–75.",
      "citeRegEx" : "Hayes and White.,? 2013",
      "shortCiteRegEx" : "Hayes and White.",
      "year" : 2013
    }, {
      "title" : "How concrete is phonology? Language, pages 58–76",
      "author" : [ "Larry M Hyman" ],
      "venue" : null,
      "citeRegEx" : "Hyman.,? \\Q1970\\E",
      "shortCiteRegEx" : "Hyman.",
      "year" : 1970
    }, {
      "title" : "Preliminaries to speech analysis: The distinctive features and their correlates",
      "author" : [ "Roman Jakobson", "C Gunnar Fant", "Morris Halle." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Jakobson et al\\.,? 1951",
      "shortCiteRegEx" : "Jakobson et al\\.",
      "year" : 1951
    }, {
      "title" : "Lexical and phonological sources of Hmong elaborate expressions",
      "author" : [ "Brenda Johns", "David Strecker." ],
      "venue" : "Linguistics of the Tibeto-Burman Area, 10(2):106–112.",
      "citeRegEx" : "Johns and Strecker.,? 1987",
      "shortCiteRegEx" : "Johns and Strecker.",
      "year" : 1987
    }, {
      "title" : "CharNER: Character-level named entity recognition",
      "author" : [ "Onur Kuru", "Ozan Arkan Can", "Deniz Yuret." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 911–921, Osaka, Japan. The",
      "citeRegEx" : "Kuru et al\\.,? 2016",
      "shortCiteRegEx" : "Kuru et al\\.",
      "year" : 2016
    }, {
      "title" : "On the ordering of elements in ideophonic echo-words versus prosaic dvandva compounds, with special reference to korean and japanese",
      "author" : [ "Nahyun Kwon", "Keiko Masuda." ],
      "venue" : "Journal of East Asian Linguistics, 28(1):29–53.",
      "citeRegEx" : "Kwon and Masuda.,? 2019",
      "shortCiteRegEx" : "Kwon and Masuda.",
      "year" : 2019
    }, {
      "title" : "Qieyun Yinxi",
      "author" : [ "Rong Li." ],
      "venue" : "Chinese Academy of Sciences, Beijing.",
      "citeRegEx" : "Li.,? 1952",
      "shortCiteRegEx" : "Li.",
      "year" : 1952
    }, {
      "title" : "The Grammar of Lahu",
      "author" : [ "James A.Matisoff." ],
      "venue" : "Number 75 inUniversity of California Publications in Linguistics. University of California Press, Berkeley.",
      "citeRegEx" : "A.Matisoff.,? 1973",
      "shortCiteRegEx" : "A.Matisoff.",
      "year" : 1973
    }, {
      "title" : "The Dictionary of Lahu",
      "author" : [ "James A. Matisoff." ],
      "venue" : "University of California Press, Berkeley and Los Angeles.",
      "citeRegEx" : "Matisoff.,? 1989",
      "shortCiteRegEx" : "Matisoff.",
      "year" : 1989
    }, {
      "title" : "English-Lahu Lexicon",
      "author" : [ "James A Matisoff." ],
      "venue" : "University of California Press, Berkeley and Los Angeles.",
      "citeRegEx" : "Matisoff.,? 2006",
      "shortCiteRegEx" : "Matisoff.",
      "year" : 2006
    }, {
      "title" : "Umap: Uniform manifold approximation and projection",
      "author" : [ "Leland McInnes", "John Healy", "Nathaniel Saul", "Lukas Großberger." ],
      "venue" : "Journal of Open Source Software, 3(29):861.",
      "citeRegEx" : "McInnes et al\\.,? 2018",
      "shortCiteRegEx" : "McInnes et al\\.",
      "year" : 2018
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Structure and substance in artificial-phonology learning, part i: Structure",
      "author" : [ "Elliott Moreton", "Joe Pater." ],
      "venue" : "Language and Linguistics Compass, 6(11):686–701.",
      "citeRegEx" : "Moreton and Pater.,? 2012a",
      "shortCiteRegEx" : "Moreton and Pater.",
      "year" : 2012
    }, {
      "title" : "Structure and substance in artificial-phonology learning, part ii: Substance",
      "author" : [ "Elliott Moreton", "Joe Pater." ],
      "venue" : "Language and Linguistics Compass, 6(11):702–718.",
      "citeRegEx" : "Moreton and Pater.,? 2012b",
      "shortCiteRegEx" : "Moreton and Pater.",
      "year" : 2012
    }, {
      "title" : "Logical and Substantive Scales in Phonology",
      "author" : [ "David R. Mortensen." ],
      "venue" : "Ph.D. thesis, University of California, Berkeley.",
      "citeRegEx" : "Mortensen.,? 2006",
      "shortCiteRegEx" : "Mortensen.",
      "year" : 2006
    }, {
      "title" : "Panphon: A resource for mapping IPA segments to articulatory feature vectors",
      "author" : [ "David R. Mortensen", "Patrick Littell", "Akash Bharadwaj", "Kartik Goyal", "Chris Dyer", "Lori S. Levin." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Con-",
      "citeRegEx" : "Mortensen et al\\.,? 2016",
      "shortCiteRegEx" : "Mortensen et al\\.",
      "year" : 2016
    }, {
      "title" : "Co-Compounds vs",
      "author" : [ "Tina Obermüller." ],
      "venue" : "Coordinate Compounds: A formal, functional and semantic analysis. AV Akademikerverlag, Saarbrücken.",
      "citeRegEx" : "Obermüller.,? 2015",
      "shortCiteRegEx" : "Obermüller.",
      "year" : 2015
    }, {
      "title" : "Four-syllable coordinative constructions in the Miao languages of eastern Kweichow",
      "author" : [ "Yuanen Pan", "Cuiyin Cao." ],
      "venue" : "Herbert C. Purnell, editor, Miao and Yao Linguistic Studies, number 88 in Data Papers, pages 211–234. Cornell University Southeast",
      "citeRegEx" : "Pan and Cao.,? 1972",
      "shortCiteRegEx" : "Pan and Cao.",
      "year" : 1972
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Constraints on variables in syntax",
      "author" : [ "John Robert Ross." ],
      "venue" : "Ph.D. thesis, Massachusetts Institute of Technology.",
      "citeRegEx" : "Ross.,? 1967",
      "shortCiteRegEx" : "Ross.",
      "year" : 1967
    }, {
      "title" : "Phonological conditions on variable adjective and nounword order in tagalog",
      "author" : [ "Stephanie S Shih", "Kie Zuraw." ],
      "venue" : "Language, 93(4):e317–e352.",
      "citeRegEx" : "Shih and Zuraw.,? 2017",
      "shortCiteRegEx" : "Shih and Zuraw.",
      "year" : 2017
    }, {
      "title" : "Directional asymmetries in place assimilation: A perceptual account",
      "author" : [ "Donca Steriade" ],
      "venue" : "The role of speech perception in phonology, pages 219–250.",
      "citeRegEx" : "Steriade,? 2001",
      "shortCiteRegEx" : "Steriade",
      "year" : 2001
    }, {
      "title" : "Tonal relationship between the two consistuent of the coordinate construction in the analects, the meng-tze, and the book of odes",
      "author" : [ "Pa-Hsin Ting." ],
      "venue" : "Bulletin of the Institute of History and Philology, Academia Sinica, 47(1):17–52.",
      "citeRegEx" : "Ting.,? 1975",
      "shortCiteRegEx" : "Ting.",
      "year" : 1975
    }, {
      "title" : "Postverbal constituent ordering in English, pages 119– 154",
      "author" : [ "Thomas Wasow", "Jennifer Arnold." ],
      "venue" : "De Gruyter Mouton.",
      "citeRegEx" : "Wasow and Arnold.,? 2003",
      "shortCiteRegEx" : "Wasow and Arnold.",
      "year" : 2003
    }, {
      "title" : "Reduplication in Pacoh",
      "author" : [ "Richard L. Watson." ],
      "venue" : "Master’s thesis, The Hartford Seminary Foundation.",
      "citeRegEx" : "Watson.,? 1966",
      "shortCiteRegEx" : "Watson.",
      "year" : 1966
    }, {
      "title" : "Burmese: A Grammatical Sketch",
      "author" : [ "Julian Wheatley." ],
      "venue" : "Ph.D. thesis, University of California, Berkeley.",
      "citeRegEx" : "Wheatley.,? 1982",
      "shortCiteRegEx" : "Wheatley.",
      "year" : 1982
    }, {
      "title" : "Design challenges and misconceptions in neural sequence labeling",
      "author" : [ "Jie Yang", "Shuailong Liang", "Yue Zhang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Ncrf++: An opensource neural sequence labeling toolkit",
      "author" : [ "Jie Yang", "Yue Zhang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Yang and Zhang.,? 2018",
      "shortCiteRegEx" : "Yang and Zhang.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964).",
      "startOffset" : 166,
      "endOffset" : 299
    }, {
      "referenceID" : 8,
      "context" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964).",
      "startOffset" : 166,
      "endOffset" : 299
    }, {
      "referenceID" : 19,
      "context" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964).",
      "startOffset" : 166,
      "endOffset" : 299
    }, {
      "referenceID" : 41,
      "context" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964).",
      "startOffset" : 166,
      "endOffset" : 299
    }, {
      "referenceID" : 33,
      "context" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964).",
      "startOffset" : 166,
      "endOffset" : 299
    }, {
      "referenceID" : 40,
      "context" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964).",
      "startOffset" : 166,
      "endOffset" : 299
    }, {
      "referenceID" : 1,
      "context" : "In many languages of East and Southeast Asia, there are common constructions in which two words or phrases are coordinated without an overt marker like a conjunction (Hanna, 2013; Filbeck, 1996; Johns and Strecker, 1987; Wheatley, 1982; Matisoff, 1973; Pan and Cao, 1972; Watson, 1966; Banker, 1964).",
      "startOffset" : 166,
      "endOffset" : 299
    }, {
      "referenceID" : 32,
      "context" : "Coordinating compounds are found throughout the world, with varying semantic relationships between the whole and the parts (Obermüller, 2015).",
      "startOffset" : 123,
      "endOffset" : 141
    }, {
      "referenceID" : 38,
      "context" : "Many of the proposed ordering hierarchies are based on phonology (Ting, 1975; Dai, 1986; Mortensen, 2006).",
      "startOffset" : 65,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Many of the proposed ordering hierarchies are based on phonology (Ting, 1975; Dai, 1986; Mortensen, 2006).",
      "startOffset" : 65,
      "endOffset" : 105
    }, {
      "referenceID" : 30,
      "context" : "Many of the proposed ordering hierarchies are based on phonology (Ting, 1975; Dai, 1986; Mortensen, 2006).",
      "startOffset" : 65,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Building on this earlier work, Mortensen (2006) posited that Lahu EE orders could be predicted based on vowel quality—like Jingpho (Dai, 1986)—and that Hmong EE orders could be predicted based on tone, echoing earlier claims for Chinese and Qe-Nao (Ting, 1975; Pan and Cao, 1972).",
      "startOffset" : 131,
      "endOffset" : 142
    }, {
      "referenceID" : 38,
      "context" : "Building on this earlier work, Mortensen (2006) posited that Lahu EE orders could be predicted based on vowel quality—like Jingpho (Dai, 1986)—and that Hmong EE orders could be predicted based on tone, echoing earlier claims for Chinese and Qe-Nao (Ting, 1975; Pan and Cao, 1972).",
      "startOffset" : 248,
      "endOffset" : 279
    }, {
      "referenceID" : 33,
      "context" : "Building on this earlier work, Mortensen (2006) posited that Lahu EE orders could be predicted based on vowel quality—like Jingpho (Dai, 1986)—and that Hmong EE orders could be predicted based on tone, echoing earlier claims for Chinese and Qe-Nao (Ting, 1975; Pan and Cao, 1972).",
      "startOffset" : 248,
      "endOffset" : 279
    }, {
      "referenceID" : 7,
      "context" : "Even more radical statements about the relationship between phonological form and substance have been made since then (Donegan and Stampe, 1979; Flemming, 2013; Hayes, 2011; Donegan and Stampe, 2009; Steriade et al., 2001).",
      "startOffset" : 118,
      "endOffset" : 222
    }, {
      "referenceID" : 9,
      "context" : "Even more radical statements about the relationship between phonological form and substance have been made since then (Donegan and Stampe, 1979; Flemming, 2013; Hayes, 2011; Donegan and Stampe, 2009; Steriade et al., 2001).",
      "startOffset" : 118,
      "endOffset" : 222
    }, {
      "referenceID" : 15,
      "context" : "Even more radical statements about the relationship between phonological form and substance have been made since then (Donegan and Stampe, 1979; Flemming, 2013; Hayes, 2011; Donegan and Stampe, 2009; Steriade et al., 2001).",
      "startOffset" : 118,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : "While there has never been a complete consensus on the matter (Fudge, 1967; Hyman, 1970; Hale and Reiss, 2000, 2008), it has been widely assumed that phonological patterns that are phonetically incoherent cannot be learned by humans or can be learned only with difficulty (Hayes and White, 2013).",
      "startOffset" : 62,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "While there has never been a complete consensus on the matter (Fudge, 1967; Hyman, 1970; Hale and Reiss, 2000, 2008), it has been widely assumed that phonological patterns that are phonetically incoherent cannot be learned by humans or can be learned only with difficulty (Hayes and White, 2013).",
      "startOffset" : 62,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "While there has never been a complete consensus on the matter (Fudge, 1967; Hyman, 1970; Hale and Reiss, 2000, 2008), it has been widely assumed that phonological patterns that are phonetically incoherent cannot be learned by humans or can be learned only with difficulty (Hayes and White, 2013).",
      "startOffset" : 272,
      "endOffset" : 295
    }, {
      "referenceID" : 35,
      "context" : "It has long been suggested that dative shift in English is sensitive to phonological weight (Ross, 1967) although this claim has also been long contested (Wasow and Arnold, 2003).",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : "It has long been suggested that dative shift in English is sensitive to phonological weight (Ross, 1967) although this claim has also been long contested (Wasow and Arnold, 2003).",
      "startOffset" : 154,
      "endOffset" : 178
    }, {
      "referenceID" : 21,
      "context" : "Some newer evidence comes from coordinate compound and echo reduplication constructions in Japanese, Korean, and Jingpho (Kwon and Masuda, 2019; Dai, 1986).",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "Some newer evidence comes from coordinate compound and echo reduplication constructions in Japanese, Korean, and Jingpho (Kwon and Masuda, 2019; Dai, 1986).",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "2An important caveat is that—in some versions of generative grammar—syntactic structures are pure hierarchy and are not linearized until PF (phonetic form), when abstract lexical and functional categories are “spelled-out” (Fox and Pesetsky, 2005).",
      "startOffset" : 223,
      "endOffset" : 247
    }, {
      "referenceID" : 36,
      "context" : "more interesting case comes from Tagalog nounadjective order, which is sometimes viewed as being free but which is actually sensitive to a set of phonological constraints (Shih and Zuraw, 2017).",
      "startOffset" : 171,
      "endOffset" : 193
    }, {
      "referenceID" : 31,
      "context" : "We found that one-hot features were sufficiently expressive, and that using articulatory features (Mortensen et al., 2016) did not further improve the performance.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 22,
      "context" : "5Reconstruction from (Li, 1952) 6Classification models are trained using scikit-learn (Pedregosa et al.",
      "startOffset" : 21,
      "endOffset" : 31
    }, {
      "referenceID" : 34,
      "context" : "5Reconstruction from (Li, 1952) 6Classification models are trained using scikit-learn (Pedregosa et al., 2011) sibility, we also report results on randomly sampled subsets of EEs wherein all (B1, B2) pairs are unique (so that there is no contamination across the train and test sets).",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 30,
      "context" : "(A, B1, B2) are proper Hmong syllables parsable by a regular expression classifier; (2) set a word vector similarity threshold between the two CC words (B1 and B2) so that cosine(vB1 , vB2) > α, since many Hmong EEs have the two CC words of similar meanings (Mortensen, 2006);8 (3) ensure the tonal scale in Table 2 is followed between B1 and B2",
      "startOffset" : 258,
      "endOffset" : 275
    }, {
      "referenceID" : 27,
      "context" : "8Word vectors are trained usingWord2Vec (Mikolov et al., 2013).",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 42,
      "context" : ", NER), where character level features are found to improve performance (Yang et al., 2018; Kuru et al., 2016).",
      "startOffset" : 72,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : ", NER), where character level features are found to improve performance (Yang et al., 2018; Kuru et al., 2016).",
      "startOffset" : 72,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "Figure 1 shows the UMAP projection (McInnes et al., 2018) of two types of learned embeddings into 2D space.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 27,
      "context" : "Embeddings from the tagging model show clear separation between words that tend to occur first in an EE (in the B1 position) and words that tend to occur second, where as embeddings trained separately on the SkipGram algorithm (Mikolov et al., 2013) show no separation.",
      "startOffset" : 227,
      "endOffset" : 249
    } ],
    "year" : 0,
    "abstractText" : "Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate constructions common in languages of East and Southeast Asia. Mortensen (2006) claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese can be predicted via phonological hierarchies and (2) these phonological hierarchies lack a clear phonetic rationale. These claims are significant because morphosyntax has often been seen as in a feed-forward relationship with phonology, and phonological generalizations have often been assumed to be phonetically “natural”. We investigate whether the ordering of CCs and EEs can be learned empirically and whether computational models (classifiers and sequence labeling models) learn unnatural hierarchies similar to those posited by Mortensen (2006). We find that decision trees and SVMs learn to predict the order of CCs/EEs on the basis of phonology, with DTs learning hierarchies strikingly similar to those proposed by Mortensen. However, we also find that a neural sequence labeling model is able to learn the ordering of elaborate expressions in Hmong very effectively without using any phonological information. We argue that EE ordering can be learned through two independent routes: phonology and lexical distribution, presenting a more nuanced picture than previous work. [ISO 639-3: hmn, lhu, cmn]",
    "creator" : null
  }
}
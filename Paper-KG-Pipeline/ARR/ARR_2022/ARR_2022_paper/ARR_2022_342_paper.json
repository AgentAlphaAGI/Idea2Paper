{
  "name" : "ARR_2022_342_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hate Speech and Counter Speech Detection: Context Does Matter",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The advent of social media has democratized public discourse on an unparalleled scale. Meanwhile, it is considered a particularly conducive arena for hate speech (Caiani et al., 2021). Online hate speech is prevalent and can lead to serious consequences. At the individual level, the victims targeted by hate speech are frightened of online threats that may materialize in the real world (Olteanu et al., 2018). At the societal level, it has been reported that there is an upsurge in offline hate crimes targeting minorities (Olteanu et al., 2018; Farrell et al., 2019).\nTwo types of strategies have been implemented or studied to combat online hate: disruption and counter speech. Disruption refers to blocking hateful content or users temporally or permanently on\na platform. To make the solution scalable, automated detection algorithms have been invented to identify hate (Waseem and Hovy, 2016; Davidson et al., 2017; Nobata et al., 2016). While these interventions could de-escalate the impact of hate speech to some extent, they may violate online free speech (Mathew et al., 2019). Additionally, attacks at the micro-level may be ineffective as hate networks often have rapid rewiring and selfrepair mechanisms (Johnson et al., 2019). Counter speech refers to the “direct response that counters hate speech” (Mathew et al., 2019), which is considered a remedy to address hate speech. It has been supported by theoretical and empirical studies to be more effective in the long term (Richards and Calvert, 2000; Mathew et al., 2020). Identifying hate and counter speech in natural conversations is critical to understand effective counter speech strategies and thus automatically generate counter speech against hate speech.\nMost corpora with either hate speech (Hate) or counter speech (Counter-hate) annotations do not include the conversational context. Indeed, they annotate a user-generated comment as Hate\nor Counter-hate based on the comment in isolation (Davidson et al., 2017; Waseem and Hovy, 2016; Mathew et al., 2019; Ziems et al., 2020). Therefore, systems trained on these corpora fail to consider the effect of contextual information on the identification of Hate and Counter-hate. Recent studies have shown that context affects annotations in toxicity and abuse detection (Pavlopoulos et al., 2020; Menini et al., 2021). We further investigate the effect of context on the task of identifying Hate and Counter-hate. Table 1 shows examples1 where a comment, denoted as Target, is Hate, Neutral or Counter-hate depending on whether the preceding comment, denoted as Parent, is taken into account. In the top example, the Target goes from Neutral to Hate when taking into account the Parent: it becomes clear that the author is disparaging short people. In the bottom example, the Target goes from Hate to Counter-hate as the author uses offensive language to counter the hateful content in the Parent. This is a common strategy to express counter speech (Mathew et al., 2019).\nIn this study we focus on the following questions:\n1. Does conversational context affect if a comment is perceived as Hate, Neutral, or Counterhate by humans? (It does.) 2. Do models to identify Hate, Neutral, and Counter-hate benefit from incorporating context? (They do.)\nTo answer the first question, we create a collection of (Parent, Target) Reddit comments and annotate the Targets with three labels (Hate, Neutral, Counter-hate) in two seperate tasks: showing annotators (a) only the Target or (b) the Parent and the Target. We find that human judgments are substantially different when the Parent is shown. Thus the task of annotating Hate and Counter-hate requires taking into account context. To answer the second question, we experiment with contextunaware and context-aware classifiers to detect if a given Target is Hate, Neutral, or Counter-hate. Results show that adding context does benefit the classifiers significantly. In summary, the main contributions of this paper are:2 (a) a corpus with 6,846 pairs of (Parent, Target) Reddit comments and annotations indicating whether each Target is Hate, Neutral, or Counter-hate; (b) analysis of annotations showing that the problem requires taking into\n1The examples in this paper contain hateful content. We cannot avoid it due to the nature of our work.\n2Code and data available at anonymous_GitHub_link\naccount context, as the ground truth changes otherwise; (c) corpus analysis detailing the kind of language people use to express Hate and Counterhate; (d) experiments showing that context-aware neural models obtain significantly better results; and (e) qualitative analysis revealing when context is beneficial and the remaining errors made by the best context-aware model."
    }, {
      "heading" : "2 Related Work",
      "text" : "Hate speech in user-generated content has been an active research area recently (Fortuna and Nunes, 2018). Researchers have built several datasets for hate speech detection from diverse sources like Twitter (Waseem and Hovy, 2016; Davidson et al., 2017), Yahoo! (Nobata et al., 2016), Fox News (Gao and Huang, 2017), Gab (Mathew et al., 2021) and Reddit (Qian et al., 2019).\nCompared to hate speech detection, few studies focus on detecting counter speech (Mathew et al., 2019; Ziems et al., 2020; Garland et al., 2020). Mathew et al. (2019) collect and handcode 6,898 counter hate comments from YouTube videos targeting Jews, Blacks and LGBT communities. Ziems et al. (2020) use a collection of hate and counter hate keywords relevant to COVID-19 and create a dataset containing 359 counter hate tweets targeting Asians. Garland et al. (2020) work with German tweets and define hate and counter speech based on the communities to which the authors belong. Another line of research focuses on curating datasets for counter speech generation using crowdsourcing (Qian et al., 2019) or with the help of trained operators (Chung et al., 2019; Fanton et al., 2021). However, synthetic language is rarely as rich as language in the wild. Even if it were, conclusions and models from synthetic data may not transfer to the real world. In this paper, we work with user-generated content expressing hate and counter-hate rather than synthetic content.\nTable 2 summarizes existing datasets for Hate and Counter-hate detection. Most of them do not include context information. In other words, the preceding comments are not provided when annotating Targets. Context does affect human judgments and has been taken into account for Hate detection (Gao and Huang, 2017; Vidgen et al., 2021; Pavlopoulos et al., 2020; Menini et al., 2021). Gao and Huang (2017) annotate hateful comments in the nested structures of 10 Fox News discussion threads. Vidgen et al. (2021) introduce a dataset of\nReddit comments with hate annotations taking into account context. Both studies use contextual information without identifying the role context plays in the annotation and detection. Pavlopoulos et al. (2020) allow annotators to see one previous comment to annotate Wikipedia conversations. They find context matters in the annotation but provide no empirical evidence showing whether models to detect toxicity benefit from incorporating context. Menini et al. (2021) re-annotate an existing corpus to investigate the role of context in abusive language. They found context does matter. Utilizing conversational context has also been explored in text classification tasks such as sentiment analysis (Ren et al., 2016), stance (Zubiaga et al., 2018) and sarcasm (Ghosh et al., 2020). To our knowledge, we are the first to investigate the role of context in Hate and Counter-hate detection."
    }, {
      "heading" : "3 Dataset Collection and Annotation",
      "text" : "We first describe our procedure to collect (Parent, Target) pairs, where both Parents and Targets are Reddit comments. Then, we describe the annotation guidelines and the two annotation phases: showing annotators (a) only the Target and (b) the Parent and Target. The two independent phases allow us to quantify how often context affects the annotation of Hate and Counter-hate.\n3.1 Collecting (Parent, Target) pairs\nIn this work, we focus on Reddit, a popular social media site. It is an ideal platform for data collection due to the large size of user populations and many diverse topics (Baumgartner et al., 2020). We start with a set of 1,726 hate words from two lexicons: Hatebase3 and a harassment\n3http://hatebase.org/\ncorpus (Rezvan et al., 2018). We remove ambiguous words following ElSherief et al. (2018). To collect (Parent, Target) pairs, we use the following steps. First, we retrieve comments containing at least one hate word (commentw/ hateword). Second, we create a (Parent, Target) pair using commentw/ hateword as Target and its preceding comment as Parent. Third, we create a (Parent, Target) pair using commentw/ hateword as Parent and each of its replies as Target. Lastly, we remove pairs if the same author posted the Parent and Target. We retrieve 6,846 (Parent, Target) pairs with PushShift (Baumgartner et al., 2020) from 416 submissions in order to keep the annotation costs reasonable while creating a (relatively) large corpus. We also collect the discussion title for each pair."
    }, {
      "heading" : "3.2 Annotation Guidelines",
      "text" : "To identify whether a Target is Hate, Neutral, or Counter-hate, we crowdsource human judgments from non-experts. Our guidelines reuse the definitions of Hate by Ward (1997) and Counter-hate by Mathew et al. (2019) and Vidgen et al. (2021):\n• Hate: the author attacks an individual or a group with the intention to vilify, humiliate, or incite hatred; • Counter-hate: the author challenges, condemns the hate expressed in another comment or call out a comment for being hateful; • Neutral: the author neither conveys hate nor opposes hate expressed in another comment. Annotation Process We chose Amazon Mechanical Turk (MTurk) as the crowdsourcing platform. We replace user names with placeholders (User_A and User_B) owing to privacy concerns. The annotations took place in two independent phases. In the first phase, annotators are first shown the Parent comment. After a short delay, they click a\nbutton to show the Target and then after another short delay they submit their annotation. Delays are at most a few seconds and proportional to the length of the comments. Our rationale behind the delays is to “force” annotators to read the Parent and Target in order. In the second phase, annotators label each Target without seeing the preceding Parent comment. A total of 375 annotators were involved in the first phase and 299 in the second phase. There is no overlap between annotators thus we eliminated the possibility of biased annotators remembering the Parent in the second phase.\nAnnotation Quality Crowdsourcing may attract spammers (Sabou et al., 2014). For quality control, we first set a few requirements for annotators: they must be located in the US and have a 95% approval rate over at least 100 Human Intelligence Tasks (HITs). We also block annotators who submit more than 10 HITs with an average completion time below 5 seconds (half the time required in our pilot study). As the corpus contains vulgar words, we require annotators to pass the Adult Content Qualification Test. The reward per HIT is $0.05.\nThe second effort is to identify bad annotators and filter out their annotations until we obtain substantial inter-annotator agreement. We collect five annotations per HIT. Then, we use MACE (Hovy et al., 2013, Multi-Annotator Competence Estimation) and Krippendorff’s α (Krippendorff, 2011). MACE is devised to rank annotators by their competence and recover adjudicate labels grounded on annotator’s competence (not the majority label). Krippendorff’s α estimates inter-annotator agreement: α coefficients at or above 0.6 are considered substantial (above 0.8 are considered nearly perfect) (Artstein and Poesio, 2008). We repeat the following steps until α ≥ 0.6:\n1. Use MACE to calculate the competence score of all annotators. 2. Discard all the annotations by the annotator with the lowest MACE score. 3. Check Krippendorff’s α on the remaining annotations. Go to (1) if α < 0.6.\nThe final corpus consists of 6,846 (Parent, Target) pairs and a label assigned to each Target (Hate, Counter-hate, or Neutral). The ground truth we experiment with (Section 5) is the label obtained taking into account the Parent (first phase)—the second phase, which disregards the Parent, was conducted for analysis purposes (Section 4). We split the corpus into two subsets: (a) Gold (4,751\npairs with α ≥ 0.6) and (b) Silver (2,095 remaining pairs). As we shall see, the Silver pairs are useful to learn models."
    }, {
      "heading" : "4 Corpus Analysis",
      "text" : "Does conversational context affect if a comment is perceived as Hate or Counter-hate? Yes, it does. Table 3 presents the percentage of labels that change and remain the same depending on whether annotators are shown the Parent, i.e., the context. Many Target comments that are perceived as Hate or Counter-hate become Neutral (34.2% and 55.1% respectively) when the Parent is provided. More surprisingly, many Target comments are perceived with the opposite label (from Hate to Counter-hate (8.4%) or from Counter-hate to Hate (18.7%)) when the Parent comments are shown.\nWe show examples of label changes in Table 4. In the first example, annotators identify the Target (“A brick is more effective.”) as Neutral without seeing the Parent. In fact, a female is the target of hate in the Parent, and the author of Target replies with even more hatred (and the ground truth label is Hate). In the second example, the Target alone is insufficient to tell if it is Counter-hate. When annotators see the Parent, however, they understand\nthe author of Target counters the hateful content in the Parent by showing empathy towards the mail carrier. In the last example, the Target alone is considered Hate because it attacks someone by using the phrase “sick person”. When the Parent is shown, however, the annotators understand the Target as calling out the Parent to be inappropriate.\nLabel distribution and linguistic insights Figure 1 shows the label distribution for all pairs (rightmost column in each block) and for pairs in which commentw/ hateword (i.e., the comment containing at least one hate word) is the Parent or Target. The most frequent label assigned to Target comments is Neutral (49%) followed by Hate (28%) and Counter-hate (23%). While Target comments containing a hate word are likely to be Hate (45%), some are Counter-hate (19%) with context.\nWe analyze the linguistic characteristics of Titles, Parents and Targets when the Targets are Hate or Counter-hate with context to shed light on the differences between the language people use in hate and counter speech. We combine the set of\nhate words with profanity words4 to count the profanity words. We analyze the components of linguistic features using the Sentiment Analysis and Cognition Engine (SEANCE) lexicon, a popular tool for psychological linguistic analysis (Crossley et al., 2017). Statistical tests are conducted using unpaired t-tests between the groups, of which the Targets are Counter-hate or Hate (Table 5). As we are performing multiple hypothesis tests, we also report whether each feature passes the Bonferroni correction. We draw several interesting insights:\n• Questions Marks in Target signal Counterhate. We observe that people are inclined to use rhetorical questions as a way to counter hateful comments. • Fewer 1st person pronouns (e.g., I, me) and more 2nd person pronouns (e.g., you, your) in the Parent signal that the Target is more likely to be Counter-hate. This is due to the fact that people tend to target others instead of themselves in hateful content. • High profanity count in the Parent signals that the Target is Counter-hate, while high profanity count in the Target signals Hate. • More words related to awareness, enlightenment and problem-solving in the Target signal Counter-hate. • When there are more negative words in the Parent, the Target tends to be Counter-hate. Targets labeled as Counter-hate contain fewer negative and disgusting words.\n4https://github.com/RobertJGabriel/google-profanitywords-node-module/blob/master/lib/profanity.js"
    }, {
      "heading" : "5 Experiments and Results",
      "text" : "We build neural network models to identify if a Target comment is Hate, Counter-hate, or Neutral. We split Gold instances (4,751) as follows: 70% for training, 15% for validation and 15% for testing. Silver instances are only used for training. Neural Network Architecture and Training We experiment with neural classifiers built on top of the RoBERTa transformer (Liu et al., 2019). The neural architecture consists of a pretrained RoBERTa transformer, a fully connected layer with 768 neurons and Tanh activation, and another fully connected layer with 3 neurons and softmax activation to make predictions (Hate, Counter-hate, or Neutral). To investigate the role of context, we consider two textual inputs:\n• the Target alone (Target), and • the Parent and the Target (Parent_Target).\nWe concatenate the Target and the Parent with the [SEP] special token. We report hyperparameters as well as other implementation details in the supplementary materials. We also experiment models that take the title of a discussion as part of the context, but it is not beneficial.\nWe implement two strategies to enhance the performance of neural models: Blending Gold and Silver We adopt the method by Shnarch et al. (2018) to determine whether Silver annotations are beneficial. There are two phases in the training process: m blending epochs using all Gold and a fraction of Silver, and then n epochs using all Gold. In each blending epoch, Silver instances are fed in a random order to the network. The fraction of Silver is determined by a blending\nfactor α∈ [0..1]. The first blending epoch is trained with all Gold and all Silver, and the amount of Silver to blend is reduced by α in each epoch. Pretraining with Related Tasks We also experiment with several corpora to investigate whether pretraining with related tasks is beneficial. Specifically, we pretrain our models with existing corpora annotating: (1) hateful comments: hateful or not hateful (Qian et al., 2019), and hate speech, offensive, or neither (Davidson et al., 2017); (2) sentiment: negative, neutral, or positive (Rosenthal et al., 2017); (3) sarcasm: sarcasm or not sarcasm (Ghosh et al., 2020); and (4) stance: agree, neutral, or attack (Pougué-Biyong et al., 2021)."
    }, {
      "heading" : "5.1 Quantitative Results",
      "text" : "We present results with the test split in Table 6. The majority baseline always predicts Neutral. The remaining rows present the results with the different training settings: training with the Target or both the Parent and Target; training with only Gold or blending Silver annotations; and pretraining with related tasks. We provide here results pretraining with the most beneficial task, stance detection, and the supplementary materials provide detailed results pretraining with all the related tasks.\nBlending Gold and Silver annotations requires tuning the α factor. We did so empirically using the training and validations splits, like any other hyperparameters. We found the optimal value to be 0.3 when blending Silver and 1.0 when utilizing both strategies.\nAs shown in Table 6, blending Gold and Silver annotations obtains better results by a small margin (Target: 0.61 vs. 0.58; Parent_Target: 0.63\nvs. 0.61). We also find that models pretrained for stance detection obtain better results than pretrained with other datasets. Pretraining with stance detection data benefits models trained without context (Target: 0.61 vs. 0.58) and models with context (Parent_Target: 0.63 vs. 0.61). These results indicate that these models have successfully transferred knowledge about stance between Parent and Target into the task of detecting whether the Target is Hate, Counter-Hate or Neutral.\nFrom the results obtained when using neither of the two strategies, we observe: First, using the Target alone obtains much better results than the majority baseline (0.58 vs. 0.34). In other words, modeling the Target alone allows the network to identify some instances of Hate and Counter-hate despite the ground truth requires the Parent. Second, incorporating the Parent comment is beneficial (0.61 vs. 0.58). The difference is statistically significant when we in the meanwhile blend Silver or pretrain with related tasks (0.63 vs. 0.58).\nFinally, the network pretrained with stance detection task first and then blending Silver in the training achieves the best performance (Parent_Target+Silver+Related task: 0.64). This result is statistically significant (p < 0.01) compared to the model trained with Target without blending Silver and pretraining with related tasks."
    }, {
      "heading" : "6 Qualitative Analysis",
      "text" : "When is adding the context beneficial? When does our best model make mistakes? To investigate these questions, we perform a qualitative analysis. In particular, we answer the following questions:\n• The errors made by the Target only net-\nwork (Trained with Target) that are fixed by the context-aware network (Trained with Parent_Target) (Table 7). • The errors made by the context-aware network pretrained on related task (stance) and blending Silver annotations (Parent_Target+Silver+Related task) (Table 8).\nWhen does the context complement Target? We manually analyze the errors made by the network using only the Target (Trained with Target) that are fixed by the context-aware network (Trained with Parent_Target). Table 7 exemplifies the most common error types.\nThe most frequent type of error fixed by the context-aware model is when there is Lack of information in the Target (48%). In this case, the Parent comment is crucial to determine the label of the Target. In the example, knowing what the author of Target refers to (i.e., a rhetorical question, Women can hover?) is crucial to determine that the Target is humiliating women as a group.\nThe second most frequent error type is Negation (27%). In the example in Table 7, taking into account the Parent allows the context-aware network to identify that the author of the Target is scolding the author of Parent and countering hate.\nNobata et al. (2016) and Qian et al. (2019) have pointed out that sarcasm and irony make detecting abusive and hateful content difficult. We find evidence supporting this claim. We also discover that by incorporating the Parent comment, a substantial amount of these errors are fixed. Indeed, 17% of errors fixed by the context-aware network include sarcasm or irony in the Target comment.\nFinally, the context-aware network taking into account the Parent fixes many errors (8%) in which the Target comment is Hate despite it does not contain swear words. In the example, the Target is introducing additional hateful content, which can be identified by the context-aware model when the Parent information is used.\nWhen does the best model make errors? In order to find out the most common error types made by the best model (context-aware, Parent_Target+Silver+Related task), we manually analyze 200 random samples in which the output of the network differs from the ground truth. Table 8 shows the results of the analysis.\nDespite 27% of errors fixed by the context-aware network (i.e., taking into account the Parent) include negation in the Target, negation is the most common type of errors made by our best network (28%). The example in Table 8 is especially challenging as it includes a double negation.\nWe observe that Rhetorical questions are almost as common (27%). This finding is consistent with the findings by Schmidt and Wiegand (2017). In the example, the best model fails to realize that the Target is hateful, as it disdains the author of Parent.\nSwear words are also the reason for a substantial number of errors. In particular, wrongly predicting a Target without swear words as Counter-hate or Neutral accounts for 8% of errors, and wrongly predicting a Target with swear words as Hate accounts for another 8% of errors. As pointed out by Davidson et al. (2017), hate speech may not contain hate or swear words at all. And vice versa, comments\ncontaining swear words may not be hateful (Zhang and Luo, 2019).\nFinally, we observe Intricate text in 7% errors. Our best network considers the Target (\"I have lost all respect for her.\") to agree with the hateful Parent, thus it is predicted as Hate in the final example. Indeed, the author of Target expresses his/her attitude without vilifying others. Hence, the ground truth label is Neutral."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "Context does matter in Hate and Counter-hate detection. We have demonstrated so by (a) analyzing whether humans perceive user-generated content as Hate or Counter-hate depending on whether we show them the Parent comment and (b) investigating whether neural networks benefit from incorporating the Parent. We find that 38.3% of human judgments change when we show the Parent to annotators. Experimental results demonstrate that networks incorporating the Parent yield better results. Additionally, we have also shown that noisy instances (Silver data) and pretraining with relevant datasets can improve model performance.\nWe have created and released a corpus of 6,846 (Parent, Target) pairs of Reddit comments with the Target annotated as Hate, Neutral or Counterhate. As part of our future work, we plan to include broader context, such as all previous comments of a Target. Also, we observe a few counter hate replies in our dataset containing hate words. Our research agenda also includes investigating the effect of different types of counter speech and which type leads to the de-escalation of hate."
    }, {
      "heading" : "A Ethical Considerations",
      "text" : "We use the PushShift API to collect data from Reddit5. Our collection is consistent with Reddit’s Terms of Service. The data are accessed through the data dumps on Google’s BigQuery using Python6.\nReddit can be considered a public space for discussion which differs from a private messaging service (Vidgen et al., 2021). Users consent to have their data made available to third parties including academics when they sign up to Reddit. Existing ethical guidance indicates that in this situation explicit consent is not required from each user (Procter et al., 2019). We encrypt the users as User_A or User_B to avoid identification of users. In compliance with Reddit policy, we would like to make sure that our dataset will be reused for non-commercial research only7.\nThe Reddit comments in this dataset were annotated by annotators using Amazon Mechanical Turk. We have followed all requirements introduced by the platform for tasks containing adult content. A warning was added in the task title. Annotators need to pass Adult Content Qualification\n5https://pushshift.io/api-parameters/ 6https://pushshift.io/ using-bigquery-with-reddit-data/ 7https://www.reddit.com/wiki/api-terms\nTest before working on our tasks. Annotators were compensated on average with 8 US$ per hour, we paid them whenever we accept their annotations or not. Annotators’ IDs are not included in the dataset following the same principle to avoid profiling."
    }, {
      "heading" : "B Annotation Interface",
      "text" : ""
    }, {
      "heading" : "C Detailed Results",
      "text" : "Table 9 presents detailed results complementing Table 6 in the paper. We provide Precision, Recall and weighted F1-score using each related task for pretraining when the input is Target and Parent_Target respectively in Table 9."
    }, {
      "heading" : "D Hyperparamters to Fine-tune the System for Each of the Training Settings",
      "text" : "The neural model takes about half an hour on average to train on a single GPU of NVIDIA TITAN Xp. We use an implementation by Phang et al. (2020) and fine-tune RoBERTa (base architecture; 12 layers) (Liu et al., 2019) model for each of the four training settings. For each setting, we set the hyperparameters to be the same when the textual input is Target and Parent_Target respectively. Hence we only report tuned hyperparameters for each setting when the input is Target in Table 10."
    } ],
    "references" : [ {
      "title" : "Inter-coder agreement for computational linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Comput. Linguist., 34(4):555–596.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "The pushshift reddit dataset",
      "author" : [ "Jason Baumgartner", "Savvas Zannettou", "Brian Keegan", "Megan Squire", "Jeremy Blackburn." ],
      "venue" : "Proceedings of the Fourteenth International AAAI Conference on Web and Social Media, ICWSM 2020, Held Virtually, Origi-",
      "citeRegEx" : "Baumgartner et al\\.,? 2020",
      "shortCiteRegEx" : "Baumgartner et al\\.",
      "year" : 2020
    }, {
      "title" : "Online hate speech and the radical right in times of pandemic: The italian and english cases",
      "author" : [ "Manuela Caiani", "Benedetta Carlotti", "Enrico Padoan." ],
      "venue" : "Javnost - The Public, 28(2):202–218.",
      "citeRegEx" : "Caiani et al\\.,? 2021",
      "shortCiteRegEx" : "Caiani et al\\.",
      "year" : 2021
    }, {
      "title" : "CONAN COunter NArratives through nichesourcing: a multilingual dataset of responses to fight online hate speech",
      "author" : [ "Yi-Ling Chung", "Elizaveta Kuzmenko", "Serra Sinem Tekiroglu", "Marco Guerini." ],
      "venue" : "Proceedings of the 57th Annual Meet-",
      "citeRegEx" : "Chung et al\\.,? 2019",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment analysis and social cognition engine (seance): An automatic tool for sentiment, social cognition, and social-order analysis",
      "author" : [ "Scott A Crossley", "Kristopher Kyle", "Danielle S McNamara." ],
      "venue" : "Behavior research methods, 49(3):803–821.",
      "citeRegEx" : "Crossley et al\\.,? 2017",
      "shortCiteRegEx" : "Crossley et al\\.",
      "year" : 2017
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael W. Macy", "Ingmar Weber." ],
      "venue" : "Proceedings of the Eleventh International Conference on Web and Social Media, ICWSM 2017, Montréal,",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "Hate lingo: A target-based linguistic analysis of hate speech in social media",
      "author" : [ "Mai ElSherief", "Vivek Kulkarni", "Dana Nguyen", "William Yang Wang", "Elizabeth M. Belding." ],
      "venue" : "Proceedings of the Twelfth International Conference on Web and Social Media,",
      "citeRegEx" : "ElSherief et al\\.,? 2018",
      "shortCiteRegEx" : "ElSherief et al\\.",
      "year" : 2018
    }, {
      "title" : "Human-in-theloop for data collection: a multi-target counter narrative dataset to fight online hate speech",
      "author" : [ "Margherita Fanton", "Helena Bonaldi", "Serra Sinem Tekiroğlu", "Marco Guerini." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for",
      "citeRegEx" : "Fanton et al\\.,? 2021",
      "shortCiteRegEx" : "Fanton et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring misogyny across the manosphere in reddit",
      "author" : [ "Tracie Farrell", "Miriam Fernandez", "Jakub Novotny", "Harith Alani." ],
      "venue" : "Proceedings of the 10th ACM Conference on Web Science, WebSci ’19, page 87–96, New York, NY, USA. Association for Com-",
      "citeRegEx" : "Farrell et al\\.,? 2019",
      "shortCiteRegEx" : "Farrell et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on automatic detection of hate speech in text",
      "author" : [ "Paula Fortuna", "Sérgio Nunes." ],
      "venue" : "ACM Comput. Surv., 51(4).",
      "citeRegEx" : "Fortuna and Nunes.,? 2018",
      "shortCiteRegEx" : "Fortuna and Nunes.",
      "year" : 2018
    }, {
      "title" : "Detecting online hate speech using context aware models",
      "author" : [ "Lei Gao", "Ruihong Huang." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 260–266, Varna, Bulgaria. INCOMA",
      "citeRegEx" : "Gao and Huang.,? 2017",
      "shortCiteRegEx" : "Gao and Huang.",
      "year" : 2017
    }, {
      "title" : "Countering hate on social media: Large scale classification of hate and counter speech",
      "author" : [ "Joshua Garland", "Keyan Ghazi-Zahedi", "Jean-Gabriel Young", "Laurent Hébert-Dufresne", "Mirta Galesic." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse",
      "citeRegEx" : "Garland et al\\.,? 2020",
      "shortCiteRegEx" : "Garland et al\\.",
      "year" : 2020
    }, {
      "title" : "A report on the 2020 sarcasm detection shared task",
      "author" : [ "Debanjan Ghosh", "Avijit Vajpayee", "Smaranda Muresan." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 1–11, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Ghosh et al\\.,? 2020",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning whom to trust with MACE",
      "author" : [ "Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hovy et al\\.,? 2013",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2013
    }, {
      "title" : "Hidden resilience and adaptive dynamics of the global online hate ecology",
      "author" : [ "Nicola F Johnson", "R Leahy", "N Johnson Restrepo", "Nicolas Velasquez", "Ming Zheng", "P Manrique", "P Devkota", "Stefan Wuchty." ],
      "venue" : "Nature, 573(7773):261–265.",
      "citeRegEx" : "Johnson et al\\.,? 2019",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "Computing krippendorff’s alpha-reliability",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "https://repository. upenn.edu/asc_papers/43/. Accessed: 2021-02-08.",
      "citeRegEx" : "Krippendorff.,? 2011",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2011
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Hate begets hate: A temporal study of hate speech",
      "author" : [ "Binny Mathew", "Anurag Illendula", "Punyajoy Saha", "Soumya Sarkar", "Pawan Goyal", "Animesh Mukherjee." ],
      "venue" : "Proceedings of the ACM on HumanComputer Interaction, 4(CSCW2):1–24.",
      "citeRegEx" : "Mathew et al\\.,? 2020",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2020
    }, {
      "title" : "Thou shalt not hate: Countering online hate speech",
      "author" : [ "Binny Mathew", "Punyajoy Saha", "Hardik Tharad", "Subham Rajgaria", "Prajwal Singhania", "Suman Kalyan Maity", "Pawan Goyal", "Animesh Mukherjee." ],
      "venue" : "Proceedings of the Thirteenth International Conference",
      "citeRegEx" : "Mathew et al\\.,? 2019",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2019
    }, {
      "title" : "Hatexplain: A benchmark dataset for explainable hate speech detection",
      "author" : [ "Binny Mathew", "Punyajoy Saha", "Seid Muhie Yimam", "Chris Biemann", "Pawan Goyal", "Animesh Mukherjee." ],
      "venue" : "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,",
      "citeRegEx" : "Mathew et al\\.,? 2021",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2021
    }, {
      "title" : "Note on the sampling error of the difference between correlated proportions or percentages",
      "author" : [ "Quinn McNemar." ],
      "venue" : "Psychometrika, 12(2):153–157.",
      "citeRegEx" : "McNemar.,? 1947",
      "shortCiteRegEx" : "McNemar.",
      "year" : 1947
    }, {
      "title" : "Abuse is contextual, what about nlp? the role of context in abusive language annotation and detection",
      "author" : [ "Stefano Menini", "Alessio Palmero Aprosio", "Sara Tonelli." ],
      "venue" : "CoRR, abs/2103.14916.",
      "citeRegEx" : "Menini et al\\.,? 2021",
      "shortCiteRegEx" : "Menini et al\\.",
      "year" : 2021
    }, {
      "title" : "Abusive language detection in online user content",
      "author" : [ "Chikashi Nobata", "Joel R. Tetreault", "Achint Thomas", "Yashar Mehdad", "Yi Chang." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 -",
      "citeRegEx" : "Nobata et al\\.,? 2016",
      "shortCiteRegEx" : "Nobata et al\\.",
      "year" : 2016
    }, {
      "title" : "The effect of extremist violence on hateful speech online",
      "author" : [ "Alexandra Olteanu", "Carlos Castillo", "Jeremy Boy", "Kush R. Varshney." ],
      "venue" : "Proceedings of the Twelfth International Conference on Web and Social Media, ICWSM 2018, Stanford, California,",
      "citeRegEx" : "Olteanu et al\\.,? 2018",
      "shortCiteRegEx" : "Olteanu et al\\.",
      "year" : 2018
    }, {
      "title" : "Toxicity detection: Does context really matter",
      "author" : [ "John Pavlopoulos", "Jeffrey Sorensen", "Lucas Dixon", "Nithum Thain", "Ion Androutsopoulos" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Pavlopoulos et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pavlopoulos et al\\.",
      "year" : 2020
    }, {
      "title" : "jiant 2.0: A software toolkit for research on general-purpose text understanding models. http://jiant.info",
      "author" : [ "Jason Phang", "Phil Yeres", "Jesse Swanson", "Haokun Liu", "Ian F. Tenney", "Phu Mon Htut", "Clara Vania", "Alex Wang", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Phang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2020
    }, {
      "title" : "DEBAGREEMENT: A comment-reply dataset for (dis)agreement detection in online debates",
      "author" : [ "John Pougué-Biyong", "Valentina Semenova", "Alexandre Matton", "Rachel Han", "Aerin Kim", "Renaud Lambiotte", "Doyne Farmer." ],
      "venue" : "Thirty-fifth Conference on Neu-",
      "citeRegEx" : "Pougué.Biyong et al\\.,? 2021",
      "shortCiteRegEx" : "Pougué.Biyong et al\\.",
      "year" : 2021
    }, {
      "title" : "A study of cyber hate on twitter with implications for social media governance strategies",
      "author" : [ "Rob Procter", "Helena Webb", "Pete Burnap", "William Housley", "Adam Edwards", "Matthew L. Williams", "Marina Jirotka." ],
      "venue" : "Proceedings of the 2019 Truth and Trust",
      "citeRegEx" : "Procter et al\\.,? 2019",
      "shortCiteRegEx" : "Procter et al\\.",
      "year" : 2019
    }, {
      "title" : "A benchmark dataset for learning to intervene in online hate speech",
      "author" : [ "Jing Qian", "Anna Bethke", "Yinyin Liu", "Elizabeth Belding", "William Yang Wang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Qian et al\\.,? 2019",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-sensitive twitter sentiment classification using neural network",
      "author" : [ "Yafeng Ren", "Yue Zhang", "Meishan Zhang", "Donghong Ji." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, page 215–221. AAAI Press.",
      "citeRegEx" : "Ren et al\\.,? 2016",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "A quality type-aware annotated corpus and lexicon for harassment research",
      "author" : [ "Mohammadreza Rezvan", "Saeedeh Shekarpour", "Lakshika Balasuriya", "Krishnaprasad Thirunarayan", "Valerie L. Shalin", "Amit Sheth." ],
      "venue" : "Proceedings of the 10th ACM Conference on Web",
      "citeRegEx" : "Rezvan et al\\.,? 2018",
      "shortCiteRegEx" : "Rezvan et al\\.",
      "year" : 2018
    }, {
      "title" : "Counterspeech 2000: A new look at the old remedy for bad speech",
      "author" : [ "Robert D Richards", "Clay Calvert." ],
      "venue" : "BYU L. Rev., page 553.",
      "citeRegEx" : "Richards and Calvert.,? 2000",
      "shortCiteRegEx" : "Richards and Calvert.",
      "year" : 2000
    }, {
      "title" : "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "author" : [ "Sara Rosenthal", "Noura Farra", "Preslav Nakov." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502– 518, Vancouver, Canada. Association for Computa-",
      "citeRegEx" : "Rosenthal et al\\.,? 2017",
      "shortCiteRegEx" : "Rosenthal et al\\.",
      "year" : 2017
    }, {
      "title" : "Corpus annotation through crowdsourcing: Towards best practice guidelines",
      "author" : [ "Marta Sabou", "Kalina Bontcheva", "Leon Derczynski", "Arno Scharl." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),",
      "citeRegEx" : "Sabou et al\\.,? 2014",
      "shortCiteRegEx" : "Sabou et al\\.",
      "year" : 2014
    }, {
      "title" : "A survey on hate speech detection using natural language processing",
      "author" : [ "Anna Schmidt", "Michael Wiegand." ],
      "venue" : "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10, Valencia, Spain. Association",
      "citeRegEx" : "Schmidt and Wiegand.,? 2017",
      "shortCiteRegEx" : "Schmidt and Wiegand.",
      "year" : 2017
    }, {
      "title" : "Will it blend? blending weak and strong labeled data in a neural network for argumentation mining",
      "author" : [ "Eyal Shnarch", "Carlos Alzate", "Lena Dankin", "Martin Gleize", "Yufang Hou", "Leshem Choshen", "Ranit Aharonov", "Noam Slonim." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Shnarch et al\\.,? 2018",
      "shortCiteRegEx" : "Shnarch et al\\.",
      "year" : 2018
    }, {
      "title" : "Introducing CAD: the contextual abuse dataset",
      "author" : [ "Bertie Vidgen", "Dong Nguyen", "Helen Margetts", "Patricia Rossini", "Rebekah Tromble." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Vidgen et al\\.,? 2021",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2021
    }, {
      "title" : "Free speech and the development of liberal virtues: An examination of the controversies involving flag-burning and hate speech",
      "author" : [ "Kenneth D Ward." ],
      "venue" : "University of Miami Law Review, 52(3):733–792.",
      "citeRegEx" : "Ward.,? 1997",
      "shortCiteRegEx" : "Ward.",
      "year" : 1997
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88–93, San Diego, California. Association for Computational Linguis-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Hate speech detection: A solved problem? the challenging case of long tail on twitter",
      "author" : [ "Ziqi Zhang", "Lei Luo." ],
      "venue" : "Semantic Web, 10(5):925–945.",
      "citeRegEx" : "Zhang and Luo.,? 2019",
      "shortCiteRegEx" : "Zhang and Luo.",
      "year" : 2019
    }, {
      "title" : "Racism is a virus: Anti-asian hate and counterhate in social media during the COVID-19 crisis",
      "author" : [ "Caleb Ziems", "Bing He", "Sandeep Soni", "Srijan Kumar." ],
      "venue" : "CoRR, abs/2005.12423.",
      "citeRegEx" : "Ziems et al\\.,? 2020",
      "shortCiteRegEx" : "Ziems et al\\.",
      "year" : 2020
    }, {
      "title" : "Discourseaware rumour stance classification in social media using sequential classifiers",
      "author" : [ "Arkaitz Zubiaga", "Elena Kochkina", "Maria Liakata", "Rob Procter", "Michal Lukasik", "Kalina Bontcheva", "Trevor Cohn", "Isabelle Augenstein." ],
      "venue" : "Information Processing",
      "citeRegEx" : "Zubiaga et al\\.,? 2018",
      "shortCiteRegEx" : "Zubiaga et al\\.",
      "year" : 2018
    }, {
      "title" : "Hyperparameters used to fine-tune RoBERTa individually for each training setting. Hp-1, Hp-2, Hp-3 and Hp-4 refer to the number of epochs, training batch size, learning rate and dropout used in the training procedure",
      "author" : [ "Phang" ],
      "venue" : null,
      "citeRegEx" : "Phang,? \\Q2020\\E",
      "shortCiteRegEx" : "Phang",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Meanwhile, it is considered a particularly conducive arena for hate speech (Caiani et al., 2021).",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "At the individual level, the victims targeted by hate speech are frightened of online threats that may materialize in the real world (Olteanu et al., 2018).",
      "startOffset" : 133,
      "endOffset" : 155
    }, {
      "referenceID" : 23,
      "context" : "At the societal level, it has been reported that there is an upsurge in offline hate crimes targeting minorities (Olteanu et al., 2018; Farrell et al., 2019).",
      "startOffset" : 113,
      "endOffset" : 157
    }, {
      "referenceID" : 8,
      "context" : "At the societal level, it has been reported that there is an upsurge in offline hate crimes targeting minorities (Olteanu et al., 2018; Farrell et al., 2019).",
      "startOffset" : 113,
      "endOffset" : 157
    }, {
      "referenceID" : 18,
      "context" : "While these interventions could de-escalate the impact of hate speech to some extent, they may violate online free speech (Mathew et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "Additionally, attacks at the micro-level may be ineffective as hate networks often have rapid rewiring and selfrepair mechanisms (Johnson et al., 2019).",
      "startOffset" : 129,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "Counter speech refers to the “direct response that counters hate speech” (Mathew et al., 2019), which is con-",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : "It has been supported by theoretical and empirical studies to be more effective in the long term (Richards and Calvert, 2000; Mathew et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 146
    }, {
      "referenceID" : 17,
      "context" : "It has been supported by theoretical and empirical studies to be more effective in the long term (Richards and Calvert, 2000; Mathew et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 146
    }, {
      "referenceID" : 18,
      "context" : "This is a common strategy to express counter speech (Mathew et al., 2019).",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "Hate speech in user-generated content has been an active research area recently (Fortuna and Nunes, 2018).",
      "startOffset" : 80,
      "endOffset" : 105
    }, {
      "referenceID" : 38,
      "context" : "hate speech detection from diverse sources like Twitter (Waseem and Hovy, 2016; Davidson et al., 2017), Yahoo! (Nobata et al.",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "hate speech detection from diverse sources like Twitter (Waseem and Hovy, 2016; Davidson et al., 2017), Yahoo! (Nobata et al.",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : ", 2017), Yahoo! (Nobata et al., 2016), Fox News (Gao and Huang, 2017), Gab (Mathew et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : ", 2016), Fox News (Gao and Huang, 2017), Gab (Mathew et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : ", 2016), Fox News (Gao and Huang, 2017), Gab (Mathew et al., 2021) and Reddit (Qian et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "curating datasets for counter speech generation using crowdsourcing (Qian et al., 2019) or with the help of trained operators (Chung et al.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : ", 2019) or with the help of trained operators (Chung et al., 2019; Fanton et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : ", 2019) or with the help of trained operators (Chung et al., 2019; Fanton et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Context does affect human judgments and has been taken into account for Hate detection (Gao and Huang, 2017; Vidgen et al., 2021; Pavlopoulos et al., 2020; Menini et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 176
    }, {
      "referenceID" : 36,
      "context" : "Context does affect human judgments and has been taken into account for Hate detection (Gao and Huang, 2017; Vidgen et al., 2021; Pavlopoulos et al., 2020; Menini et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : "Context does affect human judgments and has been taken into account for Hate detection (Gao and Huang, 2017; Vidgen et al., 2021; Pavlopoulos et al., 2020; Menini et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 176
    }, {
      "referenceID" : 21,
      "context" : "Context does affect human judgments and has been taken into account for Hate detection (Gao and Huang, 2017; Vidgen et al., 2021; Pavlopoulos et al., 2020; Menini et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : "Utilizing conversational context has also been explored in text classification tasks such as sentiment analysis (Ren et al., 2016), stance (Zubiaga et al.",
      "startOffset" : 112,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "It is an ideal platform for data collection due to the large size of user populations and many diverse topics (Baumgartner et al., 2020).",
      "startOffset" : 110,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "We retrieve 6,846 (Parent, Target) pairs with PushShift (Baumgartner et al., 2020) from 416 submissions in order to keep the annotation costs reasonable while creating a (relatively) large corpus.",
      "startOffset" : 56,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : ", 2013, Multi-Annotator Competence Estimation) and Krippendorff’s α (Krippendorff, 2011).",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "We analyze the components of linguistic features using the Sentiment Analysis and Cognition Engine (SEANCE) lexicon, a popular tool for psychological linguistic analysis (Crossley et al., 2017).",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : "We indicate statistical significance (McNemar’s test (McNemar, 1947)) with respect to the model trained with the Target only using neither Silver nor pretraining on related tasks as follows: † indicates p < 0.",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : "pora annotating: (1) hateful comments: hateful or not hateful (Qian et al., 2019), and hate speech, offensive, or neither (Davidson et al.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : ", 2019), and hate speech, offensive, or neither (Davidson et al., 2017); (2) sentiment: negative, neutral, or positive (Rosenthal et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 32,
      "context" : ", 2017); (2) sentiment: negative, neutral, or positive (Rosenthal et al., 2017); (3) sarcasm: sarcasm or not sarcasm",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "(Ghosh et al., 2020); and (4) stance: agree, neutral, or attack (Pougué-Biyong et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 26,
      "context" : ", 2020); and (4) stance: agree, neutral, or attack (Pougué-Biyong et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 79
    }, {
      "referenceID" : 39,
      "context" : "And vice versa, comments containing swear words may not be hateful (Zhang and Luo, 2019).",
      "startOffset" : 67,
      "endOffset" : 88
    } ],
    "year" : 0,
    "abstractText" : "Hate speech is plaguing the cyberspace along with user-generated content. Adding counter speech has become an effective way to combat hate speech online. Existing datasets and models target either (a) hate speech or (b) hate and counter speech but disregard the context. This paper investigates the role of context in the annotation and detection of online hate and counter speech, where context is defined as the preceding comment in a conversation thread. We created a context-aware dataset for a 3-way classification task on Reddit comments: hate speech, counter speech, or neutral. Our analyses indicate that context is critical to identify hate and counter speech: human judgments change for most comments depending on whether we show annotators the context. A linguistic analysis draws insights into the language people use to express hate and counter speech. Experimental results show that neural networks obtain significantly better results if context is taken into account. We also present qualitative error analyses shedding light into (a) when and why context is beneficial and (b) the remaining errors made by our best model when context is taken into account.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_223_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Transferability of Prompt Tuning for Natural Language Processing",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models (PLMs), such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) have achieved great performance on various natural language processing (NLP) tasks (Han et al., 2021). Recently, after the success of GPT-3 (Brown et al., 2020), people have found that extremely large PLMs can achieve remarkable improvements, and various large PLMs are continually developed (Raffel et al., 2020; Zhang et al., 2021; Zeng et al., 2021; Wei et al., 2021; Sun et al., 2021), which\ncontain up to hundreds of billions of parameters. Considering the extremely large scale of these state-of-the-art PLMs, conventional full-parameter fine-tuning methods become extremely expensive. Hence, various parameter-efficient tuning methods (Houlsby et al., 2019; Ben Zaken et al., 2021; Lester et al., 2021; Li and Liang, 2021; Liu et al., 2021) are explored, among which prompt tuning (PT) has attracted broad research attention. PT prepends some soft prompts, which are essentially learnable virtual tokens, into the input sequences and only train them while keeping all the PLM’s parameters fixed. The training objective is to generate desired outputs in the same way as the pretraining tasks. PT can match the downstream task performance of fine-tuning with only thousands of tunable parameters (Lester et al., 2021) when the PLM has billions of parameters.\nAlthough PT is an effective approach to utilize extremely large PLMs, it requires much more training time than fine-tuning to reach the convergence as shown in Figure 2; hence, it is worthwhile to explore how to improve the efficiency of PT. In this work, we attempt to improve PT via prompt transfer across different tasks and models. Knowledge\ntransfer across tasks (Vu et al., 2020) and models (Qin et al., 2021) have been widely used to improve the efficiency and effectiveness of NLP systems. Intuitively, soft prompts are the only tuned parameters in PT and thus shall concentrate the knowledge required to solve tasks conditioned on PLMs. Hence only transferring the trained prompts is promising to accelerate PT.\nAs shown in Figure 1, we empirically analyze the transferability of prompts across different tasks (cross-task transfer setting) and PLMs (crossmodel transfer setting) in this paper. The empirical analysis is conducted on 17 NLP tasks of 6 types and two representative PLM series: RoBERTa (Liu et al., 2019b) and T5 (Raffel et al., 2020). In crosstask transfer, the prompt transfer can be done by directly reusing the trained prompts of the source task on the target task. However, in cross-model transfer, directly reusing prompts is intractable since the semantic spaces of different PLMs are inconsistent; hence, we develop various prompt projectors to project the soft prompts trained on the source PLM to the semantic space of the target PLM. We conduct two lines of experiments: (1) We investigate the zero-shot transfer performance and find that the transferability of prompts is influenced by task types. In cross-task transfer, the soft prompts can directly transfer to same-type tasks and achieve non-trivial performance, but poorly transfer to different-type tasks requiring different language skills. In cross-model transfer, we can successfully train a prompt projector with PT on a task, but the trained projector also only well generalizes to the same-type tasks of the projector-training task. (2) To accelerate PT, we propose to transfer prompts with initialization. In cross-task transfer, we start PT with the trained soft prompts of similar tasks as initialization. While in cross-model transfer, the initialization is the projected prompts of the same task trained on the source PLM. The two methods\nare dubbed as TPTTASK and TPTMODEL, respectively. Experiments show that they can both significantly accelerate PT and also achieve a certain performance improvement.\nFurthermore, we explore why can the prompts transfer and what decides their transferability. To this end, we design various prompt similarity metrics from different perspectives and examine how well they can serve as transferability indicators, i.e., how well they correlate with prompt transfer performance. Experiments find that the embedding distances of prompts do not well indicate prompt transferability but the overlapping rate of the prompts’ activated neurons in the feed-forward layers can better reflect prompt transferability. This suggests the prompts are essentially stimulating PLM’s inner ability distributing among neurons to do specific NLP tasks, and future prompt transfer works should focus more on how the PLMs respond to different prompts’ stimulation rather than the prompts’ embedding properties.\nTo summarize, our contributions are three-fold: (1) We thoroughly analyze the transferability of prompts across different tasks and models, and show that improving PT with prompt transfer is possible and promising. (2) We propose to transfer prompts with initialization, which enhances both PT’s efficiency and effectiveness. (3) We explore the effectiveness of various prompt similarity metrics serving as transferability indicators and demonstrate how the prompts stimulate PLMs to decide the transferability, which may facilitate further transferrable PT research."
    }, {
      "heading" : "2 Related Work",
      "text" : "Prompt Tuning GPT-3 (Brown et al., 2020) demonstrates remarkable few-shot performance by prepending textual prompts before the inputs and thus help the PLM to generate desired outputs of NLP tasks directly. Motivated by this, many works have tried to improve various NLP tasks by creating manually-crafted (Schick and Schütze, 2021a,b; Mishra et al., 2021) or automaticallysearched (Jiang et al., 2020; Shin et al., 2020; Gao et al., 2021) hard prompts, which are discrete tokens but not necessarily human-readable. Furthermore, soft prompts (Li and Liang, 2021; Hambardzumyan et al., 2021; Zhong et al., 2021; Liu et al., 2021) are proposed, which are tuneable embeddings rather than tokens in the vocabularies and can be directly trained with task-specific supervi-\nsion. Lester et al. (2021) demonstrate that prompt tuning (PT) method can match the performance of full-parameter fine-tuning when the PLM has billions of parameters. This suggests that PT is promising to utilize extremely large PLMs. However, the much more training time needed to reach the convergence makes PT inefficient. In this work, we show that prompt transfer can remedy, improve the effectiveness to some extent with knowledge transfer, and empirically analyze the transferability of prompts across tasks and PLMs.\nKnowledge Transfer Cross-task knowledge transfer (Ruder, 2017) has been a long-standing way to improve the effectiveness and efficiency of NLP systems. In the PLM era, some works propose to tune the PLMs on intermediate tasks (Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020; Wang et al., 2019a; Vu et al., 2020; Poth et al., 2021) before fine-tuning on specific target tasks to achieve certain benefits. Vu et al. (2020) empirically analyze the transferability between tasks in this setting.\nThese explorations are all for fine-tuning. Considering the potential of PT, we believe the transferability and knowledge transfer methods for PT are worth exploring. As a prior attempt, Lester et al. (2021) demonstrate that PT’s cross-domain transferability is stronger than fine-tuning. Similar to our work, concurrent work (Vu et al., 2021) explores the cross-task transferability of PT and improves performance with transfer initialization. Differently, we attempt to improve the efficiency of PT and further analyze what decides the prompt transferability by exploring various transferability indicators. Additionally, we also attempt cross-model transfer, which is inspired by previous cross-model knowledge transfer works such as Net2Net (Chen et al., 2016), knowledge distillation (Hinton et al., 2015) and knowledge inheritance (Qin et al., 2021)."
    }, {
      "heading" : "3 Preliminary",
      "text" : "Here we introduce the basic knowledge about PT (§ 3.1) as well as the downstream tasks (§ 3.2) and models (§ 3.3) investigated in experiments."
    }, {
      "heading" : "3.1 Prompt Tuning",
      "text" : "In this work, we study the PT method that is capable of tuning large PLMs (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021), i.e., we only explore the PT method freezing PLM parameters. PT prepends some virtual tokens, i.e., the soft prompts,\ninto the inputs of the PLM to provide knowledge about downstream tasks. The soft prompts are essentially tunable embedding vectors, which are trained with the objective enforcing the PLM to generate desired outputs of the downstream task in the same way of the pre-training objective.\nFormally, given an input sequence with n tokens X = {x1, x2, . . . , xn}, we first prepend l randomly initialized soft prompts P = {p1,p2, . . . ,pl} before them, where pi ∈ Rd is an embedding vector, and d is the input dimension of the PLM. The training objective is to maximize the likelihood of decoding the desired output y:\nL = p(y|P, x1, . . . , xn), (1)\nwhere only P is learnable. For the language understanding tasks, y is the label token corresponding to the label of X . For the conditional generation tasks, y is a sequence. Especially, for the models pre-trained with the masked language modeling objective like RoBERTa, we additionally prepend a special [MASK] token before the prompts and train the prompts to let the PLM fill y into it."
    }, {
      "heading" : "3.2 Investigated NLP Tasks",
      "text" : "To comprehensively study the prompt transferability across various NLP tasks, we involve 17 diverse tasks, which can be divided into 6 types: (1) Sentiment Analysis (SA), including IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), laptop (Pontiki et al., 2014), restaurant (Pontiki et al., 2014), Movie Rationales (Movie) (Zaidan et al., 2008) and TweetEval (Tweet) (Barbieri et al., 2020); (2) Natural Language Inference (NLI), including MNLI (Williams et al., 2018), QNLI (Wang et al., 2019b) and SNLI (Bowman et al., 2015); (3) Ethical Judgement (EJ), including deontology (Hendrycks et al., 2021) and justice (Hendrycks et al., 2021); (4) Paraphrase Identification (PI), including QQP (Sharma et al., 2019) and MRPC (Dolan and Brockett, 2005); (5) Question Answering (QA), including SQuAD (Rajpurkar et al., 2016) and NQ-Open (Lee et al., 2019); (6) Summarization (SUM), including Multi-News (Fabbri et al., 2019) and SAMSum (Gliwa et al., 2019). Details for these tasks, evaluation metrics, label tokens, implementations are in appendix A."
    }, {
      "heading" : "3.3 Investigated Models",
      "text" : "We investigate prompt transferability for two series of PLMs: RoBERTa (Liu et al., 2019b) and T5 (Raf-\nfel et al., 2020), which represent two mainstream pre-training types: masked language modeling and sequence-to-sequence pre-training. Considering RoBERTa can only predict a single token (or a fixed length of tokens), for the conditional generation tasks (QA and SUM) that output multiple tokens, we only investigate T5. We mainly report results for the two largest versions of PLMs, i.e., RoBERTaLARGE and T5XXL. The more detailed results for the other sizes are attached in appendix."
    }, {
      "heading" : "4 Cross-Task Transfer",
      "text" : "We empirically study the cross-task transferability of soft prompts (§ 4.1) and try to improve the effectiveness and efficiency of PT with transfer (§ 4.2)."
    }, {
      "heading" : "4.1 Zero-shot Transfer Performance",
      "text" : "To study the cross-task transferability, we first examine PT’s zero-shot transfer performance, i.e., we conduct PT on a source task, then directly reuse the trained prompts on other target tasks and evaluate their performance. The results are shown in\nFigure 31, from which we can observe that: (1) For the tasks within the same type, transferring soft prompts between them can generally perform well and may even outperform vanilla PT on the target\n1More results on other PLMs are left in appendix B.1.\ntask, especially when the source task has more data (the case of transferring from IMDB to Movie in Figure 3 (a) and transferring from restaurant to laptop in Figure 3 (b)), which demonstrates that it is promising to improve PT’s effectiveness and efficiency with knowledge transfer from similar tasks. (2) For the tasks of different types, the transferability of soft prompts among them is generally poor, and transferring soft prompts often achieve similar performance to randomly initialized prompts. (3) However, some tasks can transfer to different-type tasks to some extent, such as the QA and SUM tasks to SA tasks in Figure 3 (b). To understand this, it is worthwhile to explore what controls the transferability between prompts, and we do some preliminary study in § 6."
    }, {
      "heading" : "4.2 Transfer with Initialization",
      "text" : "To improve the effectiveness and efficiency of PT with cross-task transfer, we explore a crosstask transferable prompt tuning (TPTTASK) method, which initializes soft prompts with well-trained prompts of the most similar task and then starts PT.\nFor a target task, we start TPTTASK with trained prompts of the source task achieving the best zeroshot transfer performance in Figure 3. From the results of the performance and training time com-\nparisons2 in Table 1, we can see TPTTASK can mostly achieve better or comparable performance to vanilla PT starting from random initialization, and TPTTASK generally takes less training time."
    }, {
      "heading" : "5 Cross-Model Transfer",
      "text" : "We further study the cross-model transferability of soft prompts. We investigate the feasibility of crossmodel transfer on transferring from a source PLM (RoBERTaLARGE) to a larger and heterogeneous target PLM (T5XXL), which shall be the most difficult setting. Appendix C shows the experimental results of other settings. Directly reusing trained soft prompts between different PLMs is infeasible since their embedding spaces are different. Hence, we investigate how to do cross-model prompt projection (§ 5.1) and see the transfer performance (§ 5.2). Furthermore, we explore to improve PT with cross-model transfer initialization (§ 5.3)."
    }, {
      "heading" : "5.1 Cross-Model Prompt Projection",
      "text" : "To project the trained soft prompts of a PLM to the semantic space of a different PLM, we train projectors with various objectives and examine their effectiveness. A good way to train the cross-model projectors may need some task-specific supervisions, but the trained projector shall generalize to different tasks so that the efficiency for learning the new tasks on the target model could be improved.\nFormally, given the prompt of the source PLM P s = {ps1, . . . ,psl }, we concatenate the l virtual tokens into a unified vector Ps ∈ Rlds . The projector Proj(·) is to project it to P̃s ∈ Rldt in the semantic space of the target PLM, where ds and dt\n2Training time comparisons are left in appendix B.3.\nare the input embedding dimensions of the source and target PLM, respectively. We parameterize the projector with a two-layer perceptron as follows:\nP̃s=Proj(Ps)=W2(σ(P sW1+b1))+b2, (2)\nwhere W1 ∈ Rdh×lds ,W2 ∈ Rldt×dh are trainable matrices, b1 ∈ Rdh ,b2 ∈ Rldt are biases, σ is a non-linear activation function. We investigate two learning objectives to train the projector3:\nDistance Minimizing We firstly try to learn cross-model projections by minimizing the distance between the projected prompt and the parallel prompt Pt originally trained on the target PLM with the same task, i.e., the training objective is to minimize their L2-distance ‖Proj(Ps)−Pt‖2.\nTask Tuning We then try to train the cross-model projector with task-specific supervision signals on the target PLM. Specifically, we directly tune the projected prompts on some tasks and back propagate the supervision signals to train the projector.\nThese methods rely on some tasks (parallel trained soft prompts or training data) to train the projector. In the experiments, we select laptop and MNLI for the projector learning."
    }, {
      "heading" : "5.2 Zero-shot Transfer Performance",
      "text" : "The zero-shot transfer performance of various projector-learning methods are shown in Table 24 (a). We can observe that: (1) Distance Minimizing works well to transfer the prompts of the projectortraining task, but falls back to random performance on the other unseen tasks, which is not practically\n3More projector-training details are left in appendix C.1. 4More results on other PLMs are left in appendix C.2.\nusable. This is consistent with our findings in § 6 that the embedding distances do not strongly correlate to prompt transferability. (2) Task Tuning performs better and successfully generalizes to same-type unseen tasks of the projector-training tasks (e.g. NLI tasks for the projectors trained with MNLI), which proves the feasibility of practical cross-model prompt transfer. (3) The projectors trained with Task Tuning still cannot work for different-type tasks, which may be limited by the cross-task prompt transferability investigated in § 4.1. This urges further attention on developing universal cross-model projections."
    }, {
      "heading" : "5.3 Transfer with Initialization",
      "text" : "Similar to § 4.2, we further study whether the projected soft prompts can initialize PT on the target PLM and accelerate training as well as improve performance. We propose cross-model transferable prompt tuning, TPTMODEL, which adopts the Task Tuning projectors to project the soft prompts trained on the source PLM into the target PLM and initialize PT with the projected prompts.\nThe performance and speedup are shown in Table 2 (b). We can see that, for the tasks within the same type of the projector-training task, compared to vanilla PT, TPTMODEL can mostly achieve comparable or better performance with much less training time, which demonstrates that practical cross-model prompt transfer is promising for improving the efficiency and effectiveness of PT."
    }, {
      "heading" : "6 Exploring Transferability Indicator",
      "text" : "Based on the positive results in cross-task and crossmodel transfer, we explore why the soft prompts can transfer across tasks and what decides the transferability between them, which may shed light on the mechanisms behind PT and help to design transferable PT methods. We explore various prompt similarity metrics and examine how well do they align with the zero-shot transfer performance. If a similarity metric can well indicate transferability, it suggests the factors considered in designing this metric decide the prompt transferability. Moreover, the prompt similarity metrics can qualify task similarities using the trained soft prompts as task embeddings and may help in developing cross-task transfer methods. As a straightforward example, if we build a prompt warehouse containing prompts of diverse tasks, we can retrieve prompts of similar tasks for a new task with a certain similarity metric and better improve PT with TPTTASK."
    }, {
      "heading" : "6.1 Prompt Similarity Metric",
      "text" : "We explore the following two kinds of metrics:\nEmbedding Similarity We firstly regard the trained soft prompts as only embeddings in the vector space and calculate their Euclidean similarity and cosine similarity.\nGiven two groups of trained prompts containing l virtual tokens: P t1 = {pt11 , . . . ,p t1 l } and P t2 = {pt21 , . . . ,p t2 l }, which correspond to tasks t1 and t2. Firstly, we concatenate the l virtual tokens for each group and get two concatenation em-\nbeddings Pt1 ,Pt2 ∈ Rld, then we compute Euclidean similarity and cosine similarity of them:\nEconcat(P t1 , P t2) =\n1\n1 + ‖Pt1 −Pt2‖ ,\nCconcat(P t1 , P t2) = Pt1 ·Pt2 ‖Pt1‖‖Pt2‖ .\n(3)\nWe further explore a simple way to make the metrics invariant to token positions. We compute Euclidean distances and cosine similarities for every virtual token pairs in the two groups and use the averaged results in the final similarity metrics:\nEaverage(P t1 , P t2) =\n1\n1 + 1 l2 l∑ i=1 l∑ j=1 ‖pt1i − p t2 j ‖ ,\nCaverage(P t1 , P t2) =\n1 l2 l∑ i=1 l∑ j=1 pt1i · p t2 j ‖pt1i ‖‖p t2 j ‖ .\n(4)\nModel Stimulation Similarity In the second way, we depict their similarities based on how they stimulate the PLMs, i.e., we examine the similarities between the responses of PLMs to the two soft prompts. Motivated by Geva et al. (2021) and Dai et al. (2021), which both find that the activation of the neurons in the feed-forward layers of Transformers (Vaswani et al., 2017) corresponds to specific model behaviors, we propose to use the overlapping rate of activated neurons as a similarity metric of prompts. Specifically, the feedforward network FFN(·) in a Transformer layer is:\nFFN(x) = max(xW>1 + b1,0)W2 + b2, (5)\nwhere x ∈ Rd is the input embedding, W1,W2 ∈ Rdm×d are trainable matrices, and b1,b2 are bias vectors. The max(xW>1 + b1,0) can be regarded as the non-negative activation values for dm hidden neurons (Geva et al., 2021). We then change all the positive elements of max(xW>1 + b1,0) to 1 and get the one-hot activation state vector s.\nWe feed an input sequence {P,<s>} into the PLMs, where <s> is the special token indicating the start of a sentence. For RoBERTa, a [MASK] is additional prepended. This sequence is in the format of PT inputs but without specific input sentences. We use the activation states of the positions used to decode outputs, which shall be more task-specific. Specifically, for T5, we use the decoder module’s activation states at the first position. For RoBERTa, we use the activation states of [MASK]. Finally, we concatenate the activation\nstates of PLM’s L layers to get the overall activation states:\nAS(P ) = [s1; s2; ...; sL]. (6)\nWe can only retrieve the activation states of a part of layers in the similarity computation. In experiments, we find that the higher layers tend to be more task-specific, which is consistent with the probing results (Liu et al., 2019a). Hence we use the activation states of the top 3 layers5 in experiments below. We calculate the overlapping rate of activated neurons ON(P t1 , P t2) between the trained soft prompts of task t1 and t2 with the cosine similarity:\nON(P t1 , P t2) = AS(P t1) ·AS(P t2) ‖AS(P t1)‖‖AS(P t2)‖ . (7)"
    }, {
      "heading" : "6.2 Experimental Results",
      "text" : "To evaluate the effectiveness of the above similarity metrics of soft prompts, we (i) test whether the similarity metrics can distinguish the trained prompts of the same tasks and different tasks, and (ii) examine whether these metrics align with the zero-shot transfer performance.\n5More results about the different layers’s performance are left in appendix D.4.\nRegarding (i), we compare the similarities of the investigated metrics for two trained prompts within the same task (trained with different random seeds) and between different tasks in Table 3. From the results, we can observe that all the metrics work well to distinguish the prompts of the same task and different tasks. This suggests that the trained soft prompts of different tasks form distinguishable clusters in the embedding space and also stimulate different abilities within the PLM.\nMoreover, to evaluate (ii), how well the similarity metrics align with the cross-task transfer performance, we quantify the correlations between the similarities and zero-shot transfer performance in Figure 3. Specifically, for each target task’s prompt, we rank various source tasks’ prompts with similarity scores and zero-shot transfer performance and then compute the Spearman’s rank correlation (Spearman, 1987) between the two ranks generated by these two ways. The overall results are shown in Table 46. We can see that: (1) The overlapping rate of activated neurons (ON) metric works better than all the embedding similarities, which suggests that model stimulation is more important for prompt transferability than embedding distances. (2) ON works much worse on T5XXL (11B parameters) than on RoBERTaLARGE (330M parameters). We guess this is because larger PLMs have higher redundancy (Aghajanyan et al., 2021), which means prompts can activate different redundant neurons to do similar jobs and thus influence the sensitivity of ON metric. This is supported by the experiments showing that the Spearman’s correlation scores of ON drop with the increase of PLM scales (Figure 4). We encourage future work to explore how to overcome the PLM redundancy for better transferrable PT. As a preliminary\n6The detailed results by task types are left in appendix D.2.\ntrial, we find that by taking the intersection of activation states of 3 prompts trained with different random seeds, ON’s correlation score on T5XXL raises from 36.9% to 46.3%.\nWe further explore whether the prompt similarity metrics also work in the cross-model transfer setting by testing whether they work between the projected prompts and original prompts of the same task. In Table 5, we show the similarities of prompts projected with Task Tuning projectors by the two best metrics Caverage and ON. We can see: (1) ON metric shows that the projected prompts are highly similar to the original prompts within the same type of projector-training tasks but are not so similar to different-type tasks, which is quite consistent with the cross-model zero-shot transfer performance in Table 2. (2) However, Caverage cannot reflect this phenomena, which shows that the perspective of model stimulation is more promising for understanding transferability again."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We empirically investigate the transferability of prompts in this paper. In the cross-task setting, we find that soft prompts can transfer to similar tasks without training. In the cross-model setting, we successfully project prompts into the space of other PLMs. Further, we utilize trained prompts of other tasks or other PLMs as initialization to significantly accelerate training and improve effectiveness. Moreover, we explore various prompt transferability indicators and show that how the prompts stimulate PLMs are important to transferability. We hope the empirical analyses and the model stimulation idea can facilitate further research on transferable and efficient PT."
    }, {
      "heading" : "A Basic Setup for Various Tasks",
      "text" : ""
    }, {
      "heading" : "A.1 Dataset and Task",
      "text" : "Sentiment Analysis (SA) Given a sentence, a PLM will identify the opinions in this sentence. We choose IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), SemEval/laptop (Pontiki et al., 2014), SemEval/restaurant (Pontiki et al., 2014), Movie Rationales (Movie) (Zaidan et al., 2008), and TweetEval (Tweet) (Barbieri et al., 2020) to analyze.\nNatural Language Inference (NLI) Given a premise and hypothesis pair, a PLM determines whether the hypothesis is entailed, contradict, or undetermined by the premise. We choose MNLI (Williams et al., 2018), QNLI (Wang et al., 2019b), and SNLI (Bowman et al., 2015) to analyze.\nEthical Judgement (EJ) Given a sentence, a PLM judges whether it is ethically acceptable. We choose Ethics/deontology (Hendrycks et al., 2021) and Ethics/justice (Hendrycks et al., 2021) to analyze.\nParaphrase Identification (PI) Given a pair of sentences, a PLM judges whether they are semantically identical. We choose QQP (Sharma et al., 2019) and MRPC (Dolan and Brockett, 2005) to analyze.\nQuestion Answering (QA) Given a question, a PLM answer the question. We choose SQuAD (Rajpurkar et al., 2016) and NQ-Open (Lee et al., 2019) to analyze. For SQuAD, a PLM finds the answer from the content. As for NQ-Open, a PLM directly generates the answer without the content.\nSummarization (SUM) Given an article, a PLM summarizes it. We choose Multi-News (Fabbri et al., 2019), and SAMSum (Gliwa et al., 2019) to analyze."
    }, {
      "heading" : "A.2 Evaluation Metrics",
      "text" : "For SA, NLI, EJ, and PI tasks, we choose accuracy (Acc.) as their evaluation metric in the experiments. For QA and SUM tasks, we utilize F1 and ROUGEL (Lin, 2004), respectively."
    }, {
      "heading" : "A.3 Prompt Tuning Setting",
      "text" : "In the experiments, for all the investigated tasks, we use AdamW (Loshchilov and Hutter, 2019) as the optimizer and set the learning rate as 0.001. We set the length of soft prompts l as 100. All the soft\nprompts are randomly initialized and optimized with Equation 1. In the inference stage, RoBERTa predicts the label tokens at the [MASK] position and T5 directly uses its decoder to do generation."
    }, {
      "heading" : "A.4 Label Tokens",
      "text" : "The used label tokens for the classification tasks (SA, NLI, EJ, PI) are shown in Table 6. For generation tasks (QA, SUM), the desired output is just the annotated answers."
    }, {
      "heading" : "B Cross-Task Transfer",
      "text" : "B.1 More Zero-shot transfer performance In § 4.1, we report the zero-shot transfer performance (relative performance) on RoBERTaLARGE and T5XXL. Here, we investigate the zero-shot transfer performance on other sizes of RoBERTa and T5, which are shown in Figure 5. According to these results, we can find that the transferability of soft prompts between the tasks of different types is generally poor, which is consistent with the conclusion in § 4.1."
    }, {
      "heading" : "B.2 Unifying Label Tokens",
      "text" : "We hypothesize that the poor transferability between different task types may result from the fact that different-type tasks usually use different label tokens, e.g., yes and no are for NLI tasks while positive and negative are for SA tasks. To verify whether this factor influences the transferability, we unify the label tokens of different tasks into the same set of numbers (1, 2, . . .) and choose\nIM DB SS T2 la pt op\nre st\nau ra nt M ov ie Tw ee t M NL I QN LI SN LI\nde on\nto lo gy ju st ice QQ P M RP C SQ uA D NQ -O pe n M ul tiNe ws SA M Su m\nTarget Task\nIMDB SST-2\nlaptop restaurant\nMovie Tweet MNLI QNLI SNLI deontology justice\nQQP MRPC SQuAD NQ-Open Multi-News SAMSum random prompt\nSo ur\nce T\nas k\n100 96 84 90 101 56 70 66 58 79 87 86 60 0 0 0 0 94 100 87 93 92 63 85 84 77 77 86 87 57 0 0 0 0 82 86 100 94 79 82 48 64 44 82 86 64 63 0 0 0 0 88 88 86 100 76 77 47 70 44 82 86 82 50 0 0 0 0 90 88 74 84 100 46 63 62 53 81 87 90 66 0 0 0 0 87 86 91 93 88 100 45 56 44 83 86 61 73 0 0 0 0 77 84 79 84 72 46 100 85 89 83 86 68 81 0 0 0 0 77 84 4 2 71 56 73 100 72 83 86 56 89 1 0 0 0 69 69 64 63 71 26 97 84 100 83 86 100 70 0 0 0 0 60 52 30 24 67 26 45 56 44 100 89 54 88 0 0 0 0 74 62 61 60 73 29 45 56 44 91 100 52 86 0 0 0 0 59 52 33 26 69 24 45 56 44 83 86 96 42 0 0 0 0 59 56 48 42 69 24 45 56 44 83 86 90 100 0 0 0 0 78 82 68 67 81 62 47 56 44 77 84 64 69 100 16 49 29 75 81 65 40 79 64 45 56 44 82 86 64 61 8 100 60 45 59 54 28 27 68 36 45 56 44 83 86 75 51 7 32 100 48 74 76 61 67 76 53 45 56 44 81 87 70 57 9 21 71 100 83 68 66 72 81 52 45 56 44 83 86 61 65 7 0 61 22\n(a) T5SMALL IM\nDB SS T2 la pt op re st au ra nt M ov ie Tw ee t M NL I QN LI SN LI de on to lo gy ju st ice QQ P M RP C SQ uA D NQ -O pe n M ul tiNe ws SA M Su m\nTarget Task\nIMDB SST-2\nlaptop restaurant\nMovie Tweet MNLI QNLI SNLI deontology justice\nQQP MRPC SQuAD NQ-Open Multi-News SAMSum random prompt So\nur ce\nT as\nk\n100 94 87 97 101 68 53 79 46 81 86 72 70 0 0 0 0 96 100 89 97 97 67 94 87 88 77 85 73 86 0 0 0 0 85 91 100 99 80 82 67 71 50 78 86 44 86 0 0 0 0 83 86 92 100 80 88 43 55 40 78 84 44 79 0 0 0 0 97 91 88 96 100 67 64 83 54 76 83 63 88 0 0 0 0 86 88 93 99 84 100 39 59 38 77 85 52 63 0 0 0 0 80 80 47 31 82 60 100 85 94 77 83 73 43 0 0 0 0 57 52 27 23 67 23 71 100 71 77 83 73 46 1 0 0 0 91 91 85 96 85 67 98 84 100 75 85 72 54 0 0 0 0 57 52 28 29 67 24 43 54 41 100 98 42 94 0 0 0 0 57 52 26 24 66 23 41 54 40 92 100 43 94 0 0 0 0 57 54 69 82 65 60 76 65 74 77 85 100 87 0 0 0 0 67 56 70 83 68 62 42 54 40 78 83 75 100 0 0 0 0 84 66 43 30 80 65 87 60 80 78 83 66 55 100 24 11 5 82 80 45 34 81 72 62 55 49 77 83 43 83 8 100 11 2 71 78 69 82 85 55 42 54 40 79 83 43 91 5 18 100 17 82 81 67 86 88 56 66 65 66 78 83 71 52 7 18 53 100 69 56 34 22 67 59 57 55 45 78 83 73 44 0 0 0 0\n(b) T5BASE\nRoBERTaBASE for the experiments. In Figure 6, we can observe that the transferability between different-type tasks are generally not improved in this way. This indicates that different-type tasks surely require distinct abilities, which prohibits reusing prompts between them."
    }, {
      "heading" : "B.3 Speedup Calculation",
      "text" : "In this paper, we compute convergence speedup and comparable-result speedup as follows:\nConvergence Speedup(x) = PT convergence time\nTPT convergence time ,\nComparable-result Speedup(x) = PT convergence time\ntime of TPT achieving comparable result to PT .\n(8)\nWe calculate the training loss and the evaluation score per 100 steps during the training. When the training loss stops dropping and the evaluation score stops increasing for 300 steps, we set the point as the convergence point. For the convergence speedup in Equation 8, the PT convergence time is divided by the TPT convergence time. As for the comparable-result speedup in Equation 8, the PT convergence time are divided by the time of TPT achieving comparable performance to PT."
    }, {
      "heading" : "C Cross-Model Transfer",
      "text" : "C.1 Implementation Details of Projector\nAs mentioned in § 5.1, we give the prompt of the source PLM, P s = {ps1, . . . ,psl }, and concatenate its l virtual tokens into a unified vector Ps ∈ Rlds , where ds is the hidden size of the source PLM. To transfer Ps to the target PLM whose hidden size is dt, we design a projection function Proj(·) parameterized by a two-layer perceptron as follows:\nP̃s=Proj(Ps)=W2(σ(P sW1+b1))+b2, (9)\nwhere W1 ∈ Rdh×lds ,W2 ∈ Rldt×dh are trainable matrices, b1 ∈ Rdh ,b2 ∈ Rldt are biases, σ is a non-linear activation function. We set the inner hidden size dh to 768. In this paper, we investigate cross-model transfer among various PLMs including BERTBASE, RoBERTaBASE, RoBERTaLARGE, T5SMALL, T5BASE, and T5XXL, whose hidden sizes are 768, 768, 1024, 512, 768, and 1024, respectively. Besides, for non-linear activation functions, we have tried tanh and LeakyReLU (Xu et al., 2015), and find their performance on various PLMs are similar. The reported results are based on the LeakyReLU activation."
    }, {
      "heading" : "C.2 More Zero-shot Transfer Performance",
      "text" : "In § 5.2, we have introduced the zero-shot transfer performance of various projector-learning methods in the setting of transferring from RoBERTaLARGE to T5XXL. We explore more crossmodel transfer settings here, which are transferring between various PLMs in different scales and heterogeneous frameworks, including from BERTBASE to RoBERTaBASE, from RoBERTaBASE to RoBERTaLARGE, and from T5BASE to T5XXL.\nTable 7 shows the experimental results. We can see the phenomena and conclusions are all consistent with § 5.2."
    }, {
      "heading" : "C.3 Technical Details of TPTMODEL (Transfer with Initialization)",
      "text" : "In § 5.3, we demonstrate cross-model transferrable prompt tuning (TPTMODEL) can well improve performance and reduce training time.\nHowever, when we apply TPTMODEL to more PLMs, we find that the projected prompts may have quite different L2 norm values with the original prompts, especially for the small-scale PLMs (e.g., from BERTBASE to RoBERTaBASE). Specifically, we obtain the projected prompts with the trained Task Tuning projector, and find that the pro-\njected prompts are hard to optimize in some tasks as shown in Figure 7 [Without LayerNorm]. Thus, we attempt to add the layer normalization operation (Ba et al., 2016) LayerNorm into the projectors to regularize the norm of the projected prompt as follows:\nP̃s = LayerNorm(Proj(Ps)). (10)\nBy the LayerNorm, the projected prompts can work well on TPTMODEL and achieve better performance and speedup as shown in Figure 7 [With LayerNorm]. Interestingly, although prompts projected by the projectors [Without LayerNorm] are hard to be trained in TPTMODEL, they can achieve similar zero-shot transfer performance with the prompts projected by the projectors [With LayerNorm] in Table 8."
    }, {
      "heading" : "D Transferability Indicator",
      "text" : ""
    }, {
      "heading" : "D.1 Effectiveness of Similarity Metrics",
      "text" : "We categorize all prompts into three groups: same tasks (prompts trained with different seeds on the same dataset), same-type tasks, and different-type tasks. Table 9 shows that all the similarity metrics successfully distinguish task types."
    }, {
      "heading" : "D.2 Correlation Between Prompt Transferability and Prompt Similarity",
      "text" : "In § 6, we provide the overall averaged Spearman’s rank correlation scores (%) between various similarity metrics and zero-shot transfer performance of soft prompts for RoBERTaLARGE and T5XXL.\nHere, we further show Spearman’s rank correlation scores grouped by the task types on more PLMs. The results are shown in Table 10 and Table 11."
    }, {
      "heading" : "D.3 PLMs’ Redundancy Influence Indicators",
      "text" : "From Table 10, we find that the correlation between prompt transferability and prompt similarity will drop with the increase of PLM size.\nWe guess that this phenomena may result from PLMs’ high redundancy (Aghajanyan et al., 2021). To try to overcome this, we simultaneously utilize the prompts trained with three random seeds on the same dataset and take their intersection of activation states as the activated neurons into the similarity (ON) computation. This similarity is called ONI. By using it, the correlation score of ON can significantly raise as shown in Table 10."
    }, {
      "heading" : "D.4 Overlapping Rate of Activated Neurons in Different Layers",
      "text" : "To further understand model stimulation in PLMs, we investigate ON in different layers of PLMs. Specifically, on RoBERTaBASE, we measure the similarity between different prompts with activation states of from 1 to 3 layers (Figure 8), from 4 to 6 layers (Figure 9), from 7 to 9 layers (Figure 10), from 10 to 12 layers (Figure 11), and all 12 layers (Figure 12), respectively.\nWe find that the activated neurons are common in the bottom layers but tend to be more task-specific in top layers, which is consistent with the findings of previous works (Liu et al., 2019a)."
    } ],
    "references" : [ {
      "title" : "Intrinsic dimensionality explains the effectiveness of language model fine-tuning",
      "author" : [ "Armen Aghajanyan", "Sonal Gupta", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th In-",
      "citeRegEx" : "Aghajanyan et al\\.,? 2021",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E. Hinton." ],
      "venue" : "ArXiv preprint, abs/1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
      "author" : [ "Francesco Barbieri", "Jose Camacho-Collados", "Luis Espinosa Anke", "Leonardo Neves." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020,",
      "citeRegEx" : "Barbieri et al\\.,? 2020",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2020
    }, {
      "title" : "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels",
      "author" : [ "Elad Ben Zaken", "Shauli Ravfogel", "Yoav Goldberg." ],
      "venue" : "ArXiv preprint, abs/2106.10199.",
      "citeRegEx" : "Zaken et al\\.,? 2021",
      "shortCiteRegEx" : "Zaken et al\\.",
      "year" : 2021
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Net2net: Accelerating learning via knowledge transfer",
      "author" : [ "Tianqi Chen", "Ian J. Goodfellow", "Jonathon Shlens." ],
      "venue" : "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Pro-",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge neurons in pretrained transformers",
      "author" : [ "Damai Dai", "Li Dong", "Yaru Hao", "Zhifang Sui", "Furu Wei." ],
      "venue" : "ArXiv preprint, abs/2104.08696.",
      "citeRegEx" : "Dai et al\\.,? 2021",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2021
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B. Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir Radev." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformer feed-forward layers are key-value memories",
      "author" : [ "Mor Geva", "Roei Schuster", "Jonathan Berant", "Omer Levy." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484–5495, Online and",
      "citeRegEx" : "Geva et al\\.,? 2021",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2021
    }, {
      "title" : "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
      "author" : [ "Bogdan Gliwa", "Iwona Mochol", "Maciej Biesek", "Aleksander Wawer." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70–79,",
      "citeRegEx" : "Gliwa et al\\.,? 2019",
      "shortCiteRegEx" : "Gliwa et al\\.",
      "year" : 2019
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "WARP: Word-level Adversarial ReProgramming",
      "author" : [ "Karen Hambardzumyan", "Hrant Khachatrian", "Jonathan May." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
      "citeRegEx" : "Hambardzumyan et al\\.,? 2021",
      "shortCiteRegEx" : "Hambardzumyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Pre-trained models: Past, present and future",
      "author" : [ "Wentao Han", "Minlie Huang" ],
      "venue" : "Proceedings of AI Open",
      "citeRegEx" : "Han and Huang,? \\Q2021\\E",
      "shortCiteRegEx" : "Han and Huang",
      "year" : 2021
    }, {
      "title" : "Aligning ai with shared human values",
      "author" : [ "Dan Hendrycks", "Collin Burns", "Steven Basart", "Andrew Critch", "Jerry Li", "Dawn Song", "Jacob Steinhardt." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Hendrycks et al\\.,? 2021",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2021
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "ArXiv preprint, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Parameter-efficient transfer learning for NLP",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin de Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "Proceedings of the 36th International Confer-",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "The power of scale for parameter-efficient prompt tuning",
      "author" : [ "Brian Lester", "Rami Al-Rfou", "Noah Constant." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Domini-",
      "citeRegEx" : "Lester et al\\.,? 2021",
      "shortCiteRegEx" : "Lester et al\\.",
      "year" : 2021
    }, {
      "title" : "Prefix-tuning: Optimizing continuous prompts for generation",
      "author" : [ "Xiang Lisa Li", "Percy Liang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Li and Liang.,? 2021",
      "shortCiteRegEx" : "Li and Liang.",
      "year" : 2021
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Gpt understands, too",
      "author" : [ "Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang." ],
      "venue" : "ArXiv preprint, abs/2103.10385.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Reframing instructional prompts to gptk’s language",
      "author" : [ "Swaroop Mishra", "Daniel Khashabi", "Chitta Baral", "Yejin Choi", "Hannaneh Hajishirzi." ],
      "venue" : "ArXiv preprint, abs/2109.07830.",
      "citeRegEx" : "Mishra et al\\.,? 2021",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2021
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R. Bowman." ],
      "venue" : "ArXiv preprint, abs/1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "SemEval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evaluation",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "What to pre-train on? Efficient intermediate task selection",
      "author" : [ "Clifton Poth", "Jonas Pfeiffer", "Andreas Rücklé", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10585–10605, Online",
      "citeRegEx" : "Poth et al\\.,? 2021",
      "shortCiteRegEx" : "Poth et al\\.",
      "year" : 2021
    }, {
      "title" : "Intermediate-task transfer learning with pretrained language models: When and why",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge inheritance for pre-trained language models",
      "author" : [ "Yujia Qin", "Yankai Lin", "Jing Yi", "Jiajie Zhang", "Xu Han", "Zhengyan Zhang", "Yusheng Su", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou." ],
      "venue" : "ArXiv preprint, abs/2105.13880.",
      "citeRegEx" : "Qin et al\\.,? 2021",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "arXiv e-prints.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "An overview of multi-task learning in deep neural networks",
      "author" : [ "Sebastian Ruder." ],
      "venue" : "ArXiv preprint, abs/1706.05098.",
      "citeRegEx" : "Ruder.,? 2017",
      "shortCiteRegEx" : "Ruder.",
      "year" : 2017
    }, {
      "title" : "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
      "citeRegEx" : "Schick and Schütze.,? 2021a",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "It’s not just size that matters: Small language models are also few-shot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Schick and Schütze.,? 2021b",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "Natural language understanding with the quora question pairs dataset",
      "author" : [ "Lakshay Sharma", "Laura Graesser", "Nikita Nangia", "Utku Evci." ],
      "venue" : "ArXiv preprint, abs/1907.01041.",
      "citeRegEx" : "Sharma et al\\.,? 2019",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2019
    }, {
      "title" : "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "The proof and measurement of association between two things",
      "author" : [ "Charles Spearman." ],
      "venue" : "Proceedings of The American journal of psychology, volume 100, pages 441–471.",
      "citeRegEx" : "Spearman.,? 1987",
      "shortCiteRegEx" : "Spearman.",
      "year" : 1987
    }, {
      "title" : "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "author" : [ "hai Yu", "Hao Tian", "Hua Wu", "Haifeng Wang" ],
      "venue" : "ArXiv preprint,",
      "citeRegEx" : "Yu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Spot: Better frozen model adaptation through soft prompt transfer",
      "author" : [ "Tu Vu", "Brian Lester", "Noah Constant", "Rami Al-Rfou", "Daniel Cer." ],
      "venue" : "ArXiv preprint, abs/2110.07904.",
      "citeRegEx" : "Vu et al\\.,? 2021",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring and predicting transferability across NLP tasks",
      "author" : [ "Tu Vu", "Tong Wang", "Tsendsuren Munkhdalai", "Alessandro Sordoni", "Adam Trischler", "Andrew MattarellaMicke", "Subhransu Maji", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Vu et al\\.,? 2020",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2020
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "7th International Conference on Learning Representa-",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Finetuned language models are zero-shot learners",
      "author" : [ "Jason Wei", "Maarten Bosma", "Vincent Y Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M Dai", "Quoc V Le." ],
      "venue" : "ArXiv preprint, abs/2109.01652.",
      "citeRegEx" : "Wei et al\\.,? 2021",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Empirical evaluation of rectified activations in convolutional network",
      "author" : [ "Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li." ],
      "venue" : "ArXiv preprint, abs/1505.00853.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Machine learning with annotator rationales to reduce annotation cost",
      "author" : [ "Omar F. Zaidan", "Jason Eisner", "Christine Piatko." ],
      "venue" : "Proceedings of NeurIPS (Workshop).",
      "citeRegEx" : "Zaidan et al\\.,? 2008",
      "shortCiteRegEx" : "Zaidan et al\\.",
      "year" : 2008
    }, {
      "title" : "Panguα: Large-scale autoregressive pretrained chinese language models with auto-parallel computation",
      "author" : [ "Wei Zeng", "Xiaozhe Ren", "Teng Su", "Hui Wang", "Yi Liao", "Zhiwei Wang", "Xin Jiang", "ZhenZhang Yang", "Kaisheng Wang", "Xiaoda Zhang" ],
      "venue" : null,
      "citeRegEx" : "Zeng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2021
    }, {
      "title" : "Cpm-2: Large-scale cost-effective pre-trained language models",
      "author" : [ "Zhengyan Zhang", "Yuxian Gu", "Xu Han", "Shengqi Chen", "Chaojun Xiao", "Zhenbo Sun", "Yuan Yao", "Fanchao Qi", "Jian Guan", "Pei Ke" ],
      "venue" : "ArXiv preprint,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Factual probing is [MASK]: Learning vs",
      "author" : [ "Zexuan Zhong", "Dan Friedman", "Danqi Chen." ],
      "venue" : "learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Zhong et al\\.,? 2021",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Pre-trained language models (PLMs), such as BERT (Devlin et al., 2019) and GPT (Radford et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 36,
      "context" : ", 2019) and GPT (Radford et al., 2018) have achieved great performance on various natural language processing (NLP) tasks (Han et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 37,
      "context" : ", 2020), people have found that extremely large PLMs can achieve remarkable improvements, and various large PLMs are continually developed (Raffel et al., 2020; Zhang et al., 2021; Zeng et al., 2021; Wei et al., 2021; Sun et al., 2021), which Initialization Cross-Task Transfer Source Task",
      "startOffset" : 139,
      "endOffset" : 235
    }, {
      "referenceID" : 56,
      "context" : ", 2020), people have found that extremely large PLMs can achieve remarkable improvements, and various large PLMs are continually developed (Raffel et al., 2020; Zhang et al., 2021; Zeng et al., 2021; Wei et al., 2021; Sun et al., 2021), which Initialization Cross-Task Transfer Source Task",
      "startOffset" : 139,
      "endOffset" : 235
    }, {
      "referenceID" : 55,
      "context" : ", 2020), people have found that extremely large PLMs can achieve remarkable improvements, and various large PLMs are continually developed (Raffel et al., 2020; Zhang et al., 2021; Zeng et al., 2021; Wei et al., 2021; Sun et al., 2021), which Initialization Cross-Task Transfer Source Task",
      "startOffset" : 139,
      "endOffset" : 235
    }, {
      "referenceID" : 51,
      "context" : ", 2020), people have found that extremely large PLMs can achieve remarkable improvements, and various large PLMs are continually developed (Raffel et al., 2020; Zhang et al., 2021; Zeng et al., 2021; Wei et al., 2021; Sun et al., 2021), which Initialization Cross-Task Transfer Source Task",
      "startOffset" : 139,
      "endOffset" : 235
    }, {
      "referenceID" : 19,
      "context" : "Hence, various parameter-efficient tuning methods (Houlsby et al., 2019; Ben Zaken et al., 2021; Lester et al., 2021; Li and Liang, 2021; Liu et al., 2021) are explored, among which prompt tuning (PT) has attracted broad research attention.",
      "startOffset" : 50,
      "endOffset" : 155
    }, {
      "referenceID" : 22,
      "context" : "Hence, various parameter-efficient tuning methods (Houlsby et al., 2019; Ben Zaken et al., 2021; Lester et al., 2021; Li and Liang, 2021; Liu et al., 2021) are explored, among which prompt tuning (PT) has attracted broad research attention.",
      "startOffset" : 50,
      "endOffset" : 155
    }, {
      "referenceID" : 23,
      "context" : "Hence, various parameter-efficient tuning methods (Houlsby et al., 2019; Ben Zaken et al., 2021; Lester et al., 2021; Li and Liang, 2021; Liu et al., 2021) are explored, among which prompt tuning (PT) has attracted broad research attention.",
      "startOffset" : 50,
      "endOffset" : 155
    }, {
      "referenceID" : 26,
      "context" : "Hence, various parameter-efficient tuning methods (Houlsby et al., 2019; Ben Zaken et al., 2021; Lester et al., 2021; Li and Liang, 2021; Liu et al., 2021) are explored, among which prompt tuning (PT) has attracted broad research attention.",
      "startOffset" : 50,
      "endOffset" : 155
    }, {
      "referenceID" : 22,
      "context" : "PT can match the downstream task performance of fine-tuning with only thousands of tunable parameters (Lester et al., 2021) when the PLM has billions of parameters.",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 49,
      "context" : "transfer across tasks (Vu et al., 2020) and models (Qin et al.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 35,
      "context" : ", 2020) and models (Qin et al., 2021) have been widely used to improve the efficiency and effectiveness of NLP systems.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "The empirical analysis is conducted on 17 NLP tasks of 6 types and two representative PLM series: RoBERTa (Liu et al., 2019b) and T5 (Raffel et al.",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "Motivated by this, many works have tried to improve various NLP tasks by creating manually-crafted (Schick and Schütze, 2021a,b; Mishra et al., 2021) or automaticallysearched (Jiang et al.",
      "startOffset" : 99,
      "endOffset" : 149
    }, {
      "referenceID" : 20,
      "context" : ", 2021) or automaticallysearched (Jiang et al., 2020; Shin et al., 2020; Gao et al., 2021) hard prompts, which are discrete tokens but not necessarily human-readable.",
      "startOffset" : 33,
      "endOffset" : 90
    }, {
      "referenceID" : 43,
      "context" : ", 2021) or automaticallysearched (Jiang et al., 2020; Shin et al., 2020; Gao et al., 2021) hard prompts, which are discrete tokens but not necessarily human-readable.",
      "startOffset" : 33,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : ", 2021) or automaticallysearched (Jiang et al., 2020; Shin et al., 2020; Gao et al., 2021) hard prompts, which are discrete tokens but not necessarily human-readable.",
      "startOffset" : 33,
      "endOffset" : 90
    }, {
      "referenceID" : 23,
      "context" : "Furthermore, soft prompts (Li and Liang, 2021; Hambardzumyan et al., 2021; Zhong et al., 2021; Liu et al., 2021) are proposed, which are tuneable embeddings rather than tokens in the vocabularies and can be directly trained with task-specific supervi-",
      "startOffset" : 26,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "Furthermore, soft prompts (Li and Liang, 2021; Hambardzumyan et al., 2021; Zhong et al., 2021; Liu et al., 2021) are proposed, which are tuneable embeddings rather than tokens in the vocabularies and can be directly trained with task-specific supervi-",
      "startOffset" : 26,
      "endOffset" : 112
    }, {
      "referenceID" : 57,
      "context" : "Furthermore, soft prompts (Li and Liang, 2021; Hambardzumyan et al., 2021; Zhong et al., 2021; Liu et al., 2021) are proposed, which are tuneable embeddings rather than tokens in the vocabularies and can be directly trained with task-specific supervi-",
      "startOffset" : 26,
      "endOffset" : 112
    }, {
      "referenceID" : 26,
      "context" : "Furthermore, soft prompts (Li and Liang, 2021; Hambardzumyan et al., 2021; Zhong et al., 2021; Liu et al., 2021) are proposed, which are tuneable embeddings rather than tokens in the vocabularies and can be directly trained with task-specific supervi-",
      "startOffset" : 26,
      "endOffset" : 112
    }, {
      "referenceID" : 39,
      "context" : "Knowledge Transfer Cross-task knowledge transfer (Ruder, 2017) has been a long-standing way to improve the effectiveness and efficiency of NLP systems.",
      "startOffset" : 49,
      "endOffset" : 62
    }, {
      "referenceID" : 31,
      "context" : "In the PLM era, some works propose to tune the PLMs on intermediate tasks (Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020; Wang et al., 2019a; Vu et al., 2020; Poth et al., 2021) before fine-tuning on specific target tasks to achieve certain benefits.",
      "startOffset" : 74,
      "endOffset" : 203
    }, {
      "referenceID" : 34,
      "context" : "In the PLM era, some works propose to tune the PLMs on intermediate tasks (Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020; Wang et al., 2019a; Vu et al., 2020; Poth et al., 2021) before fine-tuning on specific target tasks to achieve certain benefits.",
      "startOffset" : 74,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "In the PLM era, some works propose to tune the PLMs on intermediate tasks (Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020; Wang et al., 2019a; Vu et al., 2020; Poth et al., 2021) before fine-tuning on specific target tasks to achieve certain benefits.",
      "startOffset" : 74,
      "endOffset" : 203
    }, {
      "referenceID" : 49,
      "context" : "In the PLM era, some works propose to tune the PLMs on intermediate tasks (Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020; Wang et al., 2019a; Vu et al., 2020; Poth et al., 2021) before fine-tuning on specific target tasks to achieve certain benefits.",
      "startOffset" : 74,
      "endOffset" : 203
    }, {
      "referenceID" : 33,
      "context" : "In the PLM era, some works propose to tune the PLMs on intermediate tasks (Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020; Wang et al., 2019a; Vu et al., 2020; Poth et al., 2021) before fine-tuning on specific target tasks to achieve certain benefits.",
      "startOffset" : 74,
      "endOffset" : 203
    }, {
      "referenceID" : 48,
      "context" : "Similar to our work, concurrent work (Vu et al., 2021) explores the cross-task transferability of PT and improves performance with transfer initialization.",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "Additionally, we also attempt cross-model transfer, which is inspired by previous cross-model knowledge transfer works such as Net2Net (Chen et al., 2016), knowledge distillation (Hinton et al.",
      "startOffset" : 135,
      "endOffset" : 154
    }, {
      "referenceID" : 18,
      "context" : ", 2016), knowledge distillation (Hinton et al., 2015) and knowledge inheritance (Qin et al.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : ", 2015) and knowledge inheritance (Qin et al., 2021).",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "In this work, we study the PT method that is capable of tuning large PLMs (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021), i.",
      "startOffset" : 74,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "In this work, we study the PT method that is capable of tuning large PLMs (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021), i.",
      "startOffset" : 74,
      "endOffset" : 133
    }, {
      "referenceID" : 26,
      "context" : "In this work, we study the PT method that is capable of tuning large PLMs (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021), i.",
      "startOffset" : 74,
      "endOffset" : 133
    }, {
      "referenceID" : 29,
      "context" : "To comprehensively study the prompt transferability across various NLP tasks, we involve 17 diverse tasks, which can be divided into 6 types: (1) Sentiment Analysis (SA), including IMDB (Maas et al., 2011), SST-2 (Socher et al.",
      "startOffset" : 186,
      "endOffset" : 205
    }, {
      "referenceID" : 44,
      "context" : ", 2011), SST-2 (Socher et al., 2013), laptop (Pontiki et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : ", 2013), laptop (Pontiki et al., 2014), restaurant (Pontiki et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 32,
      "context" : ", 2014), restaurant (Pontiki et al., 2014), Movie Rationales (Movie) (Zaidan et al.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 54,
      "context" : ", 2014), Movie Rationales (Movie) (Zaidan et al., 2008) and TweetEval (Tweet) (Barbieri et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : ", 2008) and TweetEval (Tweet) (Barbieri et al., 2020); (2) Natural Language Inference (NLI), including MNLI (Williams et al.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 52,
      "context" : ", 2020); (2) Natural Language Inference (NLI), including MNLI (Williams et al., 2018), QNLI (Wang et al.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 50,
      "context" : ", 2018), QNLI (Wang et al., 2019b) and SNLI (Bowman et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : ", 2019b) and SNLI (Bowman et al., 2015); (3) Ethical Judgement (EJ), including deontology (Hendrycks et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : ", 2015); (3) Ethical Judgement (EJ), including deontology (Hendrycks et al., 2021) and justice (Hendrycks et al.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : ", 2021) and justice (Hendrycks et al., 2021); (4) Paraphrase Identification (PI), including QQP (Sharma et al.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 42,
      "context" : ", 2021); (4) Paraphrase Identification (PI), including QQP (Sharma et al., 2019) and MRPC (Dolan and Brockett, 2005); (5) Question Answering (QA), including SQuAD (Rajpurkar et al.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : ", 2019) and MRPC (Dolan and Brockett, 2005); (5) Question Answering (QA), including SQuAD (Rajpurkar et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 38,
      "context" : ", 2019) and MRPC (Dolan and Brockett, 2005); (5) Question Answering (QA), including SQuAD (Rajpurkar et al., 2016) and NQ-Open (Lee et al.",
      "startOffset" : 90,
      "endOffset" : 114
    }, {
      "referenceID" : 21,
      "context" : ", 2016) and NQ-Open (Lee et al., 2019); (6) Summarization (SUM), including Multi-News (Fabbri et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : ", 2019); (6) Summarization (SUM), including Multi-News (Fabbri et al., 2019) and SAMSum (Gliwa et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "We investigate prompt transferability for two series of PLMs: RoBERTa (Liu et al., 2019b) and T5 (Raf-",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 47,
      "context" : "(2021), which both find that the activation of the neurons in the feed-forward layers of Transformers (Vaswani et al., 2017) corresponds to specific model behaviors, we propose to use the overlapping rate of activated neurons as a similarity metric of prompts.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "The max(xW> 1 + b1,0) can be regarded as the non-negative activation values for dm hidden neurons (Geva et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 25,
      "context" : "In experiments, we find that the higher layers tend to be more task-specific, which is consistent with the probing results (Liu et al., 2019a).",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 45,
      "context" : "Specifically, for each target task’s prompt, we rank various source tasks’ prompts with similarity scores and zero-shot transfer performance and then compute the Spearman’s rank correlation (Spearman, 1987) between the two ranks generated by these two ways.",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 0,
      "context" : "We guess this is because larger PLMs have higher redundancy (Aghajanyan et al., 2021), which means prompts can activate different redundant neurons to do similar jobs and thus influence the sensitivity of ON metric.",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 29,
      "context" : "We choose IMDB (Maas et al., 2011), SST-2 (Socher et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 44,
      "context" : ", 2011), SST-2 (Socher et al., 2013), SemEval/laptop (Pontiki et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : ", 2013), SemEval/laptop (Pontiki et al., 2014), SemEval/restaurant (Pontiki et al.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 32,
      "context" : ", 2014), SemEval/restaurant (Pontiki et al., 2014), Movie Rationales (Movie) (Zaidan et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 54,
      "context" : ", 2014), Movie Rationales (Movie) (Zaidan et al., 2008), and TweetEval (Tweet) (Barbieri et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : ", 2008), and TweetEval (Tweet) (Barbieri et al., 2020) to analyze.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 50,
      "context" : ", 2018), QNLI (Wang et al., 2019b), and SNLI (Bowman et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "We choose Ethics/deontology (Hendrycks et al., 2021) and Ethics/justice (Hendrycks et al.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : ", 2021) and Ethics/justice (Hendrycks et al., 2021) to analyze.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 42,
      "context" : "We choose QQP (Sharma et al., 2019) and MRPC (Dolan and Brockett, 2005) to analyze.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 38,
      "context" : "We choose SQuAD (Rajpurkar et al., 2016) and NQ-Open (Lee et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "We choose Multi-News (Fabbri et al., 2019), and SAMSum (Gliwa et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 24,
      "context" : "For QA and SUM tasks, we utilize F1 and ROUGEL (Lin, 2004), respectively.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 28,
      "context" : "In the experiments, for all the investigated tasks, we use AdamW (Loshchilov and Hutter, 2019) as the optimizer and set the learning rate as 0.",
      "startOffset" : 65,
      "endOffset" : 94
    }, {
      "referenceID" : 53,
      "context" : "Besides, for non-linear activation functions, we have tried tanh and LeakyReLU (Xu et al., 2015), and find their performance on various PLMs are similar.",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "Thus, we attempt to add the layer normalization operation (Ba et al., 2016) LayerNorm into the projectors to regularize the norm of the projected prompt as follows:",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "We guess that this phenomena may result from PLMs’ high redundancy (Aghajanyan et al., 2021).",
      "startOffset" : 67,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "We find that the activated neurons are common in the bottom layers but tend to be more task-specific in top layers, which is consistent with the findings of previous works (Liu et al., 2019a).",
      "startOffset" : 172,
      "endOffset" : 191
    } ],
    "year" : 0,
    "abstractText" : "Prompt tuning (PT) is a promising parameterefficient method to utilize extremely large pre-trained language models (PLMs), which can achieve comparable performance to fullparameter fine-tuning by only tuning a few soft prompts. However, PT requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve PT via prompt transfer, we empirically investigate the transferability of soft prompts across different downstream tasks and PLMs in this work. We find that (1) in zero-shot setting, trained soft prompts can effectively transfer to similar tasks on the same PLM and also to other PLMs with a cross-model projector trained on similar tasks; (2) when used as initialization, trained soft prompts of similar tasks and projected prompts of other PLMs can significantly accelerate training and also improve the performance of PT. Moreover, to explore what decides prompt transferability, we investigate various transferability indicators and find that the overlapping rate of activated neurons strongly reflects the transferability, which suggests how the prompts stimulate PLMs is essential. Our findings show that prompt transfer is promising for improving PT, and further research shall focus more on prompts’ stimulation to PLMs. The source code will be publicly released.",
    "creator" : null
  }
}
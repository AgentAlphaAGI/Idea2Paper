{
  "name" : "ARR_2022_322_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Non-Autoregressive Machine Translation: It’s Not as Fast as it Seems",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Non-autoregressive neural machine translation (NAR NMT, or NAT; Gu et al., 2018; Lee et al., 2018) is an emerging subfield of NMT which focuses on lowering the decoding time complexity by changing the model architecture.\nThe defining feature of non-autoregressive models is the conditional independence assumption on the output probability distributions; this is in contrast to autoregressive models, where the output distributions are conditioned on the previous outputs. This conditional independence allows one to decode the target tokens in parallel. This can substantially reduce the decoding time, especially for longer target sentences.\nThe speed of the decoding is assessed by translating a test set and measuring the overall time the process takes. This may sound simple, but there are various aspects to be considered that can affect decoding speed, such as batching, number of hypotheses in beam search or hardware used (i.e., using CPU or GPU). Decoding speed evaluation is a challenging task, especially when it comes to comparability across different approaches. Unlike the translation quality, decoding speed can be measured exactly. However, also unlike the translation quality, different results are obtained under different evaluation environments. The WMT Efficient Translation Shared Task aims to evaluate efficiency research and encourages the reporting of a range of speed and translation quality values to better understand the trade-off across different model configurations (Heafield et al., 2021). In this paper, we follow the emerging best practices developed in the Efficiency Shared Task and directly compare with the submitted systems.\nOver the course of research on NAR models, modeling error and its subsequent negative effect on translation quality remains the biggest issue. Therefore, the goal of contemporary research is to close the performance gap between the AR models and their NAR counterparts, while maintaining high decoding speed. Considering these stated research goals, the evaluation should comprise of assessing translation quality as well as decoding speed.\nTranslation quality is usually evaluated by scoring translations of an unseen test set either using automatic metrics, such as BLEU (Papineni et al., 2002), ChrF (Popović, 2015) or COMET (Rei et al., 2020), or using human evaluation. To prevent the methods from eventually overfitting a single test set, new test sets are published each year as part of the WMT News Translation Shared Task. In contrast, translation quality evaluation in NAR research is often measured using BLEU scores only, measured al-\nmost exclusively on the WMT 14 English-German test set, which is highly problematic. Automatic evaluation of translation quality however remains an open research problem (Mathur et al., 2020; Kocmi et al., 2021). In our experiments, we follow the recent best practices by using multiple metrics and recent test sets.\nIn this paper, we examine the evaluation methodology generally accepted in literature on NAR methods, and we identify a number of flaws. First, the results are reported on different hardware architectures, which makes them incomparable, even when comparing only relative speedups. Second, most of the methods only report latency (decoding with a single sentence per batch) using a GPU; we show that this is the only setup favors NAR models. Third, the reported baseline performance is usually questionable, both in terms of speed and translation quality. Finally, despite the fact that the main motivation for using NAR models is the lower time complexity, the findings of the efficiency task are ignored in most of the NAR papers.\nWe try to connect the separate worlds of NAR and efficient translation research. We train nonautoregressive models based on connectionist temporal classification (CTC), an approach previously shown to be effective (Libovický and Helcl, 2018; Gu and Kong, 2021; Ghazvininejad et al., 2020). We employ a number of techniques for improving the translation quality, including data cleaning and sequence-level knowledge distillation (Kim and Rush, 2016). We evaluate our models following a unified evaluation methodology: In order to compare the translation quality with the rest of the NAR literature, we report BLEU scores measured on the WMT 14 test set, on which we achieve state-ofthe-art performance among (both single-step and iterative) NAR methods; we evaluate the translation quality and decoding speed of our models in the same conditions as the efficiency task.\nWe find that despite achieving very good results among the NAT models on the WMT 14 test set, our models fall behind in translation quality when measured on the recent WMT 21 test set using three different automatic evaluation metrics. Moreover, we show that GPU decoding latency is the only scenario in which non-autoregressive models outperform autoregressive models.\nThis paper contributes to the research community in the following aspects: First, we point out to weaknesses in standard evaluation methodology\nof non-autoregressive models. Second, we link the worlds of non-autoregressive translation and model optimization to provide a better understanding of the results achieved in the related work."
    }, {
      "heading" : "2 Non-Autoregressive NMT",
      "text" : "The current state-of-the-art NMT models are autoregressive – the output distributions are conditioned on the previously generated tokens (Bahdanau et al., 2016; Vaswani et al., 2017). The decoding process is sequential in its nature, limiting the opportunities for parallelization.\nNon-autoregressive models use output distributions which are conditionally independent on each other, which opens up the possibility of parallelization. Formally, the probability of a sequence y given the input x in a non-autoregressive model with parameters θ is modeled as\npθ(y|x) = ∏ yi∈y p(yi|x, θ). (1)\nUnsurprisingly, the independence assumption in NAR models has a negative impact on the translation quality. The culprit for this behavior is the multimodality problem – the inability of the model to differentiate between different modes of the joint probability distribution over output sequences inside the distributions corresponding to individual time steps. A classic example of this issue is the sentence “Thank you” with its two equally probable German translations “Danke schön” and “Vielen Dank” (Gu et al., 2018). Because of the independence assumption, a non-autoregressive model cannot assign high probabilities to these two translations without also allowing for the incorrect sentences “Vielen schön” and “Danke Dank”.\nKnowledge distillation (Kim and Rush, 2016) has been successfully employed to reduce the negative influence of the multimodality problem in NAR models (Gu et al., 2018; Saharia et al., 2020). Synthetic data tends to be less diverse than authentic texts, therefore the number of equally likely translation candidates gets smaller (Zhou et al., 2020).\nA number of techniques have been proposed for training NAR models, including iterative methods (Lee et al., 2018; Ghazvininejad et al., 2019), auxiliary training objectives (Wang et al., 2019; Qian et al., 2021), or latent variables (Gu et al., 2018; Lee et al., 2018; Kaiser et al., 2018). In some form, all of the aforementioned approaches use explicit target length estimation, and rely on one-to-one\ncorrespondence between the output distributions and the reference sentence.\nA group of methods that relax the requirement of the strict one-to-one alignment between the model outputs and the ground-truth target sequence include aligned cross-entropy (Ghazvininejad et al., 2020) and connectionist temporal classification (Libovický and Helcl, 2018).\nThe schema CTC-based model, as proposed by Libovický and Helcl (2018), is shown in Figure 1. The model extends the Transformer architecture (Vaswani et al., 2017). It consists of an encoder, a state-splitting layer, and a non-autoregressive decoder. The encoder has the same architecture as in the Transformer model. The state-splitting layer, applied on the encoder output, linearly projects and splits each state into k states with the same dimension. The decoder consists of a stack of Transformer layers. Unlike the Transformer model, the self-attention in the non-autoregressive decoder does not use the causal mask, so the model is not prevented from attending to future states. Since the output length is fixed to k-times the length of the source sentence, the model is permitted to output blank tokens. Different positions of the blank tokens in the output sequence represent different alignments between the outputs and the groundtruth sequence. Connectionist temporal classification (Graves et al., 2006) is a dynamic algorithm that efficiently computes the standard cross-entropy loss summed over all possible alignments.\nWe choose the CTC-based architecture for our models because it has been previously shown to be effective for NAR NMT (Gu and Kong, 2021; Saharia et al., 2020) and performs well in the context of the non-autoregressive research. It is also one of the fastest NAR architectures since it is not iterative."
    }, {
      "heading" : "3 Evaluation Methodology",
      "text" : "The research goal of the non-autoregressive methods is to improve the translation quality while maintaining the speedup brought by the conditional independence assumption. This means that careful thought should be given to both quantifying the speed gains and the translation quality evaluation. The speed-vs-quality trade-off can be characterized by the Pareto frontier. In this section we discuss the evaluation from both perspectives.\nTranslation Quality. In the world of nonautoregressive NMT, the experimental settings\nare not very diverse. The primary language pair for translation experiments is English-German, sometimes accompanied by English-Romanian to simulate the low-resource scenario. These language pairs, along with the widely used test sets – WMT 14 (Bojar et al., 2014) for En-De and WMT 16 (Bojar et al., 2016) for En-Ro – became the de facto standard benchmark for NAR model evaluation.\nA common weakness seen in the literature is the use of weak baseline models. The base variant of the Transformer model is used almost exclusively (Gu et al., 2018; Gu and Kong, 2021; Lee et al., 2018; Ghazvininejad et al., 2020; Qian et al., 2021). We argue that using weaker baselines might lead to overrating the positive effects brought by proposed improvements. Since the baseline autoregressive models are used to generate the synthetic parallel data for knowledge distillation, the weakness is potentially further amplified in this step.\nEvaluation is normally with automatic metrics only, and often only BLEU is reported. In light of recent research casting further doubt on the reliability of BLEU as a measure of translation quality (Kocmi et al., 2021), we argue that this is insufficient.\nDecoding Speed. The current standard in evaluation of NAR models is to measure translation latency using a GPU, i.e., the average time to trans-\nlate a single sentence without batching. Since the time depends on the hardware, relative speedup is usually reported along with latency.\nThis is a reasonable approach but we need to keep in mind the associated difficulties. First, the results achieved on different hardware architectures are not easily comparable even when considering the relative speedups. We also note that the relative speedup values should always be accompanied by the corresponding decoding times in absolute numbers. Sometimes, this information is missing from the published results (Qian et al., 2021).\nWe argue that measuring only GPU latency disregards other use-cases. In the WMT Efficiency Shared Task, the decoding speed is measured in five scenarios. The speed is reported using a GPU with and without batching, using all 36 CPU cores (also, with and without batching), and using a single CPU core without batching. In batched decoding, the shared task participants could choose the optimal batch size. Our results in Section 5 show that measuring latency is the only one that favors NAR models, and as the batch size increases, AR models quickly reach higher translation speeds."
    }, {
      "heading" : "4 Experiments",
      "text" : "We experiment with non-autoregressive models for English-German translation. We used the data provided by the WMT 21 News Translation Shared Task organizers (Akhbardeh et al., 2021).\nAs our baseline model, we use the CTC-based NAR model as described by Libovický and Helcl (2018). We use stack of 6 encoder and 6 decoder layers, separated by the state splitting layer which extends the state sequence 3 times.\nWe implement our models in the Marian toolkit (Junczys-Dowmunt et al., 2018). For the CTC loss computation, we use the warp-ctc library (Amodei et al., 2016)."
    }, {
      "heading" : "4.1 Teacher Models",
      "text" : "For training our baseline autoregressive models, we closely follow the approach of Chen et al. (2021). The preparation of the baseline models consists of three phases – data cleaning, backtranslation, and the training of the final models.\nWe train the teacher models on cleaned parallel corpora and backtranslated monolingual data. For the parallel data, we used Europarl (Koehn, 2005), the RAPID corpus (Rozis and Skadin, š, 2017), and the News Commentary corpus from OPUS (Tiedemann, 2012). We consider these three parallel dataset clean. We also use noisier parallel datasets, namely Paracrawl (Bañón et al., 2020), Common Crawl1, WikiMatrix (Schwenk et al., 2019), and Wikititles2. For backtranslation, we used the monolingual datasets from the News Crawl from the years 2018-2020, in both English and German.\nWe clean the parallel corpus (i.e. both clean and noisy portions) using rule-based cleaning3. Additionally, we exclude sentence pairs with nonlatin characters. Additionally, we apply dual crossentropy filtering on the noisy part of the parallel data (Junczys-Dowmunt, 2018). We train Transformer base models in both directions on the clean portion of the parallel data. Then, we select 75% of the best-scoring sentence pairs into the final parallel portion of the training dataset.\nFor backtranslation (Sennrich et al., 2016), we train four Transformer big models on the cleaned parallel data in both directions. We then use them in an ensemble to create the synthetic source side for the monolingual corpora. We add a special symbol to the generated sentences to help the models differentiate between synthetic and authentic source language data (Caswell et al., 2019).\nWe use hyperparameters of the Transformer big model, i.e. model dimension 1,024, feed-forward hidden dimension of 4,096, and 16 attention heads. For training, we use the Adam optimizer (Kingma and Ba, 2014) with β1, β2 and ϵ set to 0.9, 0.998 and 10-9 respectively. We used the inverted squareroot learning rate decay with 8,000 of linear warmup and initial learning rate of 10-4.\nThe teacher models follow the same hyperparameter settings as the models for backtranslation,\n1https://commoncrawl.org/ 2https://linguatools.org/tools/\ncorpora/wikipedia-parallel-titles-corpora/ 3https://github.com/browsermt/ students/blob/master/train-student/ clean/clean-corpus.sh\nbut are trained with the tagged backtranslations included in the data. As in the previous case, we train four teacher models with different random seeds for ensembling.\nSimilar to creating the backtranslations, we use the four teacher models in an ensemble to create the knowledge-distilled data (Kim and Rush, 2016). We translate the source side of the parallel data, as well as the source-language monolingual data. We do not translate back-translated data. Thus, the source side data for the student models is authentic, and the target side is synthetic, created by the teacher models."
    }, {
      "heading" : "4.2 Student Models",
      "text" : "We train five variants of the student models with different hyperparameter settings. The “Large” model is our baseline model – the same number of layers as the teacher models, 6 in the encoder, followed by the state splitting layer, and another 6 layers in the decoder. The “Base” model has the same number of layers with reduced dimension of the embeddings and the feed-forward Transformer sublayer, to match the Transformer base settings. We also try reducing the numbers of encoder and decoder layers. We shrink the base model to 3-3 (“Small”), 2-2 (“Micro”), and 1-1 (“Tiny”) architectures.\nWe run the training of each model for three weeks on four Nvidia Pascal P100 GPUs."
    }, {
      "heading" : "5 Results",
      "text" : "In this section, we try and view the results of the NAR and efficiency research in a shared perspective. We evaluate our models and present results in terms of translation quality and decoding speed. We compare the results to the related work on both non-autoregressive translation and model optimization.\nTranslation Quality. The research on nonautoregressive models uses the BLEU score (Papineni et al., 2002) measured on the WMT 14 test set (Bojar et al., 2014) as a standard benchmark for evaluating translation quality. We use Sacrebleu (Post, 2018) as the implementation of the BLEU score metric.4 Using a single test set for the whole volume of research on this topic may however produce misleading results. To bring the evaluation\n4Signature: nrefs:3|bs:1000|seed:12345| case:mixed|eff:no|tok:13a|smooth:exp| version:2.0.0\nup to date with the current state-of-the-art translation systems, we also evaluate our models using COMET (Rei et al., 2020)5 and BLEU scores on the recent WMT 21 test set. The same test set was used in the WMT 21 Efficiency Task.\nTable 2 shows the BLEU scores of our NAR models on the WMT 14 test set. We show the results of the five variants of the NAR models and we include three of the best-performing NAR approaches from the related work. We see from the table that using BLEU, the “Large” model scores among the best NAR models on the WMT 14 test set. As the NAR model size decreases, so does the translation quality, with the notable exception of the En→De “Micro” model, which outperforms the “Base” model consistently on different test sets.\nIn Table 3, we report the automatic evaluation results of our AR and NAR models on the multireference WMT 21 test set (Akhbardeh et al., 2021). We compare our NAR models to the AR large teacher models from Section 4.1, an AR base model trained on the original clean data, and an AR base student model trained on the distilled data. Following Heafield et al. (2021), we use references A, C, and D for English-German translation.\nWe see that there is a considerable difference in the translation quality between the NAR models and the AR large teacher model. This difference grows with beam search and ensembling applied on the AR decoding, techniques not usually used with NAR models because of the speed cost. We also note that when we train an AR base model on the distilled data, it outperforms the NAR large model by a considerable margin.\nAnother thing we notice is the enormous differ-\n5We use the COMET model wmt20-comet-da from version dd2298 (1.0.0.rc9).\nence in the COMET scores between the AR and NAR models. The AR base models achieve comparable BLEU scores to the NAR large models, but differ substantially in the COMET score. From a look at the system outputs, we hypothesize that the NAR systems produce unusual errors which BLEU does not penalise as heavily as COMET. This might suggest that NAR models would rank poorly in human evaluation relative to their autoregressive counterparts, despite the reasonable BLEU score values.\nDecoding speed. We follow the decoding time evaluation methodology of the WMT 21 Efficient Translation Shared Task (Heafield et al., 2021). We recreate the hardware conditions that were used in the task. For the GPU decoding measurements, we use a single Nvidia Ampere A100 GPU. The CPU evaluation was performed on a 36-core CPU Intel Xeon Gold 6354 server from Oracle cloud. To amortize for the various computation overheads, the models submitted to the shared task are evaluated on a million sentence benchmark dataset.\nWe measure the overall wall time to translate the shared task dataset with different batching settings on both the GPU and the 36-core CPU. The decoding times are shown in Figures 2 and 4 for the GPU and CPU times, respectively. We do not report the single-core CPU latencies as the decoding speed of the NAR models falls far behind the efficient AR models in this setup and the translation of the\ndataset takes too long.\nWe can see that in case of GPU decoding that all models benefit from having larger batch sizes. However, the non-autoregressive models are much faster when the batch size is small. We also ran the evaluation on an Nvidia Pascal P100 GPU, which showed that when the batch size is large enough, autoregressive models eventually match the speed of non-autoregressive models. We show the decoding times on the Pascal GPU in Figure 3. In Table 4, we compare the latencies measured on the Pascal GPU to some of the related NAR approaches that report results on this GPU type. Due to implementation reasons, the maximum batch size for our NAR models is around 220 sentences.\nComparison with Efficient AR Models. In Table 5, we present a comparison on the million sentence test set with “Edinburgh base”, one of the leading submissions in the WMT 21 efficiency task (Behnke et al., 2021). First, we see that using three different evaluation metrics (ChrF, COMET, and BLEU), our models lag behind the Edinburgh base model. In line with our previous observation, we see a considerable drop in the COMET score values. In terms of decoding speed, the only scenario in which the non-autoregressive model is better is on GPU with batch size 1. This is in line with our intuition that the parallelization potential brought by the GPU is utilized more efficiently by the NAR model. On one hand, larger batches open up the parallelization possibilities to AR models. On the other hand, limited parallelization potential (in form of CPU decoding) reduces the differences between AR and NAR models. The batch size of the Edinburgh base model was 1,280 in the batched decoding setup."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we challenge the evaluation methodology adopted by the research on non-autoregressive models for NMT.\nWe argue that in terms of translation quality, the evaluation should include newer test sets and metrics other than BLEU (particularly COMET and ChrF). This will provide more insight and put the results into the context of the recent research.\nFrom the decoding speed perspective, we should bear in mind various use-cases for the model deployment, such as the hardware environment or batching conditions. Preferably, the research should evaluate the speed gains across a range of scenarios. Finally, given that the latency condition – translation of one sentence at a time on a GPU – already translates too fast to be perceived by human users of MT, there is currently no compelling scenario that warrants the deployment of NAR models."
    } ],
    "references" : [ {
      "title" : "Findings of the 2021 conference on machine translation (WMT21)",
      "author" : [ "Morishita", "Masaaki Nagata", "Ajay Nagesh", "Toshiaki Nakazawa", "Matteo Negri", "Santanu Pal", "Allahsera Auguste Tapo", "Marco Turchi", "Valentin Vydrin", "Marcos Zampieri." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Morishita et al\\.,? 2021",
      "shortCiteRegEx" : "Morishita et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei", "Sundaram Ananthanarayanan", "Rishita Anubhai", "Jingliang Bai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Qiang Cheng", "Guoliang Chen" ],
      "venue" : null,
      "citeRegEx" : "Amodei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Amodei et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "ParaCrawl: Web-scale acquisition of parallel corpora",
      "author" : [ "Brian Thompson", "William Waites", "Dion Wiggins", "Jaume Zaragoza." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567, Online. Association",
      "citeRegEx" : "Thompson et al\\.,? 2020",
      "shortCiteRegEx" : "Thompson et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient machine translation with model prun",
      "author" : [ "Maximiliana Behnke", "Nikolay Bogoychev", "Alham Fikri Aji", "Kenneth Heafield", "Graeme Nail", "Qianqian Zhu", "Svetlana Tchistiakova", "Jelmer van der Linde", "Pinzhen Chen", "Sidharth Kashyap", "Roman Grundkiewicz" ],
      "venue" : null,
      "citeRegEx" : "Behnke et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Behnke et al\\.",
      "year" : 2021
    }, {
      "title" : "Findings of the 2014 workshop",
      "author" : [ "Ondřej Bojar", "Christian Buck", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve SaintAmand", "Radu Soricut", "Lucia Specia", "Aleš Tamchyna" ],
      "venue" : null,
      "citeRegEx" : "Bojar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2014
    }, {
      "title" : "Findings of the 2016 conference on machine translation",
      "author" : [ "Névéol", "Mariana Neves", "Martin Popel", "Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri." ],
      "venue" : "Proceedings of the First Conference",
      "citeRegEx" : "Névéol et al\\.,? 2016",
      "shortCiteRegEx" : "Névéol et al\\.",
      "year" : 2016
    }, {
      "title" : "Tagged back-translation",
      "author" : [ "Isaac Caswell", "Ciprian Chelba", "David Grangier." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 53–63, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Caswell et al\\.,? 2019",
      "shortCiteRegEx" : "Caswell et al\\.",
      "year" : 2019
    }, {
      "title" : "The University of Edinburgh’s English-German and English-Hausa",
      "author" : [ "Pinzhen Chen", "Jindřich Helcl", "Ulrich Germann", "Laurie Burchell", "Nikolay Bogoychev", "Antonio Valerio Miceli Barone", "Jonas Waldendorf", "Alexandra Birch", "Kenneth Heafield" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Aligned cross entropy for non-autoregressive machine translation",
      "author" : [ "Marjan Ghazvininejad", "Vladimir Karpukhin", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2020",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2020
    }, {
      "title" : "Mask-predict: Parallel decoding of conditional masked language models",
      "author" : [ "Marjan Ghazvininejad", "Omer Levy", "Yinhan Liu", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2019
    }, {
      "title" : "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning,",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Non-autoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor O.K. Li", "Richard Socher." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Fully nonautoregressive neural machine translation: Tricks of the trade",
      "author" : [ "Jiatao Gu", "Xiang Kong." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 120–133, Online. Association for Computational Lin-",
      "citeRegEx" : "Gu and Kong.,? 2021",
      "shortCiteRegEx" : "Gu and Kong.",
      "year" : 2021
    }, {
      "title" : "Dual conditional cross-entropy filtering of noisy parallel corpora",
      "author" : [ "Marcin Junczys-Dowmunt." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 888–895, Belgium, Brussels. Association for Computational",
      "citeRegEx" : "Junczys.Dowmunt.,? 2018",
      "shortCiteRegEx" : "Junczys.Dowmunt.",
      "year" : 2018
    }, {
      "title" : "Fast decoding in sequence models using discrete latent variables",
      "author" : [ "Lukasz Kaiser", "Samy Bengio", "Aurko Roy", "Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer." ],
      "venue" : "International Conference on Machine Learning, pages 2390–2399.",
      "citeRegEx" : "Kaiser et al\\.,? 2018",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "To ship or not to ship: An extensive evaluation of automatic metrics for machine translation",
      "author" : [ "Tom Kocmi", "Christian Federmann", "Roman Grundkiewicz", "Marcin Junczys-Dowmunt", "Hitokazu Matsushita", "Arul Menezes." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Kocmi et al\\.,? 2021",
      "shortCiteRegEx" : "Kocmi et al\\.",
      "year" : 2021
    }, {
      "title" : "Europarl: A parallel corpus for statistical machine translation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of Machine Translation Summit X: Papers, pages 79–86, Phuket, Thailand.",
      "citeRegEx" : "Koehn.,? 2005",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
      "author" : [ "Jason Lee", "Elman Mansimov", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182,",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "End-toend non-autoregressive neural machine translation with connectionist temporal classification",
      "author" : [ "Jindřich Libovický", "Jindřich Helcl." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3016–",
      "citeRegEx" : "Libovický and Helcl.,? 2018",
      "shortCiteRegEx" : "Libovický and Helcl.",
      "year" : 2018
    }, {
      "title" : "Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics",
      "author" : [ "Nitika Mathur", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Mathur et al\\.,? 2020",
      "shortCiteRegEx" : "Mathur et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "chrF: character n-gram F-score for automatic MT evaluation",
      "author" : [ "Maja Popović." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.",
      "citeRegEx" : "Popović.,? 2015",
      "shortCiteRegEx" : "Popović.",
      "year" : 2015
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Glancing transformer for non-autoregressive neural machine translation",
      "author" : [ "Lihua Qian", "Hao Zhou", "Yu Bao", "Mingxuan Wang", "Lin Qiu", "Weinan Zhang", "Yong Yu", "Lei Li." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qian et al\\.,? 2021",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2021
    }, {
      "title" : "COMET: A neural framework for MT evaluation",
      "author" : [ "Ricardo Rei", "Craig Stewart", "Ana C Farinha", "Alon Lavie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Association",
      "citeRegEx" : "Rei et al\\.,? 2020",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2020
    }, {
      "title" : "Tilde MODEL - multilingual open data for EU languages",
      "author" : [ "Roberts Rozis", "Raivis Skadin" ],
      "venue" : "In Proceedings of the 21st Nordic Conference on Computational Linguistics,",
      "citeRegEx" : "Rozis et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Rozis et al\\.",
      "year" : 2017
    }, {
      "title" : "Non-autoregressive machine translation with latent alignments",
      "author" : [ "Chitwan Saharia", "William Chan", "Saurabh Saxena", "Mohammad Norouzi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Saharia et al\\.,? 2020",
      "shortCiteRegEx" : "Saharia et al\\.",
      "year" : 2020
    }, {
      "title" : "Wikimatrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia",
      "author" : [ "Holger Schwenk", "Vishrav Chaudhary", "Shuo Sun", "Hongyu Gong", "Francisco Guzmán." ],
      "venue" : "arXiv preprint arXiv:1907.05791.",
      "citeRegEx" : "Schwenk et al\\.,? 2019",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86–96,",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast structured decoding for sequence models",
      "author" : [ "Zhiqing Sun", "Zhuohan Li", "Haoqing Wang", "Di He", "Zi Lin", "Zhihong Deng." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 3016– 3026. Curran Associates, Inc.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel data, tools and interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 2214–2218, Istanbul, Turkey. European Language Resources Association",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Non-autoregressive machine translation with auxiliary regularization",
      "author" : [ "Yiren Wang", "Fei Tian", "Di He", "Tao Qin", "ChengXiang Zhai", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5377–5384.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding knowledge distillation in nonautoregressive machine translation",
      "author" : [ "Chunting Zhou", "Jiatao Gu", "Graham Neubig." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Non-autoregressive neural machine translation (NAR NMT, or NAT; Gu et al., 2018; Lee et al., 2018) is an emerging subfield of NMT which focuses on lowering the decoding time complexity by changing the model architecture.",
      "startOffset" : 46,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : "Non-autoregressive neural machine translation (NAR NMT, or NAT; Gu et al., 2018; Lee et al., 2018) is an emerging subfield of NMT which focuses on lowering the decoding time complexity by changing the model architecture.",
      "startOffset" : 46,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "Translation quality is usually evaluated by scoring translations of an unseen test set either using automatic metrics, such as BLEU (Papineni et al., 2002), ChrF (Popović, 2015) or COMET (Rei et al.",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 24,
      "context" : ", 2002), ChrF (Popović, 2015) or COMET (Rei et al.",
      "startOffset" : 14,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : ", 2002), ChrF (Popović, 2015) or COMET (Rei et al., 2020), or using human evaluation.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "Automatic evaluation of translation quality however remains an open research problem (Mathur et al., 2020; Kocmi et al., 2021).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "Automatic evaluation of translation quality however remains an open research problem (Mathur et al., 2020; Kocmi et al., 2021).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "poral classification (CTC), an approach previously shown to be effective (Libovický and Helcl, 2018; Gu and Kong, 2021; Ghazvininejad et al., 2020).",
      "startOffset" : 73,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "poral classification (CTC), an approach previously shown to be effective (Libovický and Helcl, 2018; Gu and Kong, 2021; Ghazvininejad et al., 2020).",
      "startOffset" : 73,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "poral classification (CTC), an approach previously shown to be effective (Libovický and Helcl, 2018; Gu and Kong, 2021; Ghazvininejad et al., 2020).",
      "startOffset" : 73,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "sequence-level knowledge distillation (Kim and Rush, 2016).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "The current state-of-the-art NMT models are autoregressive – the output distributions are conditioned on the previously generated tokens (Bahdanau et al., 2016; Vaswani et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 182
    }, {
      "referenceID" : 34,
      "context" : "The current state-of-the-art NMT models are autoregressive – the output distributions are conditioned on the previously generated tokens (Bahdanau et al., 2016; Vaswani et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "sentence “Thank you” with its two equally probable German translations “Danke schön” and “Vielen Dank” (Gu et al., 2018).",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "Knowledge distillation (Kim and Rush, 2016) has been successfully employed to reduce the negative influence of the multimodality problem in NAR models (Gu et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "Knowledge distillation (Kim and Rush, 2016) has been successfully employed to reduce the negative influence of the multimodality problem in NAR models (Gu et al., 2018; Saharia et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 190
    }, {
      "referenceID" : 29,
      "context" : "Knowledge distillation (Kim and Rush, 2016) has been successfully employed to reduce the negative influence of the multimodality problem in NAR models (Gu et al., 2018; Saharia et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 190
    }, {
      "referenceID" : 36,
      "context" : "Synthetic data tends to be less diverse than authentic texts, therefore the number of equally likely translation candidates gets smaller (Zhou et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 20,
      "context" : "A number of techniques have been proposed for training NAR models, including iterative methods (Lee et al., 2018; Ghazvininejad et al., 2019), auxiliary training objectives (Wang et al.",
      "startOffset" : 95,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "A number of techniques have been proposed for training NAR models, including iterative methods (Lee et al., 2018; Ghazvininejad et al., 2019), auxiliary training objectives (Wang et al.",
      "startOffset" : 95,
      "endOffset" : 141
    }, {
      "referenceID" : 35,
      "context" : ", 2019), auxiliary training objectives (Wang et al., 2019; Qian et al., 2021), or latent variables (Gu et al.",
      "startOffset" : 39,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : ", 2019), auxiliary training objectives (Wang et al., 2019; Qian et al., 2021), or latent variables (Gu et al.",
      "startOffset" : 39,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "the strict one-to-one alignment between the model outputs and the ground-truth target sequence include aligned cross-entropy (Ghazvininejad et al., 2020) and connectionist temporal classification (Libovický and Helcl, 2018).",
      "startOffset" : 125,
      "endOffset" : 153
    }, {
      "referenceID" : 21,
      "context" : ", 2020) and connectionist temporal classification (Libovický and Helcl, 2018).",
      "startOffset" : 50,
      "endOffset" : 77
    }, {
      "referenceID" : 34,
      "context" : "The model extends the Transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "Connectionist temporal classification (Graves et al., 2006) is a dynamic algorithm that efficiently computes the standard cross-entropy",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "We choose the CTC-based architecture for our models because it has been previously shown to be effective for NAR NMT (Gu and Kong, 2021; Saharia et al., 2020) and performs well in the context of the non-autoregressive research.",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 29,
      "context" : "We choose the CTC-based architecture for our models because it has been previously shown to be effective for NAR NMT (Gu and Kong, 2021; Saharia et al., 2020) and performs well in the context of the non-autoregressive research.",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "These language pairs, along with the widely used test sets – WMT 14 (Bojar et al., 2014) for En-De and WMT 16 (Bojar et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "The base variant of the Transformer model is used almost exclusively (Gu et al., 2018; Gu and Kong, 2021; Lee et al., 2018; Ghazvininejad et al., 2020; Qian et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "The base variant of the Transformer model is used almost exclusively (Gu et al., 2018; Gu and Kong, 2021; Lee et al., 2018; Ghazvininejad et al., 2020; Qian et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "The base variant of the Transformer model is used almost exclusively (Gu et al., 2018; Gu and Kong, 2021; Lee et al., 2018; Ghazvininejad et al., 2020; Qian et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "The base variant of the Transformer model is used almost exclusively (Gu et al., 2018; Gu and Kong, 2021; Lee et al., 2018; Ghazvininejad et al., 2020; Qian et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 170
    }, {
      "referenceID" : 26,
      "context" : "The base variant of the Transformer model is used almost exclusively (Gu et al., 2018; Gu and Kong, 2021; Lee et al., 2018; Ghazvininejad et al., 2020; Qian et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "In light of recent research casting further doubt on the reliability of BLEU as a measure of translation quality (Kocmi et al., 2021), we argue that this is insufficient.",
      "startOffset" : 113,
      "endOffset" : 133
    }, {
      "referenceID" : 26,
      "context" : "Sometimes, this information is missing from the published results (Qian et al., 2021).",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "For the CTC loss computation, we use the warp-ctc library (Amodei et al., 2016).",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "For the parallel data, we used Europarl (Koehn, 2005), the RAPID corpus (Rozis and Skadin, š, 2017), and the News Commentary corpus from OPUS (Tiedemann, 2012).",
      "startOffset" : 40,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : "For the parallel data, we used Europarl (Koehn, 2005), the RAPID corpus (Rozis and Skadin, š, 2017), and the News Commentary corpus from OPUS (Tiedemann, 2012).",
      "startOffset" : 142,
      "endOffset" : 159
    }, {
      "referenceID" : 30,
      "context" : ", 2020), Common Crawl1, WikiMatrix (Schwenk et al., 2019), and Wikititles2.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "entropy filtering on the noisy part of the parallel data (Junczys-Dowmunt, 2018).",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 31,
      "context" : "For backtranslation (Sennrich et al., 2016), we train four Transformer big models on the cleaned parallel data in both directions.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "We add a special symbol to the generated sentences to help the models differentiate between synthetic and authentic source language data (Caswell et al., 2019).",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : "For training, we use the Adam optimizer (Kingma and Ba, 2014) with β1, β2 and ε set to 0.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "Similar to creating the backtranslations, we use the four teacher models in an ensemble to create the knowledge-distilled data (Kim and Rush, 2016).",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "The research on nonautoregressive models uses the BLEU score (Papineni et al., 2002) measured on the WMT 14 test set (Bojar et al.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : ", 2002) measured on the WMT 14 test set (Bojar et al., 2014) as a standard benchmark for evaluating translation quality.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "We use Sacrebleu (Post, 2018) as the implementation of the BLEU score metric.",
      "startOffset" : 17,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "up to date with the current state-of-the-art translation systems, we also evaluate our models using COMET (Rei et al., 2020)5 and BLEU scores on the recent WMT 21 test set.",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "In Table 5, we present a comparison on the million sentence test set with “Edinburgh base”, one of the leading submissions in the WMT 21 efficiency task (Behnke et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 174
    } ],
    "year" : 0,
    "abstractText" : "Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used efficiency approaches. We run experiments with a connectionist-temporalclassification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are nearly always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.",
    "creator" : null
  }
}
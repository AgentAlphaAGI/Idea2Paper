{
  "name" : "ARR_2022_250_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probing for Predicate Argument Structures in Pretrained Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Semantic Role Labeling (SRL) is often defined informally as the task of automatically answering the question “Who did What to Whom, Where, When and How?” (Màrquez et al., 2008) and is, therefore, thought to be a fundamental step towards Natural Language Understanding (Navigli, 2018). Over the past few years, SRL has started to gain renewed traction, thanks mainly to the effectiveness and wide availability of modern pretrained language models (PLMs) such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and BART (Lewis et al., 2020). Current approaches have, indeed, attained impressive results on standard evaluation benchmarks for dependency- and span-based, multilingual and cross-lingual SRL (He et al., 2019; Li et al., 2019; Cai and Lapata, 2020; Conia and Navigli, 2020; Blloshmi et al., 2021).\nDespite the remarkable benefits provided by the rich contextualized word representations coming from PLMs, the novelties introduced in recent state-of-the-art models for SRL revolve primarily around developing complexities on top of such word representations rather than investigating what happens inside a PLM. For example, the stateof-the-art SRL systems of He et al. (2019) and Conia and Navigli (2020) take advantage only of BERT’s uppermost hidden layers to build their input word representations. However, the revolution that PLMs sparked in an uncountable number of areas in Natural Language Processing (NLP) motivated researchers in the community to investigate the inner workings of such models with the aim of understanding how, where and to what extent they encode information about specific NLP tasks, revealing that different layers encode significantly different features (Tenney et al., 2019; Vulić et al., 2020). Most notably, Tenney et al. (2019) have demonstrated empirically that BERT “rediscovers” the classical NLP pipeline, showing that the lower layers encode mostly lexical-level information while upper layers favor sentence-level information.\nAlthough recent analyses have already provided important insights on which layers of a PLM are more relevant for SRL and how their relative importance is affected by the linguistic formalism of choice (Kuznetsov and Gurevych, 2020), not only do these analyses treat SRL as an atomic task but also do not look into taking advantage of such insights to improve current state-of-the-art SRL systems. Indeed, the SRL pipeline is usually divided into four main steps: predicate identification and disambiguation, and argument identification and classification.\nIn this paper, instead, we take an in-depth look at how predicate senses and their predicate argument structures (PASs) are encoded across different layers of different PLMs, not only providing new\ninsights on the capability of these models to capture complex linguistic features, but also showing the benefits of embedding such features into SRL systems to improve their performance.\nOur contributions can be summarized as follows:\n• We probe PLMs for PASs: do PLMs encode the argument structure of a predicate in its contextual representation?\n• We show that, even though a PAS is defined according to a predicate sense, senses and argument structures are encoded at different layers in PLMs;\n• We empirically demonstrate that verbal and nominal PASs are represented differently across the layers of a PLM;\n• Current SRL systems do not discriminate between nominal and verbal PASs: we prove that, although there exists some degree of transferability between the two, a SRL system benefits from treating them separately;\n• We find that PAS information is encoded similarly across two very different languages, English and Chinese, in multilingual PLMs;\n• We corroborate our findings by proposing a simple approach to integrate predicateargument structure knowledge into an SRL architecture, attaining improved results on standard gold benchmarks.\nWe hope that our work will contribute both to the understanding of the inner workings of modern pretrained language models and to the development of more effective SRL systems. We release our code and model checkpoints at omitted.link."
    }, {
      "heading" : "2 Related Work",
      "text" : "Probing pretrained language models. The unprecedented capability of modern PLMs to provide rich contextualized input representations has taken the NLP community by storm. Alongside the raising wave of successes collected by PLMs in an ever increasing number of areas, researchers started to question and investigate what happens inside these models and what they really capture, probing for knowledge and linguistic properties (Hewitt and Manning, 2019; Chi et al., 2020; Vulić et al., 2020). This body of work quickly attracted increasing attention and grew to become a field of study with a\nname of its own: BERTology (Rogers et al., 2020). Probing a PLM usually consists in defining a very precise task, e.g. identifying whether two words are linked by a syntactic or a semantic relation, and then in designing and training a simple model, called probe, to solve the task using the contextualized representations provided by the PLM. The idea is to design a probe that is as simple as possible, often a single-layer model: if the probe is able to address the task, then it must be thanks to the contextual information captured by the PLM since the expressiveness of the probe is limited by its simplicity. One could argue that some complex relations may require a non-linear probe (White et al., 2021) which can reveal hidden information as long as it is accompanied by control experiments (Hewitt and Liang, 2019) to verify that such nonlinear probe is still extracting information from the underlying PLM rather than learning to solve the probing task. Over the past few years, these probing techniques have been used to great effect and revealed that PLMs have been “rediscovering” the classical NLP pipeline (Tenney et al., 2019), and that they often encode distances between syntactic constituents (Hewitt and Liang, 2019), lexical relations (Vulić et al., 2020) and morphology (Chi et al., 2020), inter alia.\nProbing techniques for SRL. As in several other fields of NLP, recent studies tried to shed some light on how, where and to what extent PLMs encode information relevant to SRL. Among others, Tenney et al. (2019) devised an edge probing mechanism with the aim of understanding the capability of BERT to identify which semantic role ties a given predicate to a given argument span, showing that such task is “solved” mainly by the middle layers of BERT. Toshniwal et al. (2020) proposed and compared several techniques to better combine the contextualized representations of a PLM, finding that applying max pooling or performing a weighted average are two robust strategies for SRL. More recently, Kuznetsov and Gurevych (2020) designed a probe to analyze how different linguistic ontologies – essential to the task to define predicate senses and semantic roles – require features that encoded at different layers of a PLM. In this paper, we follow the line of research laid out by the above mentioned work, probing PLMs with the objective of understanding where and to what extent they encode a predicate argument structure into the contextualized representation of a predicate.\nRecent advances in SRL. Thanks to their effectiveness, PLMs are now the de facto input representation method in SRL (He et al., 2019; Li et al., 2019; Conia and Navigli, 2020; Blloshmi et al., 2021). Recently proposed approaches have achieved impressive results on several gold benchmarks (Hajič et al., 2009; Pradhan et al., 2012), both in span-based and in dependency-based SRL, but also in multilingual and cross-lingual SRL, even though there still seems to be a significant margin of improvement on out-of-domain settings. The innovations put forward by such approaches, however, have mainly focused on architectural novelties built on top of PLMs: Cai et al. (2018) proposed the first end-to-end architecture; He et al. (2019) and Cai and Lapata (2019) successfully exploited syntax in multilingual SRL; Marcheggiani and Titov (2020) took advantage of GCNs to capture distant semantic relations; Blloshmi et al. (2021) and Paolini et al. (2021) tackled the task as a sequence generation problem; Conia et al. (2021) introduced a model to perform SRL across heterogeneous linguistic inventories. If we look back at past work, it is easy to realize that we miss an indepth study that provides a look inside PLMs and a hint at how to better exploit them in future SRL systems."
    }, {
      "heading" : "3 Probing for Predicate Senses and Their Predicate-Argument Structures",
      "text" : "As mentioned above, some studies have already investigated how semantic knowledge is distributed among the inner layers of current PLMs, finding that information useful for SRL is mainly stored in their middle layers (Tenney et al., 2019). However, such studies have considered SRL as an atomic task, while instead the SRL pipeline can be thought of being composed of four different subtasks:\n1. Predicate identification consists in identifying all those words or multi-word expressions that denote an action or an event in the input sentence;\n2. Predicate sense disambiguation requires choosing the most appropriate sense or frame for each predicate identified, as the same predicate may acquire different senses or define different semantic scenarios depending on the context;\n3. Argument identification consists in selecting the parts of the input text that are “se-\nmantically” linked to an identified and disambiguated predicate;\n4. Argument classification, finally, is the task of assigning a relation, i.e. a semantic role, to each predicate-argument pair.\nFor our study, it is important to note that, in many of the most popular ontologies for SRL, predicate senses or frames are often tightly coupled to their possible semantic roles. In other words, the set of possible semantic roles that can be linked to a predicate p is defined according to the sense or frame of p. Hereafter, we refer to such set of possible semantic roles given a predicate p as the roleset of p. For example, the predicate love as in “He loved everything about her” belongs to the FrameNet (Baker et al., 1998) frame experiencer_focused_emotion which defines a roleset composed of {Experiencer, Content, . . ., Degree}. The same predicate sense has different rolesets in other ontologies, for example {A0 (lover), A1 (loved)} in the English PropBank (Palmer et al., 2005) and {Experiencer, Stimulus, . . ., Cause} in VerbAtlas (Di Fabio et al., 2019)."
    }, {
      "heading" : "3.1 Predicate Senses and Their Rolesets",
      "text" : "Since rolesets are often defined according predicate senses, it is interesting to investigate whether current pretrained language models store important features about senses and rolesets in their hidden layers. To this aim, we formulate two simple probing tasks:\n• Sense probing: which consists in predicting the sense s of a predicate p from the contextual vector representation xp of p, where xp is obtained from a pretrained language model.\n• Roleset probing: which consists in predicting the semantic roles {r1, r2, . . . , rn} that appear linked to a predicate p from its contextual representation xp, where xp is obtained from a pretrained language model.\nFor the choice of xp, we compare four different options:\n• Random: initializing the weights of the language model at random provides a simple control baseline to attest the ability of a probe to “learn the probing task”, i.e. learning to associate random inputs to correct labels;\n• Static: xp is the input embedding of the pretrained language model corresponding to p, e.g. the non-contextual representation before the Transformer layers in BERT.1\n• Top-4: xp is the concatenation of the topmost four hidden layers of the language model: this is the configuration used in recently proposed approaches (Conia and Navigli, 2020);\n• W-Avg: xp is the weighted average of all the hidden layers of the language model, where the weights are learned during training.\nFor each probing task, we train two simple probes, a linear classifier and a non-linear classifier23, on the verbal predicate instances of the English training datasets provided as part of the CoNLL-2009 shared task for dependency-based SRL (Hajič et al., 2009).\nResults on sense probing. Table 1 reports the results on predicate sense disambiguation of our linear and non-linear probes when using different input representations xp, namely, Static, Random, Last-4 and W-Avg, of an input predicate p in context. The Random baseline is able to disambiguate well (84.8% using BERT), which is unsurprising since CoNLL-2009 is tagged with PropBank labels and most of the predicates are annotated with their\n1In case of a predicate composed of multiple subtokens, xp is the average of the vector representations of its subtokens.\n2We use the Swish activation function for the non-linear classifier.\n3We train each probe for 20 epochs using Adam () as the optimizer with a learning rate of 1e-3.\nfirst sense (e.g. buy.01, sell.01). Interestingly, static representations from all four language models do not contain much more information about predicate senses than random representations. Using the topmost four hidden layers, instead, provides a substantial improvement over static representations for all language models (e.g. +6% for BERT), lending credibility to the fact that context is key for the disambiguation process. Most notably, the best representation for the sense probing task is consistently obtained by performing a weighted average of all the hidden layers of the language model, showing that important predicate sense information is stored not only in the topmost hidden layers and, therefore, hinting at the fact that state-of-the-art architectures such as that of He et al. (2019) and Conia and Navigli (2020) are not exploiting pretrained language models to their fullest.\nResults on roleset probing. Table 2 reports the results on roleset identification of our linear and non-linear probes when using different input representations xp, namely, Static, Random, Last-4 and W-Avg, of an input predicate p in context. As it was the case for sense probing, our simple Random baseline is able to identify the correct roleset for a predicate in context with satisfying performance (72.8% using BERT). Indeed, most predicates often have at least one argument tagged with either A0 or A1, which in PropBank usually correspond to agentive and patientive proto-roles, respectively; the Random probe merely learns to bias its predictions towards these very common semantic roles. Despite the fact that the roleset probing task is more difficult than the sense probing one, we can observe a similar trend in the results: the Top-4 probe is substantially better than the Static probe, but\nRoBERTa\nBERT\nW-Avg consistently outperforms Top-4, strongly suggesting future approaches to use them to fully take advantage of the knowledge encoded in PLMs. We stress that not taking advantage of all inner layers of a PLM is an illogical choice since the cost of computing a weighted average of their hidden representations is negligible with respect to the overall computational cost of a Transformer-based architecture."
    }, {
      "heading" : "On the correlation between senses and rolesets.",
      "text" : "Thus far, we have empirically seen that PLMs encode important features about predicate senses and their rolesets across all their hidden layers, not just the topmost ones often used in the literature by current models for SRL. However, one may wonder about how such features are distributed across these hidden layers. As we have already discussed above, predicate senses and their rolesets are tightly coupled: do PLMs distribute sense and roleset features similarly over their inner layers?\nTo answer this question, we resort to the W-Avg probe we introduced above. Indeed, its peculiarity is that it learns to assign a different weight to each hidden layer of a PLM: in order to minimize the\ntraining loss, the W-Avg probe will assign a larger weight to those layers that are most beneficial, i.e. to those layers that express features that are more relevant for the probing task. Therefore, we extract the weights learned for the two probing tasks we are considering – sense and roleset – and compare these learned weights, as shown in Figure 1 (top, blue charts). Interestingly, and perhaps surprisingly, the W-Avg probe learns a different weight distribution for the two probing tasks, even though rolesets are often defined depending on predicate senses in many of the most popular ontologies for SRL. We can observe that predicate sense features are encoded more uniformly across the hidden layers of BERT or, equivalently, that the probe assigns similar weights to each hidden layer, slightly preferring the topmost ones (Figure 1, top-left). However, this is not the case for the roleset probing task, in which the probe mostly relies on the hidden layers going from the 6th to the 10th, almost disregarding the bottom and top ones. Furthermore, we can observe the same negative correlation within the distributions of the layer weights learned for senses and rolesets when using RoBERTa, albeit the divergence is slightly less accentuated (Figure\n1, top-right).\nVerbal and Nominal Predicates. One aspect that is often overlooked when designing and proposing novel architectures for SRL is that not all predicates are verbs. In English, it is easy to find examples of nouns that evoke or imply a predication, such as producer, driver, and writer. Most nominal predicates are “verb-derived” or “deverbal” as their roleset is derived from their corresponding verbal predicates. This is why, perhaps, current state-of-the-art approaches do not distinguish between verbal and nominal predicates.4 However,\n4Languages other than English also possess, sometimes in extensive quantities, predicates that are neither verbal nor nominal. For example, Japanese features adjectival predicates.\nnominal predicates also possess peculiarities that do not appear in their verbal counterparts; for example, a nominal predicate can be its own argument, e.g., writer is the agent itself of the action write.\nWe take this chance to investigate how nominal senses and their rolesets are encoded by PLMs in their inner layers. In particular, we train a WAvg probe on the sense and roleset probing tasks, focusing only on the nominal predicate instances in CoNLL-2009. Figure 1 (bottom, green charts) shows the weights learned for the sense and roleset probing tasks when using BERT (bottom-left) and RoBERTa (bottom-right): we can immediately observe that, differently from verbal predicates, the weight distributions learned for nominal senses and their rolesets follow the same trend in both PLMs. In other words, despite the fact that most nominal predicates are verb-derived, their information is encoded dissimilarly and distributed across different layers with respect to verbal predicates.\nWe confirm our hunch by evaluating the ability of a W-Avg probe trained on roleset identification for verbal predicates to also perform roleset identification for nominal predicates, and vice versa. Although, from a first glance at the results reported in Table 3, our simple model seems to be able to\nperform nominal roleset identification despite being trained on verbal rolesets, the performance is actually worse than a control probe. In general, our analysis provides an empirical explanation on why recent approaches for nominal SRL adapted from verbal SRL are still struggling to achieve satisfying performance, despite initial promising results (Klein et al., 2020; Zhao and Titov, 2020).\nSenses and rolesets across languages. We conclude our analysis on predicate senses and their rolesets with another important finding: multilingual PLMs encode both predicate sense and roleset information at similar layers across two very different languages, English and Chinese. In order to support this statement, we train an W-Avg probe on both sense disambiguation and roleset identification, first on the English training split of CoNLL-2009 and then on the Chinese training split of CoNLL-2009.\nFigure 2 shows the distribution of the learned weights for each hidden layer of two language models, multilingual-BERT (left) and XLM-RoBERTa (right). In particular, we observe that the probe learns to almost completely discard the first five layers of multilingual-BERT for roleset identification in both English (top) and Chinese (bottom), while assigning similar weights across English and Chinese to the other hidden layers, with layer 8 being particularly important in both languages. Overall, Figure 2 supports the evidence that both multilingual-BERT and XLM-RoBERTa encode the same type of “semantic knowledge” at roughly the same hidden layers across languages, indicating a possible direction for future work in cross-lingual transfer learning for SRL."
    }, {
      "heading" : "4 Integrating Predicate-Argument Structure Knowledge",
      "text" : "Now that we have provided an in-depth look at how sense and roleset information is encoded at different inner layers of current PLMs, highlighted the differences in how PLMs encode verbal and nominal predicates, and revealed that multilingual PLMs capture semantic knowledge at similar layers across two diverse languages, one may wonder how we can take advantage of what we have learned so far in a practical setting. In this Section, we study how we can improve a modern system for end-to-end SRL by integrating sense and roleset knowledge into its architecture."
    }, {
      "heading" : "4.1 Model Description",
      "text" : "In what follows, we briefly describe the architecture of our baseline model which is based on that proposed by Conia and Navigli (2020). Notice that, even though we refer to this model as our baseline, its end-to-end architecture rivals current state-ofthe-art approaches.\nGiven an input sentence w, the model computes a contextual representation xi for each word wi in w by concatenating the representations obtained from the four topmost layers of a pretrained language model. These contextual word representations are then processed by a stack of “fully connected” BiLSTM layers in which the input to the i-th BiLSTM layer is the concatenation of the inputs of all previous BiLSTM layers in the stack, obtaining a sequence h of refined encodings. These encodings h are made “predicate-aware” by concatenating each hi of wi to the representation hp of each predicate p in the sentence, and finally processed by another stack of fully-connected BiLSTMs, resulting in a sequence a of argument encodings. We refer to Conia and Navigli (2020) for further details about the architecture of our baseline model.\nEnhancing the SRL model. Based on our observations and analyses in the Sections above, we put forward three simple enhancements to our strong baseline model:\n• Representing words using a weighted average of the inner layers of the underlying language model, since semantic features important for the task are scattered across all layers;\n• Using two different sets of weights to compute different weighted averages for predicate\nAM-EXT AM-MNR/AM-TMP AM-EXT/AM-TMPAM-MNR AM-TMP rolesets AM-EXT/AM-MNR\nsenses and predicate arguments, as semantic features important for the two tasks are distributed differently across the inner layers of the underlying PLM;\n• Adding a secondary task to predict rolesets from a predicate representation hp in a multitask learning fashion.\nResults on SRL. Table 4 compares the results obtained on the verbal predicate instances in the standard gold benchmark of CoNLL-2009 for dependency-based SRL.56 Each contribution provides an improvement over the previous one both when using BERT-base-cased and BERT-largecased, the latter being one of the most used pretrained language models to achieve state-of-the-art results on the task. In general, not only did our analysis shed light on interesting properties of current PLMs through the lens of predicate senses and their roleset, but also provided practical hints on how to better exploit such properties in SRL.\nAnalysis. Finally, we provide a look at what happens when our model is informed about predicate senses and their rolesets at training time. Figure 3 provides a bi-dimensional visualization of how predicate representations xp changes as we inject more inductive bias towards predicate-argument information in three steps: i) the baseline model,\n5We trained our model for 30 epochs using Adam with an initial learning rate of 1e-3, leaving all parameters of the underlying language model frozen and using the parameter values used in the original paper by Conia and Navigli (2020).\n6Scores computed using the official CoNLL-2009 scorer provided during the shared task.\nwhich is unaware of predicate-argument information and, therefore, does not show any significant clustering according to different rolesets; ii) the model when it can use different weighted averages to compute representations for predicate senses and their arguments; and iii) the model when it is explicitly tasked with the secondary learning objective of learning to identify the roleset of each predicate."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we probed PLMs for PASs: differently from the past, we dissected SRL into its core subtasks and analysed how PLMs encode predicateargument structure information such as predicate senses and their rolesets. In our analysis, we observed that, despite the intrinsic connection that exists in several popular SRL inventories between predicate senses and their roleset, different PLMs encode their features across significantly different layers. Not only that, we discover that verbal and nominal predicates and their PASs are represented differently, making verbal-to-nominal SRL transfer far from trivial, and providing an empirical explanation on why previous attempts in this direction have struggled to obtain satisfying results. Furthermore, our analysis uncovers that current multilingual language models encode PASs similarly across two very different languages, English and Chinese.\nFinally, in contrast to previous work on probing, we put together what we learned and found a practical application of our findings by devising simple yet effective techniques for the integration of predicate-argument structure knowledge into a state-of-the-art end-to-end architecture for SRL."
    } ],
    "references" : [ {
      "title" : "The berkeley framenet project",
      "author" : [ "Collin F. Baker", "Charles J. Fillmore", "John B. Lowe." ],
      "venue" : "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING-ACL ’98, Au-",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Generating senses and roles: An end-to-end model for dependency- and spanbased semantic role labeling",
      "author" : [ "Rexhina Blloshmi", "Simone Conia", "Rocco Tripodi", "Roberto Navigli." ],
      "venue" : "Proceedings of the Thirtieth International Joint Conference on Artifi-",
      "citeRegEx" : "Blloshmi et al\\.,? 2021",
      "shortCiteRegEx" : "Blloshmi et al\\.",
      "year" : 2021
    }, {
      "title" : "A full end-to-end semantic role labeler, syntacticagnostic over syntactic-aware",
      "author" : [ "Jiaxun Cai", "Shexia He", "Zuchao Li", "Hai Zhao" ],
      "venue" : "In Proceedings of the 27th International Conference on Computational Linguistics,",
      "citeRegEx" : "Cai et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2018
    }, {
      "title" : "Syntax-aware semantic role labeling without parsing",
      "author" : [ "Rui Cai", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:343–356.",
      "citeRegEx" : "Cai and Lapata.,? 2019",
      "shortCiteRegEx" : "Cai and Lapata.",
      "year" : 2019
    }, {
      "title" : "Alignment-free cross-lingual semantic role labeling",
      "author" : [ "Rui Cai", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3883–3894, Online. Association for Computational",
      "citeRegEx" : "Cai and Lapata.,? 2020",
      "shortCiteRegEx" : "Cai and Lapata.",
      "year" : 2020
    }, {
      "title" : "Finding universal grammatical relations in multilingual BERT",
      "author" : [ "Ethan A. Chi", "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564–5577, Online. As-",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "Unifying cross-lingual semantic role labeling with heterogeneous linguistic resources",
      "author" : [ "Simone Conia", "Andrea Bacciu", "Roberto Navigli." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Conia et al\\.,? 2021",
      "shortCiteRegEx" : "Conia et al\\.",
      "year" : 2021
    }, {
      "title" : "Bridging the gap in multilingual semantic role labeling: a language-agnostic approach",
      "author" : [ "Simone Conia", "Roberto Navigli." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 1396–1410, Barcelona,",
      "citeRegEx" : "Conia and Navigli.,? 2020",
      "shortCiteRegEx" : "Conia and Navigli.",
      "year" : 2020
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "VerbAtlas: a novel large-scale verbal semantic resource and its application to semantic role labeling",
      "author" : [ "Andrea Di Fabio", "Simone Conia", "Roberto Navigli." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Fabio et al\\.,? 2019",
      "shortCiteRegEx" : "Fabio et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntaxaware multilingual semantic role labeling",
      "author" : [ "Shexia He", "Zuchao Li", "Hai Zhao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Designing and interpreting probes with control tasks",
      "author" : [ "John Hewitt", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Hewitt and Liang.,? 2019",
      "shortCiteRegEx" : "Hewitt and Liang.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "QANom: Question-answer driven SRL for nominalizations",
      "author" : [ "Ayal Klein", "Jonathan Mamou", "Valentina Pyatkin", "Daniela Stepanov", "Hangfeng He", "Dan Roth", "Luke Zettlemoyer", "Ido Dagan." ],
      "venue" : "Proceedings of the 28th International Conference",
      "citeRegEx" : "Klein et al\\.,? 2020",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2020
    }, {
      "title" : "A matter of framing: The impact of linguistic formalism on probing results",
      "author" : [ "Ilia Kuznetsov", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 171–182, Online. Association",
      "citeRegEx" : "Kuznetsov and Gurevych.,? 2020",
      "shortCiteRegEx" : "Kuznetsov and Gurevych.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Dependency or span, end-to-end uniform semantic role labeling",
      "author" : [ "Zuchao Li", "Shexia He", "Hai Zhao", "Yiqing Zhang", "Zhuosheng Zhang", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph convolutions over constituent trees for syntax-aware semantic role labeling",
      "author" : [ "Diego Marcheggiani", "Ivan Titov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3915–3928, On-",
      "citeRegEx" : "Marcheggiani and Titov.,? 2020",
      "shortCiteRegEx" : "Marcheggiani and Titov.",
      "year" : 2020
    }, {
      "title" : "Special issue introduction: Semantic role labeling: An introduction to the special issue",
      "author" : [ "Lluís Màrquez", "Xavier Carreras", "Kenneth C. Litkowski", "Suzanne Stevenson." ],
      "venue" : "Computational Linguistics, 34(2):145–159.",
      "citeRegEx" : "Màrquez et al\\.,? 2008",
      "shortCiteRegEx" : "Màrquez et al\\.",
      "year" : 2008
    }, {
      "title" : "Natural Language Understanding: Instructions for (present and future) use",
      "author" : [ "Roberto Navigli." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 5697–",
      "citeRegEx" : "Navigli.,? 2018",
      "shortCiteRegEx" : "Navigli.",
      "year" : 2018
    }, {
      "title" : "The proposition bank: An annotated corpus of semantic roles",
      "author" : [ "Martha Palmer", "Paul R. Kingsbury", "Daniel Gildea." ],
      "venue" : "Comput. Linguistics, 31(1):71–106.",
      "citeRegEx" : "Palmer et al\\.,? 2005",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2005
    }, {
      "title" : "Structured prediction as translation between augmented natural languages",
      "author" : [ "Giovanni Paolini", "Ben Athiwaratkun", "Jason Krone", "Jie Ma", "Alessandro Achille", "Rishita Anubhai", "Cícero Nogueira dos Santos", "Bing Xiang", "Stefano Soatto." ],
      "venue" : "9th",
      "citeRegEx" : "Paolini et al\\.,? 2021",
      "shortCiteRegEx" : "Paolini et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages",
      "citeRegEx" : "Zettlemoyer.,? 2018",
      "shortCiteRegEx" : "Zettlemoyer.",
      "year" : 2018
    }, {
      "title" : "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang." ],
      "venue" : "Joint Conference on EMNLP and CoNLL - Shared Task, pages",
      "citeRegEx" : "Pradhan et al\\.,? 2012",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2012
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "A cross-task analysis of text span representations",
      "author" : [ "Shubham Toshniwal", "Haoyue Shi", "Bowen Shi", "Lingyu Gao", "Karen Livescu", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 166–176, Online. Associa-",
      "citeRegEx" : "Toshniwal et al\\.,? 2020",
      "shortCiteRegEx" : "Toshniwal et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing pretrained language models for lexical semantics",
      "author" : [ "Ivan Vulić", "Edoardo Maria Ponti", "Robert Litschko", "Goran Glavaš", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Vulić et al\\.,? 2020",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "A non-linear structural probe",
      "author" : [ "Jennifer C. White", "Tiago Pimentel", "Naomi Saphra", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "White et al\\.,? 2021",
      "shortCiteRegEx" : "White et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised transfer of semantic role models from verbal to nominal domain",
      "author" : [ "Yanpeng Zhao", "Ivan Titov." ],
      "venue" : "CoRR, abs/2005.00278. 10",
      "citeRegEx" : "Zhao and Titov.,? 2020",
      "shortCiteRegEx" : "Zhao and Titov.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Semantic Role Labeling (SRL) is often defined informally as the task of automatically answering the question “Who did What to Whom, Where, When and How?” (Màrquez et al., 2008) and is, therefore, thought to be a fundamental step towards Natural Language Understanding (Navigli, 2018).",
      "startOffset" : 154,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : ", 2008) and is, therefore, thought to be a fundamental step towards Natural Language Understanding (Navigli, 2018).",
      "startOffset" : 99,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : ", 2018), BERT (Devlin et al., 2019) and BART (Lewis et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "Current approaches have, indeed, attained impressive results on standard evaluation benchmarks for dependency- and span-based, multilingual and cross-lingual SRL (He et al., 2019; Li et al., 2019; Cai and Lapata, 2020; Conia and Navigli, 2020; Blloshmi et al., 2021).",
      "startOffset" : 162,
      "endOffset" : 266
    }, {
      "referenceID" : 16,
      "context" : "Current approaches have, indeed, attained impressive results on standard evaluation benchmarks for dependency- and span-based, multilingual and cross-lingual SRL (He et al., 2019; Li et al., 2019; Cai and Lapata, 2020; Conia and Navigli, 2020; Blloshmi et al., 2021).",
      "startOffset" : 162,
      "endOffset" : 266
    }, {
      "referenceID" : 4,
      "context" : "Current approaches have, indeed, attained impressive results on standard evaluation benchmarks for dependency- and span-based, multilingual and cross-lingual SRL (He et al., 2019; Li et al., 2019; Cai and Lapata, 2020; Conia and Navigli, 2020; Blloshmi et al., 2021).",
      "startOffset" : 162,
      "endOffset" : 266
    }, {
      "referenceID" : 7,
      "context" : "Current approaches have, indeed, attained impressive results on standard evaluation benchmarks for dependency- and span-based, multilingual and cross-lingual SRL (He et al., 2019; Li et al., 2019; Cai and Lapata, 2020; Conia and Navigli, 2020; Blloshmi et al., 2021).",
      "startOffset" : 162,
      "endOffset" : 266
    }, {
      "referenceID" : 1,
      "context" : "Current approaches have, indeed, attained impressive results on standard evaluation benchmarks for dependency- and span-based, multilingual and cross-lingual SRL (He et al., 2019; Li et al., 2019; Cai and Lapata, 2020; Conia and Navigli, 2020; Blloshmi et al., 2021).",
      "startOffset" : 162,
      "endOffset" : 266
    }, {
      "referenceID" : 25,
      "context" : "aim of understanding how, where and to what extent they encode information about specific NLP tasks, revealing that different layers encode significantly different features (Tenney et al., 2019; Vulić et al., 2020).",
      "startOffset" : 173,
      "endOffset" : 214
    }, {
      "referenceID" : 27,
      "context" : "aim of understanding how, where and to what extent they encode information about specific NLP tasks, revealing that different layers encode significantly different features (Tenney et al., 2019; Vulić et al., 2020).",
      "startOffset" : 173,
      "endOffset" : 214
    }, {
      "referenceID" : 14,
      "context" : "Although recent analyses have already provided important insights on which layers of a PLM are more relevant for SRL and how their relative importance is affected by the linguistic formalism of choice (Kuznetsov and Gurevych, 2020), not only do these analyses treat SRL as an atomic task but also do not look into taking advantage of such insights to improve current state-of-the-art SRL systems.",
      "startOffset" : 201,
      "endOffset" : 231
    }, {
      "referenceID" : 12,
      "context" : "Alongside the raising wave of successes collected by PLMs in an ever increasing number of areas, researchers started to question and investigate what happens inside these models and what they really capture, probing for knowledge and linguistic properties (Hewitt and Manning, 2019; Chi et al., 2020; Vulić et al., 2020).",
      "startOffset" : 256,
      "endOffset" : 320
    }, {
      "referenceID" : 5,
      "context" : "Alongside the raising wave of successes collected by PLMs in an ever increasing number of areas, researchers started to question and investigate what happens inside these models and what they really capture, probing for knowledge and linguistic properties (Hewitt and Manning, 2019; Chi et al., 2020; Vulić et al., 2020).",
      "startOffset" : 256,
      "endOffset" : 320
    }, {
      "referenceID" : 27,
      "context" : "Alongside the raising wave of successes collected by PLMs in an ever increasing number of areas, researchers started to question and investigate what happens inside these models and what they really capture, probing for knowledge and linguistic properties (Hewitt and Manning, 2019; Chi et al., 2020; Vulić et al., 2020).",
      "startOffset" : 256,
      "endOffset" : 320
    }, {
      "referenceID" : 24,
      "context" : "This body of work quickly attracted increasing attention and grew to become a field of study with a name of its own: BERTology (Rogers et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "One could argue that some complex relations may require a non-linear probe (White et al., 2021) which can reveal hidden information as long as it is accompanied by control experiments (Hewitt and Liang, 2019) to verify that such non-",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : ", 2021) which can reveal hidden information as long as it is accompanied by control experiments (Hewitt and Liang, 2019) to verify that such non-",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "classical NLP pipeline (Tenney et al., 2019), and that they often encode distances between syntactic constituents (Hewitt and Liang, 2019), lexical relations (Vulić et al.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : ", 2019), and that they often encode distances between syntactic constituents (Hewitt and Liang, 2019), lexical relations (Vulić et al.",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 27,
      "context" : ", 2019), and that they often encode distances between syntactic constituents (Hewitt and Liang, 2019), lexical relations (Vulić et al., 2020) and morphology (Chi et al.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : ", 2020) and morphology (Chi et al., 2020), inter alia.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : "Recently proposed approaches have achieved impressive results on several gold benchmarks (Hajič et al., 2009; Pradhan et al., 2012),",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "As mentioned above, some studies have already investigated how semantic knowledge is distributed among the inner layers of current PLMs, finding that information useful for SRL is mainly stored in their middle layers (Tenney et al., 2019).",
      "startOffset" : 217,
      "endOffset" : 238
    }, {
      "referenceID" : 20,
      "context" : "PropBank (Palmer et al., 2005) and {Experiencer, Stimulus, .",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "• Top-4: xp is the concatenation of the topmost four hidden layers of the language model: this is the configuration used in recently proposed approaches (Conia and Navigli, 2020);",
      "startOffset" : 153,
      "endOffset" : 178
    }, {
      "referenceID" : 13,
      "context" : "verbal SRL are still struggling to achieve satisfying performance, despite initial promising results (Klein et al., 2020; Zhao and Titov, 2020).",
      "startOffset" : 101,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "verbal SRL are still struggling to achieve satisfying performance, despite initial promising results (Klein et al., 2020; Zhao and Titov, 2020).",
      "startOffset" : 101,
      "endOffset" : 143
    } ],
    "year" : 0,
    "abstractText" : "Thanks to the effectiveness and wide availability of modern pretrained language models (PLMs), recently proposed approaches have achieved remarkable results in dependencyand span-based, multilingual and cross-lingual Semantic Role Labeling (SRL). These results prompted researchers to investigate the inner workings of modern PLMs with the aim of understanding how, where, and to what extent they encode information about SRL. In this paper, we follow this line of research and probe for predicate argument structures in PLMs. Our study shows that, not only do PLMs encode semantic structures directly into the contextualized representation of a predicate, but also provides insights on the correlation between predicate senses and their structures, on the degree of transferability between nominal and verbal structures, and on how such structures are encoded across languages. Finally, we look at the practical implications of such insights and demonstrate the benefits of embedding predicate argument structure information into an SRL model.",
    "creator" : null
  }
}
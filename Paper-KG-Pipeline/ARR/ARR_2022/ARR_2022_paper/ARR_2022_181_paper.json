{
  "name" : "ARR_2022_181_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Boundary Smoothing for Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) is one of the fundamental natural language processing (NLP) tasks with extensive investigations. As a common setting, an entity is regarded as correctly recognized only if its type and two boundaries exactly match the ground truth.\nThe annotation of boundaries is more ambiguous, error-prone, and raises more inconsistencies than entity types. For example, the CoNLL 2003 task contains four entity types (i.e., person, location, organization, miscellaneous), which are easy to distinguish between. However, the boundaries of a entity mention could be ambiguous, because of the “boundary words” (e.g., articles or modifiers). Considerable efforts are required to specify the “gold standard practice” case by case. Table 1 presents some examples from CoNLL 2003 Annotation Guidelines.2 In addition, some studies\n1Our code will be publicly released. 2https://www-nlpir.nist.gov/related_\nprojects/muc/proceedings/ne_task.html.\nhave also reported that incorrect boundary is a major source of entity recognition error (Wang et al., 2019; Eberts and Ulges, 2020).\nRecently, span-based models have gained much popularity in NER studies, and achieved state-ofthe-art (SOTA) results (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021). This approach typically enumerates all candidate spans and classifies them into entity types (including a “non-entity” type); the annotated spans are scarce and assigned with full probability to be an entity, whereas all other spans are assigned with zero probability. This creates noticeable sharpness between the classification targets of adjacent spans, and may thus plague the trainability of neural networks. In addition, empirical evidence shows that these models easily encounter the over-confidence issue, i.e., the confidence of a predicted entity is much higher than its correctness probability. This is a manifestation of miscalibration (Guo et al., 2017).\nInspired by label smoothing (Szegedy et al., 2016; Müller et al., 2019), we propose boundary smoothing as a regularization technique for span-based neural NER models. By explicitly reallocating entity probabilities from annotated spans to the surrounding ones, boundary smoothing can effectively mitigate over-confidence, and result in consistently better performance.\nSpecifically, our baseline employs the contextualized embeddings from a pretrained Transformer of base size (768 hidden size, 12 layers), and the biaffine decoder proposed by Yu et al. (2020). With boundary smoothing, our model outperforms previous SOTA on four English NER datasets (CoNLL 2003, OntoNotes 5, ACE 2004 and ACE 2005) and two Chinese datasets (Weibo NER and Resume NER), and achieves competitive results on other two Chinese datasets (OntoNotes 4 and MSRA). Such extensive experiments support the effectiveness and robustness of our proposed technique.\nIn addition, we show that boundary smoothing can help the trained NER models to preserve calibration, such that the produced confidences can better represent the precision rate of a predicted entity. This corresponds to the effect of label smoothing on the image classification task (Müller et al., 2019). Further, visualization results qualitatively suggest that boundary smoothing can lead to flatter solutions and more smoothed loss landscapes, which are typically associated with better generalization and trainability (Hochreiter and Schmidhuber, 1997; Li et al., 2018)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Named Entity Recognition The mainstream NER systems are designed to recognize flat entities and based on a sequence tagging framework. Collobert et al. (2011) introduced the linear-chain conditional random field (CRF) into neural networkbased sequence tagging models, which can explicitly encode the transition likelihoods between adjacent tags. Many researchers followed this work, and employed LSTM as the encoder. In addition, character-level representations are typically used for English tasks (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al., 2020; Li et al., 2020a).\nNested NER allows a token to belong to multiple entities, which conflicts with the plain sequence tagging framework. Ju et al. (2018) proposed to use stacked LSTM-CRFs to predict from inner to outer entities. Straková et al. (2019) concatenated the BILOU tags for each token inside the nested entities, which allows the LSTM-CRF to work as for flat entities. Li et al. (2020b) reformulated nested NER as a machine reading comprehension task. Shen et al. (2021) proposed to recognize nested\nentities by the two-stage object detection method widely used in computer vision.\nRecent years, a body of literature emerged on span-based models, which were compatible with both flat and nested entities, and achieved SOTA performance (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021). These models typically enumerate all possible candidate text spans and then classify each span into entity types. In this work, the biaffine model (Yu et al., 2020) is chosen and re-implemented with slight modifications as our baseline, because of its high performance and compatibility with boundary smoothing.\nIn addition, pretrained language models, also known as contextualized embeddings, were also widely introduced to NER models, and significantly boosted the model performance (Peters et al., 2018; Devlin et al., 2019). They are used in our baseline by default.\nLabel Smoothing Szegedy et al. (2016) proposed the label smoothing as a regularization technique to improve the accuracy of the Inception networks on ImageNet. By explicitly assigning a small probability to non-ground-truth labels, label smoothing can prevent the models from becoming too confident about the predictions, and thus improve generalization. It turned out to be a useful alternative to the standard cross entropy loss, and has been widely adopted to fight against the over-confidence (Zoph et al., 2018; Chorowski and Jaitly, 2017; Vaswani et al., 2017), improve the model calibration (Müller et al., 2019), and denoise incorrect labels (Lukasik et al., 2020).\nOur proposed boundary smoothing applies the smoothing technique to entity boundaries, rather than labels. This is driven by the observation that entity boundaries are more ambiguous and inconsistent to annotate in NER engineering. To the best of our knowledge, this study is the first that focuses on the effect of smoothing regularization on NER models."
    }, {
      "heading" : "3 Methods",
      "text" : ""
    }, {
      "heading" : "3.1 Biaffine Decoder",
      "text" : "A neural network-based NER model typically encodes the input tokens to a sequence of representations x = x1, x2, . . . , xT of length T , and then decodes these representations to task outputs, i.e., a list of entities specified by types and boundaries.\nWe follow Yu et al. (2020) and use the biaffine\ndecoder. Specifically, the representations x are separately affined by two feedforward networks, resulting in two representations hs ∈ RT×d and he ∈ RT×d, which correspond to the start and end positions of spans. For c entity types (a “nonentity” type included), given a span starting at the i-th token and ending at the j-th token, a scoring vector rij ∈ Rc can be computed as:\nrij = (h s i ) TUhej +W (h s i ⊕ hej ⊕wj−i) + b, (1)\nwhere wj−i ∈ Rdw is the (j − i)-th width embedding from a dedicated learnable matrix; U ∈ Rd×c×d, W ∈ Rc×(2d+dw) and b ∈ Rc are learnable parameters. rij is then fed into a softmax layer:\nŷij = softmax(rij), (2)\nwhich yields the predicted probabilities over all entity types.\nThe ground truth yij ∈ Rc is an one-hot encoded vector, with value being 1 if the index corresponds with the annotated entity type, and 0 otherwise. Thus, the model can be optimized by the standard cross entropy loss for all candidate spans:\nLCE = − ∑\n0≤i≤j<T yTij log(ŷij). (3)\nIn the inference time, the spans predicted to be “non-entity” are first discarded, and the remaining ones are ranked by their predictive confidences. Spans with lower confidences would also be discarded if they clash with the boundaries of spans with higher confidences. Refer to Yu et al. (2020) for more details."
    }, {
      "heading" : "3.2 Boundary Smoothing",
      "text" : "Figure 1a visualizes the ground truth yij for an example sentence with two annotated entities. The valid candidate spans cover the upper triangular area of the matrix. In existing NER models, the annotated boundaries are considered to be absolutely reliable. Hence, each annotated span is assigned with the full probability to be an entity, whereas all unannotated spans are assigned with zero probability. We refer to this probability allocation as hard boundary, which is, however, probably not the best choice.\nAs aforementioned, the entity boundaries may be ambiguous and inconsistent, so the spans surrounding an annotated one deserve a small probability to be an entity. Figure 1b visualizes ỹij , the boundary\nsmoothing version of yij . Specifically, given an annotated entity, a portion of probability is assigned to its surrounding spans, and the remaining probability 1− is assigned to the originally annotated span. With smoothing size D, all the spans with Manhattan distance d (d ≤ D) to the annotated entity equally share probability /D. After such entity probability re-allocation, any remaining probability of a span is assigned to be “non-entity”. We refer to this as smoothed boundary.\nThus, the biaffine model can be optimized by the boundary-smoothing regularized cross entropy loss:\nLBS = − ∑\n0≤i≤j<T ỹTij log(ŷij). (4)\nEmpirically, the positive samples (i.e., groundtruth entities) are sparsely distributed over the candidate spans. For example, the CoNLL 2003 dataset has about 35 thousand entities, which represent only 0.93% in the 3.78 million candidate spans. By explicitly assigning probability to surrounding spans, boundary smoothing prevents the model from concentrating all probability mass on the scarce positive samples. This intuitively helps alleviate over-confidence.\nIn addition, hard boundary presents noticeable sharpness between the classification targets of positive spans and surrounding ones, although they share similar contextualized representations. Smoothed boundary provides more continuous targets across spans, which are conceptually more compatible with the inductive bias of neural networks that prefers continuous solutions (Hornik et al., 1989)."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Datasets We use four English NER datasets: CoNLL 2003 (Tjong Kim Sang and Veenstra, 1999), OntoNotes 53, ACE 20044 and ACE 20055; and four Chinese NER datasets: OntoNotes 46, MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015) and Resume NER (Zhang and Yang, 2018). Among them, ACE 2004 and ACE 2005 are nested NER tasks, and the others are flat tasks.\nHyperparameters For English corpora, we use RoBERTa (Liu et al., 2019) followed by a BiLSTM layer to produce the contextualized representations. For Chinese, we choose the BERT pretrained with whole word masking (Cui et al., 2019). The BiLSTM has one layer and 200 hidden size with dropout rate of 0.5. The biaffine decoder follows Yu et al. (2020), with the affine layers of hidden size 150 and dropout rate 0.2. We additionally introduce a span width embedding of size 25. Note that the pretrained language models are all of the base size (768 hidden size, 12 layers), and the model is free of any additional auxiliary\n3https://catalog.ldc.upenn.edu/ LDC2013T19; Data splits follow Pradhan et al. (2013).\n4https://catalog.ldc.upenn.edu/ LDC2005T09; Data splits follow Lu and Roth (2015).\n5https://catalog.ldc.upenn.edu/ LDC2006T06; Data splits follow Lu and Roth (2015).\n6https://catalog.ldc.upenn.edu/ LDC2011T03; Data splits follow Che et al. (2013).\nembeddings; this configuration is relatively simple, compared with those in related work.\nThe boundary smoothing parameter is selected in {0.1, 0.2, 0.3}; smoothing size D is selected in {1, 2}.\nAll the models are trained by the AdamW optimizer (Loshchilov and Hutter, 2018) with a gradient clipping at L2-norm of 5.0 (Pascanu et al., 2013). The models are trained for 50 epochs with batch size of 48. The learning rate is searched between 1e-3 and 3e-3 on the randomly initialized weights, and between 8e-6 and 3e-5 on the pretrained weights; a scheduler of linear warmup in the first 20% steps followed by linear decay is applied.\nEvaluation A predicted entity is considered correct if its type and boundaries exactly match the ground truth. Hyperparameters are tuned according to the F1 scores on the development set, and the evaluation metrics (precision, recall, F1 score) are reported on the testing set."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "Table 2 presents the evaluation results on four English datasets, in which CoNLL 2003 and OntoNotes 5 are flat NER corpora, whereas ACE 2004 and ACE 2005 contains a high proportion of nested entities. Compared with previous SOTA systems, our simple baseline (RoBERTa-base + BiLSTM + Biaffine) achieves on-par or slightly inferior performance. Provided the strong baseline, our experiments show that boundary smoothing can effectively and consistently boost the F1 score of entity recognition across different datasets. With the help of boundary smoothing, our model outperforms the best of the previous SOTA systems by a magnitude from 0.2 to 0.5 percentages.\nTable 3 presents the results on four Chinese datasets, which are all flat NER corpora. Again, boundary smoothing consistently improves model performance against the baseline (BERT-basewwm + BiLSTM + Biaffine) across all datasets. In addition, our model outperforms previous SOTA by 2.16 and 0.55 percentages on Weibo and Resume NER datasets, and achieves comparable F1 scores on OntoNotes 4 and MSRA. Note that almost all previous systems solve these tasks within a sequence tagging framework; in contrast, this work is among the first to introduce a span-based approach to Chinese NER tasks and establish SOTA results.\nCoNLL 2003\nModel Prec. Rec. F1\nLample et al. (2016) – – 90.94 Chiu and Nichols (2016)† 91.39 91.85 91.62 Peters et al. (2018) – – 92.22 Akbik et al. (2018)† – – 93.07 Devlin et al. (2019) – – 92.8 Straková et al. (2019)† – – 93.38 Wang et al. (2019)† – – 93.43 Li et al. (2020b) 92.33 94.61 93.04 Yu et al. (2020)† 93.7 93.3 93.5 Baseline 92.93 94.03 93.48 Baseline + BS 93.61 93.68 93.65\nOntoNotes 5\nModel Prec. Rec. F1\nChiu and Nichols (2016) 86.04 86.53 86.28 Li et al. (2020b) 92.98 89.95 91.11 Yu et al. (2020) 91.1 91.5 91.3 Baseline 90.31 92.13 91.21 Baseline + BS 91.75 91.74 91.74\nACE 2004\nModel Prec. Rec. F1\nKatiyar and Cardie (2018) 73.6 71.8 72.7 Straková et al. (2019)† – – 84.40 Li et al. (2020b) 85.05 86.32 85.98 Yu et al. (2020) 87.3 86.0 86.7 Shen et al. (2021) 87.44 87.38 87.41 Baseline 86.67 88.42 87.54 Baseline + BS 88.43 87.53 87.98\nACE 2005\nIn five out of the above eight datasets, integrating boundary smoothing significantly increases the precision rate with a slight drop in the recall, resulting in a better overall F1 score. This is consistent with our expectation, because boundary smoothing discourages over-confidence when recognizing entities, which implicitly leads the model to establish a more critical threshold to admit entities.\nGiven the use of well pretrained language models, most of the performance gains are relatively marginal. However, boundary smoothing can work effectively and consistently for different languages and datasets. In addition, it is easy to implement\nOntoNotes 4\nModel Prec. Rec. F1\nZhang and Yang (2018) 76.35 71.56 73.88 Ma et al. (2020) 83.41 82.21 82.81 Li et al. (2020a) – – 81.82 Li et al. (2020b) 82.98 81.25 82.11 Chen and Kong (2021) 79.25 80.66 79.95 Wu et al. (2021) – – 82.57 Baseline 82.79 81.27 82.03 Baseline + BS 81.65 84.03 82.83\nMSRA\nand integrate into any span-based neural NER models, with almost no side effects."
    }, {
      "heading" : "4.3 Ablation Studies",
      "text" : "We perform ablation studies on CoNLL 2003, ACE 2005 and Resume NER datasets (covering flat/nested and English/Chinese datasets), to evaluate the effects of boundary smoothing parameter and D, as well as other components of our NER system.\nBoundary Smoothing Parameters We train the model with in {0.1, 0.2, 0.3} and D in {1, 2}; the corresponding results are reported in Table 4. Most combinations of the two hyperparameters can achieve higher F1 scores than the baseline, which\nsuggests the robustness of boundary smoothing. On the other hand, the best smoothing parameters are different across datasets. Hence, if the best performance is desired for a new NER task in practice, hyperparameter tuning would be necessary.\nLabel Smoothing We replace boundary smoothing with label smoothing in the span classifier. Label smoothing cannot improve, or may even impair the performance of the model, compared with the baseline (see Table 4). As aforementioned, we hypothesize that the semantic differences between the typical entity types are quite clear, so it is ineffective to smooth between them.\nPretrained Language Models We test if better pretrained language models can further improve the performance. For English datasets, we use RoBERTa of large size (1024 hidden size, 24 layers), and the F1 scores increase by 0.12 and 0.87 percentages for CoNLL 2003 and ACE 2005, respectively. For Chinese, we use MacBERT (Cui et al., 2020) of base and large sizes, and both improve the F1 score by 0.09 percentage on Resume NER (see Table 5).\nNote that boundary smoothing contributes to the F1 scores by 0.17, 0.59 and 0.32 percentages on these three datasets, which are roughly comparable to the magnitudes by switching the pretrained language model from base size to large size.\nBiLSTM Layer We remove the BiLSTM layer, directly feeding the output of pretrained language model into the biaffine decoder. Absence of the BiLSTM layer will result in drops of the F1 scores by 0.35, 0.57 and 0.1 percentages on the three datasets (see Table 5)."
    }, {
      "heading" : "5 Further In-Depth Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Over-Confidence and Entity Calibration",
      "text" : "The model performance (evaluated by, e.g., accuracy or F1 score) is certainly important. However, the confidences of model predictions are also of interest in many applications. For example, when it requires the predicted entities to be highly reliable (i.e., precision is of more priority than recall), we may filter out the entities with confidences lower than a specific threshold.\nHowever, Guo et al. (2017) have indicated that modern neural networks are poorly calibrated, and typically over-confident with their predictions. By calibration, they mean the extent to which the prediction confidences produced by a model can represent the true correctness probability. We find neural NER models also easy to become miscalibrated and over-confident. We observe that, with the standard cross entropy loss, both the development loss and F1 score increase in the later training stage, which goes against the common perception that the loss and F1 score should change in the opposite directions. This phenomenon is similar to the disconnect between negative likelihood and accuracy in image classification described by Guo et al. (2017). We suppose that the model becomes over-confident with its predictions, including the incorrect ones, which contributes to the increase of loss (see Appendix A for more details).\nTo formally investigate the over-confidence issue, we plot the reliability diagrams and calculate expected calibration error (ECE). In brief, for an NER model, we group all the predicted entities by the associated confidences into ten bins, and then calculate the precision rate for each bin. If the model is well calibrated, the precision rate should be close to the confidence level for each bin (see Appendix B for more details).\nFigure 2 compares the reliability diagrams and\nECEs between models with different smoothness on CoNLL 2003 and OntoNotes 5. For the baseline model ( = 0), the precision rates are much lower than corresponding confidence levels, suggesting significant over-confidence. By introducing boundary smoothing and increasing the smoothness , the over-confidence is gradually mitigated, and shifted to under-confidence ( = 0.3). In general, the model presents best reliability diagrams when is 0.1 or 0.2. In addition, the ECEs of the baseline model are 0.072 and 0.063 on CoNLL 2003 and OntoNotes 5, respectively; with of 0.1, the ECEs are reduced to 0.013 and 0.034.\nIn conclusion, boundary smoothing can prevent the model from becoming over-confident with the predicted entities, and result in better calibration. In addition, as mentioned previously, spans with lower confidences are discarded if they clash with those of higher confidences when decoding. With the better calibration, the model can obtain a very marginal but consistent increase in the F1 score."
    }, {
      "heading" : "5.2 Loss Landscape Visualization",
      "text" : "How does boundary smoothing improve the model performance? We originally conjectured that boundary smoothing can de-noise the inconsistently annotated entity boundaries (Lukasik et al., 2020), but failed to find enough evidence – the performance improvement did not significantly increase when we injected boundary noises into the training data.7\nAs aforementioned, positive samples are very sparse among the candidate spans. Without boundary smoothing, the annotated spans are regarded to be entities with full probability, whereas all other spans are assigned with zero probability. This creates noticeable sharpness between the targets of the annotated spans and surrounding ones, although their neural representations are similar. Boundary smoothing re-allocates the entity probabilities across contiguous spans, which mitigates the sharpness and results in more continuous targets. Conceptually, such targets are more compatible with the inductive bias of neural networks that prefers continuous solutions (Hornik et al., 1989).\nLi et al. (2018) have shown that residual connections and well-tuned hyperparameters (e.g., learning rate, batch size) can produce flatter minima and less chaotic loss landscapes, which account for the better generalization and trainability. Their findings provide important insights into the geometric properties of non-convex neural loss functions.\nFigure 3 visualizes the loss landscapes for models with different smoothness on CoNLL 2003 and OntoNotes 5, following Li et al. (2018). In short, for a trained model, a direction of the parameters is randomly sampled, normalized and fixed, and the loss landscape is computed by sampling over this direction (refer to Appendix C for more details).\nThe visualization results qualitatively show that, the solutions found by the standard cross entropy are relatively sharp, whereas boundary smoothing can help arrive at flatter minima. As many theoretical studies regard the flatness as a promising predictor for model generalization (Hochreiter and Schmidhuber, 1997; Jiang et al., 2019), this result may explain why boundary smoothing can improve the model performance. In addition, boundary smoothing is associated with more smoothed land-\n7On the other hand, this cannot rule out the de-noising effect of boundary smoothing, because the synthesized boundary noises are differently distributed from the real noises.\nscapes – the surrounding local minima are small, shallow, and thus easy for the optimizer to escape. Intuitively, such geometric property suggests that the underlying loss functions are easier to train (Li et al., 2018).\nWe believe that the sharpness in the span-based NER targets is probably the reason for the sharp and chaotic loss landscape. Boundary smoothing can effectively mitigate the sharpness, and result in loss landscapes of better generalization and trainability."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this study, we propose boundary smoothing as a regularization technique for span-based neural NER models. Boundary smoothing re-assigns entity probabilities from annotated spans to the surrounding ones. It can be easily integrated into any span-based neural NER systems, but consistently bring improved performance. Built on a simple but strong baseline (a base-sized pretrained language model followed by a BiLSTM layer, and the biaffine decoder), our model achieves SOTA results on eight well-known NER benchmarks, covering English and Chinese, flat and nested NER tasks.\nIn addition, experimental results show that boundary smoothing leads to less over-confidence, better model calibration, flatter neural minima and more smoothed loss landscapes. These properties plausibly explain the performance improvement. Our findings shed light on the effects of smoothing regularization technique in the NER task."
    }, {
      "heading" : "Acknowledgements",
      "text" : ""
    }, {
      "heading" : "A Disconnect between Development Loss and F1 Score",
      "text" : "For most machine learning tasks, the desired metric (e.g., accuracy or F1 score) is non-differentiable and thus cannot be optimized via back-propagation. The loss, on the other hand, is a designed differentiable proxy such that minimizing it can increase the original metric.\nHowever, as illustrated in Figure 4a, when training an NER model by the standard cross entropy loss, although the development F1 score keeps increasing throughout, the development loss also increases in the later stage (e.g., after ten epochs) of the training process. Guo et al. (2017) describe this phenomenon as a disconnect – the neural network overfits to the loss without overfitting to the metric. They regard this as indirect evidence for miscalibration.\nOne plausible explanation is that in the later training stage, the model becomes too confident with its predicted outcomes, including both the correct and incorrect ones. Therefore, although slightly more spans are correctly classified on the development set (as the F1 score increases), a small portion of incorrectly classified spans is assigned with much more confidence and contributes to the increase of loss.\nFigure 4b presents the curves for boundary smoothing loss. The development loss decreases\nthroughout the training process, opposite to the increasing F1 score. This result suggests that boundary smoothing can help mitigate over-confidence."
    }, {
      "heading" : "B Reliability Diagrams and Expected Calibration Error",
      "text" : "We generally follow Guo et al. (2017)’s approach to plot reliability diagrams and calculate expected calibration error (ECE).\nGiven an NER dataset and a model trained on it, denote the gold and predicted entity sets as E and Ê , respectively; the model produces a confidence p̂e for each entity e ∈ Ê . With K confidence interval bins, the predicted entities are grouped such that those with confidences falling into the k-th bin\nconstitute a subset: Êk = { e | e ∈ Ê , p̂e ∈ ( k − 1 K , k K ]} .\nThe precision rate (equivalent to the accuracy with regard to a predicted set) of k-th group Êk is:\nPreck = |Êk ∩ E| |Êk| ,\nand the corresponding average confidence is:\nConfk =\n∑ e∈Êk p̂e\n|Êk| .\nThe reliability diagrams plot Preck against Confk for k = 1, 2, . . . ,K. ECE is estimated by the weighted average of absolute difference between Preck and Confk:\nECE = K∑ k=1 |Êk| |Ê | · ∣∣∣∣Preck − Confk∣∣∣∣\nBy definition, a perfectly calibrated model will have Preck = Confk for k = 1, 2, . . . ,K. In this case, the reliability diagrams should lie along the identity line, and ECE equals to 0."
    }, {
      "heading" : "C Loss Landscape Visualization",
      "text" : "We generally follow Li et al. (2018)’s approach to visualize the loss landscape.\nGiven a trained model of parameters θ?, we sample a random direction δ from a normal distribution, and rescale it by:\nδi ← ‖θ?i ‖ ‖δi‖ δi,\nwhere δi is the i-th weight of δ.8 On a data set/split D, the loss landscape plots the function:\nf(α) = L(D; θ? + αδ),\nwhere L(D; θ) is the average loss value (in the evaluation mode) on D if the model takes parameters of θ. In practice, we evenly sample 51 points in the interval [−1, 1] for α, and plot the loss values against α.\n8Li et al. (2018) use filter-wise normalization for convolutional networks, whereas our models have no convolutional layers, so we simplify it as weight-wise normalization."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649, Santa Fe, New Mexico, USA. Associ-",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Named entity recognition with bilingual constraints",
      "author" : [ "Wanxiang Che", "Mengqiu Wang", "Christopher D. Manning", "Ting Liu." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Che et al\\.,? 2013",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2013
    }, {
      "title" : "Enhancing entity boundary detection for better Chinese named entity recognition",
      "author" : [ "Chun Chen", "Fang Kong." ],
      "venue" : "Proceedings of the 59th Annual 8",
      "citeRegEx" : "Chen and Kong.,? 2021",
      "shortCiteRegEx" : "Chen and Kong.",
      "year" : 2021
    }, {
      "title" : "Named entity recognition with bidirectional LSTM-CNNs",
      "author" : [ "Jason P.C. Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "Towards better decoding and language model integration in sequence to sequence models",
      "author" : [ "Jan Chorowski", "Navdeep Jaitly." ],
      "venue" : "INTERSPEECH 2017, pages 523–527.",
      "citeRegEx" : "Chorowski and Jaitly.,? 2017",
      "shortCiteRegEx" : "Chorowski and Jaitly.",
      "year" : 2017
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research, 12(ARTICLE):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Revisiting pretrained models for Chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 657–668,",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training with whole word masking for Chinese BERT",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Ziqing Yang", "Shijin Wang", "Guoping Hu." ],
      "venue" : "arXiv preprint arXiv:1906.08101.",
      "citeRegEx" : "Cui et al\\.,? 2019",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Span-based joint entity and relation extraction with Transformer pre-training",
      "author" : [ "Markus Eberts", "Adrian Ulges." ],
      "venue" : "Proceedings of the 24th European Conference on Artificial Intelligence, Santiago de Compostela, Spain.",
      "citeRegEx" : "Eberts and Ulges.,? 2020",
      "shortCiteRegEx" : "Eberts and Ulges.",
      "year" : 2020
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q Weinberger." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Flat minima",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(1):1–42.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "Kurt Hornik", "Maxwell Stinchcombe", "Halbert White." ],
      "venue" : "Neural networks, 2(5):359–366.",
      "citeRegEx" : "Hornik et al\\.,? 1989",
      "shortCiteRegEx" : "Hornik et al\\.",
      "year" : 1989
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Fantastic generalization measures and where to find them",
      "author" : [ "Yiding Jiang", "Behnam Neyshabur", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio." ],
      "venue" : "arXiv preprint arXiv:1912.02178.",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "A neural layered model for nested named entity recognition",
      "author" : [ "Meizhi Ju", "Makoto Miwa", "Sophia Ananiadou." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Ju et al\\.,? 2018",
      "shortCiteRegEx" : "Ju et al\\.",
      "year" : 2018
    }, {
      "title" : "Nested named entity recognition revisited",
      "author" : [ "Arzoo Katiyar", "Claire Cardie." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa-",
      "citeRegEx" : "Katiyar and Cardie.,? 2018",
      "shortCiteRegEx" : "Katiyar and Cardie.",
      "year" : 2018
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "The third international Chinese language processing bakeoff: Word segmentation and named entity recognition",
      "author" : [ "Gina-Anne Levow." ],
      "venue" : "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108–117, Sydney, Aus-",
      "citeRegEx" : "Levow.,? 2006",
      "shortCiteRegEx" : "Levow.",
      "year" : 2006
    }, {
      "title" : "A span-based model for joint overlapped and discontinuous named entity recognition",
      "author" : [ "Fei Li", "ZhiChao Lin", "Meishan Zhang", "Donghong Ji." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Visualizing the loss landscape of neural nets",
      "author" : [ "Hao Li", "Zheng Xu", "Gavin Taylor", "Christoph Studer", "Tom Goldstein." ],
      "venue" : "Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 6391–6401.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "FLAT: Chinese NER using flatlattice transformer",
      "author" : [ "Xiaonan Li", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6836–6842, Online. Association",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "A unified MRC framework for named entity recognition",
      "author" : [ "Xiaoya Li", "Jingrong Feng", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Jiwei Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849–",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2018",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Joint mention extraction and classification with mention hypergraphs",
      "author" : [ "Wei Lu", "Dan Roth." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 857–867, Lisbon, Portugal. Association for Compu-",
      "citeRegEx" : "Lu and Roth.,? 2015",
      "shortCiteRegEx" : "Lu and Roth.",
      "year" : 2015
    }, {
      "title" : "Does label smoothing mitigate label noise",
      "author" : [ "Michal Lukasik", "Srinadh Bhojanapalli", "Aditya Menon", "Sanjiv Kumar" ],
      "venue" : "In Proceedings of the 37th International Conference on Machine Learning,",
      "citeRegEx" : "Lukasik et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lukasik et al\\.",
      "year" : 2020
    }, {
      "title" : "Simplify the usage of lexicon in Chinese NER",
      "author" : [ "Ruotian Ma", "Minlong Peng", "Qi Zhang", "Zhongyu Wei", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5951–5960, Online. As-",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin, Ger-",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "When does label smoothing help? In Advances in Neural Information Processing Systems, volume 32",
      "author" : [ "Rafael Müller", "Simon Kornblith", "Geoffrey E Hinton." ],
      "venue" : "Curran Associates, Inc.",
      "citeRegEx" : "Müller et al\\.,? 2019",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2019
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "International Conference on Machine Learning, pages 1310–1318. PMLR.",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Named entity recognition for Chinese social media with jointly trained embeddings",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal.",
      "citeRegEx" : "Peng and Dredze.,? 2015",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2015
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages",
      "citeRegEx" : "Zettlemoyer.,? 2018",
      "shortCiteRegEx" : "Zettlemoyer.",
      "year" : 2018
    }, {
      "title" : "Towards robust linguistic analysis using OntoNotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Hwee Tou Ng", "Anders Björkelund", "Olga Uryupina", "Yuchen Zhang", "Zhi Zhong." ],
      "venue" : "Proceedings of the Seventeenth Conference on Computa-",
      "citeRegEx" : "Pradhan et al\\.,? 2013",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2013
    }, {
      "title" : "Locate and label: A two-stage identifier for nested named entity recognition",
      "author" : [ "Yongliang Shen", "Xinyin Ma", "Zeqi Tan", "Shuai Zhang", "Wen Wang", "Weiming Lu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural architectures for nested NER through linearization",
      "author" : [ "Jana Straková", "Milan Straka", "Jan Hajic." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5326–5331, Florence, Italy. Association for",
      "citeRegEx" : "Straková et al\\.,? 2019",
      "shortCiteRegEx" : "Straková et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Representing text chunks",
      "author" : [ "Erik F. Tjong Kim Sang", "Jorn Veenstra." ],
      "venue" : "Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 173–179, Bergen, Norway. Association for Computational Linguistics.",
      "citeRegEx" : "Sang and Veenstra.,? 1999",
      "shortCiteRegEx" : "Sang and Veenstra.",
      "year" : 1999
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "CrossWeigh: Training named entity tagger from imperfect annotations",
      "author" : [ "Zihan Wang", "Jingbo Shang", "Liyuan Liu", "Lihao Lu", "Jiacheng Liu", "Jiawei Han." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "MECT: Multi-metadata embedding based crosstransformer for Chinese named entity recognition",
      "author" : [ "Shuang Wu", "Xiaoning Song", "Zhenhua Feng." ],
      "venue" : "10",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Named entity recognition as dependency parsing",
      "author" : [ "Juntao Yu", "Bernd Bohnet", "Massimo Poesio." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470– 6476, Online. Association for Computational Lin-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Chinese NER using lattice LSTM",
      "author" : [ "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554– 1564, Melbourne, Australia. Association for Compu-",
      "citeRegEx" : "Zhang and Yang.,? 2018",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2018
    }, {
      "title" : "Learning transferable architectures for scalable image recognition",
      "author" : [ "Barret Zoph", "Vijay Vasudevan", "Jonathon Shlens", "Quoc V Le." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8697–8710.",
      "citeRegEx" : "Zoph et al\\.,? 2018",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2018
    }, {
      "title" : "2018)’s approach to visualize the loss landscape. Given a trained model of parameters θ?, we sample a random direction δ from a normal distribution, and rescale it",
      "author" : [ "Li" ],
      "venue" : null,
      "citeRegEx" : "Li,? \\Q2018\\E",
      "shortCiteRegEx" : "Li",
      "year" : 2018
    }, {
      "title" : "2018) use filter-wise normalization for convolutional networks, whereas our models have no convolutional layers, so we simplify it as weight-wise normalization",
      "author" : [ "8Li" ],
      "venue" : null,
      "citeRegEx" : "⍐8⍗Li,? \\Q2018\\E",
      "shortCiteRegEx" : "⍐8⍗Li",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "have also reported that incorrect boundary is a major source of entity recognition error (Wang et al., 2019; Eberts and Ulges, 2020).",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "have also reported that incorrect boundary is a major source of entity recognition error (Wang et al., 2019; Eberts and Ulges, 2020).",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Recently, span-based models have gained much popularity in NER studies, and achieved state-ofthe-art (SOTA) results (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 174
    }, {
      "referenceID" : 41,
      "context" : "Recently, span-based models have gained much popularity in NER studies, and achieved state-ofthe-art (SOTA) results (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 174
    }, {
      "referenceID" : 19,
      "context" : "Recently, span-based models have gained much popularity in NER studies, and achieved state-ofthe-art (SOTA) results (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "This is a manifestation of miscalibration (Guo et al., 2017).",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 36,
      "context" : "Inspired by label smoothing (Szegedy et al., 2016; Müller et al., 2019), we propose boundary smoothing as a regularization technique for span-based neural NER models.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "Inspired by label smoothing (Szegedy et al., 2016; Müller et al., 2019), we propose boundary smoothing as a regularization technique for span-based neural NER models.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "This corresponds to the effect of label smoothing on the image classification task (Müller et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "Further, visualization results qualitatively suggest that boundary smoothing can lead to flatter solutions and more smoothed loss landscapes, which are typically associated with better generalization and trainability (Hochreiter and Schmidhuber, 1997; Li et al., 2018).",
      "startOffset" : 217,
      "endOffset" : 268
    }, {
      "referenceID" : 20,
      "context" : "Further, visualization results qualitatively suggest that boundary smoothing can lead to flatter solutions and more smoothed loss landscapes, which are typically associated with better generalization and trainability (Hochreiter and Schmidhuber, 1997; Li et al., 2018).",
      "startOffset" : 217,
      "endOffset" : 268
    }, {
      "referenceID" : 13,
      "context" : "In addition, character-level representations are typically used for English tasks (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al.",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : "In addition, character-level representations are typically used for English tasks (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al.",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 28,
      "context" : "In addition, character-level representations are typically used for English tasks (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al.",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "In addition, character-level representations are typically used for English tasks (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al.",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 42,
      "context" : ", 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al., 2020; Li et al., 2020a).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 27,
      "context" : ", 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al., 2020; Li et al., 2020a).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 21,
      "context" : ", 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018; Ma et al., 2020; Li et al., 2020a).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "Recent years, a body of literature emerged on span-based models, which were compatible with both flat and nested entities, and achieved SOTA performance (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 211
    }, {
      "referenceID" : 41,
      "context" : "Recent years, a body of literature emerged on span-based models, which were compatible with both flat and nested entities, and achieved SOTA performance (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 211
    }, {
      "referenceID" : 19,
      "context" : "Recent years, a body of literature emerged on span-based models, which were compatible with both flat and nested entities, and achieved SOTA performance (Eberts and Ulges, 2020; Yu et al., 2020; Li et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 211
    }, {
      "referenceID" : 41,
      "context" : "In this work, the biaffine model (Yu et al., 2020) is chosen and re-implemented with slight modifications as our baseline, because of its high performance and compatibility with boundary smoothing.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "In addition, pretrained language models, also known as contextualized embeddings, were also widely introduced to NER models, and significantly boosted the model performance (Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 173,
      "endOffset" : 215
    }, {
      "referenceID" : 43,
      "context" : "It turned out to be a useful alternative to the standard cross entropy loss, and has been widely adopted to fight against the over-confidence (Zoph et al., 2018; Chorowski and Jaitly, 2017; Vaswani et al., 2017), improve the model calibration (Müller et al.",
      "startOffset" : 142,
      "endOffset" : 211
    }, {
      "referenceID" : 4,
      "context" : "It turned out to be a useful alternative to the standard cross entropy loss, and has been widely adopted to fight against the over-confidence (Zoph et al., 2018; Chorowski and Jaitly, 2017; Vaswani et al., 2017), improve the model calibration (Müller et al.",
      "startOffset" : 142,
      "endOffset" : 211
    }, {
      "referenceID" : 38,
      "context" : "It turned out to be a useful alternative to the standard cross entropy loss, and has been widely adopted to fight against the over-confidence (Zoph et al., 2018; Chorowski and Jaitly, 2017; Vaswani et al., 2017), improve the model calibration (Müller et al.",
      "startOffset" : 142,
      "endOffset" : 211
    }, {
      "referenceID" : 29,
      "context" : ", 2017), improve the model calibration (Müller et al., 2019), and denoise incorrect labels (Lukasik et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : ", 2019), and denoise incorrect labels (Lukasik et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "Smoothed boundary provides more continuous targets across spans, which are conceptually more compatible with the inductive bias of neural networks that prefers continuous solutions (Hornik et al., 1989).",
      "startOffset" : 181,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "Datasets We use four English NER datasets: CoNLL 2003 (Tjong Kim Sang and Veenstra, 1999), OntoNotes 53, ACE 20044 and ACE 20055; and four Chinese NER datasets: OntoNotes 46, MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015) and Resume NER (Zhang and Yang, 2018).",
      "startOffset" : 180,
      "endOffset" : 193
    }, {
      "referenceID" : 31,
      "context" : "Datasets We use four English NER datasets: CoNLL 2003 (Tjong Kim Sang and Veenstra, 1999), OntoNotes 53, ACE 20044 and ACE 20055; and four Chinese NER datasets: OntoNotes 46, MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015) and Resume NER (Zhang and Yang, 2018).",
      "startOffset" : 205,
      "endOffset" : 228
    }, {
      "referenceID" : 42,
      "context" : "Datasets We use four English NER datasets: CoNLL 2003 (Tjong Kim Sang and Veenstra, 1999), OntoNotes 53, ACE 20044 and ACE 20055; and four Chinese NER datasets: OntoNotes 46, MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015) and Resume NER (Zhang and Yang, 2018).",
      "startOffset" : 244,
      "endOffset" : 266
    }, {
      "referenceID" : 23,
      "context" : "Hyperparameters For English corpora, we use RoBERTa (Liu et al., 2019) followed by a BiLSTM layer to produce the contextualized representations.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "For Chinese, we choose the BERT pretrained with whole word masking (Cui et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "All the models are trained by the AdamW optimizer (Loshchilov and Hutter, 2018) with a gradient clipping at L2-norm of 5.",
      "startOffset" : 50,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "For Chinese, we use MacBERT (Cui et al., 2020) of base and large sizes, and both improve the F1 score by 0.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 26,
      "context" : "How does boundary smoothing improve the model performance? We originally conjectured that boundary smoothing can de-noise the inconsistently annotated entity boundaries (Lukasik et al., 2020), but failed to find enough evidence – the performance improvement did not significantly increase when we injected boundary noises into the training data.",
      "startOffset" : 169,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "Conceptually, such targets are more compatible with the inductive bias of neural networks that prefers continuous solutions (Hornik et al., 1989).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 11,
      "context" : "As many theoretical studies regard the flatness as a promising predictor for model generalization (Hochreiter and Schmidhuber, 1997; Jiang et al., 2019), this result may explain why boundary smoothing can improve the model performance.",
      "startOffset" : 98,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "As many theoretical studies regard the flatness as a promising predictor for model generalization (Hochreiter and Schmidhuber, 1997; Jiang et al., 2019), this result may explain why boundary smoothing can improve the model performance.",
      "startOffset" : 98,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "Intuitively, such geometric property suggests that the underlying loss functions are easier to train (Li et al., 2018).",
      "startOffset" : 101,
      "endOffset" : 118
    } ],
    "year" : 0,
    "abstractText" : "Neural named entity recognition (NER) models may easily encounter the over-confidence issue, which degrades the performance and calibration. Inspired by label smoothing and driven by the ambiguity of boundary annotation in NER engineering, we propose boundary smoothing as a regularization technique for span-based neural NER models. It re-assigns entity probabilities from annotated spans to the surrounding ones. Built on a simple but strong baseline, our model achieves results better than or competitive with previous stateof-the-art systems on eight well-known NER benchmarks.1 Further empirical analysis suggests that boundary smoothing effectively mitigates over-confidence, improves model calibration, and brings flatter neural minima and more smoothed loss landscapes.",
    "creator" : null
  }
}
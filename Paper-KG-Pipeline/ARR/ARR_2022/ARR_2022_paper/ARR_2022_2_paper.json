{
  "name" : "ARR_2022_2_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "WatClaimCheck: A new Dataset for Claim Entailment and Inference",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The rise of social media has lead to a democratization of news, but it has also amplified issues related to fake news and misinformation. To that effect, many fact checking organizations (e.g., Politifact, Snopes, AFP Fact Check, Alt News, FactCheck.org, Africa Check, etc.) have emerged around the globe. They investigate debatable claims made by authorities, politicians, celebrities and the public. For each claim, they publish a review article with links to sources that support a verdict (e.g., true, partly true/false, false) about the veracity of the claim. Those reviews debunk false claims and mitigate the spread of misinformation. We consider a key NLP challenge in the context of automated fact checking: claim inference from premise articles. Note that determining the veracity of a claim without additional information is nearly impossible since claims are selected by professional fact checkers in part because their veracity is far from obvious and also because of their degree of controversy. To that effect, professional fact checkers invest a fair amount of time to research each claim by finding\nrelevant sources and publishing a review article that explains their verdict of the claim. Hence there is a natural entailment problem, whereby anyone who reads a review article should be able to arrive at the same verdict as the professional fact checker regarding the claim. Unlike many entailment tasks that consist of short text (e.g., pairs of utterances) that may be artificially generated or extracted, this is a natural and challenging entailment task that involves an entire document (review article) with an utterance (claim) that requires a certain degree of reading comprehension. We note that this entailment problem has been tackled in some previous work (Augenstein et al., 2019; Shu et al., 2018; Nakov et al., 2021) and although it is a challenging NLP problem, it does not correspond to the problem that professional fact checkers need to solve.\nIn this paper, we focus on the harder problem of claim inference from premise articles. This is part of the challenge that professional fact checkers face. They find premise articles that contain relevant facts and then infer the veracity of the claim based on those facts. Unlike many existing inference tasks where it is sufficient to use one or a few facts in a few sentences (Storks et al., 2019; Schlegel et al., 2020), information from a set of premise articles must be distilled and combined in non trivial ways to infer the veracity of a claim.\nWe assembled a dataset of 33,697 claims made between December 1996 and July 2021 with associated review articles, premise articles and claim verdicts. While some other datasets include claims with associated verdicts and in some cases review articles as well as search engine results, this is the first dataset that includes premise articles, therefore enabling the inference task described above.\nSince there are several premise articles for a given claim and each premise article may be long, a simple two-stage approach to identify relevant passages would consist of a lightweight retrieval technique in a first stage, followed by a heavyweight in-\nference technique applied to those passages. When the first stage fails to retrieve some key passages, then the inferred verdict will be negatively affected regardless of how good the second stage is. To that effect, several supervised dense passage retrieval techniques have been proposed in the past for question-answering (Karpukhin et al., 2020; Qu et al., 2021; Ren et al., 2021). Unfortunately, we cannot directly apply those techniques since we do not have labels for the relevant passages in our inference task. Instead, we show how to use the review articles to train a supervised dense retrieval technique that is then transferred to premise articles.\nThe contributions of the paper can be summarized as follows:\n• New dataset of claims with review and premise articles for claim inference in automated fact checking;\n• Novel use of review articles to transfer a dense retrieval technique to premise articles;\n• Experiments establishing the state of the art for claim inference.\nThe paper is organized as follows. Section 2 reviews previous work related to automated fact checking and claim verification. Section 3 describes the new dataset and summarizes the differences with previous datasets for claim verification. Section 4 describes a two-stage process to i) extract evidence sentences from premise articles and ii) infer the veracity of claims. This section also explains how to transfer a dense passage retrieval technique trained with review articles to premise articles. Section 5 reports the results for the claim veracity inference task. Finally, Section 6 concludes and discusses possible future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is an important line of work that focuses on claim verification. This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al., 2019; Shu et al., 2018; Nakov et al., 2021), as well as relevant articles returned by a search engine (Popat et al., 2018; Augenstein et al., 2019; Mishra and Setty, 2019). To the best of our\nknowledge, none of the existing work considers the problem of claim verification based on premise articles. There is an important distinction between articles returned by a search engine in previous work and the premise articles that we consider. The techniques that use a search engine to find articles related to a claim query the search engine after a fact checking website has published a review article and therefore end up retrieving articles that include the review article as well as other articles that summarize and/or discuss the verdict of the fact checking website. Hence they are tackling an entailment problem. In contrast, the premise articles that we consider are the source articles used by a fact checker before publishing a review article. Those articles contain relevant facts, but not a summary or discussion of the review article since they are published before the review article and in fact serve as premises for the review article.\nClosely related to claim verification is the problem of fake news detection. In this problem, the credibility of an entire news article is evaluated. The credibility of a news article can be estimated based on linguistic and textual features (Conroy et al., 2015; Reis et al., 2019; Li et al., 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al., 2015), knowledge graphs (Cui et al., 2020), inter-user behaviour dynamics (Gangireddy et al., 2020) or a combination of multiple modalities (Wang et al., 2020). Some techniques reorder the articles returned by a search engine based on their degree of credibility (Olteanu et al., 2013; Beylunioglu, 2020). An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019; Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article? The following surveys summarize existing work on fake news detection: (Kumar and Shah, 2018; Bondielli and Marcelloni, 2019)."
    }, {
      "heading" : "3 Fact Checking Dataset",
      "text" : ""
    }, {
      "heading" : "3.1 Data Collection",
      "text" : "We collect fact checked claims, along with a review articles, premise articles, and claim metadata from the following eight fact checking services: Politifact, Snopes, AFP Fact Check, Alt News, FactCheck.org, Africa Check, USA Today, and Full Fact. We utilize Google’s fact check tool APIs1 to collect the claims’ metadata for all previously listed\n1https://toolbox.google.com/factcheck/apis\nfact checking services except Politifact and Snopes. The claims’ metadata collected from Google’s fact check tool APIs include the claim review article URL, which is used to retrieve the claim review article. The claim review articles published by some of the fact checking services provide the premise article URLs in a separate section while others provide the URLs as inline links in the review article body. We carefully parse the article body, retrieving the premise article URLs used in the claim review article to justify the claim veracity. Finally, the premise article URLs are used to retrieve the premise articles. We try to directly retrieve the article where possible, but also use archive.org’s APIs in case the premise article URL is no longer available online. We follow the same general procedure for data collection from Politifact and Snopes except that instead of using Google’s fact check tool APIs for collecting claims and associated metadata, we directly crawl the respective websites to collect the data.\nWe only perform some basic cleanup to the collected data before inclusion in the dataset. This includes removing articles behind paywalls, removing claims with less than two premise articles, and removing non-textual premise sources. We obtain premise article text from their HTML pages by loading the HTML files into a text based web browser (Links browser) and then dumping the web page text into a text file. This allows us to bypass the CSS styling and JavaScript code included in the HTML pages and obtain only the text displayed to end users. Admittedly, this still does not eliminate the auxiliary text usually present in the web pages such as navigation links, footer text, recommended links, etc. We include both the HTML and text version of the premise articles. We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking. We also map the numerous claim veracity labels used by the fact checking websites into three broad labels: True, Partially True/False, and False."
    }, {
      "heading" : "3.2 WatClaimCheck Dataset",
      "text" : "The contributed dataset contains a total of 33,697 claims. We split those claims into the following three sets: training set containing 26,957 claims, validation set containing 3369 claims, and test set containing 3371 claims. For each claim in the dataset, we provide the following data: ID,\nClaimant, Claim, Claim Date, Reviewer Name, Reviewer Site, Review Article URL, Review Article Date, Review Article, Rating, Original Rating, Premise Articles and Premise Article URLs. Here Original Rating refers to the rating assigned by a fact checking organization and Rating corresponds to our mapping of the original rating to true, partly true/false and false. We provide both the HTML and extracted text files for the review and premise articles.\nFigure 1 shows the number of claims collected from each of the fact checking services. Figure 2 shows the claim rating distribution. We see that the claims in the Partially True/False and False categories significantly outnumber the claims in the True category. In reality, the number of true claims is much larger then the number of partially true/false and false claims, but fact checking services focus on debunking controversial claims and therefore the majority of the claims they investigate are false or partially true/false. This imbalance poses an important challenge for the models."
    }, {
      "heading" : "3.3 Comparison with existing Datasets",
      "text" : "We compare our proposed dataset with other publicly available fact checking related datasets in Table 1. We can broadly classify the fact checking datasets into two different categories: (1) veracity detection datasets based only on claim text and some metadata, but without supporting evidence documents (Wang, 2017a; Pérez-Rosas et al., 2018) and (2) datasets that provide claim text along with\nsupporting evidence and/or context documents. We observe that the datasets that provide some evidence or context documents can be further subcategorized: (1) datasets that provide social media posts and comments related to the claim (Mitra and Gilbert, 2015; Nakamura et al., 2020; Shu et al., 2018), (2) datasets that retrieve supporting evidence for the claims by performing a web search using queries obtained from lexical and semantic features of the claim text (Baly et al., 2018; Augenstein et al., 2019), and (3) datasets that provide Wikipedia pages as supporting evidence (Thorne et al., 2018). Our proposed dataset provides the documents cited by the professional fact checkers in the claim review article to justify their claim rating. We argue that our dataset reflects the real world task of automated veracity detection more truthfully due to the availability of the premise articles cited by the professional fact checkers in claim review articles. Although, social media posts and comments can sometimes be helpful in claim veracity detection they are rarely treated as authoritative sources of information. Using a web search to retrieve evidence documents for a claim is problematic due to the fact that once a fact checking service has fact checked a claim, we observe that multiple other news agency also publish articles referencing the original fact checking review article. Retrieving top-k web search results, typically retrieves those articles as well. This can indirectly leak the veracity label in the retrieved documents."
    }, {
      "heading" : "4 Models",
      "text" : "We develop a two-stage system to perform evidence based veracity detection. The first stage selects relevant sentence level evidence from the premise articles associated with a claim and the second stage performs claim veracity inference using the claim text and selected evidence sentences. For the first stage, we use and evaluate two different approaches. The first approach is the well-known and commonly used basic text retrieval technique called term frequency inverse document frequency (TFIDF). For the second approach, we propose a novel way to adapt dense passage retrieval techniques using the review articles for evidence sentence selection. In our experiments, the aforementioned dense passage retrieval technique outperforms TF-IDF text retrieval and leads to overall system performance improvements. The second stage consists of training deep learning models to perform claim\nveracity inference using the claim text and selected evidence. We utilize multiple deep learning models to perform claim veracity inference ranging from basic bi-directional recurrent networks to state of the art transformers."
    }, {
      "heading" : "4.1 Problem Formulation",
      "text" : "We represent a claim containing l tokens as Cn = {c1, c2, . . . , cl}, where n ∈ [1, N ] and N is the size of the dataset. Each claim is associated with multiple premise articles, we represent the k-th premise article associated with the n-th claim containing m sentences as An,k = {sPn,1, sPn,2, . . . , sPn,m} where sn,i represents the i-th sentence. Similarly, we represent the review article associated with the claim Cn containing m sentences by Rn = {sRn,1, sRn,2, . . . , sRn,m}. For a given claim Cn, we represent its ground truth veracity label by yn.\nWe cast the problem as a textual inference problem. Given a claim Cn and a set of associated premise articles A, our goal is to predict the ground truth veracity yn of the claim."
    }, {
      "heading" : "4.2 Stage-1: Evidence Sentence Extraction",
      "text" : "One of the key steps in the fact checking process performed by human professional fact checkers is examining the premise articles associated with a claim and extracting useful evidence from them to establish claim veracity. Our first stage seeks to perform a similar task. Each claim in our dataset has multiple associated premise articles with each article containing a large amount of text. Our goal in the first stage is to rank the evidence available in the associated premise articles at the sentence level and extract the ones which are most useful and impactful for veracity detection in the second stage. Our experiments show that an improvement in this stage directly contributes to an overall improvement in the veracity detection performance."
    }, {
      "heading" : "4.2.1 TF-IDF",
      "text" : "TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus. We use TF-IDF based similarity measure between the claim text and the premise article sentences to rank the sentence level evidence. Top ranked sentences are used in the second stage to perform veracity detection. This approach is similar to the one used by Thorne et al. (2018) to extract evidence sentences from Wikipedia articles for fact checking."
    }, {
      "heading" : "4.2.2 Dense Passage Retrieval",
      "text" : "We propose a novel way of adapting dense passage retrieval methods proposed by Karpukhin et al. (2020) for open domain question answering to the task of retrieving evidence sentences from premise articles. The dense passage retrieval method proposed by Karpukhin et al. (2020) uses a dual encoder architecture. Each encoder is implemented using BERT (Devlin et al., 2018). The question encoder EQ and the passage encoder EP embed a given question q and passage p into d-dimensional real-valued vectors. The similarity between the question and passage is defined as the dot product of their vectors:\nsim(q, p) = EQ(q)TEP (p) (1)\nThe model is then trained to learn embedding functions such that the similarity score between relevant pairs of questions and passages will be higher than irrelevant ones.\nWe adapt this method for our first stage by taking advantage of the fact that the review article published by fact checking websites along with a claim typically contains key evidence taken from the premise articles. The evidence taken from the premise articles is usually paraphrased in order to form a coherent argument in support of the claim veracity verdict.\nTo train the dense passage retrieval model for stage-1, we use the claims and the associated review articles in the training set of our dataset. We form positive pairs using the claim and the sentences from the associated review article. The negative pairs are formed using a claim and a sentence from a review article not associated with that claim.\nLet\nD = {〈Ci, sR+i,j , s R− i,1 , s R− i,2 , . . . , s R− i,n 〉 |Ri| j=1} N i=1\nbe the training data containing ∑N\ni=1 |Ri| instances where N is the number of claims in the training set, |Ri| is the number of sentences in the review article associated with the i-th claim. Each instance is made up of a claim Ci with one positive sentence from the associated review article sR+i,j and n randomly chosen negative sentences sR−i,k . We train the model by optimizing the negative log likelihood of the positive sentences:\nL(Ci, s R+ i,j , s R− i,1 , s R− i,2 , . . . , s R− i,n )\n= −log e sim(Ci,sR+i,j )\nesim(Ci,s R+ i,j ) + ∑n k=1 e sim(Ci,sR−i,k ) (2)\nWe evaluate the model using the claims and the associated review articles in the validation and test set. For model evaluation, we use the top-k recall rate for retrieving the review article sentences corresponding to the claims in the validation and test set using the similarity score. The review article sentences are retrieved from the corpus formed by all the sentences from every review article in the corresponding set.\nAfter training the model, we use the encoders to encode the claim text and the sentences of the associated premise articles. We compute the similarity score using the dot product between the encoded claim vector and the premise article sentences. We use the top scoring sentences as evidence sentences in the next stage to perform the claim veracity inference."
    }, {
      "heading" : "4.3 Stage-2: Claim Veracity Inference",
      "text" : "In this section, we describe how several popular sequence models are used to classify a claim as true, partly true/false or false based on the text of\nthe claim, the claimant and the evidence sentences extracted in stage 1."
    }, {
      "heading" : "4.3.1 Bi-LSTM and Bi-GRU",
      "text" : "We first consider bi-directional long short term memory (Bi-LSTM) networks and bi-directional gated recurrent units (Bi-GRUs). The evidence sentences of each premise article are concatenated with the claim and claimant, and then encoded by a Bi-LSTM or Bi-GRU into a latent vector. For N premise articles, the resulting N vectors are then averaged and passed through a softmax layer with 3 outputs corresponding to the predicted probabilities of true, partly true/false and false."
    }, {
      "heading" : "4.3.2 HAN",
      "text" : "Instead of concatenating the evidence sentences of each premise article into a long sequence, we can also use hierarchical attention networks (HANs) (Yang et al., 2016; Mishra and Setty, 2019) to compute sentence level embeddings that are then combined into article level embeddings. A HAN is used to embed each premise article with the claim as follows. At the bottom of a HAN, each sentence (claimant with claim text or each evidence sentence of the premise article) is embedded as a sequence of hidden vectors (one per word) by a bi-directional recurrent network (Bi-LSTM or BiGRU). Then, a word-level attention layer computes a sentence level embedding. Next, those embeddings are fed to another bi-directional recurrent network (Bi-LSTM or Bi-GRU) that computes a sequence of hidden vectors (one per sentence) and a sentence level attention layer computes an embedding for the document-claim pair. Finally, the embeddings of the document-claim pairs are averaged and passed through a softmax over the labels true, partly true/false and false."
    }, {
      "heading" : "4.3.3 Transformer",
      "text" : "We finetune a RoBERTa-base (Liu et al., 2019) model to perform claim veracity inference using the claim and the evidence sentences. We concatenate the claim text, the name of the claimant, and the evidence sentences extracted for that particular claim in the first stage to build a training data instance. The input sequence is encoded using the RoBERTa-base model and passed through a dense linear layer followed by a softmax layer to obtain the predicted claim veracity label distribution. We use the cross entropy loss function to train the model."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate the two-stage process and the algorithms described in the previous section on the claim inference problem with our new dataset."
    }, {
      "heading" : "5.1 Stage-1 Results",
      "text" : "In order to reduce the computational resources and memory requirements, we implement the encoders in the dense passage retrieval model using DistilRoBERTa (Dis). We use a batch size of 64 and the in-batch negatives technique as described in (Karpukhin et al., 2020).\nWe evaluate the stage-1 methods by comparing their performance using the top-k recall rate metric. The claim text is used to retrieve the ground truth review article sentences from the corpus containing all the sentences of all the review articles in the test set. We report the top-k recall rate for k = 10, 25, 50, 100 in Table 2. The results clearly show that the DPR (dense passage retrieval) method outperforms the TF-IDF similarity based method."
    }, {
      "heading" : "5.2 Stage-2 Results",
      "text" : "To evaluate whether the inference models in stage2 can do better with the inclusion of additional evidence sentences, we perform the experiments in stage-2 in two settings: Pooled and Averaged. Pooled: In this setting, for each claim we pool all the sentences from every associated premise article and rank them using the similarity score. The sentences with top scores are then used to perform claim veracity inference. For each claim, we get exactly one data instance. Averaged: This refers to the setting where we generate one data instance per claim and associated premise article. So, if a claim has m premise articles, we get m data instances. For each premise article associated with a claim, we score the sentences from that article and extract the top scoring sentences to form a data instance. During training, each data instance for a claim is used independently,\nbut during inference, we compute the average of the claim veracity prediction distributions of the data instances associated with a single claim. We show in our reported results that the inclusion of additional evidence in the form of m data instances per claim (instead of 1 data instance for the pooled setting) does improve the performance when the retrieval method of stage 1 is not very effective.\nWe use macro F1 as the evaluation metric. We report the results in Table 3. We report all the hyper parameters used in our experiments in the appendix. The best performance when doing the claim veracity inference is obtained by using the DPR model in the first stage and the RoBERTa-base model in the second stage. We also report results for claim entailment from the review articles as an upper bound on the accuracy that could be achieved for claim inference based on the premise articles."
    }, {
      "heading" : "5.2.1 Prequential Results",
      "text" : "We note that the traditional experimental setup of dividing a dataset at random into train, validation and test does not reflect the streaming nature of claims. When new topics arise (i.e., election, covid19), the nature of the claims and the premise articles changes. Randomly splitting the dataset into train/validation/test ensures that all claim topics are well represented across the train/validation/test splits, which would not be the case in practice. In reality, when a new topic arises, the test split may have new types of claims that are not well represented in the train/validation splits. To evaluate the effect of this distribution shift over time, we performed a prequential evaluation (Bifet et al., 2015). More precisely, we divide the dataset into subsets corresponding to periods of 6 months. We repeatedly evaluate the performance for each 6-month period by treating the claims in that period as the test set and the claims in previous periods as the train/validation sets. This corresponds to a realistic setting where a claim verification algorithm may be re-trained every 6 months on the data seen so far to predict the veracity of the claims for the next 6 months. Naturally, the time period between each re-training iteration may be shorter than 6 months in practice. We chose 6 months simply to ensure that the size of the test set would be large enough to obtain reliable results.\nFigure 3 shows the number of claims investigated in each 6-month period in our dataset. We note two peaks. The first one in 2016 corresponds to a sudden surge of claims investigated by some\nfact checking websites regarding India politics. The second peak in 2020 corresponds to the 2020 US presidential election and the start of the covid19 pandemic. Figure 4 shows the macro F1 results achieved by the top 4 algorithms with DPR evidence in each 6-month period. We note that the prequential results are significantly lower than the results in the DPR column of Table 3. This drop of accuracy is precisely due to the distribution shift of claims that naturally occurs over time. We also note a trend whereby the accuracy increases as time passes by. This is explained by the fact that more data is available for training in later time periods. We strongly recommend that future algorithms be evaluated in prequential mode since this evaluation setup is more realistic."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper introduces a new dataset for automated fact checking. It is the first dataset that includes premise articles used by professional fact checkers and therefore corresponds more closely to the task of claim veracity inference in automated fact checking. An important challenge is the extraction of relevant facts from the premise articles since it is not generally possible to apply heavyweight models on the entire content of all premise articles. To that effect, we described how to train the encoders of a dense passage retrieval technique with the review articles and then transfer the resulting retrieval technique to the premise articles. This increased the overall performance of the claim verification algorithms. We also performed a prequential evaluation that highlighted an important distribution shift that caused a significant drop in accuracy for all algorithms. We strongly recommend that future algorithms be evaluated in prequential mode. In fact, an important direction for future research would be to design algorithms based on transfer learning or domain generalization that can cope better with this distributional shift. We also note that the techniques that we evaluated are black boxes and therefore it is not clear how they do inference. Hence, another direction for future research would be to develop inference techniques that are explainable in the sense that they could provide explanations to the users to justify their veracity prediction for a claim."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Dataset Since the dataset is larger than the 200 Mb limit for the supplementary material, we include a sample corresponding to the data collected from March 15 to July 1, 2021 in the supplementary material. A link to the entire dataset will be made available once the paper is accepted.\nA.2 Hyperparameters The code will be made public once the paper is accepted. Table 4 lists the hyperparameters used for the Bi-LSTM, Bi-GRU and HANs. Table 5 describes the hyperparameters of the DPR technique in stage 1 and Table 6 lists the hyperparameters of RoBERTA-base in stage 2."
    } ],
    "references" : [ {
      "title" : "MultiFC: A real-world multi-domain dataset for evidence-based fact checking of claims",
      "author" : [ "Isabelle Augenstein", "Christina Lioma", "Dongsheng Wang", "Lucas Chaves Lima", "Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Augenstein et al\\.,? 2019",
      "shortCiteRegEx" : "Augenstein et al\\.",
      "year" : 2019
    }, {
      "title" : "Verification and implementation of language-based deception indicators in civil and criminal narratives",
      "author" : [ "Joan Bachenko", "Eileen Fitzpatrick", "Michael Schonwetter." ],
      "venue" : "Proceedings of the 22nd International Conference on Computational Linguis-",
      "citeRegEx" : "Bachenko et al\\.,? 2008",
      "shortCiteRegEx" : "Bachenko et al\\.",
      "year" : 2008
    }, {
      "title" : "Integrating stance detection and fact checking in a unified corpus",
      "author" : [ "Ramy Baly", "Mitra Mohtarami", "James Glass", "Lluís Màrquez", "Alessandro Moschitti", "Preslav Nakov." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Baly et al\\.,? 2018",
      "shortCiteRegEx" : "Baly et al\\.",
      "year" : 2018
    }, {
      "title" : "Using a credibility classifier to improve health-related information retrieval",
      "author" : [ "Fuat Can Beylunioglu." ],
      "venue" : "Master’s thesis, University of Waterloo.",
      "citeRegEx" : "Beylunioglu.,? 2020",
      "shortCiteRegEx" : "Beylunioglu.",
      "year" : 2020
    }, {
      "title" : "Efficient online evaluation of big data stream classifiers",
      "author" : [ "Albert Bifet", "Gianmarco de Francisci Morales", "Jesse Read", "Geoff Holmes", "Bernhard Pfahringer." ],
      "venue" : "Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and",
      "citeRegEx" : "Bifet et al\\.,? 2015",
      "shortCiteRegEx" : "Bifet et al\\.",
      "year" : 2015
    }, {
      "title" : "A survey on fake news and rumour detection techniques",
      "author" : [ "Alessandro Bondielli", "Francesco Marcelloni." ],
      "venue" : "Information Sciences, 497:38–55.",
      "citeRegEx" : "Bondielli and Marcelloni.,? 2019",
      "shortCiteRegEx" : "Bondielli and Marcelloni.",
      "year" : 2019
    }, {
      "title" : "Combining similarity features and deep representation learning for stance detection in the context of checking fake news",
      "author" : [ "Luís Borges", "Bruno Martins", "Pável Calado." ],
      "venue" : "Journal of Data and Information Quality (JDIQ), 11(3):1–26.",
      "citeRegEx" : "Borges et al\\.,? 2019",
      "shortCiteRegEx" : "Borges et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic deception detection: Methods for finding fake news",
      "author" : [ "Nadia K Conroy", "Victoria L Rubin", "Yimin Chen." ],
      "venue" : "Proceedings of the Association for Information Science and Technology, 52(1):1–4.",
      "citeRegEx" : "Conroy et al\\.,? 2015",
      "shortCiteRegEx" : "Conroy et al\\.",
      "year" : 2015
    }, {
      "title" : "Deterrent: Knowledge guided graph attention network",
      "author" : [ "Limeng Cui", "Haeseung Seo", "Maryam Tabar", "Fenglong Ma", "Suhang Wang", "Dongwon Lee" ],
      "venue" : null,
      "citeRegEx" : "Cui et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised fake news detection: A graph-based approach",
      "author" : [ "Siva Charan Reddy Gangireddy", "Cheng Long", "Tanmoy Chakraborty." ],
      "venue" : "Proceedings of the 31st ACM Conference on Hypertext and Social Media, pages 75–83.",
      "citeRegEx" : "Gangireddy et al\\.,? 2020",
      "shortCiteRegEx" : "Gangireddy et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2019 task 7: RumourEval, determining rumour veracity and support for rumours",
      "author" : [ "Genevieve Gorrell", "Elena Kochkina", "Maria Liakata", "Ahmet Aker", "Arkaitz Zubiaga", "Kalina Bontcheva", "Leon Derczynski." ],
      "venue" : "Proceedings of the 13th Inter-",
      "citeRegEx" : "Gorrell et al\\.,? 2019",
      "shortCiteRegEx" : "Gorrell et al\\.",
      "year" : 2019
    }, {
      "title" : "exBAKE: Automatic fake news detection model based on bidirectional encoder representations from transformers (BERT)",
      "author" : [ "Heejung Jwa", "Dongsuk Oh", "Kinam Park", "Jang Mook Kang", "Heuiseok Lim." ],
      "venue" : "Applied Sciences, 9(19):4062.",
      "citeRegEx" : "Jwa et al\\.,? 2019",
      "shortCiteRegEx" : "Jwa et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning hierarchical discourse-level structure for fake news detection",
      "author" : [ "Hamid Karimi", "Jiliang Tang." ],
      "venue" : "arXiv preprint arXiv:1903.07389.",
      "citeRegEx" : "Karimi and Tang.,? 2019",
      "shortCiteRegEx" : "Karimi and Tang.",
      "year" : 2019
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "False information on web and social media: A survey",
      "author" : [ "Srijan Kumar", "Neil Shah." ],
      "venue" : "arXiv preprint arXiv:1804.08559.",
      "citeRegEx" : "Kumar and Shah.,? 2018",
      "shortCiteRegEx" : "Kumar and Shah.",
      "year" : 2018
    }, {
      "title" : "Multi-level word features based on CNN for fake news detection in cultural communication",
      "author" : [ "Qian Li", "Qingyuan Hu", "Youshui Lu", "Yue Yang", "Jingxian Cheng." ],
      "venue" : "Personal and Ubiquitous Computing, pages 1–14.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The lie detector: Explorations in the automatic recognition of deceptive language",
      "author" : [ "Rada Mihalcea", "Carlo Strapparava." ],
      "venue" : "Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 309– 312, Suntec, Singapore. Association for Computa-",
      "citeRegEx" : "Mihalcea and Strapparava.,? 2009",
      "shortCiteRegEx" : "Mihalcea and Strapparava.",
      "year" : 2009
    }, {
      "title" : "Sadhan: Hierarchical attention networks to learn latent aspect embeddings for fake news detection",
      "author" : [ "Rahul Mishra", "Vinay Setty." ],
      "venue" : "Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval, pages 197–204.",
      "citeRegEx" : "Mishra and Setty.,? 2019",
      "shortCiteRegEx" : "Mishra and Setty.",
      "year" : 2019
    }, {
      "title" : "Credbank: A large-scale social media corpus with associated credibility annotations",
      "author" : [ "Tanushree Mitra", "Eric Gilbert." ],
      "venue" : "ICWSM.",
      "citeRegEx" : "Mitra and Gilbert.,? 2015",
      "shortCiteRegEx" : "Mitra and Gilbert.",
      "year" : 2015
    }, {
      "title" : "Fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection",
      "author" : [ "Kai Nakamura", "Sharon Levy", "William Yang Wang." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 6149–6157, Marseille,",
      "citeRegEx" : "Nakamura et al\\.,? 2020",
      "shortCiteRegEx" : "Nakamura et al\\.",
      "year" : 2020
    }, {
      "title" : "The clef-2021 checkthat! lab on detecting check-worthy claims, previously factchecked claims, and fake news",
      "author" : [ "Mandl." ],
      "venue" : "Advances in Information Retrieval, pages 639–649, Cham. Springer International Publishing.",
      "citeRegEx" : "Mandl.,? 2021",
      "shortCiteRegEx" : "Mandl.",
      "year" : 2021
    }, {
      "title" : "Web credibility: Features exploration and credibility prediction",
      "author" : [ "Alexandra Olteanu", "Stanislav Peshterliev", "Xin Liu", "Karl Aberer." ],
      "venue" : "European conference on information retrieval, pages 557–568. Springer.",
      "citeRegEx" : "Olteanu et al\\.,? 2013",
      "shortCiteRegEx" : "Olteanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Automatic detection of fake news",
      "author" : [ "Verónica Pérez-Rosas", "Bennett Kleinberg", "Alexandra Lefevre", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3391–3401, Santa Fe, New Mexico, USA.",
      "citeRegEx" : "Pérez.Rosas et al\\.,? 2018",
      "shortCiteRegEx" : "Pérez.Rosas et al\\.",
      "year" : 2018
    }, {
      "title" : "Credibility assessment of textual claims on the web",
      "author" : [ "Kashyap Popat", "Subhabrata Mukherjee", "Jannik Strötgen", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM ’16,",
      "citeRegEx" : "Popat et al\\.,? 2016",
      "shortCiteRegEx" : "Popat et al\\.",
      "year" : 2016
    }, {
      "title" : "Where the truth lies: Explaining the credibility of emerging claims on the web and social media",
      "author" : [ "Kashyap Popat", "Subhabrata Mukherjee", "Jannik Strötgen", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web",
      "citeRegEx" : "Popat et al\\.,? 2017",
      "shortCiteRegEx" : "Popat et al\\.",
      "year" : 2017
    }, {
      "title" : "DeClarE: Debunking fake news and false claims using evidence-aware deep learning",
      "author" : [ "Kashyap Popat", "Subhabrata Mukherjee", "Andrew Yates", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Popat et al\\.,? 2018",
      "shortCiteRegEx" : "Popat et al\\.",
      "year" : 2018
    }, {
      "title" : "Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering",
      "author" : [ "Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Wayne Xin Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Qu et al\\.,? 2021",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2021
    }, {
      "title" : "Truth of varying shades: Analyzing language in fake news and political fact-checking",
      "author" : [ "Hannah Rashkin", "Eunsol Choi", "Jin Yea Jang", "Svitlana Volkova", "Yejin Choi." ],
      "venue" : "Proceedings of the 2017 conference on empirical methods in natural language",
      "citeRegEx" : "Rashkin et al\\.,? 2017",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2017
    }, {
      "title" : "Explainable machine learning for fake news detection",
      "author" : [ "Julio CS Reis", "André Correia", "Fabrício Murai", "Adriano Veloso", "Fabrício Benevenuto." ],
      "venue" : "Proceedings of the 10th ACM Conference on Web Science, pages 17–26.",
      "citeRegEx" : "Reis et al\\.,? 2019",
      "shortCiteRegEx" : "Reis et al\\.",
      "year" : 2019
    }, {
      "title" : "Pair: Leveraging passage-centric similarity relation for improving dense passage retrieval",
      "author" : [ "Ruiyang Ren", "Shangwen Lv", "Yingqi Qu", "Jing Liu", "Wayne Xin Zhao", "Qiaoqiao She", "Hua Wu", "Haifeng Wang", "Ji-Rong Wen." ],
      "venue" : "Findings of the Associ-",
      "citeRegEx" : "Ren et al\\.,? 2021",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2021
    }, {
      "title" : "Beyond leaderboards: A survey of methods for revealing weaknesses in natural language inference data and models",
      "author" : [ "Viktor Schlegel", "Goran Nenadic", "Riza BatistaNavarro." ],
      "venue" : "arXiv preprint arXiv:2005.14709.",
      "citeRegEx" : "Schlegel et al\\.,? 2020",
      "shortCiteRegEx" : "Schlegel et al\\.",
      "year" : 2020
    }, {
      "title" : "Fakenewsnet: A data repository with news content, social context and dynamic information for studying fake news on social media",
      "author" : [ "Kai Shu", "Deepak Mahudeswaran", "Suhang Wang", "Dongwon Lee", "Huan Liu." ],
      "venue" : "CoRR, abs/1809.01286.",
      "citeRegEx" : "Shu et al\\.,? 2018",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2018
    }, {
      "title" : "Recent advances in natural language inference: A survey of benchmarks, resources, and approaches",
      "author" : [ "Shane Storks", "Qiaozi Gao", "Joyce Y Chai." ],
      "venue" : "arXiv preprint arXiv:1904.01172.",
      "citeRegEx" : "Storks et al\\.,? 2019",
      "shortCiteRegEx" : "Storks et al\\.",
      "year" : 2019
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and VERification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "Fact checking: Task definition and dataset construction",
      "author" : [ "Andreas Vlachos", "Sebastian Riedel." ],
      "venue" : "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18–22, Baltimore, MD, USA. Associa-",
      "citeRegEx" : "Vlachos and Riedel.,? 2014",
      "shortCiteRegEx" : "Vlachos and Riedel.",
      "year" : 2014
    }, {
      "title" : "liar, liar pants on fire”: A new benchmark dataset for fake news detection",
      "author" : [ "William Yang Wang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422–426, Vancouver, Canada.",
      "citeRegEx" : "Wang.,? 2017a",
      "shortCiteRegEx" : "Wang.",
      "year" : 2017
    }, {
      "title" : "liar, liar pants on fire”: A new benchmark dataset for fake news detection",
      "author" : [ "William Yang Wang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422–426.",
      "citeRegEx" : "Wang.,? 2017b",
      "shortCiteRegEx" : "Wang.",
      "year" : 2017
    }, {
      "title" : "Fake news detection via knowledge-driven multimodal graph convolutional networks",
      "author" : [ "Youze Wang", "Shengsheng Qian", "Jun Hu", "Quan Fang", "Changsheng Xu." ],
      "venue" : "Proceedings of the 2020 International Conference on Multimedia Retrieval, pages 540–",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 conference of the North American chapter of the association for computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Analysing how people orient to and spread rumours in social media by looking at conversational threads",
      "author" : [ "Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Peter Tolmie." ],
      "venue" : "PLOS ONE, 11(3):1–29.",
      "citeRegEx" : "Zubiaga et al\\.,? 2016",
      "shortCiteRegEx" : "Zubiaga et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We note that this entailment problem has been tackled in some previous work (Augenstein et al., 2019; Shu et al., 2018; Nakov et al., 2021) and although it is a challenging",
      "startOffset" : 76,
      "endOffset" : 139
    }, {
      "referenceID" : 33,
      "context" : "We note that this entailment problem has been tackled in some previous work (Augenstein et al., 2019; Shu et al., 2018; Nakov et al., 2021) and although it is a challenging",
      "startOffset" : 76,
      "endOffset" : 139
    }, {
      "referenceID" : 34,
      "context" : "Unlike many existing inference tasks where it is sufficient to use one or a few facts in a few sentences (Storks et al., 2019; Schlegel et al., 2020), information from a set of premise articles must be distilled and combined in non trivial ways to infer the veracity of a claim.",
      "startOffset" : 105,
      "endOffset" : 149
    }, {
      "referenceID" : 32,
      "context" : "Unlike many existing inference tasks where it is sufficient to use one or a few facts in a few sentences (Storks et al., 2019; Schlegel et al., 2020), information from a set of premise articles must be distilled and combined in non trivial ways to infer the veracity of a claim.",
      "startOffset" : 105,
      "endOffset" : 149
    }, {
      "referenceID" : 29,
      "context" : "This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al.",
      "startOffset" : 98,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : ", 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 38,
      "context" : ", name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al.",
      "startOffset" : 50,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : ", name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al., 2019; Shu et al., 2018; Nakov et al., 2021), as well as relevant articles returned by a search engine (Popat et al.",
      "startOffset" : 81,
      "endOffset" : 144
    }, {
      "referenceID" : 33,
      "context" : ", name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al., 2019; Shu et al., 2018; Nakov et al., 2021), as well as relevant articles returned by a search engine (Popat et al.",
      "startOffset" : 81,
      "endOffset" : 144
    }, {
      "referenceID" : 27,
      "context" : ", 2021), as well as relevant articles returned by a search engine (Popat et al., 2018; Augenstein et al., 2019; Mishra and Setty, 2019).",
      "startOffset" : 66,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : ", 2021), as well as relevant articles returned by a search engine (Popat et al., 2018; Augenstein et al., 2019; Mishra and Setty, 2019).",
      "startOffset" : 66,
      "endOffset" : 135
    }, {
      "referenceID" : 19,
      "context" : ", 2021), as well as relevant articles returned by a search engine (Popat et al., 2018; Augenstein et al., 2019; Mishra and Setty, 2019).",
      "startOffset" : 66,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "The credibility of a news article can be estimated based on linguistic and textual features (Conroy et al., 2015; Reis et al., 2019; Li et al., 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al.",
      "startOffset" : 92,
      "endOffset" : 149
    }, {
      "referenceID" : 30,
      "context" : "The credibility of a news article can be estimated based on linguistic and textual features (Conroy et al., 2015; Reis et al., 2019; Li et al., 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al.",
      "startOffset" : 92,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : "The credibility of a news article can be estimated based on linguistic and textual features (Conroy et al., 2015; Reis et al., 2019; Li et al., 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al.",
      "startOffset" : 92,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : ", 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : ", 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al., 2015), knowledge",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "graphs (Cui et al., 2020), inter-user behaviour dynamics (Gangireddy et al.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : ", 2020), inter-user behaviour dynamics (Gangireddy et al., 2020) or a combination of multiple modalities (Wang et al.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : ", 2020) or a combination of multiple modalities (Wang et al., 2020).",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019; Jwa et al., 2019), i.",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 12,
      "context" : "An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019; Jwa et al., 2019), i.",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : ", does the content of an article agree or disagree with the title of the article? The following surveys summarize existing work on fake news detection: (Kumar and Shah, 2018; Bondielli and Marcelloni, 2019).",
      "startOffset" : 152,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : ", does the content of an article agree or disagree with the title of the article? The following surveys summarize existing work on fake news detection: (Kumar and Shah, 2018; Bondielli and Marcelloni, 2019).",
      "startOffset" : 152,
      "endOffset" : 206
    }, {
      "referenceID" : 37,
      "context" : "We can broadly classify the fact checking datasets into two different categories: (1) veracity detection datasets based only on claim text and some metadata, but without supporting evidence documents (Wang, 2017a; Pérez-Rosas et al., 2018) and (2) datasets that provide claim text along with",
      "startOffset" : 200,
      "endOffset" : 239
    }, {
      "referenceID" : 24,
      "context" : "We can broadly classify the fact checking datasets into two different categories: (1) veracity detection datasets based only on claim text and some metadata, but without supporting evidence documents (Wang, 2017a; Pérez-Rosas et al., 2018) and (2) datasets that provide claim text along with",
      "startOffset" : 200,
      "endOffset" : 239
    }, {
      "referenceID" : 20,
      "context" : "dence or context documents can be further subcategorized: (1) datasets that provide social media posts and comments related to the claim (Mitra and Gilbert, 2015; Nakamura et al., 2020; Shu et al., 2018), (2) datasets that retrieve supporting",
      "startOffset" : 137,
      "endOffset" : 203
    }, {
      "referenceID" : 21,
      "context" : "dence or context documents can be further subcategorized: (1) datasets that provide social media posts and comments related to the claim (Mitra and Gilbert, 2015; Nakamura et al., 2020; Shu et al., 2018), (2) datasets that retrieve supporting",
      "startOffset" : 137,
      "endOffset" : 203
    }, {
      "referenceID" : 33,
      "context" : "dence or context documents can be further subcategorized: (1) datasets that provide social media posts and comments related to the claim (Mitra and Gilbert, 2015; Nakamura et al., 2020; Shu et al., 2018), (2) datasets that retrieve supporting",
      "startOffset" : 137,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "evidence for the claims by performing a web search using queries obtained from lexical and semantic features of the claim text (Baly et al., 2018; Augenstein et al., 2019), and (3) datasets that provide Wikipedia pages as supporting evidence (Thorne",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "evidence for the claims by performing a web search using queries obtained from lexical and semantic features of the claim text (Baly et al., 2018; Augenstein et al., 2019), and (3) datasets that provide Wikipedia pages as supporting evidence (Thorne",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 40,
      "context" : "of each premise article into a long sequence, we can also use hierarchical attention networks (HANs) (Yang et al., 2016; Mishra and Setty, 2019) to compute sentence level embeddings that are then combined into article level embeddings.",
      "startOffset" : 101,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : "of each premise article into a long sequence, we can also use hierarchical attention networks (HANs) (Yang et al., 2016; Mishra and Setty, 2019) to compute sentence level embeddings that are then combined into article level embeddings.",
      "startOffset" : 101,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : "We finetune a RoBERTa-base (Liu et al., 2019) model to perform claim veracity inference using the claim and the evidence sentences.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "We use a batch size of 64 and the in-batch negatives technique as described in (Karpukhin et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "To evaluate the effect of this distribution shift over time, we performed a prequential evaluation (Bifet et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 119
    } ],
    "year" : 0,
    "abstractText" : "We contribute a new dataset for the task of automated fact checking and an evaluation of state of the art algorithms. The dataset includes claims (from speeches, interviews, social media and news articles), review articles published by professional fact checkers and premise articles used by those professional fact checkers to support their review and verify the veracity of the claims. An important challenge in the use of premise articles is the identification of relevant passages that will help to infer the veracity of a claim. We show that transferring a dense passage retrieval model trained with review articles improves the retrieval quality of passages in premise articles. We report results for the prediction of claim veracity by inference from premise articles.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_49_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Given some text (typically, a sentence) t mentioning an entity pair (e1, e2), the goal of relation extraction (RE) is to predict the relationships between e1 and e2 that can be inferred from t. Let B(e1, e2) denote the set of all sentences (bag) in the corpus mentioning e1 and e2 and let R(e1, e2) denote all relations from e1 to e2 in a KB. Distant supervision (DS) trains RE models given B(e1, e2) and R(e1, e2), without sentence level annotation (Mintz et al., 2009). Most DS-RE models use the “at-least one” assumption: ∀r ∈ R(e1, e2), ∃tr ∈ B(e1, e2) such that tr expresses (e1, r, e2).\nRecent neural approaches to DS-RE encode each sentence t ∈ B(e1, e2) and then aggregate sentence embeddings using an aggregation operator – the common operator being intra-bag attention (Lin et al., 2016). Various models differ in their approach to encoding (e.g., PCNNs, GCNs, BERT) and their loss functions (e.g., contrastive learning, MLM), but agree on the design choice of encoding each sentence independently of the others\n(Vashishth et al., 2018; Alt et al., 2019; Christou and Tsoumakas, 2021; Chen et al., 2021). We posit that this choice leads to a suboptimal usage of the available data – information from other sentences might help in better encoding a given sentence.\nWe explore this hypothesis by developing a simple baseline solution. We first construct a passage P (e1, e2) by concatenating all sentences in B(e1, e2). We then encode the whole passage through BERT (Devlin et al., 2019) (or mBERT for multilingual setting). This produces a contextualized embedding of every token in the bag. To make these embeddings aware of the candidate relation, we take a (trained) relation query vector, r, to generate a relation-aware summary of the whole passage using attention. This is then used to predict whether (e1, r, e2) is a valid prediction.\nDespite its simplicity, our baseline has some conceptual advantages. First, each token is able to exchange information with other tokens from other sentences in the bag – so the embeddings are likely more informed. Second, in principle, the model may be able to relax a part of the at-least-one assumption. For example, if no sentence individually expresses a relation, but if multiple facts in different sentences collectively predict the relation, our model may be able to learn to extract that.\nWe name our baseline model Passage-Attended Relation Extraction, PARE (mPARE for multilingual DS-RE). We experiment on four DS-RE datasets – three in English, NYT-10d (Riedel et al., 2010), NYT-10m, and Wiki-20m (Gao et al., 2021), and one multilingual, DiS-ReX (Bhartiya et al., 2021). We find that in all four datasets, our proposed baseline significantly outperforms existing state of the art, yielding up to 5 point AUC gain. Further attention analysis and ablations provide additional insight into model performance. We release our code for reproducibility. We believe that our work represents a simple but strong baseline that can form the basis for further DS-RE research."
    }, {
      "heading" : "2 Related Work",
      "text" : "Monolingual DS-RE: Early works in DS-RE build probabilistic graphical models for the task (e.g., (Hoffmann et al., 2011; Ritter et al., 2013). Most later works follow the multi-instance multilabel learning framework (Surdeanu et al., 2012) in which there are multiple labels associated with a bag, and the model is trained with at-least-one assumption. Most neural models for the task encode each sentence separately, e.g., using Piecewise CNN (Zeng et al., 2015), Graph Convolution Net (e.g., RESIDE (Vashishth et al., 2018)), GPT (DISTRE (Alt et al., 2019)) and BERT (REDSandT (Christou and Tsoumakas, 2021), CIL (Chen et al., 2021)). They all aggregate embeddings using intra-bag attention (Lin et al., 2016). Beyond Binary Cross Entropy, additional loss terms include masked language model pre-training (DISTRE, CIL), RL loss (Qin et al., 2018), and auxiliary contrastive learning (CIL). We show that PARE is competitive with DISTRE, RESIDE, CIL, and other natural baselines, without using additional pre-training, side information or auxiliary losses during training, unlike some comparison models.\nTo evaluate DS-RE, at test time, the model makes a prediction for an unseen bag. Unfortunately, most popular DS-RE dataset (NYT-10d) has a noisy test set, as it is automatically annotated (Riedel et al., 2010). Recently Gao et al. (2021) has released NYT-10m and Wiki-20m, which have manually annotated test sets. We use all three datasets in our work.\nMultilingual DS-RE: A bilingual DS-RE model named MNRE (tested on English and Mandarin) introduced cross-lingual attention in languagespecific CNN encoders (Lin et al., 2017). Recently, Bhartiya et al. (2021) has released a dataset, DiS-ReX, for four languages – English, Spanish, German and French. We compare mPARE against the state of the art on DiS-ReX, which combines MNRE architecture with mBERT encoder. See Appendix E for details on all DS-RE models."
    }, {
      "heading" : "3 Passage Attended Relation Extraction",
      "text" : "PARE explores the value of cross-sentence attention during encoding time. It uses a sequence of three key steps: passage construction, encoding and summarization, followed by prediction. Figure 1 illustrates these for a three-sentence bag. Passage Construction constructs a passage P (e1, e2) from sentences t ∈ B(e1, e2). The construction process uses a sequential sampling of sentences in the bag without replacement. It terminates if (a) adding any new sentence would exceed the maximum number of tokens allowed by the encoder (512 tokens for BERT), or (b) all sentences from the bag have been sampled. Passage Encoding takes the constructed passage and sends it to an encoder (BERT or mBERT) to generate contextualized embeddings zj of every token wj in the passage. For this, it first creates an encoder input. The input starts with the [CLS] token, followed by each passage sentence separated by [SEP], and pads all remaining tokens with [PAD]. Moreover, following best-practices in RE (Han et al., 2019), each mention of e1 and e2 in the passage are surrounded by special entity marker tokens <e1>,</e1>, and <e2>,</e2>, respectively. Passage Summarization maintains a (randomlyinitialized) query vector ri for every relation ri. It then computes αij , the normalized attention of ri on each token wj , using dot-product attention. Finally, it computes a relation-attended summary of the whole passage z(e1,ri,e2) = ∑j=L j=1 α i jzj , where L is the input length. We note that this summation also aggregates embeddings of [CLS], [SEP], [PAD], as well as entity marker tokens. Tuple Classifier passes z(e1,ri,e2) through an MLP followed by Sigmoid activation to return the probability pi of the triple (e1, ri, e2). This MLP is shared across all relation classes. At inference, a positive prediction is made if pi > threshold (0.5). Loss Function is simply Binary Cross Entropy between gold and predicted label set for each bag. No additional loss terms are used."
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : "We compare PARE and mPARE against the state of the art models on the respective datasets. We also perform ablations and analyses to understand model behavior and reasons for its performance. Datasets and Evaluation Metrics: We evaluate PARE on three English datasets: NYT-10d, NYT10m, Wiki-20m. mPARE is compared using the DiS-ReX benchmark. Data statistics are in Table 2, with more details in Appendix C. We use the evaluation metrics prevalent in literature for each dataset. These include AUC: area under the precision-recall curve, M-F1: macro-F1, µ-F1: micro-F1, and P@M : average of P@100, P@200 and P@300, where P@k denotes precision calculated over a model’s k most confidently predicted triples."
    }, {
      "heading" : "Comparison Models and Hyperparameters:",
      "text" : "Since there is substantial body of work on NYT10d, we compare against several recent models: RESIDE, DISTRE, REDSandT and the latest state of the art, CIL. For NYT-10m and Wiki-20m, we report comparisons against models in the original paper (Gao et al., 2021), and also additionally run CIL for a stronger comparison. For DiS-ReX, we compare against mBERT based models. See Appendix E for details. For PARE and mPARE, we use base-uncased checkpoints for BERT and mBERT, respectively. Hyperparameters are set based on a simple grid search over devsets. (see Appendix A)."
    }, {
      "heading" : "4.1 Comparisons against State of the Art",
      "text" : "obtains a 4.8 pt AUC gain against mBERT+MNRE. P-R curve in Figure 3 shows that it convincingly outperforms others across the entire domain of recall values. We provide language-wise and relationwise metrics in Appendix L – the gains are consistent on all languages and nearly all relations."
    }, {
      "heading" : "4.2 Analysis and Ablations",
      "text" : "Generalizing to Unseen KB: Recently, Ribeiro et al. (2020) has proposed a robustness study in which entity names in a bag are replaced by other names (from the same type) to test whether the extractor is indeed reading the text, or is simply overfitting on the regularities of the given KB. We also implement a similar robustness study (details in Appendix K), where entity replacement results in an entity-pair bag that does not exist in the original KB. We find that on this modified NYT-10m, all models suffer a drop in performance, suggesting that models are not as robust as we intend them to be. We, however, note that CIL suffers a 28.1% drop in AUC performance, but PARE remains more robust with only a 16.8% drop. We hypothesize that this may be because of PARE’s design choice of attending on all words for a given relation, which could reduce its focus on entity names themselves.\nScaling with Size of Entity-Pair Bags: Due to truncation when the number of tokens in a bag exceed 512 (limit for BERT), one would assume that the PARE may not be suited for cases where the number of tokens in a bag is large. To study this, we divide the test set of NYT-10m into 7 different groups based on the number of tokens present in the untruncated passage (Appendix J). We find that PARE shows consistent 2 to 5 pt AUC gains against CIL for all groups except the smallest group. This is not surprising, since for smallest group, there is likely only one sentence in a bag, and PARE would not gain from inter-sentence attention. For large bags, relevant information is likely already present in truncated passage, due to redundancy.\nAttention Patterns: In PARE, each relation class has a trainable query vector, which attends on every token. The attention scores could give us some insight about the words the model is focusing on. We observe that for a candidate relation that is not a gold label for a particular bag, surprisingly, the highest attention scores are obtained by [PAD] tokens. In fact, for such bags, on an average, roughly 90% of the attention weight goes to [PAD] tokens, whereas this number is only 0.1% when the rela-\ntion is in the gold set (see Appendices H and I). We find this to be an example of model ingenuity – PARE seems to have creatively learned that whenever the most appropriate words for a relation are not present, it could simply attend on [PAD] embeddings, which may lead to similar attended summaries, which may be easily decoded to a low probability of tuple validity. In fact, as a further test, we perform an ablation where we disallow relation query vectors to attend on [PAD] tokens – this results in an over 3 pt drop in AUC on NYT-10d, indicating the importance of padding for prediction. Ablations: We perform further ablations of the model by removing [SEP] tokens, entity markers and removing the relation-attention step that computes a summary (instead using [CLS] token for predicting each relation). PARE loses significantly in performance in each ablation obtaining 49.4, 14.9 and 46.1 AUC, respectively (as against 51.8 for full model) on NYT-10d. The critical importance of entity markers is not surprising, since without them the model does not know what is the entity-pair it is predicting for. We also notice a very significant gain due to relation attention, suggesting that this is an important step for the model – it allows focus on specific words relevant for predicting a relation. More details on this experiment in Appendix G. Effect of Sentence Order: We build 20 random passages per bag (by varying sentence order and also which sentences get selected if passage needs truncation). On all four datasets (Appendix M), we find that the standard deviation to be negligible."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduce PARE, a simple baseline for the task of distantly supervised relation extraction. It converts a bag of sentences containing an entity-pair into a passage. Contextual embeddings from encoding the passage can potentially benefit from attention across words from different sentences. It then creates an relation-attended summary of all contextual embeddings, which is decoded for tuple validity. Our experiments demonstrate that this simple baseline produces very strong results for the task, and outperforms existing top models by varying margins across four datasets in monolingual and multilingual settings. Several experiments for studying model behavior show its consistent performance across settings. We posit that our framework would serve as a strong backbone for further research in the field of DS-RE."
    }, {
      "heading" : "A Experimental Settings",
      "text" : "We train and test our model on two NVIDIA GeForce GTX 1080 Ti cards. We use a linear LR scheduler having weight decay of 1e-5 with AdamW (Loshchilov and Hutter, 2017; Kingma and Ba, 2014) as the optimizer. Our implementation uses PyTorch (Paszke et al., 2019), the Transformers library (Wolf et al., 2019) and OpenNRE 1 (Han et al., 2019). We use bert-base-uncased checkpoint for BERT initialization in the mono-lingual setting. For multi-lingual setting, we use bert-base-multilingual-uncased.\nFor hyperparameter tuning, we perform grid search over {1e-5, 2e-5} for learning rate and {16, 32, 64} for batch size and select the best performing configuration for each dataset.\nPARE takes 2 epochs to converge on NYT-10d (152 mins/epoch), 3 epochs for NYT-10m (138 mins/epoch), 2 epochs for Wiki-20m (166 mins/epoch) and 4 epochs for DiS-ReX (220 mins/epoch).\nThe numbers we report for the baselines come from their respective papers. We obtained the code base of CIL, BERT+Att, BERT+Avg, BERT+One from their respective authors, so that we could run them on additional datasets. We were able to replicate same numbers as reported in their papers. We trained those models on other datasets as well by carefully tuning the bag size hyperparameter."
    }, {
      "heading" : "B Sizes of different models",
      "text" : "We calculate no. of trainable parameters for each model. For fair comparison, we exclude the parameters of BERT encoder while reporting these numbers for every model. We stress that the number of parameters in BERT is same for all models including us i.e. 109482240 because all use the same bert-base-uncased checkpoint.\nWe note that the key reason why other models have significantly higher parameters is because they use entity pooling for constructing instance representations. Here, the encoded representations of tokens corresponding to the span of head and tail entity mentions are pooled together, followed by concatenation and passing through a linear layer of size 2D× 2D (where D is the dimension of the encoded token). For BERT, D = 768 due to which the size of the linear layer is = 1536*1536 = 2359296. This causes the huge difference in number of parameters between our model and their models. However, it should be noted that the entity pooling generally performs better than using [CLS] embedding for encoding instances as has been shown in recent works (Ni et al., 2020)."
    }, {
      "heading" : "C Dataset Details",
      "text" : "We evaluate our proposed model on four different datasets: NYT-10d (Riedel et al., 2010), NYT-10m (Gao et al., 2021), Wiki-20m (Gao et al., 2021) and DiS-ReX (Bhartiya et al., 2021). The statistics for each of the datasets is present in table 2."
    }, {
      "heading" : "NYT-10d",
      "text" : "NYT-10d is the most popular dataset for monolingual DS-RE, constructed by aligning Freebase entities to the New York Times Corpus. The train and test splits are both distantly supervised."
    }, {
      "heading" : "NYT-10m",
      "text" : "1https://github.com/thunlp/OpenNRE\nNYT-10m is a recently released dataset to train and evaluate models for monolingual DS-RE. The dataset is built from the same New York Times Corpus and the Freebase KB but with a new relation ontology and a manually annotated test set. It aims to tackle the existing problems with the NYT-10d dataset by 1) establishing a public validation set 2) establishing consistency among the relation classes present in the train and test set 3) providing a high quality, manually labeled test set."
    }, {
      "heading" : "Wiki-20m",
      "text" : "Wiki-20m is also a recently released dataset for training DS-RE models and evaluating them on manually annotated a test set. The test set in this case corresponds to the Wiki80 dataset (Han et al., 2019). The relation ontology of Wiki80 is used to re-structure the Wiki20 DS-RE dataset (Han et al., 2020), from which the training and validation splits are created. It is made sure that their is no overlap between the instances present in the testing and the training and validation sets."
    }, {
      "heading" : "DiS-ReX",
      "text" : "DiS-ReX is a recently released benchmarking dataset for training and evaluating DS-RE models on instances spanning multiple languages. The entities present in this dataset are linked across the different languages which means that a bag can contain sentences from more than one languages. The training, validation and testing sets present in the dataset are constructed in a way that there is no head and tail entity pair overlap between the bags present in any two different sets.\nWe obtain the first three datasets from OpenNRE and DiS-ReX from their official repository."
    }, {
      "heading" : "D Description of Intra-Bag attention",
      "text" : "Let t1, t2, ..., tn denote n instances sampled from B(e1, e2). In all models using intra-bag attention for instance-aggregation, each ti is independently encoded to form the instance representation, E(ti), following which the relation triple representation Br for the triple (e1, e2, r) is given by Br = ∑i=n i=0 α r iE(ti). Here r is any one of the relation classes present in the dataset and αri is the normalized attention score allotted to instance representationE(ti) by relation query vector−→r for relation r. The model then predicts whether the relation triple is a valid one by sending each Br through a feed-forward neural network. In some variants, −→r is replaced with a shared query vector for all relation-classes, −→q , resulting in a bag-representation B corresponding to (e1, e2) as opposed to triple-representation."
    }, {
      "heading" : "E Baselines",
      "text" : "The details for each baseline is provided below:"
    }, {
      "heading" : "PCNN-Att",
      "text" : "Lin et al. (2016) proposed the intra-bag attention aggregation scheme in 2016, obtaining the then state-of-the-art performance on NYT-10d using a piecewise convolutional neural network (PCNN (Zeng et al., 2015))."
    }, {
      "heading" : "RESIDE",
      "text" : "Vashishth et al. (2018) proposed RESIDE which uses side-information (in the form of entity types and relational aliases) in addition to sentences present in the dataset. The model uses intra-bag attention with a shared query vector to combine the representations of each instance in the bag. The sentence representations are obtained using a Graph Convolutional Network (GCN) encoder."
    }, {
      "heading" : "DISTRE",
      "text" : "Alt et al. (2019) propose the use of a pre-trained transformer based language model (OpenAI GPT Radford et al. (2018)) for the task of DS-RE. The model uses intra-bag attention for the instance aggregation step."
    }, {
      "heading" : "REDSandT",
      "text" : "Christou and Tsoumakas (2021) propose the use of a BERT encoder for DS-RE by using sub-tree parse of\nthe input sentence along with special entity type markers for the entity mentions in the text. The model uses intra-bag attention for the instance aggregation step."
    }, {
      "heading" : "CIL",
      "text" : "Chen et al. (2021) propose the use of Masked Language Modeling (MLM) and Contrastive Learning (CL) losses as auxilliary losses to train a BERT encoder + Intra-bag attention aggregator for the task."
    }, {
      "heading" : "BERT+Att/mBERT+Att",
      "text" : "The model uses intra-bag attention aggregator on top of a BERT/mBERT encoder."
    }, {
      "heading" : "BERT+Avg/mBERT+Avg",
      "text" : "The model uses “Average” aggregator which weighs each instance representation uniformly, hence denoting bag-representation as the average of instance-representations."
    }, {
      "heading" : "BERT+One/mBERT+One",
      "text" : "The model independently performs multi-label classification on each instance present in the bag and then aggregates the classification results by performing class-wise max-pooling (over sentence scores). In essence, the “One” aggregator ends up picking one instance for each class (the one which denotes the highest confidence for that particular class), hence the name.\nmBERT+MNRE The MNRE aggregator was originally introduced by Lin et al. (2017) and used with a shared mBERT encoder by Bhartiya et al. (2021) 2. The model assigns a query vector for each (relation, language) tuple. A bag is divided into sub-bags where each sub-bag contains the instances of the same language. In essence, a bag has L sub-bags and each relation class corresponds to L query vectors, where L denotes the number of languages present in the dataset. These are then used to construct L2 triple representations (using intra-bag attention aggregation on each (sub-bag,query vector) pair for a candidate relation) which are then scored independently. The final confidence score for a triple is the average of L2 triple scores."
    }, {
      "heading" : "F Statistical Significance",
      "text" : "We compare the predictions of our model on the non-NA triples present in the test set with the predictions of the second-best model using the McNemar’s test of statistical significance (McNemar, 1947). In all cases, we obtained the p-value to be many orders of magnitude smaller than 0.05, suggesting that the improvement in results is statistically significant in all cases."
    }, {
      "heading" : "G Ablation on NYT-10d",
      "text" : "2Obtained from the original repository for DiS-ReX\nWe perform ablation studies on the NYT-10d dataset to understand which components are most beneficial for our proposed model. We provide the results in table 4.\nWe observe that the performance increases with increase in maximum allowed length of the passage. This result is expected since the model would be exposed to more information for a given entity pair, allowing it to make more confident predictions for the validity of a particular candidate relation.\nUpon replacing our passage summarization step with multi-label classification using [CLS] token (present at the start of the passage), we observe a significant decrease in AUC, indicating that contextual embedding of [CLS] token might not contain enough information for multi-label prediction of bag.\nIt is interesting to note here that the AUC is still higher than that of REDSandT, a model which uses BERT+Att as the backbone. This means that one can simply obtain an improvement in performance by creating a passage from multiple instances in a bag.\nRemoving entity markers resulted in the most significant drop in performance. However, this is also expected since without them, our model would have no way to understand which entities to consider while performing relation extraction."
    }, {
      "heading" : "H Attention on [PAD] tokens",
      "text" : "In the passage summarization step (described in section 3), we allow the relation query vector −→r to also attend over the encodings of the [PAD] tokens present in the passage. We make this architectural choice in-order to provide some structure to the relation-specific summaries created by our model. If a particular relation class r is not a valid relation for entity pair (e1, e2), then ideally, we would want the attended-summary of the passage P (e1, e2) created by the relation vector −→r to represent some sort of a null state (since information specific to that relation class is not present in the passage). Allowing [PAD] tokens to be a part of the attention would provide enough flexibility to the model to represent such a state. We test our hypothesis by considering 1000 non-NA bags correctly labelled by our trained model in the test set of NYT-10d. Let R(e1, e2) denote the set of valid relation-classes for entity pair (e1, e2) and let R denote all of the relation-classes present in the dataset. We first calculate the percentage of attention given to [PAD] tokens for a given passage P (e1, e2) for all relation-classes in R. The results are condensed into two scores, sum of scores for R(e1, e2) and sum of scores for R \\R(e1, e2). The results are aggregated for all 1000 bags, and then averaged out by dividing with the total number of positive triples and negative triples respectively. We obtain that on an average, only 0.07% of attention weight is given to [PAD] tokens by relation vectors corresponding to R(e1, e2), compared to 88.35% attention weight given by relation vectors corresponding to R \\ R(e1, e2). We obtain similar statistics on other datasets as well. This suggests that for invalid triples, passage summaries generated by the model resemble the embeddings of the [PAD] token. Furthermore, since we don’t allow [PAD] tokens to be a part of self-attention update inside BERT, the [PAD] embeddings at the output of the BERT encoder are not dependent on the passage, allowing for uniformity across all bags.\nFinally, we train a model where we don’t allow the relation query vectors to attend on the [PAD] token embeddings and notice a 3.5pt drop in AUC on NYT-10d (table 4). We also note that the performance is still significantly higher than models such as REDSandT and DISTRE, suggesting that our instance aggregation scheme still performs better than the baselines, even when not optimized fully."
    }, {
      "heading" : "I Examples of Attention Weighting during Passage Summarization",
      "text" : "To understand how the query vector of a relation attends over passage tokens to correctly predict that relation, we randomly selected from correctly predicted non-NA triples and selected the token obtaining the highest attention score (by the query vector for the correct relation). For the selection, we ignore the stop words, special tokens and the entity mentions. The results are presented in table 5."
    }, {
      "heading" : "J Performance vs Length of test passages",
      "text" : "Our instance aggregation scheme truncates the passage if the number of tokens exceed the maximum number of tokens allowed by the encoder. In such cases, one would assume that the our model is not suited for cases where the number of instances present in a bag is very large. To test this hypothesis,\nwe divide the non-NA bags, (e1, e2), present in the NYT-10m data into 7 bins based on the number of tokens present in P (e1, e2) (after tokenized using BERT). We then compare the performance with CIL on examples present in each bin. The results in figure 4 indicate that a) our model beats CIL in each bin-size b) the variation among different bins is the same for both models. This trend is continued even for passages where the number of tokens present exceed the maximum number of tokens allowed for BERT (i.e. 512). This results indicate that 512 tokens provide sufficient information for correct classification of a triple. Moreover, models using intra-bag attention aggregation scheme fix the number of instances sampled from the bag in practice. For CIL, the best performing configuration uses a bag-size of 3. This analysis therefore indicates that our model doesn’t particularly suffer a drop in performance on large bags when compared with other state-of-the-art models."
    }, {
      "heading" : "K Entity Permutation Test",
      "text" : "To understand how robust our trained model would be to changes in the KB, we design the entity permutation test (inspired by Ribeiro et al. (2020)). An ideal DS-RE model should be able to correctly predict the relationship between an entity pair by understanding the semantics of the text mentioning them. Since DS-RE models under the MI-ML setting are evaluated on bag-level, it might be the case that such models are simply memorizing the KB on which they are being trained on.\nTo test this hypothesis, we construct a new test set using NYT-10m by augmenting its KB. Let B(e1, e2) denote a non-NA bag already existing in the test set of the dataset. We augment this bag to correspond to a new entity-pair (which is not present in the combined KB of all three splits of this dataset). The augmentation can be of two different types: replacing e1 with e′1 or replacing e2 with e ′ 2. We restrict such augmentations to the same type (i.e the type of ei and e′i is same for i = 1, 2). For each non-NA entity pair in the test set of the dataset, we select one such augmentation and appropriately modify each instance in B(e1, e2) to have the new entity mentions. We note that since each instance in NYT-10m is manually annotated and since our augmentation ensures that the type signature is preserved, the transformation is label preserving. For the NA bags, we use the ones already present in the original split. This entire transformation leaves us with an augmented test set, having same number of NA and non-NA bags as the original split. The non-NA entity pairs are not present in the KB on which the model is trained on."
    }, {
      "heading" : "L More Analysis on DiS-ReX",
      "text" : ""
    }, {
      "heading" : "L.1 Relation-wise F1 scores",
      "text" : "To show how our model performs on each relation label compared to other competitive baselines, we present relation-wise F1 scores on DiS-ReX in table 6."
    }, {
      "heading" : "L.2 Language-wise AUC scores",
      "text" : "We compare the performance of our model compared to other baselines on every language in DiS-ReX. For this, we partition the test data into language-wise test sets i.e. containing instances of only a particular language. The results are presented in table 7. We observe that the order of performance across languages is consistent for all models including ours i.e. German < English < Spanish < French. Further we observe that our model beats the second best model by an AUC ranging from 3 upto 4 points on all languages.\nL.3 Do multilingual bags improve performance? To understand whether the currently available aggregation schemes (including ours) are able to benefit from multilingual bags or not, we conduct an experiment where we only perform inference on test-set bags that contain instances from all four languages. In the multilingual case, the passage constructed during the Passage Summarization step will contain multiple sentences of different languages. To understand whether such an input allows improves (or hampers) the performance, we devise an experiment where we perform inference by removing sentences from any one, two or three languages from the set of bags containing instances of all four languages. There are roughly 1500 bags of such kind. Note that removing any k languages (k <= 3) would result in ( 4 k ) different sets and we take average of AUC while reporting the numbers. The results are presented in figure 5. We observe that in all aggregation schemes, AUC increases with increase in number of languages of a multilingual bag. mPARE consistently beats the other models in each scenario, indicating that the\nencoding of a multilingual passage and attention-based summarization over multilingual tokens doesn’t hamper the performance of a DS-RE model with increasing no. of languages."
    }, {
      "heading" : "M Negligible effect of random ordering",
      "text" : "Since we order the sentences randomly into a passage to be encoded by BERT, this may potentially cause some randomness in the results. However, we hypothesize that the BERT encoder must also be getting fine-tuned to treat the bag as a set (and not a sequence) of sentences when being trained with random ordering technique. And as a result, it’s performance must be agnostic to the order of sentences it sees in a passage during inference. To validate this, we perform 20 inference runs of our trained model with different seeds i.e. the ordering of sentences is entirely random in each run. We measure mean and standard deviation for each dataset as listed in table 8. We observe negligible standard deviation in all metrics. A minute variation in Macro-F1 or P@M metrics may be attributed to the fact that these are macro-aggregated metrics and a variation in performance over some data points may also affect these to some extent."
    } ],
    "references" : [ {
      "title" : "Fine-tuning pre-trained transformer language models to distantly supervised relation extraction",
      "author" : [ "Christoph Alt", "Marc Hübner", "Leonhard Hennig." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Alt et al\\.,? 2019",
      "shortCiteRegEx" : "Alt et al\\.",
      "year" : 2019
    }, {
      "title" : "Dis-rex: A multilingual dataset for distantly supervised relation extraction",
      "author" : [ "Abhyuday Bhartiya", "Kartikeya Badola", "Mausam" ],
      "venue" : null,
      "citeRegEx" : "Bhartiya et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bhartiya et al\\.",
      "year" : 2021
    }, {
      "title" : "CIL: Contrastive instance learning framework for distantly supervised relation extraction",
      "author" : [ "Tao Chen", "Haizhou Shi", "Siliang Tang", "Zhigang Chen", "Fei Wu", "Yueting Zhuang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving distantly-supervised relation extraction through bert-based label and instance embeddings",
      "author" : [ "Despina Christou", "Grigorios Tsoumakas." ],
      "venue" : "IEEE Access, 9:62574–62582.",
      "citeRegEx" : "Christou and Tsoumakas.,? 2021",
      "shortCiteRegEx" : "Christou and Tsoumakas.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Manual evaluation matters: Reviewing test protocols of distantly supervised relation extraction",
      "author" : [ "Tianyu Gao", "Xu Han", "Keyue Qiu", "Yuzhuo Bai", "Zhiyu Xie", "Yankai Lin", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou." ],
      "venue" : "arXiv preprint arXiv:2105.09543.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "More data, more relations, more context and more openness: A review and outlook for relation extraction",
      "author" : [ "Xu Han", "Tianyu Gao", "Yankai Lin", "Hao Peng", "Yaoliang Yang", "Chaojun Xiao", "Zhiyuan Liu", "Peng Li", "Jie Zhou", "Maosong Sun." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "OpenNRE: An open and extensible toolkit for neural relation extraction",
      "author" : [ "Xu Han", "Tianyu Gao", "Yuan Yao", "Deming Ye", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of EMNLP-IJCNLP: System Demonstrations, pages 169–174.",
      "citeRegEx" : "Han et al\\.,? 2019",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledgebased weak supervision for information extraction of overlapping relations",
      "author" : [ "Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld." ],
      "venue" : "Proceedings of the 49th",
      "citeRegEx" : "Hoffmann et al\\.,? 2011",
      "shortCiteRegEx" : "Hoffmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Neural relation extraction with multi-lingual attention",
      "author" : [ "Yankai Lin", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34–43, Vancouver,",
      "citeRegEx" : "Lin et al\\.,? 2017",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural relation extraction with selective attention over instances",
      "author" : [ "Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Lin et al\\.,? 2016",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2016
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "arXiv preprint arXiv:1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Note on the sampling error of the difference between correlated proportions or percentages",
      "author" : [ "Quinn McNemar." ],
      "venue" : "Psychometrika, 12(2):153–157.",
      "citeRegEx" : "McNemar.,? 1947",
      "shortCiteRegEx" : "McNemar.",
      "year" : 1947
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Cross-lingual relation extraction with transformers",
      "author" : [ "Jian Ni", "Taesun Moon", "Parul Awasthy", "Radu Florian." ],
      "venue" : "arXiv preprint arXiv:2010.08652.",
      "citeRegEx" : "Ni et al\\.,? 2020",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2020
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : null,
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Robust distant supervision relation extraction via deep reinforcement learning",
      "author" : [ "Pengda Qin", "Weiran Xu", "William Yang Wang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne,",
      "citeRegEx" : "Qin et al\\.,? 2018",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Beyond accuracy: Behavioral testing of nlp models with checklist",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling relations and their mentions without labeled text",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew McCallum." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 148–163. Springer.",
      "citeRegEx" : "Riedel et al\\.,? 2010",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling missing data in distant supervision for information extraction",
      "author" : [ "Alan Ritter", "Luke Zettlemoyer", "Mausam", "Oren Etzioni." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 1:367–378.",
      "citeRegEx" : "Ritter et al\\.,? 2013",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-instance multi-label learning for relation extraction",
      "author" : [ "Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-",
      "citeRegEx" : "Surdeanu et al\\.,? 2012",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2012
    }, {
      "title" : "RESIDE: Improving distantly-supervised neural relation extraction using side information",
      "author" : [ "Shikhar Vashishth", "Rishabh Joshi", "Sai Suman Prayaga", "Chiranjib Bhattacharyya", "Partha Talukdar." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Vashishth et al\\.,? 2018",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2018
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Distant supervision for relation extraction via piecewise convolutional neural networks",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural language processing, pages 1753–",
      "citeRegEx" : "Zeng et al\\.,? 2015",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2015
    }, {
      "title" : "intra-bag attention aggregation scheme in 2016, obtaining the then state-of-the-art performance on NYT-10d using a piecewise convolutional neural network (PCNN (Zeng et al., 2015))",
      "author" : [ "Lin" ],
      "venue" : "RESIDE Vashishth et al",
      "citeRegEx" : "Lin,? \\Q2018\\E",
      "shortCiteRegEx" : "Lin",
      "year" : 2018
    }, {
      "title" : "mBERT+MNRE The MNRE aggregator",
      "author" : [ "Lin" ],
      "venue" : null,
      "citeRegEx" : "Lin,? \\Q2021\\E",
      "shortCiteRegEx" : "Lin",
      "year" : 2021
    }, {
      "title" : "K Entity Permutation Test To understand how robust our trained model would be to changes in the KB, we design the entity permutation test (inspired",
      "author" : [ "Ribeiro" ],
      "venue" : null,
      "citeRegEx" : "Ribeiro,? \\Q2020\\E",
      "shortCiteRegEx" : "Ribeiro",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Distant supervision (DS) trains RE models given B(e1, e2) and R(e1, e2), without sentence level annotation (Mintz et al., 2009).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "Recent neural approaches to DS-RE encode each sentence t ∈ B(e1, e2) and then aggregate sentence embeddings using an aggregation operator – the common operator being intra-bag attention (Lin et al., 2016).",
      "startOffset" : 186,
      "endOffset" : 204
    }, {
      "referenceID" : 23,
      "context" : ", contrastive learning, MLM), but agree on the design choice of encoding each sentence independently of the others (Vashishth et al., 2018; Alt et al., 2019; Christou and Tsoumakas, 2021; Chen et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 206
    }, {
      "referenceID" : 0,
      "context" : ", contrastive learning, MLM), but agree on the design choice of encoding each sentence independently of the others (Vashishth et al., 2018; Alt et al., 2019; Christou and Tsoumakas, 2021; Chen et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 206
    }, {
      "referenceID" : 3,
      "context" : ", contrastive learning, MLM), but agree on the design choice of encoding each sentence independently of the others (Vashishth et al., 2018; Alt et al., 2019; Christou and Tsoumakas, 2021; Chen et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : ", contrastive learning, MLM), but agree on the design choice of encoding each sentence independently of the others (Vashishth et al., 2018; Alt et al., 2019; Christou and Tsoumakas, 2021; Chen et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "We then encode the whole passage through BERT (Devlin et al., 2019) (or mBERT for multilingual setting).",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "We experiment on four DS-RE datasets – three in English, NYT-10d (Riedel et al., 2010), NYT-10m, and Wiki-20m (Gao et al.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : ", 2010), NYT-10m, and Wiki-20m (Gao et al., 2021), and one multilingual, DiS-ReX (Bhartiya et al.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : ", 2021), and one multilingual, DiS-ReX (Bhartiya et al., 2021).",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Most later works follow the multi-instance multilabel learning framework (Surdeanu et al., 2012) in which there are multiple labels associated with a bag, and the model is trained with at-least-one assumption.",
      "startOffset" : 73,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : ", using Piecewise CNN (Zeng et al., 2015), Graph Convolution Net (e.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : ", RESIDE (Vashishth et al., 2018)), GPT (DISTRE (Alt et al.",
      "startOffset" : 9,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ", 2018)), GPT (DISTRE (Alt et al., 2019)) and BERT (REDSandT (Christou and Tsoumakas, 2021), CIL (Chen et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : ", 2019)) and BERT (REDSandT (Christou and Tsoumakas, 2021), CIL (Chen et al.",
      "startOffset" : 28,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : ", 2019)) and BERT (REDSandT (Christou and Tsoumakas, 2021), CIL (Chen et al., 2021)).",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "They all aggregate embeddings using intra-bag attention (Lin et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "Beyond Binary Cross Entropy, additional loss terms include masked language model pre-training (DISTRE, CIL), RL loss (Qin et al., 2018), and auxiliary contrastive learning (CIL).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "Unfortunately, most popular DS-RE dataset (NYT-10d) has a noisy test set, as it is automatically annotated (Riedel et al., 2010).",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Multilingual DS-RE: A bilingual DS-RE model named MNRE (tested on English and Mandarin) introduced cross-lingual attention in languagespecific CNN encoders (Lin et al., 2017).",
      "startOffset" : 156,
      "endOffset" : 174
    }, {
      "referenceID" : 7,
      "context" : "Moreover, following best-practices in RE (Han et al., 2019), each mention of e1 and e2 in the passage are surrounded by special entity marker tokens <e1>,</e1>, and <e2>,</e2>, respectively.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "For NYT-10m and Wiki-20m, we report comparisons against models in the original paper (Gao et al., 2021), and also additionally run CIL for a stronger comparison.",
      "startOffset" : 85,
      "endOffset" : 103
    } ],
    "year" : 0,
    "abstractText" : "Neural models for distantly supervised relation extraction (DS-RE) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the fullest. In response, we explore a simple baseline approach (PARE) in which all sentences of a bag are concatenated into a passage of sentences, and encoded jointly using BERT. The contextual embeddings of tokens are aggregated using attention with the candidate relation as query – this summary of whole passage predicts the candidate relation. We find that our simple baseline solution outperforms existing state-of-the-art DS-RE models in both monolingual and multilingual DS-RE datasets.",
    "creator" : null
  }
}
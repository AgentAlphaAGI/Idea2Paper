{
  "name" : "ARR_2022_169_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cross-Lingual Event Detection via Optimized Adversarial Training",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Event Detection (ED) is an important sub-task within the broader Information Extraction (IE) task. Event detection consists of being able to identify the words, commonly referred to as triggers, that denote the occurrence of events in a sentence, and classify them into a discrete set of event types. For example, in the sentence “Jamie bought a car yesterday.”, bought is considered the trigger of a TRANSACTION:TRANSFER-OWNERSHIP1\n1Event type taken from the ACE05 dataset.\nevent type. It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).\nNonetheless, ED remains quite a challenging task as the context in which a trigger occurs can change its corresponding type completely. Furthermore, the same event might also be expressed by entirely different words/phrases. Additionally, the vast majority of the aforementioned efforts are limited to a monolingual setting — performing ED on text belonging to a single language.\nAlternatively, Cross-Lingual ED (CLED) proposes the scenario of creating models that effectively perform ED on data belonging to more than one language, which entails additional challenges. For instance, trigger words present in one language might not exist in another one. An example of this phenomenon are verb conjugations where some tenses only exist in some languages, which is commonplace in ED as event triggers are usually related to the verbs in a sentence. Some recent work (Majewska et al., 2021) attempts to address this issue by injecting external linguistic knowledge into the training process. Another problematic issue are triggers with different meanings that are each distinct words in other languages. For instance, the word “juicio” in Spanish can be either “judgement” or “trial” in English, depending on the context.\nA compelling approach to creating a crosslingual model is to use transfer learning which carries the performance of a model trained on a source language over onto a second target language. The general idea is leveraging the existing high-quality annotated data available for a highresource language to train a model in a way that allows it to learn the language-invariant charac-\nteristics of the task at hand, ED in this case, so that it also performs effectively on text from a second language. Prior works on transfer learning for CLED have relied on pre-trained Multilingual Language Models (MLMs), such as multilingual BERT (mBERT) (Devlin et al., 2019), to take advantage of their innate language-invariant qualities. Yet, their performance still shows room for improvement as they are unable to handle the difficult instances, unique to cross-lingual settings, mentioned earlier. We identify a significant shortcoming of previous CLED efforts in that they do not exploit the abundant supply of unlabeled data: even though MLMs are trained on immense amounts of it, unlabeled data is not used when fine-tuning for the ED task. It is our intuition that by integrating unlabeled data into the training process, the model is exposed to more language context which should help deal with issues such as verb variation and multiple connotations.\nAs such, we propose making use of Adversarial Language Adaptation (ALA) (Joty et al., 2017; Chen et al., 2018) to train a CLED model. The key idea is to generate language-invariant representations that are not indicative of language but remain informative for the task. Unlabeled data from both the source and target languages is used to train a Language Discriminator (LD) network that learns to discern between the two. The adversarial part comes from the fact that the encoder and discriminator are trained with opposing objectives: as the LD becomes better at distinguishing between languages, the encoder learns to generate more language-invariant representations in an attempt to fool the LD. To the best of our knowledge, our work is the first one proposing the use of ALA for the CLED task.\nNonetheless, contrary to past uses of ALA where the same importance is given to all unlabeled samples, we recognize that such course of action is suboptimal as certain samples are bound to be more informative for the discriminator than others. For example, we would like to present the LD with the samples that allow it to learn the fine-grained distinctions between the source and target languages, instead of relying on syntactic differences. Moreover, in the context of ED, we suggest it would be beneficial for the LD to be trained with examples containing events, instead of non-event samples, as the presence of an event can then be incorporated into the generated representations.\nHence, we propose refining the adversarial training process by only keeping the most informative examples while disregarding less useful ones. Our intuition as to what makes samples more informative for CLED is two-fold: First, we presume that presenting the LD with examples that are too different makes the discrimination task too simple. As mentioned previously, we would like the LD to learn a fine-grained distinction between the source and target languages which, in turn, improves the language-invariance of the encoder’s representations. Thus, we suggest presenting the LD with examples that have similar contextual semantics, i.e., similar representations. Second, we consider sentences containing events to be more relevant for the LD. Accordingly, such sentences should have a larger probability of being selected for ALA training.\nAs such, we suggest using Optimal Transport (OT) (Villani, 2008) as a natural solution to simultaneously incorporate both the similarity between sample representations and the likelihood of the samples containing an event into a single framework. Therefore, we cast sample selection as an OT problem in which we attempt to find the best alignment between the samples from the source and target languages.\nFor our experiments, we focus on the widely used ACE05 and ACE05-ERE datasets (Walker et al., 2006) which, in conjuction, contain eventannotations in 4 different languages: English, Spanish, Chinese, and Arabic. We work on 8 different language pairs by selecting different languages as the source and target. Our proposed model obtains new state-of-the-art results with considerable performance improvements (+ 2-3% in F1 scores) over competitive baselines and previously published results (M’hamdi et al., 2019). These results demonstrate our model’s efficacy and applicability at creating CLED systems."
    }, {
      "heading" : "2 Model",
      "text" : ""
    }, {
      "heading" : "2.1 Problem Definition",
      "text" : "Following prior works (M’hamdi et al., 2019; Majewska et al., 2021), we treat ED as a sequence labeling problem. Given a set D of word sequences wi = {wi1, wi2, ..., win−1, win} and their corresponding label sequences yi = {yi1, yi2, ..., yin−1, yin}, we use an encoder network E to obtain a contextualized vector representation of the words in the input sequence\nhi = E(wi) = {hi1, hi2, ..., hin−1, hin}. Then, we feed the representations hi into a prediction network P to compute a distribution over the set of possible labels and train it in a supervised manner using the negative log-likelihood function LP :\nLP = − |D|∑ i=1 n∑ j=1 logP (yij |hij) (1)\nIn the cross-lingual transfer-learning setting, the data used to train the model and the data on which the model is tested come from different languages known as the source and target, respectively. As such, we deal with two datasets Dsrc and Dtgt. We assume that we do not have access to the gold labels of the target language ytgt, other than to evaluate our CLED model at testing time.\nOur goal is to define a model able to generate language-invariant word representations that are refined enough so that cross-lingual issues, such as the ones described previously, are properly handled."
    }, {
      "heading" : "2.2 Baseline Model",
      "text" : "Here we briefly describe the BERT-CRF model (M’hamdi et al., 2019) which was the previous state-of-the-art and serves as our baseline. Using mBERT (Devlin et al., 2019) as its encoder, BERT-CRF generates robust, contextualized representations for words from different languages. For words that are split into multiple word-pieces, the average of the representation vectors for all comprising sub-pieces is used as the representation of the full word.\nFor classification purposes, instead of assigning the labels of each token independently, BERT-CRF uses a Conditional Random Field (CRF) (Lafferty et al., 2001) layer on top of the prediction network to better capture the interactions between the label sequences. As such, the representation vectors hi of the words in the sequence are fed to a CRF layer which finds the optimal label sequence."
    }, {
      "heading" : "2.3 Adversarial Language Adaptation",
      "text" : "The pre-trained versions of MLMs like mBERT or XLM-RoBERTa (Conneau et al., 2019) generate contextualized representations with a certain degree of language-invariance. This can be confirmed by their successful application in cross-lingual settings (M’hamdi et al., 2019; Majewska et al., 2021).\nHowever, a lingering issue is the difficulty of learning the nuances of the target language such as verb variations that do not exist in the source language used to train them. Majewska et al. (2021), for instance, propose to address this issue by injecting external verb knowledge into the encoder via adapter modules (Pfeiffer et al., 2020).\nIt is our intuition, however, that these issues can be mitigated by achieving a more refined level of language-invariance in the word representations. As such, we propose using Adversarial Language Adaptation (ALA) (Joty et al., 2017), a technique used to create language-invariant models. The ALA framework consists in including a Language Discriminator (LD) whose purpose is to learn language-dependent features and be able to differentiate between the samples from either the source or the target languages.\nA fundamental characteristic of the ALA approach is its lack of requirements for annotated data in the target language. As such, we can use data from both Dsrc and Dtgt. An auxiliary dataset Daux = {(w1, l1), . . . , (w2m, l2m)} is created where wi is a text sequence from either Dsrc or Dtgt, and li is a language label. The cardinality of Daux is |Daux| = 2m, where m is equal to the batch size. Text samples w1 . . . wm ∈ Dsrc, and samples wm+1 . . . w2m ∈ Dtgt. As described earlier, the encoder E receives the text sequences and produces a sequence of contextualized representations E(wi) = hi = {hi0, hi1, hi2, . . . , hin} where hi0 is the representation of the [CLS] token added at the beginning of every input sequence.\nIn our work, the LD is a a simple Multi-Layer Perceptron(MLP) network that takes hi0 as input and produces a single sigmoid output. It’s trained with the usual binary cross-entropy loss function objective: LDloss = argminLD L(LD(hi0), li).\nAs the LD learns to distinguish between the source and target languages, we concurrently train the encoder to “fool” the discriminator. In other words, the encoder must learn to generate representations that are language-invariant enough that the LD is unable to classify them while still remaining predictive for event-trigger classification. We optimize the following loss:\nargmin E,C n∑ j=1 (L(C(hij), yij))− λL(LD(hi0, li))\n(2)\nWhere C refers to the CRF-based classifier network\nand λ is a hyperparameter. Equation 2 is implemented by using a GradientReversal Layer (GRL)(Ganin and Lempitsky, 2015) which acts as the identity during the forward pass, but reverses the direction of the gradients during the backward pass. The first term in Equation 2 can, of course, only be applied for annotated data from the source language.\nThe GRL is applied to the input vectors, hi0, of the LD. This way, the LD is being trained to differentiate between the two languages while the encoder is trained in the opposite direction, i.e. to generate sequence representations that are harder to discriminate."
    }, {
      "heading" : "2.4 Adversarial Training Optimization",
      "text" : "ALA has already been shown to be effective at generating language-invariant models(Joty et al., 2017; Chen et al., 2018). However, in regular ALA training, all samples in a batch, from both the source and target domains, are treated equally. That is, all samples are used as examples for the discriminator to learn how to better discern between the two domains. We propose that ALA effectiveness can be further improved by carefully selecting the samples with which to train the discriminator. We argue that some samples might be more informative than others and that, by only using such informative samples during training, better adaptation results can be achieved.\nWe base our notion as to what makes a sample more informative on two factors. First, we argue that presenting the LD with examples from the source and target language that are too dissimilar makes its task easier which, in turn, leads to the LD not learning the fine-grained distinctions between the languages. Instead, we propose using samples whose vector representations hi0 are close to each other in the embedding space. The intuition for this being that, as representations capture the contextual semantics of the samples, closer representations correspond to more similar examples. Second, we suggest that presenting the LD with samples containing events should make the encoder incorporate task-specific information into its representations."
    }, {
      "heading" : "2.4.1 Optimal Transport",
      "text" : "One challenge of using these two criteria for ALA sample selection process is that they come with two different measures which are hard to combine. We propose using Optimal Transport (OT) (Villani,\n2008) as a natural way to combine these two metrics into a single framework for sample selection. Optimal transport is, in broad terms, the problem of finding out the cheapest transformation between two discrete probability distributions. It requires a cost function to determine the cost of transforming a data point in one distribution into a data point in the second distribution. When the cost function is based on a valid distance function, the minimum cost is known as the Wasserstein distance. Formally, it solves the following optimization problem:\nπ∗(s, t) = min π∈ ∏ (s,t) ∑ s∈S ∑ t∈T π(s, t) C(s, t) ds dt\n(3)\ns.t. s ∼ p(s) and t ∼ q(t)\nWhere S and T are two domains with probability distributions p(s) and q(t), and C is a cost function for mapping S to T , C(s, t) : S × T −→ R+. Finally, π∗(s, t) is the optimal joint distribution over the set of all joint distributions ∏ (s, t). The problem described by Equation 3 is, of course, intractable. Therefore, we use instead the Sinkhorn algorithm (Cuturi, 2013) which is an entropy-based relaxation of the discrete OT problem."
    }, {
      "heading" : "2.4.2 Problem Formulation",
      "text" : "We formulate the OT problem as follows: the domains S and T are defined as the representation vectors of the text samples in either the source hsi0 or the target htj0 languages. We use the L2 distance between these representations as the cost function:\nC(hsi0, h t j0) = ||hsi0 − htj0||22 (4)\nTo define the marginal probability distributions p(s) and q(t) for the S and T domains, we propose including an Event-Presence (EP) prediction module and use its normalized likelihood scores as the probability distributions for S and T . Thus, the auxiliary dataset Daux is augmented to include an event-presence label ei for each sample, Daux = {(w1, l1, e1), . . . , (w2m, l2m, e2m)}, and the EP module is trained to optimize the following loss:\nEPloss = argmin EP L(EP (hi0), ei) (5)\nThe probability distributions p(s) and p(t) are the computed as follows:\np(s) = Softmax(EP (hsi0) | li == s) (6) p(t) = Softmax(EP (hti0) | li == t) (7)"
    }, {
      "heading" : "2.4.3 Sample Selection",
      "text" : "We use the OT solution matrix π∗, where an entry π∗(s, t) represents the optimal cost of transforming data point s ∈ S into t ∈ T , to compute an the overall similarity score vi of a sample hi0 ∈ S to the samples in the target domain T by using the average distance:\nvi =\n∑m j π ∗(hsi0, h t j0)\nm (8)\nCorrespondingly, we compute an overall similarity score vj of each sample hj0 ∈ T to the samples in the source domain S:\nvj =\n∑m i π ∗(hsi0, h t j0)\nm (9)\nLastly, we select a fraction, hyperparameter γ, of samples with the best similarity scores from both the source and target languages, and only use these selected samples during ALA training."
    }, {
      "heading" : "2.5 OACLED Model",
      "text" : "We train our Optimized Adversarial Cross-Lingual Event Detection (OACLED) model end-to-end with the following loss objective:\nLfull = CRFloss + αLDloss + βEPloss (10)\nwhere α and β are trade-off hyperparameters."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We evaluate our model on the ACE05 (Walker et al., 2006) dataset which includes annotated eventtrigger data in 3 languages: English, Chinese and Arabic. To include an additional language in our experiments, we also evaluate on the ERE version of ACE05 which has annotated data in English and Spanish. The ACE05 and ACE05-ERE versions, however, do not share the same label set: ACE05 involves 33 distinct event types while ACE05-ERE involves 38 event types. Dataset characteristics can be found in Appendix A. We follow the same data pre-processing and splits as in previous work(M’hamdi et al., 2019) to ensure a fair comparison."
    }, {
      "heading" : "3.2 Main Results",
      "text" : "In our experiments, we work with 8 distinct language pairs by selecting each of the available languages as either the source or target language: English-Chinese, Chinese-English, EnglishArabic, Arabic-English, Chinese-Arabic, ArabicChinese, English-Spanish, and Spanish-English. The Chinese-Spanish, Spanish-Chinese, ArabicSpanish, and Spanish-Arabic language combinations are unavailable due the previously mentioned incompatibility between the event type sets in ACE05 and ACE05-ERE.\nTables 1 and 2 show the results of our experiments on the ACE05 and ACE05-ERE datasets, respectively.\nWe compare our OACLED model against 2 relevant baselines. BERT-CRF (M’hamdi et al., 2019), and XLM-R-CRF which is equivalent in all regards to BERT-CRF except that it uses XLM-RoBERTa as the encoder2. In our experiments, we use bertbase-cased and xlm-roberta-base for the encoders, parameters are tuned on the development data of the source language, and all entries are the average of five runs.\n2We do not compare with the work by Majewska et al. (2021) as its reported performance is below that of BERTCRF.\nFrom Tables 1 and 2, we can observe a substantial performance increase by performing the trivial change of replacing BERT with XLM-RoBERTa as the encoder. Furthermore, our OACLED model clearly and consistently outperforms the baselines for all language pairings, with the exception of the Chinese-Arabic pair. We attribute this to the impaired performance of XLM-RoBERTa as the encoder for that specific pair as can be confirmed by the poor performance of the XLM-R-CRF baseline on the same configuration. Most importantly, OACLED’s improvement over the XLM-R-CRF baseline is present in every configuration, which confirms the effectiveness of our optimized approach to ALA training."
    }, {
      "heading" : "3.3 Ablation Study",
      "text" : "We identify 2 main components in our approach: using ALA to create refined language-invariant representations, and optimizing the adversarial training process by selecting a subset of samples chosen with OT to incorporate our measures of informativeness into the sample selection process. Of course, removing ALA training entirely restores the model to the baseline. However, adversarial training optimization via OT has various aspects to it. In order to understand the contribution of these aspects, we explore four different models: OACLED-OT presents the effects of removing sample selection entirely and using all available samples to train the LD; OACLED-L2 uses a constant distance between the unlabeled samples instead the standard L2 distance used in the Sinkhorn algorithm; OACLED-EP completely removes the EP module and a uniform distribution is used as the probability distributions for both languages; finally, OACLED-ED-Loss keeps the EP module, but removes its EPloss term from Equation 10. The performance results of these models is presented in Table 3. In this and the following sections (3.4, 3.5.2), we present the results of experiments using English as the sole source language as it is the source language most ubiquitously used. We, however, found consistency in the displayed effects for different source/target language configurations.\nAs expected, removing the sample selection through OT leads to the worst performance drop. This highlights the importance of selecting informative examples for the LD. Furthermore, removing the cost function also hurts performance greatly, which shows that a proper distance function is\nneeded for the OT algorithm to work effectively. While the effects of removing the EP module and its corresponding loss term are not of the same magnitude, they are still significant. These results support our claim for the need and utility of all the components in our approach, showing that their inclusion is crucial in achieving state-of-the-art performance."
    }, {
      "heading" : "3.4 Language Model Finetuning",
      "text" : "The key contribution of our approach is to exploit unlabeled data in the target language, which is usually abundant, by introducing it into the training process to improve our model’s language-invariant qualities.\nTo confirm the utility of our approach, Table 4 contrasts our model’s performance against a baseline whose encoder has been finetuned with the same unlabeled data using the standard masked language model objective.\nIt can be observed that our model outperforms the finetuned baseline in two out of the three target languages. Additionally, the difference in performance in those two instances is considerably larger (3.58% and 1.15%), than the setting in which the baseline performs better (0.13%)."
    }, {
      "heading" : "3.5 Analysis",
      "text" : ""
    }, {
      "heading" : "3.5.1 Learned Representation Distances",
      "text" : "First, we look at the distance between the sentencelevel representations hi0 generated by the encoder for different source/target language pairs. Figure 1 shows a plot of such distances using cosine distance as the distance function.\nWhen computing the correlation with the performance results in Table 1, we obtain a score R = −0.6616, meaning there is moderate negative correlation between the distance of the representations and model performance, i.e. closer representations lead to better performance.\nSimilarly, Table 5 shows a comparison of the distances between the representations generated by OACLED and those obtained by the XLM-R-CRF baseline.\nWe observe that OACLED representations are closer, by several orders of magnitude, than those obtained by the baseline. This supports our claim that our model’s encoder generates more refined language-invariant representations than those obtained by the default version of XLM-RoBERTa."
    }, {
      "heading" : "3.5.2 Access to Labeled Target Data",
      "text" : "Previously, we discussed how a key feature of our approach is that it does not require annotated data in the target language and, instead, leverages the use of unlabeled data which is readily available. Nonetheless, we also explore the performance of our model in the event that there exists a small amount of annotated target data available. Figure 2 shows the results of our experiments when us-\ning different amounts of labeled target data during training.\nIt can be observed that OACLED consistently outperforms the baseline even when there is some availability of annotated data. Additionally, performance steadily increases as more and more data is used. This conforms to expectations, and confirms that having labeled data in the target language available for training is ultimately beneficial to the model’s performance."
    }, {
      "heading" : "3.5.3 Case Study",
      "text" : "Next, we look into our model’s predictions and analyse instances where it outperforms the baseline to exemplify the advantages of dealing with optimized language-invariant representations. We identify two important patterns.\nFirst, our model seems to better classify events in the target language that involve trigger words that have distinct connotations that depend on context. Specially those that are two distinct words in the source language. For example, the Spanish word “juicio” can have two distinct meanings that are different words in English: “trial” and “judgement”. Our model correctly classifies it as a JUSTICE:TRIAL-HEARING trigger in the sentence “Dos llamados a juicio fueron hechos por un\njurado federal investigador”. Meanwhile, the baseline fails to even recognize it as a trigger. Another example is the word “detenido”, an adjective that can mean both “detained”, in a criminal context, and “stopped”, as in halted. Our model correctly classifies it in the sentence “Padilla no debería permanecer detenido durante meses alejado de otros reos” as a JUSTICE:ARREST-JAIL trigger while the baseline fails to detect the event. We manually identified 23 of these polysemous triggers in the Spanish3 test set: 19 (82.6%) were correctly classified by our OACLED model versus 14 (60.8%) by the baseline (27.8% improvement).\nAdditionally, we found our model correctly classifies verb conjugation variants that do not exist in the source language. For instance, our model correctly recognizes the words “venderlos”, “vender”, “vendes”, and “vendedor” (variants of the verb “to buy”) as TRANSACTION:TRANSFEROWNERSHIP triggers whereas the baseline incorrectly classifies them as being of the TRANSACTION:TRANSFER-MONEY type. As previously mentioned, Majewska et al. (2021) propose injecting external verb-knowledge into the training to help with verb interpretation for event extraction. Our empirical results, however, outperform their reports which appears to imply that, at least for CLED, holistically learning the language-invariant features shared between the target and source languages works better than injecting language-specific verb knowledge.\nWe believe these findings illustrate how, by introducing additional context in the form of unlabeled data, the model is able to learn fine-grained word representations that better capture the semantics of the words in the target language, and successfully deal with difficult cross-lingual issues."
    }, {
      "heading" : "4 Related Work",
      "text" : "Research efforts on monolingual ED are extensive and varied. Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). More recent efforts have primarily made use of deep learning techniques such as convolutional neural networks (Nguyen and Grishman,\n3We use Spanish for the analysis as it is the mother tongue of the first author.\n2015; Chen et al., 2015; Nguyen et al., 2016b), recurrent neural networks (Nguyen et al., 2016a; Sha et al., 2018; Nguyen and Nguyen, 2019), graph convolutional networks (Nguyen and Grishman, 2018a; Yan et al., 2019), adversarial networks (Hong et al., 2018; Zhang et al., 2019b), and pre-trained language models (Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Zhang et al., 2020; Liu et al., 2020).\nWorks on cross-lingual ED are not as prevalent and generally make use of cross-lingual resources employed to address the differences between languages such as bilingual dictionaries or parallel corpora (Muis et al., 2018; Liu et al., 2019) and, more recently, pre-trained multilingual language models (M’hamdi et al., 2019; Hambardzumyan et al., 2020; Majewska et al., 2021). Unlike these previous efforts, our method leverages unlabeled data to further refine the language-invariant qualities of the language models.\nAdversarial Language Adaptation, inspired by models in domain adaptation research (Ganin and Lempitsky, 2015; Naik and Rose, 2020), has been successfuly applied at generating languageinvariant models (Joty et al., 2017; Chen et al., 2018). Our method improves upon these approaches optimizing the adversarial training process by selecting the most informative examples from the unlabeled data.\nAdditional examples of downstream applications of cross-lingual learning are document classification (Holger and Xian, 2018), named entity recognition (Xie et al., 2018) and part-of-speech tagging (Cohen et al., 2011). For a thorough review on cross-lingual learning, we refer the reader to Pikuliak et al. (2021)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We present a new model for Cross-Lingual Event Detection that leverages unlabeled data through ALA and OT to achieve new state-of-the-art performance. Our experiments on 8 different language pairs demonstrate our approach’s robustness and effectiveness at generating refined language-invariant representations that allow for better event detection results. Our analysis of its intermediate outputs and predictions confirm that our model’s representations are indeed closer to each other and that this proximity translates into better handling of difficult cross-lingual instances."
    }, {
      "heading" : "A Appendix A",
      "text" : "A.1 Dataset Characteristics\nDataset Language Split Sentences Events\nACE05\nEnglish Train 19,240 4,419 Dev 902 468 Test 676 424 Chinese Train 6,841 2,926 Dev 526 217 Test 547 190 Arabic Train 2,555 1,793 Dev 301 230 Test 262 247\nACE05-ERE\nEnglish Train 14,219 6,419 Dev 1,162 552 Test 1,129 559 Spanish Train 7,067 3,272 Dev 556 210 Test 546 269"
    }, {
      "heading" : "B Reproducibility Checklist",
      "text" : "– A learning rate of 1e−5 for the transformer parameters and of 1e−4 for the rest of the parameters. We arrived at this values after searching among [1e−6, 3e−6, 1e−5, 3e−5, 1e−4, 3e−4].\n– A batch size of 16, chosen between [8, 10, 16, 24, 32].\n– 300 for the dimensionality of the layers in feed-forwards networks, chosen from [100, 200, 300, 400, 500].\n– A γ = 0.5 for the percentage of samples used in adversarial training.\n– A λ = 0.001 as the scaling factor of the GRL layer.\n– An α = 1 and β = 0.001 as the trade-off parameters of the LD loss and ED loss, respectively.\n– A dropout of 10% for added regularization during training."
    } ],
    "references" : [ {
      "title" : "The stages of event extraction",
      "author" : [ "David Ahn." ],
      "venue" : "Proceedings of the Workshop on Annotating and Reasoning about Time and Events.",
      "citeRegEx" : "Ahn.,? 2006",
      "shortCiteRegEx" : "Ahn.",
      "year" : 2006
    }, {
      "title" : "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification",
      "author" : [ "Xilun Chen", "Yu Sun", "Ben Athiwaratkun", "Claire Cardie", "Kilian Weinberger." ],
      "venue" : "Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Event extraction via dynamic multipooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised structure prediction with nonparallel multilingual guidance",
      "author" : [ "Shay B. Cohen", "Dipanjan Das", "Noah Smith." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Cohen et al\\.,? 2011",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2011
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2019",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2019
    }, {
      "title" : "Sinkhorn distances: Lightspeed computation of optimal transport",
      "author" : [ "Marco Cuturi." ],
      "venue" : "Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2.",
      "citeRegEx" : "Cuturi.,? 2013",
      "shortCiteRegEx" : "Cuturi.",
      "year" : 2013
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised domain adaptation by backpropagation",
      "author" : [ "Yaroslav Ganin", "Victor Lempitsky." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning, pages 1180–1189.",
      "citeRegEx" : "Ganin and Lempitsky.,? 2015",
      "shortCiteRegEx" : "Ganin and Lempitsky.",
      "year" : 2015
    }, {
      "title" : "The role of alignment of multilingual contextualized embeddings in zero-shot crosslingual transfer for event extraction",
      "author" : [ "Karen Hambardzumyan", "Hrant Khachatrian", "Jonathan May." ],
      "venue" : "Collaborative Technologies and Data Science in Artificial Intelli-",
      "citeRegEx" : "Hambardzumyan et al\\.,? 2020",
      "shortCiteRegEx" : "Hambardzumyan et al\\.",
      "year" : 2020
    }, {
      "title" : "A corpus for multiligual document classification in eight languages",
      "author" : [ "Schwenk Holger", "Li Xian." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC).",
      "citeRegEx" : "Holger and Xian.,? 2018",
      "shortCiteRegEx" : "Holger and Xian.",
      "year" : 2018
    }, {
      "title" : "Using cross-entity inference to improve event extraction",
      "author" : [ "Yu Hong", "Jianfeng Zhang", "Bin Ma", "Jianmin Yao", "Guodong Zhou", "Qiaoming Zhu." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Hong et al\\.,? 2011",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2011
    }, {
      "title" : "Self-regulation: Employing a generative adversarial network to improve event detection",
      "author" : [ "Yu Hong", "Wenxuan Zhou", "Jingli Zhang", "Guodong Zhou", "Qiaoming Zhu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Hong et al\\.,? 2018",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2018
    }, {
      "title" : "Refining event extraction through cross-document inference",
      "author" : [ "Heng Ji", "Ralph Grishman." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ji and Grishman.,? 2008",
      "shortCiteRegEx" : "Ji and Grishman.",
      "year" : 2008
    }, {
      "title" : "Cross-language learning with adversarial neural networks",
      "author" : [ "Shafiq Joty", "Preslav Nakov", "Lluís Màrquez", "Israa Jaradat." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL), pages 226–237.",
      "citeRegEx" : "Joty et al\\.,? 2017",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2017
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Joint event extraction via structured prediction with global features",
      "author" : [ "Qi Li", "Heng Ji", "Liang Huang." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Filtered ranking for bootstrapping in event extraction",
      "author" : [ "Shasha Liao", "Ralph Grishman." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Liao and Grishman.,? 2010a",
      "shortCiteRegEx" : "Liao and Grishman.",
      "year" : 2010
    }, {
      "title" : "Using document level cross-event inference to improve event extraction",
      "author" : [ "Shasha Liao", "Ralph Grishman." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Liao and Grishman.,? 2010b",
      "shortCiteRegEx" : "Liao and Grishman.",
      "year" : 2010
    }, {
      "title" : "Event extraction as machine reading comprehension",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu", "Wei Bi", "Xiaojiang Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural cross-lingual event detection with minimal parallel resources",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Verb knowledge injection for multilingual event processing",
      "author" : [ "Olga Majewska", "Ivan Vulić", "Goran Glavaš", "Edoardo Maria Ponti", "Anna Korhonen." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Majewska et al\\.,? 2021",
      "shortCiteRegEx" : "Majewska et al\\.",
      "year" : 2021
    }, {
      "title" : "Event extraction as dependency parsing",
      "author" : [ "David McClosky", "Mihai Surdeanu", "Christopher Manning." ],
      "venue" : "BioNLP Shared Task Workshop. 9",
      "citeRegEx" : "McClosky et al\\.,? 2011",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2011
    }, {
      "title" : "Contextualized cross-lingual event trigger extraction with minimal resources",
      "author" : [ "Meryem M’hamdi", "Marjorie Freedman", "Jonathan May" ],
      "venue" : "In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
      "citeRegEx" : "M.hamdi et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "M.hamdi et al\\.",
      "year" : 2019
    }, {
      "title" : "Comparable study of event extraction in newswire and biomedical domains",
      "author" : [ "Makoto Miwa", "Paul Thompson", "Ioannis Korkontzelos", "Sophia Ananiadou." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Miwa et al\\.,? 2014",
      "shortCiteRegEx" : "Miwa et al\\.",
      "year" : 2014
    }, {
      "title" : "Low-resource cross-lingual event type detection via distant supervision with minimal effort",
      "author" : [ "Aldrian Obaja Muis", "Naoki Otani", "Nidhi Vyas", "Ruochen Xu", "Yiming Yang", "Teruko Mitamura", "Eduard Hovy." ],
      "venue" : "Proceedings of the 27th International Conference",
      "citeRegEx" : "Muis et al\\.,? 2018",
      "shortCiteRegEx" : "Muis et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards open domain event trigger identification using adversarial domain adaptation",
      "author" : [ "Aakanksha Naik", "Carolyn Rose." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Naik and Rose.,? 2020",
      "shortCiteRegEx" : "Naik and Rose.",
      "year" : 2020
    }, {
      "title" : "Joint event extraction via recurrent neural networks",
      "author" : [ "Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Nguyen et al\\.,? 2016a",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "A two-stage approach for extending event detection to new types via neural networks",
      "author" : [ "Thien Huu Nguyen", "Lisheng Fu", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the 1st ACL Workshop on Representation Learning for NLP (RepL4NLP).",
      "citeRegEx" : "Nguyen et al\\.,? 2016b",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Event detection and domain adaptation with convolutional neural networks",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Nguyen and Grishman.,? 2015",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2015
    }, {
      "title" : "Graph convolutional networks with argument-aware pooling for event detection",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Nguyen and Grishman.,? 2018a",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2018
    }, {
      "title" : "One for all: Neural joint modeling of entities and events",
      "author" : [ "Trung Minh Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Association for the Advancement of Artificial Inteligence(AAAI).",
      "citeRegEx" : "Nguyen and Nguyen.,? 2019",
      "shortCiteRegEx" : "Nguyen and Nguyen.",
      "year" : 2019
    }, {
      "title" : "A unified model of phrasal and sentential evidence for information extraction",
      "author" : [ "Siddharth Patwardhan", "Ellen Riloff." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Patwardhan and Riloff.,? 2009",
      "shortCiteRegEx" : "Patwardhan and Riloff.",
      "year" : 2009
    }, {
      "title" : "AdapterHub: A framework for adapting transformers",
      "author" : [ "Jonas Pfeiffer", "Andreas Rücklé", "Clifton Poth", "Aishwarya Kamath", "Ivan Vulić", "Sebastian Ruder", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual learning for text processing: A survey",
      "author" : [ "Matúš Pikuliak", "Marián Šimko", "Mária Bieliková." ],
      "venue" : "Expert Systems with Applications.",
      "citeRegEx" : "Pikuliak et al\\.,? 2021",
      "shortCiteRegEx" : "Pikuliak et al\\.",
      "year" : 2021
    }, {
      "title" : "Jointly extracting event triggers and arguments by dependency-bridge rnn and tensor-based argument interaction",
      "author" : [ "Lei Sha", "Feng Qian", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Sha et al\\.,? 2018",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2018
    }, {
      "title" : "Optimal Transport: Old and New",
      "author" : [ "C. Villani." ],
      "venue" : "Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg.",
      "citeRegEx" : "Villani.,? 2008",
      "shortCiteRegEx" : "Villani.",
      "year" : 2008
    }, {
      "title" : "Entity, relation, and event extraction with contextualized span representations",
      "author" : [ "David Wadden", "Ulme Wennberg", "Yi Luan", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th",
      "citeRegEx" : "Wadden et al\\.,? 2019",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2019
    }, {
      "title" : "Ace 2005 multilingual training corpus",
      "author" : [ "Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda." ],
      "venue" : "Technical report, Linguistic Data Consortium.",
      "citeRegEx" : "Walker et al\\.,? 2006",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2006
    }, {
      "title" : "Neural crosslingual named entity recognition with minimal resources",
      "author" : [ "Jiateng Xie", "Zhilin Yang", "Graham Neubig", "Noah A. Smith", "Jaime G. Carbonell." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Xie et al\\.,? 2018",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2018
    }, {
      "title" : "Event detection with multiorder graph convolution and aggregated attention",
      "author" : [ "Haoran Yan", "Xiaolong Jin", "Xiangbin Meng", "Jiafeng Guo", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Yan et al\\.,? 2019",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint extraction of events and entities within a document context",
      "author" : [ "Bishan Yang", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Yang and Mitchell.,? 2016",
      "shortCiteRegEx" : "Yang and Mitchell.",
      "year" : 2016
    }, {
      "title" : "Exploring pre-trained language models for event extraction and generation",
      "author" : [ "Sen Yang", "Dawei Feng", "Linbo Qiao", "Zhigang Kan", "Dongsheng Li." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Extracting entities and events as a single task using a transition-based neural model",
      "author" : [ "Junchi Zhang", "Yanxia Qin", "Yue Zhang", "Mengchi Liu", "Donghong Ji." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint Entity and Event Extraction with Generative Adversarial Imitation Learning",
      "author" : [ "Tongtao Zhang", "Heng Ji", "Avirup Sil." ],
      "venue" : "Data Intelligence, 1(2):99– 120. 10",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "A question answering-based framework for one-step event argument extraction",
      "author" : [ "Yunyan Zhang", "Guangluan Xu", "Yang Wang", "Daoyu Lin", "Feng Li", "Chenglong Wu", "Jingyuan Zhang", "Tinglei Huang." ],
      "venue" : "IEEE Access, vol 8, 65420-65431.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 2,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 34,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 36,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 42,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 41,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 30,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 44,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 18,
      "context" : "It is a very well studied task in which there have been lots of previous research efforts that have recently been primarily deep learningbased (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Wadden et al., 2019; Zhang et al., 2019a; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020; Liu et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 355
    }, {
      "referenceID" : 20,
      "context" : "Some recent work (Majewska et al., 2021) attempts to address this issue by injecting external linguistic knowledge into the training process.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "Prior works on transfer learning for CLED have relied on pre-trained Multilingual Language Models (MLMs), such as multilingual BERT (mBERT) (Devlin et al., 2019), to take advantage of their innate language-invariant qualities.",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "As such, we propose making use of Adversarial Language Adaptation (ALA) (Joty et al., 2017; Chen et al., 2018) to train a CLED model.",
      "startOffset" : 72,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "As such, we propose making use of Adversarial Language Adaptation (ALA) (Joty et al., 2017; Chen et al., 2018) to train a CLED model.",
      "startOffset" : 72,
      "endOffset" : 110
    }, {
      "referenceID" : 35,
      "context" : "As such, we suggest using Optimal Transport (OT) (Villani, 2008) as a natural solution to simultaneously incorporate both the similarity between sample representations and the likelihood of the samples containing an event into a single framework.",
      "startOffset" : 49,
      "endOffset" : 64
    }, {
      "referenceID" : 37,
      "context" : "For our experiments, we focus on the widely used ACE05 and ACE05-ERE datasets (Walker et al., 2006) which, in conjuction, contain eventannotations in 4 different languages: English, Spanish, Chinese, and Arabic.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "Our proposed model obtains new state-of-the-art results with considerable performance improvements (+ 2-3% in F1 scores) over competitive baselines and previously published results (M’hamdi et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 203
    }, {
      "referenceID" : 22,
      "context" : "Following prior works (M’hamdi et al., 2019; Majewska et al., 2021), we treat ED as a sequence labeling problem.",
      "startOffset" : 22,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "Following prior works (M’hamdi et al., 2019; Majewska et al., 2021), we treat ED as a sequence labeling problem.",
      "startOffset" : 22,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "Here we briefly describe the BERT-CRF model (M’hamdi et al., 2019) which was the previous state-of-the-art and serves as our baseline.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "Using mBERT (Devlin et al., 2019) as its encoder, BERT-CRF generates robust, contextualized representations for words from different languages.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "For classification purposes, instead of assigning the labels of each token independently, BERT-CRF uses a Conditional Random Field (CRF) (Lafferty et al., 2001) layer on top of the prediction network to better capture the interactions between the label sequences.",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "The pre-trained versions of MLMs like mBERT or XLM-RoBERTa (Conneau et al., 2019) generate contextualized representations with a certain degree of language-invariance.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "This can be confirmed by their successful application in cross-lingual settings (M’hamdi et al., 2019; Majewska et al., 2021).",
      "startOffset" : 80,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "This can be confirmed by their successful application in cross-lingual settings (M’hamdi et al., 2019; Majewska et al., 2021).",
      "startOffset" : 80,
      "endOffset" : 125
    }, {
      "referenceID" : 32,
      "context" : "(2021), for instance, propose to address this issue by injecting external verb knowledge into the encoder via adapter modules (Pfeiffer et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "As such, we propose using Adversarial Language Adaptation (ALA) (Joty et al., 2017), a technique used to create language-invariant models.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "Equation 2 is implemented by using a GradientReversal Layer (GRL)(Ganin and Lempitsky, 2015) which acts as the identity during the forward pass, but reverses the direction of the gradients during the backward pass.",
      "startOffset" : 65,
      "endOffset" : 92
    }, {
      "referenceID" : 35,
      "context" : "We propose using Optimal Transport (OT) (Villani, 2008) as a natural way to combine these two metrics into a single framework for sample selection.",
      "startOffset" : 40,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Therefore, we use instead the Sinkhorn algorithm (Cuturi, 2013) which is an entropy-based relaxation of the discrete OT problem.",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 37,
      "context" : "We evaluate our model on the ACE05 (Walker et al., 2006) dataset which includes annotated eventtrigger data in 3 languages: English, Chinese and Arabic.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "We follow the same data pre-processing and splits as in previous work(M’hamdi et al., 2019) to ensure a fair comparison.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "BERT-CRF (M’hamdi et al., 2019), and XLM-R-CRF which is equivalent in all regards to BERT-CRF except that it uses XLM-RoBERTa as the encoder2.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 12,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 31,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 10,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 21,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 15,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 23,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 40,
      "context" : "Hand-crafted, feature-based, languagespecific methods were the basis of early ED approaches (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016).",
      "startOffset" : 92,
      "endOffset" : 286
    }, {
      "referenceID" : 26,
      "context" : ", 2016b), recurrent neural networks (Nguyen et al., 2016a; Sha et al., 2018; Nguyen and Nguyen, 2019), graph convolutional networks (Nguyen and Grishman, 2018a; Yan et al.",
      "startOffset" : 36,
      "endOffset" : 101
    }, {
      "referenceID" : 34,
      "context" : ", 2016b), recurrent neural networks (Nguyen et al., 2016a; Sha et al., 2018; Nguyen and Nguyen, 2019), graph convolutional networks (Nguyen and Grishman, 2018a; Yan et al.",
      "startOffset" : 36,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : ", 2016b), recurrent neural networks (Nguyen et al., 2016a; Sha et al., 2018; Nguyen and Nguyen, 2019), graph convolutional networks (Nguyen and Grishman, 2018a; Yan et al.",
      "startOffset" : 36,
      "endOffset" : 101
    }, {
      "referenceID" : 29,
      "context" : ", 2018; Nguyen and Nguyen, 2019), graph convolutional networks (Nguyen and Grishman, 2018a; Yan et al., 2019), adversarial networks (Hong et al.",
      "startOffset" : 63,
      "endOffset" : 109
    }, {
      "referenceID" : 39,
      "context" : ", 2018; Nguyen and Nguyen, 2019), graph convolutional networks (Nguyen and Grishman, 2018a; Yan et al., 2019), adversarial networks (Hong et al.",
      "startOffset" : 63,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : ", 2019), adversarial networks (Hong et al., 2018; Zhang et al., 2019b), and pre-trained language models (Wadden et al.",
      "startOffset" : 30,
      "endOffset" : 70
    }, {
      "referenceID" : 43,
      "context" : ", 2019), adversarial networks (Hong et al., 2018; Zhang et al., 2019b), and pre-trained language models (Wadden et al.",
      "startOffset" : 30,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "Works on cross-lingual ED are not as prevalent and generally make use of cross-lingual resources employed to address the differences between languages such as bilingual dictionaries or parallel corpora (Muis et al., 2018; Liu et al., 2019) and, more recently, pre-trained multilingual language models (M’hamdi et al.",
      "startOffset" : 202,
      "endOffset" : 239
    }, {
      "referenceID" : 19,
      "context" : "Works on cross-lingual ED are not as prevalent and generally make use of cross-lingual resources employed to address the differences between languages such as bilingual dictionaries or parallel corpora (Muis et al., 2018; Liu et al., 2019) and, more recently, pre-trained multilingual language models (M’hamdi et al.",
      "startOffset" : 202,
      "endOffset" : 239
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and, more recently, pre-trained multilingual language models (M’hamdi et al., 2019; Hambardzumyan et al., 2020; Majewska et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and, more recently, pre-trained multilingual language models (M’hamdi et al., 2019; Hambardzumyan et al., 2020; Majewska et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : ", 2019) and, more recently, pre-trained multilingual language models (M’hamdi et al., 2019; Hambardzumyan et al., 2020; Majewska et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Lempitsky, 2015; Naik and Rose, 2020), has been successfuly applied at generating languageinvariant models (Joty et al., 2017; Chen et al., 2018).",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "Lempitsky, 2015; Naik and Rose, 2020), has been successfuly applied at generating languageinvariant models (Joty et al., 2017; Chen et al., 2018).",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "Additional examples of downstream applications of cross-lingual learning are document classification (Holger and Xian, 2018), named entity recognition (Xie et al.",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 38,
      "context" : "Additional examples of downstream applications of cross-lingual learning are document classification (Holger and Xian, 2018), named entity recognition (Xie et al., 2018) and part-of-speech tag-",
      "startOffset" : 151,
      "endOffset" : 169
    } ],
    "year" : 0,
    "abstractText" : "In this work, we focus on Cross-Lingual Event Detection where a model is trained on data from a source language but its performance is evaluated on data from a second, target, language. Most recent works in this area have harnessed the language-invariant qualities displayed by pre-trained Multi-lingual Language Models. Their performance, however, reveals there is room for improvement as they mishandle delicate cross-lingual instances. We employ Adversarial Language Adaptation to train a Language Discriminator to discern between the source and target languages using unlabeled data. The discriminator is trained in an adversarial manner so that the encoder learns to produce refined, language-invariant representations that lead to improved performance. More importantly, we optimize the adversarial training . by only presenting the discriminator with the most informative samples. We base our intuition about what makes a sample informative on two disparate metrics: sample similarity and event presence. Thus, we propose using Optimal Transport as a solution to naturally combine these two distinct information sources into the selection process. Extensive experiments on 8 different language pairs, using 4 languages from unrelated families, show the flexibility and effectiveness of our model that achieves new state-of-the-art results.",
    "creator" : null
  }
}
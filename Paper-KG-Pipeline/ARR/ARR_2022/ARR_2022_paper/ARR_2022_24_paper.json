{
  "name" : "ARR_2022_24_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Pretraining with Synthetic Language: Studying Transferable Knowledge in Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pretrained language models (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020) have demonstrated strong empirical performance not only within a language but also across languages. Language models pretrained with a mix of monolingual corpora, such as multilingual BERT1, exhibit a decent zero-shot cross-lingual transfer capability, i.e., a model fine-tuned in a single source language (L1) can solve the task in another language (L2) (Conneau et al., 2020a; Xue et al., 2021). Surprisingly, the transfer happens without lexical overlaps between L1 and L2 (Karthikeyan K and Roth, 2020; Conneau et al., 2020b) or even without joint pretraining (Artetxe et al., 2020): an encoder only pretrained on L1 can be transferred to L2 without any parameter updates. These results suggest\n1https://github.com/google-research/ bert/blob/master/multilingual.md\nthat, whether when trained on single or multiple languages, the encoder learns some transferable knowledge about language.\nHowever, the characteristics of such transferable knowledge are still underexplored. Recent studies with the probing methodology (Hupkes and Zuidema, 2018; Conneau et al., 2018) have revealed that multilingual BERT captures languageindependent linguistic structures such as universal dependency relations (Chi et al., 2020) and subjecthood (Papadimitriou et al., 2021), but it remains unknown whether learning such linguistic properties actually contributes to the performance, and whether there exists more abstract knowledge transferred across languages.\nIn this study, we try to shed light on these questions with the framework of the Test for Inductive\nBias via Language Model Transfer (Papadimitriou and Jurafsky, 2020), focusing on designing synthetic languages with natural-language-like structural properties (Figure 1). We pretrain encoders with synthetic languages and transfer the encoders to natural language tasks with their parameters frozen. This enables us to see how the learning of the specific structural property in the synthetic language affects the downstream performance.\nSpecifically, we explore whether it is beneficial for the encoder to know the following two characteristics of natural language: word distributions and latent dependency structures. We design synthetic languages that represent such characteristics, and perform an extensive study with different encoder architectures (LSTM and Transformer), pretraining objectives (causal and masked language modelings), and downstream syntactic tasks (partof-speech tagging and dependency parsing).\nThe contribution is summarized as follows:\n• We first start by complementing the study in Papadimitriou and Jurafsky (2020). We train LSTM and Transformer encoders with the sentence-level causal language modeling task and evaluate the encoders in English. We show that a synthetic language that models simple statistical dependency within a sentence provides decent transferable knowledge on natural language modeling. Furthermore, contrary to the conclusion suggested by Papadimitriou and Jurafsky (2020), we find that the inductive bias of the nested head-to-tail dependency structure is more useful than the flat dependency structure. Our results also highlight that Transformer is less susceptible than LSTM to the structural difference of pretraining data.\n• We then proceed to investigate transfer learning in masked language modeling (Devlin et al., 2019), one of the current dominant pretraining paradigms. We evaluate pretrained Transformer encoders with two syntactic tasks, part-of-speech tagging and dependency parsing. We find that pretraining with unstructured synthetic languages does not provide useful transferable knowledge for these tasks, and again, we confirm that the nested dependency structure is important to learn the structure of natural language."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Transferable Structural Knowledge in Pretrained Encoders",
      "text" : "Multilingual language models trained with masked language modeling objective (Devlin et al., 2019; Doddapaneni et al., 2021) have demonstrated a surprisingly strong cross-lingual transfer capability (Liu et al., 2020a), given the model is only trained with a mix of monolingual corpora. This leads to several studies investigating the source of the cross-lingual capability of multilingual models.\nAn early common hypothesis was that the models take advantage of a common word-piece vocabulary across languages (Wu and Dredze, 2019; Pires et al., 2019), which provides cross-lingual alignment signals to learn useful multilingual representations. However, this hypothesis has been questioned by recent studies (Karthikeyan K and Roth, 2020; Conneau et al., 2020b), which shows that shared word-pieces only play a minor role in the performance. These studies suggest that the model can exploit abstract structures of languages to learn shared multilingual representations.\nAnother line of research suggests that the learning of transferable knowledge happens even in monolingual pretraining. Artetxe et al. (2020) showed that a Transformer encoder pretrained only on L1 exhibits strong cross-lingual transfer performance simply by aligning the L2 embeddings to the encoder. Papadimitriou and Jurafsky (2020) pretrained LSTM encoders with natural languages and non-linguistic data (e.g., code, music, and synthetic data) to demonstrate that the encoders achieve reasonable performance in Spanish language modeling. These studies provide additional evidence for the existence of transferable linguistic knowledge learned in the model.\nThen what is such knowledge? Probing studies (Hupkes and Zuidema, 2018; Conneau et al., 2018) has revealed that the model captures languageindependent structures such as universal dependency relations (Chi et al., 2020) and subjecthood (Papadimitriou et al., 2021). However, the probing methodology does not answer whether such linguistic knowledge contributes to the performance in cross-lingual transfer.\nIn this study, we shed light on this question by studying transfer learning from synthetic language with the framework of the Test for Inductive Bias via Language Model Transfer (TILT) (Papadimitriou and Jurafsky, 2020). The original motivation\nof the framework is that it enables us to assess if abstract features generalizable to L2 (natural language) are encoded in L1 without identifying those specific features, but here we explicitly design synthetic languages with some structural properties to investigate their transferability."
    }, {
      "heading" : "2.2 Synthetic Language",
      "text" : "To study the behavior of language models, several studies have employed a specific type of synthetic language: synthetic variants of natural languages. A typical experimental framework is as follows: (1) creating a synthetic language that differs from a natural language in one linguistic property, such as word orders (Sinha et al., 2021b; Dufter and Schütze, 2020; Sinha et al., 2021a), scripts (Karthikeyan K and Roth, 2020; Dufter and Schütze, 2020; Conneau et al., 2020b), or morphology (Ravfogel et al., 2019); (2) train or evaluate natural/synthetic language models and compare the performance to analyze the model’s sensitivity to the linguistic property.\nHowever, this methodology is limited to studying linguistic properties that are easily editable to create synthetic variants, and it is hard to study more abstract properties of language, such as token distributions and dependency structures.\nIn this research, we take a different approach: instead of creating synthetic variants of natural languages, we design synthetic languages with a certain property from scratch. On the basis of TILT, we study transfer learning to natural language from synthetic languages with abstract structural properties and see how the inductive bias on the structural properties affects the downstream performance."
    }, {
      "heading" : "3 Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Framework",
      "text" : "We first describe the experimental framework used throughout this paper, the Test for Inductive Bias via Language Model Transfer (TILT) introduced by Papadimitriou and Jurafsky (2020). TILT consists of pretraining and transfer steps:\n1. Pretrain an encoder with a pretraining task in the source language (L1). We explore pretraining with causal language modeling in §4 and masked language modeling in §5.\n2. Transfer the encoder to the target language (L2) in a downstream task. As we are interested in structural prior knowledge learned\nin the encoder, we discard the learned L1 word embeddings and initialize the embedding layer with the L2 vocabulary. We then train the model with the encoder parameters frozen and evaluate the task performance.\nTILT reveals how transferrable the computation induced to solve the L1 pretraining task is to processing L2. In this study, we are interested in the transferability of certain types of structures to natural language, and thus we primarily use handdesigned synthetic languages with the structural properties as L1 and natural language as L2."
    }, {
      "heading" : "3.2 Designing Synthetic Languages",
      "text" : "Synthetic languages are designed to mimic a certain property of natural language. After providing a formal definition of synthetic language, we introduce several languages used in this paper."
    }, {
      "heading" : "3.2.1 Formulation of Synthetic Language",
      "text" : "Synthetic language refers to a set of vocabulary and algorithms to generate sequential data for pretraining. Each language has a sentence-length distribution plen(l), token vocabulary {w|w ∈ V}, and sentence-sampling function f(l) : l 7→ V l. The training data is generated sentence by sentence as follows: we first sample a sentence length (l ∼ plen(l)) and then sample a sequence of tokens with that length ([w1, ..., wl] ∼ f(l)).\nIn this study, the token vocabulary V simply consists of integers (or integers with a special symbol) and is not intended to correspond to a vocabulary of any natural language. Also the sentence-length distribution plen(l) is fitted with a baseline dataset in each experiment. The focus is how to design the sentence-sampling function f(l). This determines what kind of characteristics we want to encode in the synthetic dataset."
    }, {
      "heading" : "3.2.2 Modeling Word Distribution",
      "text" : "Words in natural language are distributed in nontrivial fashions. We will study whether prior knowledge on token distribution facilitates learning from natural language. We first present the simplest synthetic language that serves as a baseline. Uniform language samples each token in a sentence independently and uniformly. Specifically, the probability of a token w being sampled is:\np(w) = 1\n|V| . (1)\nHowever, this deviates from the token distribution of natural language. Natural language is empirically known to follow the Zipf’s law (Zipf, 1949), i.e., the relation between the frequency of a word and its rank is given by frequency(w) ∝ rank(w)−α. The coefficient α is typically around 1, although the coefficient shows some variation according to the corpus domain (Zanette and Montemurro, 2005; Arnold and Schmidt, 2017). Zipf language captures this property and samples each token w from the following probability distribution assuming α = 1:\np(w) ∝ 1 rank(w) . (2)\nThe two languages introduced so far generates tokens in a sentence independently. However, words within a sentence of natural language are known to have statistical dependencies, i.e., specific cooccurrence patterns (Church and Hanks, 1989). Consider the sentence “The cat and dog are fighting over food.” The words the and cat would cooccur much more often than chance because cat (noun) is dependent on the (determinant); dog and cat would also do because they are topically related. The words in a sentence are usually coherent according to some syntactic and semantic dependencies. Random walk language is design to capture this property. Inspired by the random walk model in Arora et al. (2017), tokens in a sentence s are drawn from the following probability distribution:\np(w|s) ∝ exp(~cs · ~vw), (3)\nwhere ~cs is the discourse vector of the sentence and ~vw is the word vector of the token w. Intuitively, we can imagine that the discourse vector represents the topic of the sentence and decides the unigram distribution over the vocabulary (Blei et al., 2003). Sampling tokens this way, non-trivial cooccurrence patterns within sentences emerge in the language.\nWe speculate that pretraining with Random walk language will endow the model with an inductive bias to aggregate the context in a sentence to predict the identity or property of tokens, which is likely to benefit natural language processing.\nIn the experiments, the word vectors ~vw are initialized with the normal distribution, and the discourse vector ~cs is also drawn from the normal distribution each time we generate a sentence. We\nAlgorithm 1 Generating a sentence from the Nesting Dependency language. Input: input_pairs: Stack[(w, w)]] Output: sentence: List[w] 1: closing_stack = [] 2: while not input_pairs.is_empty() do 3: Uniform sampling p ∼ [0, 1] 4: if closing_stack.is_empty() or p < 0.4 then 5: head, tail = input_pairs.pop() 6: sentence.append(head) 7: closing_stack.push(tail) 8: else 9: tail = closing_stack.pop() 10: sentence.append(tail) 11: end if 12: end while 13: while not closing_stack.is_empty() do 14: tail = closing_stack.pop() 15: sentence.append(tail) 16: end while 17: return sentence\nset the dimension of the word and discourse vector to 10 as we empirically find this makes the entire token distribution close to the Zipfian distribution."
    }, {
      "heading" : "3.2.3 Modeling Latent Dependency Structure",
      "text" : "Sentences in natural language are known to have latent structures, which are often described in the form of trees (Chomsky, 1957) or dependency graphs (Mel’čuk, 1988). Now we consider how to endow the sampled tokens with such structures.\nIn this study, we adopt a dependency-based latent structure. Words in sentences of natural language often have dependency relations and the existence of a certain word can be predictive of another word (e.g., the verb am always cooccurs with I). We hypothesize that, pretrained on such data, language models may acquire inductive bias towards finding relations between tokens in the input, which is presumably important in processing natural language.\nInspired by Papadimitriou and Jurafsky (2020), we design algorithms that generate structured sentences given a set of tokens sampled with any of the strategies described in §3.2.2. The general idea is that half of the tokens (heads) in the vocabulary are all paired with another half of tokens (tails). A pair of head and tail can be represented in right and left brackets with the same integer (e.g., “<123”, “123>”). The pairs always appear together in a sentence and express simple dependency relations. After determining the sentence length l ∼ f(l), we first sample l2 (round to integer) pairs of head and tail, and then arrange them with one of the following structures. Flat Dependency structure simply arrange the to-\nkens randomly while keeping the right order of the brackets (e.g., [“<5”, “<84”, “5>”, “<123”, “123>”, “84>”]). The dependency arcs are allowed to be crossed and thus that often results in a non-projective dependency structure. Nesting Dependency language, in contrast, does not allow any dependency arcs to be crossed, and the brackets are nested hierarchically (e.g., [“<5”, “<84”, “84>”, “5>”, “<123”, “123>”]). The sentences are generated from a simple stack-based algorithm, described in Algorithm 1.\nThese structures are similar to the Parenthesis languages used to study the inductive bias of language models in Papadimitriou and Jurafsky (2020). However, our Dependency languages differ from them in how to represent the head and tail tokens. In the Parenthesis language, the head and tail are represented with the same token (e.g., [“5”, “84”, “84”, “5”, “123”, “123”]), which we argue deviates from the dependency structure in natural language, because in natural language, dependency relations usually hold between different words (e.g., I and am). We will show that this difference is in fact crucial and draw a different conclusion from Papadimitriou and Jurafsky (2020) on the importance of the nested structure (§4.2)."
    }, {
      "heading" : "4 Transfer from Synthetic Language in Language Modeling Task",
      "text" : "In this section, we complement the study of Papadimitriou and Jurafsky (2020). While they studied the inductive bias learned in LSTM encoders with some synthetic languages, here we provide additional studies with the newly introduced Random walk and Dependency synthetic languages, and the Transformer encoder."
    }, {
      "heading" : "4.1 Experimental Setups",
      "text" : "Task. We study sentence-level causal (left-to-right) language modeling, where the model needs to predict the next word given the previous context in the sentence. Note that, Papadimitriou and Jurafsky (2020) experiment with language modeling across sentences, but we adopt sentence-level modeling because we would like to focus on the learning of sentence structures and also simplify the task setting. As we will see in §4.2, we observe the same tendency in regard to the effect of synthetic pretraining where we share the setups. The task performance is measured by the average perplexity scores for each token.\nModel. We study two encoder architectures: LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017). These architectures are known to exhibit different abilities in capturing the underlying hierarchical structure of sequential data (Tran et al., 2018).\nThe size of word embeddings is set to 512. For both LSTM and Transformer encoders, the number of layers are set to 3, and the number of parameters is configured to be the same (9.4M parameters) to enable a fair comparison between architectures (for further details, see Appendix A). Pretraining Data. We generate synthetic corpora with three unstructured languages, which randomly arrange the tokens sampled from Uniform, Zipf, and Random walk languages, and four structured languages, which the combination of the Zipf sampling strategy and the structures of Flat Parenthesis, Nesting Parenthesis, Flat Dependency, and Nesting Dependency.\nWe also experiment with natural language corpora for comparison. We create training corpora from Wikipedia dumps of English, Japanese, and Spanish. The sentences are word-tokenized with the Moses tokenizer2 for English and Spanish and MeCab3 for Japanese.\nThe sentence lengths of synthetic data were sampled from the empirical distribution of the English Wikipedia corpus. The size of the vocabulary |V | is set to 32,000 for both synthetic and natural corpora, and out-of-vocabulary words in natural language are replaced with the OOV token. For each corpus, we sample 12.8 M sentences and train the model with one iteration over the corpus. Evaluation Data. We evaluate the pretrained encoders the Penn Treebank (PTB) corpus (Marcus et al., 1993) with preprocessing from Mikolov et al. (2010). Note that, when we train language models with the pretrained encoders, the parameters of the encoder is not updated and only the English word embeddings are learned from scratch (optimization details in Appendix A.2)."
    }, {
      "heading" : "4.2 Results",
      "text" : "We provide two baseline models trained on the L2 training corpus from scratch and trained with frozen random weights in the encoder to compare with pretrained encoders. To mitigate the bias from pretrained instances (Sellam et al., 2021), for each\n2https://github.com/moses-smt/ mosesdecoder\n3http://taku910.github.io/mecab/\nconfiguration we pretrained three encoders with different random seeds, and for each encoder finetuned three models, which results in nine models in total. We summarize the average scores and standard deviations in Figure 2.\nThe Transformer encoder is more flexible than LSTM. We observe that the Transformer encoders gives lower perplexity scores compared to LSTM regardless of pretraining language. This tendency is in line with the observations on the flexibility of Transformer encoders, e.g., pretrained Transformer encoders exhibit surprisingly good transferability to other languages (Conneau et al., 2020a), or even other modalities (Lu et al., 2021).\nThe Random walk language provides a useful inductive bias to language modeling. Looking at the difference among unstructured languages (Figure 2a), Uniform and Zipf languages give significantly higher perplexities than the Random weights baseline within each architecture, indicating that they provide no useful inductive bias for the language modeling task. On the contrary, the Random walk language gives significantly lower perplexities compared to Random weights (224.7 ± 11.2 vs. 411.2 ± 21.5 with LSTM and 178.6±3.5 vs. 192.9±1.8 with Transformer). This\nindicates that knowing the existence of statistical dependency within a sentence, or learning to predict tokens from the cooccurrence information, is a useful inductive bias even though the cooccurrence statistics is not necessarily in line with L2.\nWe do not observe the importance of the nested structure in the Parenthesis languages. Papadimitriou and Jurafsky (2020) showed that LSTM encoders trained on the Flat Parenthesis and Nesting Parenthesis structures provide no different perplexity points, and concluded that simple non-hierarchical head-dependent-type relations are important in LSTM language processing. The same observation can be made in Figure 2b, the Nesting Parenthesis and Flat Parenthesis languages do not differ significantly (187.9± 11.9 vs. 181.1± 8.1, p > 0.01 in Welch’s t-test).\nHowever, the Dependency languages suggest that the nested structure is actually important in LSTM language modeling. While the Parenthesis language represents dependency relations with two same tokens (e.g., “4543” and “4543”), our Dependency language with two different tokens (e.g., “<4543” and “4543>”). We expect that expressing dependency relations with two different tokens is closer to natural language and thus\nprovides more viable insights into natural language. When we compare the scores from Flat Dependency and Nesting Dependency with LSTM, Nesting Dependency performs much better (212.6±4.3 vs. 170.0± 1.7, p < 0.01). Also, Nesting Dependency with LSTM performs best among other synthetic languages, indicating our Dependency structure is closer to natural language and the nested structure is useful for LSTM.\nOn the contrary, the Transformer encoders seem to be less susceptible to the structure of pretraining language. We do not observe significant difference between Flat Dependency and Nesting Dependency with Transformer (169.6± 1.8 vs. 169.6 ± 2.3 perplexity points, p > 0.01). The insensitivity of Transformer can also be observed in the comparison among natural languages (Figure 2c). We can expect that the typologically closer L1 to L2, the lower the perplexity. Given Spanish is closer to English than Japanese, it is natural to observe that Spanish performs better than Japanese with LSTM (164.4± 3.0 vs. 173.5± 2.3, p < 0.01). However, this does not hold true for Transformer (157.2±2.0 in Spanish vs. 154.7±1.2 in Japanese, p > 0.01) and Japanese even performs the same with English (154.7±1.2 vs. 156.6±1.8, p > 0.01). This again indicates the flexibility of the Transformer architecture as computational engine (Lu et al., 2021) and may partially explain the strong cross-lingual transfer performance of Transformer language models (Liu et al., 2020b)."
    }, {
      "heading" : "5 Masked Language Model Pretraining with Synthetic Language",
      "text" : "We proceed to investigate transfer learning from synthetic languages in one of the most successful pretraining paradigms, masked language modeling (Devlin et al., 2019). Several studies have suggested that the source of high performance could be the lexical semantics derived from the distributional hypothesis (Mikolov et al., 2013; Sinha et al., 2021a), language-specific syntax (Liu et al., 2019; Tenney et al., 2019), and world knowledge presented in the pretraining text (Petroni et al., 2019). Here we investigate other possible factors, abstract structural knowledge presented in data, through pretraining with synthetic languages."
    }, {
      "heading" : "5.1 Experimental Setups",
      "text" : "Pretraining. To allow for fast experimentation, we train small Transformer encoders. The size of word\nembeddings is set to 300 and the encoders have three layers (further details in Appendix B). The pretraining datasets are the same as in §4.1, except that we do not train models with the Parenthesis languages in this experiment. Downstream Tasks. We evaluate the pretrained encoder with two syntactic tasks of natural language, part-of-speech (PoS) tagging and dependency parsing, to see if the structural prior knowledge learned with synthetic language is beneficial to predict the structure of natural language.\nFor both tasks, we use Universal Dependencies (UD) v2.84. We experiment with 3 typologically different languages, English, Finnish and Japanese, and observed similar trends across languages. Thus, here we report the results from the English-EWT dataset and present the results from other languages and discuss the differences in Appendix C.\nTo perform PoS tagging, we simply stack a linear layer on top of the Transformer encoder to predict the PoS labels of the input words. For dependency parsing, we adopt the biaffine graph-based parser (Dozat and Manning, 2017). The word representations for both models are the concatenation of word embeddings and character features computed by a character-level bi-directional LSTM encoder (Ling et al., 2015). For the details on fine-tuning these models, please refer to Appendix B."
    }, {
      "heading" : "5.2 Results",
      "text" : "We provide two baseline models trained from scratch and trained with random encoder weights. For each pretraining language, we again train three encoders and fine-tune three models for each, and take the mean and standard deviation of the nine models. Figure 3 shows the results on PoS tagging and dependency parsing.\nFirstly, we observe that the Random weights baseline exhibits decent performance on PoS tagging (91.2± 0.4 points in accuracy) compared to the encoder learned from scratch (92.6± 0.4). The task of PoS tagging seems to require little structural knowledge on language, and decent performance can be achieved solely on the word features of each token. This is in contrast with dependency parsing, where the Random weights baseline performs much worse than the fully trained model (63.4±0.1 vs. 80.0± 0.6 points in UAS). Thus, we will focus on the results of dependency parsing to discuss the structural knowledge learned in the encoder.\n4https://universaldependencies.org/\nThe unstructured languages do not provide useful transferable knowledge for these syntactic tasks. Pretraining with the Uniform, Zipf, and Random walk languages all results in performance worse than the Random weights baseline. This is in contrast with the causal language modeling task, where the Random walk language at least outperforms the Random weights baseline (§4.2).\nOn the other hand, learning from structured languages seems to be important in dependency parsing. Flat Dependency and Nesting Dependency show better performance (65.3±1.2 and 69.3±0.6 UAS respectively) than Random weights (63.4±0.1).\nBy comparing between the structured synthetic languages, we can see that learning the nested structure is important to solve the syntactic tasks in natural language. Nesting Dependency outperforms Flat Dependency significantly in dependency parsing (69.3±0.6 vs. 65.3±1.2 points). However, there are still gaps between the synthetic languages and natural languages, which suggest that there is more for the encoder to learn from natural language than the simple dependency structures in the synthetic languages."
    }, {
      "heading" : "6 Discussion and Future Work",
      "text" : "In this paper, we studied what kind of structural knowledge represented in synthetic language can be transferred to solve natural language tasks. One recurring observation is the importance of the nested structure: pretrained encoders with the Nesting Dependency language provide better task performance compared to other synthetic languages. We conjecture that this is because the nested structure is more common in natural language (e.g., for\nmost languages in the UD treebanks, over 90% of the dependency trees are projective) and thus prior knowledge on such structure facilitates processing natural language.\nWe also observe that the Nesting Dependency language provides downstream performance close to natural languages. This suggests that a simple computational capacity to process such a structure goes a long way in processing natural language.\nOne interesting future direction is to develop synthetic languages and a pretraining scheme that improve downstream performance. Its feasibility is supported by emerging studies on synthetic pretraining in other domains, such as image processing (Kataoka et al., 2020) and mathematical reasoning (Wu et al., 2021). By designing more natural-language-like languages, we can not only deepen our understanding of natural language but also develop a practical yet resource-free pretraining method.\nFinally, our synthetic languages are reminiscent of Universal Grammar (UG) (Chomsky, 1957; Nivre, 2015). Although the definition of UG, or even its existence, is controversial (Dąbrowska, 2015), here we refer to UG as underlying structural properties that all human languages have in common. In this study, we discussed statistical dependency within a sentence and head-to-tail dependency structures as underlying properties of language. Future work could investigate if other types of linguistic universals (Greenberg, 1963), such as the existence of grammatical categories (e.g., noun and verb) or tendencies about word orders, benefit language processing, and if any, how much it does."
    }, {
      "heading" : "Appendix for “Pretraining with Synthetic Language: Studying Transferable Knowledge in Language Models”",
      "text" : ""
    }, {
      "heading" : "A Details of Causal Language Modeling Task",
      "text" : "A.1 Model configuration For the experiment with causal language modeling (§4), we set the number of layers of the LSTM and Transformer encoders to 3 and configure them so that they have the same number of parameters (9.4 M parameters without the embedding and output projection layers). The details of configuration are shown in Table 1 and Table 2.\nThe weights of the output projection layer are tied with the word embedding layer (Press and Wolf, 2017). Note that, to enable this, the LSTM encoder has an additional linear layer to project the hidden vector (626 dim) to the input size (512 dim), which the Transformer encoder does not have.\nA.2 Optimization We optimize the pretrained models for 10k steps with 12.8 M sentences and the batch size of 128 using AdamW (Loshchilov and Hutter, 2019). We use the the Noam Learning rate scheduler described in Vaswani et al. (2017) with the warmup steps of 4000, and the other hyper-parameter details are shown in Table 3. We use the same hyper-parameters for fine-tuning with the L2 language."
    }, {
      "heading" : "B Details of Masked Language Modeling Task",
      "text" : "B.1 Model configuration For the experiment with masked language modeling (§5), we set the number of layers of the Transformer encoders to 3. The details of configuration are shown in Table 4 (2.1 M parameters without the embedding and output projection layers).\nThe hyper-parameters for the masked language modeling task is shown in Table 5. For optimization, we used the same hyper-parameters as in Appendix A.2."
    }, {
      "heading" : "C Results from other languages",
      "text" : "Here we present the result for §5 in Finnish and Japanese in Figure 4 and 5. The statistics of the dataset are show in Table 6.\nOverall, we confirm that we observe similar trend as in English (§5). PoS tagging does not seem to require structural knowledge, as indicated by less variance in the performance across pretraining languages. In Finnish, the Random weights baselines even outperforms the model trained from scratch (84.3± 0.7 vs. 87.1± 0.2) in this hyper-parameter setting. Pretraining with the unstructured languages provides lower scores than the Random weights baseline overall, confirming the importance of structural knowledge encoded in the pretraining data.\nWe can also confirm that the Nesting Dependency outperforms Flat Dependency in dependency parsing (65.1± 1.2 vs. 61.9± 1.0 in Finnish, and 32.8± 0.2 vs. 31.9± 0.4 in Japanese).\nInterestingly, in Finnish, the Nesting Dependency language outperforms natural languages (English, Spanish, Japanese), in contrast with the results from other languages where pretraining with natural languages performs better than Nesting Dependency in English (§5.2) and they exhibit comparable\nperformance in the Japanese dataset (Figure 5). We leave the investigation of this dataset or languagedependent difference to future work."
    }, {
      "heading" : "D Computing Infrastructure",
      "text" : "We run the experiments on a server with a Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz CPU and 10 NVIDIA TITAN Xp GPUs. Each pretraining and finetuning are run with a single GPU."
    } ],
    "references" : [ {
      "title" : "Changes in the coefficients of zipf’s law for english corpora of different contexts",
      "author" : [ "Robert Arnold", "Deena R. Schmidt." ],
      "venue" : "Undergraduate Research Journal, 3:14–25.",
      "citeRegEx" : "Arnold and Schmidt.,? 2017",
      "shortCiteRegEx" : "Arnold and Schmidt.",
      "year" : 2017
    }, {
      "title" : "A simple but tough-to-beat baseline for sentence embeddings",
      "author" : [ "Sanjeev Arora", "Yingyu Liang", "Tengyu Ma." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Arora et al\\.,? 2017",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2017
    }, {
      "title" : "On the Cross-lingual Transferability of Monolingual Representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "A. Ng", "Michael I. Jordan." ],
      "venue" : "Journal of Machine Learning Research, 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Finding universal grammatical relations in multilingual BERT",
      "author" : [ "Ethan A. Chi", "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564–5577, Online. As-",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "Word association norms, mutual information, and lexicography",
      "author" : [ "Kenneth Ward Church", "Patrick Hanks." ],
      "venue" : "27th Annual Meeting of the Association for Computational Linguistics, pages 76–83, Vancouver, British Columbia, Canada. Association for",
      "citeRegEx" : "Church and Hanks.,? 1989",
      "shortCiteRegEx" : "Church and Hanks.",
      "year" : 1989
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020a",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
      "author" : [ "Alexis Conneau", "German Kruszewski", "Guillaume Lample", "Loïc Barrault", "Marco Baroni." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the As-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Emerging Cross-lingual Structure in Pretrained Language Models",
      "author" : [ "Alexis Conneau", "Shijie Wu", "Haoran Li", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Conneau et al\\.,? 2020b",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A Primer on Pretrained Multilingual Language Models",
      "author" : [ "Sumanth Doddapaneni", "Gowtham Ramesh", "Anoop Kunchukuttan", "Pratyush Kumar", "Mitesh M. Khapra." ],
      "venue" : "ArXiv, abs/2107.00676.",
      "citeRegEx" : "Doddapaneni et al\\.,? 2021",
      "shortCiteRegEx" : "Doddapaneni et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Identifying elements essential for BERT’s multilinguality",
      "author" : [ "Philipp Dufter", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4423–4437, Online. Association for Computa-",
      "citeRegEx" : "Dufter and Schütze.,? 2020",
      "shortCiteRegEx" : "Dufter and Schütze.",
      "year" : 2020
    }, {
      "title" : "What exactly is universal grammar, and has anyone seen it",
      "author" : [ "Ewa Dąbrowska" ],
      "venue" : "Frontiers in Psychology,",
      "citeRegEx" : "Dąbrowska.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dąbrowska.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9:1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Visualisation and ’diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure (extended abstract)",
      "author" : [ "Dieuwke Hupkes", "Willem Zuidema." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Con-",
      "citeRegEx" : "Hupkes and Zuidema.,? 2018",
      "shortCiteRegEx" : "Hupkes and Zuidema.",
      "year" : 2018
    }, {
      "title" : "Cross-Lingual Ability of Multilingual BERT: An Empirical Study",
      "author" : [ "Stephen Mayhew Karthikeyan K", "Zihan Wang", "Dan Roth." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "K et al\\.,? 2020",
      "shortCiteRegEx" : "K et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training without natural images",
      "author" : [ "Hirokatsu Kataoka", "Kazushige Okayasu", "Asato Matsumoto", "Eisuke Yamagata", "Ryosuke Yamada", "Nakamasa Inoue", "Akio Nakamura", "Yutaka Satoh." ],
      "venue" : "ACCV.",
      "citeRegEx" : "Kataoka et al\\.,? 2020",
      "shortCiteRegEx" : "Kataoka et al\\.",
      "year" : 2020
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramón Fermandez", "Silvio Amir", "Luís Marujo", "Tiago Luís." ],
      "venue" : "Proceedings of the 2015",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Linguistic knowledge and transferability of contextual",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual Denoising Pre-training for Neural Machine Translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Lin-",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Decoupled Weight Decay Regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Pretrained transformers as universal computation engines",
      "author" : [ "Kevin Lu", "Aditya Grover", "P. Abbeel", "Igor Mordatch." ],
      "venue" : "ArXiv, abs/2103.05247.",
      "citeRegEx" : "Lu et al\\.,? 2021",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2021
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz." ],
      "venue" : "Comput. Linguistics, 19:313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Dependency syntax: Theory and practice",
      "author" : [ "Igor Mel’čuk" ],
      "venue" : null,
      "citeRegEx" : "Mel.čuk.,? \\Q1988\\E",
      "shortCiteRegEx" : "Mel.čuk.",
      "year" : 1988
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "G. Corrado", "J. Dean." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukás Burget", "Jan Honza Cernocký", "Sanjeev Khudanpur." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Towards a universal grammar for natural language processing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "CICLing.",
      "citeRegEx" : "Nivre.,? 2015",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2015
    }, {
      "title" : "Deep subjecthood: Higher-order grammatical features in multilingual BERT",
      "author" : [ "Isabel Papadimitriou", "Ethan A. Chi", "Richard Futrell", "Kyle Mahowald." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computa-",
      "citeRegEx" : "Papadimitriou et al\\.,? 2021",
      "shortCiteRegEx" : "Papadimitriou et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning Music Helps You Read: Using transfer to study linguistic structure in language models",
      "author" : [ "Isabel Papadimitriou", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Papadimitriou and Jurafsky.,? 2020",
      "shortCiteRegEx" : "Papadimitriou and Jurafsky.",
      "year" : 2020
    }, {
      "title" : "Language Models as Knowledge Bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Using the output embedding to improve language models",
      "author" : [ "Ofir Press", "Lior Wolf." ],
      "venue" : "EACL.",
      "citeRegEx" : "Press and Wolf.,? 2017",
      "shortCiteRegEx" : "Press and Wolf.",
      "year" : 2017
    }, {
      "title" : "Exploring the Limits of Transfer Learning with a Unified Textto-Text Transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Studying the inductive biases of RNNs with synthetic variations of natural languages",
      "author" : [ "Shauli Ravfogel", "Yoav Goldberg", "Tal Linzen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Ravfogel et al\\.,? 2019",
      "shortCiteRegEx" : "Ravfogel et al\\.",
      "year" : 2019
    }, {
      "title" : "The multiberts: Bert reproductions for robustness analysis",
      "author" : [ "Thibault Sellam", "Steve Yadlowsky", "Jason Wei", "Naomi Saphra", "Alexander D’Amour", "Tal Linzen", "Jasmijn Bastings", "Iulia Turc", "Jacob Eisenstein", "Dipanjan Das", "Ian Tenney", "Ellie Pavlick" ],
      "venue" : null,
      "citeRegEx" : "Sellam et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2021
    }, {
      "title" : "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
      "author" : [ "Koustuv Sinha", "Robin Jia", "Dieuwke Hupkes", "Joëlle Pineau", "Adina Williams", "Douwe Kiela." ],
      "venue" : "ArXiv, abs/2104.06644.",
      "citeRegEx" : "Sinha et al\\.,? 2021a",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "UnNatural Language Inference",
      "author" : [ "Koustuv Sinha", "Prasanna Parthasarathi", "Joelle Pineau", "Adina Williams." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
      "citeRegEx" : "Sinha et al\\.,? 2021b",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "What do you learn from context? probing for sentence structure",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R. Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "The importance of being recurrent for modeling hierarchical structure",
      "author" : [ "Ke Tran", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4731–4736, Brussels, Bel-",
      "citeRegEx" : "Tran et al\\.,? 2018",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Lime: Learning inductive bias for primitives of mathematical reasoning",
      "author" : [ "Yuhuai Wu", "Markus N. Rabe", "Wenda Li", "Jimmy Ba", "Roger B. Grosse", "Christian Szegedy." ],
      "venue" : "ArXiv, abs/2101.06223.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Xue et al\\.,? 2021",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Dynamics of text generation with realistic zipf’s distribution",
      "author" : [ "Damián H. Zanette", "Marcelo A. Montemurro." ],
      "venue" : "Journal of Quantitative Linguistics, 12:29 – 40.",
      "citeRegEx" : "Zanette and Montemurro.,? 2005",
      "shortCiteRegEx" : "Zanette and Montemurro.",
      "year" : 2005
    }, {
      "title" : "Human behavior and the principle of least effort",
      "author" : [ "George Kingsley Zipf." ],
      "venue" : "Cambridge, MA: AddisonWesley. 11",
      "citeRegEx" : "Zipf.,? 1949",
      "shortCiteRegEx" : "Zipf.",
      "year" : 1949
    }, {
      "title" : "2017) with the warmup steps of 4000, and the other hyper-parameter details are shown in Table 3. We use the same hyper-parameters for fine-tuning with the L2",
      "author" : [ "Vaswani" ],
      "venue" : null,
      "citeRegEx" : "Vaswani,? \\Q2017\\E",
      "shortCiteRegEx" : "Vaswani",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Pretrained language models (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020) have demonstrated strong empirical performance not only",
      "startOffset" : 27,
      "endOffset" : 88
    }, {
      "referenceID" : 45,
      "context" : "Pretrained language models (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020) have demonstrated strong empirical performance not only",
      "startOffset" : 27,
      "endOffset" : 88
    }, {
      "referenceID" : 34,
      "context" : "Pretrained language models (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020) have demonstrated strong empirical performance not only",
      "startOffset" : 27,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : ", a model fine-tuned in a single source language (L1) can solve the task in another language (L2) (Conneau et al., 2020a; Xue et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 139
    }, {
      "referenceID" : 44,
      "context" : ", a model fine-tuned in a single source language (L1) can solve the task in another language (L2) (Conneau et al., 2020a; Xue et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "Surprisingly, the transfer happens without lexical overlaps between L1 and L2 (Karthikeyan K and Roth, 2020; Conneau et al., 2020b) or even without joint pretraining (Artetxe et al.",
      "startOffset" : 78,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : ", 2020b) or even without joint pretraining (Artetxe et al., 2020): an encoder only",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : ", 2018) have revealed that multilingual BERT captures languageindependent linguistic structures such as universal dependency relations (Chi et al., 2020) and subjecthood (Papadimitriou et al.",
      "startOffset" : 135,
      "endOffset" : 153
    }, {
      "referenceID" : 29,
      "context" : ", 2020) and subjecthood (Papadimitriou et al., 2021), but it remains unknown whether learning such linguistic properties actually contributes to the performance, and whether there exists more abstract knowledge transferred across languages.",
      "startOffset" : 24,
      "endOffset" : 52
    }, {
      "referenceID" : 30,
      "context" : "Bias via Language Model Transfer (Papadimitriou and Jurafsky, 2020), focusing on designing synthetic languages with natural-language-like structural properties (Figure 1).",
      "startOffset" : 33,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "• We then proceed to investigate transfer learning in masked language modeling (Devlin et al., 2019), one of the current dominant pretraining paradigms.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "Multilingual language models trained with masked language modeling objective (Devlin et al., 2019; Doddapaneni et al., 2021) have demonstrated a surprisingly strong cross-lingual transfer capability (Liu et al.",
      "startOffset" : 77,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "Multilingual language models trained with masked language modeling objective (Devlin et al., 2019; Doddapaneni et al., 2021) have demonstrated a surprisingly strong cross-lingual transfer capability (Liu et al.",
      "startOffset" : 77,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : ", 2021) have demonstrated a surprisingly strong cross-lingual transfer capability (Liu et al., 2020a), given the model is only trained with a mix of monolingual corpora.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 42,
      "context" : "An early common hypothesis was that the models take advantage of a common word-piece vocabulary across languages (Wu and Dredze, 2019; Pires et al., 2019), which provides cross-lingual alignment signals to learn useful multilingual representations.",
      "startOffset" : 113,
      "endOffset" : 154
    }, {
      "referenceID" : 32,
      "context" : "An early common hypothesis was that the models take advantage of a common word-piece vocabulary across languages (Wu and Dredze, 2019; Pires et al., 2019), which provides cross-lingual alignment signals to learn useful multilingual representations.",
      "startOffset" : 113,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "questioned by recent studies (Karthikeyan K and Roth, 2020; Conneau et al., 2020b), which shows that shared word-pieces only play a minor role in the performance.",
      "startOffset" : 29,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "Then what is such knowledge? Probing studies (Hupkes and Zuidema, 2018; Conneau et al., 2018) has revealed that the model captures languageindependent structures such as universal dependency relations (Chi et al.",
      "startOffset" : 45,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "Then what is such knowledge? Probing studies (Hupkes and Zuidema, 2018; Conneau et al., 2018) has revealed that the model captures languageindependent structures such as universal dependency relations (Chi et al.",
      "startOffset" : 45,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : ", 2018) has revealed that the model captures languageindependent structures such as universal dependency relations (Chi et al., 2020) and subjecthood (Papadimitriou et al.",
      "startOffset" : 115,
      "endOffset" : 133
    }, {
      "referenceID" : 30,
      "context" : "In this study, we shed light on this question by studying transfer learning from synthetic language with the framework of the Test for Inductive Bias via Language Model Transfer (TILT) (Papadimitriou and Jurafsky, 2020).",
      "startOffset" : 185,
      "endOffset" : 219
    }, {
      "referenceID" : 38,
      "context" : "A typical experimental framework is as follows: (1) creating a synthetic language that differs from a natural language in one linguistic property, such as word orders (Sinha et al., 2021b; Dufter and Schütze, 2020; Sinha et al., 2021a), scripts (Karthikeyan K and Roth, 2020; Dufter and Schütze, 2020; Conneau et al.",
      "startOffset" : 167,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : "A typical experimental framework is as follows: (1) creating a synthetic language that differs from a natural language in one linguistic property, such as word orders (Sinha et al., 2021b; Dufter and Schütze, 2020; Sinha et al., 2021a), scripts (Karthikeyan K and Roth, 2020; Dufter and Schütze, 2020; Conneau et al.",
      "startOffset" : 167,
      "endOffset" : 235
    }, {
      "referenceID" : 37,
      "context" : "A typical experimental framework is as follows: (1) creating a synthetic language that differs from a natural language in one linguistic property, such as word orders (Sinha et al., 2021b; Dufter and Schütze, 2020; Sinha et al., 2021a), scripts (Karthikeyan K and Roth, 2020; Dufter and Schütze, 2020; Conneau et al.",
      "startOffset" : 167,
      "endOffset" : 235
    }, {
      "referenceID" : 35,
      "context" : "ogy (Ravfogel et al., 2019); (2) train or evaluate natural/synthetic language models and compare the performance to analyze the model’s sensitivity to the linguistic property.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 47,
      "context" : "Natural language is empirically known to follow the Zipf’s law (Zipf, 1949), i.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Intuitively, we can imagine that the discourse vector represents the topic of the sentence and decides the unigram distribution over the vocabulary (Blei et al., 2003).",
      "startOffset" : 148,
      "endOffset" : 167
    }, {
      "referenceID" : 25,
      "context" : "Sentences in natural language are known to have latent structures, which are often described in the form of trees (Chomsky, 1957) or dependency graphs (Mel’čuk, 1988).",
      "startOffset" : 151,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "We study two encoder architectures: LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al.",
      "startOffset" : 41,
      "endOffset" : 75
    }, {
      "referenceID" : 41,
      "context" : "We study two encoder architectures: LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017).",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 40,
      "context" : "These architectures are known to exhibit different abilities in capturing the underlying hierarchical structure of sequential data (Tran et al., 2018).",
      "startOffset" : 131,
      "endOffset" : 150
    }, {
      "referenceID" : 24,
      "context" : "We evaluate the pretrained encoders the Penn Treebank (PTB) corpus (Marcus et al., 1993) with preprocessing from Mikolov et al.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 36,
      "context" : "To mitigate the bias from pretrained instances (Sellam et al., 2021), for each",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "This again indicates the flexibility of the Transformer architecture as computational engine (Lu et al., 2021) and may partially explain the strong cross-lingual transfer performance of Transformer language models (Liu et al.",
      "startOffset" : 93,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : ", 2021) and may partially explain the strong cross-lingual transfer performance of Transformer language models (Liu et al., 2020b).",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "We proceed to investigate transfer learning from synthetic languages in one of the most successful pretraining paradigms, masked language modeling (Devlin et al., 2019).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 26,
      "context" : "Several studies have suggested that the source of high performance could be the lexical semantics derived from the distributional hypothesis (Mikolov et al., 2013; Sinha et al., 2021a), language-specific syntax (Liu et al.",
      "startOffset" : 141,
      "endOffset" : 184
    }, {
      "referenceID" : 37,
      "context" : "Several studies have suggested that the source of high performance could be the lexical semantics derived from the distributional hypothesis (Mikolov et al., 2013; Sinha et al., 2021a), language-specific syntax (Liu et al.",
      "startOffset" : 141,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : ", 2021a), language-specific syntax (Liu et al., 2019; Tenney et al., 2019), and world knowledge presented in the pretraining text (Petroni et al.",
      "startOffset" : 35,
      "endOffset" : 74
    }, {
      "referenceID" : 39,
      "context" : ", 2021a), language-specific syntax (Liu et al., 2019; Tenney et al., 2019), and world knowledge presented in the pretraining text (Petroni et al.",
      "startOffset" : 35,
      "endOffset" : 74
    }, {
      "referenceID" : 31,
      "context" : ", 2019), and world knowledge presented in the pretraining text (Petroni et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "For dependency parsing, we adopt the biaffine graph-based parser (Dozat and Manning, 2017).",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "by a character-level bi-directional LSTM encoder (Ling et al., 2015).",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Its feasibility is supported by emerging studies on synthetic pretraining in other domains, such as image processing (Kataoka et al., 2020) and mathematical reasoning (Wu et al.",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 43,
      "context" : ", 2020) and mathematical reasoning (Wu et al., 2021).",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 28,
      "context" : "Finally, our synthetic languages are reminiscent of Universal Grammar (UG) (Chomsky, 1957; Nivre, 2015).",
      "startOffset" : 75,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "Although the definition of UG, or even its existence, is controversial (Dąbrowska, 2015), here we refer to UG as underlying structural properties that all human languages have in common.",
      "startOffset" : 71,
      "endOffset" : 88
    } ],
    "year" : 0,
    "abstractText" : "We investigate what kind of linguistic structural knowledge in neural network encoders is transferable and beneficial to processing natural language. We design synthetic languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language. We show that encoders pretrained on a synthetic language with simple statistical dependency within a sentence perform reasonably well on causal language modeling in English, and learning to process nested dependency structures is beneficial for syntactic tasks. Also, we compare two different encoder architecture, LSTM and Transformer, and show that Transformer is less susceptible to structural differences in pretraining data. Our results provide insights on how neural network encoders process human languages, and the source of cross-lingual transferability of recent multilingual language models.",
    "creator" : null
  }
}
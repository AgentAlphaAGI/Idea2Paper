{
  "name" : "ARR_2022_269_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Natural language understanding (NLU) refers to the ability of a system to ‘comprehend’ the meaning (semantics) and the structure (syntax) of human language (Wang et al., 2019) to enable the interaction with a system or device. Cross-lingual natural language understanding (XNLU) alludes to a system that is able to handle multiple languages simultaneously (Artetxe and Schwenk, 2019; Hu et al., 2020). We focus on task-oriented XNLU that comprises two correlated objectives: i) Intent Classification, which identifies the type of user command, e.g. ‘edit_reminder’, ‘send_message’ or ‘play_music’ and ii) Entity Recognition, which identifies relevant entities in the utterance including their types such as dates, messages, music tracks, locations, etc. In a modular dialogue system, this information is used by the dialogue manager to de-\ncide how to respond to the user (Casanueva et al., 2017; Gritta et al., 2021). For neural XNLU systems, the limited availability of annotated data is a significant barrier to scaling dialogue systems to more users (Razumovskaia et al., 2021). Therefore, we can use cross-lingual methods to zero-shot transfer the knowledge learnt in a high-resource language such as English to the target language of choice (Artetxe et al., 2020; Siddhant et al., 2020). To this end, we introduce a variety of alignment methods for zero-shot cross-lingual transfer, most notably CrossAligner. Our methods leverage unlabelled parallel data and can be integrated with pretrained language models such as XLMRoBERTa (Conneau et al., 2020) or Multilingual BERT (Devlin et al., 2019), which we refer to as XLM1 throughout the paper. Our methods (also referred to as auxiliary losses) help the XLM align its cross-lingual representations while optimising the primary XNLU tasks, which are learned only in the source language and transferred zero-shot to the target language. Finally, we also investigate the effectiveness of simple and weighted combinations of multiple alignment losses, which leads to further model improvements and insights. Our contributions can be summarised as follows:\n• We introduce CrossAligner, a cross-lingual transfer method that achieves SOTA performance on three benchmark XNLU datasets.\n• We introduce Translate-Intent, a simple and effective baseline, which outperforms its commonly used counterpart ‘Translate-Train’.\n• We introduce Contrastive Alignment, an auxiliary loss that leverages contrastive learning at a much smaller scale than past work.\n• We introduce weighted combinations of the above losses to further improve SOTA scores.\n• Qualitative analysis aims to guide future research by examining the remaining errors.\n1Not to be confused with Lample and Conneau (2019)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Several approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models’ training routine.\nData-based Transfer Translating utterances for the intent classification task is relatively straightforward so previous works focused on projecting and/or aligning the entity labels between translated utterances (Dou and Neubig, 2021). This technique is followed by standard supervised training with those pseudo-labels and is commonly known as a translate-train method. One of the earliest works still being used for this purpose is fastalign (Dyer et al., 2013). It’s an unsupervised word aligner trained on a parallel corpus to map each word (thus its entity label) in the source utterance to the word(s) in the target user utterance. Projecting the entity labels can also be done with word-by-word translation and source label copying (Yi and Cheng, 2021). A teacher model then weakly labels the target data, which is used to train the final student model. Sometimes, this type of label projection is complemented with an additional entity alignment step (Li et al., 2021a). Better performance can be achieved by using machine translation with entity matching and distributional statistics (Jain et al., 2019) though this can be a costly process for each language. A category of ‘word substitution’ methods such as code-switching (Qin et al., 2020; Kuwanto et al., 2021) or dictionary-enhanced pretraining (Chaudhary et al., 2020) have also been shown to improve cross-lingual transfer.\nModel-based Transfer Prior to the adoption of multilingual transformers (Lample and Conneau, 2019), task-oriented XNLU methods employed a BiLSTM encoder combined with different multilingual embeddings (Schuster et al., 2019). Newer approaches usually involve a pretrained XLM and the addition of some new training component with the inference routine remaining mostly unchanged. Xu et al. (2020) learn to jointly align and predict entity labels by fusing the source and target language embeddings with attention and using the resulting cross-lingual representation for entity prediction. Qi and Du (2020) include an adversarial language detector in training whose loss encourages the model to generate language-agnostic sentence representations for improved zero-shot trans-\nfer. Pan et al. (2020) and Chi et al. (2020) added a contrastive loss to pretraining that treats translated sentences as positive examples and unrelated sentences as negative samples. This training step helps the XLM produce similar embeddings in different languages. However, these methods require large annotated datasets and expensive model pretraining (Chi et al., 2020). Our proposed methods only use the English task data (which is relatively limited) and its translations for each language.\nThe most related prior works are Arivazhagan et al. (2019) for machine translation and Gritta and Iacobacci (2021) for task-oriented XNLU. Both of these are cross-lingual alignment methods that use translated training data to zero-shot transfer the source language model to the target language. We focus on the latter work, called XeroAlign, which reported the most recent SOTA scores on our evaluation datasets. XeroAlign works by generating a sentence embedding of the user utterance for each language, e.g. English (source) and Thai (target) using the CLS token of the XLM. A Mean Squared Error loss function minimises the difference between the multilingual sentence embeddings and is backpropagated along with the main task loss. XeroAlign aims to bring sentence embeddings in different languages closer together with a bias towards intent classification due to the CLS embedding, which is the standard input to the intent classifier. We reproduce this method for analysis and comparisons but add a small post-processing step that distinctly improves the reported scores."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 CrossAligner",
      "text" : "Intuition We introduce CrossAligner, the most notable of our proposed cross-lingual alignment methods, outlined in Algorithm 1. CrossAligner enables effective zero-shot transfer by leveraging unlabelled parallel data for our new language-agnostic objective created through a transformation of the English entity labels. CrossAligner was borne out of early error analysis where we observed that the model incorrectly predicted entities that didn’t occur in the input and failed to predict entities that did occur in the input. Using this insight as our main motivation, the essence of CrossAligner is being able to exploit information about the presence of entities in the user utterance despite losing their positional information during translation. While we have used a proprietary service to translate the\nEnglish user utterances XEng into each target language XTar, a publicly available translator can also be used. Note that we use the same translations for each of our alignment methods to compare them fairly. Our language-agnostic objective is created by transforming the English slot labels yec into a fixed binary vector yca indicating which entities are present in the input (lines 1-7 in Algorithm 1), irrespective of the frequency of their occurrence. Our new objective is shared between languages.\nTraining The standard XNLU training (lines 15- 20) features an Intent Classifier (IC) and an Entity Classifier (EC). Each computes a cross-entropy loss (ce_loss) with a softmax activation using English labelled data (a multi-class classification). This yields the standard losses Lic and Lec. The CrossAligner (CA) classifier then pools the EC logits matrix by reshaping it into a long vector (lines 24 and 29) and predicts which entities are present in the user utterance (a multi-label classification). We compute a Binary Cross-Entropy loss (bce_loss) with a sigmoid activation between the predicted labels predeng and predtar (for English and Target languages respectively) and our language-agnostic labels yca (lines 26 and 31). This yields the CrossAligner losses Leng and Ltar. The fact that these gradients are propagated through the EC to the XLM token embeddings ensures a good alignment for entity recognition, shown later in Results. Note that EC, IC and CA are shared between languages to aid zero-shot cross-lingual transfer.\nArchitecture We use a common task-oriented XNLU model that employs a pretrained XLM, e.g. JointBERT (Chen et al., 2019). The IC, EC and CA each feature a single Multi-Layer Perceptron of sizes: [hidden_size, len(intent_classes)], [hidden_size, len(entity_classes)] and [seq_len× len(entity_classes), len(entity_classes)]. Depending on the dataset, seq_len varies from 50-100 tokens. The model architecture is shown in Fig 1.\nAlgorithm 1 The CrossAligner alignment/loss. 1: function TRANSFORMLABELS(yec) 2: yca← zeros(len(entity_classes)) 3: for entity ∈ yec do 4: yca[index_of(entity)]← 1 5: end for 6: return yca 7: end function\n8: XLM← Cross-lingual language model 9: IC← Intent Classifier\n10: EC← Entity Classifier 11: CA← CrossAligner Classifier 12: XEng ← Standard training data in English 13: XTar ← XEng translated into Target language\n14: for (xeng, y), (xtar, y) ∈ XEng,XTar do —Standard XNLU Training— 15: yic, yec← y 16: clseng, tokenseng ← XLM(xeng) 17: predic← IC(clseng) 18: Lic← ce_loss(predic, yic) 19: predec← EC(tokenseng) 20: Lec← ce_loss(predec, yec)\n—CrossAligner Training—\n21: yca← TRANSFORMLABELS(yec) 22: shape← (seq_len× len(entity_classes)) 23: logitseng ← EC(tokenseng) 24: logitseng.reshape_matrix_into(shape) 25: predeng ← CA(logitseng) 26: Leng ← bce_loss(predeng, yca) 27: clstar, tokenstar ← XLM(xtar) 28: logitstar ← EC(tokenstar) 29: logitstar.reshape_matrix_into(shape)) 30: predtar ← CA(logitstar) 31: Ltar ← bce_loss(predtar, yca) 32: Ltotal←Lic + Lec + Leng + Ltar 33: end for"
    }, {
      "heading" : "3.2 Contrastive Alignment",
      "text" : "We now introduce our Contrastive Alignment for XNLU, based on the InfoNCE loss (Oord et al., 2018). Previous work has employed a contrastive loss for cross-lingual alignment (Pan et al., 2020), however, the datasets were out-of-domain and orders of magnitude larger. We show that strong results can be obtained using only in-domain (finetuning) data. Similar to (Wu et al., 2021), if given a randomly sampled batch of N English sentences XEng and its parallel sentences XTar in the target language, then to obtain the loss on the ith sentence\npair xengi ∈ XEng and xtari ∈ XTar, we calculate:\n`(xengi , xtari) = − log exp(sim(xengi , xtari))∑N k=1 exp(sim(xengi , xtark))\n(1)\nwhere sim(u, v) = u · v / ||u||2 · ||v||2 is the cosine similarity between two sentence embeddings. A sentence xengi ∈ XEng symmetrically forms a positive pair with its translation xtari ∈ XTar while the other N− 1 sentence embeddings are treated as negative samples. The batch loss is calculated as the average of all positive pair losses. Algorithm 2 below shows the steps that replace/complement the CrossAligner block (lines 21-32 in Algorithm 1).\nAlgorithm 2 The Contrastive Alignment loss. 1: clseng, tokenseng ← XLM(xeng) 2: clstar, tokenstar ← XLM(xtar) 3: sim← batch_cosine_sim(clseng, clstar) 4: labels← arange(batch_size) 5: Lcl← ce_loss(sim, labels) 6: Ltotal←Lic + Lec + Lcl"
    }, {
      "heading" : "3.3 Translate-Intent",
      "text" : "The translate-train method is commonly used in multilingual NLP as a competitive baseline (Liang et al., 2020; Hu et al., 2020). However, for taskoriented XNLU, data transformations such as entity label projection and/or word alignment are typically performed in addition to translation (Schuster et al., 2019; Li et al., 2021b; Xu et al., 2020). This is followed by supervised training on the transformed data. However, both label projection and word alignment are also a source of common errors.\nAlgorithm 3 The Translate-Intent loss. 1: clstar, tokenstar ← XLM(xtar) 2: predic← IC(clstar) 3: Lti← ce_loss(predic, yic) 4: Ltotal←Lic + Lec + Lti\nWe therefore introduce a simple baseline called Translate-Intent, which to the best of our knowledge, has not been featured in task-oriented XNLU. We omit the entity recognition step for the target language (due to a lack of reliable labels) and only use the IC, which is trained with the unlabelled parallel data XTar. Algorithm 3 above shows the steps that either replace or complement (as in the case of a combination of multiple losses) the CrossAligner steps, shown in lines 21-32 in Algorithm 1."
    }, {
      "heading" : "3.4 Adaptive Weighting of Auxiliary Losses",
      "text" : "In order to evaluate the benefits of combinations of two or more alignments, we employ the Multi-Loss Weighting with Coefficient of Variations (Groenendijk et al., 2021) technique (CoV) to calculate a weighted sum of auxiliary losses (Aux) that we add to the main XNLU losses Lic and Lec as follows:\nLtotal = Lic + Lec + ∑ a∈Aux waLa (2)\nThe sole difference to CoV is that we opt to omit the loss weight normalisation step before application. The weights for an auxiliary loss La,t for a ∈ Aux at training step t are calculated as follows:\nwa,t = σ`a,t µ`a,t `a,t = La,t µLa,t−1\n(3)\nwhere `a,t is the loss ratio of loss a ∈ Aux at training step t, σ is the standard deviation over the history of loss ratios and µ`a,t−1 is the mean of the loss ratio `a up to and including step t− 1. We also compare CoV to a simple sum of all losses i.e. equal weight for each loss, as shown in Algorithms 1, 2 and 3 (line beginning with Ltotal)."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Datasets Three multilingual datasets are used to compare our methods with their most relevant counterparts. The datasets comprise nine unique languages (de, pt, zh, ja, hi, tr, fr, es, th) from 15 test sets (20,000+ instances) featuring examples of users interacting with a task-oriented personal assistant designed to test the XNLU capabilities of multilingual models. Two tasks are being evaluated: Intent Classification and Entity Recognition.\nMultilingual Task-Oriented Parsing (MTOP) comprises 15K-22K utterances in each of 6 languages (en, de, fr, es, hi, th) spanning 11 domains (Li et al., 2021b). The Multilingual TaskOriented Dialogue (MTOD) consists of 43K English, 8K Spanish and 5K Thai utterances covering 3 domains (Schuster et al., 2019). The Multilingual ATIS++ (M-ATIS) contains up to 4.5K commands in each of 8 languages (en, es, pt, de, fr, zh, ja, hi, tr) featuring user interactions with a travel information system (Xu et al., 2020).\nXLM Our pretrained language model of choice is XLM-RoBERTa (Conneau et al., 2020). We use the large (550M parameters) model from HuggingFace (Wolf et al., 2019) with a hidden_size = 1,024.\nTraining Setup We use a minimalist setup that features default settings and components to focus the results on the methods rather than hyperparameter tuning or custom architecture design. We implemented all models with PyTorch using fixed hyperparameters between experiments except for MTOD, where due to its size, we trained with fewer epochs and a lower learning rate (both 50% lower2)."
    }, {
      "heading" : "5 Results",
      "text" : "Terminology Henceforth, we refer to models trained with labelled data in each language as Target Language, the models trained only on English data as Zero-Shot, our translate-intent method as Translate-Intent (TI), the scores reported by Gritta and Iacobacci (2021) as Previous SOTA, our IO-only implementation of that model as XeroAlignIO (XAIO), our contrastive alignment method as Contrastive (CTR) and our main method as CrossAligner (CA). Lastly, the simple sum of alignment losses is referred to as 1+1 and the weighted sum from 3.4 as CoV.\nMetrics We use Accuracy for intent classification and the F-Score for entity recognition. In addition, we use an Overall score (the average of F-Score and Accuracy) for model ranking, similar to Hu et al. (2020); Wang et al. (2019, 2018). Results are shown as averages (MEAN) over all test sets and datasets, presented in Tables 1 and 2. Intent classification is thus evaluated on 20,000+ diverse user commands and entity recognition on 80,000+ individual slots from 100+ slot types. For a full breakdown, see Tables 4, 5 and 6 in Appendix A.3.\nCrossAligner The focus of our primary method is to improve slot filling as the model classifies\n2Code and data will be made publicly available on Github.\ndozens of entity types in each dataset and to that end, it’s an effective approach. CrossAligner exceeds the F-Score of the Previous SOTA by 2.7 points (82.5 versus 79.8). This is 1.4 points higher than XeroAlignIO and 6 points higher than ZeroShot. Despite the intent accuracy being 1.4 points lower than Previous SOTA and 1.6 lower than XeroAlignIO, 94.7 is still 0.4 higher than Target Language. CrossAligner’s overall score is 0.6 higher than Previous SOTA (which outperformed the common ’translate-train’ models, including entity projection and word alignment). CrossAligner also combines well with other losses as we show in 5.1. In order to demonstrate the necessity and specificity of the proposed architecture, we tested mean-pooled token embeddings as well as a CLS embedding as the input to CrossAligner instead of the entity classifier logits. The scores declined from 94.7/82.5 (88.6 Overall) to 92.3/80 (86.2 Overall) with a CLS sentence representation and 82.1/78.7 (80.4 Overall) for mean-pooled embeddings.\nTranslate-Intent Our alternative to the common ‘translate-train’ baseline is not only conceptually simpler (no explicit entity recognition training), it also outperforms the previous Translate-Train SOTA scores (78.5 vs 76.6 F-Score, 95.9 vs 95.1 accuracy and 87.2 vs 85.9 Overall). Translate-Intent does not require error-prone preprocessing such as word/label alignment and can therefore be readily used as a default ‘translate-train’ baseline in future work. Note that using mean-pooled token embeddings as sentence representations is not recommended for Translate-Intent as this causes the F-Score to decline sharply (-25 points).\nContrastive Alignment Despite orders of magnitude less data than used in Related Work, our Contrastive Alignment managed to marginally out-\nperform the Previous SOTA on intent classification (96.3 vs 96.1) thus by 0.1 Overall. That said, even though the contrastive loss pushes negative sentence embeddings away from the positives, this does not seem to confer a strong advantage over the Previous SOTA, which only used positive examples. We have also evaluated an implementation of Contrastive Alignment using mean-pooled token embeddings as sentence representations, however, the Overall score declined to 86.8 (versus 88.1 for a standard CLS embedding).\nXeroAlignIO Our implementation of the Previous SOTA with an additional post-processing step (see ‘BIO versus IO’ in Appendix A.2) increased the F-Score by 1.3 points and accuracy by 0.2 (+0.7 Overall). For a comparison, training XeroAlignIO with the conventional BIO tags results in a drop of 1.8 points (81.1 to 79.3 F-Score) on entity recognition and 0.4 on intent classification (96.1 to 95.7). Mean-pooled tokens are not recommended for XeroAlignIO as this yields a 2-point decline (88.7 to 86.7 Overall). Other models also benefit from IO-only training, for example, the Zero-Shot model gains 2.6 points (73.9 up to 76.5 F-Score). One theoretical limitation of IO-only training is that given a sequence of ‘B-LOC I-LOC B-LOC’, the IO-only models would incorrectly classify it as a single entity. However, in practice, this is rare and not something we have encountered during preprocessing, implementation or error analysis."
    }, {
      "heading" : "5.1 Combinations of Losses",
      "text" : "As our alignment methods have different strengths and weaknesses, we have also evaluated their combinations (see Table 2) as either a simple sum of losses (1+1) or a weighted sum of losses (CoV) using the Coefficient of Variation. The highest overall score was achieved by a CoV-weighted combination of XeroAlignIO and CrossAligner, which considerably improved on the previous SOTA (96.5 vs 96.1 Accuracy, 82.1 vs 79.8 F-Score, 89.3 vs 88.0 Overall). In total, three individual and almost a dozen combinations of losses improve over the best previously reported scores. In the following paragraphs, we analyse and explain why the combinations that include CrossAligner consistently produce higher scores and why adding more losses can result in diminishing returns.\nCompatibility of Losses We propose a hypothesis that can further help us interpret the numbers in Table 2. It states that combining losses which use dissimilar sentence representations may be more beneficial than combining losses using similar sentence embeddings. In order to investigate this claim, we clustered our alignment methods into two groups based on how their sentence representations are obtained: 1) XeroAlignIO, TranslateIntent and Contrastive Alignment as they align through the CLS embedding and 2) CrossAligner, which aligns through the model’s token embeddings, used as the entity classifier input.\nIn Figure 2, we note that for combinations of any two alignment losses using the CLS embedding (shown as blue squares), there is no difference in the overall scores when using CoV or 1+1. However, when combining losses with different sentence representations (orange with any blue square) using CoV weighting, we observe consistent increases over the 1+1 setup (on average 1+ point overall) as well as an increase over their highest individual score. Additionally, in a 3-loss combination, we note that adding CrossAligner to any two losses from the CLS-embedding group using CoV weighting yields an average improvement of 0.7 points compared to no improvement using 1+1.\nOversaturation of Losses Another important observation harks back to our hypothesis stating that alignment methods with similar input embeddings do not lend themselves to being readily combined. We offer further evidence of this by testing combinations of CrossAligner with each of the CLS-embedding losses [XAIO, TI and CTR], however, we use mean-pooled embeddings. The Overall scores decline in line with our hypothesis (XAIO by -1.2, TI by -4.9, CTR by -0.6) with CoV-weighted losses and even more with the 1+1 weighted combinations (XAIO by -2.1, TI by -7.6, CTR by -1.4). Similarly, combining multiple CLSembedding losses leads to a gradually diminishing benefit relative to the individual scores. Once again, the CoV-weighted losses show a significantly lower decline than the 1+1 combinations (Table 2). Note that in our multi-loss scenario, intent classification remains unaffected by the choice of input embeddings as the accuracy remains stable at SOTA levels across experiments. We think this is due to the un-\nequal task difficulty. In other words, intent-level inference is easier than token-level inference because entity recognition has a higher task granularity."
    }, {
      "heading" : "6 Error Analysis",
      "text" : "In order to contextualise the numbers reported in Tables 1 and 2 in relevant linguistic insights, we have conducted a qualitative error analysis and present the highlights in this section. Readers interested in language-specific analysis (including many more examples) are encouraged to read Appendix A.1. We focused on errors committed by CrossAligner and XeroAlignIO, which achieved the best individual and combined scores. We sampled 100 random errors from each of the following settings: Hindi, French and German from MTOP, Portuguese, Chinese and Spanish from M-ATIS and Thai from MTOD for a diverse pool of errors. The authors adjudicated with native speakers to categorise mistakes into the following types.\nError Types We discovered two main sources of mistakes: A Boundary Error occurs when the model predicts more /fewer entity words than was expected. A Semantics Error occurs when the wrong entity class is predicted. Models can therefore commit: 1) both errors resulting in Poor Transfer (with some exceptions), 2) a boundary error without a semantic error and vice versa giving us a Partial Transfer or 3) neither ‘error’, which we deemed an Acceptable Transfer. We report individual and average error occurrences in Table 3.\nPoor Transfer indicates that the prediction error is too serious and unusable (even misleading) in a real-world personal assistant. This is typically due to both a boundary and a semantics error, however, some mistakes can be serious enough alone to result in poor transfer. For example, a boundary error can cause the retrieved name of a dish, person or a location to be incomplete and therefore invalid. A semantics error that classifies ‘10 secondes’ (French) as ‘date_time’ instead of ‘music_rewind_time’ would elicit the wrong agent response thus is unusable. On average, ∼28% of mistakes fall into the ‘Poor Transfer’ category.\nPartial Transfer is defined as either a boundary or a semantics error where neither is considered a serious transfer problem. Such entities could be made usable in a personal assistant application with simple post-processing rules. Around 33 percent of errors were deemed to be partially correct. Often,\nthis was due to including some adjacent punctuation or an article/preposition as part of the entity or a slightly shorter/longer news headline even though a search engine query with that string would have returned the relevant article. Entities such as ‘24 minat ka’ (Hindi) versus ‘24 minat’ (24 minutes) exemplify the fact that a disputed entity boundary is the most frequent source of error in this category. On the semantics side, we considered a location partially correct if ‘state_name’ instead of ‘city_name’ (for Washington D.C.) was predicted, a location was expected and the boundary was accurate.\nAcceptable Transfer examples are ‘errors’ that we considered correct and usable ‘as is’ because neither the entity boundary nor its semantics were thought to be wrong. On average, we deemed almost 39% of entities acceptable for a real-world personal assistant application with around half of those being down to annotation problems (labels missing or incorrect). In other cases, we accepted predictions that offered a valid alternative e.g. when both ‘me’ (French) and ‘je’ (I/me) are present in the user utterance and both refer to the same ‘person_reminded’. Valid alternatives were predicted but annotated somewhat differently. For example, when the entity boundary was slightly wider ‘de ida e volta’ (Portuguese) instead of ‘ida e volta’ (round trip) where both entities are correct. Similarly, classifying ‘salmon’ as an ingredient rather than a dish (when ‘salmon’ is an object of ‘prepare’) was considered an acceptable transfer."
    }, {
      "heading" : "6.1 Error Analysis Summary",
      "text" : "While the intent classification task is transferred well in a cross-lingual setting, performing better than training on labelled data, our SOTA entity recognition F-Score is almost 7 points behind Target Language. We think there are several factors involved. Articles, some prepositions, conjunctions, determiners and/or possessives do not transfer easily and may largely be ignored by the XLM as they\ndon’t carry important sentence level (e.g. intent) semantics. English is not ideal as a cross-lingual pivot for many of the dozens of languages covered by the XLM as elements of culture and vernacular that may not have a direct English equivalent don’t transfer easily in a zero-shot setting. Aligning on the most well-resourced language in the same family should help (Xia et al., 2021). The limits of machine translation, especially for low-resource languages, can further inhibit alignment methods that leverage parallel data. Inconsistency of annotation (intra-language and inter-language) is a source of errors when the key concepts are learnt in one language and evaluated (sometimes unreliably) in the target language. Finally, there were no substantial qualitative differences between XeroAlignIO and CrossAligner in our error analysis suggesting that the aforementioned error patterns may be a feature of the XLM model itself, the nature of the datasets or some as yet unknown confounding variable rather than the choice of the alignment method."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have introduced a variety of cross-lingual methods for task-oriented XNLU to enable effective zero-shot transfer by learning alignment with unlabelled parallel data. The principal method, CrossAligner, transforms English train data into a new language-agnostic task used to align model predictions across languages, achieving SOTA on entity recognition. We then presented a Contrastive Alignment that optimises for a small cosine distance between translated sentences while increasing it between unrelated sentences, using orders of magnitude less data than previous works. We proposed Translate-Intent, a fast and simple baseline that beats previous Translate-Train SOTA approaches without error-prone data transformations such as slot label projection. The best overall performance across nine languages, fifteen tests sets and three task-oriented multilingual datasets was achieved by a Coefficient of Variation weighted combination of CrossAligner and XeroAlignIO. Our quantitative analysis investigated which types of auxiliary losses yielded the most effective combinations resulting in several other proposed configurations also exceeding Previous SOTA scores. Our detailed qualitative error analysis revealed that the best methods have the potential to approach target language performance as most errors were deemed to be of low to medium severity."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Error Analysis per Language\nGerman examples of partial transfer are boundary errors such as tagging punctuation ‘-’ as part of the entity (e.g. in a timer or alarm name) as well as not tagging some punctuation e.g. in ‘14. Mai’ where ‘.’ is equivalent to the English ‘th’ in ‘14th’ and is expected in German dates. Such entities can be used with a basic post-processing rule as their classes and contents were sufficiently well predicted. Similarly, including ‘die’, ‘den’, ‘das’, ‘mir’, ‘des’, ‘der’ in the retrieved entity, particularly in free-format entities such as news headlines, text message contents and memos need not invalidate the prediction, e.g. ‘die hausschliessung’ (house closure), ‘des Reiseverbots’ (of travel ban) and ‘den Termin’ (appointment). Just a few of such linguistic ‘bad habits’ can quickly accumulate to cause more than half of all errors.\nChinese cross-lingual transfer problems often include boundary issues featuring the ‘of’ preposition or the possessive ‘’s’ (de in Chinese) e.g. ‘Zuì piányí de’ (cheapest), ‘Jiāzhōu de’ (California) or ‘āndàlüè de hángbān’ (flights to Ontario). Depending on context, we considered these at least partially correct rather than a failed transfer. More serious though less explicable errors were ‘Gěi wǒ zhōu’èr’ (give me Tuesday’s), ‘Liè chū zhōu liù’ (list Saturday’s) or ‘Xiǎnshì zhōusān’ (show me Wednesday’s) where ‘give me’, ‘list’ and ‘show me’ were tagged as part of ‘date_time’ a total of 20 times. The most frequent semantic (partial) error was ‘Washington D.C.’, which was tagged as a state rather than a city no fewer than 16 times.\nFrench instances of acceptable transfer include tagging ‘Ankara’ and ‘Turquie’ separately rather than as a single chunk ‘Ankara, en Turquie’ (possible annotation problem). Reminiscent of the patterns seen in other languages, articles tend to feature prominently in boundary errors, e.g. ‘la famille’ (family), ‘l’arrosage’ (watering), ‘les elections’ (elections), ‘le chat Zoom’ (Zoom chat) and ‘la mere de Kylie’ (Kylie’s mother), which we considered usable ‘as is’. For an example of annotation inconsistency across languages, consider the entity ‘gros titres’ or ‘top headlines’ in English. The model correctly transferred the English tags for ‘top’ (news_reference) and ‘headline’ (news_type) although the French annotation was given as ‘gros\ntitres’ (news_type), which is plausible but less coherent than the model’s prediction.\nSpanish Once again, articles, some prepositions and occasionally conjunctions e.g. ‘de’, ‘las’, ‘y’, ‘la’, ‘por’ (of, the, and, a, in/for) have caused the majority of boundary errors, most of which are partially acceptable. Examples include time ‘las 10 a.m.’ (10 a.m.) and ‘no mas tarde que las’ (no later than), journey specifications ‘de ida’ and ‘ida y vuelta’ (round trip), periods of day ‘la mañana’ (morning) and ‘la noche’ (night), dates ‘seis de junio’ (June 6th) as well as skipping ‘en punto’ (o’clock) in ‘antes de las 4 p.m. en punto’ (before 4 p.m.). These minor errors show that the current SOTA in cross-lingual zero-shot transfer is close to solving these cases. Other errors such as ‘conexión’ instead of ‘con conexión’ (with connection) and ‘la mañana’ rather than ‘por la mañana’ (in the morning) are more examples of disputed annotations.\nPortuguese predictions closely follow Spanish error patterns and reflect the wider issues with articles and prepositions, e.g. ‘terça-feira de manhã’ (Tuesday morning), ‘de ida e volta’ (round trip), ‘cinco de abril’ (April 5th) or ‘5 horas da tarde’ (5 p.m.) with ’de’ and ‘da’ (both of ) being the unannotated parts that did not transfer optimally. Other boundary mistakes were caused by ‘das’ (of) and ‘as’ (the), for example, ‘antes das 6 horas da tarde’ and ‘após as 6 horas da tarde’ (before and after 6 p.m). Annotations that needlessly punished cross-lingual transfer included ‘somente de ida’ (one way) where ‘somente’ (only) was not annotated in the English dataset and ‘econômica’ (economy), which was annotated as ‘class_type’ in English, correctly transferred but flagged as wrong.\nHindi errors have a relatively high number (26) of problematic annotations although most mistakes are caused by the now familiar improper handling of prepositions, articles and/or possessives e.g. ‘ke’, ‘tak’, ‘ka’ (‘of’, ‘by’, ‘’s’) in phrases such as ’30 minat ka’ (30 minutes), ‘kitanee der tak’ (how long), ‘kal ke’ (yesterday’s), ‘aaj ke’ (today’s) or ‘1 baje ka’ (1 p.m.). Transliterated entities i.e. English pronunciation written in Devanagari, is the second largest category of transfer problems in Hindi, e.g. ‘pakrino romaano’ (Pecorino Romano), ‘goda cheez’ (Goda cheese), ‘braun aaid garl’ (Brown Eyed Girl), ‘paindora’ (Pandora), ‘pool leeg’ (Pool League), ‘daayanaasor jooniyar’ (Dinosaur Junior) or ‘da most byooteephul moment’ (The Most Beat-\niful Moment). These are problematic because such entities are neither native to Hindi nor are they written in Latin alphabet hence may not have been observed in this form during XLM pretraining.\nThai errors were analysed with a translation service as we were unable to secure a native speaker. Even so, we observed boundary errors previously seen in other languages. Words such as ‘nai’ (‘of’ or ‘in’, the most frequent cause) and ‘bpai’ (‘in’, ‘off’ or ‘to’, no direct English translation) were the typical sources of boundary issues, e.g. ’nai sùt sàp-daa née’ (this weekend), ‘nai wan pút’ (Wednesday), ‘bpai séu XYZ’ (go buy XYZ) and ‘nai wan née’ (today or on this day). Such patterns accounted for more than half of all mistakes. Machine translation can also be a source of errors. For example, the word ‘reminder’ is an entity in English (tag: reminder/noun). It was translated as ‘kam dteuuan’, however, ‘reminder’ appears in Thai data as ‘dteuuan kwaam jam’, which the model repeatedly missed, leading to 18 errors for what should be an easy case of zero-shot transfer.\nA.2 BIO versus IO Using the BIO sequence tagging format (Sang and De Meulder, 2003) can introduce avoidable errors, e.g. predicting B after I, two Bs in succession or skipping B altogether. We have therefore simplified the training by making it agnostic w.r.t. the entity’s BI order. The B tags were removed in preprocessing, meaning the entity classifier predicts only IO tags. At inference, the B tags get restored with a simple post-processing rule. Note that all our models use the IO-only training.\nA.3 Full Tables The full language breakdown for MultiATIS++ (Table 4) and MTOP+MTOD (Table 5). Table 6 shows the full details of the combinations of losses from Table 2 in Results (5)."
    } ],
    "references" : [ {
      "title" : "The missing ingredient in zero-shot neural machine translation",
      "author" : [ "Naveen Arivazhagan", "Ankur Bapna", "Orhan Firat", "Roee Aharoni", "Melvin Johnson", "Wolfgang Macherey." ],
      "venue" : "arXiv preprint arXiv:1903.07091.",
      "citeRegEx" : "Arivazhagan et al\\.,? 2019",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2019
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "A benchmarking environment for reinforcement learning based task oriented dialogue",
      "author" : [ "Inigo Casanueva", "Paweł Budzianowski", "Pei-Hao Su", "Nikola Mrkšić", "Tsung-Hsien Wen", "Stefan Ultes", "Lina Rojas-Barahona", "Steve Young", "Milica Gašić" ],
      "venue" : null,
      "citeRegEx" : "Casanueva et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Casanueva et al\\.",
      "year" : 2017
    }, {
      "title" : "Dict-mlm: Improved multilingual pre-training using bilingual dictionaries",
      "author" : [ "Aditi Chaudhary", "Karthik Raman", "Krishna Srinivasan", "Jiecao Chen." ],
      "venue" : "arXiv preprint arXiv:2010.12566.",
      "citeRegEx" : "Chaudhary et al\\.,? 2020",
      "shortCiteRegEx" : "Chaudhary et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert for joint intent classification and slot filling",
      "author" : [ "Qian Chen", "Zhu Zhuo", "Wen Wang." ],
      "venue" : "arXiv preprint arXiv:1902.10909.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou." ],
      "venue" : "arXiv",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Édouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Pro-",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Word alignment by fine-tuning embeddings on parallel corpora",
      "author" : [ "Zi-Yi Dou", "Graham Neubig." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2112–2128.",
      "citeRegEx" : "Dou and Neubig.,? 2021",
      "shortCiteRegEx" : "Dou and Neubig.",
      "year" : 2021
    }, {
      "title" : "A simple, fast, and effective reparameterization of ibm model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A Smith." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Xeroalign: Zero-shot cross-lingual transformer alignment",
      "author" : [ "Milan Gritta", "Ignacio Iacobacci." ],
      "venue" : "arXiv:2105.02472.",
      "citeRegEx" : "Gritta and Iacobacci.,? 2021",
      "shortCiteRegEx" : "Gritta and Iacobacci.",
      "year" : 2021
    }, {
      "title" : "Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management",
      "author" : [ "Milan Gritta", "Gerasimos Lampouras", "Ignacio Iacobacci." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:36–52.",
      "citeRegEx" : "Gritta et al\\.,? 2021",
      "shortCiteRegEx" : "Gritta et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-loss weighting with coefficient of variations",
      "author" : [ "Rick Groenendijk", "Sezer Karaoglu", "Theo Gevers", "Thomas Mensink." ],
      "venue" : "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1469–1478.",
      "citeRegEx" : "Groenendijk et al\\.,? 2021",
      "shortCiteRegEx" : "Groenendijk et al\\.",
      "year" : 2021
    }, {
      "title" : "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "International Conference on Machine Learn-",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Entity projection via machine translation for cross-lingual NER",
      "author" : [ "Alankar Jain", "Bhargavi Paranjape", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Jain et al\\.,? 2019",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2019
    }, {
      "title" : "Low-resource machine translation for low-resource languages: Leveraging comparable data, codeswitching and compute resources",
      "author" : [ "Garry Kuwanto", "Afra Feyza Akyürek", "Isidora Chara Tourni", "Siyang Li", "Derry Wijaya." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Kuwanto et al\\.,? 2021",
      "shortCiteRegEx" : "Kuwanto et al\\.",
      "year" : 2021
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "arXiv preprint arXiv:1901.07291.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment",
      "author" : [ "Bing Li", "Yujie He", "Wenjin Xu." ],
      "venue" : "arXiv preprint arXiv:2101.11112.",
      "citeRegEx" : "Li et al\\.,? 2021a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
      "author" : [ "Haoran Li", "Abhinav Arora", "Shuohui Chen", "Anchit Gupta", "Sonal Gupta", "Yashar Mehdad." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Asso-",
      "citeRegEx" : "Li et al\\.,? 2021b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation",
      "author" : [ "Yaobo Liang", "Nan Duan", "Yeyun Gong", "Ning Wu", "Fenfei Guo", "Weizhen Qi", "Ming Gong", "Linjun Shou", "Daxin Jiang", "Guihong Cao" ],
      "venue" : null,
      "citeRegEx" : "Liang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilingual bert post-pretraining alignment",
      "author" : [ "Lin Pan", "Chung-Wei Hang", "Haode Qi", "Abhishek Shah", "Mo Yu", "Saloni Potdar." ],
      "venue" : "arXiv preprint arXiv:2010.12547.",
      "citeRegEx" : "Pan et al\\.,? 2020",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2020
    }, {
      "title" : "Translation-based matching adversarial network for cross-lingual natural language inference",
      "author" : [ "Kunxun Qi", "Jianfeng Du." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8632–8639.",
      "citeRegEx" : "Qi and Du.,? 2020",
      "shortCiteRegEx" : "Qi and Du.",
      "year" : 2020
    }, {
      "title" : "Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp",
      "author" : [ "Libo Qin", "Minheng Ni", "Yue Zhang", "Wanxiang Che." ],
      "venue" : "arXiv preprint arXiv:2006.06402.",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems",
      "author" : [ "Evgeniia Razumovskaia", "Goran Glavaš", "Olga Majewska", "Anna Korhonen", "Ivan Vulić." ],
      "venue" : "arXiv preprint arXiv:2104.08570.",
      "citeRegEx" : "Razumovskaia et al\\.,? 2021",
      "shortCiteRegEx" : "Razumovskaia et al\\.",
      "year" : 2021
    }, {
      "title" : "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Cross-lingual transfer learning for multilingual task oriented dialog",
      "author" : [ "Sebastian Schuster", "Sonal Gupta", "Rushin Shah", "Mike Lewis." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Schuster et al\\.,? 2019",
      "shortCiteRegEx" : "Schuster et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating the cross-lingual effectiveness of massively multilingual neural machine translation",
      "author" : [ "Aditya Siddhant", "Melvin Johnson", "Henry Tsai", "Naveen Ari", "Jason Riesa", "Ankur Bapna", "Orhan Firat", "Karthik Raman." ],
      "venue" : "Proceedings of the AAAI Con-",
      "citeRegEx" : "Siddhant et al\\.,? 2020",
      "shortCiteRegEx" : "Siddhant et al\\.",
      "year" : 2020
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "Advances in Neural Information",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Huggingface’s transformers: State-ofthe-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking infonce: How many negative samples do you need",
      "author" : [ "Chuhan Wu", "Fangzhao Wu", "Yongfeng Huang" ],
      "venue" : "arXiv preprint arXiv:2105.13003",
      "citeRegEx" : "Wu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Metaxl: Meta representation transformation for low-resource cross-lingual learning",
      "author" : [ "Mengzhou Xia", "Guoqing Zheng", "Subhabrata Mukherjee", "Milad Shokouhi", "Graham Neubig", "Ahmed Hassan Awadallah." ],
      "venue" : "arXiv preprint arXiv:2104.07908.",
      "citeRegEx" : "Xia et al\\.,? 2021",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2021
    }, {
      "title" : "End-to-end slot alignment and recognition for crosslingual nlu",
      "author" : [ "Weijia Xu", "Batool Haider", "Saab Mansour." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5052–5063.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Zero-shot entity recognition via multi-source projection and unlabeled data",
      "author" : [ "Huixiong Yi", "Jin Cheng." ],
      "venue" : "IOP Conference Series: Earth and Environmental Science, volume 693, page 012084. IOP Publishing.",
      "citeRegEx" : "Yi and Cheng.,? 2021",
      "shortCiteRegEx" : "Yi and Cheng.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Natural language understanding (NLU) refers to the ability of a system to ‘comprehend’ the meaning (semantics) and the structure (syntax) of human language (Wang et al., 2019) to enable the interaction with a system or device.",
      "startOffset" : 156,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : "Cross-lingual natural language understanding (XNLU) alludes to a system that is able to handle multiple languages simultaneously (Artetxe and Schwenk, 2019; Hu et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "Cross-lingual natural language understanding (XNLU) alludes to a system that is able to handle multiple languages simultaneously (Artetxe and Schwenk, 2019; Hu et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 173
    }, {
      "referenceID" : 3,
      "context" : "In a modular dialogue system, this information is used by the dialogue manager to decide how to respond to the user (Casanueva et al., 2017; Gritta et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "In a modular dialogue system, this information is used by the dialogue manager to decide how to respond to the user (Casanueva et al., 2017; Gritta et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 25,
      "context" : "For neural XNLU systems, the limited availability of annotated data is a significant barrier to scaling dialogue systems to more users (Razumovskaia et al., 2021).",
      "startOffset" : 135,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "Therefore, we can use cross-lingual methods to zero-shot transfer the knowledge learnt in a high-resource language such as English to the target language of choice (Artetxe et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 164,
      "endOffset" : 209
    }, {
      "referenceID" : 28,
      "context" : "Therefore, we can use cross-lingual methods to zero-shot transfer the knowledge learnt in a high-resource language such as English to the target language of choice (Artetxe et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 164,
      "endOffset" : 209
    }, {
      "referenceID" : 7,
      "context" : "Our methods leverage unlabelled parallel data and can be integrated with pretrained language models such as XLMRoBERTa (Conneau et al., 2020) or Multilingual",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "BERT (Devlin et al., 2019), which we refer to as XLM1 throughout the paper.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "Data-based Transfer Translating utterances for the intent classification task is relatively straightforward so previous works focused on projecting and/or aligning the entity labels between translated utterances (Dou and Neubig, 2021).",
      "startOffset" : 212,
      "endOffset" : 234
    }, {
      "referenceID" : 10,
      "context" : "One of the earliest works still being used for this purpose is fastalign (Dyer et al., 2013).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 35,
      "context" : "Projecting the entity labels can also be done with word-by-word translation and source label copying (Yi and Cheng, 2021).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "Sometimes, this type of label projection is complemented with an additional entity alignment step (Li et al., 2021a).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "Better performance can be achieved by using machine translation with entity matching and distributional statistics (Jain et al., 2019) though this can be a costly process for each language.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "A category of ‘word substitution’ methods such as code-switching (Qin et al., 2020; Kuwanto et al., 2021) or dictionary-enhanced pretraining (Chaudhary et al.",
      "startOffset" : 65,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "A category of ‘word substitution’ methods such as code-switching (Qin et al., 2020; Kuwanto et al., 2021) or dictionary-enhanced pretraining (Chaudhary et al.",
      "startOffset" : 65,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : ", 2021) or dictionary-enhanced pretraining (Chaudhary et al., 2020) have also been shown to improve cross-lingual transfer.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "Model-based Transfer Prior to the adoption of multilingual transformers (Lample and Conneau, 2019), task-oriented XNLU methods employed a BiLSTM encoder combined with different multilingual embeddings (Schuster et al.",
      "startOffset" : 72,
      "endOffset" : 98
    }, {
      "referenceID" : 27,
      "context" : "Model-based Transfer Prior to the adoption of multilingual transformers (Lample and Conneau, 2019), task-oriented XNLU methods employed a BiLSTM encoder combined with different multilingual embeddings (Schuster et al., 2019).",
      "startOffset" : 201,
      "endOffset" : 224
    }, {
      "referenceID" : 21,
      "context" : "We now introduce our Contrastive Alignment for XNLU, based on the InfoNCE loss (Oord et al., 2018).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 22,
      "context" : "Previous work has employed a contrastive loss for cross-lingual alignment (Pan et al., 2020), however, the datasets were out-of-domain and orders of magnitude larger.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 32,
      "context" : "Similar to (Wu et al., 2021), if given a randomly sampled batch of N English sentences XEng and its parallel sentences XTar in the target language, then to obtain the loss on the ith sentence",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "The translate-train method is commonly used in multilingual NLP as a competitive baseline (Liang et al., 2020; Hu et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "The translate-train method is commonly used in multilingual NLP as a competitive baseline (Liang et al., 2020; Hu et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 127
    }, {
      "referenceID" : 27,
      "context" : "label projection and/or word alignment are typically performed in addition to translation (Schuster et al., 2019; Li et al., 2021b; Xu et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "label projection and/or word alignment are typically performed in addition to translation (Schuster et al., 2019; Li et al., 2021b; Xu et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 148
    }, {
      "referenceID" : 34,
      "context" : "label projection and/or word alignment are typically performed in addition to translation (Schuster et al., 2019; Li et al., 2021b; Xu et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "In order to evaluate the benefits of combinations of two or more alignments, we employ the Multi-Loss Weighting with Coefficient of Variations (Groenendijk et al., 2021) technique (CoV) to calculate a weighted sum of auxiliary losses (Aux) that we add to the main XNLU losses Lic and Lec as follows:",
      "startOffset" : 143,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "Multilingual Task-Oriented Parsing (MTOP) comprises 15K-22K utterances in each of 6 languages (en, de, fr, es, hi, th) spanning 11 domains (Li et al., 2021b).",
      "startOffset" : 139,
      "endOffset" : 157
    }, {
      "referenceID" : 27,
      "context" : "The Multilingual TaskOriented Dialogue (MTOD) consists of 43K English, 8K Spanish and 5K Thai utterances covering 3 domains (Schuster et al., 2019).",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 34,
      "context" : "5K commands in each of 8 languages (en, es, pt, de, fr, zh, ja, hi, tr) featuring user interactions with a travel information system (Xu et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "XLM Our pretrained language model of choice is XLM-RoBERTa (Conneau et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 31,
      "context" : "We use the large (550M parameters) model from HuggingFace (Wolf et al., 2019) with a hidden_size = 1,024.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "Translate-Train SOTA: (Li et al., 2021b) for MTOP/MTOD and (Xu et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 34,
      "context" : ", 2021b) for MTOP/MTOD and (Xu et al., 2020) for M-ATIS.",
      "startOffset" : 27,
      "endOffset" : 44
    } ],
    "year" : 0,
    "abstractText" : "Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the lowresource language(s). To this end, we introduce CrossAligner, the principal method of a variety of effective approaches for zero-shot cross-lingual transfer based on learning alignment from unlabelled parallel data. We present a quantitative analysis of individual methods as well as their weighted combinations, several of which exceed state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test sets and three benchmark multilingual datasets. A detailed qualitative error analysis of the best methods shows that our fine-tuned language models can zero-shot transfer the task knowledge better than anticipated.",
    "creator" : null
  }
}
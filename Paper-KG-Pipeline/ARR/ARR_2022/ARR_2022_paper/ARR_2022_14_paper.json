{
  "name" : "ARR_2022_14_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : null,
    "references" : [ {
      "title" : "Summary level training of sentence rewriting for abstractive summarization",
      "author" : [ "Sanghwan Bae", "Taeuk Kim", "Jihoon Kim", "Sanggoo Lee." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 10–20.",
      "citeRegEx" : "Bae et al\\.,? 2019",
      "shortCiteRegEx" : "Bae et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Long document summarization in a low resource setting",
      "author" : [ "Ahsaas Bajaj", "Pavitra Dangati", "Kalpesh Krishna", "Pradhiksha Ashok Kumar", "Rheeya Uppaal", "Bradford Windsor", "Eliot Brenner", "Dominic Dotterrer", "Rajarshi Das", "Andrew McCallum" ],
      "venue" : null,
      "citeRegEx" : "Bajaj et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bajaj et al\\.",
      "year" : 2021
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "CoRR, abs/2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning opinion summarizers by selecting informative reviews",
      "author" : [ "Arthur Bražinskas", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "arXiv e-prints, pages arXiv– 2109.",
      "citeRegEx" : "Bražinskas et al\\.,? 2021",
      "shortCiteRegEx" : "Bražinskas et al\\.",
      "year" : 2021
    }, {
      "title" : "The ami meeting corpus: A pre-announcement",
      "author" : [ "Jean Carletta", "Simone Ashby", "Sebastien Bourban", "Mike Flynn", "Mael Guillemot", "Thomas Hain", "Jaroslav Kadlec", "Vasilis Karaiskos", "Wessel Kraaij", "Melissa Kronenthal" ],
      "venue" : null,
      "citeRegEx" : "Carletta et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carletta et al\\.",
      "year" : 2005
    }, {
      "title" : "Training deep nets with sublinear memory cost",
      "author" : [ "Tianqi Chen", "Bing Xu", "Chiyuan Zhang", "Carlos Guestrin." ],
      "venue" : "CoRR, abs/1604.06174.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Generating long sequences with sparse transformers",
      "author" : [ "Rewon Child", "Scott Gray", "Alec Radford", "Ilya Sutskever." ],
      "venue" : "CoRR, abs/1904.10509.",
      "citeRegEx" : "Child et al\\.,? 2019",
      "shortCiteRegEx" : "Child et al\\.",
      "year" : 2019
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of the 2018 Conference of the North",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "Sliding selector network with dynamic memory for extractive summarization of long documents",
      "author" : [ "Peng Cui", "Le Hu." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Cui and Hu.,? 2021",
      "shortCiteRegEx" : "Cui and Hu.",
      "year" : 2021
    }, {
      "title" : "A divide-and-conquer approach to the summarization of long documents",
      "author" : [ "Alexios Gidiotis", "Grigorios Tsoumakas." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:3029– 3040.",
      "citeRegEx" : "Gidiotis and Tsoumakas.,? 2020",
      "shortCiteRegEx" : "Gidiotis and Tsoumakas.",
      "year" : 2020
    }, {
      "title" : "Globalizing BERT-based transformer architectures for long document summarization",
      "author" : [ "Quentin Grail", "Julien Perez", "Eric Gaussier." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Grail et al\\.,? 2021",
      "shortCiteRegEx" : "Grail et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient attentions for long document summarization",
      "author" : [ "Luyang Huang", "Shuyang Cao", "Nikolaus Nova Parulian", "Heng Ji", "Lu Wang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "The icsi meeting corpus",
      "author" : [ "Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke" ],
      "venue" : null,
      "citeRegEx" : "Janin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Janin et al\\.",
      "year" : 2003
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Scoring sentence singletons and pairs for abstractive summarization",
      "author" : [ "Logan Lebanoff", "Kaiqiang Song", "Franck Dernoncourt", "Doo Soon Kim", "Seokhwan Kim", "Walter Chang", "Fei Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Lebanoff et al\\.,? 2019",
      "shortCiteRegEx" : "Lebanoff et al\\.",
      "year" : 2019
    }, {
      "title" : "2020a. BART: denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Retrieval-augmented generation",
      "author" : [ "Patrick S.H. Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Ranking sentences for extractive summarization with reinforcement learning",
      "author" : [ "Shashi Narayan", "Shay B Cohen", "Mirella Lapata." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "J. Mach. Learn. Res., 21:140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical learning for generation with long source sequences",
      "author" : [ "Tobias Rohde", "Xiaoxia Wu", "Yinhan Liu." ],
      "venue" : "arXiv preprint arXiv:2104.07545.",
      "citeRegEx" : "Rohde et al\\.,? 2021",
      "shortCiteRegEx" : "Rohde et al\\.",
      "year" : 2021
    }, {
      "title" : "Long range arena: A benchmark for efficient transformers",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Samira Abnar", "Yikang Shen", "Dara Bahri", "Philip Pham", "Jinfeng Rao", "Liu Yang", "Sebastian Ruder", "Donald Metzler." ],
      "venue" : "arXiv preprint arXiv:2011.04006.",
      "citeRegEx" : "Tay et al\\.,? 2020a",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient transformers: A survey",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler." ],
      "venue" : "CoRR, abs/2009.06732.",
      "citeRegEx" : "Tay et al\\.,? 2020b",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams." ],
      "venue" : "Mach. Learn., 8:229–256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Recursively summarizing books with human feedback",
      "author" : [ "Jeff Wu", "Long Ouyang", "Daniel M Ziegler", "Nissan Stiennon", "Ryan Lowe", "Jan Leike", "Paul Christiano." ],
      "venue" : "arXiv preprint arXiv:2109.10862.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Extractive summarization of long documents by combining global and local context",
      "author" : [ "Wen Xiao", "Giuseppe Carenini." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Xiao and Carenini.,? 2019",
      "shortCiteRegEx" : "Xiao and Carenini.",
      "year" : 2019
    }, {
      "title" : "Neural extractive text summarization with syntactic compression",
      "author" : [ "Jiacheng Xu", "Greg Durrett." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Xu and Durrett.,? 2019",
      "shortCiteRegEx" : "Xu and Durrett.",
      "year" : 2019
    }, {
      "title" : "Big bird: Transformers for longer sequences. In NeurIPS",
      "author" : [ "Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontanon", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang" ],
      "venue" : null,
      "citeRegEx" : "Zaheer et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zaheer et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretraining-based natural language generation for text summarization",
      "author" : [ "Haoyu Zhang", "Jingjing Cai", "Jianjun Xu", "Ji Wang." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 789–797.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter Liu." ],
      "venue" : "International Conference on Machine Learning, pages 11328–11339. PMLR.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Dialoglm: Pre-trained model for long dialogue understanding and summarization",
      "author" : [ "Ming Zhong", "Yang Liu", "Yichong Xu", "Chenguang Zhu", "Michael Zeng." ],
      "venue" : "arXiv preprint arXiv:2109.02492.",
      "citeRegEx" : "Zhong et al\\.,? 2021a",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    }, {
      "title" : "2021b. Qmsum: A new benchmark for query-based multi-domain meeting",
      "author" : [ "Ming Zhong", "Da Yin", "Tao Yu", "Ahmad Zaidi", "Mutethia Mutuma", "Rahul Jha", "Ahmed Hassan Awadallah", "Asli Celikyilmaz", "Yang Liu", "Xipeng Qiu", "Dragomir R. Radev" ],
      "venue" : null,
      "citeRegEx" : "Zhong et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    }, {
      "title" : "A hierarchical network for abstractive meeting summarization with cross-domain pretraining",
      "author" : [ "Chenguang Zhu", "Ruochen Xu", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Transformer-based (Vaswani et al., 2017) pretrained language models (PLMs) such as BART (Lewis et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : ", 2020a) and T5 (Raffel et al., 2020), have achieved state-of-the-art performance on short text summarization.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "However, due to the high memory complexity of the full self-attention (Tay et al., 2020a), PLMs still struggle to handle long inputs (Rohde et al.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 23,
      "context" : ", 2020a), PLMs still struggle to handle long inputs (Rohde et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "Model efficiency and summary quality present a pair of challenges (Huang et al., 2021): models need to capture information scattered across the long input while maintaining a low computational cost.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 27,
      "context" : ", 2021b) or optimized using reinforcement learning (Williams, 1992; Chen and Bansal, 2018; Bae et al., 2019; Bražinskas et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : ", 2021b) or optimized using reinforcement learning (Williams, 1992; Chen and Bansal, 2018; Bae et al., 2019; Bražinskas et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : ", 2021b) or optimized using reinforcement learning (Williams, 1992; Chen and Bansal, 2018; Bae et al., 2019; Bražinskas et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : ", 2021b) or optimized using reinforcement learning (Williams, 1992; Chen and Bansal, 2018; Bae et al., 2019; Bražinskas et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Third, models are proposed to divide source text into sections (Gidiotis and Tsoumakas, 2020; Wu et al., 2021) which are individually summarized and combined to form a full summary.",
      "startOffset" : 63,
      "endOffset" : 110
    }, {
      "referenceID" : 28,
      "context" : "Third, models are proposed to divide source text into sections (Gidiotis and Tsoumakas, 2020; Wu et al., 2021) which are individually summarized and combined to form a full summary.",
      "startOffset" : 63,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "Fourth, hierarchical models (Rohde et al., 2021; Zhu et al., 2020) improve summarization by capturing sentence or discourse level dependencies.",
      "startOffset" : 28,
      "endOffset" : 66
    }, {
      "referenceID" : 36,
      "context" : "Fourth, hierarchical models (Rohde et al., 2021; Zhu et al., 2020) improve summarization by capturing sentence or discourse level dependencies.",
      "startOffset" : 28,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "We conducted experiments on three long-input summarization datasets: GovReport (Huang et al., 2021) and arXiv (Cohan et al.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : ", 2021) and arXiv (Cohan et al., 2018) for longdocument summarization, and QMSum (Zhong et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "One approach is to adopt RL-based optimizations (Chen and Bansal, 2018; Bae et al., 2019), which has two drawbacks.",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "One approach is to adopt RL-based optimizations (Chen and Bansal, 2018; Bae et al., 2019), which has two drawbacks.",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "Second, current methods mostly use sentence-level ROUGE (Chen and Bansal, 2018) or summary-level ROUGE (Bae et al.",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Second, current methods mostly use sentence-level ROUGE (Chen and Bansal, 2018) or summary-level ROUGE (Bae et al., 2019) as training rewards.",
      "startOffset" : 103,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "Using sentence-level ROUGE could potentially select sentences with overlapping contents (Narayan et al., 2018), resulting in redundant final summaries.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "For Transformers (Vaswani et al., 2017) and encoderdecoder with attention models (Bahdanau et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : ", 2017) and encoderdecoder with attention models (Bahdanau et al., 2015), ht is usually the model’s output before the final language model head.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "It consists of meetings from three domains: AMI (Carletta et al., 2005), ICSI (Janin et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : ", 2005), ICSI (Janin et al., 2003), and committee meetings of the Welsh Parliament and Parliament of Canada;",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "GovReport (Huang et al., 2021) is a large-scale long document summarization dataset, consisting of about 19.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "arXiv (Cohan et al., 2018) is a dataset of scientific articles from arXiv.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "ArXiv is chosen over PubMed (Cohan et al., 2018) as arXiv contains much longer articles compared to PubMed.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "2 Evaluation Metrics and Baselines Evaluation Metrics ROUGE scores (Lin, 2004) are used as the automatic evaluation metrics for all experiments.",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "(2021a); 3) For arXiv, we include the results from the best performing models in previous works, including ExtSum-LG (Xiao and Carenini, 2019), PEGASUS (Zhang et al.",
      "startOffset" : 117,
      "endOffset" : 142
    }, {
      "referenceID" : 33,
      "context" : "(2021a); 3) For arXiv, we include the results from the best performing models in previous works, including ExtSum-LG (Xiao and Carenini, 2019), PEGASUS (Zhang et al., 2020), DANCER (Gidiotis and Tsoumakas, 2020), BigBird (Zaheer et al.",
      "startOffset" : 152,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : ", 2020), DANCER (Gidiotis and Tsoumakas, 2020), BigBird (Zaheer et al.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 31,
      "context" : ", 2020), DANCER (Gidiotis and Tsoumakas, 2020), BigBird (Zaheer et al., 2020), HEPOS + LSH (Huang et al.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : ", 2020), HEPOS + LSH (Huang et al., 2021), HAT-BART (Rohde et al.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : ", 2021), HAT-BART (Rohde et al., 2021), Longformer (Beltagy et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : ", 2021), Longformer (Beltagy et al., 2020), and SSN-DM (Cui and Hu, 2021).",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : "The extractor is initialized with the pretrained RoBERTa-base model (Liu et al., 2019) weights.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "We apply gradient checkpointing (Chen et al., 2016) to both the extractor and the generator to conserve GPU memory.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "We notice that while DYLE largely outperforms the LSH baseline (Huang et al., 2021) on the GovReport dataset, it underperforms the LSH baseline on arXiv.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : ", SSN-DM (Cui and Hu, 2021)) and divide-and-conquer approaches (e.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "Longformer (Beltagy et al., 2020) uses a dilated sliding window of blocks and global attention patterns.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 31,
      "context" : "BigBird (Zaheer et al., 2020) employs sliding window and random blocks.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "Reformer (Kitaev et al., 2020) uses the locality-sensitive hashing.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "Most two-stage summarization approaches (Zhang et al., 2019; Lebanoff et al., 2019; Xu and Durrett, 2019; Bajaj et al., 2021) are trained separately, which suffer from information loss due to the cascaded errors.",
      "startOffset" : 40,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "Most two-stage summarization approaches (Zhang et al., 2019; Lebanoff et al., 2019; Xu and Durrett, 2019; Bajaj et al., 2021) are trained separately, which suffer from information loss due to the cascaded errors.",
      "startOffset" : 40,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "Most two-stage summarization approaches (Zhang et al., 2019; Lebanoff et al., 2019; Xu and Durrett, 2019; Bajaj et al., 2021) are trained separately, which suffer from information loss due to the cascaded errors.",
      "startOffset" : 40,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "Most two-stage summarization approaches (Zhang et al., 2019; Lebanoff et al., 2019; Xu and Durrett, 2019; Bajaj et al., 2021) are trained separately, which suffer from information loss due to the cascaded errors.",
      "startOffset" : 40,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Divide-and-conquer approach A common approach in long input summarization is divide-andconquer (Gidiotis and Tsoumakas, 2020; Grail et al., 2021).",
      "startOffset" : 95,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "Divide-and-conquer approach A common approach in long input summarization is divide-andconquer (Gidiotis and Tsoumakas, 2020; Grail et al., 2021).",
      "startOffset" : 95,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "HAT-Bart (Rohde et al., 2021) proposes a new Hierarchical Attention Transformer-based architecture that attempts to capture sentence and paragraphlevel information.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 36,
      "context" : "HMNet (Zhu et al., 2020) builds a hierarchical structure that includes discourselevel information and speaker roles.",
      "startOffset" : 6,
      "endOffset" : 24
    } ],
    "year" : 0,
    "abstractText" : "Transformer-based models have achieved state-of-the-art performance on short-input summarization. However, they still struggle with summarizing longer text. In this paper, we present DYLE, a novel dynamic latent extraction approach for abstractive long-input summarization. DYLE jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable, allowing dynamic snippet-level attention weights during decoding. To provide adequate supervision, we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator. We evaluate our method on different long-document and long-dialogue summarization tasks: GovReport, QMSum, and arXiv. Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum, with gains up to 6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that the proposed dynamic weights provide interpretability of our generation process. 1",
    "creator" : null
  }
}
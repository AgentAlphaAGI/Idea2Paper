{
  "name" : "ARR_2022_360_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Around 200 languages in the world are signed rather than spoken, featuring their own vocabulary and grammatical structures. For example the American Sign Language (ASL) is not a mere translation of English into signs and is unrelated to the British Sign Language (BSL). This introduces many novel challenges to their automated processing. Research on Sign Language Processing (SLP) encompasses tasks such as sign language detection, i.e. recognising if and which signed language is performed (Moryossef et al., 2020) and sign language recognition (SLR) (Koller, 2020), i.e. the identification of signs either in isolation or in continuous speech. Other tasks concern the translation from signed to spoken (or written) (Camgoz et al., 2018) language or the production of signs from text\n(Rastgoo et al., 2021). With the recent success of deep learning-based approaches in computer vision (CV), as well as advancements in —from the CV perspective—related tasks of action and gesture recognition (Asadi-Aghbolaghi et al., 2017), SLP is gaining more attention in the CV community (Zheng et al., 2017).\nSome recent approaches to various SLP tasks rely on phonological features, perhaps due to the complexity of the tasks (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; Tavella et al., 2021). Surprisingly, however, little work has been carried out on explicitly modelling the phonology of signed languages. This presents a timely opportunity to investigate signed languages from a linguist’s perspective (Yin et al., 2021). In the context of signed languages, phonology typically distinguishes between manual features, such as usage, position and movement of hands and fingers, and non-manual features, such as facial expression. Sign language phonology is a matured field with well-developed theoretical frameworks (Liddell and Johnson, 1989; Fenlon et al., 2017; Sandler, 2012). These phonological features, or phonemes, are drawn from a fixed inventory of possible configurations which is typically much smaller than the vocabulary of signed languages (Borg and Camilleri, 2020). For example, there is only a limited number of fingers that can be used to perform a sign due to anatomical constraints. Hence, different signs share phonolog-\nical properties and well performing classifiers can be used to predict those properties for signs unseen during training. This potentially holds even across different languages, because, while different languages may dictate different combinations of phonemes, there are also significant overlaps (Tornay et al., 2020).\nFinally, these phonological properties have a strong discriminatory power when determining signs. For example, in ASL-Lex (Caselli et al., 2017), a lexicon which also captures phonology information, the authors report that more than 50% of its 994 described signs have a unique combination of only six phonological properties and more than 80% of the signs share their combination with at most two other signs. By relying on additional (i.e., phonological) information from resources such as ASL-Lex, many signs can be determined from (predicted) phonological properties alone, without encountering them in training data. This is a capability that current data-driven approaches to SLR lack by design (Koller, 2020). Thus, in combination, mature approaches to phonology recognition can facilitate the development of sign language resources. This is an important task for both documenting low-resource sign languages as well as rapid developing of large-scale datasets, to fully harness data-driven CV approaches.\nTo spur research in this direction, we extend the preliminary work by Tavella et al. (2021) and introduce the task of Phonological Property Recognition (PPR). More specifically, this paper contributes (i) WLASLLex2001, a large-scale, automatically constructed PPR dataset, (ii) an analysis of the dataset quality, and (iii) an empirical study of the performance of different deep-learning based baselines thereon."
    }, {
      "heading" : "2 Methodology",
      "text" : "We address PPR as a classification problem based on features extracted from videos of people speaking SL. Albeit manual annotation approaches are generally adopted, an automated approach would be less time and resource consuming, allowing researchers to limit their efforts to data validation. To extract such features, we take advantage of pretrained deep models from the computer vision community (Rong et al., 2021; Wang et al., 2019). Finally, we train several deep models to classify them as phonological classes.\nDataset construction: As previously men-\ntioned, ASL-Lex (Caselli et al., 2017) contains phonological features of American Sign Language, such as where the sign is executed, the movement performed by the hand or the number of hands involved. The latter properties were coded by 3 ASL-versed people. In our work, we are interested in recognising phonological classes from videos of people speaking ASL. Consequently, we aim to construct a dataset suitable for supervised learning, containing videos labelled with 6 phonological properties. We choose: (i) flexion, aperture of the selected fingers of the dominant hand at sign onset, (ii) major location, general location of the dominant hand at sign onset, (iii) minor location, specific location of the dominant hand at sign onset, (iv) movement, path movement of the first morpheme in the sign, (v) selected fingers, fingers that are moving or foregrounded in the first morpheme of the sign, and (vi) sign type, symmetry of the hands according to Battison (1978). A detailed description of all the properties is provided in the appendix. We selected these manual properties as they have a strong discriminatory power to predict signs based on their configuration (Caselli et al., 2017). One of the limitations of ASL-Lex is the small number of examples and its limited variety: its first iteration (ASL-Lex 1.0) contains less than 1000 videos, all signed by the same person. While sufficient for educational purposes, these videos are of limited suitability for developing robust classifiers that can capture the diversity of ASL speakers (Yin et al., 2021). To this end, we source videos from WLASL (Li et al., 2020) (Word Level-ASL), one of the largest available SL datasets, featuring more than 2000 glosses demonstrated by over 100 people, for a total of more than 20000 videos. Each sign is performed by at least 3 different signers, which implies greater variability compared to having one gloss performed by only one user. By cross referencing ASL-Lex and WLASL2000 based on corresponding glosses, we can increase the number of samples available to train our models. Finally, to leverage state of the art SLR architectures that operate over structured input, we enrich each raw video with its extracted keypoints that represent the joints of the speaker. To do so, we use two pretrained models, FrankMocap (Rong et al., 2021) and HRNet (Wang et al., 2019). While these tracking algorithms follow different paradigms, the former extracting 3D coordinates based on a predicted human model and the latter predicting keypoints as\ncoordinates from videos directly, they produce similar outputs. An important distinction is that while FrankMocap estimates the 3D keypoints, HRNet outputs 2D keypoints with associated prediction confidence scores. We use these different models to explore whether different tracking algorithms affect the recognition of phonological classes.We select a subset of features of the upper body, namely: nose, eyes, shoulders, elbows, wrists, thumbs and first/last knuckles of the fingers. These manual features were determined to be the most informative while performing sign language recognition (Jiang et al., 2021b).\nOur final dataset, WLASL-Lex2001 (WLASL2000 + ASL-Lex 1.0), is composed of 10017 videos corresponding to 800 glosses, 3D skeletons (x, y, z from FrankMocap and x, y and score from HRNet) labelled with their phonological properties. A characteristic of this dataset is that it follows a long tailed distribution. Due to the nature of language, some phonological properties are more common than others, which means that some classes are more represented than others. On the one hand, the training setup for our models should take this factor into account, but on the other hand, the advantage of training over phonological classes instead of glosses is that different glosses can share phonological classes.\nModels: To estimate the complexity of the dataset, we use the majority-class baseline and the Multi-Layer Perceptron (MLP) as a basic deep model. We further use Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) as models capable of capturing the temporal component of videos. As state-of-the-art SLP architectures that have been used to perform SLR, we use the I3D 3D Convolutional Neural Network (Carreira and Zisserman, 2017; Li et al., 2020) able to learn from raw videos, and the Spatio-Temporal Graph Convolutional Network (STGCN) (Jiang et al., 2021b) that captures both spatial and temporal components from the extracted keypoints.\nExperimental Setup: We generate one dataset and train different models for each phonological property. While this might not be the optimal way, as opposed to a multiclass multilabel approach, it is the best one in order to understand which features can and cannot be singularly learned, making the error analysis much easier. From now on, when we cite the dataset, we refer to an instance of the WLASL-Lex 2001 dataset, whose labels are the\nvalues of a single phonological class. We make this distinction because we split the dataset into train, validation and test sets (with a 70 : 15 : 15 ratio) using a stratified strategy based on the selected phonological class (Phoneme). By doing so, we make sure that all the different splits contain all possible values for a phonological class. Because our dataset features multiple videos per gloss, glosses in the test set appear in the training set as well. Thus, to investigate how well the models can predict properties on unseen glosses, we also produce label-stratified splits on gloss-level (Gloss), such that videos of glosses in the validation and test set do not appear in training data and vice versa.\nThe I3D is pre-trained on Kinetics-400 (Carreira and Zisserman, 2017) and fine-tuned on our datasets. The other models are trained from scratch using keypoints as input. We fix the length of all input to 150 frames, longer sequences are truncated while shorter sequences are looped to reach the fixed length. We select the best performing model based on performance on the validation set and for the final test set performance we train the models on both train and validation set. For more details on model selection, consult the appendix. We measure both accuracy, to investigate how well models perform in general, and class-balanced accuracy to take into account how well they are able to model different classes of the phonological properties."
    }, {
      "heading" : "3 Results and discussion",
      "text" : "The upper half of Table 1 presents the results for the six datasets split in a stratified fashion, not taking into account the corresponding glosses. The poor performance of the simple MLP architecture suggests that the tasks are in fact challenging and do not exhibit easily exploitable regularities. Due to its simplicity, for some properties it is barely able to reach the baseline (34% vs. 35% and 44% vs. 50% for movement and flexion respectively). In particular, MLP classifying based on FrankMocap (MLPF ) output is often the worst performing combination. Conversely, STGCN using HRNet output (STGCNH ) outperforms other models on all six tasks. In some cases, for example when predicting movement or flexion, it is the only model which significantly surpasses the majority class baseline. This superior performance is expected, as specifically this combination of the STGCN operating over HRNet-extracted keypoints has been shown to be the largest contributor to the SLR performance\non the WLASL2000 dataset (Jiang et al., 2021a). Models that operate over structured input often outperform the 3D CNN, demonstrating the utility of additional information provided by the skeleton features. The results also suggest that models using the HRNet skeleton output outperform those who use FrankMocap, possibly due to confidence scores produced by HRNet and associated with the coordinates. This difference in performance suggests to conduct a more rigorous study to investigate the impact of different feature extraction methods as a possible future research direction.\nThe lower half of Table 1 shows the evaluation results on unseen glosses (Gloss). The performance of all tasks and all models deteriorates, suggesting that their success is partly derived from exploiting the similarities of videos that appear in training and test data and refer to the same gloss. However, the best model, STGCNH , performs comparably to the Phoneme-split, with a drop of less than 10 accuracy points for five of the six tasks.\nOften, automatically constructed datasets such as ours, have a performance ceiling, for example due to incorrectly assigned ground truth labels or low quality of input data (Chen et al., 2016). To investigate the former, we measure the agreement on videos that all models misclassify using Fleiss’ κ. Intuitively, if all models agree on a label different than the ground truth, the ground truth label might be wrong. We find that averaged across the six tasks, the agreement is negligible: 0.09± 0.06 and 0.11 ± 0.09 for Phoneme and Gloss split, respectively. Similarly, for the latter, if all models consistently fail to assign any correct label for a\ngiven video (e.g. all models err on a video appearing in the test sets of movement and flexion), this can hint at low quality of the input, exacerbating processing it correctly. We find that this is not the case with WLASL-LEX2001, as videos appearing in test sets of different tasks tend to have a low mutual misclassification rate: 1% and 0.7% of videos appearing in test sets of two and three tasks were misclassified by all models for all associated tasks for the Phoneme split. For the Gloss split the numbers are 3 and 0% for two and three tasks, respectively. Together, these observations suggest that the models presented in this paper are unlikely to reach the performance ceiling on WLASL-Lex2001 and more advanced approaches could obtain even higher accuracy scores."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we discuss the task of Phonological Property Recognition (PPR). We automatically construct a dataset for the task featuring six phonological properties and analyse it extensively. We find that there is potential for improvement over our presented data-driven baseline approaches. Researchers pursuing this direction can focus on developing better-performing models, for example by relying on jointly learning all properties, as labels for different properties can be mutually dependent.\nAnother possible avenue is to investigate the feasibility of using PRR to perform tokenisation of continuous sign language speech, by decomposing it into multiple phonemes, which is identified as one of the big challenges of SLP (Yin et al., 2021)."
    }, {
      "heading" : "A Hyperparameters optimization",
      "text" : "Table 2 contains all the hyperparameters explored during our experiment over each different model. The best model is the one that maximises the Matthew’s correlation coefficient\nMCC = TP ·TN−FP ·FN√ (TP+FP )(TP+FN)(TN+FP )(TN+FN)\nwith TP, TN,FP, FN being true/false positive/negative. For the STGCN we use hyperparameters chosen by Jiang et al. (2021a), because initial experiments on our data showed a difference of at most 2% accuracy, which is within the uncertainty estimate. To find the optimal hyperparameters for the other models, we perform Bayesian optimisation over a pre-defined set We maximise Matthew’s correlation coefficient (MCC) (Matthews, 1975) on the validation sets of all six tasks. We choose MCC as it provides a good trade-off between overall and class-level accuracy which is necessary due to the unbalance inherently present in our dataset."
    }, {
      "heading" : "B Seed dependency",
      "text" : "Table 3 illustrates the performance on the test set for each model with respect to chance as measured by training 5 models from different random seeds. The performance difference is negligible suggesting that model training is largely stable with regard to chance."
    }, {
      "heading" : "C Phonological classes description",
      "text" : "Tables 4 to 9 describe in detail the meaning of values for all the phonological classes according to ASL-Lex (Caselli et al., 2017).\nThe cardinality is calculated on WLASL-Lex, which is why some classes that are in ASL-Lex are not represented (i.e., cardinality equal to 0)."
    } ],
    "references" : [ {
      "title" : "A Survey on Deep Learning Based Approaches for Action and Gesture Recogni",
      "author" : [ "Maryam Asadi-Aghbolaghi", "Albert Clapes", "Marco Bellantonio", "Hugo Jair Escalante", "Victor Ponce-Lopez", "Xavier Baro", "Isabelle Guyon", "Shohreh Kasaei", "Sergio Escalera" ],
      "venue" : null,
      "citeRegEx" : "Asadi.Aghbolaghi et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Asadi.Aghbolaghi et al\\.",
      "year" : 2017
    }, {
      "title" : "Lexical borrowing in american sign language",
      "author" : [ "Robbin Battison" ],
      "venue" : null,
      "citeRegEx" : "Battison.,? \\Q1978\\E",
      "shortCiteRegEx" : "Battison.",
      "year" : 1978
    }, {
      "title" : "Phonologically-Meaningful Subunits for Deep Learning-Based Sign Language Recognition",
      "author" : [ "Mark Borg", "Kenneth P. Camilleri." ],
      "venue" : "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture",
      "citeRegEx" : "Borg and Camilleri.,? 2020",
      "shortCiteRegEx" : "Borg and Camilleri.",
      "year" : 2020
    }, {
      "title" : "Quo vadis, action recognition? a new model and the kinetics dataset",
      "author" : [ "João Carreira", "Andrew Zisserman." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4724– 4733.",
      "citeRegEx" : "Carreira and Zisserman.,? 2017",
      "shortCiteRegEx" : "Carreira and Zisserman.",
      "year" : 2017
    }, {
      "title" : "Asllex: A lexical database of american sign language",
      "author" : [ "Naomi K. Caselli", "Zed Sevcikova Sehyr", "Ariel M. Cohen-Goldberg", "Karen Emmorey." ],
      "venue" : "Behavior Research Methods, 49(2):784–801.",
      "citeRegEx" : "Caselli et al\\.,? 2017",
      "shortCiteRegEx" : "Caselli et al\\.",
      "year" : 2017
    }, {
      "title" : "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Sign language phonology",
      "author" : [ "J Fenlon", "Kearsy A Cormier", "Diane Brentari." ],
      "venue" : "Routledge Handbook of Phonological Theory.",
      "citeRegEx" : "Fenlon et al\\.,? 2017",
      "shortCiteRegEx" : "Fenlon et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic sign language identification",
      "author" : [ "Binyam Gebrekidan Gebre", "Peter Wittenburg", "Tom Heskes." ],
      "venue" : "2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings, pages 2626– 2630.",
      "citeRegEx" : "Gebre et al\\.,? 2013",
      "shortCiteRegEx" : "Gebre et al\\.",
      "year" : 2013
    }, {
      "title" : "2021a. Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble",
      "author" : [ "Songyao Jiang", "Bin Sun", "Lichen Wang", "Yue Bai", "Kunpeng Li", "Yun Fu" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Skeleton Aware Multimodal Sign Language Recognition",
      "author" : [ "Songyao Jiang", "Bin Sun", "Lichen Wang", "Yue Bai", "Kunpeng Li", "Yun Fu." ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 3408–3418.",
      "citeRegEx" : "Jiang et al\\.,? 2021b",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Quantitative Survey of the State of the Art in Sign Language Recognition",
      "author" : [ "Oscar Koller" ],
      "venue" : null,
      "citeRegEx" : "Koller.,? \\Q2020\\E",
      "shortCiteRegEx" : "Koller.",
      "year" : 2020
    }, {
      "title" : "Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison",
      "author" : [ "Dongxu Li", "Cristian Rodriguez", "Xin Yu", "Hongdong Li." ],
      "venue" : "The IEEE Winter Conference on Applications of Computer Vision, pages 1459–1469.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "American Sign Language: The Phonological Base",
      "author" : [ "Scott K. Liddell", "Robert E. Johnson." ],
      "venue" : "Sign Language Studies, 1064(1):195–277.",
      "citeRegEx" : "Liddell and Johnson.,? 1989",
      "shortCiteRegEx" : "Liddell and Johnson.",
      "year" : 1989
    }, {
      "title" : "Comparison of the predicted and observed secondary structure of t4 phage lysozyme",
      "author" : [ "B.W. Matthews." ],
      "venue" : "Biochimica et Biophysica Acta (BBA) - Protein Structure, 405(2):442–451.",
      "citeRegEx" : "Matthews.,? 1975",
      "shortCiteRegEx" : "Matthews.",
      "year" : 1975
    }, {
      "title" : "Scalable ASL sign recognition using modelbased machine learning and linguistically annotated corpora",
      "author" : [ "Dimitris Metaxas", "Mark Dilsizian", "Carol Neidle" ],
      "venue" : null,
      "citeRegEx" : "Metaxas et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Metaxas et al\\.",
      "year" : 2018
    }, {
      "title" : "Real-Time Sign Language Detection Using Human Pose Estimation",
      "author" : [ "Amit Moryossef", "Ioannis Tsochantaridis", "Roee Aharoni", "Sarah Ebling", "Srini Narayanan." ],
      "venue" : "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and",
      "citeRegEx" : "Moryossef et al\\.,? 2020",
      "shortCiteRegEx" : "Moryossef et al\\.",
      "year" : 2020
    }, {
      "title" : "Sign Language Recognition: A Deep Survey",
      "author" : [ "Razieh Rastgoo", "Kourosh Kiani", "Sergio Escalera." ],
      "venue" : "Expert Systems with Applications, 164:113794.",
      "citeRegEx" : "Rastgoo et al\\.,? 2021",
      "shortCiteRegEx" : "Rastgoo et al\\.",
      "year" : 2021
    }, {
      "title" : "Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration",
      "author" : [ "Yu Rong", "Takaaki Shiratori", "Hanbyul Joo." ],
      "venue" : "IEEE International Conference on Computer Vision Workshops.",
      "citeRegEx" : "Rong et al\\.,? 2021",
      "shortCiteRegEx" : "Rong et al\\.",
      "year" : 2021
    }, {
      "title" : "The Phonological Organization of Sign Languages",
      "author" : [ "Wendy Sandler." ],
      "venue" : "Language and Linguistics Compass, 6(3):162–182.",
      "citeRegEx" : "Sandler.,? 2012",
      "shortCiteRegEx" : "Sandler.",
      "year" : 2012
    }, {
      "title" : "Phonology recognition in american sign language",
      "author" : [ "Federico Tavella", "Aphrodite Galata", "Angelo Cangelosi" ],
      "venue" : null,
      "citeRegEx" : "Tavella et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Tavella et al\\.",
      "year" : 2021
    }, {
      "title" : "Explainable Phonology-based Approach for Sign Language Recognition and Assessment",
      "author" : [ "Sandrine Tornay." ],
      "venue" : "Ph.D. thesis, Lausanne, EPFL.",
      "citeRegEx" : "Tornay.,? 2021",
      "shortCiteRegEx" : "Tornay.",
      "year" : 2021
    }, {
      "title" : "Towards Multilingual Sign Language Recognition",
      "author" : [ "Sandrine Tornay", "Marzieh Razavi", "Mathew Magimai.-Doss." ],
      "venue" : "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6304–6308.",
      "citeRegEx" : "Tornay et al\\.,? 2020",
      "shortCiteRegEx" : "Tornay et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep high-resolution representation learning for visual recognition",
      "author" : [ "Jingdong Wang", "Ke Sun", "Tianheng Cheng", "Borui Jiang", "Chaorui Deng", "Yang Zhao", "Dong Liu", "Yadong Mu", "Mingkui Tan", "Xinggang Wang", "Wenyu Liu", "Bin Xiao." ],
      "venue" : "TPAMI.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Including Signed Languages in Natural Language Processing",
      "author" : [ "Kayo Yin", "Amit Moryossef", "Julie Hochgesang", "Yoav Goldberg", "Malihe Alikhani." ],
      "venue" : "pages 7347–7360.",
      "citeRegEx" : "Yin et al\\.,? 2021",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2021
    }, {
      "title" : "Recent Advances of Deep Learning for Sign Language Recognition",
      "author" : [ "Lihong Zheng", "Bin Liang", "Ailian Jiang." ],
      "venue" : "DICTA 2017 - 2017 International Conference on Digital Image Computing: Techniques and Applications, 2017-Decem:1–7.",
      "citeRegEx" : "Zheng et al\\.,? 2017",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2017
    }, {
      "title" : "(TP+FN)(TN+FP )(TN+FN) with TP, TN,FP, FN being true/false positive/negative. For the STGCN we use hyperparameters chosen by Jiang et al",
      "author" : [ "√ (TP+FP" ],
      "venue" : null,
      "citeRegEx" : ".TP+FP,? \\Q2021\\E",
      "shortCiteRegEx" : ".TP+FP",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "recognising if and which signed language is performed (Moryossef et al., 2020) and sign language recognition (SLR) (Koller, 2020), i.",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : ", 2020) and sign language recognition (SLR) (Koller, 2020), i.",
      "startOffset" : 44,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "With the recent success of deep learning-based approaches in computer vision (CV), as well as advancements in —from the CV perspective—related tasks of action and gesture recognition (Asadi-Aghbolaghi et al., 2017), SLP is gaining more attention in the CV community (Zheng et al.",
      "startOffset" : 183,
      "endOffset" : 214
    }, {
      "referenceID" : 24,
      "context" : ", 2017), SLP is gaining more attention in the CV community (Zheng et al., 2017).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 20,
      "context" : "Some recent approaches to various SLP tasks rely on phonological features, perhaps due to the complexity of the tasks (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; Tavella et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 196
    }, {
      "referenceID" : 14,
      "context" : "Some recent approaches to various SLP tasks rely on phonological features, perhaps due to the complexity of the tasks (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; Tavella et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 196
    }, {
      "referenceID" : 7,
      "context" : "Some recent approaches to various SLP tasks rely on phonological features, perhaps due to the complexity of the tasks (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; Tavella et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 196
    }, {
      "referenceID" : 19,
      "context" : "Some recent approaches to various SLP tasks rely on phonological features, perhaps due to the complexity of the tasks (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; Tavella et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 196
    }, {
      "referenceID" : 23,
      "context" : "This presents a timely opportunity to investigate signed languages from a linguist’s perspective (Yin et al., 2021).",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Sign language phonology is a matured field with well-developed theoretical frameworks (Liddell and Johnson, 1989; Fenlon et al., 2017; Sandler, 2012).",
      "startOffset" : 86,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "Sign language phonology is a matured field with well-developed theoretical frameworks (Liddell and Johnson, 1989; Fenlon et al., 2017; Sandler, 2012).",
      "startOffset" : 86,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "Sign language phonology is a matured field with well-developed theoretical frameworks (Liddell and Johnson, 1989; Fenlon et al., 2017; Sandler, 2012).",
      "startOffset" : 86,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "These phonological features, or phonemes, are drawn from a fixed inventory of possible configurations which is typically much smaller than the vocabulary of signed languages (Borg and Camilleri, 2020).",
      "startOffset" : 174,
      "endOffset" : 200
    }, {
      "referenceID" : 21,
      "context" : "This potentially holds even across different languages, because, while different languages may dictate different combinations of phonemes, there are also significant overlaps (Tornay et al., 2020).",
      "startOffset" : 175,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "For example, in ASL-Lex (Caselli et al., 2017), a lexicon which also captures phonology information, the authors report that more than 50% of its 994 described signs have a unique combination of only six phonological properties and more than 80% of the signs share their combination with at most two other signs.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "This is a capability that current data-driven approaches to SLR lack by design (Koller, 2020).",
      "startOffset" : 79,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "To extract such features, we take advantage of pretrained deep models from the computer vision community (Rong et al., 2021; Wang et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : "To extract such features, we take advantage of pretrained deep models from the computer vision community (Rong et al., 2021; Wang et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "Dataset construction: As previously mentioned, ASL-Lex (Caselli et al., 2017) contains phonological features of American Sign Language, such as where the sign is executed, the movement performed by the hand or the number of hands involved.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "We selected these manual properties as they have a strong discriminatory power to predict signs based on their configuration (Caselli et al., 2017).",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "While sufficient for educational purposes, these videos are of limited suitability for developing robust classifiers that can capture the diversity of ASL speakers (Yin et al., 2021).",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 11,
      "context" : "To this end, we source videos from WLASL (Li et al., 2020) (Word Level-ASL), one of the largest available SL datasets, featuring more than 2000 glosses demonstrated by over 100 people, for a total of more than 20000 videos.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "To do so, we use two pretrained models, FrankMocap (Rong et al., 2021) and HRNet (Wang et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "These manual features were determined to be the most informative while performing sign language recognition (Jiang et al., 2021b).",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "As state-of-the-art SLP architectures that have been used to perform SLR, we use the I3D 3D Convolutional Neural Network (Carreira and Zisserman, 2017; Li et al., 2020) able to learn from raw videos, and the Spatio-Temporal Graph Convolutional Network (STGCN) (Jiang et al.",
      "startOffset" : 121,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "As state-of-the-art SLP architectures that have been used to perform SLR, we use the I3D 3D Convolutional Neural Network (Carreira and Zisserman, 2017; Li et al., 2020) able to learn from raw videos, and the Spatio-Temporal Graph Convolutional Network (STGCN) (Jiang et al.",
      "startOffset" : 121,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : ", 2020) able to learn from raw videos, and the Spatio-Temporal Graph Convolutional Network (STGCN) (Jiang et al., 2021b) that captures both spatial and temporal components from the extracted keypoints.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "The I3D is pre-trained on Kinetics-400 (Carreira and Zisserman, 2017) and fine-tuned on our datasets.",
      "startOffset" : 39,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Often, automatically constructed datasets such as ours, have a performance ceiling, for example due to incorrectly assigned ground truth labels or low quality of input data (Chen et al., 2016).",
      "startOffset" : 173,
      "endOffset" : 192
    }, {
      "referenceID" : 23,
      "context" : "Another possible avenue is to investigate the feasibility of using PRR to perform tokenisation of continuous sign language speech, by decomposing it into multiple phonemes, which is identified as one of the big challenges of SLP (Yin et al., 2021).",
      "startOffset" : 229,
      "endOffset" : 247
    }, {
      "referenceID" : 13,
      "context" : "To find the optimal hyperparameters for the other models, we perform Bayesian optimisation over a pre-defined set We maximise Matthew’s correlation coefficient (MCC) (Matthews, 1975) on the validation sets of all six tasks.",
      "startOffset" : 166,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : "Tables 4 to 9 describe in detail the meaning of values for all the phonological classes according to ASL-Lex (Caselli et al., 2017).",
      "startOffset" : 109,
      "endOffset" : 131
    } ],
    "year" : 0,
    "abstractText" : "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.",
    "creator" : null
  }
}
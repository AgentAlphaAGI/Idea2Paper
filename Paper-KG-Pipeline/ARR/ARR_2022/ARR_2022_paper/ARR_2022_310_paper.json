{
  "name" : "ARR_2022_310_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BiTIIMT: A Bilingual Text-infilling Method for Interactive Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent years have witnessed significant development in neural machine translation(NMT) (Bahdanau et al., 2015; Vaswani et al., 2017). Despite of their success, their translation is still too poor in quality to be directly used in industrial applications. On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.\nTraditional IMT generates a translation in the left-to-right completing paradigm (Langlais et al.,\n2000; Sanchis-Trilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016; Zhao et al., 2020) where human translators are required to revise words in the translation prefix. This strict leftto-right manner limits its flexibility because some human translators may enjoy their translation manners beyond left-to-right. As a result, another part of works (Weng et al., 2019; Huang et al., 2021) propose an alternative IMT paradigm under which human translators can revise words at arbitrary positions of a translation. The essential technique to this paradigm is lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), which extends beam search in the decoding stage to include revised words as constraints.\nUnfortunately, LCD-based IMT suffers from two major shortcomings on efficiency and translation quality in practice. Firstly, LCD-based IMT usually involves multiple interactions between a human translator and machine and runs the LCD algorithm multiple times to translate a sentence. Since each LCD run takes considerable time compared with NMT decoding, the human translator will encounter severe latency, leading to poor user experience. In addition, LCD is based on the standard translation model, which is defined on top of the prefix context, and thus cannot make use of the revised words to assist the model in predicting target words to their left. Hence this characteristic limits its overall translation quality.\nThis paper proposes a simple yet effective IMT approach, BiTIIMT, which addresses the issues above. The core idea to BiTIIMT is the Bilingual Text-infilling (BiTI) task which extends textinfilling (Zhu et al., 2019) from monolingual setting to bilingual setting and aims to fill in missing segments in a revised translation for a given source sentence. Unlike Zhu et al. (2019) carefully designing a model, we simply cast the bilingual text-infilling task as a sequence-to-sequence task and then employ the standard NMT model to perform this task.\nTo train the model, we construct simulated data by randomly sampling revised sentences from a bilingual corpus and augment the simulated data with bilingual corpus for further improvements. In this way, our model is able to yield a valid output that can be seamlessly filled in the revised translation in an efficient way similar to the standard NMT decoding without explicit constraints. Moreover, the proposed model makes full use of all revised words to predict a target word and thus has the potential to obtain better translation than LCD.\nWe conduct extensive experiments on WMT14 En-De, WMT14 En-Fr and Zh-En tasks. Our simulated experiments demonstrate that the proposed model indeed outperforms LCD in terms of translation quality and efficiency; and the proposed BiTIIMT is better than LCDIMT according to both translation quality and human editing costs. The advantages of BiTIIMT over LCDIMT are also verified in the real-world IMT experiments with human translators.\nThis paper makes the following contributions:\n• It proposes the bilingual text-infilling task and provides a simple yet effective solution to address this task.\n• It proposes a novel IMT system on top of bilingual text-infilling which empirically outperforms a strong baseline in both translation quality and efficiency."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Neural Machine Translation",
      "text" : "Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) is based on a sequence to sequence model which adopts an encoder-decoder architecture. The encoder summarizes the source sentence into an intermediate representation, and the decoder generates the target sentence.\nGiven a source sentenceX = {x1, · · · , xi, · · · }, a NMT model factors the distribution over possible output Y = {y1, · · · , yT } into a chain of conditional probabilities from left to right:\nP (Y |X; θ) = T+1∏ t=1 P (yt|y1, y2, · · · , yt−1, X; θ)\n(1) where the special tokens y0 (e.g. <bos>) and yT+1 (e.g. <eos>) are used to represent the beginning and end of all target sentences."
    }, {
      "heading" : "2.2 Text Infilling",
      "text" : "Text infilling (Zhu et al., 2019) is a task that fills missing text segments of a sentence or paragraph by a model (Berglund and Leo; Fedus et al., 2018; Zhu et al., 2019) trained on a large amount of data in a fill-in-the-blank format. The input text X = {x1,<blank>, xi, ...,<blank>, ...} has an unknown number of blanks whose positions are arbitrary, and each blank has an arbitrary unknown length.\nTo address this task, a text infilling model fills in each blank from left to right by predicting a target word yj at each time step j. As a solution, Zhu et al. (2019) proposes a model based on a variant of Transformer, whose position encoding takes both segment positions and token positions into account."
    }, {
      "heading" : "3 Proposed BiTIIMT",
      "text" : "This section illustrates the overview of the proposed BiTIIMT system and accordingly presents its essential technique (i.e., bilingual text-infilling) to take human interactions into the NMT."
    }, {
      "heading" : "3.1 Overview of BiTIIMT",
      "text" : "In general, the proposed BiTIIMT enjoys a humanin-the-loop manner to output the final translation, similar to conventional IMT systems (Cheng et al., 2016). Specifically, for a given source sentence X , BiTIIMT iteratively performs the following two steps:\n• A human translator edits a translation Y from the translation engine;\n• Then the engine updates Y based on the edited translation as well as its source X .\nThis procedure terminates until the human translator is satisfied with the quality of Y . This procedure is illustrated in Figure 1. The key to BiTIIMT is its second step which relies on Bilingual Text-infilling to update a translation Y . In the rest of this section, the details about bilingual text-infilling will be described."
    }, {
      "heading" : "3.2 Bilingual Text-infilling",
      "text" : ""
    }, {
      "heading" : "3.2.1 Problem Statement and Model",
      "text" : "Problem Statement Generally, bilingual textinfilling extends text-infilling from the monolingual setting (Zhu et al., 2019) to the bilingual setting. Suppose Ȳ is a template, i.e., the edited (partial) translation, which includes some blanks to be filled in; Y b = {yb1, yb2, · · · , } is a sequence of segments used to fill in the blanks in Ȳ . BiTI aims to generate Y b for filling in the blanks in Ȳ , to obtain a translation Y for a source sentence X .\nTake Figure 2 as an example, the template Ȳ is a partial translation edited by a translator which contains three blanks. Y b includes three segments to fill in each blank in Ȳ , and Y is the translation after filling in Ȳ with Y b. It is worth noting that Y b contains three special tokens “<eob>”, indicating the end of a segment, which corresponds to the blanks in Ȳ , respectively.\nModel Definition Formally, bilingual textinfilling can be addressed by the following probabilistic model:\nP (Y b|X, Ȳ ; θ) = ∏ t P (ybt | X, Ȳ , Y b<t; θ) (2)\nTo implement this model, it is possible to extend the Transformer (Vaswani et al., 2017) by using two encoders (i.e., one is for X , and the other is for Ȳ ) and taking both segment and token positions into account. However, for simplicity, we instead employ the standard Transformer via a transformation on the input format:\nwe treat two input sequences X and Ȳ as one input sequence “X <sep> Ȳ ”, where “<sep>” is a speck token for concatenation. In this way, the task of bilingual text-infilling is actually a standard sequence-to-sequence task, where the input is “X <sep> Ȳ ” and the output is Y b. Figure 3 shows an example for this trick.\nRelation to Previous Work Our method is similar to previous works, including lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), MT with soft constraints (Dinu et al., 2019) and code-switch enhanced MT (Song et al., 2019) in the sense that generating output in the condition of given constraints which is Ȳ in our work. However, LCD treat Ȳ as constraints by extending beam search in the decoding stage, leading to the suffering of decoding speed. And another two series of works can not guarantee the constraints Ȳ will be in the output translation."
    }, {
      "heading" : "3.2.2 Training via Data Augmentation",
      "text" : "Typically, to train the model in Eq.(2), one needs to obtain large amount of data consisting of triples D = {〈X, Ȳ , Y b〉}. Unfortunately, this is impractical because both Ȳ and Y b are obtained by human\ntranslators. To this end, we apply a simple simulation method to obtain {〈X, Ȳ , Y b〉} on top of bilingual corpus {〈X,Y 〉} through random sampling. Specifically, for each oracle target sentence Y in bilingual data, we randomly sample an integer k ∈ [0, 5] and randomly sample k non-overlapped segments in Y . Then Ȳ can be obtained by replacing each remaining segment with “<blank>” in Y , and Y b can be obtained by wrapping k sampled segments and then joining them together in the same order. This sampling procedure is shown in Figure 4.\nGiven a training data D = {〈X, Ȳ , Y b〉}, it is straightforward to optimize the following objective function according to maximum likelihood estimation:\n` = − ∑\n〈X,Ȳ ,Y b〉\nlogP (Y b | X, Ȳ ; θ)\nwhere the model P is defined in Eq. (2). Models trained on training data D = {〈X, Ȳ , Y b〉} possess the ability to translate sentences with constraints in fill-in-blank format style, but it may lose its strength in translating normal sentences. As a result, for the first iteration in IMT, the initial translation may be worse, leading to more interactive iterations between human and machine. One possible solution is to build an additional NMT model to generate the initial translation. Instead, we apply the data augmentation technique, making our model to perform both tasks. For the standard bilingual parallel data {〈X,Y 〉}, we construct its trivial triple data D′ = {〈X, ∅, Y 〉}, where ∅ means that no revised words are provided in Ȳ for each bilingual sentence. Then we combine the sampled data D and the trivial data D ′ as the augmented data to train the model P in our experiments."
    }, {
      "heading" : "3.3 Decoding",
      "text" : "The task of bilingual text-infilling is reduced to decode Y b according to the model P in Eq. (2) via maximum a posteriori (MAP) estimation as follows:\narg max Y b:#b(Y b)=#b(Ȳ )\nP (Y b | X, Ȳ ; θ)\nwhere #b denotes the number of blanks, i.e., #b(Y\nb) is the number of “<eob>” in Y b and #b(Ȳ ) counts the number of “ ” in Ȳ . The constraints in the above equation is used to guarantee that all\nblanks in Ȳ can be exactly filled by Y b to obtain a valid translation Y , otherwise Y b would lead to an invalid Y .\nTheoretically, the constrained optimization is more difficult than the unconstrained one. For example, lexical constrained decoding (Hokamp and Liu, 2017) is more complex and less efficient than NMT decoding without constraints. In practice, thanks to the robust Transformer model architecture and sufficient training data in our scenario, our model is able to implicitly learn the constraint #b(Y\nb) = #b(Ȳ ). In other words, we employ the standard beam search algorithm to yield Y b, which leads to a valid Y , without explicitly imposing the constraint during decoding. As a result, our decoding algorithm is as efficient as the standard NMT."
    }, {
      "heading" : "4 Experiments",
      "text" : "Following previous works (Peris et al., 2017b; Cheng et al., 2016; Weng et al., 2019; Li et al., 2020), we experiment on two simulated scenarios and a real-world scenario."
    }, {
      "heading" : "4.1 Experiment Settings",
      "text" : ""
    }, {
      "heading" : "4.1.1 Dataset",
      "text" : "We conduct experiments on WMT14 EnglishGerman dataset (En-De), WMT14 English-French (En-Fr), and a Chinese-English dataset (Zh-En) proposed in Li et al. (2020), which includes about 2 million bilingual news sentences in total. For En-De, we use newstest13 as valid dataset and use Stanford newstest14 as test dataset. We use the approach mentioned in Section 3.2 to construct synthetic bilingual parallel sentence pairs based on all the datasets above. Based on the idea of data augmentation, we combine original datasets and their corresponding synthetic datasets. As Table 1 shows, we get an 8M English-German dataset\nbased on the WMT14 En-De, a 72M EnglishFrench dataset based on WMT14 En-Fr, and a 4M Chinese-English dataset based on a Zh-En dataset mentioned above. Valid sets are obtained the same as training datasets.\nFor all datasets mentioned above, we use Moses toolkit to tokenize and clean data. Besides, we use BPE (Sennrich et al.) to process all the source and target sentences."
    }, {
      "heading" : "4.1.2 System Configurations",
      "text" : "We train and evaluate the following systems to evaluate BiTiIMT thoroughly.\n• Transformer. We set Transformer (Vaswani et al., 2017) using fairseq (Ng et al., 2019). The Transformer model is trained on the bilingual datasets: WMT14 En-De, WMT14 EnFr, and the Zh-En dataset.\n• LCDIMT. Since LCD-based decoding is widely used in IMT scenario (Weng et al., 2019; Huang et al., 2021), we build an LCDbased IMT as our strong baseline. Because the original LCD algorithm is very slow, we implement its efficient version (Post and Vilar, 2018) which achieves comparable translation quality to the original version but 5x speedup in decoding.\n• Our System. Our BiTiIMT model is based on the base Transformer architecture and trained on the synthetic datasets mentioned in Section 3.3.\nAll the NMT and the BiTIIMT models are based on the architecture with dmodel = 512, dhidden = 2048, nheads = 8, nlayers = 6, and pdropout = 0.1. We use the Adam Optimizer (Kingma and Ba, 2015) with β1 = 0.9 and β2 = 0.98 to train our models. We adopt a warm-up of 10,000 steps and set the initial learning rate to 0.0007. We set the maximum tokens in batch to 4096, and we share all embeddings for all models. Training stops until the maximum update 400,000. We train all models on 16 NVIDIA V100 Tensor Core GPUs. We use beam size of 10 throughout our experiments."
    }, {
      "heading" : "4.1.3 Evaluation Metrics",
      "text" : "In evaluating IMT systems, there are two criteria: one is the final translation quality and the other is efficiency to yield the final translation. This makes evaluating IMT systems as bi-objective evaluation: in this sense, IMT system A is better than system B\nif and only if A achieves higher BLEU while using less efficiency cost than B. We employ BLEU (Papineni et al., 2002) to measure translation quality. For the second criteria, two types of costs exist to measure efficiency, i.e., human editing cost and decoding time cost. Human editing cost is a widely used cost (Cheng et al., 2016; Weng et al., 2019; Zhao et al., 2020; Huang et al., 2021). We qualify the human interaction cost by calculating edit distance by counting deletions on word level and insertions on char level. In addition, we take the decoding time into account because it is directly related to the latency for human translators, which is critical for user experience."
    }, {
      "heading" : "4.2 Simulated Scenario",
      "text" : "We conducted two different simulated experiments, including IMT with a single iteration and IMT with multiple iterations, to validate the effectiveness of our method on translation quality and costs mentioned above."
    }, {
      "heading" : "4.2.1 IMT with a Single Iteration",
      "text" : "Since each iteration of IMT can be seen as machine translation with lexical constraints where human interactions are considered as constraints, we first conduct an experiment to evaluate the performance of BiTIIMT by interactive once. Following Hokamp and Liu (2017); Post and Vilar (2018), we use the references as oracle to sample constraints for all datasets and we consider five settings: they include gradually increasing constraint segments. In more detail, we randomly add a constraint segment with a random length between 1 to 3 for each setting. To ensure fairness, the constraints provided to both BiTIIMT and LCDIMT are exactly the same. We qualify decoding cost by using the time required to translate the whole test set with a batch size of one (excluding the model loading time).\nAs shown in Table 2, with only one constraint segment, both BiTIIMT and LCDIMT obtain substantial improvements compared with Transformer and BiTIIMT significantly outperforms LCDIMT with a margin of +2.2, +4.63, and +1.12 BLEU points for En-De, En-Fr and Zh-En tasks, respectively. This finding clearly verifies our hypothesis: BiTIIMT indeed makes full use of the constraint segments and thus yields better translations than LCDIMT.\nTable 3 reports the results of relative decoding cost with respect to the Transformer baseline. As\nwe can see, In the first setting with one constraint segment, BiTIIMT achieves modest speedup in decoding time compared with the Transformer baseline. With the growing number of constraint segments, BiTIIMT keeps reducing the decoding time, and it gives a factor of 0.27 decrease in decoding time cost on En-De when running on five constraint segments. Meanwhile, the time cost of LCDIMT keeps growing, and it is almost twice the baseline in the 5th setting. Similar results can be found on En-Fr and Zh-En datasets."
    }, {
      "heading" : "4.2.2 IMT with Multiple Iterations",
      "text" : "We now turn to the evaluation of BiTIIMT in a simulated IMT scenario where multiple iterations of interactions are allowed. To simulate the human interactive process of IMT, at each iteration, we use the reference as oracle and match the oracle\nwith the translation from each system to calculate Ȳ . Specifically, we delete unmatched words in the translation as simulated deletion and add a word from oracle as simulated revision. By using the edit distance algorithm, which includes only deletion and revision operators, we can figure out Ȳ given a translation and its reference. We use the edit cost mentioned in Section 4.1.3 to qualify human interaction cost.\nFigure 5 shows the BLEU score and simulated interaction cost along with different iterations on En-De, En-Fr, and Zh-En datasets. As expected, the BLEU score consistently increases with an increase in interaction cost. As we can see on En-De, BiTIIMT obtains improvements of about 5 BLEU points over the baseline LCDIMT when using similar human interaction costs. The BLEU gap between BiTIIMT and LCDIMT is further enlarged\nin the late stage of the interactive process. Results on En-Fr and Zh-En give similar conclusions. The results show that BiTIIMT outperforms LCDIMT as it can reduce the human interaction cost to get a satisfying translation."
    }, {
      "heading" : "4.3 Real-world Scenario",
      "text" : "To validate the efficiency of BiTIIMT in the realworld scenario, we conduct IMT experiments on Zh-En with human translators. We randomly sample 200 sentences from the Zh-En test set and ask two professional human translators to interact with both systems. Translators are asked to do interactions (deletions, revisions, and insertions) multiple times until they get a satisfactory translation. We compared LCD-based and BiTI-based IMT systems on averaged BLEU and averaged edit cost of deletions, revisions, and insertions supplied by human translators. As shown in Table 4, BiTIIMT can reach a higher BLEU with lower edit costs."
    }, {
      "heading" : "4.4 Analysis",
      "text" : "Effect of Data Augmentation As the description in Section 3.2.2, we train our model (#Augmented in Table 5) on the augmented data which includes synthetic bilingual data D = {〈X, Ȳ , Y b〉} and their corresponding bilingual parallel data D′ = {〈X, ∅, Y 〉} by data augmentation. To further show the advantage of data augmentation tech-\nnique, we train two additional models: one is on D only (#Synthetic Only in Table 5) and the other is trained on D′ (#Raw in Table 5). We compare all these three models according to their BLEU on the Zh-En dataset. Note that in this experiment, we do not provide any human interactions to all three models, i.e., Ȳ = ∅, during testing.\nTable 5 summarizes the results of three different models. The model trained on the only synthetic dataset almost collapses on machine translation task: it is worse than # Raw model by a substantial margin of 18 BLEU points. In addition, it can be found that the model trained on the augmented dataset achieves +0.68 BLEU improvements over # Raw model. Although such improvements are not that large, it still shows that data augmentation plays a critical role in making BiTiIMT successful, because # Raw model can not handle the bilingual text-infilling problem since it does not learn from Ȳ during training.\nEffect of Random Sampling Strategy When training the model for BiTIMT in section 3.2.2, we employ a random strategy to sample Ȳ such that it contains a random number of constraint segments. We are also curious about the effect of such a random strategy. In comparison, we fix the num-\nber of constraint segments as k and then train three models for k = {1, 3, 5}. Under all five settings as in Section 4.2.1, we compare BiTIIMT with these three models on the Zh-En task. Figure 6 shows that, by training on data with a diverse number of constraints, our model achieves increasing BLEU on all five settings while the BLEU of the model (k = 1) gradually decreases and another two models could not give continuous improvement. Results suggest that our random sampling strategy assists BiTIIMT to translate with a various number of constraints."
    }, {
      "heading" : "5 Related Work",
      "text" : "From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016). Recently, with the development of NMT (Bahdanau et al., 2015; Vaswani et al., 2017), researchers turned to employing IMT on it.\nA classical type of IMT uses a left-to-right sentence completing framework proposed in Langlais et al. (2000), in which human translators can only do revisions on the translation generated by models from left to right. Generally, the text portion from the beginning to the current modified part is called prefix, and the system will generate a\nnew translation based on the given prefix (SanchisTrilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016).\nCheng et al. (2016) propose a pick-revise framework that enables translators do revisions on arbitrary positions to improve efficiency. Huang et al. (2021) allow users to make any interaction on random position by using LCD (Hokamp and Liu, 2017; Post and Vilar, 2018), algorithms in the decoding stage which can integrate lexical constraints into translation. However, LCD can not achieve a win-win of decoding speed and translation quality. Weng et al. (2019) propose a bidirectional IMT framework also based on LCD, which could fix minor mistakes left to the revisions by doing two constrained decoding processes with opposite directions in tandem. However, it needs to train two decoders, and in each constrained decoding process, the model can only use part of the constraints supplied by translators, making it inefficient both in using human knowledge and decoding speed. But BiTIIMT constructs all constraints into a template as part of the input, which makes it possible for models to use all human knowledge at the same time to fix minor mistakes automatically in the whole sentence.\nAnother series of works (Alkhouli et al., 2019; Song et al., 2020; Chen et al., 2021) apply alignment information to improve the decoding efficiency of LCD. Alkhouli et al. (2019) use alignment extracted by vanilla transformer, which is reported by Garg et al. is poor. Song et al. (2020) need an external aligner to train the alignment module. These works can only do constrained decoding based on a dictionary-style constraint pair, which means a burden for human translators."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Traditional IMT systems often use LCD to incorporate manually revised words into translations. In this paper, we propose BiTIIMT, a novel IMT method that outperforms LCD-based IMT in both translation quality and efficiency. The key to BiTIIMT is the bilingual text-infilling task which extends text-infilling from a monolingual setting to a bilingual one. We cast this task as a sequence-tosequence task and propose a simple yet effective solution to address it. Experiments show that BiTIIMT achieves a significantly improved efficiency in the area of IMT."
    } ],
    "references" : [ {
      "title" : "On The Alignment Problem In MultiHead Attention-Based Neural Machine Translation",
      "author" : [ "Tamer Alkhouli", "Gabriel Bretschner", "Hermann Ney." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, 1:177–185.",
      "citeRegEx" : "Alkhouli et al\\.,? 2019",
      "shortCiteRegEx" : "Alkhouli et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyung Hyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, pages 1–15.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical Approaches to Computer-Assisted Translation",
      "author" : [ "Sergio Barrachina", "Oliver Bender", "Francisco Casacuberta", "Jorge Civera", "Elsa Cubel", "Shahram Khadivi", "Antonio Lagarda", "Hermann Ney", "Jesús Tomás", "Enrique Vidal", "Juan-Miguel Vilar" ],
      "venue" : null,
      "citeRegEx" : "Barrachina et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Barrachina et al\\.",
      "year" : 2009
    }, {
      "title" : "Lexically Constrained Neural Machine Translation with Explicit Alignment Guidance",
      "author" : [ "Guanhua Chen", "Yun Chen", "Victor O K Li." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12630–12638.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "PRIMT: A pick-revise framework for interactive machine translation",
      "author" : [ "Shanbo Cheng", "Shujian Huang", "Huadong Chen", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Training Neural Machine Translation To Apply Terminology Constraints",
      "author" : [ "Georgiana Dinu", "Marcello Federico", "Yaser Al-onaizan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Dinu et al\\.,? 2019",
      "shortCiteRegEx" : "Dinu et al\\.",
      "year" : 2019
    }, {
      "title" : "MaskGAN: Better Text Generation via Filling in the",
      "author" : [ "William Fedus", "Ian Goodfellow", "Andrew M Dai." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Fedus et al\\.,? 2018",
      "shortCiteRegEx" : "Fedus et al\\.",
      "year" : 2018
    }, {
      "title" : "Target-Text Mediated Interactive Machine Translation",
      "author" : [ "George Foster", "Pierre Isabelle", "Pierre Plamondon." ],
      "venue" : "Machine Translation, 12(1):175–194.",
      "citeRegEx" : "Foster et al\\.,? 1997",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 1997
    }, {
      "title" : "Interactive machine translation using hierarchical translation models",
      "author" : [ "Jesús González-Rubio", "Daniel Ortiz-Martínez", "José Miguel Benedí", "Francisco Casacuberta." ],
      "venue" : "EMNLP 2013 - 2013 Conference on Empirical Methods in",
      "citeRegEx" : "González.Rubio et al\\.,? 2013",
      "shortCiteRegEx" : "González.Rubio et al\\.",
      "year" : 2013
    }, {
      "title" : "Lexically constrained decoding for sequence generation using grid beam search",
      "author" : [ "Chris Hokamp", "Qun Liu." ],
      "venue" : "ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers), vol-",
      "citeRegEx" : "Hokamp and Liu.,? 2017",
      "shortCiteRegEx" : "Hokamp and Liu.",
      "year" : 2017
    }, {
      "title" : "TranSmart: A Practical Interactive Machine Translation System",
      "author" : [ "Guoping Huang", "Lemao Liu", "Xing Wang", "Longyue Wang", "Huayang Li", "Zhaopeng Tu", "Chengyan Huang", "Shuming Shi." ],
      "venue" : "arXiv preprint arXiv:2105.13072, pages 1–21.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Adam- A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Lei Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980, pages 1–15.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Neural Interactive Translation Prediction",
      "author" : [ "Rebecca Knowles", "Philipp Koehn." ],
      "venue" : "Proceedings AMTA 2016: 12th Conference of the Association for Machine Translation in the Americas, 1:107– 120.",
      "citeRegEx" : "Knowles and Koehn.,? 2016",
      "shortCiteRegEx" : "Knowles and Koehn.",
      "year" : 2016
    }, {
      "title" : "TransType: a Computer-Aided Translation Typing System",
      "author" : [ "Philippe Langlais", "George Foster", "Guy Lapalme." ],
      "venue" : "ANLP-NAACL 2000 Workshop: Embedded Machine Translation Systems.",
      "citeRegEx" : "Langlais et al\\.,? 2000",
      "shortCiteRegEx" : "Langlais et al\\.",
      "year" : 2000
    }, {
      "title" : "Neural Machine Translation With Noisy Lexical Constraints",
      "author" : [ "Huayang Li", "Guoping Huang", "Deng Cai", "Lemao Liu." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, PP:1.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "A Fast, Extensible Toolkit for Sequence Modeling",
      "author" : [ "Nathan Ng", "David Grangier", "Michael Auli", "Sam Gross." ],
      "venue" : "arXiv preprint arXiv:1904.01038.",
      "citeRegEx" : "Ng et al\\.,? 2019",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2019
    }, {
      "title" : "BLEU : a Method for Automatic Evaluation of Machine Translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Weijing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, (July):311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Online Learning for Neural Machine Translation Post-editing",
      "author" : [ "Álvaro Peris", "Luis Cebrián", "Francisco Casacuberta." ],
      "venue" : "arXiv preprint arXiv:1706.03196, pages 1–12.",
      "citeRegEx" : "Peris et al\\.,? 2017a",
      "shortCiteRegEx" : "Peris et al\\.",
      "year" : 2017
    }, {
      "title" : "Interactive neural machine translation",
      "author" : [ "Álvaro Peris", "Miguel Domingo", "Francisco Casacuberta." ],
      "venue" : "Computer Speech & Language, 45:201–220.",
      "citeRegEx" : "Peris et al\\.,? 2017b",
      "shortCiteRegEx" : "Peris et al\\.",
      "year" : 2017
    }, {
      "title" : "Fast lexically constrained decoding with dynamic beam allocation for neural machine translation",
      "author" : [ "Matt Post", "David Vilar." ],
      "venue" : "NAACL HLT 2018 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Post and Vilar.,? 2018",
      "shortCiteRegEx" : "Post and Vilar.",
      "year" : 2018
    }, {
      "title" : "Interactive translation prediction versus conventional post-editing in practice: a study with the CasMaCat workbench",
      "author" : [ "Luis A Leiva", "Bartolomé Mesa-Lao", "Daniel OrtizMartínez", "Herve Saint-Amand", "Chara Tsoukala", "Enrique Vidal." ],
      "venue" : "Machine",
      "citeRegEx" : "Leiva et al\\.,? 2014",
      "shortCiteRegEx" : "Leiva et al\\.",
      "year" : 2014
    }, {
      "title" : "Rule-based translation with statistical phrase-based post-editing",
      "author" : [ "Michel Simard", "Nicola Ueffing", "Pierre Isabelle", "Roland Kuhn." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation, (June):203–206.",
      "citeRegEx" : "Simard et al\\.,? 2007",
      "shortCiteRegEx" : "Simard et al\\.",
      "year" : 2007
    }, {
      "title" : "Alignment-enhanced transformer for constraining NMT with pre-specified translations",
      "author" : [ "Kai Song", "Kun Wang", "Heng Yu", "Yue Zhang", "Zhongqiang Huang", "Weihua Luo", "Xiangyu Duan", "Min Zhang." ],
      "venue" : "AAAI 2020 - 34th AAAI Conference",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Code-switching for enhancing NMT with pre-specified translation",
      "author" : [ "Kai Song", "Yue Zhang", "Heng Yu", "Weihua Luo", "Kun Wang", "Min Zhang." ],
      "venue" : "NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, 2017-December(Nips):5999–",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Correct-andmemorize: Learning to translate from interactive revisions",
      "author" : [ "Rongxiang Weng", "Hao Zhou", "Shujian Huang", "Lei Li", "Yifan Xia", "Jiajun Chen." ],
      "venue" : "IJCAI International Joint Conference on Artificial Intelligence, 2019-August:5255–5263.",
      "citeRegEx" : "Weng et al\\.,? 2019",
      "shortCiteRegEx" : "Weng et al\\.",
      "year" : 2019
    }, {
      "title" : "Balancing quality and human involvement: An effective approach to interactive neural machine translation",
      "author" : [ "Tianxiang Zhao", "Lemao Liu", "Guoping Huang", "Zhaopeng Tu", "Huayang Li", "Yingling Liu", "Guiquan Liu", "Shuming Shi." ],
      "venue" : "AAAI 2020",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Text infilling",
      "author" : [ "Wanrong Zhu", "Zhiting Hu", "Eric P. Xing." ],
      "venue" : "arXiv preprint arXiv:1901.00158. 10",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Recent years have witnessed significant development in neural machine translation(NMT) (Bahdanau et al., 2015; Vaswani et al., 2017).",
      "startOffset" : 87,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "Recent years have witnessed significant development in neural machine translation(NMT) (Bahdanau et al., 2015; Vaswani et al., 2017).",
      "startOffset" : 87,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 21,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 8,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 4,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 26,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, interactive neural machine translation (IMT) (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016; Weng et al., 2019; Huang et al., 2021) is able to guarantee high-quality translation: it is an iterative collaboration process between human and machine that involves multiple interactive steps to obtain a satisfactory translation.",
      "startOffset" : 64,
      "endOffset" : 242
    }, {
      "referenceID" : 13,
      "context" : "Traditional IMT generates a translation in the left-to-right completing paradigm (Langlais et al., 2000; Sanchis-Trilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016; Zhao et al., 2020) where human translators are required to revise words in the translation prefix.",
      "startOffset" : 81,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "Traditional IMT generates a translation in the left-to-right completing paradigm (Langlais et al., 2000; Sanchis-Trilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016; Zhao et al., 2020) where human translators are required to revise words in the translation prefix.",
      "startOffset" : 81,
      "endOffset" : 199
    }, {
      "referenceID" : 12,
      "context" : "Traditional IMT generates a translation in the left-to-right completing paradigm (Langlais et al., 2000; Sanchis-Trilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016; Zhao et al., 2020) where human translators are required to revise words in the translation prefix.",
      "startOffset" : 81,
      "endOffset" : 199
    }, {
      "referenceID" : 27,
      "context" : "Traditional IMT generates a translation in the left-to-right completing paradigm (Langlais et al., 2000; Sanchis-Trilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016; Zhao et al., 2020) where human translators are required to revise words in the translation prefix.",
      "startOffset" : 81,
      "endOffset" : 199
    }, {
      "referenceID" : 26,
      "context" : "As a result, another part of works (Weng et al., 2019; Huang et al., 2021) propose an alternative IMT paradigm under which human translators can revise words at arbitrary po-",
      "startOffset" : 35,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "As a result, another part of works (Weng et al., 2019; Huang et al., 2021) propose an alternative IMT paradigm under which human translators can revise words at arbitrary po-",
      "startOffset" : 35,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "The essential technique to this paradigm is lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), which extends beam search in the decoding stage to include revised words as constraints.",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "The essential technique to this paradigm is lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), which extends beam search in the decoding stage to include revised words as constraints.",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 28,
      "context" : "The core idea to BiTIIMT is the Bilingual Text-infilling (BiTI) task which extends textinfilling (Zhu et al., 2019) from monolingual setting to bilingual setting and aims to fill in missing segments in a revised translation for a given source sentence.",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) is based on a sequence to sequence model which adopts an encoder-decoder architecture.",
      "startOffset" : 33,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) is based on a sequence to sequence model which adopts an encoder-decoder architecture.",
      "startOffset" : 33,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) is based on a sequence to sequence model which adopts an encoder-decoder architecture.",
      "startOffset" : 33,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Text infilling (Zhu et al., 2019) is a task that fills",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "missing text segments of a sentence or paragraph by a model (Berglund and Leo; Fedus et al., 2018; Zhu et al., 2019) trained on a large amount of data in a fill-in-the-blank format.",
      "startOffset" : 60,
      "endOffset" : 116
    }, {
      "referenceID" : 28,
      "context" : "missing text segments of a sentence or paragraph by a model (Berglund and Leo; Fedus et al., 2018; Zhu et al., 2019) trained on a large amount of data in a fill-in-the-blank format.",
      "startOffset" : 60,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "In general, the proposed BiTIIMT enjoys a humanin-the-loop manner to output the final translation, similar to conventional IMT systems (Cheng et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 155
    }, {
      "referenceID" : 28,
      "context" : "Problem Statement Generally, bilingual textinfilling extends text-infilling from the monolingual setting (Zhu et al., 2019) to the bilingual setting.",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 25,
      "context" : "To implement this model, it is possible to extend the Transformer (Vaswani et al., 2017) by using two encoders (i.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "Relation to Previous Work Our method is similar to previous works, including lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), MT with soft constraints",
      "startOffset" : 114,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "Relation to Previous Work Our method is similar to previous works, including lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), MT with soft constraints",
      "startOffset" : 114,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "(Dinu et al., 2019) and code-switch enhanced MT (Song et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : ", 2019) and code-switch enhanced MT (Song et al., 2019) in the sense that generating output in the condition of given constraints which is Ȳ in our work.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "For example, lexical constrained decoding (Hokamp and Liu, 2017) is more complex and less efficient than NMT decoding without constraints.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "Following previous works (Peris et al., 2017b; Cheng et al., 2016; Weng et al., 2019; Li et al., 2020), we experiment on two simulated scenarios and a real-world scenario.",
      "startOffset" : 25,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "Following previous works (Peris et al., 2017b; Cheng et al., 2016; Weng et al., 2019; Li et al., 2020), we experiment on two simulated scenarios and a real-world scenario.",
      "startOffset" : 25,
      "endOffset" : 102
    }, {
      "referenceID" : 26,
      "context" : "Following previous works (Peris et al., 2017b; Cheng et al., 2016; Weng et al., 2019; Li et al., 2020), we experiment on two simulated scenarios and a real-world scenario.",
      "startOffset" : 25,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Following previous works (Peris et al., 2017b; Cheng et al., 2016; Weng et al., 2019; Li et al., 2020), we experiment on two simulated scenarios and a real-world scenario.",
      "startOffset" : 25,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "We set Transformer (Vaswani et al., 2017) using fairseq (Ng et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 26,
      "context" : "Since LCD-based decoding is widely used in IMT scenario (Weng et al., 2019; Huang et al., 2021), we build an LCDbased IMT as our strong baseline.",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Since LCD-based decoding is widely used in IMT scenario (Weng et al., 2019; Huang et al., 2021), we build an LCDbased IMT as our strong baseline.",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "Because the original LCD algorithm is very slow, we implement its efficient version (Post and Vilar, 2018) which achieves comparable translation quality to the original version but 5x speedup in decoding.",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "We use the Adam Optimizer (Kingma and Ba, 2015) with β1 = 0.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "We employ BLEU (Papineni et al., 2002) to measure translation quality.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Human editing cost is a widely used cost (Cheng et al., 2016; Weng et al., 2019; Zhao et al., 2020; Huang et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "Human editing cost is a widely used cost (Cheng et al., 2016; Weng et al., 2019; Zhao et al., 2020; Huang et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : "Human editing cost is a widely used cost (Cheng et al., 2016; Weng et al., 2019; Zhao et al., 2020; Huang et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "Human editing cost is a widely used cost (Cheng et al., 2016; Weng et al., 2019; Zhao et al., 2020; Huang et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 315
    }, {
      "referenceID" : 13,
      "context" : "From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 315
    }, {
      "referenceID" : 21,
      "context" : "From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 315
    }, {
      "referenceID" : 2,
      "context" : "From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 315
    }, {
      "referenceID" : 8,
      "context" : "From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 315
    }, {
      "referenceID" : 4,
      "context" : "From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 315
    }, {
      "referenceID" : 1,
      "context" : "Recently, with the development of NMT (Bahdanau et al., 2015; Vaswani et al., 2017), researchers turned to employing IMT on it.",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "Recently, with the development of NMT (Bahdanau et al., 2015; Vaswani et al., 2017), researchers turned to employing IMT on it.",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "Generally, the text portion from the beginning to the current modified part is called prefix, and the system will generate a new translation based on the given prefix (SanchisTrilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016).",
      "startOffset" : 167,
      "endOffset" : 242
    }, {
      "referenceID" : 12,
      "context" : "Generally, the text portion from the beginning to the current modified part is called prefix, and the system will generate a new translation based on the given prefix (SanchisTrilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016).",
      "startOffset" : 167,
      "endOffset" : 242
    }, {
      "referenceID" : 9,
      "context" : "(2021) allow users to make any interaction on random position by using LCD (Hokamp and Liu, 2017; Post and Vilar, 2018), algorithms in the decoding stage which can integrate lexical constraints into translation.",
      "startOffset" : 75,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : "(2021) allow users to make any interaction on random position by using LCD (Hokamp and Liu, 2017; Post and Vilar, 2018), algorithms in the decoding stage which can integrate lexical constraints into translation.",
      "startOffset" : 75,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Another series of works (Alkhouli et al., 2019; Song et al., 2020; Chen et al., 2021) apply alignment information to improve the decoding efficiency of LCD.",
      "startOffset" : 24,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Another series of works (Alkhouli et al., 2019; Song et al., 2020; Chen et al., 2021) apply alignment information to improve the decoding efficiency of LCD.",
      "startOffset" : 24,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Another series of works (Alkhouli et al., 2019; Song et al., 2020; Chen et al., 2021) apply alignment information to improve the decoding efficiency of LCD.",
      "startOffset" : 24,
      "endOffset" : 85
    } ],
    "year" : 0,
    "abstractText" : "Interactive neural machine translation (INMT) is able to guarantee high-quality translations by taking human interactions into account. Existing IMT systems relying on lexical constrained decoding (LCD) enable humans to translate in a flexible translation manner beyond left-to-right. However, they typically suffer from limitations in translation efficiency and quality due to the reliance on LCD. In this work, we propose a novel BiTIIMT system, Bilingual Text-Infilling for Interactive Neural Machine Translation. The key idea to BiTIIMT is the Bilingual Text-infilling (BiTI) task which aims to fill in missing segments in a manually revised translation for a given source sentence. We propose a simple yet effective solution by casting this task as a sequence-tosequence task. In this way, our system performs efficient decoding without explicit constraints and makes full use of revised words for better translation prediction. Experiment results show that BiTiIMT performs significantly better and faster than state-of-the-art LCD-based IMT on three translation tasks.",
    "creator" : null
  }
}
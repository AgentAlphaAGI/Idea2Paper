{
  "name" : "ARR_2022_193_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "More Than Words: Collocation Retokenization for Latent Dirichlet Allocation Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nLatent Dirichlet allocation (LDA) models provide useful insights into themes and trends in a large text collection through the unsupervised inference of topics, or probability distributions over unigram word types in the corpus (Blei et al., 2003). Topics from these models are often interpreted based on their highest-probability words, with documents expressed as vectors of proportions of each topic. Unfortunately, the context in which these tokens arise can be obscured in the bag-of-words rendering of text as unigram counts in documents. For instance, a topic with high probabilities of both “coffee” and “table” is tempting to interpret as focusing on the furniture item “coffee table”, but both words could be frequent in a discussion of cafes containing no coffee tables. This problem is amplified in languages without marked word boundaries, such as Chinese and Thai: while existing tokenizers in these languages can segment characters into words, there is always a question about to what extent the tokenizers should group words together.\nWords that have been segmented by tokenizers may not express the concept of the original text if they were found as parts of collocations. Meaningful interpretation of topics can be lost without careful recombination of these words.\nWe hypothesize that the morphology of the language should play an important role in determining the suitable pre-processing steps that would improve the results of topic models. The main morphological types we consider are synthetic language and analytic language. Synthetic languages use many morphemes to compose a word and can be further divided into fusional and agglutinative languages. Fusional languages such as German differ from agglutinative languages such as Korean and Japanese: a single morpheme in fusional languages can code for many morphosyntactic features. On the other hand, analytic languages such as Thai and Chinese convey meanings by relating many words together, and morphological devices are more rarely used. Under our hypothesis, analytic languages should benefit from token merging, but synthetic languages might not because the meaning is conveyed by inflection (through bound morphemes) and agglutination (through free morphemes).\nIn this project, we investigate the effects of token merging as a pre-processing step, and study how those effects vary based on the writing systems and the morphological features of the languages. We evaluate three measures to determine when to merge multiple adjacent words into conceptuallyunified phrasal tokens prior to LDA model training: chi-squared statistics, t-statistics, and raw frequency counts of phrases. We test these merging strategies on English, German, Chinese, Japanese, Korean, Thai, and Arabic. This set of languages are drawn from various writing systems and different morphological typology to see which type of language favors which type of merging strategy.\nThe main contributions of this paper are as fol-\nlows:\n• We determine through empirical studies that a t-statistic and raw-frequency approach to token merging improves the topic modeling results across all language types and writing systems for the corpora that do not differ much from the collocation training data.\n• We also show the positive consequences of token merging: the percentage of merged tokens in the LDA training data is correlated with the quality of the topic modeling results.\n• Finally, we provide evidence that the popular approach of applying a χ2measure to token merging tends to overfit to the collocation training data and result in a low percentage of merged tokens in a number of languages, making it a less suitable general-purpose approach than t-statistics.\n2 Related Work\nPre-processing steps can substantially alter the results of the LDA models even in languages with good tokenization heuristics such as English (Schofield and Mimno, 2016; May et al., 2016). We believe that languages that do not have clear tokenization standards deserve investigation into what kind of processing is appropriate. Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012; Lau et al., 2013; Yu et al., 2013; El-Kishky et al., 2014; Wang et al., 2016; Bin et al., 2018; Li et al., 2018). We consider it valuable to specifically assess approaches to determining these phrases.\nDespite their popularity in analyzing large amounts of text data, LDA models are notoriously complex to evaluate. One must evaluate both the statistical fit of a model and the human-registered thematic coherence of the words found to arise in the high-probability words, or keys, of a topic, which may not correlate (Chang et al., 2009). Analyses often combine evaluations of fit (Wallach et al., 2009) and automated approximations of human judgments of coherence (Bouma, 2009; Mimno et al., 2011) based on mutual information, even with the expectation these may only somewhat correlate with true human judgments (Lau et al., 2014). A limitation of these existing approaches, however, is that they expect the vocabulary and tokenization to remain constant between two models. For our\nevaluation, we use a normalized log likelihood approach to capture fit while accounting for changes in vocabulary (Schofield and Mimno, 2016)."
    }, {
      "heading" : "3 Collocations as LDA Token",
      "text" : "Collocations consist of two or more words that express conventional meaning, which can convey information about multi-word entities, context, and word usage. We hypothesize that the introduction of multi-word tokens, which capture collocations as bigrams or trigrams by way of concatenation of adjacent tokens, can help achieve more useful and coherent topic models. For languages without clear word boundaries, there is a possible additional benefit to multi-word tokens: it can be hard to intuit whether inferred word boundaries will have a large impact on the final results. Merging adjacent words into ‘multi-word’ tokens may help remedy the potential problem of a segmentation that is not optimal for topic modeling purposes.\nMany methods are possible to select collocations to merge from tokenized text (Manning and Schutze, 1999). In this paper, we evaluate the chisquared statistics (χ2), the t-statistic and raw frequency as approaches to develop a threshold for merging collocations into multi-word tokens prior to topic model training. The chi-squared measure χ2(w1, w2) and t(w1, w2) t-statistic for two adjacent tokens w1 and w2 are defined as:\nχ2(w1, w2) = (P (w1, w2)− P (w1)P (w2))2\nP (w1)P (w2) (1) 160\nt(w1, w2) = x̄− µ\ns2 N\n161\n≈ P (w1, w2)− P (w1)P (w2)√ P (w1,w2)\nN\n(2) 162\nWe first compute the collocation measures for 163 all bigrams on a large collocation training corpus. 164 Then we select the top bigrams that score the high- 165 est on the collocation measures and add those to 166 our lexicon. After we tokenize and pre-process the 167 collection of documents on which we would like 168 to train LDA, we retokenize the data based on the 169 collocation training corpus. We find all of the bi- 170 grams in the LDA training data that are also found 171 in the top bigram lexicons that we obtain from the 172 collocation training corpus. Then, the LDA train- 173 ing process proceeds as usual but with some of the 174\noriginal tokens merged into a multi-word tokens as175 defined from the collocation training data.176\n4 Evaluation Metrics177\nWe consider two primary evaluation metrics for178 exploring the effect of merging tokens: one based179 on log likelihood, and one based on silhouette co-180 efficients.181\nHeld-Out Likelihood. When multi-word182 phrases are converted to individual tokens, the183 number of tokens in the document decreases while184 the size of the corpus vocabulary increases. It is185 therefore illogical to compare the likelihoods of186 the word-token model and collocation-token model187 directly. In order to normalize the scores between188 the two models that do not have the exact same189 vocabulary and tokens, we use the log-likelihood190 ratio between the LDA model likelihood and the191 null (unigram) likelihood for each model. In other192 words, we normalize the LDA model likelihood193 (Lmodel) by dividing it with the unigram likeli-194 hood (Lunigram) as introduced by Schofield and195 Mimno (2016). Therefore, the normalized loglike-196 lihood per token (PTLLnorm) is197\nPTLLnorm = logLmodel − logLunigram\nN (3)198\nwhere N is the number of tokens. Since likelihood199 per token has been normalized by the unigram like-200 lihood per token, the higher the PTLL, the better201 the model.202\nConcatenation-based Embedding Silhouette203 (CBES) Previous measures of topic coherence rely204 on statistics from the training data and assume205 that the vocabularies are identical for both models,206 which is not the case for our settings. To address207 this, we propose a new application of the silhouette208 coefficients (Rousseeuw, 1987), a common cluster-209 ing evaluation metric to measure topic coherence.210\nA good topic should have all of its topic keys211 close to each other and away from other words that212 do not belong in the same topic. Therefore, the213 word embeddings of these topic keys should have214 shorter cosine distances within the same topic, and215 longer distances to the topic keys in other topics.216 When words are represented as a vector, this is217 exactly what the silhouette coefficients measure.218 To compute them, we first compute the a(i), which219 is the mean cosine distance between topic-key i220\nand other topic-keys in the same topic. 221\na(i) = 1 | Ci | −1 ∑\nj∈Ci,i 6=j d(i, j) (4) 222\nwhere d(i, j) is the distance between ith and jth 223 topic-key. Then for each other topic, we compute 224 the mean of the distance of topic-key i to topic- 225 keys in that other topic. And b(i) is the smallest of 226 such mean among other topics. 227\nb(i) = min k 6=i\n1 | Ck | ∑ j∈Ck d(i, j) (5) 228\nAfter obtaining a(i) and b(i), the silhouette coeffi- 229 cient for topic-key i is defined as: 230\ns(i) = b(i)− a(i)\nmax(a(i), b(i)) , if | Ci |> 1 (6) 231\nand 232 s(i) = 0, if | Ci |= 1 (7) 233\nThe silhouette coefficient for the entire model is the 234 average s(i) over all i. The larger silhouette coef- 235 ficient means that topic-keys are relatively similar 236 within its topic and different from other topics. 237\nIn order to compare the distances among words 238 merged by different criteria, all compared word em- 239 beddings must be in the same space. Since merged 240 tokens will modify the vocabulary of the corpus, 241 we create four versions of the word embedding 242 training corpus: the original version and the three 243 other versions where tokens are merged based on 244 χ2, t and frequency collocation measures. We train 245 the word embeddings on these four versions of the 246 corpus so we can then compare word embeddings 247 on a consistent vocabulary in each retokenization 248 scheme. 249\n5 Experiments 250\nWe hypothesize that the morphology should play an 251 important role in determining the suitable prepro- 252 cessing steps. We test our methods on one fusional 253 language (German), two agglutinative languages 254 (Japanese and Korean), three analytic languages 255 (Chinese, Thai, and Arabic), and English, which 256 can be thought of as either analytic or fusional. 257 These languages also represent languages drawn 258 from all writing systems: logograms (Chinese), syl- 259 labic system (Japanese), featural system (Korean), 260 abugida (Thai), abjad (Arabic), and true alphabets 261 (English and German). 262\nThe English corpora are drawn from The New263 York Times (Sandhaus, 2008), the Yelp Dataset1,264 and United States State of the Union addresses265 (1790 to 2018) divided into paragraphs2. The266 German data come from Ten Thousand German267 News Articles Dataset3. The Chinese data come268 from three corpora: the news articles from Chi-269 nanews4, restaurant reviews from Dianping5, and270 the movie reviews from Douban6. The Japanese271 data is from the Webhose’s Free Datasets7. The Ko-272 rean data come from the KAIST Corpus8. The Thai273 data come from the news articles in Prachathai9,274 the restaurant reviews from Wongnai10, the BEST275 corpus11, and the Thai National Corpus (Aroon-276 manakun, 2007). The Arabic data come from the277 Antcorpus (Chouigui et al., 2017). Each corpus is278 separated into 75% training documents and 25%279 test documents (Table 1).280 We train the χ2, t, and frequency-based tokeniz-281 ers for each language on Wikipedia articles for that282 language. For all languages, we use the reduced283 version of Wikipedia database, except for English284 we use the filtered Wiki103 dataset (Merity et al.,285 2016). English, German, Chinese, Japanese, Ko-286 rean, Thai and Arabic documents are tokenized287 with NLTK (Bird, 2006), SoMaJo (Proisl and288\n1www.yelp.com/dataset 2www.kaggle.com/rtatman/state-of-the-union-corpus-\n1989-2017 3github.com/tblock/10kGNAD 4www.chinanews.com 5github.com/zhangxiangxiao/glyph 6www.kaggle.com/utmhikari/doubanmovieshortcomments 7webhose.io/free-datasets/japanese-news-articles/ 8semanticweb.kaist.ac.kr/home/index.php/KAIST Corpus 9github.com/PyThaiNLP/prachathai-67k\n10www.kaggle.com/c/wongnai-challenge-review-ratingprediction\n11thailang.nectec.or.th/downloadcenter\nUhrig, 2016), Stanford Word Segmenter (Tseng 289 et al., 2005), Fugashi (McCann, 2020), KoNLPy 290 (Park and Cho, 2014), Attacut (Chormai et al., 291 2020) and Camel-tools (Obeid et al., 2020) respec- 292 tively. For each criterion, we create a list of 50,000 293 top bigrams that have the highest scores. These 294 lists of top bigrams will be used to merge words 295 in the input of the LDA, effectively training a new 296 tokenizer. 297\nTo train word embeddings, we use the gensim 298 (Řehůřek and Sojka, 2010) implementation with 299 the Continuous Bag-of-Word (CBOW) algorithm 300 (Mikolov et al., 2013) to obtain word embeddings. 301 The training corpora and their collocation versions 302 are prepared based on the tokenizers that we dis- 303 cuss above. We preprocess the word embedding 304 training data and the LDA training data the same 305 way. For English, we lemmatize and lowercase the 306 data. For Korean, Japanese, and Arabic, we lem- 307 matize the data. For German, Chinese, and Thai, 308 we do not do any normalization. 309\nWe use MALLET (McCallum, 2002) implemen- 310 tation of LDA with the default hyperparameters 311 to train and evaluate topic models in both word 312 and multi-word (collocation) documents with 10, 313 50, 100 topics. We run the experiment 3 times for 314 each combination of corpus, type of retokenization 315 (no retokenization, χ2, t or frequency) and number 316 of topics to compute the means of the normalized 317 held-out likelihood and CBES, discussed in section 318 4. 319\n6 Results and Discussion 320\nThe normalized log-likelihood per token of the t 321 and frequency-based retokenization is significantly 322 higher than the baseline for English, German, Chi- 323 nese, Japanese, Korean, and Arabic for all text col- 324 lections and the number of topics except EN-Yelp, 325 TH-BEST, and TH-TNC (Table 3 ). Frequency- 326\nbased retokenization gives the best results for most327 settings but not significantly higher than t retok-328 enization. However, we observe mixed results from329 χ2retokenization for some languages. This is quite330 surprising because raw frequency was previously331 found to be an inferior measure of collocation. This332 suggests that t and frequency-based retokenization333 might be a more reliable method for improving the334 goodness of fit of the LDA model. This also sug-335 gests that Japanese and Korean might have some336 specific quality that interacts well with all three337 types of retokenization.338\nSimilarly, we observe a general improvement in339 coherence for the t and frequency retokenization340 (Table 3). The higher CBES score indicates that341 topic-keys are more semantically coherent and top-342 ics are more distinct. The coherence improves after343 t and frequency-based retokenization for English,344 Japanese, Korean, and Arabic corpora regardless of345 the number of topics. The improvement for Thai is346\nspotty, and Chinanews is the only Chinese corpus 347 in which we see improvement. This suggests that 348 the choice of retokenization strategy might depend 349 on the language types or the content of corpora it- 350 self. Consistent with the normalized log-likelihood 351 results, Japanese and Korean corpora interact well 352 with all three types of retokenization, suggesting 353 that the morphology or typology of these two lan- 354 guages consistently benefit from collocation before 355 training LDA models. 356\nWhat could account for this discrepancy across 357 languages and corpora? First, we observe a large 358 variation of percentages of merged tokens across 359 corpora. Because we fix the number of bigrams 360 types to merge during the tokenizer training pro- 361 cess to 50,000 for all three criteria (Table 1), we 362 can use this analysis to find trends in the relative 363 frequency of merged tokens. We see that χ2 retok- 364 enizer only merges barely 1% of all the tokens be- 365 fore training the LDA models for English, Chinese, 366\nGerman, Arabic, and Thai corpora, possibly intro-367 ducing noise in the data that yield the results sim-368 ilar to or worse than the baseline. In contrast, the369 t and frequency-based retokenizers merge around370 8%- 15% of all the tokens for English, German,371 and Chinese. Arabic has seen the highest merging372 percentage of 26%-27%. Notably, around 20 %373 of tokens are retokenized by all three retokeniz-374 ers in Japanese and Korean. The truncation of the375 top χ2bigrams list might cause this different be-376 havior. The number of χ2collocations that pass377 the hypothesis testing are significantly larger than378 that of t collocations. For example, there are 3.73379 million χ2collocations versus 231 thousand t col-380 locations in Thai for the same significance level381 α = 0.005. This full list of χ2collocations in-382 cludes all the top collocations from the t score and383 frequency treatments, implying that were we to384 use this significance threshold, the percentage of385 merged word would be at least as high as the two386 methods. However, the large vocabulary that the387 χ2approach induces is impractical in many appli-388 cations, suggesting it is an inefficient approach if389 the goal is primarily to merge frequent ngrams.390\nAnother possible effect these results may show391 is that the writing system or the morphology could392 account for this notable discrepancy in retokeniza-393 tion percentage across languages. For English,394 the top 20 χ2collocations are primarily specific395\nnamed entities, but the t and frequency-based reto- 396 kenizers yield more general compound nouns and 397 common phrases (Figure 1). As the top 50,000 398 χ2collocations contains primarily rare words, these 399 are expected to co-occur rarely enough that even a 400 few co-occurrences can trigger significance. There- 401 fore, when we use this truncated list of rarely- 402 occurring χ2collocations, we generally see very 403 low merged token percentage. 404\nThe quality of retokenization impacts both the 405 goodness of fit the model, as indicated by the nor- 406 malized log-likelihood score, and the coherence of 407 the model, as indicated by the CBES score. Within 408 the same language, news corpora have higher per- 409 centages of merged words when merged with t and 410 frequency collocations, while corpora containing 411 restaurant and movie reviews tend to see lower 412 percentages (Table 1). This could be because the 413 news corpora are in a similar domain to that of 414 the Wikipedia which we use to build the list of 415 co-occurring words. A good retokenizer (in our 416 cases, trained on Wikipedia data) should gener- 417 alize well and recognize many collocations in a 418 new corpus, which differs somewhat from the re- 419 tokenizer training data. We found a significant 420 positive correlation between merge percentage and 421 the margin of improvement over the baseline (the 422 difference between the PTLL of the model without 423 retokenization and the PTLL or CBES of the model 424\nwith retokenization). Pooling across all languages425 and corpora, we found the correlation coefficients426 of 0.41, 0.77, and 0.68 for the models with 10, 50,427 and 100 topics respectively for PTLL. As for the428 coherence metric, we found the correlation coeffi-429 cients of 0.73, 0.76, and 0.79 for the models with430 10, 50, and 100 topics respectively for CBES. This431 means the models with higher merge percentage432 are better than their corresponding word models433 in reproducing the statistics of the held-out data.434 This suggests that the quality of the LDA models435 depends on the generalizability of the retokenizers.436\nThe LDA model results become more under-437 standable when certain tokens are retokenized. We438 see merged tokens in the topic key sets of almost439 all topics in all corpora when retokenized based440 on t or raw frequency. Many of these represent441 non-compositional meanings that might have been442 lost without retokenization: for example, the col-443 location “social security” is not fully represented444 by the individual tokens “social” or “security” sep-445 arately. More strikingly, the collocation ‘kōn sǔa446 dāng’ refers to a political movement group in Thai-447 land. When it is separated into kōn (people) sǔa448 (shirt) dāng (red), the key meaning is totally lost.449 When we compare by looking at the topic-keys of450 the word and multi-word models, we can come up451 with similar topics because we as a human who452 understands English and have general knowledge453 of the world can make the connection based on454 surrounding topic-keys even though they are not455 explicitly merged. However, if we want to use these456 topic keys as input to other downstream tasks such457 as information retrieval or text classification, the458 merged tokens help retain the specificity of the “red459 shirt people” as a meaningful entity distinct from460 the phrase’s constituting parts.461\n7 Conclusion462\nIn this work, we improve the quality of LDA mod-463 els by better processing the input text before train-464 ing the model. We found that the retokenizers465 trained based on t statistics and raw frequency yield466 an improvement across all languages considered in467 this study, while the χ2approach was a less efficient468 approach that focuses more on rare named entities469 than common noun phrases. Using retokenizers470 ensures that LDA models can fit better to the data,471 the topic keys are more coherent, and the topics are472 more distinct. Outputs from retokenization with t473 statistics and frequency approaches yield common474\nnoun phrases in the most frequent terms of topics 475 that represent a significant aid to both direct topic 476 interpretation and expected utility of these topics 477 in downstream tasks. 478\nReferences 479 Wirote Aroonmanakun. 2007. Creating the thai na- 480\ntional corpus. MANUSYA: Journal of Humanities, 481 10(3):4–17. 482\nGE Bin, Chun-hui HE, Sheng-ze HU, and GUO Cheng. 483 2018. Chinese news hot subtopic discovery and rec- 484 ommendation method based on key phrase and the 485 lda model. DEStech Transactions on Engineering 486 and Technology Research, (ecar). 487\nSteven Bird. 2006. Nltk: the natural language toolkit. 488 In Proceedings of the COLING/ACL 2006 Interac- 489 tive Presentation Sessions, pages 69–72. 490\nDavid M Blei, Andrew Y Ng, and Michael I Jordan. 491 2003. Latent dirichlet allocation. Journal of ma- 492 chine Learning research, 3(Jan):993–1022. 493\nGerlof Bouma. 2009. Normalized (pointwise) mutual 494 information in collocation extraction. Proceedings 495 of GSCL, pages 31–40. 496\nJonathan Chang, Sean Gerrish, Chong Wang, Jordan 497 Boyd-graber, and David Blei. 2009. Reading tea 498 leaves: How humans interpret topic models. In Ad- 499 vances in Neural Information Processing Systems, 500 volume 22. Curran Associates, Inc. 501\nPattarawat Chormai, Ponrawee Prasertsom, Jin Chee- 502 vaprawatdomrong, and Attapol Rutherford. 2020. 503 Syllable-based neural thai word segmentation. In 504 Proceedings of the 28th International Conference on 505 Computational Linguistics, pages 4619–4637. 506\nAmina Chouigui, Oussama Ben Khiroun, and Bilel 507 Elayeb. 2017. Ant corpus: an arabic news text col- 508 lection for textual classification. In 2017 IEEE/ACS 509 14th International Conference on Computer Systems 510 and Applications (AICCSA), pages 135–142. IEEE. 511\nAhmed El-Kishky, Yanglei Song, Chi Wang, Clare 512 Voss, and Jiawei Han. 2014. Scalable topical 513 phrase mining from text corpora. arXiv preprint 514 arXiv:1406.6312. 515\nJey Han Lau, Timothy Baldwin, and David Newman. 516 2013. On collocations and topic models. ACM 517 Transactions on Speech and Language Processing 518 (TSLP), 10(3):1–14. 519\nJey Han Lau, David Newman, and Timothy Baldwin. 520 2014. Machine reading tea leaves: Automatically 521 evaluating topic coherence and topic model quality. 522 In Proceedings of the 14th Conference of the Euro- 523 pean Chapter of the Association for Computational 524 Linguistics, pages 530–539. 525\nBing Li, Xiaochun Yang, Rui Zhou, Bin Wang,526 Chengfei Liu, and Yanchun Zhang. 2018. An effi-527 cient method for high quality and cohesive topical528 phrase mining. IEEE Transactions on Knowledge529 and Data Engineering, 31(1):120–137.530\nRobert Lindsey, William Headden, and Michael531 Stipicevic. 2012. A phrase-discovering topic model532 using hierarchical pitman-yor processes. In Pro-533 ceedings of the 2012 Joint Conference on Empirical534 Methods in Natural Language Processing and Com-535 putational Natural Language Learning, pages 214–536 222.537\nChristopher Manning and Hinrich Schutze. 1999.538 Foundations of statistical natural language process-539 ing. MIT press.540\nChandler May, Ryan Cotterell, and Benjamin541 Van Durme. 2016. An analysis of lemmatization542 on topic models of morphologically rich language.543 arXiv preprint arXiv:1608.03995.544\nAndrew Kachites McCallum. 2002. Mallet:545 A machine learning for language toolkit.546 Http://mallet.cs.umass.edu.547\nPaul McCann. 2020. fugashi, a tool for tokenizing548 Japanese in python. In Proceedings of Second Work-549 shop for NLP Open Source Software (NLP-OSS),550 pages 44–51, Online. Association for Computational551 Linguistics.552\nStephen Merity, Caiming Xiong, James Bradbury, and553 Richard Socher. 2016. Pointer sentinel mixture mod-554 els.555\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-556 frey Dean. 2013. Efficient estimation of word557 representations in vector space. arXiv preprint558 arXiv:1301.3781.559\nDavid Mimno, Hanna Wallach, Edmund Talley,560 Miriam Leenders, and Andrew McCallum. 2011.561 Optimizing semantic coherence in topic models. In562 Proceedings of the 2011 Conference on Empirical563 Methods in Natural Language Processing, pages564 262–272.565\nOssama Obeid, Nasser Zalmout, Salam Khalifa, Dima566 Taji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl567 Eryani, Alexander Erdmann, and Nizar Habash.568 2020. CAMeL tools: An open source python toolkit569 for Arabic natural language processing. In Proceed-570 ings of the 12th Language Resources and Evaluation571 Conference, pages 7022–7032, Marseille, France.572 European Language Resources Association.573\nEunjeong L. Park and Sungzoon Cho. 2014. Konlpy:574 Korean natural language processing in python. In575 Proceedings of the 26th Annual Conference on Hu-576 man & Cognitive Language Technology, Chuncheon,577 Korea.578\nThomas Proisl and Peter Uhrig. 2016. SoMaJo: State- 579 of-the-art tokenization for German web and social 580 media texts. In Proceedings of the 10th Web as Cor- 581 pus Workshop (WAC-X) and the EmpiriST Shared 582 Task, pages 57–62, Berlin. Association for Compu- 583 tational Linguistics (ACL). 584\nRadim Řehůřek and Petr Sojka. 2010. Software Frame- 585 work for Topic Modelling with Large Corpora. In 586 Proceedings of the LREC 2010 Workshop on New 587 Challenges for NLP Frameworks, pages 45–50, Val- 588 letta, Malta. ELRA. 589\nPeter J Rousseeuw. 1987. Silhouettes: a graphical aid 590 to the interpretation and validation of cluster anal- 591 ysis. Journal of computational and applied mathe- 592 matics, 20:53–65. 593\nEvan Sandhaus. 2008. The new york times annotated 594 corpus. Linguistic Data Consortium, Philadelphia, 595 6(12):e26752. 596\nAlexandra Schofield and David Mimno. 2016. Com- 597 paring apples to apple: The effects of stemmers on 598 topic models. Transactions of the Association for 599 Computational Linguistics, 4:287–300. 600\nHuihsin Tseng, Pi-Chuan Chang, Galen Andrew, Dan 601 Jurafsky, and Christopher D Manning. 2005. A 602 conditional random field word segmenter for sighan 603 bakeoff 2005. In Proceedings of the fourth SIGHAN 604 workshop on Chinese language Processing. 605\nHanna M. Wallach, Iain Murray, Ruslan Salakhutdi- 606 nov, and David Mimno. 2009. Evaluation methods 607 for topic models. In Proceedings of the 26th An- 608 nual International Conference on Machine Learn- 609 ing, ICML ’09, page 1105–1112, New York, NY, 610 USA. Association for Computing Machinery. 611\nMinmei Wang, Bo Zhao, and Yihua Huang. 2016. 612 Ptr: phrase-based topical ranking for automatic 613 keyphrase extraction in scientific publications. In In- 614 ternational Conference on Neural Information Pro- 615 cessing, pages 120–128. Springer. 616\nZhiguo Yu, Todd R Johnson, and Ramakanth Kavuluru. 617 2013. Phrase based topic modeling for semantic in- 618 formation processing in biomedicine. In 2013 12th 619 International Conference on Machine Learning and 620 Applications, volume 1, pages 440–445. IEEE. 621"
    } ],
    "references" : [ {
      "title" : "Nltk: the natural language toolkit",
      "author" : [ "Steven Bird" ],
      "venue" : null,
      "citeRegEx" : "Bird.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bird.",
      "year" : 2006
    }, {
      "title" : "Normalized (pointwise) mutual",
      "author" : [ "Gerlof Bouma" ],
      "venue" : null,
      "citeRegEx" : "Bouma.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bouma.",
      "year" : 2009
    }, {
      "title" : "Reading tea",
      "author" : [ "Boyd-graber", "David Blei" ],
      "venue" : null,
      "citeRegEx" : "Boyd.graber and Blei.,? \\Q2009\\E",
      "shortCiteRegEx" : "Boyd.graber and Blei.",
      "year" : 2009
    }, {
      "title" : "An analysis of lemmatization",
      "author" : [ "Van Durme" ],
      "venue" : null,
      "citeRegEx" : "Durme.,? \\Q2016\\E",
      "shortCiteRegEx" : "Durme.",
      "year" : 2016
    }, {
      "title" : "fugashi, a tool for tokenizing",
      "author" : [ "Paul McCann" ],
      "venue" : null,
      "citeRegEx" : "McCann.,? \\Q2020\\E",
      "shortCiteRegEx" : "McCann.",
      "year" : 2020
    }, {
      "title" : "2016. Pointer sentinel mixture",
      "author" : [ "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Socher.,? \\Q2016\\E",
      "shortCiteRegEx" : "Socher.",
      "year" : 2016
    }, {
      "title" : "The new york times annotated",
      "author" : [ "Evan Sandhaus" ],
      "venue" : null,
      "citeRegEx" : "Sandhaus.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sandhaus.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", 2009) and automated approximations of human judgments of coherence (Bouma, 2009; Mimno et al., 2011) based on mutual information, even with the expectation these may only somewhat correlate with true human judgments (Lau et al.",
      "startOffset" : 69,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "The English corpora are drawn from The New 263 York Times (Sandhaus, 2008), the Yelp Dataset1, 264 and United States State of the Union addresses 265 (1790 to 2018) divided into paragraphs2.",
      "startOffset" : 58,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "English, German, Chinese, Japanese, Ko286 rean, Thai and Arabic documents are tokenized 287 with NLTK (Bird, 2006), SoMaJo (Proisl and 288",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : ", 2005), Fugashi (McCann, 2020), KoNLPy 290 (Park and Cho, 2014), Attacut (Chormai et al.",
      "startOffset" : 17,
      "endOffset" : 31
    } ],
    "year" : 0,
    "abstractText" : "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, tstatistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_3_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Visual Commonsense in Pretrained Unimodal and Multimodal Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The observation that human language understanding happens in a rich multimodal environment has led to an increased focus on visual grounding in natural language processing (NLP) (Bisk et al., 2020; Baltrusaitis et al., 2019), driving comparisons between traditional unimodal text-only models and multimodal models which take both text and image inputs. In this work, we explore to what extent unimodal and multimodal models are able to capture commonsense visual concepts across five types of relations: color, shape, material, size, and visual co-occurrence (see Fig. 1). We further explore how such an ability is influenced by the reporting bias (defined in Section 2.3) in training data. We measure visual commonsense, defined as knowledge\nabout visual properties which humans are implicitly aware of even without explicit visual cues, through frequency distributions. A visually aware language model should be able to capture such properties upon elicitation. The color, shape, material, and co-occurrence data are mined from Visual Genome (Krishna et al., 2016), and the size data are created from object lists.\nPaik et al. (2021) evaluate language models’ color perception using a human-annotated color dataset (CoDa), finding that reporting bias negatively influences model performance and that multimodal training can mitigate those effects. In this work, we confirm those findings while extending the evaluation to a broader range of visually salient properties, resulting in a more comprehensive metric for visual commonsense. In order to elicit visual commonsense from language models, we utilize soft prompt tuning (Qin and Eisner, 2021), which trains optimal templates by gradient descent for each model and relation type that we explore. We also utilize knowledge distillation to enhance a textonly model’s visual commonsense ability, where the vision-language model serves as the teacher.\nThe major contributions of this work are: (1) we design a comprehensive analytic dataset for probing English visual commonsense that is applicable to any language model; (2) apply the dataset to study the capacity of unimodal language models and multimodal vision-language (VL) models to capture empirical distributions of visually salient properties; and (3) train a knowledge-distilled version of a VL model that achieves improved performance on our task. The dataset and code will be made available at http://anonymous_url."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Vision-Language Modeling",
      "text" : "Recent advances in vision-language modeling have achieved great success. Most of them learn joint\nimage and text representations from cross-modal training of transformers with self-attention, including LXMERT (Tan and Bansal, 2019), ViLBERT (Lu et al., 2019), UNITER (Chen et al., 2020), etc. Oscar (Li et al., 2020) additionally uses object tags in images as anchor points to ease the learning of image-text alignments and VinVL (Zhang et al., 2021) presents an improved object detection model. CLIP (Radford et al., 2021) learns by predicting caption-image alignment from a large internet corpus of (image, text) pairs.\nWhile our work uses textual prompt tuning techniques, there have also been work on visual prompt engineering to enhance the performance of pretrained vision-language models. Zhou et al. (2021) model context in prompts as continuous representations and learn to optimize that context. Yao et al. (2021) develop a cross-modal prompt tuning framework that reformulates visual grounding as a fill-in-the-blank problem for both image and text."
    }, {
      "heading" : "2.2 Visual Commonsense",
      "text" : "In one of the early attempts at learning visual commonsense, Vedantam et al. (2015) measure the plausibility of a commonsense assertion in the form of (obj1, relation, obj2) based on how similar it is to known plausible assertions, using both visual scenes and accompanying text. Zellers et al. (2021) learn physical commonsense through interaction, and uses this knowledge to ground language. Frank et al. (2021) probe whether vision-language models\nhave learned to construct cross-modal representations using both modalities via cross-modal input ablation.\nNote that our definition of visual commonsense differs from that of Zellers et al. (2019), where the model is required to perform commonsense reasoning based on an image. Our idea of visual commonsense is more similar to the idea of stereotypic tacit assumptions (Prince, 1978) – the propositional beliefs that humans hold about generic concepts, such as “dogs have to be walked.” Weir et al. (2020) probe neural language models for such human tacit assumptions and demonstrate the models’ success. We extend this intuition to visual concepts and explore how visual information may help language models to capture such assumptions.\nZhu et al. (2020) investigate the “language prior” problem in Visual Question Answering models, where models tend to answer questions based on word frequencies in the data and ignore the image contents. In this work, we explore to what extent such language prior is correct when there is no image input."
    }, {
      "heading" : "2.3 Reporting Bias",
      "text" : "Pretrained language models such as BERT (Devlin et al., 2019) are trained on millions of tokens of text, capturing statistical regularities present in the training corpora. However, their textual training data can suffer from reporting bias, where the frequency distribution of specific events and properties in text\nmay not reflect the real-world distribution of such properties (Gordon and Van Durme, 2013). For example, while grass is most commonly green, this may not be reported as much in web corpora, and while motorcycle crashes may be more common in the real world, plane crashes may be mentioned far more in news text.\nMisra et al. (2016) shows that “human-centric” image annotations contain reporting bias as well and that the noise in annotations exhibits structure and can be modeled."
    }, {
      "heading" : "3 Datasets",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset Mining",
      "text" : "Our data take the form of (subject, object) tuples for each relation, with the goal being to predict the object (and its distribution) from the subject and relation. Relations include color, shape, material, size, and object co-occurrence. Table 1 summarizes the number of classes and subject-object pairs for each relation. 1\nColor, Shape, Material For color, shape, and material, the subject is a noun and the object is the color, shape, or material property of the noun, mined from Visual Genome (VG) (Krishna et al., 2016) 2 attributes. We manually create a list of single-word attributes for each relation, and only VG subjects that are matched with a specific attribute for more than a threshold number of times are recorded, in order to avoid noise in the dataset. The thresholds for color, material, and shape are 5, 2, and 1, respectively, chosen based on the availability of attributes of each relation in VG. VG attributes are filtered with the following steps: (1) attribute “Y colored / made / shaped” is treated as “Y”; (2) select only the last word for compound attributes (e.x. treat “forest green” as “green”); (3) similar attributes are merged into one of the main attribute classes (e.x. “maroon” and “crimson” are merged into “red”).\nThe above procedure produces a distribution over the set of attributes for each subject noun. From that distribution, a (subject, object) data instance is generated for each subject where the object is the attribute that associates with it the most. See the first three rows of Table 1 for examples.\n1See Appendix A.1 for more information on the object classes.\n2Licensed under CC-BY 4.0.\nSize Size is separated into size_smaller and size_larger, where the subject is a noun and the object is another noun that is smaller or larger, respectively, than the subject. To form the size dataset, we obtain a set of concrete nouns which we classify into 5 size categories (tiny, small, medium, large, and huge). We randomly pick two nouns from different categories to form a (subject, object) pair.\nVisual Co-occurrence The visual co-occurrence dataset is generated in a way similar to that of the color, shape, and material datasets. The (subject, object) tuple here contains two nouns corresponding to objects that may appear in the same context. Co-occurrence distribution is extracted from Visual Genome where two objects that occur in the same scene graph together for more than 8 times are recorded."
    }, {
      "heading" : "3.2 Data Grouping",
      "text" : "Following Paik et al. (2021), we split the color, shape, and material datasets each into three groups: Single, Multi, and Any. The Single group is for subjects whose most common attribute covers more than 80% of the probability, e.g., the color of snow is almost always white. The Multi group is defined as subjects not in the Single group where more than 90% of the probability falls in the top 4 attribute classes, e.g., the color of a penguin in Fig. 1. The rest of the subjects are in the Any group. Lower model performance for the Single group would indicate the influence of reporting bias."
    }, {
      "heading" : "3.3 Templates",
      "text" : "In order to elicit model response and extract target objects and distributions from text, we manually design a set of templates for each relation. There are 7 templates for color, shape, and material each, 8 for size, and 4 for visual co-occurrence. See Table 1 for example templates."
    }, {
      "heading" : "3.4 Wikipedia Data",
      "text" : "In order to compare the text-only and multimodal datasets, we mine the color, shape, and material datasets from Wikipedia data, which is typically used in model pretraining. To mine these textbased datasets, we combine the sets of subjects in VG, take the manual list of attributes as objects again, and extract (subject, object) pairs if the pair matches any of the pre-defined templates. In Section 3.5 we will show the advantages of the VG\ndataset over this text-based data."
    }, {
      "heading" : "3.5 Dataset Evaluation",
      "text" : "To ensure the validity of the datasets mined from Visual Genome, we compare our color dataset with the human annotated CoDa dataset (Paik et al., 2021), which we assume is close to real-world color distributions and has minimal reporting bias. We see a reasonably strong correlation with human annotations, indicating that our dataset is a good and cost-effective approximation to human annotations.\nMetrics We report the Spearman’s rank-order correlation between the two distributions in comparison, averaged across all subjects. The Spearman correlation is used instead of the Pearson correlation since we care more about the rank of the object distributions than the exact values, which may be variable due to data variability. The top-1 accuracy (Acc@1) is measured by the percentage of the objects with the highest probability in the source distribution matching those in the target distribution. Those two metrics are also used in later sections when evaluating model distributions.\nAnalysis Table 2 shows the detailed results of the evaluation of the VG and Wikipedia color datasets by comparing with the human-annotated dataset, CoDa. We can see that the VG dataset has much higher Spearman correlation with CoDa, as well as substantially higher top-1 accuracy for the Single\ngroup. The VG correlation is expected to be low for the Any group, because objects in the Any group can have many possible colors.\nReporting bias is present in both datasets, as the average number of occurrences of Single group subjects are much fewer than that of the Multi and Any group subjects. Counter-intuitively, for VG, the highly-correlated Single group subjects have fewer average occurrences than the ones with low correlations. This is contrary to our expectation that more frequent objects would better reflect the human-perceived distribution and can be explained by Single subjects being easier to represent even without a large amount of data.\nOne example where the Wikipedia distribution diverges from the CoDa distribution is “penguin,” whose most likely color in CoDa is black, followed by white and gray; however, its top color in Wikipedia is blue, because “blue penguin” is a specific species with an entry in Wikipedia, even if it is not as common as black and white penguins. One example where the VG distributions diverge from CoDa is “mouse,” because in VG, most occurrences of “mouse” are computer mice, which are most commonly black, whereas when asked about the word “mouse”, human annotators typically think about the animal, meaning that the most likely colors in CoDa are white and gray.3\n3Additional examples are provided in Appendix A.3."
    }, {
      "heading" : "4 Probing Visual Commonsense",
      "text" : ""
    }, {
      "heading" : "4.1 Models",
      "text" : "Following Paik et al. (2021), we apply zero-shot probes to models that are trained on a language modeling objective, and conduct representation probes for those that are not. We report the prediction accuracy and the Spearman correlation of the output distribution with the true distribution.\nWe examine 6 transformer-based models, trained on a variety of data. BERT (Devlin et al., 2019), ALBERT (Lan et al., 2020), and RoBERTa (Liu et al., 2019) are trained on text only using a masked language modeling objective (MLM). Oscar (Li et al., 2020) is a vision-language model based on the BERT architecture, trained with an combined MLM and contrastive loss on text-image pairs. As our experiments involve exclusively textual inputs, we later describe a knowledge-distilled version of Oscar (“Distilled”) which corrects for the lack of image input in our task. Finally, we use representations from CLIP (ViT-B/32) (Radford et al., 2021), which is trained with a contrastive image-caption matching loss.\nWe use models trained with an MLM objective (BERT, Distilled, etc) directly for zero-shot prediction of masked tokens4. For Oscar we add a wordprediction head on top of it. The results across templates are aggregated in two modes. In the “best template” mode, for each example, the highest Spearman correlation among all templates is reported, and the top-1 result is regarded as correct if the true target object is the same as the top-1 result of any of the templates. In the “average template” mode, the output distribution is the mean of the distributions across all templates.\nSince CLIP is not trained on a token-prediction objective, we implement logistic regression on top of the frozen encoder output, to predict the target attribute or object. The input is each of the templates with the subject [X] filled with an input in the dataset. Like Paik et al. (2021), to give the model ample chance of success, we take the template that results in the best test accuracy score, report that accuracy and the Spearman correlation associated with that template.\nVokenization Tan and Bansal (2020) introduce the “vokenization” method, which aligns language tokens to their related images, mitigating the short-\n4For the target words that contain more than one subword tokens, we use the first token as the target.\ncomings of models trained on visually-grounded datasets in text-only tasks. Since our task is purely text-based, we also experiment with a pretrained vokenization model (BERT + VLM on Wiki)."
    }, {
      "heading" : "4.2 Elicitation Methods",
      "text" : "We compare the visual commonsense abilities of pretrained unimodal and multimodal models. Given a list of prompts and a subject word, each model outputs the distribution of the target word.\nSoft prompt tuning In order to overcome the limitation of self-designed prompts, we incorporate prompt tuning techniques in Qin and Eisner (2021), which learns soft prompts by gradient descent. The algorithm minimizes the log loss:∑\n(x,y)∈Er − log ∑ t∈Tr p(y|t, x)\nfor a set of example pairs Er and template set Tr.\nKnowledge distillation Through preliminary experiments, we notice, as expected, that pretrained Oscar, even without visual input, achieved better results than BERT. This led us to consider knowledge distillation (Hinton et al., 2015; Sanh et al., 2019). We use Oscar as the teacher and BERT as the student, and the weights of the student are adjusted to simulate the output distribution of the teacher. The training data is part of the Oscar pretraining corpus: COCO (Lin et al., 2014), Flickr30k (Young et al., 2014), and GQA (Hudson and Manning, 2019)."
    }, {
      "heading" : "4.3 Size Evaluation",
      "text" : "We use two evaluation strategies for size, because the size dataset differs from the other datasets in that we use relative sizes (X is larger/smaller than Y), as absolute size information is hard to obtain.\nRank partition First, similar to the previous prediction task, given a template such as “[X] is larger than [Y]” and an object [X], we ask the model to predict the distribution of [Y], taking only the distribution D of nouns in the size dataset. For the current object [X], we take the nouns in size categories that are smaller than the category of [X] (Nsm), and those that are in larger categories (Nlg). Let the length of Nsm be m and the length of Nlg be n. Then for the “larger” templates, we compute the average percentage of overlap between the top n objects in D and Nlg and that between the bottom m objects in D and and Nsm. For the “smaller” templates, the “top” and “bottom” are reversed.\nAdjective projection The second approach follows that of van Paridon et al. (2021), which projects the word to be evaluated onto an adjective scale. In this case, we compute the word embeddings of the adjectives “small” and “large” and the nouns from models, so the scale is −−→ large − −−−→small and the projection is calculated by cosine similarity. For instance, for the example noun “bear”, the projection score is given by:\ncos_sim( −−→ large −−−−→small,−−→bear)\nWith good word embeddings, larger nouns are expected to have higher projection scores. The validity of the adjective scales from word representations is shown by Kim and de Marneffe (2013)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Implementation Details",
      "text" : "Dataset splits Each of the color, shape, material, size, and co-occurrence datasets is split into 80% training data and 20% test data. All evaluation metrics are reported on the test set. The training set is used for the logistic regression and the soft prompt tuning algorithm.\nModel training For the classification head, we use the sklearn implementation of Logistic Regression (random_state=0, C=0.316, max_iter=2000). For soft prompt tuning, we use the implementation from Qin and Eisner (2021) 5. For knowledge distillation, we use the Kullback-Leibler loss to measure the divergence between the output distributions of BERT and Oscar, and optimize the\n5https://github.com/hiaoxui/ soft-prompts\npretrained BERT on that loss to match the outputs of Oscar. Configurable parameters are set the same as for Oscar pretraining."
    }, {
      "heading" : "5.2 Results",
      "text" : "The experimental results show that multimodal models outperform text-only models in capturing visual commonsense. However, all models are subject to the influence of reporting bias, as they correlate better with the distributions from Wikipedia than those from VG. Prompt tuning and knowledge distillation substantially enhance model performance, while increasing model size does not.\nColor, Shape, Material The resulting model performance for the “average template” mode is shown in Table 3. Prompt tuning is done in this mode only. Note that because the top-1 accuracy is taken among all possible classes of each relation, it should be interpreted together with the number of classes (Table 1).\nWe can see from Table 3 that Oscar does better than BERT in almost all cases. Significant difference between Oscar (base) and BERT (base) is seen in most cases. Also, after soft prompt tuning, both the Spearman correlation and the accuracy substantially improved. Although the standard deviations of the Spearman correlations are large, we find consistent improvement per example with both prompt tuning and multimodal pretraining (Appendix A.2).\nTable 3 also shows that knowledge distillation helps improve the performance of BERT in all cases, and the distilled model can sometimes even outperform the teacher model, Oscar. Moreover, the large version of each model does not necessarily perform better than their base counterparts,"
    }, {
      "heading" : "46.2 47.5 64.0",
      "text" : ""
    }, {
      "heading" : "53.8 50.0 57.4",
      "text" : ""
    }, {
      "heading" : "57.5 53.7 60.2",
      "text" : ""
    }, {
      "heading" : "36.3 44.8 50.0",
      "text" : ""
    }, {
      "heading" : "14.6 20.2 32.3",
      "text" : "suggesting that increasing the size of the model would not enhance the model’s ability to understand visual commonsense. Instead, training with visually grounded data would.\nFig. 2 illustrates the Spearman correlations of different models with the color distributions from CoDa, VG and Wikipedia, under the “best template” mode.6 We assume that CoDa contains no reporting bias, in which case we can interpret Table 2 as showing that VG contains a relatively small amount of it, and Wikipedia contains a relatively large amount. All models correlate moderately with all three datasets, with the highest correlations to Wikipedia, indicating text-based reporting bias in all model types. BERT has the largest correlation gap between Wikipedia and CoDa.\nVisual Co-occurrence Table 3 also contains the results on visual co-occurrence before and after prompt tuning. Only the Spearman correlations are reported, because the top-1 accuracy is meaningless due to the large number of possible co-occurring objects with any noun.\nBefore prompt tuning, BERT has small Spearman correlations, suggesting that it may contain little knowledge about the visual co-occurrence relationship. Oscar demonstrates more such knowledge under the zero-shot setting. After prompt tuning, all model performances improve.\nSize Table 4 shows results of the rank partition method (Section 4.3), before and after prompt tuning. Surprisingly, prompt tuning does not help in this case. Moreover, the performance for the\n6Appendix A.2 contains further details.\n“larger” templates is higher than that of the “smaller” templates, suggesting that the models contain inherent preference towards the “larger” templates.\nFig. 3 shows the results of the adjective projection method.7 For BERT and Oscar, we use the average embedding of the subword tokens of the nouns projected onto that of the adjectives “large” and “small”. For CLIP, we take the textual encoder outputs as the embeddings, resulting in a different score range from that of BERT and Oscar. The results show the following trend: larger objects are projected onto the “large” end of the spectrum, although the trend is sometimes broken towards the “huge” end. This may be due to the “huge” group including nouns such as “pool” and “house” which can be modified by a relative size indicator “small”."
    }, {
      "heading" : "5.3 Results with Classification Head",
      "text" : "Table 5 shows the results of BERT, CLIP, and Oscar when topped with a classification head. We observe that Oscar and CLIP achieve similar per-\n7Appendix A.2 contains per-object plot for BERT vs Oscar.\nformance and are both better than BERT. Note that, while Visual Genome is part of Oscar’s pretraining corpus and one might suspect that that gives it an advantage, CLIP is trained on a large corpus from web search that is unrelated to Visual Genome. Therefore, we can conclude that multimodal models pretrained on both images and text outperform text-only models.\nTable 6 breaks down the results in Table 5 into three subject groups. Oscar and CLIP outperform BERT in almost all cases. The top-1 accuracy is higher for the Single group than for the Multi and Any groups, perhaps because the Single group subjects have only one most likely target attribute, which may be easier to predict. Note that the Spearman correlations for all three models become higher from group Single to Multi to Any. Paik et al. (2021) argue that higher correlation for the Any and Multi groups is a sign of model reporting bias, as objects in those two groups are more often reported. Thus, the results here indicate that reporting bias is still present in multimodal models."
    }, {
      "heading" : "5.4 Analysis and Limitations",
      "text" : "In Table 3, the accuracy of BERT for shape is particularly low (only 6.7%), despite that shape has only 12 classes. We hypothesize that this is due to reporting bias on shape in the text corpora that BERT is trained on. This hypothesis is supported by mining sentences from Wikipedia that contain\n(noun, attribute) pairs, where we see that the relation shape has fewer number of occurrences than material and color (see Appendix A.3).\nFinally, although multimodal models show improvement on the task, the improvement is sometimes not significant and the resulting correlations are still weak. Further work is needed to enhance the visual commonsense abilities of the models and mitigate reporting bias, and our datasets can serve as an evaluation method."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we probe knowledge about visually salient properties from pretrained neural networks. We automatically extract dataset of five visual relations–color, shape, material, size, and co-occurrence, and show that it has much higher correlation with human perception for color than data mined from text copora. Then, we apply various types of probing techniques and discover that visually-supervised models can better capture such visual properties than pure language models. Knowledge distillation can sometimes further enhance model performance. Despite their higher performance, visually-supervised models are still subject to the influence of reporting bias, as shown by the per-group analysis, where both types of models perform better for the Multi group than the Single group."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 List of Objects\nTable 7 shows the list of all possible attributes for relations color, shape, and material. Table 8 shows the list of objects in the five categories of relation size. Visual co-ocurrence has a large number of objects that are not listed here for space reasons.\nHuge building, airplane, plane, clocktower, tower, earth,\nA.2 Additional Probing\nBest template mode Table 9 contains zero-shot results under the “best template” mode, for BERT (base), Oscar (base), BERT distilled from Oscar, RoBERTa (base), ALBERT (base), and Vokenization. These results demonstrate similar trends as the ones in the “average template” mode.\nPer-object analysis Fig. 4 illustrates the finegrained Spearman correlation ± standard deviation per object group for BERT and CLIP.\nSize per-object Fig. 5 shows how the per-object projection scores on the size spectrum from BERT and Oscar are correlated.\nPer-Subject Comparison Fig. 6 and Fig. 7 show how the Spearman correlations of 10 individual subjects improve after soft prompt tuning and after multimodal pretraining. Consistent improvement\ncan be seen in color, material, and cooccurrence. Although we report average Spearman correlations in Table 3 and there are large standard deviations, here we show that when improvement is observed collectively, it is also consistent across subjects. With shape, the improvement is less obvious (45.9 to 50.4 for prompt tuning and 49.2 to 50.4 for multimodal pretraining).\nA.3 Error Analysis\nData The three subjects with the highest and lowest Spearman correlation are shown in Fig. 8 and Fig. 9.\nWikipedia Table 10 shows the number of (noun, attribute) pairs of the three relation types in Wikipedia. Shape has fewer occurrences than material and color.\nModel Table 11 shows the errors made by BERT and Oscar in the “average template” mode before prompt tuning. Overall, subjects with low correlation are those that are less often reported in Visual Genome as well as in textual data.\nA.4 Resources\nBERT, RoBERTa, ALBERT We use the Huggingface implementations of BERT, RoBERTa, and ALBERT.\nOscar See the GitHub repository for the code and pretrained Oscar: https://github.com/ microsoft/Oscar.\nCLIP We use the CLIP model released by OpenAI: https://github.com/openai/ CLIP.\nVokenization See the GitHub repository for the pretrained model: https://github.com/ airsplay/vokenization."
    } ],
    "references" : [ {
      "title" : "Multimodal machine learning: A survey and taxonomy",
      "author" : [ "Tadas Baltrusaitis", "Chaitanya Ahuja", "Louis-Philippe Morency." ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 41(2):423–443.",
      "citeRegEx" : "Baltrusaitis et al\\.,? 2019",
      "shortCiteRegEx" : "Baltrusaitis et al\\.",
      "year" : 2019
    }, {
      "title" : "Experience grounds language",
      "author" : [ "Yonatan Bisk", "Ari Holtzman", "Jesse Thomason", "Jacob Andreas", "Yoshua Bengio", "Joyce Chai", "Mirella Lapata", "Angeliki Lazaridou", "Jonathan May", "Aleksandr Nisnevich", "Nicolas Pinto", "Joseph Turian." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Bisk et al\\.,? 2020",
      "shortCiteRegEx" : "Bisk et al\\.",
      "year" : 2020
    }, {
      "title" : "UNITER: Universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Vision-and-language or vision-forlanguage? on cross-modal influence in multimodal transformers",
      "author" : [ "Stella Frank", "Emanuele Bugliarello", "Desmond Elliott." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Frank et al\\.,? 2021",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2021
    }, {
      "title" : "Reporting bias and knowledge acquisition",
      "author" : [ "Jonathan Gordon", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2013 Workshop on Automated Knowledge Base Construction.",
      "citeRegEx" : "Gordon and Durme.,? 2013",
      "shortCiteRegEx" : "Gordon and Durme.",
      "year" : 2013
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "ArXiv, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "GQA: A new dataset for compositional question answering over real-world images",
      "author" : [ "Drew A. Hudson", "Christopher D. Manning." ],
      "venue" : "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (ICCV), page 6700–6709.",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Deriving adjectival scales from continuous space word representations",
      "author" : [ "Joo-Kyung Kim", "Marie-Catherine de Marneffe." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1625–1630, Seattle,",
      "citeRegEx" : "Kim and Marneffe.,? 2013",
      "shortCiteRegEx" : "Kim and Marneffe.",
      "year" : 2013
    }, {
      "title" : "ALBERT: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Xiaowei Hu", "Pengchuan Zhang", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "In",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft COCO: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "Lubomir D. Bourdev", "Ross B. Girshick", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "European Conference on",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Annual Conference on Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels",
      "author" : [ "Ishan Misra", "C. Lawrence Zitnick", "Margaret Mitchell", "Ross Girshick." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages",
      "citeRegEx" : "Misra et al\\.,? 2016",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2016
    }, {
      "title" : "The world of an octopus: How reporting bias influences a language model’s perception of color",
      "author" : [ "Cory Paik", "Stéphane Aroca-Ouellette", "Alessandro Roncone", "Katharina Kann." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Paik et al\\.,? 2021",
      "shortCiteRegEx" : "Paik et al\\.",
      "year" : 2021
    }, {
      "title" : "How do blind people know that blue is cold? distributional semantics encode color-adjective associations",
      "author" : [ "Jeroen van Paridon", "Qiawen Liu", "Gary Lupyan." ],
      "venue" : "Proceedings of the Annual Meeting of the Cognitive Science Society.",
      "citeRegEx" : "Paridon et al\\.,? 2021",
      "shortCiteRegEx" : "Paridon et al\\.",
      "year" : 2021
    }, {
      "title" : "On the function of existential presupposition in discourse",
      "author" : [ "Ellen F. Prince." ],
      "venue" : "Chicago Linguistic Society (Vol. 14, pp. 362–376).",
      "citeRegEx" : "Prince.,? 1978",
      "shortCiteRegEx" : "Prince.",
      "year" : 1978
    }, {
      "title" : "Learning how to ask: Querying lms with mixtures of soft prompts",
      "author" : [ "Guanghui Qin", "Jason Eisner." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Qin and Eisner.,? 2021",
      "shortCiteRegEx" : "Qin and Eisner.",
      "year" : 2021
    }, {
      "title" : "Learning transferable visual models from natural language",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "ArXiv, abs/1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "LXMERT: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Vokenization: Improving language understanding with contextualized, visual-grounded supervision",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2066–2080,",
      "citeRegEx" : "Tan and Bansal.,? 2020",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2020
    }, {
      "title" : "Learning common sense through visual abstraction",
      "author" : [ "Ramakrishna Vedantam", "Xiaoyu Lin", "Tanmay Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "2015 IEEE International Conference on Computer Vision (ICCV), pages 2542–2550.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "On the existence of tacit assumptions in contextualized language models",
      "author" : [ "Nathaniel Weir", "Adam Poliak", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Annual Meeting of the Cognitive Science Society.",
      "citeRegEx" : "Weir et al\\.,? 2020",
      "shortCiteRegEx" : "Weir et al\\.",
      "year" : 2020
    }, {
      "title" : "CPT: colorful prompt tuning for pre-trained vision-language models",
      "author" : [ "Yuan Yao", "Ao Zhang", "Zhengyan Zhang", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun." ],
      "venue" : "ArXiv, abs/2109.11797.",
      "citeRegEx" : "Yao et al\\.,? 2021",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2021
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, vol-",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "From recognition to cognition: Visual commonsense reasoning",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Matthew Peters", "Roozbeh Mottaghi", "Aniruddha Kembhavi", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Asso-",
      "citeRegEx" : "Zellers et al\\.,? 2021",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2021
    }, {
      "title" : "VinVL: Making visual representations matter in vision-language models",
      "author" : [ "Pengchuan Zhang", "Xiujun Li", "Xiaowei Hu", "Jianwei Yang", "Lei Zhang", "Lijuan Wang", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "2021 IEEE Conference on Computer Vision and Pattern",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to prompt for visionlanguage models",
      "author" : [ "Kaiyang Zhou", "Jingkang Yang", "Chen Change Loy", "Ziwei Liu." ],
      "venue" : "arXiv preprint arXiv:2109.01134.",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Overcoming language priors with self-supervised learning for visual question answering",
      "author" : [ "Xi Zhu", "Zhendong Mao", "Chunxiao Liu", "Peng Zhang", "Bin Wang", "Yongdong Zhang." ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI), page",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The observation that human language understanding happens in a rich multimodal environment has led to an increased focus on visual grounding in natural language processing (NLP) (Bisk et al., 2020; Baltrusaitis et al., 2019), driving comparisons between traditional unimodal text-only models and multimodal models which take both text and image inputs.",
      "startOffset" : 178,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : "The observation that human language understanding happens in a rich multimodal environment has led to an increased focus on visual grounding in natural language processing (NLP) (Bisk et al., 2020; Baltrusaitis et al., 2019), driving comparisons between traditional unimodal text-only models and multimodal models which take both text and image inputs.",
      "startOffset" : 178,
      "endOffset" : 224
    }, {
      "referenceID" : 18,
      "context" : "In order to elicit visual commonsense from language models, we utilize soft prompt tuning (Qin and Eisner, 2021), which trains optimal templates by gradient descent for each model and relation type that we explore.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "image and text representations from cross-modal training of transformers with self-attention, including LXMERT (Tan and Bansal, 2019), ViLBERT (Lu et al.",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "image and text representations from cross-modal training of transformers with self-attention, including LXMERT (Tan and Bansal, 2019), ViLBERT (Lu et al., 2019), UNITER (Chen et al.",
      "startOffset" : 143,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "Oscar (Li et al., 2020) additionally uses object tags in images as anchor points to ease the learning of image-text alignments and VinVL (Zhang et al.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 29,
      "context" : ", 2020) additionally uses object tags in images as anchor points to ease the learning of image-text alignments and VinVL (Zhang et al., 2021) presents an improved object detection model.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "CLIP (Radford et al., 2021) learns by predicting caption-image alignment from a large internet corpus of (image, text) pairs.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "Our idea of visual commonsense is more similar to the idea of stereotypic tacit assumptions (Prince, 1978) – the propositional beliefs that humans hold about generic concepts, such as “dogs have to be walked.",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "Pretrained language models such as BERT (Devlin et al., 2019) are trained on millions of tokens of text, capturing statistical regularities present in the training corpora.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "To ensure the validity of the datasets mined from Visual Genome, we compare our color dataset with the human annotated CoDa dataset (Paik et al., 2021), which we assume is close to real-world color distributions and has minimal reporting bias.",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : ", 2019), ALBERT (Lan et al., 2020), and RoBERTa (Liu et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : ", 2020), and RoBERTa (Liu et al., 2019) are trained on text only using a masked language modeling objective (MLM).",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "Oscar (Li et al., 2020) is a vision-language model based on the BERT architecture, trained with an combined MLM and contrastive loss on text-image pairs.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "Finally, we use representations from CLIP (ViT-B/32) (Radford et al., 2021), which is trained with a contrastive image-caption matching loss.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "This led us to consider knowledge distillation (Hinton et al., 2015; Sanh et al., 2019).",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "This led us to consider knowledge distillation (Hinton et al., 2015; Sanh et al., 2019).",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "The training data is part of the Oscar pretraining corpus: COCO (Lin et al., 2014), Flickr30k (Young et al.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : ", 2014), Flickr30k (Young et al., 2014), and GQA (Hudson and Manning, 2019).",
      "startOffset" : 19,
      "endOffset" : 39
    } ],
    "year" : 0,
    "abstractText" : "Our commonsense knowledge about objects includes their typical visual attributes; we know that bananas are typically yellow or green, and not purple. Text and image corpora, being subject to reporting bias, represent this worldknowledge to varying degrees of faithfulness. In this paper, we investigate to what degree unimodal (language-only) and multimodal (image and language) models capture a broad range of visually salient attributes. To that end, we automatically extract a visually-grounded commonsense dataset covering 5 property types (color, shape, material, size, and visual co-occurrence) for over 5000 subjects. We validate this dataset by showing that our grounded color data correlates much better than ungrounded text-only data with crowdsourced color judgments provided by Paik et al. (2021). We then use our dataset to evaluate pretrained unimodal models and multimodal models. Our results show that multimodal models better reconstruct attribute distributions, but are still subject to reporting bias. Moreover, increasing model size does not enhance performance, suggesting that the key to visual commonsense lies in the data.",
    "creator" : null
  }
}
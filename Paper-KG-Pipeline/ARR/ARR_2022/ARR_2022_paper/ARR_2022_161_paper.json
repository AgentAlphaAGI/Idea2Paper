{
  "name" : "ARR_2022_161_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural reality of argument structure constructions",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pretrained Transformer-based language models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have recently achieved impressive results on many natural language tasks, spawning a new interdisciplinary field of aligning LMs with linguistic theory and probing the linguistic capabilities of LMs (Linzen and Baroni, 2021). Most probing work so far has assumed a transformational generative syntactic framework (Chomsky, 1965), accounting for phenomena such as agreement, binding, licensing, and movement (Warstadt et al., 2020a; Hu et al., 2020) with a\nparticular focus on determining whether a sentence is linguistically acceptable (Schütze, 1996). Relatively little work has been done on probing LMs from construction grammar (Goldberg, 1995, 2006), where there is a greater emphasis in identifying grammatically relevant classes of multi-word constructions and exploring their interactions.\nOne area where construction grammar disagrees with many generative theories of language is in the analysis of the argument structure of verbs, that is, the specification of the number of arguments that a verb takes, their semantic relation to the verb, and their syntactic form (Levin and Rappaport Hovav, 2005). Lexicalist theories were long dominant in generative grammar (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987). In lexicalist theories, argument structure is assumed to be encoded in the lexical entry of the verb: for example, the verb visit is lexically specified as being transitive and as requiring a noun phrase object (Chomsky, 1986). In contrast, construction grammar suggests that argument structure is encoded in form-meaning pairs known as argument\nstructure constructions (ASCs, Figure 1), which are distinct from verbs. The argument structure of a verb is determined by pairing it with an ASC (Goldberg, 1995). To date, a substantial body of psycholinguistic work has provided evidence for the psychological reality of ASCs in sentence sorting (Bencini and Goldberg, 2000; Gries and Wulff, 2005), priming (Ziegler et al., 2019), and novel verb experiments (Kaschak and Glenberg, 2000; Johnson and Goldberg, 2013).\nHere, we connect basic research in ASCs with neural probing by adapting several psycholinguistic studies to Transformer-based LMs and show evidence for the neural reality of ASCs. Our first case study is based on sentence sorting (Bencini and Goldberg, 2000); we discover that in English, German, Italian, and Spanish, LMs consider sentences that share the same construction to be more semantically similar than sentences sharing the main verb. Furthermore, this preference for constructional meaning only manifests in larger LMs (trained with more data), whereas smaller LMs rely on the main verb, an easily accessible surface feature. Human experiments with non-native speakers found a similarly increased preference for constructional meaning in more proficient speakers (Liang, 2002; Baicchi and Della Putta, 2019), suggesting commonalities in language acquisition between LMs and humans.\nOur second case study is based on nonsense “Jabberwocky” sentences that nevertheless contain meaning when they are arranged in constructional templates (Johnson and Goldberg, 2013). We adapt the original priming experiment to LMs and show that RoBERTa is able to derive meaning from ASCs, even without any lexical cues. This finding offers counter-evidence to earlier claims that LMs are relatively insensitive to word order when constructing sentence meaning (Yu and Ettinger, 2020; Sinha et al., 2021). We include the source code and data for reproducing this work as supplementary material and will release the GitHub repository upon publication."
    }, {
      "heading" : "2 Psycholinguistic background",
      "text" : ""
    }, {
      "heading" : "2.1 Construction grammar and ASCs",
      "text" : "Construction grammar is a family of linguistic theories proposing that all linguistic knowledge consists of constructions: pairings between form and meaning where some aspects of form or meaning are not predictable from its parts (Fillmore et al., 1988;\nKay and Fillmore, 1999; Goldberg, 1995, 2006). Common examples include idiomatic expressions such as under the weather (meaning “to feel unwell”), but many linguistic patterns are constructions, including morphemes (e.g., -ify), words (e.g., apple), and abstract patterns like the ditransitive and passive. In contrast to lexicalist theories of argument structure, construction grammar rejects the dichotomy between syntax and lexicon. In contrast to transformational grammar, it rejects any distinction between surface and underlying structure.\nWe focus on a specific family of constructions for which there is an ample body of psycholinguistic evidence: argument structure constructions (ASCs). ASCs are constructions that specify the argument structure of a verb (Goldberg, 1995). In the lexicalist, verb-centered view, argument structure is a lexical property of the verb, and the main verb of a sentence determines the form and meaning of the sentence (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987; Levin and Rappaport Hovav, 1995). For example, sneeze is intransitive (allowing no direct object) and hit is transitive (requiring one direct object). However, lexicalist theories encounter difficulties with sentences like “he sneezed the napkin off the table” since intransitive verbs are not permitted to have object arguments.\nRather than assuming multiple implausible senses for the verb “sneeze” with different argument structures, Goldberg (1995) proposed that ASCs operate on an arbitrary verb, altering its argument structure while at the same time modifying its meaning. For example, the caused-motion ASC adds a direct object and a path argument to the verb sneeze, with the semantics of causing the object to move along the path. Other ASCs include the transitive, ditransitive, and resultative (Figure 1), which specify the argument structure of a verb and interact with its meaning in different ways."
    }, {
      "heading" : "2.2 Psycholinguistic evidence for ASCs",
      "text" : "Sentence sorting. Several psycholinguistic studies have found evidence for argument structure constructions using experimental methods. Bencini and Goldberg (2000) was the first such study: they used a sentence sorting setup to determine whether the verb or construction in a sentence was the main contributor to sentence meaning. 17 participants were given 16 index cards with sentences containing 4 verbs (throw, get, slice, and take) and 4 con-\nstructions (transitive, ditransitive, caused-motion, and resultative) and were instructed to sort them into 4 piles by overall sentence meaning (Table 1). The experimenters measured the deviation to a purely verb-based or construction-based sort, and found that on average, the piles were closer to a construction sort.\nNon-native sentence sorting. The same set of experimental stimuli was used with L2 (non-native) English speakers. Gries and Wulff (2005) ran the experiment with 22 German native speakers, who preferred the construction-based sort over the verbbased sort, showing that constructional knowledge is not limited to native speakers. Liang (2002) ran the experiment on Chinese native speakers of 3 different English levels (46 beginner, 31 intermediate, and 33 advanced), and found that beginners preferred a verb-based sort, while advanced learners produced construction-based sorts similar to native speakers (Figure 2). Likewise, Baicchi and Della Putta (2019) found the same result in Italian native speakers with B1 and B2 English proficiency levels. Overall, these studies show evidence for ASCs in the mental representations of native and L2 English speakers alike, and furthermore, preference for constructional over verb sorting increases with increasing English proficiency.\nMultilingual sentence sorting. Similar sentence sorting experiments have been conducted in other languages, with varying results. Kirsch (2019) ran a sentence sorting experiment in German with 40 participants and found that they mainly sorted by verb but rarely by construction. Baicchi and Della Putta (2019) ran an experiment with non-native learners of Italian (15 participants of B1 level and 10 participants of B2 level): both groups preferred the constructional sort, and similar to Liang (2002), the B2 learners sorted more by construction than the B1 learners. Vázquez (2004) ran an experiment in Spanish with 16 participants, and found approximately equal proportions of construc-\ntions and verb sort. In Italian and Spanish, some different constructions were substituted as not all of the English constructions had an equivalent in these languages; see Appendix for the complete set of stimuli in each language.\nPriming. Another line of psycholinguistic evidence comes from priming studies. Priming refers to the condition where exposure to a (prior) stimulus influences the response to a later stimulus (Pickering and Ferreira, 2008). Bock and Loebell (1990) found that participants were more likely to produce sentences of a given syntactic structure when primed with a sentence of the same structure; Ziegler et al. (2019) argued that Bock and Loebell (1990) did not adequately control for lexical overlap, and instead, they showed that the construction must be shared for the priming effect to occur, not just shared abstract syntax.\nNovel verbs. Even with unfamiliar words, there is evidence that constructions are associated with meaning. Kaschak and Glenberg (2000) constructed sentences with novel denominal verbs and found that participants were more likely to interpret a transfer event when the denominal verb was used in a ditransitive sentence (Tom crutched Lyn an apple) than a transitive one (Tom crutched an apple).\nJohnson and Goldberg (2013) used a “Jabberwocky” priming setup to show that abstract constructional templates are associated with meaning. Participants were primed with a nonsense sentence of a given construction (e.g., He daxed her the norp for the ditransitive construction), followed by a lexical decision task of quickly deciding if a string of characters was a real English word or a non-word. The word in the decision task was semantically congruent with the construction (gave) or incongruent (made); furthermore, they experimented with target words that were high-frequency (gave), lowfrequency (handed), or semantically related but not associated with the construction (transferred).\nThey found priming effects (faster lexical decision times) in all three conditions, with the strongest effect for the high-frequency condition, followed by the low-frequency and the semantically nonassociate conditions.\nWe adapt several of these psycholinguistic studies to LMs: the sentence sorting experiments in Case study 1, and the Jabberwocky priming experiment in Case study 2. We choose these studies because their designs allow for thousands of stimuli sentences to be generated automatically using templates, avoiding issues caused by small sample sizes from manually constructed sentences."
    }, {
      "heading" : "3 Related work in NLP",
      "text" : ""
    }, {
      "heading" : "3.1 Linguistic probing of LMs",
      "text" : "Many studies have probed for various aspects of syntax in LSTMs and Transformer-based LMs. Linzen et al. (2016) tested LSTMS on their ability to capture subject-verb agreement, using templates to generate test data. This idea was extended by BLiMP (Warstadt et al., 2020a), a suite encompassing 67 linguistic phenomena, including fillergap effects, NPI licensing, and ellipsis; Hu et al. (2020) released a similar test suite. Template generation is a convenient method to construct stimuli exhibiting specific linguistic properties, but alternative approaches include CoLA (Warstadt et al., 2019), which compiled an acceptability benchmark of sentences drawn from linguistic publications, and Gulordava et al. (2018), who perturbed natural sentences to study LMs’ knowledge of agreement on nonsense sentences. We refer to Linzen and Baroni (2021) for a comprehensive review of the linguistic probing literature.\nSo far, most probing work has assumed a theoretical framework based on generative syntax, and relatively few papers approached LM probing from a construction grammar perspective. Madabushi et al. (2020) probed for BERT’s knowledge of constructions via a sentence pair classification setup of predicting whether two sentences share the same construction. Their probe was based on data from Dunn (2017), who used an unsupervised algorithm to extract plausible constructions from corpora based on association strength. However, the linguistic validity of these automatically induced constructions is uncertain, and there is currently no human-labelled wide-coverage construction grammar dataset in any language suitable for probing. Other computational work focused on\na few specific constructions, such as identifying caused-motion constructions in corpora (Hwang and Palmer, 2015) and annotating constructions related to causal language (Dunietz et al., 2015). Lebani and Lenci (2016) is most similar to our work: they probed distributional vector space models for ASCs based on the Jabberwocky priming experiment by Johnson and Goldberg (2013)."
    }, {
      "heading" : "3.2 Psycholinguistic treatment of LMs",
      "text" : "Some recent probing studies adapted methods and data from psycholinguistic research, treating LMs as psycholinguistic participants. Using a cloze completion setup, Ettinger (2020) found that BERT was less sensitive than humans at commonsense inferences and detecting role reversals, and fails completely at understanding negation. Michaelov and Bergen (2020) compared LM surprisals with the N400 (a measure of human language processing difficulty) across a wide range of conditions; Li et al. (2021) used psycholinguistic stimuli and found that LMs exhibit different layerwise surprisal patterns for morphosyntactic, semantic, and commonsense anomalies. Wilcox et al. (2021) compared LM and human sensitivities to syntactic violations using a maze task to collect human reaction times. Prasad et al. (2019); Misra et al. (2020) investigated whether LMs are sensitive to priming effects like humans. The advantage of psycholinguistic data is that they are carefully constructed by expert linguists to test theories of language processing in humans; however, their small sample size makes it challenging to make statistically meaningful conclusions when the (oft-sparse) experimental stimuli are used to probe a language model."
    }, {
      "heading" : "4 Case study 1: Sentence sorting",
      "text" : "This section describes our adaptation of the sentence sorting experiments to Transformer LMs."
    }, {
      "heading" : "4.1 Methodology",
      "text" : "Models. To simulate varying non-native English proficiency levels, we use MiniBERTa models (Warstadt et al., 2020b), trained with 1M, 10M, 100M, and 1B tokens. We also use the base RoBERTa model (Liu et al., 2019b), trained with 30B tokens. In other languages, there are no available pretrained checkpoints with varying amounts of pretraining data, so we use the mBERT model\n1Bencini and Goldberg (2000) ran the sentence sorting experiment twice, so we take the average of the two runs.\n(Devlin et al., 2019) and a monolingual Transformer LM in each language.2 We obtain sentence embeddings for our models by taking the average of their contextual token embeddings at the secondto-last layer (i.e., layer 11 for base RoBERTa). We use the second-to-last because the last layer is more specialized for the LM pretraining objective and less suitable for sentence embeddings (Liu et al., 2019a).\nTemplate generation. We use templates to generate stimuli similar to the 4x4 design in the Bencini and Goldberg (2000) experiment. To ensure an adequate sample size, we run multiple empirical trials. In each trial, we sample 4 random distinct verbs from a pool of 10 verbs that are compatible with all 4 constructions (cut, hit, get, kick, pull, punch, push, slice, tear, throw). We then randomly fill in the slots for proper names, objects, and complements for each sentence according to its verb, such that the sentence is semantically coherent, and there is no lexical overlap among the sentences of any construction. Table 3 in the appendix shows a set of template-generated sentences. In English, we generate 1000 sets of stimuli using this procedure; for other languages, we use the original stimuli from their respective publications.\nEvaluation. Similar to the human experiments, we group the sentence embeddings into 4 clusters\n2We use monolingual German and Italian models from https://github.com/dbmdz/berts, and the monolingual Spanish model from Cañete et al. (2020).\n(not necessarily of the same size) using agglomerative clustering by Euclidean distance (Pedregosa et al., 2011). We then compute the deviation to a pure construction and pure verb sort using the Hungarian algorithm for optimal bipartite matching. This measures the minimal number of cluster assignment changes necessary to reach a pure construction or verb sort, ranging from 0 to 12. Thus, lower construction deviation indicates that constructional information is more salient in the LM’s embeddings."
    }, {
      "heading" : "4.2 Results and interpretation",
      "text" : "Figure 2 shows the LM sentence sorting results for English. All differences are statistically significant (p < .001). The smallest 1M MiniBERTa model is the only LM to prefer verb over construction sorting, and as the amount of pretraining data grows, the LMs increasingly prefer sorting by construction instead of by verb. This closely mirrors the trend observed in the human experiments.\nThe results for multilingual sorting are shown in Figure 3. Both mBERT and the monolingual LMs consistently prefer constructional sorting over verb sorting in all three languages, whereas the results from the human experiments are less consistent.\nOur results show that RoBERTa can generalize meaning from abstract constructions without lexical overlap. Only larger LMs and English speakers of more advanced proficiency are able to make this generalization, while smaller LMs and less profi-\ncient speakers derive meaning more from surface features like lexical content. This finding agrees with Warstadt et al. (2020b), who found that larger LMs have an inductive bias towards linguistic generalizations, while smaller LMs have an inductive bias towards surface generalizations; this may explain the success of large LMs on downstream tasks. A small quantity of data (10M tokens) is sufficient for LMs to prefer the constructional sort, indicating that ASCs are relatively easy to learn: roughly on par with other types of linguistic knowledge, and requiring less data than commonsense knowledge (Zhang et al., 2021; Liu et al., 2021).\nWe note some limitations in these results, and reasons to avoid drawing unreasonably strong conclusions from them. Human sentence sorting experiments can be influenced by minor differences in the experimental setup: Bencini and Goldberg (2000) obtained significantly different results in two runs that only differed on the precise wording of instructions. In the German experiment (Kirsch, 2019), the author hypothesized that the participants were influenced by a different experiment that they had completed before the sentence sorting one. Given this experimental variation, we cannot attribute differences across languages to differences in their linguistic typology. Although LMs do not suffer from the same experimental variation, we cannot conclude statistical significance from the multilingual experiments, where only one set of stimuli is available in each language.\nConstruction Template / Examples Ditransitive S/he V-ed him/her the N. She traded her the epicenter. He flew her the donut. Resultative S/he V-ed it Adj. He cut it seasonal. She surged it civil. Caused-motion S/he V-ed it on the N. He registered it on the diamond. She awarded it on the corn. Removal S/he V-ed it from him/her. He declined it from her. She drove it from him.\nTable 2: Templates and example sentences for the Jabberwocky construction experiments. The templates are identical to the ones used in Johnson and Goldberg (2013), except that we use random real words instead of nonce words."
    }, {
      "heading" : "5 Case study 2: Jabberwocky constructions",
      "text" : "We next adapt the “Jabberwocky” priming experiment from Johnson and Goldberg (2013) to LMs, and make several changes to the original setup to better assess the capabilities of LMs. Priming is a standard experimental paradigm in psycholinguistic research, but it is not directly applicable to LMs: existing methods simulate priming either by applying additional fine-tuning (Prasad et al., 2019), or by concatenating sentences that typically do not co-occur in natural text (Misra et al., 2020). There-\nfore, we instead propose a method to probe LMs for the same linguistic information using only distance measurements on their contextual embeddings."
    }, {
      "heading" : "5.1 Methodology",
      "text" : "Template generation. We generate sentences for the four constructions randomly using the templates in Table 2. Instead of filling nonce words like norp into the templates as in the original study, we take an approach similar to Gulordava et al. (2018) and generate 5000 sentences for each construction by randomly filling real words of the appropriate part-of-speech into construction templates (Table 2). This gives nonsense sentences like “She traded her the epicenter”; we refer to these random words as Jabberwocky words. By using real words, we avoid any potential instability from feeding tokens into the model that it has never seen during pretraining. We obtain a set of singular nouns, past tense verbs, and adjectives from the Penn Treebank (Marcus et al., 1993), excluding words with fewer than 10 occurrences.\nVerb embeddings. Our probing strategy is based on the assumption that the contextual embedding for a verb captures its meaning in context. Therefore, if LMs associate ASCs with meaning, we should expect the contextual embedding for the Jabberwocky verb to contain the meaning of the construction. Specifically, we measure the Euclidean distance to a prototype verb for each construction (Figure 4). These are verbs that Johnson and Goldberg (2013) selected whose meaning closely resembles the construction’s meaning: gave, made, put, and took for the ditransitive, resultative, caused-motion, and removal constructions, respectively. We also run the same setup using lower frequency prototype verbs from the same study: handed, turned, placed, and removed.3 As\n3Johnson and Goldberg (2013) also included a third experimental condition using four verbs that are semantically related but not associated with the construction, but one of the verbs\na control, we measure the Euclidean distance to the prototype verbs of the other three unrelated constructions.\nThe prototype verb embeddings are generated by taking the average across their contextual embeddings across a 4M-word subset of the British National Corpus (Leech, 1992). We use the secondto-last layer of RoBERTa-base, and in cases where a verb is split into multiple subwords, we take the embedding of the first subword token as the verb embedding."
    }, {
      "heading" : "5.2 Results and interpretation",
      "text" : "We find that the Euclidean distance between the prototype and Jabberwocky verb embeddings is significantly lower (p < .001) when the verb is congruent with the construction than when they are incongruent, and this is observed for both high and low-frequency prototype verbs (Figure 5). Examining the individual constructions and verbs (Figure 6), we note that in the high-frequency scenario, the lowest distance prototype verb is always the congruent one, for all four constructions. In the low-frequency scenario, the result is less consistent: the congruent verb is not always the lowest distance one, although it is always still at most the second-lowest distance out of the four.\nThe randomized experiment design ensures that the Jabberwocky words cannot be lexically biased towards any construction, since each verb is equally likely to occur in every construction. Thus, there is substantial evidence that RoBERTa is able to associate abstract constructional templates with their meaning, without relying on lexical cues.4 This re-\nis very low-frequency (ousted), so we exclude this condition in our experiment.\n4Technically, the lexical content in the four constructions are not identical: i.e., the reader may wonder whether words\nHigh frequency Low frequency\nsult is perhaps surprising, given that previous work found that LMs are relatively insensitive to word order in compositional phrases (Yu and Ettinger, 2020) and downstream inference tasks (Sinha et al., 2021; Pham et al., 2021), where their performance can be largely attributed to lexical overlap.\nThe main result holds for both high and lowfrequency scenarios, but the correct prototype verb is associated more consistently in the highfrequency case. This agrees with Wei et al. (2021), who found that LMs have greater difficulty learning the linguistic properties of less frequent words. We also note that the Euclidean distances are higher overall in the low-frequency scenario, which is consistent with previous work that found lower frequency words to occupy a peripheral region of the embedding space (Li et al., 2021).\nOne confounding factor in this experiment is our assumption that RoBERTa’s contextual embeddings represent word meaning, when in reality, they contain a mixture of syntactic and semantic information. Contextual embeddings are known to contain syntax trees (Hewitt and Manning, 2019) and linguistic information about neighboring words in a sentence (Klafka and Ettinger, 2020); although previous work did not consider ASCs, it is plausible that our verb embeddings leak information about the sentence’s construction in a similar manner. If this were the case, the prototype verb em-\nlike “from” (occurring only in the removal construction) or “on” (in the caused-motion construction) may provide hints to the sentence meaning. However, the ditransitive and resultative constructions do not contain any such informative words, yet RoBERTa still associates the correct prototype verb for these constructions, so we consider it unlikely to be relying solely on lexical cues.\nbedding for gave would contain not only the semantics of transfer that we intended, but also information about its usual syntactic form5 of “S gave NP1 NP2”, and both would be captured by our Euclidean distance measurement. Controlling for this syntactic confound is difficult – one could alternatively probe for transfer semantics without syntactic confounds using a natural language inference setup (e.g., whether the sentence entails the statement “NP1 received NP2”), but we leave further exploration of this idea to future work."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We find evidence for argument structure constructions in Transformer language models from two separate angles: sentence sorting and Jabberwocky construction experiments. Our work extends the existing body of literature on LM probing by taking a constructionist instead of generative approach to linguistic probing. Our sentence sorting experiments identified a striking resemblance between humans’ and LMs’ internal language representations as LMs are exposed to increasing quantities of data, despite the differences between neural language models and the human brain. Our two studies suggest that LMs are able to derive meaning from abstract constructional templates with minimal lexical overlap. Both sets of experiments were inspired by psycholinguistic studies, which we adapted to fit the capabilities of LMs – this illustrates the potential for future work on grounding LM probing methodologies in psycholinguistic research.\n5Bresnan and Nikitina (2003) estimated that 87% of usages of the word “give” occur in the ditransitive construction."
    }, {
      "heading" : "B Additional experimental stimuli",
      "text" : "Table 3 shows an example set of templategenerated stimuli for sentence sorting: we generate 1000 similar sets of 16 sentences to increase the sample size. We also present the sentence sorting stimuli for German (Table 4), Italian (Table 5), and Spanish (Table 6). German uses the same four constructions as English. Italian does not have the ditransitive construction but instead uses the prepositional dative construction to express transfer semantics. Spanish has no equivalents for the causedmotion and resultative constructions, so the authors in that experiment instead used the unplanned reflexive (expressing accidental or unplanned events), and the middle construction (expressing states pertaining to the subject)."
    } ],
    "references" : [ {
      "title" : "Constructions at work in foreign language learners’ mind: A comparison between two sentence-sorting experiments with English and Italian learners",
      "author" : [ "Annalisa Baicchi", "Paolo Della Putta." ],
      "venue" : "Review of Cognitive Linguistics. Published under the",
      "citeRegEx" : "Baicchi and Putta.,? 2019",
      "shortCiteRegEx" : "Baicchi and Putta.",
      "year" : 2019
    }, {
      "title" : "The contribution of argument structure constructions to sentence meaning",
      "author" : [ "Giulia ML Bencini", "Adele E Goldberg." ],
      "venue" : "Journal of Memory and Language, 43(4):640–651.",
      "citeRegEx" : "Bencini and Goldberg.,? 2000",
      "shortCiteRegEx" : "Bencini and Goldberg.",
      "year" : 2000
    }, {
      "title" : "Framing sentences",
      "author" : [ "Kathryn Bock", "Helga Loebell." ],
      "venue" : "Cognition, 35(1):1–39.",
      "citeRegEx" : "Bock and Loebell.,? 1990",
      "shortCiteRegEx" : "Bock and Loebell.",
      "year" : 1990
    }, {
      "title" : "The gradience of the dative alternation",
      "author" : [ "Joan Bresnan", "Tatiana Nikitina." ],
      "venue" : "Unpublished manuscript, Stanford University.",
      "citeRegEx" : "Bresnan and Nikitina.,? 2003",
      "shortCiteRegEx" : "Bresnan and Nikitina.",
      "year" : 2003
    }, {
      "title" : "Spanish pre-trained BERT model and evaluation data",
      "author" : [ "José Cañete", "Gabriel Chaperon", "Rodrigo Fuentes", "JouHui Ho", "Hojin Kang", "Jorge Pérez." ],
      "venue" : "PML4DC at ICLR 2020.",
      "citeRegEx" : "Cañete et al\\.,? 2020",
      "shortCiteRegEx" : "Cañete et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspects of the Theory of Syntax",
      "author" : [ "Noam Chomsky." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Chomsky.,? 1965",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1965
    }, {
      "title" : "Lectures on Government and Binding",
      "author" : [ "Noam Chomsky." ],
      "venue" : "Foris Publications, Dordrecht.",
      "citeRegEx" : "Chomsky.,? 1981",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1981
    }, {
      "title" : "Knowledge of Language",
      "author" : [ "Noam Chomsky." ],
      "venue" : "Praeger, New York.",
      "citeRegEx" : "Chomsky.,? 1986",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1986
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Annotating causal language using corpus lexicography of constructions",
      "author" : [ "Jesse Dunietz", "Lori Levin", "Jaime G Carbonell." ],
      "venue" : "Proceedings of The 9th Linguistic Annotation Workshop, pages 188– 196.",
      "citeRegEx" : "Dunietz et al\\.,? 2015",
      "shortCiteRegEx" : "Dunietz et al\\.",
      "year" : 2015
    }, {
      "title" : "Computational learning of construction grammars",
      "author" : [ "Jonathan Dunn." ],
      "venue" : "Language and Cognition, 9(2):254–292.",
      "citeRegEx" : "Dunn.,? 2017",
      "shortCiteRegEx" : "Dunn.",
      "year" : 2017
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Regularity and idiomaticity in grammatical constructions: The case of let alone",
      "author" : [ "Charles J. Fillmore", "Paul Kay", "Mary Catherine O’Connor" ],
      "venue" : null,
      "citeRegEx" : "Fillmore et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Fillmore et al\\.",
      "year" : 1988
    }, {
      "title" : "Constructions: A construction grammar approach to argument structure",
      "author" : [ "Adele E Goldberg." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Goldberg.,? 1995",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 1995
    }, {
      "title" : "Constructions at Work: The Nature of Generalization in Language",
      "author" : [ "Adele E. Goldberg." ],
      "venue" : "Oxford University Press, Oxford.",
      "citeRegEx" : "Goldberg.,? 2006",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2006
    }, {
      "title" : "Do foreign language learners also have constructions",
      "author" : [ "Stefan Th Gries", "Stefanie Wulff" ],
      "venue" : "Annual Review of Cognitive Linguistics,",
      "citeRegEx" : "Gries and Wulff.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gries and Wulff.",
      "year" : 2005
    }, {
      "title" : "Colorless green recurrent networks dream hierarchically",
      "author" : [ "Kristina Gulordava", "Piotr Bojanowski", "Édouard Grave", "Tal Linzen", "Marco Baroni." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Gulordava et al\\.,? 2018",
      "shortCiteRegEx" : "Gulordava et al\\.",
      "year" : 2018
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "A systematic assessment of syntactic generalization in neural language models",
      "author" : [ "Jennifer Hu", "Jon Gauthier", "Peng Qian", "Ethan Wilcox", "Roger Levy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Identification of caused motion construction",
      "author" : [ "Jena D Hwang", "Martha Palmer." ],
      "venue" : "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 51–60.",
      "citeRegEx" : "Hwang and Palmer.,? 2015",
      "shortCiteRegEx" : "Hwang and Palmer.",
      "year" : 2015
    }, {
      "title" : "Evidence for automatic accessing of constructional meaning: Jabberwocky sentences prime associated verbs",
      "author" : [ "Matt A Johnson", "Adele E Goldberg." ],
      "venue" : "Language and Cognitive Processes, 28(10):1439–1452.",
      "citeRegEx" : "Johnson and Goldberg.,? 2013",
      "shortCiteRegEx" : "Johnson and Goldberg.",
      "year" : 2013
    }, {
      "title" : "Lexical functional grammar: A formal system for grammatical representation",
      "author" : [ "R.M. Kaplan", "Joan Bresnan." ],
      "venue" : "Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173–282. MIT Press, Cambridge, MA.",
      "citeRegEx" : "Kaplan and Bresnan.,? 1982",
      "shortCiteRegEx" : "Kaplan and Bresnan.",
      "year" : 1982
    }, {
      "title" : "Constructing meaning: The role of affordances and grammatical constructions in sentence comprehension",
      "author" : [ "Michael P Kaschak", "Arthur M Glenberg." ],
      "venue" : "Journal of memory and language, 43(3):508– 529.",
      "citeRegEx" : "Kaschak and Glenberg.,? 2000",
      "shortCiteRegEx" : "Kaschak and Glenberg.",
      "year" : 2000
    }, {
      "title" : "Grammatical constructions and linguistic generalizations: The What’s X Doing Y? construction",
      "author" : [ "Paul Kay", "Charles J. Fillmore." ],
      "venue" : "Language, 75(1):1–33. 9",
      "citeRegEx" : "Kay and Fillmore.,? 1999",
      "shortCiteRegEx" : "Kay and Fillmore.",
      "year" : 1999
    }, {
      "title" : "The psychological reality of argument structure constructions: A visual world eye tracking study",
      "author" : [ "Simon Kirsch." ],
      "venue" : "Unpublished MSc thesis, University of Freiburg.",
      "citeRegEx" : "Kirsch.,? 2019",
      "shortCiteRegEx" : "Kirsch.",
      "year" : 2019
    }, {
      "title" : "Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words",
      "author" : [ "Josef Klafka", "Allyson Ettinger." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Klafka and Ettinger.,? 2020",
      "shortCiteRegEx" : "Klafka and Ettinger.",
      "year" : 2020
    }, {
      "title" : "beware the Jabberwock, dear reader!” Testing the distributional reality of construction semantics",
      "author" : [ "Gianluca E Lebani", "Alessandro Lenci." ],
      "venue" : "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), pages 8–18.",
      "citeRegEx" : "Lebani and Lenci.,? 2016",
      "shortCiteRegEx" : "Lebani and Lenci.",
      "year" : 2016
    }, {
      "title" : "100 million words of English: the British National Corpus (BNC)",
      "author" : [ "Geoffrey Neil Leech." ],
      "venue" : "Language Research, 28:1–13.",
      "citeRegEx" : "Leech.,? 1992",
      "shortCiteRegEx" : "Leech.",
      "year" : 1992
    }, {
      "title" : "Unaccusativity in the Syntax-Lexical Semantics Interface",
      "author" : [ "Beth Levin", "Malka Rappaport Hovav." ],
      "venue" : "MIT Press, Cambridge, MA.",
      "citeRegEx" : "Levin and Hovav.,? 1995",
      "shortCiteRegEx" : "Levin and Hovav.",
      "year" : 1995
    }, {
      "title" : "Argument Realization",
      "author" : [ "Beth Levin", "Malka Rappaport Hovav." ],
      "venue" : "Cambridge University Press, Cambridge, UK.",
      "citeRegEx" : "Levin and Hovav.,? 2005",
      "shortCiteRegEx" : "Levin and Hovav.",
      "year" : 2005
    }, {
      "title" : "How is BERT surprised? Layerwise detection of linguistic anomalies",
      "author" : [ "Bai Li", "Zining Zhu", "Guillaume Thomas", "Yang Xu", "Frank Rudzicz." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Sentence comprehension by Chinese learners of English: Verb-centered or construction-based? Unpublished MA thesis, Guangdong University of Foreign Studies",
      "author" : [ "Junying Liang" ],
      "venue" : null,
      "citeRegEx" : "Liang.,? \\Q2002\\E",
      "shortCiteRegEx" : "Liang.",
      "year" : 2002
    }, {
      "title" : "Syntactic structure from deep learning",
      "author" : [ "Tal Linzen", "Marco Baroni." ],
      "venue" : "Annual Review of Linguistics, 7:195–212.",
      "citeRegEx" : "Linzen and Baroni.,? 2021",
      "shortCiteRegEx" : "Linzen and Baroni.",
      "year" : 2021
    }, {
      "title" : "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:521– 535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Probing across time: What does RoBERTa know and when",
      "author" : [ "Leo Z Liu", "Yizhong Wang", "Jungo Kasai", "Hannaneh Hajishirzi", "Noah A Smith" ],
      "venue" : "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Findings",
      "citeRegEx" : "Liu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Linguistic knowledge and transferability of contextual",
      "author" : [ "Nelson F Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E Peters", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "CxGBERT: BERT meets construction grammar",
      "author" : [ "Harish Tayyar Madabushi", "Laurence Romain", "Dagmar Divjak", "Petar Milin." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 4020–4032.",
      "citeRegEx" : "Madabushi et al\\.,? 2020",
      "shortCiteRegEx" : "Madabushi et al\\.",
      "year" : 2020
    }, {
      "title" : "Building a large annotated corpus of English: The Penn Treebank",
      "author" : [ "Mitchell Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz" ],
      "venue" : null,
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "How well does surprisal explain N400 amplitude under different experimental conditions",
      "author" : [ "James Michaelov", "Benjamin Bergen" ],
      "venue" : "In Proceedings of the 24th Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Michaelov and Bergen.,? \\Q2020\\E",
      "shortCiteRegEx" : "Michaelov and Bergen.",
      "year" : 2020
    }, {
      "title" : "Exploring BERT’s sensitivity to lexical cues using tests from semantic priming",
      "author" : [ "Kanishka Misra", "Allyson Ettinger", "Julia Rayz." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages",
      "citeRegEx" : "Misra et al\\.,? 2020",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2020
    }, {
      "title" : "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
      "author" : [ "Thang Pham", "Trung Bui", "Long Mai", "Anh Nguyen" ],
      "venue" : null,
      "citeRegEx" : "Pham et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2021
    }, {
      "title" : "Structural priming: a critical review",
      "author" : [ "Martin J Pickering", "Victor S Ferreira." ],
      "venue" : "Psychological bulletin, 134(3):427.",
      "citeRegEx" : "Pickering and Ferreira.,? 2008",
      "shortCiteRegEx" : "Pickering and Ferreira.",
      "year" : 2008
    }, {
      "title" : "Information-Based Syntax and Semantics, volume 13",
      "author" : [ "Carl Pollard", "Ivan A. Sag." ],
      "venue" : "CSLI, Stanford.",
      "citeRegEx" : "Pollard and Sag.,? 1987",
      "shortCiteRegEx" : "Pollard and Sag.",
      "year" : 1987
    }, {
      "title" : "Using priming to uncover the organization of syntactic representations in neural language models",
      "author" : [ "Grusha Prasad", "Marten van Schijndel", "Tal Linzen." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages",
      "citeRegEx" : "Prasad et al\\.,? 2019",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2019
    }, {
      "title" : "The Empirical Base of Linguistics: Grammaticality Judgments and Linguistic Methodology",
      "author" : [ "Carson T. Schütze." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Schütze.,? 1996",
      "shortCiteRegEx" : "Schütze.",
      "year" : 1996
    }, {
      "title" : "UnNatural Language Inference",
      "author" : [ "Koustuv Sinha", "Prasanna Parthasarathi", "Joelle Pineau", "Adina Williams." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
      "citeRegEx" : "Sinha et al\\.,? 2021",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning argument structure generalizations in a foreign language",
      "author" : [ "Montserrat Martínez Vázquez." ],
      "venue" : "Vigo International Journal of Applied Linguistics, (1):151–165.",
      "citeRegEx" : "Vázquez.,? 2004",
      "shortCiteRegEx" : "Vázquez.",
      "year" : 2004
    }, {
      "title" : "BLiMP: The benchmark of linguistic minimal pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Warstadt et al\\.,? 2020a",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually)",
      "author" : [ "Alex Warstadt", "Yian Zhang", "Xiaocheng Li", "Haokun Liu", "Samuel R Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Warstadt et al\\.,? 2020b",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Frequency effects on syntactic rule learning in transformers",
      "author" : [ "Jason Wei", "Dan Garrette", "Tal Linzen", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Wei et al\\.,? 2021",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    }, {
      "title" : "A targeted assessment of incremental processing in neural language models and humans",
      "author" : [ "Ethan Wilcox", "Pranali Vani", "Roger Levy." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
      "citeRegEx" : "Wilcox et al\\.,? 2021",
      "shortCiteRegEx" : "Wilcox et al\\.",
      "year" : 2021
    }, {
      "title" : "Assessing phrasal representation and composition in transformers",
      "author" : [ "Lang Yu", "Allyson Ettinger." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4896–4907.",
      "citeRegEx" : "Yu and Ettinger.,? 2020",
      "shortCiteRegEx" : "Yu and Ettinger.",
      "year" : 2020
    }, {
      "title" : "When do you need billions of words of pretraining data",
      "author" : [ "Yian Zhang", "Alex Warstadt", "Haau-Sing Li", "Samuel R Bowman" ],
      "venue" : "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "How abstract is syntax? Evidence from structural priming",
      "author" : [ "Jayden Ziegler", "Giulia Bencini", "Adele Goldberg", "Jesse Snedeker." ],
      "venue" : "Cognition, 193:104045. 11",
      "citeRegEx" : "Ziegler et al\\.,? 2019",
      "shortCiteRegEx" : "Ziegler et al\\.",
      "year" : 2019
    }, {
      "title" : "Table 4: German sentence sorting stimuli, obtained from Kirsch (2019)",
      "author" : [ "Theresa nahm das Plakat herunter" ],
      "venue" : null,
      "citeRegEx" : "herunter.,? \\Q2019\\E",
      "shortCiteRegEx" : "herunter.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Pretrained Transformer-based language models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 36,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019b) have recently achieved impressive results on many natural language tasks, spawning a new interdisciplinary field of aligning LMs with linguistic theory and probing the linguistic capabilities of LMs (Linzen and Baroni, 2021).",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : ", 2019b) have recently achieved impressive results on many natural language tasks, spawning a new interdisciplinary field of aligning LMs with linguistic theory and probing the linguistic capabilities of LMs (Linzen and Baroni, 2021).",
      "startOffset" : 208,
      "endOffset" : 233
    }, {
      "referenceID" : 5,
      "context" : "Most probing work so far has assumed a transformational generative syntactic framework (Chomsky, 1965), accounting for phenomena such as agreement, binding, licensing, and movement (Warstadt et al.",
      "startOffset" : 87,
      "endOffset" : 102
    }, {
      "referenceID" : 48,
      "context" : "Most probing work so far has assumed a transformational generative syntactic framework (Chomsky, 1965), accounting for phenomena such as agreement, binding, licensing, and movement (Warstadt et al., 2020a; Hu et al., 2020) with a Transitive Bob cut the bread",
      "startOffset" : 181,
      "endOffset" : 222
    }, {
      "referenceID" : 18,
      "context" : "Most probing work so far has assumed a transformational generative syntactic framework (Chomsky, 1965), accounting for phenomena such as agreement, binding, licensing, and movement (Warstadt et al., 2020a; Hu et al., 2020) with a Transitive Bob cut the bread",
      "startOffset" : 181,
      "endOffset" : 222
    }, {
      "referenceID" : 45,
      "context" : "particular focus on determining whether a sentence is linguistically acceptable (Schütze, 1996).",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Lexicalist theories were long dominant in generative grammar (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987).",
      "startOffset" : 61,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "Lexicalist theories were long dominant in generative grammar (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987).",
      "startOffset" : 61,
      "endOffset" : 125
    }, {
      "referenceID" : 43,
      "context" : "Lexicalist theories were long dominant in generative grammar (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987).",
      "startOffset" : 61,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "In lexicalist theories, argument structure is assumed to be encoded in the lexical entry of the verb: for example, the verb visit is lexically specified as being transitive and as requiring a noun phrase object (Chomsky, 1986).",
      "startOffset" : 211,
      "endOffset" : 226
    }, {
      "referenceID" : 13,
      "context" : "The argument structure of a verb is determined by pairing it with an ASC (Goldberg, 1995).",
      "startOffset" : 73,
      "endOffset" : 89
    }, {
      "referenceID" : 55,
      "context" : "2005), priming (Ziegler et al., 2019), and novel verb experiments (Kaschak and Glenberg, 2000; Johnson and Goldberg, 2013).",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : ", 2019), and novel verb experiments (Kaschak and Glenberg, 2000; Johnson and Goldberg, 2013).",
      "startOffset" : 36,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : ", 2019), and novel verb experiments (Kaschak and Glenberg, 2000; Johnson and Goldberg, 2013).",
      "startOffset" : 36,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Our first case study is based on sentence sorting (Bencini and Goldberg, 2000); we discover that in English, German, Italian, and Spanish, LMs consider sentences that share the same construction to be more",
      "startOffset" : 50,
      "endOffset" : 78
    }, {
      "referenceID" : 31,
      "context" : "Human experiments with non-native speakers found a similarly increased preference for constructional meaning in more proficient speakers (Liang, 2002; Baicchi and Della Putta, 2019), suggesting commonalities in language acquisition between LMs and humans.",
      "startOffset" : 137,
      "endOffset" : 181
    }, {
      "referenceID" : 20,
      "context" : "Our second case study is based on nonsense “Jabberwocky” sentences that nevertheless contain meaning when they are arranged in constructional templates (Johnson and Goldberg, 2013).",
      "startOffset" : 152,
      "endOffset" : 180
    }, {
      "referenceID" : 53,
      "context" : "This finding offers counter-evidence to earlier claims that LMs are relatively insensitive to word order when constructing sentence meaning (Yu and Ettinger, 2020; Sinha et al., 2021).",
      "startOffset" : 140,
      "endOffset" : 183
    }, {
      "referenceID" : 46,
      "context" : "This finding offers counter-evidence to earlier claims that LMs are relatively insensitive to word order when constructing sentence meaning (Yu and Ettinger, 2020; Sinha et al., 2021).",
      "startOffset" : 140,
      "endOffset" : 183
    }, {
      "referenceID" : 12,
      "context" : "Construction grammar is a family of linguistic theories proposing that all linguistic knowledge consists of constructions: pairings between form and meaning where some aspects of form or meaning are not predictable from its parts (Fillmore et al., 1988; Kay and Fillmore, 1999; Goldberg, 1995, 2006).",
      "startOffset" : 230,
      "endOffset" : 299
    }, {
      "referenceID" : 23,
      "context" : "Construction grammar is a family of linguistic theories proposing that all linguistic knowledge consists of constructions: pairings between form and meaning where some aspects of form or meaning are not predictable from its parts (Fillmore et al., 1988; Kay and Fillmore, 1999; Goldberg, 1995, 2006).",
      "startOffset" : 230,
      "endOffset" : 299
    }, {
      "referenceID" : 13,
      "context" : "ASCs are constructions that specify the argument structure of a verb (Goldberg, 1995).",
      "startOffset" : 69,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "In the lexicalist, verb-centered view, argument structure is a lexical property of the verb, and the main verb of a sentence determines the form and meaning of the sentence (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987; Levin and Rappaport Hovav, 1995).",
      "startOffset" : 173,
      "endOffset" : 270
    }, {
      "referenceID" : 21,
      "context" : "In the lexicalist, verb-centered view, argument structure is a lexical property of the verb, and the main verb of a sentence determines the form and meaning of the sentence (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987; Levin and Rappaport Hovav, 1995).",
      "startOffset" : 173,
      "endOffset" : 270
    }, {
      "referenceID" : 43,
      "context" : "In the lexicalist, verb-centered view, argument structure is a lexical property of the verb, and the main verb of a sentence determines the form and meaning of the sentence (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987; Levin and Rappaport Hovav, 1995).",
      "startOffset" : 173,
      "endOffset" : 270
    }, {
      "referenceID" : 42,
      "context" : "Priming refers to the condition where exposure to a (prior) stimulus influences the response to a later stimulus (Pickering and Ferreira, 2008).",
      "startOffset" : 113,
      "endOffset" : 143
    }, {
      "referenceID" : 48,
      "context" : "This idea was extended by BLiMP (Warstadt et al., 2020a), a suite encom-",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 49,
      "context" : "native approaches include CoLA (Warstadt et al., 2019), which compiled an acceptability benchmark of sentences drawn from linguistic publications, and Gulordava et al.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "Other computational work focused on a few specific constructions, such as identifying caused-motion constructions in corpora (Hwang and Palmer, 2015) and annotating constructions related to causal language (Dunietz et al.",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "Other computational work focused on a few specific constructions, such as identifying caused-motion constructions in corpora (Hwang and Palmer, 2015) and annotating constructions related to causal language (Dunietz et al., 2015).",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 50,
      "context" : "To simulate varying non-native English proficiency levels, we use MiniBERTa models (Warstadt et al., 2020b), trained with 1M, 10M, 100M, and 1B tokens.",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "We also use the base RoBERTa model (Liu et al., 2019b), trained with 30B tokens.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 50,
      "context" : "1LM results are obtained using MiniBERTas (Warstadt et al., 2020b) and RoBERTa (Liu et al.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 36,
      "context" : ", 2020b) and RoBERTa (Liu et al., 2019b) on templated stimuli.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "(Devlin et al., 2019) and a monolingual Transformer LM in each language.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 24,
      "context" : "Figure 3: Multilingual sentence sorting results for German (Kirsch, 2019), Italian (Baicchi and Della Putta, 2019), and Spanish (Vázquez, 2004).",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 47,
      "context" : "Figure 3: Multilingual sentence sorting results for German (Kirsch, 2019), Italian (Baicchi and Della Putta, 2019), and Spanish (Vázquez, 2004).",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 54,
      "context" : "A small quantity of data (10M tokens) is sufficient for LMs to prefer the constructional sort, indicating that ASCs are relatively easy to learn: roughly on par with other types of linguistic knowledge, and requiring less data than commonsense knowledge (Zhang et al., 2021; Liu et al., 2021).",
      "startOffset" : 254,
      "endOffset" : 292
    }, {
      "referenceID" : 34,
      "context" : "A small quantity of data (10M tokens) is sufficient for LMs to prefer the constructional sort, indicating that ASCs are relatively easy to learn: roughly on par with other types of linguistic knowledge, and requiring less data than commonsense knowledge (Zhang et al., 2021; Liu et al., 2021).",
      "startOffset" : 254,
      "endOffset" : 292
    }, {
      "referenceID" : 24,
      "context" : "In the German experiment (Kirsch, 2019), the author hypothesized that the participants were influenced by a different experiment that they had completed before the sentence sorting one.",
      "startOffset" : 25,
      "endOffset" : 39
    }, {
      "referenceID" : 44,
      "context" : "Priming is a standard experimental paradigm in psycholinguistic research, but it is not directly applicable to LMs: existing methods simulate priming either by applying additional fine-tuning (Prasad et al., 2019), or by concatenating sentences that typically do not co-occur in natural text (Misra et al.",
      "startOffset" : 192,
      "endOffset" : 213
    }, {
      "referenceID" : 40,
      "context" : ", 2019), or by concatenating sentences that typically do not co-occur in natural text (Misra et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 38,
      "context" : "We obtain a set of singular nouns, past tense verbs, and adjectives from the Penn Treebank (Marcus et al., 1993), excluding words with fewer than 10 occurrences.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : "beddings across a 4M-word subset of the British National Corpus (Leech, 1992).",
      "startOffset" : 64,
      "endOffset" : 77
    }, {
      "referenceID" : 53,
      "context" : "sult is perhaps surprising, given that previous work found that LMs are relatively insensitive to word order in compositional phrases (Yu and Ettinger, 2020) and downstream inference tasks (Sinha et al.",
      "startOffset" : 134,
      "endOffset" : 157
    }, {
      "referenceID" : 46,
      "context" : "sult is perhaps surprising, given that previous work found that LMs are relatively insensitive to word order in compositional phrases (Yu and Ettinger, 2020) and downstream inference tasks (Sinha et al., 2021; Pham et al., 2021), where their performance can be largely attributed to lexical overlap.",
      "startOffset" : 189,
      "endOffset" : 228
    }, {
      "referenceID" : 41,
      "context" : "sult is perhaps surprising, given that previous work found that LMs are relatively insensitive to word order in compositional phrases (Yu and Ettinger, 2020) and downstream inference tasks (Sinha et al., 2021; Pham et al., 2021), where their performance can be largely attributed to lexical overlap.",
      "startOffset" : 189,
      "endOffset" : 228
    }, {
      "referenceID" : 30,
      "context" : "We also note that the Euclidean distances are higher overall in the low-frequency scenario, which is consistent with previous work that found lower frequency words to occupy a peripheral region of the embedding space (Li et al., 2021).",
      "startOffset" : 217,
      "endOffset" : 234
    }, {
      "referenceID" : 17,
      "context" : "Contextual embeddings are known to contain syntax trees (Hewitt and Manning, 2019) and linguistic information about neighboring words in a sentence (Klafka and Ettinger, 2020); although previous work did not consider ASCs, it is plausible that our verb embeddings leak information about the sentence’s construction in a similar manner.",
      "startOffset" : 56,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "Contextual embeddings are known to contain syntax trees (Hewitt and Manning, 2019) and linguistic information about neighboring words in a sentence (Klafka and Ettinger, 2020); although previous work did not consider ASCs, it is plausible that our verb embeddings leak information about the sentence’s construction in a similar manner.",
      "startOffset" : 148,
      "endOffset" : 175
    } ],
    "year" : 0,
    "abstractText" : "In lexicalist linguistic theories, argument structure is assumed to be predictable from the meaning of verbs. As a result, the verb is the primary determinant of the meaning of a clause. In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs. Two decades of psycholinguistic research have produced substantial empirical evidence in favor of the construction view. Here, we adapt several psycholinguistic studies to probe for the existence of argument structure constructions (ASCs) in Transformerbased language models (LMs). First, using a sentence sorting experiment, we find that sentences sharing the same construction are closer in embedding space than sentences sharing the same verb. Furthermore, LMs increasingly prefer grouping by construction with more input data, mirroring the behaviour of non-native language learners. Second, in a “Jabberwocky” priming-based experiment, we find that LMs associate ASCs with meaning, even in semantically nonsensical sentences. Our work offers the first evidence for ASCs in LMs and highlights the potential to devise novel probing methods grounded in psycholinguistic research.",
    "creator" : null
  }
}
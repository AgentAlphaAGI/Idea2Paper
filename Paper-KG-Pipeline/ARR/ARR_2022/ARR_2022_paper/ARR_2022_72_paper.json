{
  "name" : "ARR_2022_72_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CARETS: A Consistency And Robustness Evaluative Test Suite for VQA",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The task of visual question answering integrates the domains of computer vision and NLP by probing models’ understanding of images through natural language queries. After the introduction of the Visual Question Answering (VQA) benchmark (Antol et al., 2015), subsequent work identified the presence of several superficial correlations and other weaknesses latent in the VQA question gathering process (Goyal et al., 2017; Agrawal et al., 2018), which lead to potentially optimistic evaluations when considering accuracy alone. More recently developed benchmarks (Hudson and Manning, 2019; Goyal et al., 2017; Agrawal et al., 2018) explicitly avoid these weaknesses by introducing question, answer, and image balancing, or distributional shifts. While these efforts provide more difficult benchmarks, a thorough evaluation of model capabilities requires a deeper and more detailed approach.\nTo this end, we introduce CARETS – a Consistency and Robustness Evaluative Test Suite for visual question answering. Inspired by recent work in NLP that generates ‘unit tests’ for models (Ribeiro et al., 2020b), CARETS contains systematically generated VQA tests that evaluate six different capabilities that any VQA model should handle – robustness to question rephrasings, ontological reasoning, symmetrical logic, visual perturbations, question negation, and attribute antonymy. Each test point in CARETS consists of a pair of instances which are small but strategic variations of each other either visually or in the question’s text. This allows us to conduct fine-grained capability evaluations beyond just measuring high-level accuracy scores.\nAcross tests, we generate over 190,000 instance pairs in total using a programmatic approach that fills in templates using ground-truth scene graphs (Krishna et al., 2017) from the GQA (Hudson and Manning, 2019) validation split. We then evaluate six modern VQA models on each test using metrics of overall accuracy, self-consistency and comprehensive accuracy. Self-consistency measures models’ ability to maintain their answer across question variations, while comprehensive accuracy estimates their ability to answer all instance variants correctly.\nOur experiments reveal several interesting findings: (1) most modern VQA systems achieve only middling self-consistency (∼60-80%) which is further not always correlated with their accuracy, (2) all models struggle to comprehend the concept of negation (self-consistency of 18-28% and comprehensive accuracy <17%), and (3) even simple perturbations like changing the order of choices in the question text can induce a substantial drop (10-15%) in performance. Moreover, even stateof-the-art models like LXMERT (Tan and Bansal, 2020) are highly sensitive to the type of questions (binary vs multi-choice) and the number of\nchoices provided. These results reveal several shortcomings in modern VQA systems and hint at potential areas for future improvement. Going beyond our current discoveries, CARETS is an extensible framework and allows for continually adding new capability tests for fine-grained evaluation of future models."
    }, {
      "heading" : "2 Related Work",
      "text" : "VQA evaluation. The textual, visual, and answer biases discovered in the VQA dataset (Antol et al., 2015) spurred on recent work seeking to improve model evaluation for the task by eliminating these biases (Goyal et al., 2017), introducing distributional shifts (Agrawal et al., 2018), requiring model explanations (Li et al., 2018), thoroughly analyzing biases in datasets and models (Manjunatha et al., 2019), or evaluating on different recognition subtasks (Kafle and Kanan, 2017). While debiased and challenging benchmarks are important, their focus on accuracy as the sole evaluation metric leaves much to be desired (Ribeiro et al., 2020a; Kervadec et al., 2020). In contrast, our testbed provides question or image pairs that compares models’ predictions between questions; measuring their accuracy, consistency, and robustness to a variety of text and image perturbations.\nSynthetic VQA. One way in which we can generate diverse and balanced datasets is to generate them synthetically, as is done by (Johnson et al., 2015; Zhang et al., 2016; Hudson and Manning,\n2019). Synthetically generating questions, images, or both, allows fine control over the distribution of questions, answers, and images. As both the CLEVR (Johnson et al., 2015) and GQA (Hudson and Manning, 2019) datasets use image scene graphs for question and label generation, they contain questions combining a variety of required capabilities, including compositionality. We feature real-world images with synthetically generated questions as well, but in contrast to GQA, our evaluation has instance pairs to systematically test a focused set of capabilities, showing that models sometimes still struggle with simpler, non-compositional questions."
    }, {
      "heading" : "Consistency as Model Comprehension.",
      "text" : "Some recent work has sought to evaluate models using consistency and other metrics (Hudson and Manning, 2019; Shah et al., 2019; Ribeiro et al., 2020a; Selvaraju et al., 2020; Bitton et al., 2021). These evaluations often evaluate consistency through question entailment and implication, or simply contrasting examples in the case of (Bitton et al., 2021). While we consider such methods important for evaluating model comprehension, they often combine question types and capabilities, changing the kind of expected answer, or evaluating consistency on a tree or set of entailed questions. While models would ideally be consistent and robust for these more complex types of tests, our approach reveals\nthat models can fail even on simple implications."
    }, {
      "heading" : "3 Fine-grained capability tests",
      "text" : "Our goal is to provide a testbed for fine-grained evaluation of VQA models’ capabilities. To do so, we generate multiple tests, each corresponding to a specific model capability. In contrast to standard VQA benchmarks (Antol et al., 2015; Goyal et al., 2017; Hudson and Manning, 2019), our test sets consist of a pair of original and perturbed instances 〈(I1, q1, a1), (I2, q2, a2)〉, each with an image, a question and an answer. The two instances within a pair differ from each other in a minimal yet carefully constructed way to hone in on a particular capability, similar to BLiMP (Warstadt et al., 2020). A model is then evaluated on its overall accuracy, self-consistency (ability to produce consistent, even if wrong, answers to both instances within a pair), and comprehensive accuracy (ability to answer consistently and correctly for an instance pair).\nOverall, we create six tests corresponding to key capabilities. We provide a high-level description of each test here and describe generation details in Section 4. First, we create four invariance tests1 that use variations of the question phrasing and expect the model to produce the same answer to both questions within an instance pair:\n1. Rephrasing invariance (REPHRASE-INV) measures the model’s understanding of minor, meaning-preserving textual changes, e.g.: “What color is the bottle on the shelf, white or blue?” and “Does the color of the bottle on the shelf seem more white or blue?” 2. Ontological invariance (ONTOLOGICALINV) measures understanding of ontology, e.g. changing a hyponym in: “Do you see a green jacket?” to a hypernym “Do you see any green clothes?” 3. Order invariance (ORDER-INV) measures understanding of logically equivalent questions containing different argument orders, e.g.: “Is the black vehicle a van or a truck?” and “Is the black vehicle a truck and a van?” 4. Visual obfuscation invariance (VISUALINV) measures the model’s answering ability when parts of the image not directly relevant to the visual question are removed. Specifically, we explore blurring, masking and cropping techniques to modify the image.\n1Reusing terminology from Ribeiro et al. (2020b).\nWe also create directional expectation tests to measure model behavior on instance pairs where the answer is expected to change:\n5. Attribute antonym directional expectation (ANTONYM-DIR) measures the model’s understanding of antonyms, e.g., “Do you think that the wood table is short?” and “Do you think that the wood table is long?” 6. Negation directional expectation (NEGATION-DIR) measures a model’s grasp of negation, e.g., “ Are there any apples in this picture?” and “Are there no apples in this picture?"
    }, {
      "heading" : "4 Dataset generation",
      "text" : "Each of the six test datasets start with the generation of ‘original’, unperturbed instances (I1, q1, a1) (Section 4.1). Then, for each such instance, we generate a variation (I2, q2, a2) by either perturbing the original question q1 or image I1 (Section 4.2). Further, each test set is composed of a diverse set of questions. These may broadly be grouped into verification (or binary) questions, with expected answers being either yes or no, and multi-choice questions, with expected answers derived from a list of objects or attributes provided in the question."
    }, {
      "heading" : "4.1 Original instance generation",
      "text" : "Questions for each test are generated from question templates (full list with examples provided in Appendix A.1) grouped into the following types.\nQ1: Object verification (54 templates): e.g., “Is there a <obj> in the image?” Q2: Conjunctive verification (18 templates): e.g., “Is there a <obj1> and a <obj2> in the image?” Q3: Disjunctive verification (18 templates): e.g., “Is there a <obj1> or a <obj2> in the image?” Q4: Attribute verification (25 templates): e.g., “Is the <obj> in the image <attr>?” Q5: Object multi-choice (25 templates): e.g., “Is the <obj-class>, <choices>?” Q6: Attribute multi-choice (39 templates): e.g., “What sort of <attr-class> is the <obj>, <choices>?” Q7: Action multi-choice (28 templates): e.g., “What is the <action-class> that the <obj> doing, <choices>?”\nWords in typewriter font represent template\narguments. Generally, each <obj> argument can be filled by a singular object (“cup”) or an attribute+object (“red cup”) while <attr> arguments are filled with singular attributes (“shiny”). For object verification (Q1), attribute verification (Q4), attribute multi-choice (Q6), and action multi-choice (Q7) questions, some templates let <obj> arguments be filled with an object related to another object (e.g. “red cup on the table”); this type is excluded from conjunctive verification (Q2) and disjunctive verification (Q3) questions to prevent the generation of verbose questions.\nFor the multi-choice questions (Q5, Q6, Q7), <choices> are replaced with a list of 2 or 3 singular objects, attributes, or actions respectively (e.g. “cow or horse” or “wood, metal, or plastic”). The <obj-class> argument is filled with a hypernym of all object choices and always appears with either an attribute or a related object (“black animal”, “animal eating grass”). The <attr-class> argument is filled with the attribute category of all attribute choices (e.g. “material”). Finally, the <action-class> argument is filled with the action category of all action choices (e.g. “sport”).\nQuestion argument generation. The question arguments are generated using images from the validation split of the GQA dataset (Hudson and Manning, 2019) which contains 10,696 images manually annotated with 1,536 different objects and 603 different object attributes.\nTo generate questions, we sample objects and attributes directly from an image’s scene graph annotation to populate a question type’s arguments. For binary question types this results in questions with solely affirmative answers. To produce an answer balanced dataset, we run a second stage of question argument generation for binary questions to generate plausible negative questions with false objects or attributes. We sample false objects from a distribution conditioned on an image’s objects, and optionally sample object attributes from a distribution conditioned on the chosen object.\nFor <choices> arguments, false choices are again generated from a distribution conditioned on the object’s hypernym for Q5 questions, the attribute category for Q6 questions, or the action category for the Q7 questions. We additionally ensure that the generated choices are mutually exclusive (e.g. “tan or beige” would be an invalid generation). To get more diverse multi-choice ques-\ntions, we first generate a large pool of question candidates, and then select only a small number of questions sampled from this pool with sample probabilities inversely proportional to the count of the questions’ hypernym, attribute class, or action class, and the count of the generated answer.\nQuestion argument refinement. To improve the reliability of generated questions, we apply a variety of checks and constraints. For example, when sampling false objects from the conditional distribution, we filter out all objects (and their hypernyms2) present in the scene graph in order to guarantee that the sampled object is truly not present. We also filter out question arguments that are not included in the image scene graph but are sub-parts of objects that are annotated (e.g., “tire” when a “car” is annotated). Finally, we enforce various logical constraints on question arguments to prevent trivial or malformed questions. For example, for conjunctive and disjunctive questions (Q2, Q3), we apply a hypernym exclusion constraint to prevent questions like “Is there a black cat and a black animal in the image?”."
    }, {
      "heading" : "4.2 Perturbed pair generation",
      "text" : "We now describe our procedure for creating perturbed instances (I2, q2, a2) for the six tests. In all tests except visual obfuscation, the image remains unchanged, i.e. I2 = I1.\n(a) Rephrasing invariance. Since each original question q1 was generated using a text template, we simply use a different template of the same type to generate a valid rephrasing q2. The image and answer remain the same, i.e. I1 = I2, a1 = a2 and the model is expected to be invariant to this rephrasing. We apply this to Q1, Q2, Q3, Q5, Q6 and Q7.\n(b) Ontological invariance. Here, we use object verification questions (Q1) only and perform two types of transformations. For positive questions (i.e. a1 = yes), we filter question arguments to only include objects that are valid hyponyms (using WordNet again) and use those to generate a perturbed question q2 by changing the hyponym to a hypernym. For example, q1 = “Do you see a jogging woman?” with a1 = yes is paired with q2 = “Do you see a jogging person?” containing a\n2We use an ontology with hypernym paths generated with WordNet (Miller, 1995). We manually review and revise the default synset annotations from Visual Genome (Krishna et al., 2017) for the entire object vocabulary, and compare to a sample of annotated images to ensure general validity.\nhypernym. Similarly, for negative questions (a1 = no), we filter question arguments to only include valid hypernyms and generate a q2: thus for example, q1 = “Do you see a jogging person” with a1 = no is paired with q2 = “Do you see a jogging woman?” containing a hyponym, a2 = no also.\n(c) Order invariance. Order invariance tests apply to conjunctive verification, disjunctive verification, and all multi-choice question types; models are expected to be invariant to the logical order of arguments. We perturb conjunctive verification and disjunctive verification questions by swapping the questions’ first and second arguments (<obj1>, <obj2>). For multi-choice question types, we perturb instances by generating the <choices> argument with different orders. The answer remains the same in both cases by construction.\n(d) Visual obfuscation invariance. For this test, we let q1 = q2 and a1 = a2 but generate a perturbed image I2 by obscuring parts of I1 that are irrelevant to the question at hand using bounding box annotations from Visual Genome (Krishna et al., 2017). For all true objects in a question, we consider the bounding boxes around these object(s) to be the foreground and all other pixels in the image to be the background. For negative verification questions asking about object(s) not present in the image, we select one (or two) random object bounding box(es) as the foreground and consider everything else to be the background, since focusing on any image region should not affect the model’s answer.3 We then apply five types of perturbations to obscure the background: (i-iii) Gaussian blurring using the soft masking method of (Yang et al., 2021) with light (σ = 3), medium (σ = 6), or heavy (σ = 9) blurring, (iv) Masking with the channel-wise averaged pixel value from the GQA (Hudson and Manning, 2019) training dataset, entirely obscuring the context, and (v) Cropping to the smallest rectangular region including the foreground. Example images are shown in Appendix A.2.\n(e) Negation directional expectation. For the negation directional test, we use object verification, conjunctive verification, and disjunctive verification questions. Each question q1 is perturbed by substituting the original’s text template with a\n3We choose 32× 32 as a minimum bounding box size, shown to be reliably recognized by humans (Torralba et al., 2008).\npaired negated text template to create q2. Since each perturbed question represents the negation of the original, the expected answers a1 6= a2. (f) Attribute antonym directional expectation. We perturb the generated attribute verification questions by changing the <attr> question argument to its antonym using WordNet. All attribute antonym relations are manually curated to remove unintuitive examples; questions with arguments without a valid antonym are discarded. The original and perturbed questions of a pair have opposite answers a1 6= a2."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Human baseline",
      "text" : "To assess the quality, difficulty and validity of the generated tests, we sample 100 question pairs (200 questions) from each question type for the 6 tests and procure 5 annotations per question from workers on Amazon Mechanical Turk. Workers are vetted for a 97% approval rating and minimum of 500 completed HITs. Workers take ∼ 2 minutes per task on average and are compensated $0.50 per task and thus ∼ $15 per hour. Human agreement. In addition to “yes” and “no” for binary questions and the appropriate choices for multi-choice questions, all questions offer an ambiguous option. The human answers are the majority vote among the 5 workers; questions failing to reach majority or with ambiguous as the majority are always counted against accuracy. This process is inspired by the human evaluations of implied question pairs of (Ribeiro et al., 2020a). We report both human and model performance in Section 6."
    }, {
      "heading" : "5.2 Evaluated models",
      "text" : "We evaluate six recent models on our tests, and compare them to human accuracy. Models are trained on the GQA (Hudson and Manning, 2019) balanced training split (using hyperparameters suggested from the original papers). All models, except LXMERT4, are trained and finetuned using the MMF (Singh et al., 2020) library and region of interest (RoI) features from Faster R-CNN (Ren et al., 2015) with a ResNeXt-152 (Xie et al., 2017) backbone pre-trained on the Visual Genome (Krishna et al., 2017) dataset for object-based models. More details are in Appendix A.4.\n4For LXMERT we use the authors’ open source repository at https://github.com/airsplay/lxmert\nModel initialization and pre-training. Of the six models evaluated, a defining characterstic of each model relates to their weight initailizations, image-encoding choice, and the use of multi-modal pre-training. Our most basic model (CNN+LSTM) is randomly initialized and uses no pre-trained components; however, GloVe (Pennington et al., 2014) word embeddings are used for representing tokens. Another class of models use pre-trained image encoders to extract object features from images. Of our models, BAN (Kim et al., 2018) is randomly initialized prior to training but ingests pre-trained Faster R-CNN features which should provide the model with enhanced visual capabilities over the CNN+LSTM model. MultiModal BiTransformer (MMBT) (Kiela et al., 2019), uses similar pre-trained image features as BAN but is further initialized with pre-trained BERT (Devlin et al., 2019) weights prior to training on GQA. The last class of models are multimodal pre-trained models; those that use pretrained image features, are initialized with BERT pre-trained weights, and are pre-trained on multimodal tasks, such as image-based masked language modeling. Models in this class include LXMERT (Tan and Bansal, 2020), ViLBERT (Lu et al., 2019), and VisualBERT (Li et al., 2019)."
    }, {
      "heading" : "5.3 Metrics",
      "text" : "Accuracy (ACC). On our test datasets with K paired instances, we define accuracy as:\n1\n2K K∑ i=1 1[âi1 = a i 1] + 1[â i 2 = a i 2]\nwhere the model answers âia, â i 2 on the original and perturbed questions respectively are compared to the ground truth answers ai1, a i 2. Self-consistency (CONS). We measure selfconsistency of the model predictions across the original and perturbed questions as\n1\nK K∑ i=1 { 1[âi1 = â i 2] on invariance tests 1[âi1 6= âi2] on directional exp. tests Note that these metrics only measure the internal consistency of the model and do not include the ground truth answers ai1, a i 2. Comprehensive accuracy (C-ACC). We define comprehensive accuracy as:\n1\nK K∑ i=1 1[âi1 = a i 1 ∧ âi2 = ai2]\nmeasuring whether model predictions are both accurate and self-consistent across perturbations."
    }, {
      "heading" : "6 Results",
      "text" : "(R1) Modern VQA models are not robust to invariance and directionality tests. Fig 2 details the performance of various models under our suite of tests. Each bar in the figure shows both ACC and C-ACC for the model, with the arrow representing the gap between the two. We first observe that all models achieve significantly lower performance (at least a 8% drop) compared to humans (grey). Even simple tests such as REPHRASEINV (Figure 2(a)) prove to be quite challenging, with models managing < 68% ACC compared to humans’ 86%. On tests like NEGATION-DIR (Figure 2(e)), models only get about 50% accuracy, substantially worse than human scores of 78%.\nMoreover, C-ACC is considerably lower than ACC across the board, with as much as a 35% gap on NEGATION-DIR and 14.5% on ANTONYMDIR tests, even for a state-of-the-art model like LXMERT.5 Even though this gap is smaller on other tests like REPHRASE-INV or VISUAL-INV, the performance drop is still at least 6-7% in most cases. This means that models are not invariant to textual rephrasings of questions and do not have a strong grasp of concepts like attributes and negation, despite negation of attributes appearing in the GQA training dataset.\n(R2) VQA systems are not self-consistent in their predictions. Table 1 shows the selfconsistency scores for all models under our different tests. While humans achieve CONS > 88% in all the tests, VQA models are much worse – at least 6% lower CONS in all cases, with the best\n5Other modern systems like ViLBERT and VisualBERT have even worse C-ACC.\nperforming model (LXMERT) still 26% lower than human performance on average across tests and models. Scores are especially low on the directional tests (antonym and negation), which means that models are confused in their decisions simply with the addition of negation words – this hints at issues of overfitting to spurious feature without understanding the presence or absence of specific concepts, corroborating the findings of (Bitton et al., 2021). Interestingly, the best performing model (LXMERT) is not always the most consistent. Furthermore, there is no single model that is the most self-consistent, with LXMERT, ViLBERT and VisualBERT each returning the highest consistency scores on different tests.\n(R3) Models are more robust to hyponym vs hypernym variations. Breaking out the results on the ontological invariance test (Figure 2 (c)) in the last two columns of Table 2, we see that\nself-consistency is higher on the hyponym perturbations (on negative answer questions) than on hypernym perturbations (positive questions); this effect is particularly noticeable for MMBT and ViLBERT with a 19% and 15% difference, respectively. Thus, when an object is not detected in an image its hyponym elicits a negative response as expected; however when an object (like “steak”) is detected, the hypernym question (“Is there any meat in the image?”) may trip the model to generate a negative response. This points to the need for more structured, hierarchical grounding of concepts in these models.\n(R4) Models perform better on conjunctive rather than disjunctive tests. From Table 3, we note that models generally have higher accuracy on conjunctive rather than disjunctive tests, with the largest discrepancy for LXMERT at 81% accuracy on conjunctive tests vs only 62% on disjunctive. Many models seem to exhibit a strong positive bias for disjunctive questions, suggesting they may just be short-cutting to answering ‘yes’ for disjunctive questions. LXMERT also seems to frequently confuse disjunctive questions for an open-ended or multi-choice question.\n(R5) Models are sensitive to answer types and the number of choices in a question. Table 4 provides a breakdown of LXMERT’s scores for binary and multi-choice questions. It is evident that multi-choice questions are harder for the model, with self-consistency dropping by 16% between\nbinary and multi-choice questions, and C-ACC dropping by 33%. This is surprising since the multi-choice questions only include two or three choices and hence are quite similar to the binary (yes/no) questions. This may indicate a bias in the models towards binary questions with simple answers. Furthermore, Table 4 also shows that models consistently perform worse on 3-choice questions than 2-choice ones, with even the topperforming LXMERT having a 7% drop from 62% to 55%. This hints that there may be some effect of randomness in the way these models pick their answers. In contrast and as expected, humans are robust to the number of choices.\n(R6) Visual perturbations are easier for models to deal with. From Figure 2 and Table 1, we notice that the models are slightly more robust to visual perturbations on average compared to the lexical ones. All models only have a drop of 4-8% from ACC to C-ACC, while self-consistency of all models is also 78% or higher. Appendix A.2 provides a more detailed breakdown of all the different visual perturbation tests we performed.\nSetting an upper bound for CARETS through data augmentation. We construct an upper bound for performance on CARETS by performing data augmentation. We add 95,000 questions generated from CARETS question templates, and using a similar distribution of question types, to the original training split of GQA and re-train the LXMERT model. Table 5 shows that this dramatically improves the model on all three metrics (ACC, self-consistency and C-ACC), with the LXMERT(Augmented) model achieving near human performance on tests like ORDER-INV and ANTONYM-DIR. Since CARETS is designed to be an evaluation suite, these numbers provide an upper bound for training techniques that use models similar to LXMERT; showing that models can perform well on our tests if trained appropriately."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we have developed CARETS – a new test suite for capability-focused robustness testing and comprehension evaluation of visual question answering (VQA) models. CARETS consists of six different tests that use instance pairs to evaluate models on their understanding of various linguistic and visual concepts. Using this test suite, we evaluated six modern VQA systems to reveal several inconsistencies in state-of-theart models and provide a fine-grained view of their comprehension of different visuo-linguistic concepts. Quite surprisingly, we find that even state-of-the-art models struggle with concepts like negation, disjunction, order invariance and multichoice questions. CARETS can also support the addition of more tests in the future and we view it as a platform for continually stress testing and improving VQA models in the future."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Test Dataset Statistics",
      "text" : "In this section we, provide more details on test dataset statistics.\nA.2 Visual Obfuscation Invariance Details\nTable 3 shows examples of context blurring, masking, and cropping. Five perturbations are done in total, blurring (with σ ∈ {3, 6, 9}), masking context by replacing pixel values with the channelwise average computed from the GQA training data, and cropping around the tightest bounding box containing the question’s objects."
    }, {
      "heading" : "A.3 Dataset Error Analysis From Human Evaluations",
      "text" : "We provide an analysis of our instances of humanlabel disagreement.\nWe evaluate human performance by sampling 100 instance pairs per question type for each test dataset. Since human-label agreement isn’t perfect, we seek to identify common reasons for when they disagree, including those with no majority answer or are rated ambiguous. We sample 100 questions that human’s predicted “incorrectly” (10 from each question type for textual perturbation tests. Of the cases that we reviewed, we identify 5 common causes of disagreement:\n1. Missing annotation, the question queried about an object that was in the image but was not properly annotated (i.e. false negative).\n2. Semantically ambiguous, the name used for an annotation is correct but possibly ambiguous or uncommon, leading to confusion (e.g. referring to “nose of an airplane” simply as “nose”).\n3. Visually ambiguous, when it is difficult to concretely answer a question because of visual ambiguity (e.g. an object is blurry or the lighting is poor)\n4. Wrong annotation, when there is an object recorded in an image but in fact there is no such object visible (i.e. false positive).\n5. Unknown, when an a question appears to be unambiguous and the label seems to be correct, but there was disagreement anyway.\nFrom the 100 samples reviewed we recorded 32 missing annotations, 18 semantically ambiguous, 33 visually ambiguous, 7 wrong annotations, 10 unknowns."
    }, {
      "heading" : "A.4 Training Details",
      "text" : "We provide greater detail on training environments and hyperparameter choices.\nAll models are trained using the GQA (Hudson and Manning, 2019) balanced training set and validated on the balanced validation set (with minimal parameter tuning, aiming to stay faithful to the original implementation). All non-LXMERT models are trained and finetuned withing the MMF (Singh et al., 2020) framework, trained using binary cross-entropy loss for a set maximum number of epochs, taking the epoch checkpoint that best performs on the validation set, and using features from Faster R-CNN (Ren et al., 2015) with a ResNext-152 (Xie et al., 2017) backbone pre-trained on the Visual Genome (Krishna et al., 2017) dataset for object-based models. BAN. The Bilinear Attention Network (Kim et al., 2018) (BAN) uses pre-trained Faster RCNN features (Ren et al., 2015) and GloVe (Pennington et al., 2014) embeddings with an attention\nmodel and early bilinear fusion mechanism. We train “four glimpse” (with 4 attention heads) BAN (Kim et al., 2018) (BAN) for 13 epochs using the Adamax (Kingma and Ba, 2015) optimizer with an initial learning rate of 1e−3 and batch size of 256, decaying the learning rate at epochs 11 and 13. As in the original hyperparameter configuration, we perform gradient clipping at 0.25.\nLXMERT. LXMERT (Tan and Bansal, 2020) is a transformer-based architecture (Vaswani et al., 2017) with pre-trained Faster R-CNN features and a BERT-style language encoder (Devlin et al., 2019). It undergoes an extended pre-training procedure using 5 different pre-training tasks, including image question answering. We first pretrain a version of LXMERT from scratch with all GQA validation instances removed from the pretraining data to prevent direct leakage. We use all the default hyperparameters used in the author’s GitHub repository.6 We then finetune LXMERT base (Tan and Bansal, 2020) on the GQA training dataset for 4 epochs with the same hyperparameter configuration as the original implementation, using a batch size of 32, and initial learning rate of 1e−5. LXMERT base is pre-trained using the MS COCO (Lin et al., 2014) and Visual Genome (Krishna et al., 2017) datasets. LXMERT also\n6github.com/airsplay/lxmert\nuses a Faster R-CNN (Ren et al., 2015) with a ResNet-101 (He et al., 2016) backbone.\nVisual BERT. VisualBERT (Li et al., 2019) is similar in architecture and pre-training method to BERT. It performs an early fusion of text and image features immediately before several transformer layers. It uses Faster R-CNN features, is initialized using weights from BERT (Devlin et al., 2019), and pre-trained on 2 different tasks using the MS COCO (Lin et al., 2014) dataset. We finetune an MS COCO (Lin et al., 2014) pretrained version of Visual BERT (Li et al., 2019) using the same hyper parameters and training scheme of the original implementation for the VQA task. We use the Adam W optimizer with an initial learning rate of 2e−5 and a batch size of 64 for a maximum of 20 epochs.\nCNN+LSTM. This model uses a 6 layer CNN module and a bidirectional LSTM module before concatenating the output of each module and passing the combined output to a FC classifier. The LSTM module uses GloVe word embeddings (Pennington et al., 2014). Model weights are randomly initialized. We train the model for 25 epochs using the Adam W optimizer with an initial learning rate of 1e−4 and batch size of 256. This model uses a 6 layer CNN module and a bidirectional LSTM module with a hidden size of 128, and concatenates the output of each module before passing the combined output to 2 layer MLP classifier with a ReLU activation. The LSTM module uses GloVe word embeddings (Pennington et al., 2014) to represent questions.\nMMBT. The MultiModal BiTransformer (MMBT) (Kiela et al., 2019) is an early fusion model, which uses Faster R-CNN features projected to a common space and concatenated with contextual BERT embeddings before being passed to transformer layers. MMBT uses pre-trained Faster R-CNN features and is initialized with BERT pre-trained weights. We finetune MMBT (Kiela et al., 2019) with the Adam W (Kingma and Ba, 2015) optimizer with an initial learning rate of 5e−5 with a batch size of 64, for a maximum of 15 epochs.\nViLBERT. ViLBERT (Lu et al., 2019) uses two parallel transformer “streams” for vision and language separately. These streams interact using multi-modal co-attentional transformer blocks. It uses Faster R-CNN features and is initialized using some weights from BERT (Devlin et al., 2019).\nViLBERT is pre-trained using 2 different tasks, using the MS COCO (Lin et al., 2014) dataset. We finetune the MS COCO (Lin et al., 2014) pretrained version of ViLBERT (Lu et al., 2019) in a similar manner to the finetuning scheme used for the VQA task of the original implementation. We use the Adam W (Kingma and Ba, 2015) optimizer with an initial learning rate of 4e−5 and batch size 64, for a maximum of 20 epochs."
    }, {
      "heading" : "A.5 Test Results",
      "text" : "We provide fuller test results for each test dataset, including accuracy on the original instances and perturbed instances. These results supplement the primary results reported in Section 6."
    }, {
      "heading" : "A.6 Model comparison results",
      "text" : "We provide additional model comparison results for each test dataset in Tables 13, 14, 15, 16, 17."
    } ],
    "references" : [ {
      "title" : "Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
      "author" : [ "Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh", "Aniruddha Kembhavi." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Agrawal et al\\.,? 2018",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2018
    }, {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic generation of contrast sets from scene graphs: Probing the compositional consistency of gqa",
      "author" : [ "Yonatan Bitton", "Gabriel Stanovsky", "Roy Schwartz", "Michael Elhadad." ],
      "venue" : "ArXiv, abs/2103.09591.",
      "citeRegEx" : "Bitton et al\\.,? 2021",
      "shortCiteRegEx" : "Bitton et al\\.",
      "year" : 2021
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "J. Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recog-",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "X. Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "GQA: A new dataset for real-world visual reasoning and compositional question answering",
      "author" : [ "Drew A. Hudson", "Christopher D. Manning." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Image retrieval using scene graphs",
      "author" : [ "Justin Johnson", "Ranjay Krishna", "Michael Stark", "Li Jia Li", "David A. Shamma", "Michael S. Bernstein", "Fei Fei Li." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Johnson et al\\.,? 2015",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2015
    }, {
      "title" : "An analysis of visual question answering algorithms",
      "author" : [ "Kushal Kafle", "Christopher Kanan." ],
      "venue" : "IEEE International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Kafle and Kanan.,? 2017",
      "shortCiteRegEx" : "Kafle and Kanan.",
      "year" : 2017
    }, {
      "title" : "Roses are red, violets are blue.",
      "author" : [ "Corentin Kervadec", "Grigory Antipov", "Moez Baccouche", "Christian Wolf" ],
      "venue" : null,
      "citeRegEx" : "Kervadec et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kervadec et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised multimodal bitransformers for classifying images and text",
      "author" : [ "Douwe Kiela", "Suvrat Bhooshan", "Hamed Firooz", "Ethan Perez", "Davide" ],
      "venue" : "Testuggine",
      "citeRegEx" : "Kiela et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2019
    }, {
      "title" : "Bilinear attention networks",
      "author" : [ "Jin-Hwa Kim", "Jaehyun Jun", "Byoung-Tak Zhang." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc.",
      "citeRegEx" : "Kim et al\\.,? 2018",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Visual genome: Connecting language and vision using crowdsourced dense image",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2017
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions",
      "author" : [ "Qing Li", "Qingyi Tao", "Shafiq Joty", "Jianfei Cai", "Jiebo Luo." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "M. Maire", "Serge J. Belongie", "James Hays", "P. Perona", "D. Ramanan", "Piotr Dollár", "C.L. Zitnick." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Explicit bias discovery in visual question answering models",
      "author" : [ "Varun Manjunatha", "Nirat Saini", "Larry S. Davis." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Manjunatha et al\\.,? 2019",
      "shortCiteRegEx" : "Manjunatha et al\\.",
      "year" : 2019
    }, {
      "title" : "Wordnet: A lexical database for english",
      "author" : [ "George A. Miller." ],
      "venue" : "Commun. ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "J. Sun." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Are red roses red? Evaluating consistency of question-answering models",
      "author" : [ "Marco Tulio Ribeiro", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Confer-",
      "citeRegEx" : "Ribeiro et al\\.,? 2020a",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Ribeiro et al\\.,? 2020b",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Squinting at vqa models: Introspecting vqa models with subquestions",
      "author" : [ "Ramprasaath R. Selvaraju", "Purva Tendulkar", "Devi Parikh", "E. Horvitz", "Marco Túlio Ribeiro", "Besmira Nushi", "Ece Kamar." ],
      "venue" : "IEEE Conference on Computer Vision",
      "citeRegEx" : "Selvaraju et al\\.,? 2020",
      "shortCiteRegEx" : "Selvaraju et al\\.",
      "year" : 2020
    }, {
      "title" : "Cycle-consistency for robust visual question answering",
      "author" : [ "Meet Shah", "Xinlei Chen", "Marcus Rohrbach", "Devi Parikh." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Shah et al\\.,? 2019",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2019
    }, {
      "title" : "Mmf: A multimodal framework for vision and language research",
      "author" : [ "Amanpreet Singh", "Vedanuj Goswami", "Vivek Natarajan", "Yu Jiang", "Xinlei Chen", "Meet Shah", "Marcus Rohrbach", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "https://github.com/",
      "citeRegEx" : "Singh et al\\.,? 2020",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2020
    }, {
      "title" : "LXMert: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Tan and Bansal.,? 2020",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2020
    }, {
      "title" : "80 million tiny images: A large data set for nonparametric object and scene recognition",
      "author" : [ "A. Torralba", "R. Fergus", "W. Freeman." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Torralba et al\\.,? 2008",
      "shortCiteRegEx" : "Torralba et al\\.",
      "year" : 2008
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam M. Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "BLiMP: The Benchmark of Linguistic Minimal Pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational",
      "citeRegEx" : "Warstadt et al\\.,? 2020",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Aggregated residual transformations for deep neural networks",
      "author" : [ "Saining Xie", "Ross B. Girshick", "Piotr Dollár", "Zhuowen Tu", "Kaiming He." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "A study of face obfuscation in imagenet",
      "author" : [ "Kaiyu Yang", "Jacqueline Yau", "Li Fei-Fei", "Jia Deng", "Olga Russakovsky" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Yin and yang: Balancing and answering binary visual questions",
      "author" : [ "P. Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "After the introduction of the Visual Question Answering (VQA) benchmark (Antol et al., 2015), subsequent work identified the presence of several superficial correlations and other weaknesses latent in the VQA question gathering process (Goyal et al.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : ", 2015), subsequent work identified the presence of several superficial correlations and other weaknesses latent in the VQA question gathering process (Goyal et al., 2017; Agrawal et al., 2018), which lead to potentially optimistic evaluations when considering accuracy alone.",
      "startOffset" : 151,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : ", 2015), subsequent work identified the presence of several superficial correlations and other weaknesses latent in the VQA question gathering process (Goyal et al., 2017; Agrawal et al., 2018), which lead to potentially optimistic evaluations when considering accuracy alone.",
      "startOffset" : 151,
      "endOffset" : 193
    }, {
      "referenceID" : 6,
      "context" : "More recently developed benchmarks (Hudson and Manning, 2019; Goyal et al., 2017; Agrawal et al., 2018) explicitly avoid these weaknesses by introducing question, answer, and image balancing, or distributional shifts.",
      "startOffset" : 35,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "More recently developed benchmarks (Hudson and Manning, 2019; Goyal et al., 2017; Agrawal et al., 2018) explicitly avoid these weaknesses by introducing question, answer, and image balancing, or distributional shifts.",
      "startOffset" : 35,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "More recently developed benchmarks (Hudson and Manning, 2019; Goyal et al., 2017; Agrawal et al., 2018) explicitly avoid these weaknesses by introducing question, answer, and image balancing, or distributional shifts.",
      "startOffset" : 35,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "Inspired by recent work in NLP that generates ‘unit tests’ for models (Ribeiro et al., 2020b), CARETS contains systematically generated VQA tests that evaluate six different capabilities that any VQA model should handle – robustness to question rephrasings, ontological reasoning, symmetrical logic, visual perturbations, question negation, and at-",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "Across tests, we generate over 190,000 instance pairs in total using a programmatic approach that fills in templates using ground-truth scene graphs (Krishna et al., 2017) from the GQA (Hudson and Manning, 2019) validation split.",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : ", 2017) from the GQA (Hudson and Manning, 2019) validation split.",
      "startOffset" : 21,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "Moreover, even stateof-the-art models like LXMERT (Tan and Bansal, 2020) are highly sensitive to the type of questions (binary vs multi-choice) and the number of",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "The textual, visual, and answer biases discovered in the VQA dataset (Antol et al., 2015) spurred on recent work seeking to improve model evaluation for the task by eliminating these biases (Goyal et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : ", 2015) spurred on recent work seeking to improve model evaluation for the task by eliminating these biases (Goyal et al., 2017), introducing distributional shifts (Agrawal et al.",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : ", 2017), introducing distributional shifts (Agrawal et al., 2018), requiring model explanations (Li et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : ", 2018), requiring model explanations (Li et al., 2018), thoroughly analyzing biases in datasets and models (Manjunatha et al.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : ", 2018), thoroughly analyzing biases in datasets and models (Manjunatha et al., 2019), or evaluating on different recognition subtasks (Kafle and Kanan, 2017).",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : ", 2019), or evaluating on different recognition subtasks (Kafle and Kanan, 2017).",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 22,
      "context" : "While debiased and challenging benchmarks are important, their focus on accuracy as the sole evaluation metric leaves much to be desired (Ribeiro et al., 2020a; Kervadec et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "While debiased and challenging benchmarks are important, their focus on accuracy as the sole evaluation metric leaves much to be desired (Ribeiro et al., 2020a; Kervadec et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "One way in which we can generate diverse and balanced datasets is to generate them synthetically, as is done by (Johnson et al., 2015; Zhang et al., 2016; Hudson and Manning, 2019).",
      "startOffset" : 112,
      "endOffset" : 180
    }, {
      "referenceID" : 33,
      "context" : "One way in which we can generate diverse and balanced datasets is to generate them synthetically, as is done by (Johnson et al., 2015; Zhang et al., 2016; Hudson and Manning, 2019).",
      "startOffset" : 112,
      "endOffset" : 180
    }, {
      "referenceID" : 6,
      "context" : "One way in which we can generate diverse and balanced datasets is to generate them synthetically, as is done by (Johnson et al., 2015; Zhang et al., 2016; Hudson and Manning, 2019).",
      "startOffset" : 112,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "As both the CLEVR (Johnson et al., 2015) and GQA (Hudson and Manning, 2019) datasets use image scene graphs for question and label generation, they contain questions combining a variety of required capabilities, including compositionality.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and GQA (Hudson and Manning, 2019) datasets use image scene graphs for question and label generation, they contain questions combining a variety of required capabilities, including compositionality.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "Some recent work has sought to evaluate models using consistency and other metrics (Hudson and Manning, 2019; Shah et al., 2019; Ribeiro et al., 2020a; Selvaraju et al., 2020; Bitton et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 196
    }, {
      "referenceID" : 25,
      "context" : "Some recent work has sought to evaluate models using consistency and other metrics (Hudson and Manning, 2019; Shah et al., 2019; Ribeiro et al., 2020a; Selvaraju et al., 2020; Bitton et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 196
    }, {
      "referenceID" : 22,
      "context" : "Some recent work has sought to evaluate models using consistency and other metrics (Hudson and Manning, 2019; Shah et al., 2019; Ribeiro et al., 2020a; Selvaraju et al., 2020; Bitton et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 196
    }, {
      "referenceID" : 24,
      "context" : "Some recent work has sought to evaluate models using consistency and other metrics (Hudson and Manning, 2019; Shah et al., 2019; Ribeiro et al., 2020a; Selvaraju et al., 2020; Bitton et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Some recent work has sought to evaluate models using consistency and other metrics (Hudson and Manning, 2019; Shah et al., 2019; Ribeiro et al., 2020a; Selvaraju et al., 2020; Bitton et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "These evaluations often evaluate consistency through question entailment and implication, or simply contrasting examples in the case of (Bitton et al., 2021).",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "In contrast to standard VQA benchmarks (Antol et al., 2015; Goyal et al., 2017; Hudson and Manning, 2019), our test sets consist of a pair of original and perturbed instances 〈(I1, q1, a1), (I2, q2, a2)〉, each with an image, a question and an answer.",
      "startOffset" : 39,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "In contrast to standard VQA benchmarks (Antol et al., 2015; Goyal et al., 2017; Hudson and Manning, 2019), our test sets consist of a pair of original and perturbed instances 〈(I1, q1, a1), (I2, q2, a2)〉, each with an image, a question and an answer.",
      "startOffset" : 39,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "In contrast to standard VQA benchmarks (Antol et al., 2015; Goyal et al., 2017; Hudson and Manning, 2019), our test sets consist of a pair of original and perturbed instances 〈(I1, q1, a1), (I2, q2, a2)〉, each with an image, a question and an answer.",
      "startOffset" : 39,
      "endOffset" : 105
    }, {
      "referenceID" : 30,
      "context" : "The two instances within a pair differ from each other in a minimal yet carefully constructed way to hone in on a particular capability, similar to BLiMP (Warstadt et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "The question arguments are generated using images from the validation split of the GQA dataset (Hudson and Manning, 2019) which contains 10,696 images manually annotated with 1,536 different objects and 603 different object attributes.",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "We use an ontology with hypernym paths generated with WordNet (Miller, 1995).",
      "startOffset" : 62,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "We manually review and revise the default synset annotations from Visual Genome (Krishna et al., 2017) for the entire object vocabulary, and compare to a sample of annotated images to ensure general validity.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "For this test, we let q1 = q2 and a1 = a2 but generate a perturbed image I2 by obscuring parts of I1 that are irrelevant to the question at hand using bounding box annotations from Visual Genome (Krishna et al., 2017).",
      "startOffset" : 195,
      "endOffset" : 217
    }, {
      "referenceID" : 32,
      "context" : "3 We then apply five types of perturbations to obscure the background: (i-iii) Gaussian blurring using the soft masking method of (Yang et al., 2021) with light (σ = 3), medium (σ = 6), or heavy (σ = 9) blurring, (iv) Masking with the channel-wise averaged pixel value from the GQA (Hudson and Manning, 2019) training dataset, entirely obscuring the context, and (v) Cropping to the smallest rectangular region including the foreground.",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : ", 2021) with light (σ = 3), medium (σ = 6), or heavy (σ = 9) blurring, (iv) Masking with the channel-wise averaged pixel value from the GQA (Hudson and Manning, 2019) training dataset, entirely obscuring the context, and (v) Cropping to the smallest rectangular region including the foreground.",
      "startOffset" : 140,
      "endOffset" : 166
    }, {
      "referenceID" : 28,
      "context" : "We choose 32× 32 as a minimum bounding box size, shown to be reliably recognized by humans (Torralba et al., 2008).",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "This process is inspired by the human evaluations of implied question pairs of (Ribeiro et al., 2020a).",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "Models are trained on the GQA (Hudson and Manning, 2019) balanced training split (using hyperparameters suggested from the original papers).",
      "startOffset" : 30,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "All models, except LXMERT4, are trained and finetuned using the MMF (Singh et al., 2020) library and region of interest (RoI) features from Faster R-CNN (Ren et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : ", 2020) library and region of interest (RoI) features from Faster R-CNN (Ren et al., 2015) with a ResNeXt-152 (Xie et al.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 31,
      "context" : ", 2015) with a ResNeXt-152 (Xie et al., 2017) backbone pre-trained on the Visual Genome (Krishna et al.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : ", 2017) backbone pre-trained on the Visual Genome (Krishna et al., 2017) dataset for object-based models.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Of our models, BAN (Kim et al., 2018) is randomly initialized prior to training but ingests pre-trained Faster R-CNN features which should provide the model with enhanced visual capabilities over the CNN+LSTM model.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "MultiModal BiTransformer (MMBT) (Kiela et al., 2019), uses similar pre-trained image features as BAN but is further initialized with pre-trained BERT (Devlin et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : ", 2019), uses similar pre-trained image features as BAN but is further initialized with pre-trained BERT (Devlin et al., 2019) weights prior to training on GQA.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "Models in this class include LXMERT (Tan and Bansal, 2020), ViLBERT (Lu et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "Models in this class include LXMERT (Tan and Bansal, 2020), ViLBERT (Lu et al., 2019), and VisualBERT (Li et al.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Scores are especially low on the directional tests (antonym and negation), which means that models are confused in their decisions simply with the addition of negation words – this hints at issues of overfitting to spurious feature without understanding the presence or absence of specific concepts, corroborating the findings of (Bitton et al., 2021).",
      "startOffset" : 330,
      "endOffset" : 351
    } ],
    "year" : 0,
    "abstractText" : "We introduce CARETS, a systematic test suite to measure consistency and robustness of modern VQA models through a series of six fine-grained capability tests. In contrast to existing VQA test sets, CARETS features balanced question generation to create pairs of instances to test models, with each pair focusing on a specific capability such as rephrasing, logical symmetry or image obfuscation. We evaluate six modern VQA systems on CARETS and identify several actionable weaknesses in model comprehension, especially with concepts such as negation, disjunction, or hypernym invariance. Interestingly, even the most sophisticated models are sensitive to aspects such as swapping the order of terms in a conjunction or changing the number of answer choices mentioned in the question. We release CARETS to be used as an extensible tool for evaluating multimodal model robustness.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_341_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Entity-based Neural Local Coherence Modeling",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Coherence describes the semantic relation between elements of a text. It recognizes how well a text is organized to convey the information to the reader effectively. Modeling coherence can be beneficial to any system which needs to process a text.\nRecent neural coherence models (Mesgar and Strube, 2018; Moon et al., 2019) encode the input\n1Our implementation will be publicly available upon publication.\ndocument using large-scale pretrained language models (Peters et al., 2018). These neural models compute local coherence, semantic relations between items in adjacent sentences, on the basis of words and even sub-words.\nHowever, it has been unclear on which basis these models compute local coherence. Jeon and Strube (2020) present a neural coherence model, which allows to interpret focus information for the first time. Their investigation reveals that neural models, adopting large-scale pretrained language models, frequently compute coherence on the basis of connections between any (sub-)words or function words. In these cases, the model might capture the focus based on spurious information. While such a model might reach or set the state of the art in some end applications, it will do so for the wrong reasons from a linguistic perspective.\nThis problem did not appear with pre-neural models of coherence, since they compute coherence on the basis of entities. Early work about pronoun and anaphora resolution by Sidner (1981, 1983) assumes that there is one single salient entity in a sentence, its focus, which serves as a preferred antecedent for anaphoric expressions. Centering theory (Joshi and Weinstein, 1981; Grosz et al., 1995) builds on these insights and introduces an algorithm for tracking changes in focus. Centering theory serves as basis for many researchers to develop systems computing local coherence based on the approximations of entities (Barzilay and Lapata 2008; Feng and Hirst 2012; Guinaudeau and Strube 2013, inter alia).\nIn this paper, we propose a neural coherence model which is linguistically more sound than previously proposed neural coherence models. We compute coherence on the basis of entities by constraining our model to capture focus on noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences, leading to the notion of focus.\nThis brings our model linguistically in line with pre-neural models of coherence.\nOur approach is not only linguistically more sound but also is in accord with the recent empirical study by O’Connor and Andreas (2021) who investigate what contextual information contributes to accurate predictions in transformer-based language models. Their experiments show that most usable information is captured by nouns and verbs. Their findings suggest that we can design better neural models by focusing on specific context words. Our work follows their findings by modeling entitybased coherence in an end-to-end framework to improve a neural coherence model.\nOur model integrates a local coherence module with a component which takes context into account. Our model first encodes a document using a pretrained language model and identifies entities using an linguistic parser. The local coherence module captures the most related representations of entities between adjacent sentences, the local focus. Then it tracks the changes of local foci. The second component captures the context of a text by averaging sentence representations.\nWe evaluate our model on three downstream tasks: automated essay scoring (AES), assessing writing quality (AWQ), and assessing discourse coherence (ADC). AES and AWQ determine text quality for a given text, aiming to replicate human scoring results. Since coherence is an essential factor in assessing text quality, many previous coherence models are evaluated on AES and AWQ. ADC evaluates coherence models on informal texts such as emails and online reviews. In our evaluation, our model achieves state-of-the-art performance.\nWe also perform a series of analyses to investigate how our model works. Our analyses show that capturing focus on entities gives us better insight into the behaviour of the model, leading to better explainability. Using this information, we examine the statistical differences of texts assigned to different qualities. From the perspective of local coherence, we find that texts of higher quality are neither semantically too consistent nor too variant. Finally, we inspect error cases to investigate how the models achieve their performance differently."
    }, {
      "heading" : "2 Related Work",
      "text" : "Entity-based modeling has been the prevailing approach to model coherence in pre-neural models. The entity grid is its most well-known implementa-\ntion (Barzilay and Lapata, 2008). It represents entities in a two-dimensional array to track their transitions between sentences. Many variations have been proposed to improve this model, e.g., projecting the grid into a graph representation (Guinaudeau and Strube, 2013) or converting the grid to a neural model (Tien Nguyen and Joty, 2017).\nHowever, the neural version of the entity grid (Tien Nguyen and Joty, 2017) has two limitations. First, Lai and Tetreault (2018) state that entity grids applied to downstream tasks are often extremely sparse. In their evaluation, it is difficult to find meaningful entity transitions between sentences in the grids. Accordingly, this model performs worse than other neural models. More importantly, this neural model cannot provide any clues of how this model works since Tien Nguyen and Joty (2017) apply a convolutional layer on the entity grid. The feature map of the convolutional layer is not interpretable. They cannot examine which entity is assigned more importantly than others by their model. In contrast, we constrain our model to capture focus on entities using noun phrases. Then our model tracks the changes of focus. Hence, it provides us with an interpretable focus (Section 5).\nMore recently, Moon et al. (2019) propose a neural coherence model to exploit both local and structural aspects. They evaluate their model on an artificial task only, the shuffle test, which determines whether sentences in a document are shuffled or not. However, recent studies (Pishdad et al., 2020) claim that this artificial task is not suitable to evaluate coherence models. Lai and Tetreault (2018) show that the neural coherence models, which achieve the best performance on this task, do not outperform non-neural models on downstream tasks. More recently, Mohiuddin et al. (2021) find a weak correlation between the model performance in artificial tasks and downstream tasks. In our evaluation, we compare Moon et al. (2019) with ours in an artificial task as well as in three downstream tasks. Moon et al. (2019) perform the best in the artificial task, but do not outperform our model in three downstream tasks (Section 4)."
    }, {
      "heading" : "3 Our Model",
      "text" : "Figure 1 presents an overview of our model architecture. We first introduce our entity representation and sentence encoding using a pretrained language model. Next, we describe a novel local coherence model. We then combine the two representations of\nlocal coherence and the context vector, simply averaged sentence representations. Finally, we apply a feedforward network to produce a score label."
    }, {
      "heading" : "3.1 Entity Identification",
      "text" : "Pretrained language models encode sequences as sub-words, but to our knowledge, there is no linguistic parser using sub-words as input. Hence, we use an linguistic parser to identify noun phrases in each sentence separately. Kitaev and Klein (2018) present a neural constituency parser which determines the syntactic structure of a sentence. To identify noun phrases and proper names, we apply this parser to the original sentences, then map parsed constituents to sub-word tokens.\nSince pretrained language models do not have the means to represent phrase meaning composition, we average sub-word representations for phrases which consists of multiple sub-words. While this implementation does not capture the complicate meaning of phrases, Yu and Ettinger (2020) report that it shows higher correlation with human annotations than using the last word of phrases, assuming that the last word of a phrase is its head."
    }, {
      "heading" : "3.2 Sentence Encoding",
      "text" : "We use a pretrained language model (Yang et al., 2019) to encode sentences. XLNet learns bidirectional contexts by maximizing expected likelihood using an autoregressive training objective, hence it has the advantage of capturing focus in sentences. XLNet outperforms other language models in tasks which require to process long texts.\nRecent work investigates that pretrained lan-\nguage models learn linguistic features that are helpful for language understanding (Tenney et al., 2019; Warstadt et al., 2020). Inspired by this, we encode two adjacent sentences at once to capture discourse features, such as coreference relations. In this strategy, items are encoded twice except the items included in the first and the last sentence. We interpolate items encoded twice to consider context with regard to the preceding and succeeding sentence.\nWe first encode an input document using XLNet to obtain word representations. Sentence representations are means of all word representations in a sentence. We then feed sentence representations and the noun phrase representations into the coherence modules."
    }, {
      "heading" : "3.3 Local Coherence Module",
      "text" : "We compare the semantic representations of noun phrases between adjacent sentences. The two most similar representations of noun phrases are determined as local focus of the respective sentences. These two representations are averaged to capture the common context. We use cosine similarity to measure semantic similarity.\nWe notice that some sentences do not include noun phrases, approximately 3.5% in the three datasets used in our evaluation. This mostly occurs when some words are omitted as in cases of ellipsis (Hardt and Romero, 2004). In such cases, we maintain the focus of the previous sentence to preserve the context.\nA depthwise convolutional layer is applied to the local focus to record its transitions. Unlike a typical convolutional layer, the depthwise convolutional layer captures the patterns of semantic\nchanges between different time-steps for the same spatial information (Chollet, 2017). Hence, this layer captures the semantic changes between local foci considering the context, but it does not hurt the explainability of our model. We use the lightweight depthwise convolutional layer (Wu et al., 2019).\nThen we update the representations of local foci to track the semantic changes between them. We use the Tree-Transformer which updates its hidden representations by inducing a tree-structure from a document (Wang et al., 2019). It generates constituent priors by calculating neighboring attention which represents the probability of whether adjacent items are in the same constituent. The constituent priors constrain the self-attention of the transformer to follow the induced structure.\nFinally, we apply document attention to produce the weighted sum of all the updated local focus representations. The document attention identifies relative weights of updated representations which enables our model to handle any document length."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Implementation Details",
      "text" : "We implement our model using the PyTorch library and use the Stanford Stanza library2 for sentence tokenization. We employ XLNet for the pretrained language model. For the baselines which do not employ a pretrained language model (Dong et al., 2017; Mesgar and Strube, 2018), GloVe is employed for word embeddings, trained on Google News (Pennington et al., 2014) (see Appendix A for more details).\nTo compare baselines within the same framework, we re-implement all of them in PyTorch. We then use our re-implementation to report the performance of models with 10 runs with different random seeds. We verify statistical significance (pvalue<0.01) with both a one-sample t-test, which verifies the reproducibility of the performance of each model, and a two-sample t-test, which verifies that the performance of our model is statistically significantly different from other models.\nWithin same framework we compare the size of models used in our experiment. Our neural model uses a number of parameters comparable to the state of the art, the transformer-based model (Moon et al. (2019): 118M < Jeon and Strube (2020): 136M < Our model: 137M).\n2https://stanfordnlp.github.io/stanza"
    }, {
      "heading" : "4.2 Baselines: Neural Coherence Models",
      "text" : "In all three downstream tasks, we compare our model against recent neural coherence models. First, Mesgar and Strube (2018) propose a neural local coherence model, based on Centering theory. This model connects the most related states of a Recurrent Neural Network, then represents the coherence patterns using semantic distances between the states. Second, Moon et al. (2019) propose a unified neural coherence model to consider local and structural aspects. This model consists of two modules when they employ a pretrained language model (Peters et al., 2018): a module of inter-sentence relations using a bilinear layer and a topic structure module applying a depth-wise convolutional layer to the sentence representations. To ensure fair comparison, XLNet is employed for this model as well, instead of ELMo (Peters et al., 2018)."
    }, {
      "heading" : "4.3 Artificial Task: Shuffle Test",
      "text" : "We first evaluate our model on the artificial setup, the shuffle test, used in the earlier works. We follow the setup used in Lai and Tetreault (2018). In this setup, our model outperforms a simple neural model relying on the pretrained language model. Moon et al. (2019) evaluate their models only in this setup. It achieves outstanding performance in this setup. However, in the following sections, our results show that this model does not outperform our model in downstream tasks.\nOur results are not surprising. There is a line of recent work which shows that this setup is not desirable to evaluate coherence from diverse perspectives. Laban et al. (2021) show that employing fine-tuned language models simply achieves a near-perfect accuracy on this setup. O’Connor and Andreas (2021) measure usable information by selectively ablating lexical and structural information in transformer-based language models. Their findings show that prediction accuracy depends on information about local word co-occurrence, but\nnot word order or global position. We suspect that exploiting all information of a sentence is more beneficial to shuffle tests than entity-based modeling. Based on these findings, we evaluate our model on three downstream tasks used for evaluating coherence models, automated essay scoring, assessing writing quality, and assessing discourse coherence. We encourage future work not to evaluate coherence models on the artificial setup solely."
    }, {
      "heading" : "4.4 Automated Essay Scoring (AES)",
      "text" : "Dataset. To evaluate the coherence models on AES, we evaluate them on the Test of English as a Foreign Language (TOEFL) dataset (Blanchard et al., 2013). While the Automated Student Assessment Prize (ASAP) dataset3 is commonly used for AES, TOEFL has generally higher quality of essays compared to essays in ASAP. The prompts in ASAP are written by students in grade levels 7 to 10 of US middle schools. Many essays in ASAP consist of only a few sentences. In contrast, the prompts in TOEFL are submitted for the standard English test for the entrance to universities by nonnative students. The prompts in TOEFL do not vary so much, the student population is more controlled, and essays have a similar length (see Appendix A for more details).\nEvaluation Setup. We follow the evaluation setup of previous work on AES (Taghipour and Ng, 2016). For TOEFL, we evaluate performance with accuracy for the 3-class classification problem with 5-fold cross-validation. We use the same split for the cross-validation, used by Jeon and Strube (2020). The cross-entropy loss is deployed for training. The ADAM optimizer is used for our model with a learning rate of 0.003. We evaluate performance for 25 epochs on the validation set with a mini-batch size of 32. The model which reaches the\n3https://kaggle.com/c/asap-aes\nbest accuracy on the validation set is then applied to the test set. Baselines. We compare against Dong et al. (2017), a neural model proposed for AES. They present a model which consists of a convolutional layer, followed by a recurrent layer, and an attention layer (Bahdanau et al., 2015) between the adjacent tokens. Results. Table 2 reports the performance on TOEFL. Dong et al. (2017) report better performance than the more recent neural model based on Centering theory (Mesgar and Strube, 2018). A simple model relying on the pretrained language model outperforms this model, which averages all sentences to a vector representation (henceforth, Avg-XLNet). Moon et al. (2019) show that their unified model outperforms previous models on the artificial task, the shuffle test. However, it does not outperform the previous models on the AES task. Jeon and Strube (2020) outperform previous models. Finally, our model, which integrates local and structural aspects, achieves state-of-the-art performance. We perform an ablation study to investigate the contribution of individual components. We compare with Jeon and Strube (2020) who encode two adjacent sentences using the pretrained language model (2SentsEnc). Our results verify that this encoding improves performance, but our model benefits from the novel local coherence module even more."
    }, {
      "heading" : "4.5 Assessing Writing Quality (AWQ)",
      "text" : "Dataset. Louis and Nenkova (2013) create a dataset of scientific articles from the New York Times (NYT) for assessing writing quality. They assign each article to one of two classes by a semisupervised approach: typical or good. Though articles included in both classes are of good quality overall, Louis and Nenkova (2013) show that linguistic features contribute to distinguish different\nclasses of writing quality. Evaluation Setup. For NYT, we follow the setup used in previous work. Louis and Nenkova (2013) and Ferracane et al. (2019) undersample the dataset to mitigate the bias of the uneven label distribution. Following Ferracane et al. (2019), Jeon and Strube (2020) partition the dataset into 80% training, 10% validation, and 10% test set, respectively. We use the ADAM optimizer with a learning rate of 0.001 and a mini-batch size of 32. We evaluate performance for 25 epochs. Baselines. Liu and Lapata (2018) propose a neural model which induces structural information without a labeled resource. It induces the non-projective dependency structure by structured attention.\nResults. Table 4 shows the performance on NYT. Ferracane et al. (2019) reported the best performance of the latent learning model for discourse structure (Liu and Lapata, 2018) on NYT. However, Jeon and Strube (2020) show that the good results are due to the embeddings trained from the target dataset. They also report that Avg-XLNet outperforms this model which employs Glove embeddings. Moon et al. (2019) show better performance than this simple model, but it does not outperform Jeon and Strube (2020). Our model achieves stateof-the-art performance. An ablation study of the joint sentence encoding verifies that our model gains improvement not only from this encoding but also from our local coherence module."
    }, {
      "heading" : "4.6 Assessing Discourse Coherence (ADC)",
      "text" : "Dataset. While previous work evaluates coherence models on formally written texts (Barzilay and Lapata, 2008), GCDC (Lai and Tetreault, 2018) is designed to evaluate coherence models on informal texts, such as emails or online reviews. The dataset contains four domains: Clinton and Enron for emails, Yahoo for questions and answers in an online forum, and Yelp for online reviews of businesses. The quality of the dataset is controlled to have evenly-distributed scores and a low correlation between discourse length and scores4. Evaluation Setup. For GCDC, we perform the experiments following previous work (Lai and Tetreault, 2018). We perform 10-fold crossvalidation, use accuracy as evaluation measure on the 3-class classification, and use the cross-entropy loss function. Baselines. Li and Jurafsky (2017) propose a neural model based on cliques, that are sets of adjacent sentences. This model uses the cliques taken from the original article as a positive label and uses cliques with randomly permutated ones as a negative label. Lai and Tetreault (2018) show that a simple neural model which uses paragraph information outperforms previous models on GCDC. Results. Table 3 summarizes the performance on GCDC. While Avg-XLNet outperforms previous baselines, other advanced neural models show similar performance. Our model performs slightly better than Jeon and Strube (2020) with two sentences encoding. This shows that the gains mainly benefit from this encoding strategy. We suspect that Jeon and Strube (2020) do not benefit from structural information since texts on GCDC are not well-organized. The texts mostly consist of a few sentences, and they express the writers’ emotion. Based on this, Lai and Tetreault (2018) state that\n4The Pearson correlation between text length and scores is lower than 0.12 in all domains.\ntexts of lower quality have sudden topic changes. We also suspect that human annotators recognize important entities in the texts, such as the name of a person in the US government."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Capturing Focus Using Entities",
      "text" : "In Centering theory, the focus is described as the most important item in a sentence. Jeon and Strube (2020) capture the focus using attention weight scores and analyze texts assigned to different qualities using this focus. They state that the focus is difficult to interpret when it is composed of sub-words. To investigate this further, we compare the focus captured on (sub-)words and the focus constrained to entities. Table 5 indicates that constraining focus to entities leads to better explainability, in particular on NYT (see Table 11 in the Appendix D for more details). For example, in the NYT-1516415 news article about the String theory, a subword of “ein” is not interpretable focus while it may represent a useful representation in the vector space for a neural model. In contrast, our entity-based modeling leads our model to better explainability. Instead\nof “ein”, it provides more interpretable focus, “Einstein”, a theoretical physicist. In TOEFL, “many academic subjects” is more interpretable focus than focus consists of a single subword token either “many” or “subjects”. Table 5 also shows that our model mainly uses pronouns, and noun phrases are playing an important role in the text to represent focus. Our findings suggest that further investigation is needed to understand how pretrained language models work on pronouns to process a long text."
    }, {
      "heading" : "5.2 Local Coherence Patterns",
      "text" : "Using interpretable focus information, we investigate differences in focus transitions of texts assigned to different scores. Motivated by the definition of the continue and the shift transition in Centering theory, we define semantic consistency which represents the degree of semantic changes between local foci. Two adjacent sentences are semantically consistent when the semantic similarity (simi) between the local foci (lf ) is higher than a semantic threshold (θsem;score). This threshold is determined as the average of semantic similarities between local foci of adjacent sentences in the texts assigned the same score. Otherwise, a\nsemantic transition (st) occurs between the local foci: sti = 1 if simi < θsem;score. Finally the semantic consistency (SC) is defined as follows: SC = 1− (count(sti)/|lf |).\nFigure 2 illustrates the semantic consistency on TOEFL, and Table 6 shows the statistics of the semantic consistency on texts assigned to different scores. Texts assigned a high score show lower semantic consistency on average. This indicates that texts of higher quality are overall more semantically variant than texts of lower quality. Additionally, we observe that texts assigned a low score show significantly larger proportions of an extreme level of semantic consistency. We define the extreme level as either texts whose semantic consistency is lower than 5%, indicating texts are highly variant, or texts whose semantic consistency is higher than 75%, indicating texts are highly consistent. Hence, these findings indicate that texts of lower quality are semantically too variant or too consistent. Texts of higher quality are are neither too variant nor too consistent.\nWe next inspect the focus of texts assigned to different scores (see Table 12,13, and 14 in the Appendix D for more details). It shows that the proportion of pronouns is higher in the local focus compared with their proportion in the focus captured on a sentence solely. The essays in TOEFL are argumentative essays, and good essays should use facts and evidence to support their claim (Wingate, 2012). We observe that texts assigned a low score frequently include claims without convincing evidence. This causes our model to capture focus on pronouns more frequently in these texts. In contrast, texts assigned a high score include convincing evidence to support claims, and this lets our model capture different types of foci in these texts."
    }, {
      "heading" : "5.3 Error Analysis",
      "text" : "Finally, we conduct an error analysis to investigate how our model works differently compared to previous coherence models on TOEFL. We first compare the predicted scores with Moon et al. (2019) and a simple model which only considers context, averaged-XLNet. These two baselines show biased predictions on the middle score. We suspect that this is caused by the label bias in TOEFL (Blanchard et al., 2013). Biased label distributions cause biased predictions, and they benefit from these biased predictions. In contrast, our model benefits more from predicting high scores correctly as well\nas other scores, indicating our coherence model assess text quality better.\nWe then compare with the previous state of the art (Jeon and Strube, 2020). This baseline induces discourse structure to model structural coherence. It captures semantic relations between discourse segments, not just between adjacent sentences. We observe two error cases when this baseline struggles to predict correctly. It predicts scores lower than the ground-truth score for texts which lack support and evidence for claims. However, these texts have a well-organized paragraph for one or two claims. We suspect that this leads human annotators to assign a mid or a high score though the text is not well-organized overall. In contrast, it predicts scores higher than ground-truth scores when unrelated claims are listed or claims are listed without evidence. Our model, which captures local coherence between adjacent sentences, deals with these cases better (see Table 15 and 16 in the Appendix D for more details)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We propose a neural coherence model based on entities by constraining the input to noun phrases. It leads our model to better explainability and to set a new state of the art in end applications. It also allows us to reveal that texts of higher qualities are neither semantically too consistent nor too variant.\nOur findings suggest a few interesting directions for future work. As our model sets a new state of the art by constraining models to focus on entities, we could design more efficient modeling instead of considering all information on other tasks as well. Our analysis shows that pretrained language models frequently exploit coreference relations to capture semantic relations. We could design an advanced neural model which exploits these relations explicitly, which could lead to better explainability and better understanding of how transformer-based models work."
    }, {
      "heading" : "A Training and Parameters",
      "text" : "For the three datasets, we use a mini-batch size of 32 with random-shuffle. The ADAM optimizer is used to train our models with a learning rate of 0.001 and epsilon of 1e-4. We evaluate performance for 25 epochs. For the baseline models which do not use a pretrained language model, we use Glove pretrained embeddings with 100- dimensional for TOEFL and with 50-dimensional for NYT. We clip gradients by 1.0. To update sentence representations obtained by a pretrained language model, we use the same dimension of the pretrained language model on a tree-transformer. We manually tune hyperparameters.\nWe encode adjacent two sentences at once using XLNet instead of the whole document at once. Our dataset consists of long documents i.e., journal articles with more than 3,000 tokens. For employing the pretrained model, it is practically infeasible to encode all words in a document at once due to memory limitations. We use 23GB GPU memory a NVidia P40 on ADC and AES and 46GB GPU memory of two NVidia P40s for each run on AWQ. For training our model, it takes approximately 0.8 days on TOEFL, 6.5 days on NYT, and 0.6 days on GCDC."
    }, {
      "heading" : "B Data Description Details",
      "text" : "Table 7 describes statistics on two datasets, TOEFL5 and NYT6. We split a text at the sentence level by Stanford Stanza library, and tokenize them by the XLNet tokenizer. Table 8 describes the topic of each prompt in TOEFL. They are all open-ended tasks, that do not have given context but require students to submit their opinion."
    }, {
      "heading" : "C Evaluation: Shuffle Test",
      "text" : "The shuffle test is introduced to evaluate coherence models with other tests in the previous work (Barzilay and Lapata, 2008).\nBarzilay and Lapata (2008) introduce the shuffle test to evaluate coherence models with other tests."
    }, {
      "heading" : "D Evaluations Details",
      "text" : "E Analysis Details\n5https://catalog.ldc.upenn.edu/LDC2014T06 6https://catalog.ldc.upenn.edu/LDC2008T19"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the ICLR Conference.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Modeling local coherence: An entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Computational Linguistics, 34(1):1–34.",
      "citeRegEx" : "Barzilay and Lapata.,? 2008",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2008
    }, {
      "title" : "TOEFL11: A corpus of non-native english",
      "author" : [ "Daniel Blanchard", "Joel Tetreault", "Derrick Higgins", "Aoife Cahill", "Martin Chodorow." ],
      "venue" : "ETS Research Report Series, 2013(2):i–15.",
      "citeRegEx" : "Blanchard et al\\.,? 2013",
      "shortCiteRegEx" : "Blanchard et al\\.",
      "year" : 2013
    }, {
      "title" : "Xception: Deep learning with depthwise separable convolutions",
      "author" : [ "François Chollet." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1251–1258.",
      "citeRegEx" : "Chollet.,? 2017",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2017
    }, {
      "title" : "Attentionbased recurrent convolutional neural network for automatic essay scoring",
      "author" : [ "Fei Dong", "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 153–162, Vancou-",
      "citeRegEx" : "Dong et al\\.,? 2017",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2017
    }, {
      "title" : "Extending the entity-based coherence model with multiple ranks",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315–324, Avignon, France.",
      "citeRegEx" : "Feng and Hirst.,? 2012",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2012
    }, {
      "title" : "Evaluating discourse in structured text representations",
      "author" : [ "Elisa Ferracane", "Greg Durrett", "Junyi Jessy Li", "Katrin Erk." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 646–653, Florence, Italy. Associ-",
      "citeRegEx" : "Ferracane et al\\.,? 2019",
      "shortCiteRegEx" : "Ferracane et al\\.",
      "year" : 2019
    }, {
      "title" : "Centering: A framework for modeling the local coherence of discourse",
      "author" : [ "Barbara J. Grosz", "Aravind K. Joshi", "Scott Weinstein." ],
      "venue" : "Computational Linguistics, 21(2):203–225.",
      "citeRegEx" : "Grosz et al\\.,? 1995",
      "shortCiteRegEx" : "Grosz et al\\.",
      "year" : 1995
    }, {
      "title" : "Graphbased local coherence modeling",
      "author" : [ "Camille Guinaudeau", "Michael Strube." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93–103, Sofia, Bulgaria. Association for Com-",
      "citeRegEx" : "Guinaudeau and Strube.,? 2013",
      "shortCiteRegEx" : "Guinaudeau and Strube.",
      "year" : 2013
    }, {
      "title" : "Ellipsis and the structure of discourse",
      "author" : [ "Daniel Hardt", "Maribel Romero." ],
      "venue" : "Journal of Semantics, 21(4):375–414.",
      "citeRegEx" : "Hardt and Romero.,? 2004",
      "shortCiteRegEx" : "Hardt and Romero.",
      "year" : 2004
    }, {
      "title" : "Centeringbased neural coherence modeling with hierarchical discourse segments",
      "author" : [ "Sungho Jeon", "Michael Strube." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7458–7472, On-",
      "citeRegEx" : "Jeon and Strube.,? 2020",
      "shortCiteRegEx" : "Jeon and Strube.",
      "year" : 2020
    }, {
      "title" : "Control of inference: Role of some aspects of discourse structure-centering",
      "author" : [ "Aravind K Joshi", "Scott Weinstein." ],
      "venue" : "IJCAI, pages 385–387.",
      "citeRegEx" : "Joshi and Weinstein.,? 1981",
      "shortCiteRegEx" : "Joshi and Weinstein.",
      "year" : 1981
    }, {
      "title" : "Constituency parsing with a self-attentive encoder",
      "author" : [ "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676–2686, Melbourne, Australia. Associa-",
      "citeRegEx" : "Kitaev and Klein.,? 2018",
      "shortCiteRegEx" : "Kitaev and Klein.",
      "year" : 2018
    }, {
      "title" : "Can transformer models measure coherence in text? Re-thinking the shuffle test",
      "author" : [ "Philippe Laban", "Luke Dai", "Lucas Bandarkar", "Marti A Hearst." ],
      "venue" : "arXiv preprint arXiv:2107.03448.",
      "citeRegEx" : "Laban et al\\.,? 2021",
      "shortCiteRegEx" : "Laban et al\\.",
      "year" : 2021
    }, {
      "title" : "Discourse coherence in the wild: A dataset, evaluation and methods",
      "author" : [ "Alice Lai", "Joel Tetreault." ],
      "venue" : "Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, pages 214–223, Melbourne, Australia. Association for Computational",
      "citeRegEx" : "Lai and Tetreault.,? 2018",
      "shortCiteRegEx" : "Lai and Tetreault.",
      "year" : 2018
    }, {
      "title" : "Neural net models of open-domain discourse coherence",
      "author" : [ "Jiwei Li", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198–209, Copenhagen, Denmark. Association for Computa-",
      "citeRegEx" : "Li and Jurafsky.,? 2017",
      "shortCiteRegEx" : "Li and Jurafsky.",
      "year" : 2017
    }, {
      "title" : "Learning structured text representations",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:63–75.",
      "citeRegEx" : "Liu and Lapata.,? 2018",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2018
    }, {
      "title" : "What makes writing great? first experiments on article quality prediction in the science journalism domain",
      "author" : [ "Annie Louis", "Ani Nenkova." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 1:341–352.",
      "citeRegEx" : "Louis and Nenkova.,? 2013",
      "shortCiteRegEx" : "Louis and Nenkova.",
      "year" : 2013
    }, {
      "title" : "A neural local coherence model for text quality assessment",
      "author" : [ "Mohsen Mesgar", "Michael Strube." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4328–4339, Brussels, Belgium. Association",
      "citeRegEx" : "Mesgar and Strube.,? 2018",
      "shortCiteRegEx" : "Mesgar and Strube.",
      "year" : 2018
    }, {
      "title" : "Rethinking coherence modeling: Synthetic vs",
      "author" : [ "Tasnim Mohiuddin", "Prathyusha Jwalapuram", "Xiang Lin", "Shafiq Joty." ],
      "venue" : "downstream tasks. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Mohiuddin et al\\.,? 2021",
      "shortCiteRegEx" : "Mohiuddin et al\\.",
      "year" : 2021
    }, {
      "title" : "A unified neural coherence model",
      "author" : [ "Han Cheol Moon", "Tasnim Mohiuddin", "Shafiq Joty", "Chi Xu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Moon et al\\.,? 2019",
      "shortCiteRegEx" : "Moon et al\\.",
      "year" : 2019
    }, {
      "title" : "What context features can transformer language models use",
      "author" : [ "Joe O’Connor", "Jacob Andreas" ],
      "venue" : "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
      "citeRegEx" : "O.Connor and Andreas.,? \\Q2021\\E",
      "shortCiteRegEx" : "O.Connor and Andreas.",
      "year" : 2021
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "How coherent are neural models of coherence? In Proceedings of the 28th International Conference on Computational Linguistics, pages 6126–6138, Barcelona, Spain (Online)",
      "author" : [ "Leila Pishdad", "Federico Fancellu", "Ran Zhang", "Afsaneh Fazly." ],
      "venue" : "Inter-",
      "citeRegEx" : "Pishdad et al\\.,? 2020",
      "shortCiteRegEx" : "Pishdad et al\\.",
      "year" : 2020
    }, {
      "title" : "Focusing in the comprehension of definite anaphora",
      "author" : [ "Candace Sidner." ],
      "venue" : "Computational models of discourse, pages 267–330.",
      "citeRegEx" : "Sidner.,? 1983",
      "shortCiteRegEx" : "Sidner.",
      "year" : 1983
    }, {
      "title" : "Focusing for interpretation of pronouns",
      "author" : [ "Candace L. Sidner." ],
      "venue" : "American Journal of Computational Linguistics, 7(4):217–231.",
      "citeRegEx" : "Sidner.,? 1981",
      "shortCiteRegEx" : "Sidner.",
      "year" : 1981
    }, {
      "title" : "A neural approach to automated essay scoring",
      "author" : [ "Kaveh Taghipour", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1882–1891, Austin, Texas. Association for Computational Lin-",
      "citeRegEx" : "Taghipour and Ng.,? 2016",
      "shortCiteRegEx" : "Taghipour and Ng.",
      "year" : 2016
    }, {
      "title" : "What do you learn from context? Probing for sentence structure in contextual",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R Bowman", "Dipanjan Das" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "A neural local coherence model",
      "author" : [ "Dat Tien Nguyen", "Shafiq Joty." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1320–1330, Vancouver, Canada. Association for Computational",
      "citeRegEx" : "Nguyen and Joty.,? 2017",
      "shortCiteRegEx" : "Nguyen and Joty.",
      "year" : 2017
    }, {
      "title" : "2019. Tree transformer: Integrating tree structures",
      "author" : [ "Yaushian Wang", "Hung-Yi Lee", "Yun-Nung Chen" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually)",
      "author" : [ "Alex Warstadt", "Yian Zhang", "Xiaocheng Li", "Haokun Liu", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Warstadt et al\\.,? 2020",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Argument!’Helping students understand what essay writing is about",
      "author" : [ "Ursula Wingate." ],
      "venue" : "Journal of English for Academic Purposes, 11(2):145–154.",
      "citeRegEx" : "Wingate.,? 2012",
      "shortCiteRegEx" : "Wingate.",
      "year" : 2012
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Assessing phrasal representation and composition in transformers",
      "author" : [ "Lang Yu", "Allyson Ettinger." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4896–4907, Online. Association for Computa-",
      "citeRegEx" : "Yu and Ettinger.,? 2020",
      "shortCiteRegEx" : "Yu and Ettinger.",
      "year" : 2020
    }, {
      "title" : "introduce the shuffle test to evaluate coherence models with other tests",
      "author" : [ "Barzilay", "Lapata" ],
      "venue" : "D Evaluations Details E Analysis Details",
      "citeRegEx" : "Barzilay and Lapata,? \\Q2008\\E",
      "shortCiteRegEx" : "Barzilay and Lapata",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Our approach is also in accord with a recent study (O’Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models.",
      "startOffset" : 51,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "Recent neural coherence models (Mesgar and Strube, 2018; Moon et al., 2019) encode the input",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "Recent neural coherence models (Mesgar and Strube, 2018; Moon et al., 2019) encode the input",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "document using large-scale pretrained language models (Peters et al., 2018).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Jeon and Strube (2020) present a neural coherence model, which allows to interpret focus information for the first time.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "Centering theory (Joshi and Weinstein, 1981; Grosz et al., 1995) builds on these insights and introduces an algorithm for tracking changes in focus.",
      "startOffset" : 17,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "Centering theory (Joshi and Weinstein, 1981; Grosz et al., 1995) builds on these insights and introduces an algorithm for tracking changes in focus.",
      "startOffset" : 17,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : "Our approach is not only linguistically more sound but also is in accord with the recent empirical study by O’Connor and Andreas (2021) who inves-",
      "startOffset" : 108,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "The entity grid is its most well-known implementation (Barzilay and Lapata, 2008).",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : ", projecting the grid into a graph representation (Guinaudeau and Strube, 2013) or converting the grid to a neural model (Tien Nguyen and Joty, 2017).",
      "startOffset" : 50,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "First, Lai and Tetreault (2018) state that entity grids applied to downstream tasks are often extremely sparse.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "More importantly, this neural model cannot provide any clues of how this model works since Tien Nguyen and Joty (2017) apply a convolutional layer on the entity grid.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "More recently, Moon et al. (2019) propose a neural coherence model to exploit both local and struc-",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : "However, recent studies (Pishdad et al., 2020) claim that this artificial task is not suitable to evaluate co-",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "Lai and Tetreault (2018) show that the neural coherence models, which achieve the best performance on this task, do not outperform non-neural models on downstream tasks.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "In our evaluation, we compare Moon et al. (2019) with ours in an artificial task as well as in three downstream tasks.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "In our evaluation, we compare Moon et al. (2019) with ours in an artificial task as well as in three downstream tasks. Moon et al. (2019) perform the best in the artificial task, but do not outperform our model in three downstream tasks (Section 4).",
      "startOffset" : 30,
      "endOffset" : 138
    }, {
      "referenceID" : 35,
      "context" : "While this implementation does not capture the complicate meaning of phrases, Yu and Ettinger (2020) report that it shows higher correlation with human annotations than using the last word of phrases, assuming that the last word of a phrase is its head.",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 34,
      "context" : "We use a pretrained language model (Yang et al., 2019) to encode sentences.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "This mostly occurs when some words are omitted as in cases of ellipsis (Hardt and Romero, 2004).",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "changes between different time-steps for the same spatial information (Chollet, 2017).",
      "startOffset" : 70,
      "endOffset" : 85
    }, {
      "referenceID" : 33,
      "context" : "We use the lightweight depthwise convolutional layer (Wu et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : "den representations by inducing a tree-structure from a document (Wang et al., 2019).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "For the baselines which do not employ a pretrained language model (Dong et al., 2017; Mesgar and Strube, 2018), GloVe is employed for word embeddings, trained on Google News (Pennington et al.",
      "startOffset" : 66,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "For the baselines which do not employ a pretrained language model (Dong et al., 2017; Mesgar and Strube, 2018), GloVe is employed for word embeddings, trained on Google News (Pennington et al.",
      "startOffset" : 66,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : ", 2017; Mesgar and Strube, 2018), GloVe is employed for word embeddings, trained on Google News (Pennington et al., 2014) (see Appendix A",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "Our neural model uses a number of parameters comparable to the state of the art, the transformer-based model (Moon et al. (2019): 118M < Jeon and Strube (2020): 136M < Our model: 137M).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "(2019): 118M < Jeon and Strube (2020): 136M < Our model: 137M).",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "First, Mesgar and Strube (2018) propose a neural local coherence model, based on Centering theory.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "Second, Moon et al. (2019) propose a unified neural coherence model to consider lo-",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "This model consists of two modules when they employ a pretrained language model (Peters et al., 2018): a module of inter-sentence relations using a bilinear layer and a topic structure module applying a depth-wise con-",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "To ensure fair comparison, XLNet is employed for this model as well, instead of ELMo (Peters et al., 2018).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "We follow the setup used in Lai and Tetreault (2018). In this setup, our model outperforms a simple neural model relying on the pretrained language model.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "We follow the setup used in Lai and Tetreault (2018). In this setup, our model outperforms a simple neural model relying on the pretrained language model. Moon et al. (2019) evaluate their models only in this setup.",
      "startOffset" : 28,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "Laban et al. (2021) show that employing fine-tuned language models simply achieves a near-perfect accuracy on this setup.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "Laban et al. (2021) show that employing fine-tuned language models simply achieves a near-perfect accuracy on this setup. O’Connor and Andreas (2021) measure usable information by selectively ablating lexical and structural information in transformer-based language models.",
      "startOffset" : 0,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "Model Prompt Avg Acc 1 2 3 4 5 6 7 8 Dong et al. (2017) 69.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Model Prompt Avg Acc 1 2 3 4 5 6 7 8 Dong et al. (2017) 69.30 66.47 65.84 66.38 68.89 64.20 67.11 65.73 66.74 Mesgar and Strube (2018) 56.",
      "startOffset" : 37,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "Model Prompt Avg Acc 1 2 3 4 5 6 7 8 Dong et al. (2017) 69.30 66.47 65.84 66.38 68.89 64.20 67.11 65.73 66.74 Mesgar and Strube (2018) 56.25 55.94 55.20 57.20 56.57 55.10 56.97 58.39 56.45 Averaged-XLNet-1SentEnc 70.73 69.48 68.98 67.52 72.35 70.94 70.14 69.01 69.89 Moon et al. (2019)-1SentEnc 73.",
      "startOffset" : 37,
      "endOffset" : 286
    }, {
      "referenceID" : 4,
      "context" : "Model Prompt Avg Acc 1 2 3 4 5 6 7 8 Dong et al. (2017) 69.30 66.47 65.84 66.38 68.89 64.20 67.11 65.73 66.74 Mesgar and Strube (2018) 56.25 55.94 55.20 57.20 56.57 55.10 56.97 58.39 56.45 Averaged-XLNet-1SentEnc 70.73 69.48 68.98 67.52 72.35 70.94 70.14 69.01 69.89 Moon et al. (2019)-1SentEnc 73.75 72.13 72.92 73.29 75.12 74.69 72.89 72.09 73.36 Jeon and Strube (2020)-1SentEnc 75.",
      "startOffset" : 37,
      "endOffset" : 372
    }, {
      "referenceID" : 27,
      "context" : "We follow the evaluation setup of previous work on AES (Taghipour and Ng, 2016).",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "We use the same split for the cross-validation, used by Jeon and Strube (2020). The cross-entropy loss is deployed for train-",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "We compare against Dong et al. (2017), a neural model proposed for AES.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "model which consists of a convolutional layer, followed by a recurrent layer, and an attention layer (Bahdanau et al., 2015) between the adjacent tokens.",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "(2017) report better performance than the more recent neural model based on Centering theory (Mesgar and Strube, 2018).",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Dong et al. (2017) report better performance than the more recent neural model based on Centering theory (Mesgar and Strube, 2018).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "Moon et al. (2019) show that their unified model outperforms previous models on the artificial task, the shuffle test.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "Jeon and Strube (2020) outperform previous models. Finally, our model, which integrates local and structural aspects, achieves state-of-the-art performance. We perform an ablation study to investigate the contribution of individual components. We compare with Jeon and Strube (2020) who encode two adjacent sentences using the pretrained language model (2SentsEnc).",
      "startOffset" : 0,
      "endOffset" : 283
    }, {
      "referenceID" : 17,
      "context" : "Louis and Nenkova (2013) create a dataset of scientific articles from the New York Times (NYT) for assessing writing quality.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "Louis and Nenkova (2013) create a dataset of scientific articles from the New York Times (NYT) for assessing writing quality. They assign each article to one of two classes by a semisupervised approach: typical or good. Though articles included in both classes are of good quality overall, Louis and Nenkova (2013) show that linguistic features contribute to distinguish different",
      "startOffset" : 0,
      "endOffset" : 315
    }, {
      "referenceID" : 13,
      "context" : "Model Yahoo Clinton Enron Yelp Avg Acc ∗Li and Jurafsky (2017) 53.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "Model Yahoo Clinton Enron Yelp Avg Acc ∗Li and Jurafsky (2017) 53.5 61.0 54.4 49.1 51.7 Mesgar and Strube (2018) 47.",
      "startOffset" : 40,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "Table 3: ADC: Mean (standard deviation) accuracy performance on the test sets in GCDC (∗: reported performance in Lai and Tetreault (2018)).",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Louis and Nenkova (2013) and Ferracane et al. (2019) undersample the dataset to mitigate the bias of the uneven label distribution.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "Louis and Nenkova (2013) and Ferracane et al. (2019) undersample the dataset to mitigate the bias of the uneven label distribution. Following Ferracane et al. (2019), Jeon and Strube (2020) partition the dataset into 80% training, 10%",
      "startOffset" : 29,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "Louis and Nenkova (2013) and Ferracane et al. (2019) undersample the dataset to mitigate the bias of the uneven label distribution. Following Ferracane et al. (2019), Jeon and Strube (2020) partition the dataset into 80% training, 10%",
      "startOffset" : 29,
      "endOffset" : 190
    }, {
      "referenceID" : 16,
      "context" : "Liu and Lapata (2018) propose a neural model which induces structural information without a labeled resource.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "(2019) reported the best performance of the latent learning model for discourse structure (Liu and Lapata, 2018) on NYT.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "Ferracane et al. (2019) reported the best performance of the latent learning model for discourse structure (Liu and Lapata, 2018) on NYT.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "Ferracane et al. (2019) reported the best performance of the latent learning model for discourse structure (Liu and Lapata, 2018) on NYT. However, Jeon and Strube (2020) show that the good results are due to the embeddings trained from the target dataset.",
      "startOffset" : 0,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Ferracane et al. (2019) reported the best performance of the latent learning model for discourse structure (Liu and Lapata, 2018) on NYT. However, Jeon and Strube (2020) show that the good results are due to the embeddings trained from the target dataset. They also report that Avg-XLNet outperforms this model which employs Glove embeddings. Moon et al. (2019) show better performance than this simple model, but it does not outperform Jeon and Strube (2020).",
      "startOffset" : 0,
      "endOffset" : 362
    }, {
      "referenceID" : 6,
      "context" : "Ferracane et al. (2019) reported the best performance of the latent learning model for discourse structure (Liu and Lapata, 2018) on NYT. However, Jeon and Strube (2020) show that the good results are due to the embeddings trained from the target dataset. They also report that Avg-XLNet outperforms this model which employs Glove embeddings. Moon et al. (2019) show better performance than this simple model, but it does not outperform Jeon and Strube (2020). Our model achieves stateof-the-art performance.",
      "startOffset" : 0,
      "endOffset" : 460
    }, {
      "referenceID" : 1,
      "context" : "While previous work evaluates coherence models on formally written texts (Barzilay and Lapata, 2008), GCDC (Lai and Tetreault, 2018) is",
      "startOffset" : 73,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "While previous work evaluates coherence models on formally written texts (Barzilay and Lapata, 2008), GCDC (Lai and Tetreault, 2018) is",
      "startOffset" : 107,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "For GCDC, we perform the experiments following previous work (Lai and Tetreault, 2018).",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "Li and Jurafsky (2017) propose a neural model based on cliques, that are sets of adjacent sentences.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "Lai and Tetreault (2018) show that a simple neural model which uses paragraph information outperforms previous models on GCDC.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "better than Jeon and Strube (2020) with two sentences encoding.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "better than Jeon and Strube (2020) with two sentences encoding. This shows that the gains mainly benefit from this encoding strategy. We suspect that Jeon and Strube (2020) do not benefit from structural information since texts on GCDC are not well-organized.",
      "startOffset" : 12,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "Jeon and Strube (2020) capture the focus using attention weight scores and analyze texts assigned to different qualities using this focus.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 32,
      "context" : "facts and evidence to support their claim (Wingate, 2012).",
      "startOffset" : 42,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "We suspect that this is caused by the label bias in TOEFL (Blanchard et al., 2013).",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "We first compare the predicted scores with Moon et al. (2019) and a simple model which only considers context, averaged-XLNet.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "We then compare with the previous state of the art (Jeon and Strube, 2020).",
      "startOffset" : 51,
      "endOffset" : 74
    } ],
    "year" : 0,
    "abstractText" : "In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role. Still, these models achieve state-of-the-art performance in several end applications. In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences leading to the notion of focus. This brings our model linguistically in line with pre-neural models of computing coherence. It also gives us better insight into the behaviour of the model thus leading to better explainability. Our approach is also in accord with a recent study (O’Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models. We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications1.",
    "creator" : null
  }
}
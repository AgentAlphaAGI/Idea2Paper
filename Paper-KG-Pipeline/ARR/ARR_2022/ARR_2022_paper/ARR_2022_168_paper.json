{
  "name" : "ARR_2022_168_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cross-modal Contrastive Learning for Speech Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "End-to-end speech-to-text translation (E2E ST) has been becoming important in many products and real applications. An E2E ST system accepts audio signals as the input and generates the target translation using a single model. Compared with the conventional cascade ST models, E2E ST models have achieved almost comparable (Bentivogli et al., 2021; Dong et al., 2018) or even superior (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Xu et al., 2021) performance.\nThe performance of an E2E ST model is still restricted by the relatively small parallel data, compared to text machine translation (MT). Existing approaches for ST focus on using additional data from MT and automatic speech recognition (ASR). This can be realized through pre-training approaches (Zheng et al., 2021) or multi-task training frameworks (Tang et al., 2021b; Ye et al., 2021; Han et al., 2021).\nDifferent from the data perspective, this paper investigates the bottleneck of E2E ST from the neural representation perspective. We believe that a\nright representation for audio input is fundamental to effective speech translation. What is the right representation? A recent neurocognitive study reveals that the human brain processes speech and written text at the same region of the cortex (Regev et al., 2013). Listening to spoken utterance and reading its corresponding sentence result in the same activation patterns in the superior temporal sulcus (Wilson et al., 2018). Drawing an analogy from the human brain to artificial neurons, does this unified representation benefit speech translation?\nWith this hint from the human brain, we analyze Transformer models for speech translation. We observe a noticeable modality gap between encoder representations of speech and text ( Sec. 6 has more details) from existing ST models. An ideal representation should satisfy: if the content of the speech and transcription are similar, their encoded representations should likewise be close to each other. Nevertheless, how to learn unified and aligned speech-text representations?\nInspired by the recent progress of contrastive learning approaches in cross-lingual (Lample and Conneau, 2019; Pan et al., 2021) and cross-modal vision-and-language domains (Li et al., 2021; Zhou et al., 2020; Dong et al., 2019), we designed a simple contrastive learning method for ST (ConST) to learn the representations that meet the afore-\nmentioned conditions explicitly. On the one hand, our model inherits the advantages of the previous multi-task learning methods. On the other hand, it reduces the gap between the representations of speech and its corresponding transcription.\nOur contributions are as follows. • We develop ConST for speech translation, a\ncross-modal contrastive learning method, on top of the multi-task training framework.\n• Our experiments on the MuST-C benchmark to show ConST achieves an average BLEU score of 28.5, outperforming the best previous baseline.\n• We conduct a cross-modal retrieval experiment and demonstrate that ConST closes the representation gap of two modalities by projecting them into a unified space."
    }, {
      "heading" : "2 Related Work",
      "text" : "End-to-end ST To alleviate the error propagation in the cascaded ST systems and to make the deployment simpler, Bérard et al. (2016); Weiss et al. (2017) proposed to use an end-to-end architecture to directly translate speech into text in another language, without the intermediate transcription. Kano et al. (2017); Berard et al. (2018); Inaguma et al. (2020); Wang et al. (2020a); Zhao et al. (2021a) implemented several off-the-shelf encoderdecoder E2E-ST models, such as BiLSTM (Greff et al., 2016) and Speech-Transformer (Dong et al., 2018). However, training an end-to-end speech translation model is difficult because we need to design a cross-modal cross-language model, meanwhile, the speech-transcription-translation supervised data for speech translation is significantly less than that of MT and ASR. Methods, like data augmentation (Park et al., 2019; Pino et al., 2020; Chen et al., 2021), pre-training (Weiss et al., 2017; Berard et al., 2018; Bansal et al., 2019; Wang et al., 2020b; Alinejad and Sarkar, 2020; Dong et al., 2021a; Zheng et al., 2021), self-training (Pino et al., 2020; Wang et al., 2021), utilizing self-supervised pre-trained audio representation (Wu et al., 2020; Han et al., 2021; Ye et al., 2021; Wang et al., 2021), are proved to be effective. Meanwhile, some work has shown that the encoder-decoder model with a single encoder cannot encode speech information well. For example, Dong et al. (2021b) first proposed a second encoder to further extract semantic information of the speech sequence. Xu et al. (2021) proposed a stacked acoustic-and-textual encoder and introduced large-scale out-of-domain\ndata. Also, multi-task frameworks (Le et al., 2020; Tang et al., 2021b; Ye et al., 2021) are widely applied to further enhance the robustness for ST. As a cross-modal task, some work has noted the problem of the modality gap. (Han et al., 2021) designed a fix-size semantic memory module to bridge such a gap, from the neuroscience perspective. However, we find that this approach actually sacrifices the effect of MT. So in this paper, we propose a simple yet effective contrastive learning method to bridge the gap and to improve ST performance. Contrastive learning Our method is motivated by the recent success in contrastive representation learning. The contrastive learning method was first proposed to learn representations from unlabeled datasets (hence the term, self-supervised learning) by telling which data points are similar or distinct, especially in the field of computer vision (Chopra et al., 2005; Gutmann and Hyvärinen, 2010; Schroff et al., 2015; Sohn, 2016; Oord et al., 2018). Khosla et al. (2020) extended the self-supervised batch contrastive approach to the fully-supervised setting and proposed a supervised contrastive learning method. In speech processing, representative methods focused on speaker identification (Ravanelli and Bengio, 2018), speech recognition (Schneider et al., 2019), and audio representation learning (van den Oord et al., 2018; Baevski et al., 2020). In the NLP area, the contrastive framework is used for sentence representation learning (Fang and Xie, 2020; Shen et al., 2020; Gao et al., 2021; Wu et al., 2021; Yan et al., 2021) and machine translation Pan et al. (2021). Very recently, contrastive learning is also applied to learning a unified representation of image and text (Dong et al., 2019; Zhou et al., 2020; Li et al., 2021). Motivated by the contrastive learning frameworks in cross-lingual and cross-modal topics, we introduce a similar idea in speech translation."
    }, {
      "heading" : "3 The ConST Approach",
      "text" : "An end-to-end speech translation model directly translates audio sequence s = (s1, ..., s|s|) to the text y = (y1, ..., y|y|) in the target language. Speech translation corpus D = {(s,x,y)} provides transcript x = (x1, ..., x|x|) in the source language, as well.\nIn this section, we present the overall speech translation model and cross-modal contrastive learning. We also provide several feasible strategies to construct more positive and negative pairs\nto enhance the contrastive learning."
    }, {
      "heading" : "3.1 Model Framework",
      "text" : "Our model consists fout sub-modules: a speech encoder, a word embedding layer, a Transformer Encoder and a Transformer decoder (Figure 2). It is designed to take either speech or a sentence as input, and to output either source transcript or target translation text. Such architecture enables a universal framework for multiple tasks, including ST, MT and ASR.\nThe speech encoder module (S-Enc) is designed to extract low-level features for speech signals. It contains Wav2vec2.0 (Baevski et al., 2020) and two additional convolutional layers. The input is raw waveform signal sampled at 16kHz. Each convolutional layer has a stride of 4 and d channels. In total, it shrinks the time dimension by a factor of 4. Denote a = S-Enc(s) as the audio representation of the speech, |a| |s|.\nParallel to the speech encoder is the word embeeding layer. It is the same as word embedding for text translation.\nBoth the speech encoder and word embedding layer are connect to Transformer encoder and then passed to the Transformer decoder. The Transformer encoder and decoder are using the same configuration as the original (Vaswani et al., 2017). To explain, the Transformer encoder further extracts the high-level semantic hidden representation of two modalities. The Transformer decoder generates the word sequences (transcription and translation) for ST, MT and ASR tasks. Since our model has a complete Transformer encoder-decoder as\na sub-module, this makes it possible to pre-train using large-scale extra MT parallel data.\nPrevious work has shown that multi-task learning on ST, MT and ASR improves translation performance (Indurthi et al., 2020; Tang et al., 2021b; Ye et al., 2021). Our training loss consists of the following elements.\nL = LST + LASR + LMT + λLCTR (1)\nwhere\nLST = − ∑ n logP (yn|sn)\nLASR = − ∑ n logP (xn|sn)\nLMT = − ∑ n logP (yn|xn)\nThe first three elements are cross-entropy losses on <speech, target text>, <speech, source text> and <source text, target text> pairs. These pairs are built from the triplet ST data. We also introduce a cross-modal contrastive loss term LCTR (see Section 3.2 for details). It aims to bring the representation between the speech and textual transcription modalities closer (its effect will be analyzed in detail in Section 6). λ is a tuned hyper-parameter of the weighted contrastive loss term."
    }, {
      "heading" : "3.2 Cross-modal Contrastive Learning",
      "text" : "As mentioned in the beginning, since we need to produce similar representations for the speech and transcript sharing the same semantic meanings, we propose cross-modal contrastive learning method to bring their representations closer together. The\nmain idea of cross-modal contrastive learning is to introduce a loss that brings speech and its corresponding transcript (positive example) near together while pushing irrelevant ones (negative examples) far apart.\nGiven a positive example of such a speechtranscript pair (s,x), we randomly pick a set of N − 1 transcripts {x−i } N−1 i=1 from the same batch as negative examples. For speech s and its transcript x, we first average them in terms of the time dimension,\nu = MeanPool(S-Enc(s)) (2)\nv = MeanPool(Emb(x)) (3)\nand apply the multi-class N-pair contrastive loss (Sohn, 2016): LCTR = − ∑ s,x log exp(sim(u, v)/τ)∑ xj∈A exp(sim(u, v(xj))/τ) (4) where A = {x} ∪ {x−i } N−1 i=1 , τ is the temperature hyper-parameter, and sim is the cosine similarity function sim(a, b) = a>b/‖a‖‖b‖. In the implementation, negative examples {x−i } N−1 i=1 are from the same training batch of data (Figure 2(b))."
    }, {
      "heading" : "3.3 Mining Hard Examples for Contrastive Learning",
      "text" : "To further enhance the contrastive learning, we introduce three strategies to mine additional hard examples. These strategies are at input and representation (gray shaded modules in Figure 2(a)). Specific schematic illustrations of each operations are shown in Figure 3. Span-Masked Augmentation We mask consecutive segments of an original audio waveform sequence s to obtain a new modified speech s′. We take s′ as an input to the model, and compute the contrastive loss its original corresponding transcript. We randomly sample without replacement all time steps in the original waveform of the speech to be the starting indices with a probability p, and then we set the sub-sequence M successive time steps to be blank. In the experiment, we tried multiple configurations, and found p = 0.25,M = 3600 the best, resulting in a masked span of 0.225 second. Since the masked speech fragment is very short, we consider the masked speech and the original transcript to be positive pairs, and the remaining transcripts in the same batch to be negative pairs.\nWord Repetition The word repetition strategy randomly replicates some words (or sub-words) in the original sentences, with two advantages for improving representation robustness. First, as the length of the sentence is shorter than that of its audio representation, randomly repeating the words in the sentence is a simple yet useful technique to increase the length. Second, repeating words does not change the semantics and is suitable as an extra positive example of the corresponding speech. Specifically, given sentence x, each sub-word token xi can be duplicated k more times, resulting in the duplicated sentence x′, where k = 0, 1, 2, ... and k ∼ Poisson(1). We regard x′ as the additional positive example for the speech s and the samples with the same operation in the same batch as the negative examples. Cut-off strategy Recent studies on natural language understanding and generation have proved cut-off data augmentation strategy to be successful (Shen et al., 2020; Yan et al., 2021). We analogize a similar idea to the cut-off data augment approach for speech representation. We entirely erase the T × d representation matrix along each dimension and set the erased terms to 0. Here, we present two variants: sequence cut-off , which erases some sequence dimension, and feature cut-off , which erases some feature dimension. Note that there is a difference between cut-off and dropout. Dropout randomly sets some elements to 0, while cut-off is a dimensional “block\" dropout. Similarly, we treat the cut-off audio representation and the original transcribed sentence as positive pairs, and the rest sentences in the same batch as negative pairs."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setups",
      "text" : "ST datasets We conduct experiments on three representative directions from MuST-C\ndataset 1 (Di Gangi et al., 2019): En-De, En-Fr and En-Ru. Due to the computation limatation, we do not preform for the rest language directions. As one of the largest ST benchmarks, MuST-C contains more than 385 hours of TED talks for each direction. MT datasets We introduce external WMT datasets (Bojar et al., 2016) for each translation direction, as the expanded setup.\nTable 6 (in Appendix. A) lists the statistics of all the datasets included. Model Configurations The Wav2vec2.0 in the S-Enc is only pre-trained on Librispeech (Panayotov et al., 2015) speech only without any downstream fine-tuning2. Two layers of CNNs after the Wav2vec2.0 are set to kernel size 5, stride size 2 and hidden size 512. The Transformer follows the base configuration, with 6 layers of encoder and decoder, hidden size d = 512, 8 attention heads, and 2048 FFN hidden states. We use pre-layer normalization for stable training. Experiment Details We evaluate case-sensitive detokenized BLEU using sacreBLEU3 (Post, 2018) on MuST-C tst-COMMON set. We also report the ChrF++ score 4 (Popović, 2017) and translation error rate (TER) 5 in the analysis. We use the raw 16-bit 16kHz mono-channel speech input. We jointly tokenize the bilingual text using SentencePiece (Kudo and Richardson, 2018), with a vocabulary size of 10k. For the training loss, we set contrastive temperature τ = 0.02 and weight of contrastive term λ = 1.5.\nAppendix B contains more detailed settings and explanations for the baseline models in Table 1. Appendix C shows the experiments on the choice of the hyper-parameters."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "Comparison with end-to-end ST models Table 1 shows the main results. Since many existing works regard “leveraging external data” to be one of their model’s features, their strong performances are largely predicated on the utilization of auxiliary\n1We use v1.0. https://ict.fbk.eu/must-c/ 2https://dl.fbaipublicfiles.com/ fairseq/wav2vec/wav2vec_small.pt 3https://github.com/mjpost/sacrebleu, BLEU Signature: nrefs:1 | bs:1000 | seed:12345 | case:mixed | eff:no | tok:13a | smooth:exp | version:2.0.0\n4ChrF2++ Signature: nrefs:1 | bs:1000 | seed:12345 | case:mixed | eff:yes | nc:6 | nw:2 | space:no | version:2.0.0\n5TER Signature: nrefs:1 | bs:1000 | seed:12345 | case:lc | tok:tercom | norm:no | punct:yes | asian:no | version:2.0.0\ndata, especially large-scale MT data. For a relatively fair comparison, we investigate two cases: (1) without external MT data and (2) with external MT data. Without the external MT data, our method already gains an average improvement of 0.5 BLEU over the previous best models. Also when speech data is introduced for pre-training, our method works better than others (Self-training, W-Transf. and XSTNet). When extra MT data are introduced, our method also outperforms SOTA by an average of 0.7 BLEU. Among the benchmark models, with the same goal of closing two modality gaps, Chimera (Han et al., 2021) constructed an extra fixed-length shared semantic space. However, the shared memory with fixed size actually compromises the MT performance, while our contrastive learning approach is more straightforward and effective. Comparison with cascaded ST systems We compare our method with several cascade baselines, where Ye et al. (2021) and Xu et al. (2021) provided two strong cascade systems trained using MuSTC and external ASR and MT data (LibriSpeech, WMT, and Opensubtitles). From Table 2, we find that as an end-to-end model, ConST can outperform these strong cascade models. In Appendix E, we provide a case study to show such improvement."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Is contrastive loss effective?",
      "text" : "With the same model architecture and the same pre-training + fine-tuning procedure, the main difference between ConST and XSTNet (Ye et al., 2021) is whether we use the contrastive loss term during the fine-tuning or not. Comparing the BLEU results of w/o and w/ external MT data situations in Table 1, we find that ConST further improves 0.5 and 0.7 BLEU scores in terms of three translation directions on average. This demonstrates the effectiveness of the cross-modal contrastive learning."
    }, {
      "heading" : "5.2 Which layer to contrast on?",
      "text" : "An intriguing question is which representations should be considered in the contrastive loss function. In the method part (Section 3.2), we use averaged audio representation u for speech s (Eq.(2)) and averaged lexical embedding v for the transcript x (Eq.(3)), denoted as low-level repr.. Whereas inspired by a recent study in multilingual MT (Pan et al., 2021), we also provide an alternative contrastive loss as a comparison, whose speech and\ntext features are average-pooled semantic representations derived from the Transformer encoder, denoted as high-level repr..\nTable 3 shows that contrastive learning using the low-level representations (Line 1) is better than using the high-level ones (Line 2). On the other hand, although the performance of Line 2 is relatively inferior, it still outperforms the multi-task model without the contrastive loss (Line 3)."
    }, {
      "heading" : "5.3 Is contrastive loss better than other losses?",
      "text" : "Our goal for introducing the contrastive loss term (denoted as CTR Loss) is to close the distance between speech and text representations. Whereas,\nthere are other options to achieve this goal, such as L2 loss and CTC loss. • L2 Loss: Without introducing any negative sam-\nples, L2 loss directly reduces the Euclidean distance between the representations of two modalities by minimizing L = ‖u − v‖2. L2 loss can be viewed as an implementation based on the idea of knowledge distillation (Heo et al., 2019; Dong et al., 2021b). • CTC Loss: The connectionist temporal classification (CTC) loss (Graves et al., 2006) is commonly used in speech-related tasks (Xu et al., 2021; Dong et al., 2021b). Unlike contrastive loss that cares about the representation, CTC loss connects the two modalities by establishing speech-text alignment and maximizing p(x|a) =\nπ∈Πs,a\n∏T\nt=1 pt(πt|a), where Πs,a is the set of all valid alignments. Compared to the other two ways of bridging the modality gap, L2 and CTC loss, is the contrastive loss term better? The answer is yes according to the results in Table 4. Our explanation is that information on the negative samples benefits the contrastive loss, bringing the the distance between the speech and its corresponding transcription closer while pushing the distance to the irrelevant text farther."
    }, {
      "heading" : "5.4 Analysis on the data augmentation strategies",
      "text" : "In Section 3.3, we proposed four methods to mine the hard examples for contrastive learning, namely span-masked augmentation (SMA), word repetition (Rep), sequence cut-off (SCut), and feature cut-off (FCut). In this section, we study how effective these methods are, and to do so, we consider the BLEU performances of their 5×5 combinations (Figure 4). Note that “Original” means the original contrastive loss in Eq.(4) without any data augmen-\ntation, and the diagonal in the heat map represents only one strategy used. For an easy and fair comparison, we set the weight of the contrastive term to 1.0 uniformly. We have the following observations.\nAll the data augmentation methods are effective. All the BLEU scores in Figure 4 exceed the strong multi-task model trained without contrastive learning (27.1). Among all the strategies, the combination of the original and SCut reaches the best result (28.3), and is better than the model without any expanded operations (p < 0.01). Generally, to find the best model, we suggest adopting multiple strategies and choosing the best checkpoint on the dev-set.\nThe combinations of the data augmentation methods and the “original” have relatively better performances. We argue that we need the original positive and negative examples to give more accurate representations (without any dropout) for contrastive learning. On the contrary, without the help of “original” loss, the performance with both sequence cut-off and feature cut-off is the worst in Figure 4, probably because too much information is lost by superimposing the two."
    }, {
      "heading" : "6 Why does cross-modal contrastive learning work? — Analysis on the Modality Gap",
      "text" : "As mentioned earlier, the existing multi-task training models cannot address the speech-text modality gap. Does ConST reduce the representation gap between speech and text?"
    }, {
      "heading" : "6.1 Visualization of Representation",
      "text" : "Does the speech-text modality gap exist without explicitly bridging the two? Speech-text modality gap means the discrepancy between the audio representations and transcription sentence embeddings. To visualize it, we plot the bivariate kernel density estimation (Parzen, 1962) (KDE) contour of the dim-reduced feature of them, where TSNE (Van der Maaten and Hinton, 2008) is used to reduce the dimension into two (Figure 5). Ideally, if the representations of speech and its corresponding transcript are similar, their KDEs will be similar, and thus the contour lines will overlap as much as possible. However, Figure 5(a) is the KDE contour of the multi-task framework without any explicit modeling to bring two modalities together (Ye et al., 2021). It shows that the representations are so dissimilar that they are organically divided into two clusters, i.e. speech-text modality gap exists. Does ConST reduce the modality gap? As shown in Figure 5(b), compared to the baseline model without contrastive learning, ConST with cross-modal contrastive learning is able to bring representations of different modalities much closer. This means that the audio representation contains more linguistic information similar to that of the textual transcription, which is more advantageous for the downstream ST generation through the shared Transformer encoder and Transformer decoder."
    }, {
      "heading" : "6.2 Cross-modal Retrieval",
      "text" : "How good is the cross-modal representation space learned from ConST? To answer this question, we conduct a retrieval experiment, i.e. finding the nearest (smallest cosine similarity) transcript based on the speech representation. We compare ConST model with the baseline without crossmodal contrastive learning and report the top-1 retrieval accuracy using (1) the low-level representations and (2) the high-level semantic representations, in Table 5.\nWhen retrieving the text using low-level representations, our method gains a substantial 79% increase over the baseline. In addition, we find that without explicit contrastive modeling, the baseline can achieve retrieval accuracy up to 94% according to the semantic representations outputted from the Transformer encoder. We believe that such high accuracy is automatically learned from the triplesupervised data itself under the multi-task learning\nframework. With such a degree of cross-modal alignment, if we construct the contrastive loss with semantic representations, its gain to the ST performance turns out to be limited, which exactly corroborates the findings in Section 5.2 – low-level representations are preferred in the cross-modal contrastive learning."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose ConST, a simple yet effective contrastive learning framework bridging the speech-text representation gap and facilitating the ST with limited data. We also provide feasible data augmentation methods to learn robust representations. The results on the MuST-C ST dataset prove the effectiveness of the method."
    }, {
      "heading" : "8 Broader Impact",
      "text" : "This work improves the performance of ST tasks on public datasets by learning speech representations that are more similar to text representations, but the model is far from being achieved for industrialgrade implementations. In real scenarios, for example, the original voice is noisier and the distribution of speech lengths is more complex than in the public dataset, which cannot be handled by an end-toend model alone. The shortcoming of this model is that it still needs a certain amount of labeled data for training, especially <speech,transcription> to learn better speech representation, and for the more than 7, 000 languages and dialects in the world, most of them do not have corresponding translations or even transcriptions, our method does not work in untranscribed scenarios. In this paper, we focus on the improvement brought by the better speech representation on the ST task, and obtained good results with hundreds of hours of speech data. We hope that our work achieves better results using more data (e.g. raw speech, raw text, ASR, MT data) in the future."
    }, {
      "heading" : "A Statistics of all datasets",
      "text" : ""
    }, {
      "heading" : "B Experimental Details",
      "text" : "Training and Implementation Details We use Adam optimizer (β1 = 0.9, β2 = 0.98) with learning rate = 1e−4 and warmup 25k steps during the ST training. We also implement the expanded setting with the introduction of external WMT to train the Transformer module. In the pre-training stage, we set the learning rate = 7e−4 and warmup 4000 steps. For robust training, we set label smoothing to 0.1, and dropout rate to 0.1. The hyper-parameters for different data augmentation methods are as follows: for masked audio span strategy, we set masking probability p = 0.25 and masking span length M = 3600 frames; for both sequence and feature cut-off, we set the cut-off dropout rate as 0.1. We save the checkpoint with the best BLEU on dev-set and average the last 10 checkpoints. For decoding, we use a beam size of 10 and length penalty 0.7 for German, 1.0 for French, and 0.4 for Russian. We train the models in 8 Nvidia Tesla V100 GPUs for each experiment. We use Fairseq (Ott et al., 2019) as the code-base for our implementation. Baseline Models In Table 1, we compared our method with end-to-end baseline models whose audio inputs are 80-channel log Mel-filter bank, including: FairseqST (Wang et al., 2020a), NeurST (Zhao et al., 2021a), Espnet ST (Inaguma et al., 2020), Dual-decoder Transformer (Le et al., 2020), SATE (Xu et al., 2021), Speechformer (Papi et al., 2021), self training (Pino et al., 2020) and mutual learning (Zhao et al., 2021b) method, STAST (Liu et al., 2020b), bi-KD (Inaguma et al., 2021), MLT method (Tang et al., 2021b), Lightweight Adaptor (Le et al., 2021), and JT-S-MT (Tang et al., 2021a), FAT-ST (Zheng et al., 2021), We also compare our method to baseline models that have pretrained Wav2vec2.0 as a module, including: • W-Transf. (Ye et al., 2021): the model has the\nsame structure as ours, but is only trained on <speech, translation> parallel data. • Chimera-ST (Han et al., 2021): the model that builds a shared semantic memory for both audio and text modalities. • XSTNet (Ye et al., 2021): the model has the same structure as ours, and adopted a multi-task fine-tuning strategy."
    }, {
      "heading" : "C The Choice for Hyper-parameters",
      "text" : "Influence of Temperature In the contrastive loss, the temperature hyper-parameter is provided to control the smoothness of the distribution normalized by softmax operation. A high temperature helps to smooth the distribution, making it more difficult for the model to distinguish between positive and negative samples (corresponding to correct transcriptions and other transcriptions in this work), while the low temperature behaves just the opposite. We choose several temperature hyper-parameters ranging from 0.01 to 0.5, and Figure 6 shows their BLEUs on the test and dev sets . We find that (1) the choice of the temperature does not drastically affect the final BLEU score, and (2) we recommend that the temperature τ be set between 0.02 and 0.05 to ensure a relatively good ST performance. In the experiment, we use τ = 0.02.\nInfluence of Contrastive Loss Weight The total loss we optimize, Eq.(1), is a linear combination of the multi-task cross-entropy losses LMLT and the contrastive term LCTR. To investigate how much the contrastive terms affect BLEU, we fix its temperature τ = 0.02, adjust the values of its loss weight λ from 0.1 to 2.0, performed three experiments for each value, and test the average BLEU on En-De tst-COMMON set. Figure 7 depicts the performances. First, all objective functions containing\nLCTR, even if their weights λ take different values, are apparently better than the baseline model with LMLT only LCTR. Then, the best BLEU score is achieved at loss weight λ = 1.5, corresponding to the results in Table 1. And when analyzing the effect of data augmentation strategies (Section 5.4), since we need to consider the combination between them, which is more complicated. Therefore, we set the loss weight to 1.0 uniformly for simplicity. In general, we recommend that the weight hyperparameter takes a value between 0.8 and 1.5."
    }, {
      "heading" : "D Data Scale for Fine-tuning",
      "text" : "The experiments in the main paper show that our model can perform well by introducing external MT data pre-training. Here, we simulate the scenario with plenty of MT and speech data and limited ST triple-labeled data, and does ConST have the ability of low-resource learning? In the experiment, we reduce the labeled ST data to 1, 10, and 100 hours, corresponding to sentence counts of about 500, 5k, and 50k sentences. For a fair comparison, we use the same MT pre-trained Transformer module as in the main paper. We find the contrastive loss particularly helpful when the amount of speech data is extremely small, like only 1 hour of speech. Second, the multi-task training strategy is also very effective in improving the robustness of the model performance. We also find that by using easily accessible MT and speech pretraining, our model could reach the previous baseline results without pre-training using only 1/4 of the original data, i.e. 100 hours of labeled ST data."
    }, {
      "heading" : "E Case Analysis",
      "text" : "In this section, we use several cased that our proposed ConST model generates to compare our model with the cascaded model and the previous end-to-end model, XSTNet6 (Ye et al., 2021).\nFor this first case, the cascaded system fails to give a right translation due to the mis-punctuation issue (klingt is a verb), while the end-to-end model, XSTNet and ConST translate correctly. For the second case, the previous end-to-end XSTNet model cannot accurately translate the phrase “started exploring this idea of”, which performs worse than the cascaded one. Whereas ConST successfully conveys the meaning of “this idea” , and translates more accurately than XSTNet. We believe this improvement comes from the cross-modal contrastive learning.\n6The generation cases of the previous models can be found at https://reneeye.github.io/projects/ XSTNet."
    } ],
    "references" : [ {
      "title" : "Findings of the iwslt",
      "author" : [ "Jiatao Gu" ],
      "venue" : null,
      "citeRegEx" : "Gu,? \\Q2020\\E",
      "shortCiteRegEx" : "Gu",
      "year" : 2020
    }, {
      "title" : "Cascade versus direct speech",
      "author" : [ "Marco Turchi" ],
      "venue" : null,
      "citeRegEx" : "Turchi.,? \\Q2021\\E",
      "shortCiteRegEx" : "Turchi.",
      "year" : 2021
    }, {
      "title" : "Listen and translate: A",
      "author" : [ "Laurent Besacier" ],
      "venue" : null,
      "citeRegEx" : "Besacier.,? \\Q2016\\E",
      "shortCiteRegEx" : "Besacier.",
      "year" : 2016
    }, {
      "title" : "Findings of the 2016 conference",
      "author" : [ "Zampieri" ],
      "venue" : null,
      "citeRegEx" : "2016.,? \\Q2016\\E",
      "shortCiteRegEx" : "2016.",
      "year" : 2016
    }, {
      "title" : "Specrec: An alternative solution for",
      "author" : [ "Huang" ],
      "venue" : null,
      "citeRegEx" : "2021.,? \\Q2021\\E",
      "shortCiteRegEx" : "2021.",
      "year" : 2021
    }, {
      "title" : "MuST-C: a Multilingual Speech Translation Corpus",
      "author" : [ "Mattia A. Di Gangi", "Roldano Cattoni", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proc. of NAACL-HLT, pages 2012–2017.",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Proc. of NeurIPS, pages",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Speechtransformer: A no-recurrence sequence-to-sequence model for speech recognition",
      "author" : [ "Linhao Dong", "Shuang Xu", "Bo Xu." ],
      "venue" : "Proc. of ICASSP, pages 5884–5888.",
      "citeRegEx" : "Dong et al\\.,? 2018",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2018
    }, {
      "title" : "Consecutive decoding for speech-to-text translation",
      "author" : [ "Qianqian Dong", "Mingxuan Wang", "Hao Zhou", "Shuang Xu", "Bo Xu", "Lei Li." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Dong et al\\.,? 2021a",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2021
    }, {
      "title" : "Listen, understand and translate: Triple supervision decouples end-to-end speech-to-text translation",
      "author" : [ "Qianqian Dong", "Rong Ye", "Mingxuan Wang", "Hao Zhou", "Shuang Xu", "Bo Xu", "Lei Li." ],
      "venue" : "Proc. of AAAI, volume 35, pages 12749–12759.",
      "citeRegEx" : "Dong et al\\.,? 2021b",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2021
    }, {
      "title" : "CERT: contrastive self-supervised learning for language understanding",
      "author" : [ "Hongchao Fang", "Pengtao Xie." ],
      "venue" : "CoRR, abs/2005.12766.",
      "citeRegEx" : "Fang and Xie.,? 2020",
      "shortCiteRegEx" : "Fang and Xie.",
      "year" : 2020
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino J. Gomez", "Jürgen Schmidhuber." ],
      "venue" : "Proc. of ICML, volume 148, pages 369–376.",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Lstm: A search space odyssey",
      "author" : [ "Klaus Greff", "Rupesh K Srivastava", "Jan Koutník", "Bas R Steunebrink", "Jürgen Schmidhuber." ],
      "venue" : "IEEE transactions on neural networks and learning systems, 28(10):2222– 2232.",
      "citeRegEx" : "Greff et al\\.,? 2016",
      "shortCiteRegEx" : "Greff et al\\.",
      "year" : 2016
    }, {
      "title" : "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "Michael Gutmann", "Aapo Hyvärinen." ],
      "venue" : "Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 297–304.",
      "citeRegEx" : "Gutmann and Hyvärinen.,? 2010",
      "shortCiteRegEx" : "Gutmann and Hyvärinen.",
      "year" : 2010
    }, {
      "title" : "Learning shared semantic space for speech-to-text translation",
      "author" : [ "Chi Han", "Mingxuan Wang", "Heng Ji", "Lei Li." ],
      "venue" : "Proc. of ACL - Findings.",
      "citeRegEx" : "Han et al\\.,? 2021",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2021
    }, {
      "title" : "A comprehensive overhaul of feature distillation",
      "author" : [ "Byeongho Heo", "Jeesoo Kim", "Sangdoo Yun", "Hyojin Park", "Nojun Kwak", "Jin Young Choi." ],
      "venue" : "Proc. of the ICCV, pages 1921–1930.",
      "citeRegEx" : "Heo et al\\.,? 2019",
      "shortCiteRegEx" : "Heo et al\\.",
      "year" : 2019
    }, {
      "title" : "Source and target bidirectional knowledge distillation for end-to-end speech translation",
      "author" : [ "Hirofumi Inaguma", "Tatsuya Kawahara", "Shinji Watanabe." ],
      "venue" : "Proc. of NAACL, pages 1872–1881.",
      "citeRegEx" : "Inaguma et al\\.,? 2021",
      "shortCiteRegEx" : "Inaguma et al\\.",
      "year" : 2021
    }, {
      "title" : "ESPnet-ST: All-in-one speech translation toolkit",
      "author" : [ "Hirofumi Inaguma", "Shun Kiyono", "Kevin Duh", "Shigeki Karita", "Nelson Yalta", "Tomoki Hayashi", "Shinji Watanabe." ],
      "venue" : "Proc. of ACL, pages 302–311.",
      "citeRegEx" : "Inaguma et al\\.,? 2020",
      "shortCiteRegEx" : "Inaguma et al\\.",
      "year" : 2020
    }, {
      "title" : "Data efficient direct speech-to-text translation with modality agnostic meta-learning",
      "author" : [ "Sathish Indurthi", "Houjeung Han", "Nikhil Kumar Lakumarapu", "Beomseok Lee", "Insoo Chung", "Sangha Kim", "Chanwoo Kim." ],
      "venue" : "Proc. of ICASSP. IEEE.",
      "citeRegEx" : "Indurthi et al\\.,? 2020",
      "shortCiteRegEx" : "Indurthi et al\\.",
      "year" : 2020
    }, {
      "title" : "Structured-based curriculum learning for endto-end english-japanese speech translation",
      "author" : [ "Takatomo Kano", "Sakriani Sakti", "Satoshi Nakamura." ],
      "venue" : "Proc. of INTERSPEECH, pages 2630–2634.",
      "citeRegEx" : "Kano et al\\.,? 2017",
      "shortCiteRegEx" : "Kano et al\\.",
      "year" : 2017
    }, {
      "title" : "Supervised contrastive learning",
      "author" : [ "Prannay Khosla", "Piotr Teterwak", "Chen Wang", "Aaron Sarna", "Yonglong Tian", "Phillip Isola", "Aaron Maschinot", "Ce Liu", "Dilip Krishnan." ],
      "venue" : "Proc. of NeurIPS, 33.",
      "citeRegEx" : "Khosla et al\\.,? 2020",
      "shortCiteRegEx" : "Khosla et al\\.",
      "year" : 2020
    }, {
      "title" : "Europarl: A parallel corpus for statistical machine translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "MT summit, volume 5, pages 79–86. Citeseer.",
      "citeRegEx" : "Koehn,? 2005",
      "shortCiteRegEx" : "Koehn",
      "year" : 2005
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proc. of EMNLP, pages 66–71.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "Proc. of NeurIPS.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Dualdecoder transformer for joint automatic speech recognition and multilingual speech translation",
      "author" : [ "Hang Le", "Juan Pino", "Changhan Wang", "Jiatao Gu", "Didier Schwab", "Laurent Besacier." ],
      "venue" : "Proc. of COLING, pages 3520–3533.",
      "citeRegEx" : "Le et al\\.,? 2020",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "Lightweight adapter tuning for multilingual speech translation",
      "author" : [ "Hang Le", "Juan Pino", "Changhan Wang", "Jiatao Gu", "Didier Schwab", "Laurent Besacier." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Le et al\\.,? 2021",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2021
    }, {
      "title" : "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
      "author" : [ "Wei Li", "Can Gao", "Guocheng Niu", "Xinyan Xiao", "Hao Liu", "Jiachen Liu", "Hua Wu", "Haifeng Wang" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles",
      "author" : [ "Pierre Lison", "Jörg Tiedemann." ],
      "venue" : "Proc. of LREC, pages 923–929.",
      "citeRegEx" : "Lison and Tiedemann.,? 2016",
      "shortCiteRegEx" : "Lison and Tiedemann.",
      "year" : 2016
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "TACL, 8:726–742.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Bridging the modality gap for speech-to-text translation",
      "author" : [ "Yuchen Liu", "Junnan Zhu", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "arXiv preprint arXiv:2010.14920.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proc. of NAACL Demonstrations, pages 48–53.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Contrastive learning for many-to-many multilingual neural machine translation",
      "author" : [ "Xiao Pan", "Liwei Wu", "Mingxuan Wang", "Lei Li." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Pan et al\\.,? 2021",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2021
    }, {
      "title" : "Librispeech: An ASR corpus based on public domain audio books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "Proc. of ICASSP, pages 5206–5210.",
      "citeRegEx" : "Panayotov et al\\.,? 2015",
      "shortCiteRegEx" : "Panayotov et al\\.",
      "year" : 2015
    }, {
      "title" : "Speechformer: Reducing information loss in direct speech translation",
      "author" : [ "Sara Papi", "Marco Gaido", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Papi et al\\.,? 2021",
      "shortCiteRegEx" : "Papi et al\\.",
      "year" : 2021
    }, {
      "title" : "Specaugment: A simple augmentation method for automatic speech recognition",
      "author" : [ "Daniel S. Park", "William Chan", "Yu Zhang", "ChungCheng Chiu", "Barret Zoph", "Ekin Dogus Cubuk", "Quoc V. Le." ],
      "venue" : "Proc. of INTERSPEECH.",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "On estimation of a probability density function and mode",
      "author" : [ "Emanuel Parzen." ],
      "venue" : "The annals of mathematical statistics, 33(3):1065–1076.",
      "citeRegEx" : "Parzen.,? 1962",
      "shortCiteRegEx" : "Parzen.",
      "year" : 1962
    }, {
      "title" : "Self-training for end-to-end speech translation",
      "author" : [ "Juan Pino", "Qiantong Xu", "Xutai Ma", "Mohammad Javad Dousti", "Yun Tang." ],
      "venue" : "Proc. of INTERSPEECH, pages 1476–1480.",
      "citeRegEx" : "Pino et al\\.,? 2020",
      "shortCiteRegEx" : "Pino et al\\.",
      "year" : 2020
    }, {
      "title" : "chrf++: words helping character n-grams",
      "author" : [ "Maja Popović." ],
      "venue" : "Proceedings of the second conference on machine translation, pages 612–618.",
      "citeRegEx" : "Popović.,? 2017",
      "shortCiteRegEx" : "Popović.",
      "year" : 2017
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Srpol’s system for the iwslt 2020 end-to-end speech translation task",
      "author" : [ "Tomasz Potapczyk", "Paweł Przybysz." ],
      "venue" : "Proc. of IWSLT, pages 89–94.",
      "citeRegEx" : "Potapczyk and Przybysz.,? 2020",
      "shortCiteRegEx" : "Potapczyk and Przybysz.",
      "year" : 2020
    }, {
      "title" : "Learning speaker representations with mutual information",
      "author" : [ "Mirco Ravanelli", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1812.00271.",
      "citeRegEx" : "Ravanelli and Bengio.,? 2018",
      "shortCiteRegEx" : "Ravanelli and Bengio.",
      "year" : 2018
    }, {
      "title" : "Selective and invariant neural responses to spoken and written narratives",
      "author" : [ "Mor Regev", "Christopher J Honey", "Erez Simony", "Uri Hasson." ],
      "venue" : "Journal of Neuroscience, 33(40):15978–15988.",
      "citeRegEx" : "Regev et al\\.,? 2013",
      "shortCiteRegEx" : "Regev et al\\.",
      "year" : 2013
    }, {
      "title" : "wav2vec: Unsupervised pre-training for speech recognition",
      "author" : [ "Steffen Schneider", "Alexei Baevski", "Ronan Collobert", "Michael Auli." ],
      "venue" : "Proc. of INTERSPEECH.",
      "citeRegEx" : "Schneider et al\\.,? 2019",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2019
    }, {
      "title" : "Facenet: A unified embedding for face recognition and clustering",
      "author" : [ "Florian Schroff", "Dmitry Kalenichenko", "James Philbin." ],
      "venue" : "Proc. of CVPR, pages 815–823.",
      "citeRegEx" : "Schroff et al\\.,? 2015",
      "shortCiteRegEx" : "Schroff et al\\.",
      "year" : 2015
    }, {
      "title" : "A simple but toughto-beat data augmentation approach for natural language understanding and generation",
      "author" : [ "Dinghan Shen", "Mingzhi Zheng", "Yelong Shen", "Yanru Qu", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:2009.13818.",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved deep metric learning with multi-class n-pair loss objective",
      "author" : [ "Kihyuk Sohn." ],
      "venue" : "Proc. of NeurIPS, volume 29, pages 1857–1865. Curran Associates, Inc.",
      "citeRegEx" : "Sohn.,? 2016",
      "shortCiteRegEx" : "Sohn.",
      "year" : 2016
    }, {
      "title" : "Improving speech translation by understanding and learning from the auxiliary text translation task",
      "author" : [ "Yun Tang", "Juan Pino", "Xian Li", "Changhan Wang", "Dmitriy Genzel." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Tang et al\\.,? 2021a",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "A general multi-task learning framework to leverage text data for speech to text tasks",
      "author" : [ "Yun Tang", "Juan Pino", "Changhan Wang", "Xutai Ma", "Dmitriy Genzel." ],
      "venue" : "Proc. of ICASSP, pages 6209–6213. IEEE.",
      "citeRegEx" : "Tang et al\\.,? 2021b",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "Parallel wavenet: Fast high-fidelity speech synthesis",
      "author" : [ "Dieleman", "Erich Elsen", "Nal Kalchbrenner", "Heiga Zen", "Alex Graves", "Helen King", "Tom Walters", "Dan Belov", "Demis Hassabis." ],
      "venue" : "Proc. of ICML, volume 80 of Proceedings of Machine",
      "citeRegEx" : "Dieleman et al\\.,? 2018",
      "shortCiteRegEx" : "Dieleman et al\\.",
      "year" : 2018
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(11).",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proc. of NeurIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Fairseq s2t: Fast speech-to-text modeling with fairseq",
      "author" : [ "Changhan Wang", "Yun Tang", "Xutai Ma", "Anne Wu", "Dmytro Okhonko", "Juan Pino." ],
      "venue" : "Proc. of AACL, pages 33–39.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Largescale self-and semi-supervised learning for speech translation",
      "author" : [ "Changhan Wang", "Anne Wu", "Juan Pino", "Alexei Baevski", "Michael Auli", "Alexis Conneau." ],
      "venue" : "arXiv preprint arXiv:2104.06678.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Curriculum pre-training for end-to-end speech translation",
      "author" : [ "Chengyi Wang", "Yu Wu", "Shujie Liu", "Ming Zhou", "Zhenglu Yang." ],
      "venue" : "Proc. of ACL, pages 3728–3738.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-tosequence models can directly translate foreign speech",
      "author" : [ "Ron J. Weiss", "Jan Chorowski", "Navdeep Jaitly", "Yonghui Wu", "Zhifeng Chen." ],
      "venue" : "Proc. of INTERSPEECH, pages 2625– 2629.",
      "citeRegEx" : "Weiss et al\\.,? 2017",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2017
    }, {
      "title" : "Convergence of spoken and written language processing in the superior temporal sulcus",
      "author" : [ "Stephen M. Wilson", "Alexa Bautista", "Angelica McCarron." ],
      "venue" : "NeuroImage, 171:62–74.",
      "citeRegEx" : "Wilson et al\\.,? 2018",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-supervised representations improve end-to-end speech translation",
      "author" : [ "Anne Wu", "Changhan Wang", "Juan Pino", "Jiatao Gu." ],
      "venue" : "Proc. of INTERSPEECH.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Esimcse: Enhanced sample building method for contrastive learning of unsupervised sentence embedding",
      "author" : [ "Xing Wu", "Chaochen Gao", "Liangjun Zang", "Jizhong Han", "Zhongyuan Wang", "Songlin Hu." ],
      "venue" : "arXiv preprint arXiv:2109.04380.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Stacked acoustic-and-textual encoding: Integrating the pretrained models into speech translation encoders",
      "author" : [ "Chen Xu", "Bojie Hu", "Yanyang Li", "Yuhao Zhang", "Qi Ju", "Tong Xiao", "Jingbo Zhu" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Xu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "Consert: A contrastive framework for self-supervised sentence representation transfer",
      "author" : [ "Yuanmeng Yan", "Rumei Li", "Sirui Wang", "Fuzheng Zhang", "Wei Wu", "Weiran Xu" ],
      "venue" : null,
      "citeRegEx" : "Yan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "End-toend speech translation via cross-modal progressive training",
      "author" : [ "Rong Ye", "Mingxuan Wang", "Lei Li." ],
      "venue" : "Proc. of INTERSPEECH.",
      "citeRegEx" : "Ye et al\\.,? 2021",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2021
    }, {
      "title" : "NeurST: Neural speech translation toolkit",
      "author" : [ "Chengqi Zhao", "Mingxuan Wang", "Qianqian Dong", "Rong Ye", "Lei Li." ],
      "venue" : "Proc. of ACL - System Demonstrations.",
      "citeRegEx" : "Zhao et al\\.,? 2021a",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "Mutual-learning improves end-toend speech translation",
      "author" : [ "Jiawei Zhao", "Wei Luo", "Boxing Chen", "Andrew Gilman." ],
      "venue" : "Proc. of the EMNLP, pages 3989–3994.",
      "citeRegEx" : "Zhao et al\\.,? 2021b",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation",
      "author" : [ "Renjie Zheng", "Junkun Chen", "Mingbo Ma", "Liang Huang." ],
      "venue" : "Proc. of ICML.",
      "citeRegEx" : "Zheng et al\\.,? 2021",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Unified vision-language pre-training for image captioning and vqa",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason Corso", "Jianfeng Gao." ],
      "venue" : "Proc. of AAAI, volume 34, pages 13041–13049.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Compared with the conventional cascade ST models, E2E ST models have achieved almost comparable (Bentivogli et al., 2021; Dong et al., 2018) or even superior (Ansari et al.",
      "startOffset" : 96,
      "endOffset" : 140
    }, {
      "referenceID" : 41,
      "context" : ", 2018) or even superior (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Xu et al., 2021) performance.",
      "startOffset" : 25,
      "endOffset" : 93
    }, {
      "referenceID" : 60,
      "context" : ", 2018) or even superior (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Xu et al., 2021) performance.",
      "startOffset" : 25,
      "endOffset" : 93
    }, {
      "referenceID" : 65,
      "context" : "This can be realized through pre-training approaches (Zheng et al., 2021) or multi-task training frameworks (Tang et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 49,
      "context" : ", 2021) or multi-task training frameworks (Tang et al., 2021b; Ye et al., 2021; Han et al., 2021).",
      "startOffset" : 42,
      "endOffset" : 97
    }, {
      "referenceID" : 62,
      "context" : ", 2021) or multi-task training frameworks (Tang et al., 2021b; Ye et al., 2021; Han et al., 2021).",
      "startOffset" : 42,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : ", 2021) or multi-task training frameworks (Tang et al., 2021b; Ye et al., 2021; Han et al., 2021).",
      "startOffset" : 42,
      "endOffset" : 97
    }, {
      "referenceID" : 43,
      "context" : "What is the right representation? A recent neurocognitive study reveals that the human brain processes speech and written text at the same region of the cortex (Regev et al., 2013).",
      "startOffset" : 160,
      "endOffset" : 180
    }, {
      "referenceID" : 57,
      "context" : "Listening to spoken utterance and reading its corresponding sentence result in the same activation patterns in the superior temporal sulcus (Wilson et al., 2018).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "Inspired by the recent progress of contrastive learning approaches in cross-lingual (Lample and Conneau, 2019; Pan et al., 2021) and cross-modal vision-and-language domains (Li et al.",
      "startOffset" : 84,
      "endOffset" : 128
    }, {
      "referenceID" : 33,
      "context" : "Inspired by the recent progress of contrastive learning approaches in cross-lingual (Lample and Conneau, 2019; Pan et al., 2021) and cross-modal vision-and-language domains (Li et al.",
      "startOffset" : 84,
      "endOffset" : 128
    }, {
      "referenceID" : 27,
      "context" : ", 2021) and cross-modal vision-and-language domains (Li et al., 2021; Zhou et al., 2020; Dong et al., 2019), we designed a simple contrastive learning method for ST (ConST) to learn the representations that meet the afore-",
      "startOffset" : 52,
      "endOffset" : 107
    }, {
      "referenceID" : 66,
      "context" : ", 2021) and cross-modal vision-and-language domains (Li et al., 2021; Zhou et al., 2020; Dong et al., 2019), we designed a simple contrastive learning method for ST (ConST) to learn the representations that meet the afore-",
      "startOffset" : 52,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : ", 2021) and cross-modal vision-and-language domains (Li et al., 2021; Zhou et al., 2020; Dong et al., 2019), we designed a simple contrastive learning method for ST (ConST) to learn the representations that meet the afore-",
      "startOffset" : 52,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "(2021a) implemented several off-the-shelf encoderdecoder E2E-ST models, such as BiLSTM (Greff et al., 2016) and Speech-Transformer (Dong et al.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "Methods, like data augmentation (Park et al., 2019; Pino et al., 2020; Chen et al., 2021), pre-training (Weiss et al.",
      "startOffset" : 32,
      "endOffset" : 89
    }, {
      "referenceID" : 38,
      "context" : "Methods, like data augmentation (Park et al., 2019; Pino et al., 2020; Chen et al., 2021), pre-training (Weiss et al.",
      "startOffset" : 32,
      "endOffset" : 89
    }, {
      "referenceID" : 56,
      "context" : ", 2021), pre-training (Weiss et al., 2017; Berard et al., 2018; Bansal et al., 2019; Wang et al., 2020b; Alinejad and Sarkar, 2020; Dong et al., 2021a; Zheng et al., 2021), self-training (Pino et al.",
      "startOffset" : 22,
      "endOffset" : 171
    }, {
      "referenceID" : 55,
      "context" : ", 2021), pre-training (Weiss et al., 2017; Berard et al., 2018; Bansal et al., 2019; Wang et al., 2020b; Alinejad and Sarkar, 2020; Dong et al., 2021a; Zheng et al., 2021), self-training (Pino et al.",
      "startOffset" : 22,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : ", 2021), pre-training (Weiss et al., 2017; Berard et al., 2018; Bansal et al., 2019; Wang et al., 2020b; Alinejad and Sarkar, 2020; Dong et al., 2021a; Zheng et al., 2021), self-training (Pino et al.",
      "startOffset" : 22,
      "endOffset" : 171
    }, {
      "referenceID" : 65,
      "context" : ", 2021), pre-training (Weiss et al., 2017; Berard et al., 2018; Bansal et al., 2019; Wang et al., 2020b; Alinejad and Sarkar, 2020; Dong et al., 2021a; Zheng et al., 2021), self-training (Pino et al.",
      "startOffset" : 22,
      "endOffset" : 171
    }, {
      "referenceID" : 38,
      "context" : ", 2021), self-training (Pino et al., 2020; Wang et al., 2021), utilizing self-supervised pre-trained audio representation (Wu et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 54,
      "context" : ", 2021), self-training (Pino et al., 2020; Wang et al., 2021), utilizing self-supervised pre-trained audio representation (Wu et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 58,
      "context" : ", 2021), utilizing self-supervised pre-trained audio representation (Wu et al., 2020; Han et al., 2021; Ye et al., 2021; Wang et al., 2021), are proved to be effective.",
      "startOffset" : 68,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : ", 2021), utilizing self-supervised pre-trained audio representation (Wu et al., 2020; Han et al., 2021; Ye et al., 2021; Wang et al., 2021), are proved to be effective.",
      "startOffset" : 68,
      "endOffset" : 139
    }, {
      "referenceID" : 62,
      "context" : ", 2021), utilizing self-supervised pre-trained audio representation (Wu et al., 2020; Han et al., 2021; Ye et al., 2021; Wang et al., 2021), are proved to be effective.",
      "startOffset" : 68,
      "endOffset" : 139
    }, {
      "referenceID" : 54,
      "context" : ", 2021), utilizing self-supervised pre-trained audio representation (Wu et al., 2020; Han et al., 2021; Ye et al., 2021; Wang et al., 2021), are proved to be effective.",
      "startOffset" : 68,
      "endOffset" : 139
    }, {
      "referenceID" : 25,
      "context" : "Also, multi-task frameworks (Le et al., 2020; Tang et al., 2021b; Ye et al., 2021) are widely applied to further enhance the robustness for ST.",
      "startOffset" : 28,
      "endOffset" : 82
    }, {
      "referenceID" : 49,
      "context" : "Also, multi-task frameworks (Le et al., 2020; Tang et al., 2021b; Ye et al., 2021) are widely applied to further enhance the robustness for ST.",
      "startOffset" : 28,
      "endOffset" : 82
    }, {
      "referenceID" : 62,
      "context" : "Also, multi-task frameworks (Le et al., 2020; Tang et al., 2021b; Ye et al., 2021) are widely applied to further enhance the robustness for ST.",
      "startOffset" : 28,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "(Han et al., 2021) designed a fix-size semantic memory module to bridge such a gap, from the neuroscience perspective.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "The contrastive learning method was first proposed to learn representations from unlabeled datasets (hence the term, self-supervised learning) by telling which data points are similar or distinct, especially in the field of computer vision (Chopra et al., 2005; Gutmann and Hyvärinen, 2010; Schroff et al., 2015; Sohn, 2016; Oord et al., 2018).",
      "startOffset" : 240,
      "endOffset" : 343
    }, {
      "referenceID" : 45,
      "context" : "The contrastive learning method was first proposed to learn representations from unlabeled datasets (hence the term, self-supervised learning) by telling which data points are similar or distinct, especially in the field of computer vision (Chopra et al., 2005; Gutmann and Hyvärinen, 2010; Schroff et al., 2015; Sohn, 2016; Oord et al., 2018).",
      "startOffset" : 240,
      "endOffset" : 343
    }, {
      "referenceID" : 47,
      "context" : "The contrastive learning method was first proposed to learn representations from unlabeled datasets (hence the term, self-supervised learning) by telling which data points are similar or distinct, especially in the field of computer vision (Chopra et al., 2005; Gutmann and Hyvärinen, 2010; Schroff et al., 2015; Sohn, 2016; Oord et al., 2018).",
      "startOffset" : 240,
      "endOffset" : 343
    }, {
      "referenceID" : 31,
      "context" : "The contrastive learning method was first proposed to learn representations from unlabeled datasets (hence the term, self-supervised learning) by telling which data points are similar or distinct, especially in the field of computer vision (Chopra et al., 2005; Gutmann and Hyvärinen, 2010; Schroff et al., 2015; Sohn, 2016; Oord et al., 2018).",
      "startOffset" : 240,
      "endOffset" : 343
    }, {
      "referenceID" : 42,
      "context" : "In speech processing, representative methods focused on speaker identification (Ravanelli and Bengio, 2018), speech recognition (Schneider et al.",
      "startOffset" : 79,
      "endOffset" : 107
    }, {
      "referenceID" : 44,
      "context" : "In speech processing, representative methods focused on speaker identification (Ravanelli and Bengio, 2018), speech recognition (Schneider et al., 2019), and audio representation learning (van den Oord et al.",
      "startOffset" : 128,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "In the NLP area, the contrastive framework is used for sentence representation learning (Fang and Xie, 2020; Shen et al., 2020; Gao et al., 2021; Wu et al., 2021; Yan et al., 2021) and machine translation Pan et al.",
      "startOffset" : 88,
      "endOffset" : 180
    }, {
      "referenceID" : 46,
      "context" : "In the NLP area, the contrastive framework is used for sentence representation learning (Fang and Xie, 2020; Shen et al., 2020; Gao et al., 2021; Wu et al., 2021; Yan et al., 2021) and machine translation Pan et al.",
      "startOffset" : 88,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : "In the NLP area, the contrastive framework is used for sentence representation learning (Fang and Xie, 2020; Shen et al., 2020; Gao et al., 2021; Wu et al., 2021; Yan et al., 2021) and machine translation Pan et al.",
      "startOffset" : 88,
      "endOffset" : 180
    }, {
      "referenceID" : 59,
      "context" : "In the NLP area, the contrastive framework is used for sentence representation learning (Fang and Xie, 2020; Shen et al., 2020; Gao et al., 2021; Wu et al., 2021; Yan et al., 2021) and machine translation Pan et al.",
      "startOffset" : 88,
      "endOffset" : 180
    }, {
      "referenceID" : 61,
      "context" : "In the NLP area, the contrastive framework is used for sentence representation learning (Fang and Xie, 2020; Shen et al., 2020; Gao et al., 2021; Wu et al., 2021; Yan et al., 2021) and machine translation Pan et al.",
      "startOffset" : 88,
      "endOffset" : 180
    }, {
      "referenceID" : 6,
      "context" : "Very recently, contrastive learning is also applied to learning a unified representation of image and text (Dong et al., 2019; Zhou et al., 2020; Li et al., 2021).",
      "startOffset" : 107,
      "endOffset" : 162
    }, {
      "referenceID" : 66,
      "context" : "Very recently, contrastive learning is also applied to learning a unified representation of image and text (Dong et al., 2019; Zhou et al., 2020; Li et al., 2021).",
      "startOffset" : 107,
      "endOffset" : 162
    }, {
      "referenceID" : 27,
      "context" : "Very recently, contrastive learning is also applied to learning a unified representation of image and text (Dong et al., 2019; Zhou et al., 2020; Li et al., 2021).",
      "startOffset" : 107,
      "endOffset" : 162
    }, {
      "referenceID" : 52,
      "context" : "The Transformer encoder and decoder are using the same configuration as the original (Vaswani et al., 2017).",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 19,
      "context" : "Previous work has shown that multi-task learning on ST, MT and ASR improves translation performance (Indurthi et al., 2020; Tang et al., 2021b; Ye et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 160
    }, {
      "referenceID" : 49,
      "context" : "Previous work has shown that multi-task learning on ST, MT and ASR improves translation performance (Indurthi et al., 2020; Tang et al., 2021b; Ye et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 160
    }, {
      "referenceID" : 62,
      "context" : "Previous work has shown that multi-task learning on ST, MT and ASR improves translation performance (Indurthi et al., 2020; Tang et al., 2021b; Ye et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 160
    }, {
      "referenceID" : 47,
      "context" : "and apply the multi-class N-pair contrastive loss (Sohn, 2016):",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 46,
      "context" : "Cut-off strategy Recent studies on natural language understanding and generation have proved cut-off data augmentation strategy to be successful (Shen et al., 2020; Yan et al., 2021).",
      "startOffset" : 145,
      "endOffset" : 182
    }, {
      "referenceID" : 61,
      "context" : "Cut-off strategy Recent studies on natural language understanding and generation have proved cut-off data augmentation strategy to be successful (Shen et al., 2020; Yan et al., 2021).",
      "startOffset" : 145,
      "endOffset" : 182
    }, {
      "referenceID" : 34,
      "context" : "0 in the S-Enc is only pre-trained on Librispeech (Panayotov et al., 2015) speech only without any downstream fine-tuning2.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 40,
      "context" : "Experiment Details We evaluate case-sensitive detokenized BLEU using sacreBLEU3 (Post, 2018) on MuST-C tst-COMMON set.",
      "startOffset" : 80,
      "endOffset" : 92
    }, {
      "referenceID" : 39,
      "context" : "We also report the ChrF++ score 4 (Popović, 2017) and translation error rate (TER) 5 in the analysis.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "We jointly tokenize the bilingual text using SentencePiece (Kudo and Richardson, 2018), with a vocabulary size of 10k.",
      "startOffset" : 59,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "Among the benchmark models, with the same goal of closing two modality gaps, Chimera (Han et al., 2021) constructed an extra fixed-length shared semantic space.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 62,
      "context" : "With the same model architecture and the same pre-training + fine-tuning procedure, the main difference between ConST and XSTNet (Ye et al., 2021) is whether we use the contrastive loss term during the fine-tuning or not.",
      "startOffset" : 129,
      "endOffset" : 146
    }, {
      "referenceID" : 33,
      "context" : "Whereas inspired by a recent study in multilingual MT (Pan et al., 2021), we also provide an alternative contrastive loss as a comparison, whose speech and",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : "† use external 40M OpenSubtitles (Lison and Tiedemann, 2016) MT data.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "L2 loss can be viewed as an implementation based on the idea of knowledge distillation (Heo et al., 2019; Dong et al., 2021b).",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "L2 loss can be viewed as an implementation based on the idea of knowledge distillation (Heo et al., 2019; Dong et al., 2021b).",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "• CTC Loss: The connectionist temporal classification (CTC) loss (Graves et al., 2006) is commonly used in speech-related tasks (Xu et al.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 60,
      "context" : ", 2006) is commonly used in speech-related tasks (Xu et al., 2021; Dong et al., 2021b).",
      "startOffset" : 49,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : ", 2006) is commonly used in speech-related tasks (Xu et al., 2021; Dong et al., 2021b).",
      "startOffset" : 49,
      "endOffset" : 86
    }, {
      "referenceID" : 37,
      "context" : "To visualize it, we plot the bivariate kernel density estimation (Parzen, 1962) (KDE) contour of the dim-reduced feature of them, where TSNE (Van der Maaten and Hinton, 2008) is used to reduce the dimension into two (Figure 5).",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 62,
      "context" : "However, Figure 5(a) is the KDE contour of the multi-task framework without any explicit modeling to bring two modalities together (Ye et al., 2021).",
      "startOffset" : 131,
      "endOffset" : 148
    } ],
    "year" : 0,
    "abstractText" : "How to learn similar representations for spoken utterances and their written text? We believe a unified and aligned representation of speech and text will lead to improvement in speech translation. To this end, we propose ConST, a cross-modal contrastive learning method for end-to-end speech-to-text translation. We evaluate ConST and a variety of previous baselines on multiple language directions (En-De/Fr/Ru) of a popular benchmark MuST-C. Experiments show that the proposed ConST consistently outperforms all previous methods, and achieves the state-of-the-art average BLEU of 28.5. The analysis further verifies that ConST indeed closes the representation gap of different modalities — its learned representation improves the accuracy of crossmodal text retrieval from 4% to 88%.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_5_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "FRUIT : Faithfully Reflecting Updated Information in Text",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Information changes on a constant basis. Every day, athletes are traded to new teams, and musicians and actors produce new albums and TV shows. Maintaining textual knowledge bases to keep track of these changes requires considerable community effort. For instance, a team of 120K volunteer editors make 120 edits to English Wikipedia every minute, and write 600 new articles a day.1 As the knowledge base grows, the amount of maintenance effort is compounded by the need to keep the knowledge base consistent; e.g., each edit may render information in one of the existing 6.3M+ articles obsolete.\nAssistive writing technologies have the potential to substantially reduce the burden of keeping\n1https://en.wikipedia.org/wiki/ Wikipedia:Statistics\ntext corpora up to date and consistent. However, existing work has mainly focused on correcting grammar (Wang et al., 2020), reducing repetitive typing (Chen et al., 2019), and following rhetorical directives (Sun et al., 2021), whereas the problem of producing edits grounded in external knowledge has received little attention (Kang et al., 2019). In contrast, numerous works have developed systems for distilling external knowledge into text (e.g., Wikipedia article generation) by treating the problem as multi-document summarization (Liu et al., 2018; Shi et al., 2021) or data-to-text generation (Bao et al., 2018; Parikh et al., 2020). However, these systems are not useful for updating existing texts as they can only generate text from scratch.\nTo help endow writing assistants with grounded editing capabilities, we introduce the novel generation task of faithfully reflecting updated information in text (FRUIT), where the goal is to incorporate new information into an existing piece of text. An illustration is provided in Figure 1. Given an outdated Wikipedia article and collection of new information about the article’s subject, FRUIT requires updating the existing text so that it is consistent with the new information, as well as adding text to reflect new salient facts, e.g., in Figure 1, the first sentence is updated to reflect that Tom Kristensson now drives in the Junior World Championship, and new sentences are added to reflect his achievements in 2019 and 2020.\nFRUIT presents several unique challenges. First, unlike many generation tasks, models cannot obtain good performance by solely relying on their parametric world knowledge. Whenever the provided evidence contradicts parametric knowledge, the model must prefer the evidence, which recent work has shown is difficult for pretrained language models (Krishna et al., 2021; Longpre et al., 2021). Second, the generated text needs to be faithful to both the original article and the new evidence, except when evidence invalidates information in the\nexisting article. Finally, this task requires models to jointly read and analyze evidence from both textual and tabular sources and determine which is relevant and which can be ignored, thus combining challenging aspects of both multi-document summarization and data-to-text generation.\nTo facilitate research on this task, we release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised (“silver”) update-evidence pairs. This dataset is produced by comparing pairs of English Wikipedia snapshots to identify updates to an article between two snapshots, and associating information from the other articles that supports these updates under a distant supervision assumption. As there is no guarantee that updates in the later Wikipedia snapshots can be supported by the collected evidence, we also collect a “gold” evaluation set of 914 human annotated update-evidence pairs where unsupported claims have been removed without disturbing fluency. We train and validate our models using silver data and then evaluate the final performance using gold data.\nWe establish initial benchmark results for a number of trivial and neural sequence-to-sequence baselines. We also introduce EDIT5, a T5-based model specially adapted for grounded editing, which establishes state-of-the-art performance on FRUITWIKI. Through an extensive set of analyses, we identify a number of failure modes needed to be im-\nproved upon in order to obtain better performance on FRUIT-WIKI, as well as other interesting topics for future work on this task. We additionally release our data collection pipeline to allow researchers to produce data from future Wikipedia snapshots and other languages, which we show to produce high-quality silver data. Our data and code will be available at: www.omitted.link."
    }, {
      "heading" : "2 The FRUIT Task",
      "text" : ""
    }, {
      "heading" : "2.1 Task Definition",
      "text" : "In this section we introduce the task of faithfully reflecting updated information in text (FRUIT). Given an input piece of text focused on a topic or event, along with a collection of potentially new information about the subject of the text, the goal is to update the input text to reflect the new information. A concrete illustration of the task is provided in Figure 1. The original piece of text along with its updates are shown on the left, while the new information is shown on the right.\nFormally, we assume access to pair of texts, At and At ′ , pertaining to a given subject, written at times t and t′ (respectively). In addition, we assume access to a set of new information, a.k.a., evidence, E t→t′ = { E1, . . . , E|E| } , mentioning the subject written between times t and t′. As is shown in Figure 1, the evidence can contain structured objects (e.g., excerpts from tables) as well as\nunstructured text. Given At and E t→t′ the goal is produce the updated text At ′ .\nSuccessful completion of this task requires a number of complex and inter-related reasoning capabilities. For one, models must be able to identify which evidence contradicts existing portions of the source article, and which evidence introduces new salient information about the subject in order to correctly choose whether to alter the existing text vs. add new text. For example, in Figure 1 the first sentence is updated to reflect that Tom Kristensson now races in a different competition, whereas new sentences are added describing his achievements in the years 2019 and 2020. Models must also be able to determine whether a given piece of evidence should be used at all, i.e., perform content selection. For example, in Figure 1, the number of rounds won by Kristennsen appears in the evidence but does not correspond to any piece of updated text. Although some evidence may not appear in the updated article, the converse is not true, the system should aim to generate an updated article where all the updates are faithful to the evidence."
    }, {
      "heading" : "2.2 Evaluation",
      "text" : "In this section we introduce important considerations for evaluating FRUIT systems.\nEvaluate on Updated Text There is often considerable overlap between the original and updated text. As we will see in Section 5 this poses a challenge for standard evaluation metrics like ROUGE (Lin, 2004) as systems can achieve high scores without making any updates. In this work, we propose to evaluate FRUIT systems using an alternative metric, UpdateROUGE, that only considers updated sentences instead full texts. For example, in Figure 1, the reference for UpdateROUGE only consists of the first and last two sentences.\nEvaluate Faithfulness Ensuring that generations faithfully reflect information in the evidence and updated article is crucial. However measuring faithfulness of generations is an active area of research (Çelikyilmaz et al., 2020) and adapting existing metrics to the FRUIT task is non-trivial.\nAs a simple proxy for faithfulness, we choose to measure the token overlap between named entities appearing in the generation and the target article/evidence, where entities are identified using the named entity recognizer used by Guu et al. (2020). We specifically introduce the following measurements:\n1. Unsupported Entity Tokens. This metric shows the average number of entity tokens appearing in generated updates that do not appear in the source article or evidence. This is intended to capture the overall amount of unfaithful text, focusing on entities, where higher numbers indicate less faithfulness. 2. Entity Precision and Recall. Entity precision measures the fraction of entity tokens appearing in the generated updates that appear in target entities, whereas entity recall measures the fraction of entity tokens in the target that appear in the entities in generated updates. The latter is similar to UpdateROUGE but only evaluated on entities, and thus, potentially less sensitive to paraphrasing.\nParametric Knowledge Consideration FRUIT systems should incorporate information from the provided evidence into the update, and not information that happened to be present during training or pretraining. In this work we attempt to address this by evaluating models only on updates that were made to the text after the data used to pretrain and finetune the model was collected. As this setup precludes evaluating models trained after 2020 on FRUIT-WIKI, we release our data collection pipeline so that researchers can produce evaluation datasets from future versions of Wikipedia."
    }, {
      "heading" : "3 Dataset Collection and Analysis",
      "text" : "As discussed in the introduction, keeping track of new information and then updating articles to reflect that information requires a massive amount of manual effort. Thus, in order to scalably collect sufficient data for training and evaluating FRUIT systems, some amount of automation is likely required. In this section we introduce the FRUIT-WIKI dataset and associated data collection pipeline, which allows the automatic collection of high-quality training and evaluation data for FRUIT from pairs of Wikipedia snapshots."
    }, {
      "heading" : "3.1 Pipeline",
      "text" : "Our data collection pipeline produces distantly annotated training and evaluation data from pairs of Wikipedia snapshots. We will refer to the earlier snapshot as the source snapshot, and the later snapshot as the target snapshot.\nStep 1. Collect Article Updates We compute the diff between the introductory sections of arti-\ncles appearing in both the source and target snapshot to identify all of the material that has been updated (which will serve as At and At ′ ). We also compute the diff between the non-introductory sections of articles to find new mentions of the subjects of other articles (which will serve as E t→t′). These mentions can take the form of sentences in the text, as well as new table rows and list entries. Entities are disambiguated using Wikipedia hyperlinks.\nStep 2. Filter Stylistic Updates A large number of edits to Wikipedia are stylistic (Daxenberger and Gurevych, 2012), and are therefore irrelevant to our task. In the next step of the pipeline, we attempt to filter articles that have only been superficially edited by keeping only those where at least one new added entity appears in the target snapshot.\nStep 3. Identify Supporting Evidence In the last step of our pipeline, we seek to determine which pieces of evidence in E t→t′ justify each of the updated sentences in At ′ . To do so, we make the following distant supervision assumption: an updated sentence a ∈ At′ containing an added entity s′ is substantiated by a piece of evidence E ∈ E t→t′ only if s′ is also mentioned in E. The accuracy of the annotations produced by this assumption will be measured in Section 3.3.\nOur pipeline is implemented using Apache Beam,2 to allow for distributed processing. We plan on releasing the code upon publication to enable other users to produce FRUIT data from future Wikipedia snapshots, as well as languages other than English."
    }, {
      "heading" : "3.2 FRUIT-WIKI",
      "text" : "We run our pipeline on English Wikipedia snapshots from Nov. 20, 2019 to Nov. 20, 2020 to produce the training dataset, and from Nov. 20, 2020 to June 1, 2021 to produce the evaluation\n2https://beam.apache.org/\ndataset. Detailed statistics are provided in Table 1 and analysis of the distribution of topics in the data is provided in Appendix A. On average, there are around 3 to 4 updates per article, and around 7 pieces of associated evidence. About 80% of updates require some form of content selection, i.e., ignoring some evidence, when performing updates.\nWe find that only a third of the updates are substantiated by one or more pieces of evidence according to our distant supervision assumption. Thus, the remaining updates are either: a) superficial changes to the source article, or b) additions of new unsupported claims. The latter is a particular issue as unsupported claims can cause the model to learn to hallucinate during training, and should be impossible for the model to guess during evaluation. Through the usage of human annotations and carefully selected evaluation metrics we will study the extent to which this is an issue throughout the rest of the paper."
    }, {
      "heading" : "3.3 Gold Evaluation Data",
      "text" : "To address the issue of unsupported claims during evaluation, we hired a team of 9 annotators to produce a “gold” evaluation subset of our test dataset. We collect annotations for 914 update-evidence pairs where each instance is corrected to ensure that all of the updates are supported. For the remainder of the paper we will refer to the distantly supervised test dataset annotations as “silver”.\nAnnotation Process For each instance, annotators were shown the source article, evidence, and a marked up copy of the target article. In the marked up article, each updated sentence was highlighted and prefixed with reference labels to the supporting evidence identified by our pipeline. The correction process proceeded in two steps. In the first step, annotators were asked to highlight all of the unsupported claims and incorrect reference labels in the target article. In the second step, annotators were then asked to remove the unsupported text and minimally update the article to preserve fluency. A completed annotation and the annotator interface\nare shown in Figure A8. Additional details about the annotation process are provided in Appendix C.\nAgreement We measure annotator agreement using a subset of 100 instances that were annotated by multiple annotators. Following Chen et al. (2015) and Shi et al. (2021), we quantify agreement by computing the evaluation metrics described in Section 2.2. The results are provided in Table 2. We observe high inter-annotator agreement with all scores in the 80s and 90s.\nAnalysis Statistics for the gold evaluation dataset are provided in Table 1. Overall, they closely resemble the statistics for the distantly supervised data with one exception: the fraction of substantiated updates has increased.\nTo measure the quality of our silver data, we reapply the approach used to measure inter-annotator agreement to compute agreement between the gold and silver annotations. We also measure the reference agreement, i.e., the fraction of reference labels kept by the annotators. Results are provided in Table 3. We find that agreement is high with most scores in the 80s, a strong indication that the data produced by our pipeline is high quality. In particular, the high UpdateROUGE scores provide further evidence that only a small amount of the updated text in the weakly supervised data is unsupported, while the high reference agreement indicates that our distant supervision assumption is usually accurate."
    }, {
      "heading" : "4 Methods",
      "text" : "In this section we introduce baseline methods to establish initial benchmark results on FRUIT-WIKI. We consider trivial approaches that copy task inputs, as well as T5, a neural sequence-to-sequence baseline which has shown strong performance on related tasks such as summarization (Raffel et al., 2020; Rothe et al., 2021) We additionally introduce EDIT5, a variant of T5 that produces a sequence of edits instead of the entire updated text, and employs additional tweaks to improve performance."
    }, {
      "heading" : "4.1 Copy Baselines",
      "text" : "The first set of baselines we introduce are trivial methods that merely copy the input. We consider two variants: • Copy Source: Generates a copy of the source\narticle, and • Copy Source + Evidence: Generates a copy of\nthe source article concatenated with the evidence. Our evaluation metrics only apply to unstructured text, however the evidence may contain structured tables. In order to convert these tables to text, we apply a conventional linearization scheme (Lebret et al., 2016; Wiseman et al., 2017) that separates table entries using row and column delimiters."
    }, {
      "heading" : "4.2 T5",
      "text" : "T5 (Raffel et al., 2020) is a pretrained sequence-tosequence (Sutskever et al., 2014) model based on the transformer architecture (Vaswani et al., 2017). Similar to the previous section we experiment with two variants: • T5: Only includes the source article in its input, • T5 + Evidence Inputs: Includes both the source\narticle and evidence in the input. Tabular inputs are linearized using the same approach described in the previous section. Experiments are performed using the JAX-based T5X library.3 Hyperparameters and additional training details are described in Appendix D."
    }, {
      "heading" : "4.3 EDIT5",
      "text" : "Lastly, we introduce EDIT5, which improves upon the T5-based approach described in the previous section through the usage of a compressed output format that removes the need to write the entire update from scratch and encourages content planning. The output is modified in two ways:\nFirst, as the majority of text in the target article is copied from the source, we replace any copied sentence with a single copy token identifying the sentence, e.g., if the second sentence is copied it is replaced by the token [2]. Similar to a copy mechanism (See et al., 2017), this allows the model to dedicate less capacity to repeating sequences from the input. As the resulting output resembles that produced by the diff data comparison utility, we refer to this as a diff-formatted output.\nSecond, before each update we insert a sequence of reference tokens identifying the pieces of evidence that support the update, e.g., if the first and\n3https://github.com/google-research/t5x\n(2) Tom Krister Kristensson (born 30 April 1991) is a Swedish rally driver, who drives in the Junior World Championship. [1] [2] (1) In the 2019 season of JWRC, Tom finished second behind Jan Solans. (2) The next season he went on to become the 2020 Junior World Rally champion.\nFigure 2: EDIT5 Output Format. Instead of generating the fully updated text, EDIT5 generates sequences of edited sentences, copy tokens (e.g., [2], which means copy the second sentence), and reference tokens (e.g., (1), which means the following sentence should use the first piece of evidence).\nthird piece of evidence in E t→t′ support an update then the update is prefaced by (1)(3). This approach, inspired by the use of entity chains for summarization (Narayan et al., 2021), trains the model to plan which references to use before generating an update. These reference tokens are removed from the output text of the model prior to computing the evaluation metrics.\nAn example of the EDIT5 output format is provided in Figure 2, and a comparison to the T5 output format is provided in Appendix F. Training details and hyperparameters match the setup described in Section 4.2."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "Baseline results on the gold evaluation data are provided in Table 4a, and ablation results are provided in Appendix B. In general, we find that the copy baselines perform worse than T5 and T5 performs worse than EDIT5. Notably, the copy source baseline rightfully scores zero on all metrics, while we will later find that it obtains a high ROUGE score.\nAlthough our models are trained on silver data, they still obtain good performance on the gold evaluation set. This shows the high quality of our silver data collection pipeline, and T5’s ability to generate reasonable updates based on the evidence.\nFor the T5 baselines, we find that adding evidence to the input results significant increase in all metrics, demonstrating that using the evidence is crucial to obtaining good performance.\nEDIT5 obtains additional 3-5% absolute increase in all performance metrics compared to T5, establishing EDIT5 as a strong baseline for future systems to be compared against. The reduction of unsupported entity tokens implies that EDIT5 hallucinates less frequently than T5 models. Results are provided for different model sizes to illustrate how performance scales with parameter counts.\nExample Output An example EDIT5 output is provided in Figure 3, and additional outputs in Appendix G. The examples illustrate important features of the task. In Figure 3 the goal is to update the Wikipedia article for Holli Sullivan to reflect her new role of Secretary of State of Indiana. In the reference, this information is reflected in an updated version of the first sentence as well as in a newly added last sentence. An additional sentence is added after the first sentence paraphrasing the introduction of the source article, which describes Sullivan’s previous position as a member of the Indiana House of Representatives.\nIn the EDIT5 output for this example, information is only added at the end of the article. While the model correctly states that Sullivan was appointed to be Secretary of State by Governor Eric Holcomb, as well as includes additional context\nsurrounding Sullivan’s appointment that is paraphrased from the evidence, there are some issues with the output. First, because the first sentence of the article is not updated there is conflicting information about Sullivan’s current position. Second, the added sentence hallucinates that Sullivan was appointed in January 2020 when she was actually appointed in March 2021, a fact that directly appears in the evidence.\nCategorizing Errors To better understand the types of errors made by EDIT5, we review a random sample of 100 of its predictions on the gold evaluation data and categorize them as either: grounded updates, meaning all generated claims are supported, ungrounded updates, meaning at least one unsupported claim appears in the output, or no updates, meaning the model did not predict any updates. For grounded updates we additionally keep track of how many updates include additional content not present in the ground truth update, or are missing content that appears in the ground truth update. For ungrounded updates we track whether an incorrect number/date appears in the update, the model distorted evidence, i.e., paraphrased or combined claims in the evidence in a way that changed their meaning, or hallucinated new claims.\nThe results of this analysis are presented in Table 4b. We find that EDIT5 makes no mistakes on half of the examples, however a substantial portion of these updates had some issue with content selection. Of the incorrect updates, the most common\nmistake was incorrect numbers and dates, followed by hallucinations, and finally distorted evidence. This suggests that improving numeracy could be a fruitful line of study in future work on this task.\nROUGE is Problematic We provide ROUGE scores for each of the baseline models on the gold evaluation data in Table 5. In contrast to the previous results, we find that the simple copy source baseline attains a strong score of 77.4 despite making no updates. This is better than the T5 baseline results and comparable to the EDIT5 results. This illustrates the importance of evaluating on updates rather than the whole text.\nSilver Data is Useful for Evaluation The results in Section 3.3 demonstrate high agreement between the silver and gold evaluation data which begs the question: can silver data be used in place of gold data for evaluation? To answer this, we measure the Spearman rank correlation between the gold baseline results in Table 4a and silver baseline results (provided in Table A2 of the Appendix to save space). Rank correlations for each of the\nmetrics are shown in Table 6. Overall we find high rank correlation for each of the metrics, which suggests silver evaluation performance is a reliable indicator of gold performance. Thus, models whose pretraining data overlaps FRUIT-WIKI may be evaluated and compared on data produced by running our pipeline on future Wikipedia snapshots without requiring further human evaluation.\nControllability The improvement we obtained from EDIT5 over T5 implies that more controls can be added into the model. In this section we investigate whether additional control provided by the users can improve the overall generations. We follow Keskar et al. (2019) and Narayan et al. (2021), and provide more detailed instruction by adding control codes, i.e., special tokens, to the input that instruct the model whether to add, copy, edit or remove a sentence, as well as which evidence to use when making an addition or edit. We use the target text to provide oracle labels for the control code, and see if the EDIT5 can take advantage of the codes. Example inputs and predictions are provided in Figure A7 of the Appendix.\nResults on the gold evaluation data are provided in Table 7. Including oracle control codes in the input produces a substantial 10% absolute improvement in all metrics besides unsupported tokens. This demonstrates that increased user control has the potential to produce updates that more closely resemble the desired output."
    }, {
      "heading" : "6 Related Work",
      "text" : "Early work on writing assistants largely focuses on grammar error correction; for a survey see Wang\net al. (2020). Neural models have expanded the capabilities of writing assistants to solve a wider variety of tasks including: autocompletion (Chen et al., 2019), and following rhetorical directives such as paraphrasing, elaborating, etc. (Sun et al., 2021). In this work, we seek to expand these capabilities further to producing grounded updates, which has been previously studied by Kang et al. (2019), however only for post-modifier generation.\nAs our primary focus is on writing grounded updates to Wikipedia articles, our work is closely related to existing works on Wikipedia article generation, which generally uses one of two approaches: data-to-text generation (Lebret et al., 2016; Bao et al., 2018; Parikh et al., 2020; Chen et al., 2021; Cheng et al., 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al., 2018; Shi et al., 2021). In particular, the hyperlink-based approach for associating evidence to articles is directly inspired by these works, and our annotation procedure for removing unsupported text directly draws from Parikh et al. (2020).\nDetermining which facts contradict claims in the existing article is a central topic of work on fact extraction and verification (Thorne et al., 2018). Recently, Schuster et al. (2021) introduced the VITAMIN-C dataset of factual revisions to Wikipedia articles and the task of factually consistent generation. This work differs from FRUIT in that it only focuses on sentences and does not require adding new facts or content selection."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work we introduced FRUIT, a novel text generation task where the goal is to update an article to reflect new information about its subject. To enable research on this task, we formulated a pipeline for extracting weakly supervised training and evaluation data from pairs of Wikipedia snapshots, and collected data for the years 2019- 2020 and 2020-2021, as well as human annotated gold evaluation data. We additionally provided results for several strong baselines, that demonstrate both the feasibility of this task, as well as strong correlation between gold and distantly supervised data evaluation performance that establishes the trustworthiness of future data produced using our pipeline for evaluation. Our data, pipeline code, and model checkpoints will be made available at www.omitted.link upon publication.\nEthical Considerations\nThis paper introduces a dataset and system for updating an existing piece of text to incorporate information from external evidence. Depending on the veracity of the external evidence, systems for solving this task could potentially be abused by bad actors to spread misinformation."
    }, {
      "heading" : "A Topic Distribution",
      "text" : "We categorize articles in our dataset using the Wikimedia Foundation’s topic model (Asthana and Halfaker, 2018). The distribution of topics is displayed in Figure A1. We find that the majority (approximately 50%) of updates deal with cultural topics (e.g., sports, media, personal biographies), and geographic entities (e.g., countries, states) which intuitively are likely to be affected by current events. while there are few updates to STEM- and historyrelated articles.\nCulture - Biography8.2%\nCulture - Media\n17.9%\nCulture - Other\n6.1% Culture - Sports\n14.8%\nGeography 37.4%\nHistory/Society - Other 5.0% History/Society - Govt.\n3.0% STEM7.6%\nFigure A1: Topic Distribution."
    }, {
      "heading" : "B Ablation Study",
      "text" : "We perform an ablation study to measure the impact of the modifications made to the target output of EDIT5. The results are provided in Table A1 We observe that both the diff format and including reference tokens have a positive impact on the evaluation metrics, with reference tokens having the larger impact.\nUpdateROUGE Entity Unsupp.\n1 2 L Prec. Rec. Tokens\nEDIT5 46.3 32.4 39.6 67.2 53.1 1.54 - Diff 45.5 31.7 39.1 66.8 50.8 1.66 - Ref. 45.1 31.6 38.8 66.3 50.7 1.89\nTable A1: EDIT5 Ablations."
    }, {
      "heading" : "C Additional Annotation Details",
      "text" : "Annotators attended an initial 30 minute training and were provided regular feedback from the authors during the early stages of annotation. An additional annotator was hired with the sole job of checking the other annotator’s work and correcting their mistakes. In total annotators spent\nroughly 500 hours on annotation. The annotation interface and a completed annotation are shown in Figure A8."
    }, {
      "heading" : "D Model Training Details",
      "text" : "Optimizer: AdaFactor (Shazeer and Stern, 2018), Batch Size: 128, Learning Rate: 1e-3, Dropout Rate: 0.1, Training Iterations: 30,000. Training performed on a cluster of 16 2nd generation TPUs for <3B param models, and 32 TPUS for 3B parameter models."
    }, {
      "heading" : "E Silver Baseline Results",
      "text" : "UpdateROUGE Target Entity Evid.\n1 2 L P R Acc\nT5-Large 26.8 15.9 22.3 56.3 29.8 2.33 + Evid. 39.2 27.3 34.2 66.9 42.4 1.63\nEDIT5 Small 37.8 24.9 32.6 61.4 41.2 1.53 Base 42.8 28.7 36.4 60.5 49.2 2.32 Large 42.7 29.9 37.2 66.1 47.5 1.47 3B 43.8 31.5 38.6 68.4 48.6 1.53\nTable A2: Baseline Results on Silver Evaluation Data.\nF Input and Output Formats\n(2) [0] Elizabeth Lynne Cheney (; born July 28, 1966) is an American attorney and politician serving as the U.S. Representative for since 2017. [1] Cheney is the House Republican Conference Chair, the third-highest position in GOP House leadership. [2] She is the third woman elected to that position after Deborah Pryce and Cathy McMorris Rodgers. [3] Cheney is the elder daughter of former Vice President Dick Cheney and Lynne Cheney. [4] She held several positions in the U.S. State Department during the George W. Bush administration. [5] She has been politically active on behalf of the Republican Party and is a co-founder of Keep America Safe, a nonprofit organization concerned with national security issues. [6] She was a candidate for the 2014 election to the United States Senate in Wyoming, challenging the three-term incumbent Mike Enzi, before withdrawing from the race. [7] In the House of Representatives, she holds the seat that was held by her father from 1979 to 1989. [8] She is known for her hawkish foreign policy views. [CONTEXT] (0) Andy Biggs U.S. House of Representatives - Tenure - 2021 storming of the United States Capitol On January 12, 2021, Biggs called on fellow GOP Representative Liz Cheney (R-WY) to resign from her leadership position within the Republican Caucus, after she voted in favor of Donald Trump’s second impeachment. (1) 116th United States Congress Leadership - House of Representatives - Minority (Republican) leadership * House Minority Leader and Chair of the House Republican Steering Committee: Kevin McCarthy * House Minority Whip: Steve Scalise * Chair of the House Republican Conference: Liz Cheney * Vice Chair of the House Republican Conference: Mark Walker * Secretary of the House Republican Conference: Jason Smith * Chair of the House Republican Policy Committee: Gary Palmer * Chair of the National Republican Congressional Committee: Tom Emmer * House Republican Chief Deputy Whip: Drew Ferguson (2) A Call for American Renewal INTRODUCTION The manifesto was released one day after the ousting of Representative Liz Cheney as chair of the House Republican Conference, and was largely seen as a reaction against the influence of Trumpism within the Republican Party. (3) List of nicknames used by Donald Trump Domestic political figures - Table-0-11 [HEADER] [COL] Nickname [COL] Personal name [COL] Notes [ROW] id=\"The Warmonger\" [COL] The Warmonger [COL] Liz Cheney [COL] U.S. representative for Wyoming’s at-large congressional district; Chair of the House Republican Conference (4) Conscience vote Practice in various countries - United States Similarly, when House Republican leadership decided not to whip votes against the second impeachment of Donald Trump, Liz Cheney--the third-highest-ranking Republican--referred to the matter as a \"vote of conscience\".\nFigure A2: Input Format.\n(2) Elizabeth Lynne Cheney ( ; born July 28, 1966) is an American attorney and politician who has served as the U.S. Representative for since 2017. She was the Chair of the House Republican Conference, the third-highest position in the House Republican leadership. She is the third woman elected to that position after Deborah Pryce and Cathy McMorris Rodgers. She held several positions in the U.S. State Department during the George W. Bush administration, notably as Deputy Assistant Secretary of State for Near Eastern Affairs and Coordinator for Broader Middle East and North Africa Initiatives. She promoted regime change in Iran while chairing the Iran Syria Policy and Operations Group with Elliott Abrams. In 2009 Cheney and Bill Kristol founded Keep America Safe, a nonprofit organization concerned with national security issues that advocated the positions of the former Bush administration. She was a candidate for the 2014 election to the U.S. Senate in Wyoming, challenging three-term incumbent Mike Enzi, before withdrawing from the race. In the House of Representatives, she holds the seat her father held for a decade, representing Wyoming from 1979 to 1989. Cheney is a neoconservative. She later supported the second impeachment of Donald Trump for his role in the 2021 storming of the U.S. Capitol.\nFigure A3: T5 Output Format.\n(2) [0] [1] [2] [3] [4] [5] [6] In the House of Representatives, she holds the seat that was held by her father from 1979 to 1989. (6) She is known for her neoconservative foreign policy views, and her affiliation with the Trump campaign. (0) (1) (2) (3) (4) Cheney is under fire for her role in the second impeachment of Donald Trump in January 2021.\nFigure A4: EDIT5 Output Format."
    }, {
      "heading" : "G More Qualitative Examples",
      "text" : "Mike McMeeken (born 10 May 1994) is an English rugby league footballer who plays as a forward for the Castleford Tigers in the Super League. McMeeken has also represented England at international level, playing in two games at the 2017 World Cup. He started his career in the Super League with the London Broncos, also playing on loan in League 1 at the London Skolars before joining the Tigers.\nOriginal Article\nNew Information\nGround Truth Mike McMeeken (born 10 May 1994) is an English rugby league footballer who plays as a forward for the Catalans Dragons in the Super League…Copied text...He joined Catalans Dragons in December 2020, ahead of the 2021 season.\nEDIT5 Mike McMeeken (born 10 May 1994) is an English rugby league footballer who plays as a forward for the Catalans Dragons in the Super League…Copied text…\nCastleford Tigers 2021 Transfers - Losses\nPlayer Club Contract Date\nMike McMeeken Catalans Dragons\n2 Year December 2020\nCatalans Dragons 2021 Transfers - Gains\nPlayer Club Contract Date\nMike McMeeken Castleford Tigers 3 Year June 2020\nFigure A5: Example 1.\nIsidore Mankofsky (born September 22, 1931, in New York City, New York) is an American cinematographer. He shot more than 200 educational movies for Encyclopaedia Britannica.\nOriginal Article\nNew Information\nGround Truth Isidore Mankofsky (September 22, 1931 – March 11, 2021) was an American cinematographer, best known for his work on films such as ''The Muppet Movie'' (1979) and ''The Jazz Singer'' (1980)...Copied text…He died at his home in Los Angeles, California in March 2021 at the age of 89.\nEDIT5 Isidore Mankofsky (September 22, 1931 – March 11, 2021) was an American cinematographer…Copied text…, and worked on ''The Muppet Movie'', ''Somewhere in Time'', and ''The Jazz Singer''.\nThe Parent Trap (franchise) Additional crew and production details 2021 Deaths in the United States Isidore Mankofsky, cinematographer (''The Muppet Movie'', ''Somewhere in Time'', ''The Jazz Singer'') Deaths in March 2021 11 - Isidore Mankofsky, 89, American cinematographer (''The Muppet Movie'', ''Somewhere in Time'', ''The Jazz Singer'') Film Crew / Detail Parent Trap III Joel McNeely, Isidore Mankofsky, Howard Kunin & Duane Hartzell\n(4) The_Parent_Trap_(franchise) Additional crew and production details - T ble-0-3 [HEADER] [COL] Film [COL] Crew/ etail [ROW] [COL] ''Parent Trap III'' [COL] Joel McNeely [COL] Isidore Mankofsky [COL] Howard Kunin & Duane Har zell [COL] Buena Vista Television, Disney-ABC Domestic Television, National Broadcasting Company [COL] 85 minutes\nFigure A6: Example 2.\n[0] ''Shuggie Bain'' is the debut novel by Scottish-American writer Douglas Stuart, published in 2020. [EDIT] [1] It tells the story of the youngest of the three children, Shuggie, growing up with his alcoholic mother, Agnes in the 1980s, in Thatcher-era Glasgow, Scotland. [EDIT] [2] (0) The novel won the 2020 Booker Prize. [EDIT] [3] (1) It was also a finalist for the 2020 National Book Award for Fiction.\nOriginal Article\nNew Information\nGround Truth …Copied text… It tells the story of the youngest of the three children, Shuggie, growing up with his alcoholic mother, Agnes, in the 1980s, in a working-class Glasgow, Scotland. The novel was awarded the 2020 Booker Prize, making Stuart the second Scottish winner of the prize in its history, following James Kelman. ''Shuggie Bain'' was also a finalist for the 2020 National Book Award for Fiction and a finalist for the 2020 John Leonard Prize for Best First Book from the National Book Critics Circle. EDIT5 - Controllable\nCopied text... It tells the story of the youngest of three children, Shuggie, growing up with his alcoholic mother, Agnes, in [DELETED] thatcher-era Glasgow, Scotland. The novel won the 2020 Booker Prize, and was a finalist for the 2020 National Book Award for Fiction and the 2021 John Leonard Prize. It was also a finalist for the 2020 National Book Critics Circle Award.\nJames Kelman Critical reception In his essay \"The Importance of Glasgow in My Work\", he compares the presentation of working-class and Scottish characters with those of the traditional \"upper-class\" English protagonist: In 2020, Douglas Stuart on becoming the second Scottish writer to be awarded the Booker Prize, for his novel ''Shuggie Bain'', said that his life was changed by Kelman's win with ''How Late It Was, How Late'': \"It is such a bold book, the prose and stream of consciousness is really inventive. National Book Critics Circle Award Finalists 2020 - John Leonard Prize Kerri Arsenault, ''Mill Town: Reckoning with What Remains'' (St. Martin’s), Karla Cornejo Villavicencio, ''The Undocumented Americans'' (One World), Raven Leilani, ''Luster'' (Farrar, Straus and Giroux), Megha Majumdar, ''A Burning'' (Knopf), Douglas Stuart, ''Shuggie Bain'' (Grove), Brandon Taylor, ''Real Life'' (Riverhead), C Pam Zhang, ''How Much of These Hills Is Gold'' (Riverhead)\nFigure A7: Using Control Codes.\n24 43\n85 56 In st\nru ct\nio ns\nFl ag\ngi ng\nP ro\nbl em\nat ic\nIn st\nan ce s O ve rv ie w Th e go al o f t hi s ta sk is to c ol le ct e va lu at io n da ta fo r a s ys te m th at c an a ut om at ic al ly u pd at e W ik ip ed ia a rti cl es fr om n ew in fo rm at io n ab ou t t he a rti cl e' s su bj ec t. Th e se ct io ns b el ow p ro vi de th e te xt o f t he o rig in al p as sa ge to b e up da te d, a c ol le ct io n of a dd ed in fo rm at io n ab ou t t he a rti cl e su bj ec t, an d th e te xt o f t he u pd at ed p as sa ge . W ha t w e ne ed fr om y ou Th e is su e w e ar e fa ce d w ith is th at s om e of th e ud pa te d te xt m ay n ot b e su pp or te d by th e ad de d in fo rm at io n se ct io n. W e ne ed y ou to id en tif y al l o f t he u ns up po rte d in fo rm at io n, a nd e di t t he a rti cl e to re m ov e un su pp or te d te xt w hi le p re se rv in g flu en cy . P le as e m ak e su re th at y ou r e di ts o nl y re m ov e in fo rm at io n; w hi le y ou m ay n ee d to w rit e so m e te xt to e ns ur e th at th e ed ite d pa ss ag e is fl ue nt , n o ne w fa ct s sh ou ld b e ad de d (e ve n if th ey a re s up po rte d) . Th e or ig in al p as sa ge a nd a dd ed in fo rm at io n ar e be lo w th is b ox . W e re qu es t t ha t y ou fi rs t r ea d th e up da te d pa ss ag e in th e gr ee n bo x, a nd h ig hl ig ht a ny u ns up po rte d te xt . T he n co py th e co nt en ts fr om th e gr ee n bo x to th e or an ge b ox a nd e di t t he m s o th at a ll of th e te xt is s up po rte d. If yo u ha ve q ue st io ns p le as e do n ot h es ita te to e m ai l: R E D A C TE D If yo u fe el th at th er e ar e fu nd am en\nta l\nis su\nes (e\n.g .,\nm or\ne th\nan 5\n0% o\nf s en\nte nc\nes\nar e\nun su\npp or\nte d)\nth at\nm ak\ne th\ne up\nda te d ar tic le e ith er in co m pl et e or in flu en t p le as e us e th e be lo w c he ck bo x to fl ag th e in st an ce fo r r ev ie w . W e st ill re qu es t y ou fi ni sh e di tin g ev en if yo u fla g th e in st an ce fo r r ev ie w .\nO rig\nin al\nP as\nsa ge\n- D\nO N\nO T\nC H\nA N\nG E\nA dd\ned In\nfo rm\nat io\nn - D\nO N\nO T\nC H\nA N\nG E\nFl ag\nfo r R\nev ie w [ 0 ] J o s h u a C h r i s t i a n K o j o K i n g ( b o r n 1 5 J a n u a r y 1 9 9 2 ) i s a N o r w e g i a n p r o f e s s i o n a l f o o t b a l l e r w h o p l a y s a s a f o r w a r d f o r C h a m p i o n s h i p c l u b B o u r n e m o u t h a n d t h e N o r w a y n a t i o n a l t e a m . [ 1 ] K i n g w a s s i g n e d b y M a n c h e s t e r U n i t e d f r o m V å l e r e n g a i n 2 0 0 8 . [ 2 ] A f t e r l o a n s p e l l s w i t h P r e s t o n N o r t h E n d , B o r u s s i a M ö n c h e n g l a d b a c h , H u l l C i t y a n d B l a c k b u r n R o v e r s , h e s i g n e d p e r m a n e n t l y w i t h B l a c k b u r n i n J a n u a r y 2 0 1 3 , b e f o r e s w i t c h i n g t o B o u r n e m o u t h i n M a y 2 0 1 5 . [ 3 ] A f t e r r e p r e s e n t i n g N o r w a y a t u n d e r - 1 5 , u n d e r - 1 6 , u n d e r - 1 8 , u n d e r - 1 9 a n d u n d e r - 2 1 l e v e l s , K i n g m a d e h i s s e n i o r i n t e r n a t i o n a l d e b u t a g a i n s t I c e l a n d i n 2 0 1 2 , a n d s c o r e d h i s f i r s t i n t e r n a t i o n a l g o a l a g a i n s t C y p r u s l a t e r t h a t y e a r . ( 0 ) 2 0 2 0 – 2 1 _ A F C _ B o u r n e m o u t h _ s e a s o n T r a n s f e r s - T r a n s f e r s o u t - T a b l e - 0 - 2 9 | D a t e | P o s i t i o n | N a t i o n a l i t y | N a m e | T o | F e e | R e f . | | - - - - - - - - - - - - - - - - - | - - - - - - - - - - | - - - - - - - - - - - - - | - - - - - - - - - - - - - | - - - - - - - - - | - - - - - - - - - - - - - | - - - - - - | | 2 F e b r u a r y 2 0 2 1 | S S | | J o s h u a K i n g | E v e r t o n | N o m i n a l f e e | | ( 1 ) 2 0 2 0 – 2 1 _ E v e r t o n _ F . C . _ s e a s o n T r a n s f e r s - T r a n s f e r s i n - T a b l e - 0 - 6 | D a t e | P o s i t i o n | N a t i o n a l i t y | N a m e | F r o m | F e e | T e a m | R e f . | | - - - - - - - - - - - - - - - - - | - - - - - - - - - - | - - - - - - - - - - - - - | - - - - - - - - - - - - - | - - - - - - - - - - - - - | - - - - - - - - - | - - - - - - - - - - - - | - - - - - - | | 1 F e b r u a r y 2 0 2 1 | F W | | J o s h u a K i n g | B o u r n e m o u t h | N o m i n a l | F i r s t t e a m | | ( 2 ) G u l l b a l l e n W i n n e r s - 2 0 1 4 – 2 0 1 7 - T a b l e - 0 - 3 | Y e a r | W i n n e r | C l u b ( s ) | | - - - - - - | - - - - - - - - - - - - - | - - - - - - - - - - - - - | | 2 0 1 7 | J o s h u a K i n g | B o u r n e m o u t h | ( 3 ) 2 0 2 0 – 2 1 _ C r a w l e y _ T o w n _ F . C . _ s e a s o n R e v i e w - J a n u a r y N i c h o l s e q u a l i s e d f r o m c l o s e r a n g e i n t h e 5 9 t h m i n u t e b e f o r e J o s h K i n g s c o r e d B o u r n e m o u t h ' s w i n n e r . ( 4 ) 2 0 2 0 – 2 1 _ M a n c h e s t e r _ U n i t e d _ F . C . _ s e a s o n P r e m i e r L e a g u e M c T o m i n a y r e s t o r e d t h e l e a d o n l y f o r D o m i n i c C a l v e r t - L e w i n t o e q u a l i s e a g a i n i n t h e f i n a l m i n u t e o f s t o p p a g e t i m e f o l l o w i n g T u a n z e b e ' s f o u l o n E v e r t o n s u b s t i t u t e a n d f e l l o w U n i t e d A c a d e m y g r a d u a t e J o s h u a K i n g . ( 5 ) O s l o N o t a b l e r e s i d e n t s - S p o r t [ S o n j a H e n i e ( 1 9 1 2 – 1 9 6 9 ) t h r e e - t i m e O l y m p i c c h a m p i o n f i g u r e s k a t e r a n d a c t r e s s , K n u t J o h a n n e s e n ( b o r n 1 9 3 3 ) t w i c e O l y m p i c C h a m p i o n s p e e d s k a t e r , G r e t e W a i t z ( 1 9 5 3 – 2 0 1 1 ) m a r a t h o n r u n n e r , s i l v e r m e d a l l i s t a t t h e 1 9 8 4 O l y m p i c G a m e s , J ø r n G o l d s t e i n ( b o r n 1 9 5 3 ) O l y m p i c i c e h o c k e y g o a l i e , E s p e n B r e d e s e n ( b o r n 1 9 6 8 ) s k i j u m p e r , g o l d a n d s i l v e r m e d a l s a t t h e 1 9 9 4 W i n t e r O l y m p i c s , K j e t i l A n d r é A a m o d t ( b o r n 1 9 7 1 ) a l p i n e s k i e r w i t h e i g h t O l y m p i c m e d a l s , E s p e n K n u t s e n ( b o r n 1 9 7 2 ) , f o r m e r p r o f e s s i o n a l i c e h o c k e y p l a y e r , S u z a n n P e t t e r s e n ( b o r n 1 9 8 1 ) a r e t i r e d p r o f e s s i o n a l g o l f e r , p l a y e d o n t h e L P G A T o u r , M a t s Z u c c a r e l l o ( b o r n 1 9 8 7 ) p r o f e s s i o n a l i c e h o c k e y p l a y e r i n t h e N a t i o n a l H o c k e y L e a g u e , J o s h u a K i n g ( b o r n 1 9 9 2 ) f o o t b a l l e r , 1 7 2 c a p s f o r A F C B o u r n e m o u t h a n d 5 1 f o r N o r w a y ] ( 6 ) 2 0 2 0 – 2 1 _ F A _ C u p T o p s c o r e r s - T a b l e - 0 - 6 | R a n k | P l a y e r | C l u b | G o a l s | | - - - - - - - - - - - - - | - - - - - - - - - - - - - | - - - - - - | - - - - - - - | | J o s h u a K i n g | B o u r n e m o u t h | | | U pd at ed P as sa ge - H IG H LI G H T U N SU PP O R TE D T EX T Ed ite d Pa ss ag e - C O PY F R O M T H E C EL L O N T H E LE FT A N D E D IT ( 0 ) ( 1 ) J o s h u a C h r i s t i a n K o j o K i n g ( b o r n 1 5 J a n u a r y 1 9 9 2 ) i s a N o r w e g i a n p r o f e s s i o n a l f o o t b a l l e r w h o p l a y s a s a f o r w a r d f o r P r e m i e r L e a g u e c l u b E v e r t o n a n d t h e N o r w a y n a t i o n a l t e a m . [ 1 ] K i n g w a s s i g n e d b y M a n c h e s t e r U n i t e d f r o m V å l e r e n g a i n 2 0 0 8 . [ 2 ] A f t e r l o a n s p e l l s w i t h P r e s t o n N o r t h E n d , B o r u s s i a M ö n c h e n g l a d b a c h , H u l l C i t y a n d B l a c k b u r n R o v e r s , h e s i g n e d p e r m a n e n t l y w i t h B l a c k b u r n i n J a n u a r y 2 0 1 3 , b e f o r e s w i t c h i n g t o B o u r n e m o u t h i n M a y 2 0 1 5 . ( 0 ) ( 1 ) I n F e b r u a r y 2 0 2 1 , i n a d e a d l i n e d a y d e a l , h e r e t u r n e d t o t h e t o p f l i g h t w i t h a m o v e t o E v e r t o n . [ 3 ] A f t e r r e p r e s e n t i n g N o r w a y a t u n d e r - 1 5 , u n d e r - 1 6 , u n d e r - 1 8 , u n d e r - 1 9 a n d u n d e r - 2 1 l e v e l s , K i n g m a d e h i s s e n i o r i n t e r n a t i o n a l d e b u t a g a i n s t I c e l a n d i n 2 0 1 2 , a n d s c o r e d h i s f i r s t i n t e r n a t i o n a l g o a l a g a i n s t C y p r u s l a t e r t h a t y e a r . ( 0 ) ( 1 ) J o s h u a C h r i s t i a n K o j o K i n g ( b o r n 1 5 J a n u a r y 1 9 9 2 ) i s a N o r w e g i a n p r o f e s s i o n a l f o o t b a l l e r w h o p l a y s a s a f o r w a r d f o r P r e m i e r L e a g u e c l u b E v e r t o n a n d t h e N o r w a y n a t i o n a l t e a m . [ 1 ] K i n g w a s s i g n e d b y M a n c h e s t e r U n i t e d f r o m V å l e r e n g a i n 2 0 0 8 . [ 2 ] A f t e r l o a n s p e l l s w i t h P r e s t o n N o r t h E n d , B o r u s s i a M ö n c h e n g l a d b a c h , H u l l C i t y a n d B l a c k b u r n R o v e r s , h e s i g n e d p e r m a n e n t l y w i t h B l a c k b u r n i n J a n u a r y 2 0 1 3 , b e f o r e s w i t c h i n g t o B o u r n e m o u t h i n M a y 2 0 1 5 . ( 0 ) ( 1 ) I n F e b r u a r y 2 0 2 1 , h e r e t u r n e d t o E v e r t o n . [ 3 ] A f t e r r e p r e s e n t i n g N o r w a y a t u n d e r - 1 5 , u n d e r - 1 6 , u n d e r - 1 8 , u n d e r - 1 9 a n d u n d e r - 2 1 l e v e l s , K i n g m a d e h i s s e n i o r i n t e r n a t i o n a l d e b u t a g a i n s t I c e l a n d i n 2 0 1 2 , a n d s c o r e d h i s f i r s t i n t e r n a t i o n a l g o a l a g a i n s t C y p r u s l a t e r t h a t y e a r .\nSt ep\n1 St\nep 2\nTh e\nse ct\nio n\nbe lo\nw a\nbo ve\nth e\nte xt\no f t\nhe u\npd at\ned p\nas sa\nge . U\nnc ha\nng ed\ns en\nte nc\nes fr\nom th\ne or\nig in\nal p\nas sa\nge a\nre in\ng re\ny, w\nhi le\nad\nde d\nor u\npd at\ned s\nen te\nnc es\na re\nin b\nla ck\n.\nW e\nha ve\ntr ie\nd to\na ut\nom at\nic al\nly d\net ec\nt w hi\nch p\nie ce\ns of\na dd\ned in\nfo rm\nat io\nn ju\nst ify\nth e\nch an\nge d\nte xt\n. I f a\nju st\nifi ca\ntio n\nis d\net ec\nte d,\nth en\nth\ne ed\nite d\nse nt\nen ce\nw ill\nb e\npr ef\nac ed\nw ith\nth e\nde lim\nite r o\nf t he\na dd\ned in\nfo rm\nat io n. Fo r e xa m pl e: ( 0) (1 ) U pd at ed s en te nc e m ea ns th at w e th in k th at a dd ed in fo rm at io n 0 an d 1 ju st ify (a t l ea st s om e of th\ne ed\nit) .\nW ha\nt t o\ndo fo\nr t hi\ns co\nlu m n - H ig hl ig ht a ny te xt /e vi de nc\ne de\nlim ite\nrs th\nat a\nre u\nns up\npo rte\nd by\nth e\nor ig\nin al\np as\nsa ge\no r a\ndd ed\nin fo\nrm at\nio n,\nu si\nng th\nis re\nd co\nlo r (\nin\nth e\ncu st\nom s\nec tio\nn) .\n- D o\nno t e\ndi t t\nhe te\nxt .\nW ha\nt t o\ndo fo\nr t hi\ns co\nlu m n - C op y th e hi gh lig ht ed u pd at\ned p\nas sa\nge fr\nom th\ne pr\nev io\nus s\nte p.\n- E di\nt t ex\nt s o\nth at\n: a ) a\nny u\nns up\npo rte\nd te\nxt is\nre m\nov ed\n, a nd\nb ) t\nhe p\nas sa\nge is\ns til\nl f lu\nen t.\n- D o\nno t a\ndd a\nny n\new in\nfo rm\nat io\nn\nFi gu\nre A\n8: A\nnn ot\nat or\nIn te\nrf ac\ne"
    } ],
    "references" : [ {
      "title" : "2021) We additionally introduce EDIT5, a variant of T5 that produces a sequence of edits instead of the entire updated text, and employs additional tweaks to improve performance",
      "author" : [ "Rothe" ],
      "venue" : null,
      "citeRegEx" : "Rothe,? \\Q2021\\E",
      "shortCiteRegEx" : "Rothe",
      "year" : 2021
    }, {
      "title" : "2021), and provide more detailed instruction by adding control codes, i.e., special tokens, to the input that instruct the model whether to add",
      "author" : [ "Keskar" ],
      "venue" : null,
      "citeRegEx" : "Keskar,? \\Q2021\\E",
      "shortCiteRegEx" : "Keskar",
      "year" : 2021
    }, {
      "title" : "2020), or multi-document summarization (Banerjee and Mitra, 2016",
      "author" : [ "2020 Parikh et al", "2021 Chen et al", "Cheng" ],
      "venue" : "Shi et al.,",
      "citeRegEx" : "al. et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2021
    }, {
      "title" : "With few eyes, all hoaxes are deep",
      "author" : [ "Sumit Asthana", "Aaron Halfaker." ],
      "venue" : "Proc. ACM Hum.-Comput. Interact., 2(CSCW).",
      "citeRegEx" : "Asthana and Halfaker.,? 2018",
      "shortCiteRegEx" : "Asthana and Halfaker.",
      "year" : 2018
    }, {
      "title" : "Wikiwrite: Generating wikipedia articles automatically",
      "author" : [ "Siddhartha Banerjee", "Prasenjit Mitra." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Banerjee and Mitra.,? 2016",
      "shortCiteRegEx" : "Banerjee and Mitra.",
      "year" : 2016
    }, {
      "title" : "Table-totext: Describing table region with natural language",
      "author" : [ "Junwei Bao", "Duyu Tang", "Nan Duan", "Zhao Yan", "Yuanhua Lv", "M. Zhou", "Tiejun Zhao." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Bao et al\\.,? 2018",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2018
    }, {
      "title" : "Gmail smart compose: Real-time assisted writing",
      "author" : [ "Mia Xu Chen", "Benjamin N. Lee", "Gagan Bansal", "Yuan Cao", "Shuyuan Zhang", "Justin Lu", "Jackie Tsay", "Yinan Wang", "Andrew M. Dai", "Zhifeng Chen", "Timothy Sohn", "Yonghui Wu." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "WikiTableT: A large-scale data-to-text dataset for generating Wikipedia article sections",
      "author" : [ "Mingda Chen", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 193–209, Online. Association",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "ArXiv, abs/1504.00325.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "ENTDESC: Entity description generation by exploring knowledge graph",
      "author" : [ "Liying Cheng", "Dekun Wu", "Lidong Bing", "Yan Zhang", "Zhanming Jie", "Wei Lu", "Luo Si." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "A corpus-based study of edit categories in featured and non-featured Wikipedia articles",
      "author" : [ "Johannes Daxenberger", "Iryna Gurevych." ],
      "venue" : "Proceedings of COLING 2012, pages 711–726, Mumbai, India. The COLING 2012 Organizing Committee.",
      "citeRegEx" : "Daxenberger and Gurevych.,? 2012",
      "shortCiteRegEx" : "Daxenberger and Gurevych.",
      "year" : 2012
    }, {
      "title" : "Realm: Retrievalaugmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang" ],
      "venue" : null,
      "citeRegEx" : "Guu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "PoMo: Generating entity-specific post-modifiers in context",
      "author" : [ "Jun Seok Kang", "Robert L Logan IV", "Zewei Chu", "Yang Chen", "Dheeru Dua", "Kevin Gimpel", "Sameer Singh", "Niranjan Balasubramanian." ],
      "venue" : "Proceedings of the 2019 Conference of the North",
      "citeRegEx" : "Kang et al\\.,? 2019",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2019
    }, {
      "title" : "Ctrl: A conditional transformer language model for controllable generation",
      "author" : [ "Nitish Shirish Keskar", "Bryan McCann", "Lav R. Varshney", "Caiming Xiong", "Richard Socher." ],
      "venue" : "ArXiv, abs/1909.05858.",
      "citeRegEx" : "Keskar et al\\.,? 2019",
      "shortCiteRegEx" : "Keskar et al\\.",
      "year" : 2019
    }, {
      "title" : "Hurdles to progress in long-form question answering",
      "author" : [ "Kalpesh Krishna", "Aurko Roy", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Krishna et al\\.,? 2021",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural text generation from structured data with application to the biography domain",
      "author" : [ "Rémi Lebret", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203–1213,",
      "citeRegEx" : "Lebret et al\\.,? 2016",
      "shortCiteRegEx" : "Lebret et al\\.",
      "year" : 2016
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "ACL 2004.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Generating wikipedia by summarizing long sequences",
      "author" : [ "Peter J. Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam M. Shazeer." ],
      "venue" : "ArXiv, abs/1801.10198.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Entity-based knowledge conflicts in question answering",
      "author" : [ "Shayne Longpre", "Kartik Perisetla", "Anthony Chen", "Nikhil Ramesh", "Chris DuBois", "Sameer Singh." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Longpre et al\\.,? 2021",
      "shortCiteRegEx" : "Longpre et al\\.",
      "year" : 2021
    }, {
      "title" : "Planning with entity chains for abstractive summarization",
      "author" : [ "Shashi Narayan", "Yao Zhao", "Joshua Maynez", "Gonccalo Simoes", "Ryan T. McDonald." ],
      "venue" : "ArXiv, abs/2104.07606.",
      "citeRegEx" : "Narayan et al\\.,? 2021",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2021
    }, {
      "title" : "ToTTo: A controlled table-totext generation dataset",
      "author" : [ "Ankur Parikh", "Xuezhi Wang", "Sebastian Gehrmann", "Manaal Faruqui", "Bhuwan Dhingra", "Diyi Yang", "Dipanjan Das." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Parikh et al\\.,? 2020",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research, 21(140):1–67.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "A thorough evaluation of task-specific pretraining for summarization",
      "author" : [ "Sascha Rothe", "Joshua Maynez", "Shashi Narayan." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 140–145, Online and",
      "citeRegEx" : "Rothe et al\\.,? 2021",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2021
    }, {
      "title" : "Get your vitamin C! robust fact verification with contrastive evidence",
      "author" : [ "Tal Schuster", "Adam Fisch", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Schuster et al\\.,? 2021",
      "shortCiteRegEx" : "Schuster et al\\.",
      "year" : 2021
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Adafactor: Adaptive learning rates with sublinear memory cost",
      "author" : [ "Noam Shazeer", "Mitchell Stern." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596–4604.",
      "citeRegEx" : "Shazeer and Stern.,? 2018",
      "shortCiteRegEx" : "Shazeer and Stern.",
      "year" : 2018
    }, {
      "title" : "DESCGEN: A distantly supervised datasetfor generating entity descriptions",
      "author" : [ "Weijia Shi", "Mandar Joshi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
      "citeRegEx" : "Shi et al\\.,? 2021",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2021
    }, {
      "title" : "IGA: An intent-guided authoring assistant",
      "author" : [ "Simeng Sun", "Wenlong Zhao", "Varun Manjunatha", "Rajiv Jain", "Vlad Morariu", "Franck Dernoncourt", "Balaji Vasan Srinivasan", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods",
      "citeRegEx" : "Sun et al\\.,? 2021",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2021
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and VERification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam M. Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A comprehensive survey of grammar error correction",
      "author" : [ "Yu Wang", "Yuelin Wang", "Jie Liu", "Zhuo Liu." ],
      "venue" : "ArXiv, abs/2005.06600.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Challenges in data-to-document generation",
      "author" : [ "Sam Wiseman", "Stuart Shieber", "Alexander Rush." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Wiseman et al\\.,? 2017",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluation of text generation: A survey",
      "author" : [ "Asli Çelikyilmaz", "Elizabeth Clark", "Jianfeng Gao." ],
      "venue" : "ArXiv, abs/2006.14799.",
      "citeRegEx" : "Çelikyilmaz et al\\.,? 2020",
      "shortCiteRegEx" : "Çelikyilmaz et al\\.",
      "year" : 2020
    }, {
      "title" : "Batch Size: 128, Learning Rate: 1e-3, Dropout Rate: 0.1, Training Iterations: 30,000. Training performed on a cluster of 16 2nd generation TPUs for <3B param models, and 32 TPUS",
      "author" : [ "Optimizer: AdaFactor (Shazeer", "Stern" ],
      "venue" : null,
      "citeRegEx" : ".Shazeer and Stern,? \\Q2018\\E",
      "shortCiteRegEx" : ".Shazeer and Stern",
      "year" : 2018
    }, {
      "title" : "American attorney and politician who has served as the U.S. Representative for since 2017. She was the Chair of the House Republican Conference, the third-highest position in the House Republican leadership. She is the third woman elected to that position",
      "author" : [ "Elizabeth Lynne Cheney" ],
      "venue" : null,
      "citeRegEx" : "Cheney,? \\Q2017\\E",
      "shortCiteRegEx" : "Cheney",
      "year" : 2017
    }, {
      "title" : "Qualitative Examples Mike McMeeken (born 10 May 1994) is an English rugby league footballer who plays as a forward for the Castleford Tigers in the Super League. McMeeken has also represented England at international level, playing in two games at the 2017 World Cup. He started his career in the Super League with the London Broncos, also playing on loan in League 1 at the London Skolars before joining the Tigers",
      "author" : [ "G More" ],
      "venue" : null,
      "citeRegEx" : "More,? \\Q2017\\E",
      "shortCiteRegEx" : "More",
      "year" : 2017
    }, {
      "title" : "born 10 May 1994) is an English rugby league footballer who plays as a forward for the Catalans Dragons in the Super League...Copied text...He joined Catalans Dragons in December 2020, ahead of the 2021 season",
      "author" : [ "Mike McMeeken" ],
      "venue" : null,
      "citeRegEx" : "McMeeken,? \\Q2021\\E",
      "shortCiteRegEx" : "McMeeken",
      "year" : 2021
    }, {
      "title" : "born 10 May 1994) is an English rugby league footballer who plays as a forward for the Catalans Dragons in the Super League...Copied text.",
      "author" : [ "Mike McMeeken" ],
      "venue" : null,
      "citeRegEx" : "McMeeken,? \\Q1994\\E",
      "shortCiteRegEx" : "McMeeken",
      "year" : 1994
    }, {
      "title" : "The novel won the 2020 Booker Prize, and was a finalist for the 2020 National Book Award for Fiction and the 2021 John Leonard Prize",
      "author" : [ "thatcher-era Glasgow", "Scotland" ],
      "venue" : "It was also a finalist for the 2020 National Book Critics Circle Award",
      "citeRegEx" : "Glasgow and Scotland.,? \\Q2020\\E",
      "shortCiteRegEx" : "Glasgow and Scotland.",
      "year" : 2020
    }, {
      "title" : "Critical reception In his essay \"The Importance of Glasgow in My Work\", he compares the presentation of working-class and Scottish characters with those of the traditional \"upper-class\" English protagonist",
      "author" : [ "James Kelman" ],
      "venue" : null,
      "citeRegEx" : "Kelman,? \\Q2020\\E",
      "shortCiteRegEx" : "Kelman",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "However, existing work has mainly focused on correcting grammar (Wang et al., 2020), reducing repetitive typing (Chen et al.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : ", 2020), reducing repetitive typing (Chen et al., 2019), and following rhetorical directives (Sun et al.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : ", 2019), and following rhetorical directives (Sun et al., 2021), whereas the problem of producing edits grounded in external knowledge has received little attention (Kang et al.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : ", 2021), whereas the problem of producing edits grounded in external knowledge has received little attention (Kang et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : ", Wikipedia article generation) by treating the problem as multi-document summarization (Liu et al., 2018; Shi et al., 2021) or data-to-text generation (Bao et al.",
      "startOffset" : 88,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : ", Wikipedia article generation) by treating the problem as multi-document summarization (Liu et al., 2018; Shi et al., 2021) or data-to-text generation (Bao et al.",
      "startOffset" : 88,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : ", 2021) or data-to-text generation (Bao et al., 2018; Parikh et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : ", 2021) or data-to-text generation (Bao et al., 2018; Parikh et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Whenever the provided evidence contradicts parametric knowledge, the model must prefer the evidence, which recent work has shown is difficult for pretrained language models (Krishna et al., 2021; Longpre et al., 2021).",
      "startOffset" : 173,
      "endOffset" : 217
    }, {
      "referenceID" : 18,
      "context" : "Whenever the provided evidence contradicts parametric knowledge, the model must prefer the evidence, which recent work has shown is difficult for pretrained language models (Krishna et al., 2021; Longpre et al., 2021).",
      "startOffset" : 173,
      "endOffset" : 217
    }, {
      "referenceID" : 16,
      "context" : "As we will see in Section 5 this poses a challenge for standard evaluation metrics like ROUGE (Lin, 2004) as systems can achieve high scores without making any updates.",
      "startOffset" : 94,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "However measuring faithfulness of generations is an active area of research (Çelikyilmaz et al., 2020) and adapting existing metrics to the FRUIT task is non-trivial.",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Filter Stylistic Updates A large number of edits to Wikipedia are stylistic (Daxenberger and Gurevych, 2012), and are therefore irrelevant to our task.",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "We consider trivial approaches that copy task inputs, as well as T5, a neural sequence-to-sequence baseline which has shown strong performance on related tasks such as summarization (Raffel et al., 2020; Rothe et al., 2021) We additionally introduce EDIT5, a variant of T5 that produces a sequence of edits instead of the entire updated text, and employs additional tweaks to improve performance.",
      "startOffset" : 182,
      "endOffset" : 223
    }, {
      "referenceID" : 15,
      "context" : "In order to convert these tables to text, we apply a conventional linearization scheme (Lebret et al., 2016; Wiseman et al., 2017) that separates table entries using row and column delimiters.",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 32,
      "context" : "In order to convert these tables to text, we apply a conventional linearization scheme (Lebret et al., 2016; Wiseman et al., 2017) that separates table entries using row and column delimiters.",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 28,
      "context" : ", 2020) is a pretrained sequence-tosequence (Sutskever et al., 2014) model based on the transformer architecture (Vaswani et al.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : ", 2014) model based on the transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "Similar to a copy mechanism (See et al., 2017), this allows the model to dedicate less capacity to repeating sequences from the input.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "This approach, inspired by the use of entity chains for summarization (Narayan et al., 2021), trains the model to plan which references to use before generating an update.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "Neural models have expanded the capabilities of writing assistants to solve a wider variety of tasks including: autocompletion (Chen et al., 2019), and following rhetorical directives such as paraphrasing, elaborating, etc.",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "As our primary focus is on writing grounded updates to Wikipedia articles, our work is closely related to existing works on Wikipedia article generation, which generally uses one of two approaches: data-to-text generation (Lebret et al., 2016; Bao et al., 2018; Parikh et al., 2020; Chen et al., 2021; Cheng et al., 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al.",
      "startOffset" : 222,
      "endOffset" : 321
    }, {
      "referenceID" : 5,
      "context" : "As our primary focus is on writing grounded updates to Wikipedia articles, our work is closely related to existing works on Wikipedia article generation, which generally uses one of two approaches: data-to-text generation (Lebret et al., 2016; Bao et al., 2018; Parikh et al., 2020; Chen et al., 2021; Cheng et al., 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al.",
      "startOffset" : 222,
      "endOffset" : 321
    }, {
      "referenceID" : 20,
      "context" : "As our primary focus is on writing grounded updates to Wikipedia articles, our work is closely related to existing works on Wikipedia article generation, which generally uses one of two approaches: data-to-text generation (Lebret et al., 2016; Bao et al., 2018; Parikh et al., 2020; Chen et al., 2021; Cheng et al., 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al.",
      "startOffset" : 222,
      "endOffset" : 321
    }, {
      "referenceID" : 7,
      "context" : "As our primary focus is on writing grounded updates to Wikipedia articles, our work is closely related to existing works on Wikipedia article generation, which generally uses one of two approaches: data-to-text generation (Lebret et al., 2016; Bao et al., 2018; Parikh et al., 2020; Chen et al., 2021; Cheng et al., 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al.",
      "startOffset" : 222,
      "endOffset" : 321
    }, {
      "referenceID" : 9,
      "context" : "As our primary focus is on writing grounded updates to Wikipedia articles, our work is closely related to existing works on Wikipedia article generation, which generally uses one of two approaches: data-to-text generation (Lebret et al., 2016; Bao et al., 2018; Parikh et al., 2020; Chen et al., 2021; Cheng et al., 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al.",
      "startOffset" : 222,
      "endOffset" : 321
    }, {
      "referenceID" : 4,
      "context" : ", 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al., 2018; Shi et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : ", 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al., 2018; Shi et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 103
    }, {
      "referenceID" : 26,
      "context" : ", 2020), or multi-document summarization (Banerjee and Mitra, 2016; Liu et al., 2018; Shi et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "Determining which facts contradict claims in the existing article is a central topic of work on fact extraction and verification (Thorne et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 150
    } ],
    "year" : 0,
    "abstractText" : "Textual knowledge bases such as Wikipedia require considerable effort to keep up to date and consistent. While automated writing assistants could potentially ease this burden, the problem of suggesting edits grounded in external knowledge has been under-explored. In this paper, we introduce the novel generation task of faithfully reflecting updated information in text (FRUIT) where the goal is to update an existing article given new evidence. We release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data produced from pairs of Wikipedia snapshots, along with our data generation pipeline and a gold evaluation set of 914 instances whose edits are guaranteed to be supported by the evidence. We provide benchmark results for popular generation systems as well as EDIT5—a T5-based approach tailored to editing we introduce that establishes the state of the art. Our analysis shows that developing models that can update articles faithfully requires new capabilities for neural generation models, and opens doors to many new applications. Our data and code will be available at: www.omitted.link.",
    "creator" : null
  }
}
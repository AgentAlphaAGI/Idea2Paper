{
  "name" : "ARR_2022_179_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Focus on the Target’s Vocabulary: Masked Label Smoothing for Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent advances in Transformer-based (Vaswani et al., 2017) models have achieved remarkable success in Neural Machine Translation (NMT). For most NMT studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021), there are two widely used techniques to improve the quality of the translation: Label Smoothing (LS) and Vocabulary Sharing (VS). Label smoothing (Pereyra et al., 2017) turns the hard one-hot labels into a soft weighted mixture of the golden label and the uniform distribution over the whole vocabulary, which serves as an effective regularization technique to prevent over-fitting and overconfidence (Müller et al., 2019) of the model. In addition, vocabulary sharing (Xia et al., 2019) is another commonly used technique, which unifies the vocabulary of both source and target language into a whole vocabulary, and therefore the vocabulary is shared. It enhances the semantic correlation\nbetween the two languages and reduces the number of total parameters of the embedding matrices.\nHowever, in this paper, we argue that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting, and leads to suboptimal performance. Specifically, with vocabulary sharing, the shared vocabulary can be divided into three parts as shown in Figure 1. But with label smoothing, the soft label still considers the words at the source side that are impossible to appear at the target side. This would mislead the translation model and exerts a negative effect on the translation performance. As shown in Table 1, although introducing label smoothing or vocabulary sharing alone can outperform the vanilla Transformer, jointly adopting both of them cannot obtain further improvements but achieves sub-optimal results.\nTo address the conflict of label smoothing and\nvocabulary sharing, we first propose a new mechanism named Weighted Label Smoothing (WLS) to control the label distribution and its parameter-free version Masked Label Smoothing (MLS). Simple yet effective, MLS constrains the soft label not to assign soft probability label towards the words belonging to the source side. In this way, we not only keeps the benefits of both label smoothing and vocabulary sharing, but also address the conflict of these two techniques to improve the quality of the translation.\nAccording to our experiments, MLS leads to a better translation not only in BLEU scores but also reports improvement in model’s calibration. Compared with original label smoothing with vocabulary sharing, MLS outperforms in WMT’14 EN-DE(+0.47 BLEU), IWSLT’16 ENRO (+0.33 BLEU) and other 7 language pairs including DE,RO-EN multilingual translation task."
    }, {
      "heading" : "2 Background",
      "text" : "Label Smoothing The original label smoothing can be formalized as:\nŷLS = ŷ(1− α) +α/K (1)\nK denotes the number of classes, α is the label smoothing parameter, α/K is the soft label, ŷ is a vector where the correct label equals to 1 and others equal to zero and ŷLS is the modified targets.\nLabel smoothing is first introduced to image classification (Szegedy et al., 2016) task. Pereyra et al. (2017); Edunov et al. (2018) explore label smoothing’s application in Sequence generation from token level and Norouzi et al. (2016) propose sentence level’s label smoothing. Theoretically, Gao et al. (2020); Müller et al. (2019); Meister et al. (2020) all point out the relation between label smoothing and entropy regularization. To generate more reliable soft labels, Lukasik et al. (2020) takes semantically similar n-grams overlap into consideration level label smoothing, Zhang et al. (2021) investigate generating soft labels based on the statistics of the model prediction for the target category and Wang et al. (2021) propose a new form of label smoothing to diversify dialog generation.\nVocabulary Sharing Vocabulary sharing is widely applied in most neural machine translation studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021). Researchers have conducted in-depth studies in Vocabulary Sharing.\nLiu et al. (2019) propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings. While Kim et al. (2019) point out that there is an vocabulary mismatch between parent and child languages in shared multilingual word embedding."
    }, {
      "heading" : "3 Conflict Between Label Smoothing and Vocabulary Sharing",
      "text" : "Words or subwords in a language pair’s joint dictionary can be categorized into three classes: source, common and target using Venn Diagram according to their belonging to certain language as depicted in Figure 1. This can be achieved by checking whether one token in the joint vocabulary also belongs to the source/target vocabulary. We formalized the categorization algorithm in Appendix A.\nThen we compute the tokens’ distribution in different translation directions as shown in Table 2. Tokens in source class account for a large proportion up to 50%. When label smoothing and vocabulary sharing are together applied, the smoothed probability will be allocated to words that belong to the source class. Those words have zero overlap with the possible target words, therefore they have no chance to appear in the target sentence, which might introduce extra bias for the translation system during training process.\nTable 3 reveals the existence of conflict, that the joint use of label smoothing and vocabulary sharing doesn’t compare with solely use one technique in all language pairs with a maximum loss of 0.32 BLEU score."
    }, {
      "heading" : "4 Methods",
      "text" : ""
    }, {
      "heading" : "4.1 Weighted Label Smoothing",
      "text" : "To deal with the conflict when executing label smoothing, we propose a plug-and-play Weighted Label Smoothing mechanism to control the smoothed probability’s distribution.\nWeighted Label Smoothing(WLS) has three parameters βt, βc, βs apart from the label smoothing parameter α, where the ratio of the three parameters represents the portion of smoothed probability allocated to the target, common and source class and the sum of the three parameters is 1. The distribution within token class follows a uniform distribution. WLS can be formalized as:\nŷWLS = ŷ(1− α) + β (2)\nwhere ŷ is a vector where the element corresponding to the correct token equals to 1 and others equal to zero. β is a vector that controls the distribution of probability allocated to incorrect tokens. We use ti, ci, si to represent probability allocated to the i-th token in the target,common,source category, all of which form the distribution controlling vector β with ∑K i βi = α. The restriction can be formalized as:∑ ti : ∑ ci : ∑ si = βt : βc : βs (3)"
    }, {
      "heading" : "4.2 Masked Label Smoothing",
      "text" : "Based on the Weight Label Smoothing mechanism, we can now implement Masked Label Smoothing by set βs to 0 and regard the target and common category as one category. In this way, Masked Label Smoothing is parameter-free and implicitly injects external knowledge to the model. And we have found out that this simple setting can reach satisfactory results according our experiments.\nWe illustrate different label smoothing methods in Figure 2. It is worth noticing that MLS is different from setting WLS’s parameters to 1-1-0 since there might be different number of tokens in the common and target vocab."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Task Settings",
      "text" : "For bilingual translation, we conduct experiments on 7 translation tasks. We choose language pairs that have different ratio of common subwords. These include WMT’14 DE-EN,EN-DE, IWSLT’14 DE-EN, IWSLT’15 VI-EN, IWSLT’16 RO-EN,EN-RO and CASIA ZH-EN.\nFor multilingual translation, we combine the IWSLT’16 RO-EN and IWSLT’14 DE-EN datasets to formulate a RO,DE-EN translation task. We make a balanced multilingual dataset that has equal numbers of DE-EN and RO-EN training examples to reduce the impact of imbalance languages.\nWe use the Transformer base (Vaswani et al., 2017) model as our baseline model. During training, we fix the label smoothing parameter α to 0.1 whenever LS is applied. We list the concrete training and evaluation settings in Appendix B."
    }, {
      "heading" : "5.2 Results",
      "text" : "Bilingual Table 3 shows the results of bilingual translation experiments. The results reveal the conflict between LS and VS that models with only LS outperform models with both LS and VS in all experiments. Our Masked Label Smoothing obtained consistent improvements over original LS+VS in all tested language pairs significantly.\nThe effectiveness of MLS maintained under different α value as shown in Table 6,7 for both BLEU and chrF, which further proves that not only the increase in target vocabulary, but also the decrease of probabilities in source vocabulary matters in the improvement of translation performance.\nMultilingual As shown in Table 3 , MLS achieves consistent improvement over the original label smoothing in both the original and the balanced multilingual translation dataset under all translation directions. Compared with the imbalanced version, the balanced version gave better BLEU scores in DE-EN direction while much worse performance in RO-EN translation. We discuss more of the multilingual result in Appendix C.\nMLS proves its robustness and effectiveness under different translation settings."
    }, {
      "heading" : "6 Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Improvement in Model’s Calibration",
      "text" : "Guo et al. (2017) have pointed out that by softening the estimation targets, label smoothing prevents\nthe model from becoming over-confident therefore improve the calibration of model as analyzed in (Müller et al., 2019). Inference ECE score (Wang et al., 2020) reflects models calibration during inference. We compute the inference ECE scores of our models as shown in Table 4. The results indicate that MLS will lead to better calibration."
    }, {
      "heading" : "6.2 Exploring of Weighted Label Smoothing",
      "text" : "As reported in Table 5, we further explore the influence of different weighted label smoothing settings on multiple translation tasks including IWSLT’16 RO-EN,EN-RO and WMT’14 DE-EN.\nAccording to the result, though the best BLEU score’s WLS setting vary from different tasks, we still have two observations: First, applying WLS can generally boost the quality of translation compared to the original label smoothing. Second, only WLS with βt, βc, βs each equals to 1/2-1/2-0 can outperform the original label smoothing on all tasks, which suggests the setting is the most robust one. Thus we recommend using this setting as the initial setting when applying our WLS.\nFurthermore, the winner setting agrees with the form of Masked Label Smoothing since they both allocate zero probability to the source category’s tokens, which further proves the effectiveness and robustness of Masked Label Smoothing."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We reveal and analyse the conflict between label smoothing and vocabulary sharing techniques, and point out that jointly adopting them may lead to sub-optimal performance. To address this issue, we introduce a plug-and-play Masked Label Smoothing mechanism to eliminate the conflict. Simple yet effective, MLS shows a consistent and significant improvement over original label smoothing with vocabulary sharing."
    }, {
      "heading" : "A Algorithms",
      "text" : "Algorithm 1 Divide Token Categories Input: List: S, T, J Output: List: A,B,C Description: S is the vocabulary list for source language, T for target language, J for joint vocabulary. A is the output vocabulary for source tokens, B for common tokens, C for target tokens.\n1: Initialize empty list A,B,C 2: for i in J do 3: if i in S and i in T then 4: B.add(i) 5: else 6: if i in S then 7: A.add(i) 8: else 9: C.add(i)\n10: return A,B,C"
    }, {
      "heading" : "B Experiment Details",
      "text" : "We use the official train-dev-test split of WMT’14 and IWSLT’14,15,16 dataset. For CASIA ZH-EN dataset, we randomly select 5000 sentences as development set and 5000 sentences as test set from the total dataset.\nWe evaluate our method upon Transformer-Base (Vaswani et al., 2017) and conduct experiments under same hyper-parameters for fair comparison. We use compound_split_bleu.sh from fairseq to compute the final bleu scores. The inference ECE score1 and chrF score2 are computed through open source scripts.\nBefore training, we first apply BPE(Sennrich et al., 2016) to tokenize the corpus for 16k steps each language and then learn a joint dictionary. During training, the label smoothing parameter α is set to 0.1 in all experiments. We use Adam optimizer with betas to be (0.9,0.98) and learning rate is 0.0007. During warming up steps, the initial learning rate is 1e-7 and there are 5000 warm-up steps. We use a batchsize of 4096 together with an update-freq of 4 on two Nvidia 3090 GPUs. Dropout rate is set to 0.3 and weight decay is set to 0.0001 for all experiments. We use beam size as 5 during all testing.\n1https://github.com/shuo-git/InfECE 2https://github.com/m-popovic/chrF"
    }, {
      "heading" : "C Result Analysis",
      "text" : "Multilingual Compared with the imbalanced version, the balanced version gave better BLEU scores in DE-EN direction while much worse performance in RO-EN translation for both the original label smoothing and MLS.\nIt indicates that the cut down on RO-EN training examples does weaken the generalization of model in RO-EN translation however doesn’t influence the DE-EN translation quality since the RO-EN data might introduce bias to the training process for DE-EN translation. Compared with the bilingual translation, DE-EN translation outperform by a large margin with RO-EN data enhancement no matter whether MLS is applied, which is consistent to (Aharoni et al., 2019).\nDifferent Alpha for LS We report the result of different alpha value in following tables:"
    } ],
    "references" : [ {
      "title" : "Massively multilingual neural machine translation",
      "author" : [ "Roee Aharoni", "Melvin Johnson", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Aharoni et al\\.,? 2019",
      "shortCiteRegEx" : "Aharoni et al\\.",
      "year" : 2019
    }, {
      "title" : "Classical structured prediction losses for sequence to sequence learning",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier", "Marc’Aurelio Ranzato" ],
      "venue" : null,
      "citeRegEx" : "Edunov et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards a better understanding of label smoothing in neural machine translation",
      "author" : [ "Yingbo Gao", "Weiyue Wang", "Christian Herold", "Zijian Yang", "Hermann Ney." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Associa-",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, page 1321–1330. JMLR.org.",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Effective cross-lingual transfer of neural machine translation models without shared vocabularies",
      "author" : [ "Yunsu Kim", "Yingbo Gao", "Hermann Ney." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1246–",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretraining multilingual neural machine translation by leveraging alignment information",
      "author" : [ "Zehui Lin", "Xiao Pan", "Mingxuan Wang", "Xipeng Qiu", "Jiangtao Feng", "Hao Zhou", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Shared-private bilingual word embeddings for neural machine translation",
      "author" : [ "Xuebo Liu", "Derek F. Wong", "Yang Liu", "Lidia S. Chao", "Tong Xiao", "Jingbo Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic label smoothing for sequence to sequence problems",
      "author" : [ "M. Lukasik", "Himanshu Jain", "A. Menon", "Seungyeon Kim", "Srinadh Bhojanapalli", "F. Yu", "Sanjiv Kumar." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Lukasik et al\\.,? 2020",
      "shortCiteRegEx" : "Lukasik et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalized entropy regularization or: There’s nothing special about label smoothing",
      "author" : [ "Clara Meister", "Elizabeth Salesky", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6870–",
      "citeRegEx" : "Meister et al\\.,? 2020",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2020
    }, {
      "title" : "When does label smoothing help? In NeurIPS",
      "author" : [ "Rafael Müller", "Simon Kornblith", "Geoffrey E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Müller et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2019
    }, {
      "title" : "Reward augmented maximum likelihood for neural structured prediction",
      "author" : [ "Mohammad Norouzi", "Samy Bengio", "zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Norouzi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Norouzi et al\\.",
      "year" : 2016
    }, {
      "title" : "Contrastive learning for many-to-many multilingual neural machine translation",
      "author" : [ "Xiao Pan", "Mingxuan Wang", "Liwei Wu", "Lei Li." ],
      "venue" : "Proceedings of ACL 2021.",
      "citeRegEx" : "Pan et al\\.,? 2021",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2021
    }, {
      "title" : "Regularizing neural networks by penalizing confident output distributions",
      "author" : [ "Gabriel Pereyra", "G. Tucker", "J. Chorowski", "Lukasz Kaiser", "Geoffrey E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Pereyra et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Pereyra et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "ICML.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "V. Vanhoucke", "S. Ioffe", "Jonathon Shlens", "Z. Wojna." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "On the inference calibration of neural machine translation",
      "author" : [ "Shuo Wang", "Zhaopeng Tu", "Shuming Shi", "Yang Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3070–3079, Online. Association for",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Diversifying dialog generation via adaptive label smoothing",
      "author" : [ "Yida Wang", "Yinhe Zheng", "Yong Jiang", "Minlie Huang." ],
      "venue" : "ACL/IJCNLP.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Tied transformers: Neural machine translation with shared encoder and decoder",
      "author" : [ "Yingce Xia", "Tianyu He", "Xu Tan", "Fei Tian", "Di He", "Tao Qin." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Delving deep into label smoothing",
      "author" : [ "Chang-Bin Zhang", "Peng-Tao Jiang", "Qibin Hou", "Yunchao Wei", "Qi Han", "Zhen Li", "Ming-Ming Cheng." ],
      "venue" : "IEEE Transactions on Image Processing, 30:5984–5996.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Recent advances in Transformer-based (Vaswani et al., 2017) models have achieved remarkable success in Neural Machine Translation (NMT).",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "For most NMT studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021), there are two widely used techniques to improve the quality of the translation: Label Smoothing (LS) and Vocabulary Sharing (VS).",
      "startOffset" : 21,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "For most NMT studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021), there are two widely used techniques to improve the quality of the translation: Label Smoothing (LS) and Vocabulary Sharing (VS).",
      "startOffset" : 21,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "For most NMT studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021), there are two widely used techniques to improve the quality of the translation: Label Smoothing (LS) and Vocabulary Sharing (VS).",
      "startOffset" : 21,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "For most NMT studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021), there are two widely used techniques to improve the quality of the translation: Label Smoothing (LS) and Vocabulary Sharing (VS).",
      "startOffset" : 21,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "Label smoothing (Pereyra et al., 2017) turns the hard one-hot labels into a soft weighted mixture of the golden label and the uniform distribution over the whole vocabulary, which serves as an effective regularization technique to prevent over-fitting and overconfidence (Müller et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : ", 2017) turns the hard one-hot labels into a soft weighted mixture of the golden label and the uniform distribution over the whole vocabulary, which serves as an effective regularization technique to prevent over-fitting and overconfidence (Müller et al., 2019) of the model.",
      "startOffset" : 240,
      "endOffset" : 261
    }, {
      "referenceID" : 19,
      "context" : "In addition, vocabulary sharing (Xia et al., 2019) is another commonly used technique, which unifies the vocabulary of both source and target language into a whole vocabulary, and therefore the vocabulary is shared.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "Label smoothing is first introduced to image classification (Szegedy et al., 2016) task.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "Vocabulary Sharing Vocabulary sharing is widely applied in most neural machine translation studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : "Vocabulary Sharing Vocabulary sharing is widely applied in most neural machine translation studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "Vocabulary Sharing Vocabulary sharing is widely applied in most neural machine translation studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "Vocabulary Sharing Vocabulary sharing is widely applied in most neural machine translation studies (Vaswani et al., 2017; Song et al., 2019; Lin et al., 2020; Pan et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : "We use the Transformer base (Vaswani et al., 2017) model as our baseline model.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "the model from becoming over-confident therefore improve the calibration of model as analyzed in (Müller et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "Inference ECE score (Wang et al., 2020) reflects models calibration during inference.",
      "startOffset" : 20,
      "endOffset" : 39
    } ],
    "year" : 0,
    "abstractText" : "Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that jointly adopting these two techniques can be conflicting and even leads to sub-optimal performance, since the soft label produced by label smoothing still considers the source-side words that would not appear at the target side. To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing and hence improves the quality of the translation. Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation in both BLEU and calibration scores.",
    "creator" : null
  }
}
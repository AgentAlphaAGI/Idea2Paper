{
  "name" : "ARR_2022_358_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TEAM: A multitask learning based Taxonomy Expansion approach for Attach and Merge",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Taxonomy, such as the WordNet, is a crucial resource for developing NLP related technologies, as it plays a vital role in various text processing tasks such as information retrieval, information extraction, text classification, summarization, etc. (Pang et al., 2008; Allan et al., 1998; Singhal et al., 2001) (Miller, 1998). As most of the WordNets are manually curated, it often suffers from the problem of limited coverage. Therefore, an automatic taxonomy expansion is a crucial problem to handle the above issue. For taxonomy expansion, WordNet in particular, may need two types of operations; (i) merge, where a new concept 1 is merged to an existing node, and (ii) attach, where a new concept is inserted as a new node. Figure 1 illustrates these two operations where the word Mango is inserted as a new concept with the attach operation, and the\n1Concept is a basic building block of WordNet, which refers a definition with associated synonym words\n“Mango” is a specific concept of Fruit not present in the existing WordNet. Hence, a new concept node is created in the taxonomy by attaching it to its generic concept Fruit . As “Nutrient” refers to the same concept as \"Food\", no new concept is created. “Nutrient” is merged with the existing concept “Food”.\nword Nutrient is inserted as a new synonymy in an existing concept with the merge operation.\nThough both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together. Realizing the need to apply both the operation, SemEval-2016:task 14 (Semantic taxonomy enrichment) Jurgens and Pilehvar (2016) includes a call for expansion with both attach and merge operations. However, none of the submissions incorporate both operations in a single model.\nMotivated by the above observations, in this study, we propose an integrated deep learningbased method, namely, Taxonomy Expansion with Attach and Merge (TEAM), which performs both the attach and merge operations in a multitasklearning framework. Though most of the existing studies consider the expansion a regression problem, considering that our method performs both the attach and merge operation in a single model, it can also be considered a classification task. As a result, we propose two versions of TEAM, namely, TEAM-RG: Regression, and TEAM-CL: Classification to perform with implicit and explicit rankings. The proposed models have been evaluated on three different WordNet taxonomies, viz., Assamese, Bangla, and Hindi. From the various experimental setups, it is observed that the proposed TEAM-RG and TEAM-CL outperform their baselines counterparts for attach operation, and also obtained encouraging performance for merge operation as well. The major contributions of the paper are summarized as follows:\n• A multi-task learning based taxonomy expansion framework TEAM that performs both the Attach and Merge operations together. To the best of our knowledge, it is the first integrated model to perform both the Attach and Merge operations in a single model.\n• Two variants of TEAM, namely TEAMRegression (RG) and TEAM-Classification (CL) are proposed."
    }, {
      "heading" : "2 Taxonomy Expansion - Attach and Merge",
      "text" : "In this study, we have considered WordNets as our target taxonomies. A WordNet may be defined by a collection of concepts connected by various semantic relationships such as hypernymy, hyponymy, troponymy, etc., where each concept is further defined by a set of attributes such as definition, synonyms, examples, etc. In this study, we have considered only the hypernymy relation and the definition and synonymy attributes.\nIn order to be able to apply the proposed model, we first transform the original WordNet taxonomy into an experimental intermediate taxonomy ( directed unweighted acyclic graph) T = (V,E) where V represents the set of concepts and E represents the set of hypernymy relations between the concepts. A concept v ∈ V is further defined by\na tuple v = (dv, sv) where dv represents the definition of the concept, and sv represents the set of associated synonyms. An edge e ∈ E represents a hypernymy relation from a parent concept vp to its child concept vch and is denoted as e : (vp hyper−−−→ vch). The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1. Given the taxonomy T and a query concept q = (dq, sq), the attach and the merge expansion operations are defined below.\nAttach (A) — An attach operation is performed when the concept q is not present in T. The objective of the attach operation is to identify the best matching parent node in taxonomy network known as anchor concept a = (da, sa), and insert a the new concept q with an edge e : (a hyper−−−→ q). In a taxonomy network, a parent node represents a more generic concept of its children. After an attach operation i.e., insertion of q in T under the anchor a, the expanded taxonomy is updated as follows.\nT = (V ∪ {q}, E ∪ {e}) (1)\nMerge (M) — A merge operation is performed when an equivalent concept a = (da, sa) of the query q (i.e., da ≡ dq) is already present in T, but the synset sq is not present in a (i.e., sq ∩ sa = ∅). The objective of the merge operation is to identify the best matching concept a = (da, sa), known as the anchor concept, in the taxonomy network T and add the synset sq to sa. It neither creates a new node nor adds a new edge. It only updates the synset of the anchor concept. After the merge operation, the updated anchor concept in the expanded taxonomy can be expressed as follows.\na = (da, sa ∪ sq) : a ∈ V (2)"
    }, {
      "heading" : "3 Proposed Methods",
      "text" : "Our objective is to develop an integrated model that performs both attach and merge operations for taxonomy expansion. Since we have two tasks to unify in a single model, we resort to a multi-task learning framework known as Taxonomy Expansion Framework with Attach and Merge (TEAM). This joint learning objective facilitates information flow so that the two tasks can aid each other. Also, we are interested in deciding which expansion operation is to perform given a triplet (expansion task classification) and retrieving the ranked list of candidates\nConcept is a tuple of definition, synset(d, s) Fruit (s) The ripened reproductive body of a seed plant.(d) Vegetable, Veggie (s)\nEdible seeds or roots or stems or leaves or bulbs or tubers or non sweet fruits of any of numerous herbaceous plant.(d)\n(ranking) as prospective anchors to associate the query with. For this first-of-its-kind novel taxonomy expansion task, we propose two versions of TEAM, namely TEAM–Regression (TEAM-RG) and TEAM–Classification (TEAM-CL) — where we show that using either regression or classification learning objectives, this task can be accomplished."
    }, {
      "heading" : "3.1 Training dataset generation",
      "text" : "Given a transformed taxonomy T (as described in Section 2), we generate a training dataset for building the model as follows. The training samples are defined by a 3-tuple < q, a, label >, where q is the query, a is the potential anchor, and label is associated class, i.e., true/false (1/0). We randomly select a set of nodes in T as a set of queries 2, and generate the training samples for the attach and the merge operations separately as follows.\nAttach (A) — We first remove the query nodes from the T. For each query q = (dq, sq), we consider its parent as anchor node a = (da, sa) and generate positive sample < q, a, TRUE >. We then randomly pick up N number other nodes a′ = (da′ , sa′), and generate N negative samples < q, a′, FALSE >. Thus, for a given query node q, we extract one positive and N negative samples.\nMerge (M) — For each of the randomly selected query node x = (dx, sx >) in T, we generate the following positive training sample < q, x, True > where q = (dx, sq), sq ⊂ sx is the query and x = (dx, sx − sq) is the anchor. The sq is a randomly selected synonym in sx. Unlike attach, for generating the training sample for the query q, we only remove the query synset sq from the anchor synset sx i.e., sx = sx − sq, and, not the node. Like attach, we randomly pick up N number other nodes a′, and generate N negative samples\n2As we consider the same query set for both attach and merge experiments, nodes with at least two synonyms are considered.\n< q, a′, FALSE >. Figure 2 illustrates the generation of the training samples from a taxonomy."
    }, {
      "heading" : "3.2 TEAM-Regression (TEAM-RG)",
      "text" : "The proposed TEAM-RG works in two tiers process. Given a training input sample < q, a, c >, it first generates encoding of the query q and the anchor a. It then merges to a shared layer to produce two different multi-tasking dense networks; one for merge and another for attach, as shown in figure 4.D.\nFor learning embedding of the anchor concept from the taxonomy network and the query concept from the associated attributes, we consider the publicly available Fasttext pre-trained embedding available at https://fasttext.cc/docs/ en/crawl-vectors.html.\nProcessing of the query concept: As mentioned in Section 2, a query concept consists of its definition and the associated synset i.e., q = (dq, sq). The definition is a piece of text describing the concept, and the synset is a synonym associated with the query concept. The definition is defined by the average of the pre-trained embeddings of the terms the definition, and the synset is represented by the pre-trained embedding of the synonym word itself. The two embeddings are then concatenated to represent the query.\nProcessing of the anchor concept: For generating the encoding of the anchor concept, we exploit the proximity structure of the nodes in the taxonomy T. For a given anchor node a ∈ T, we first extract its ego-tree from the taxonomy. An ego tree Ta : (Va, Ea) of a node a in the taxonomy T is a sub-tree comprising of the node a and its k-hop neighborhood nodes. In this study, we considered k = 1, i.e., the anchor node, its parent node, and all its children nodes. Figure 3 illustrates an example of an ego tree. A similar approach has also been used in (Wang et al., 2021; Yu et al., 2020b; Zhang et al., 2021; Shen et al., 2020) studies. To obtain the embedding of the anchor concept, we further apply graph embedding as described below."
    }, {
      "heading" : "3.2.1 Embedding Ego-tree",
      "text" : "Ideally, we should be able to use any graph embedding method to obtain the embedding of the anchor node. As the objective is to incorporate the positional information of the parent and children node in the ego tree, we use the Graph Attention Network (GAT) proposed in Taxo-Expan (Shen et al., 2020). This GAT is a special type of graph neural network (GNN) (Kipf and Welling, 2016) with a\nneighborhood-based attention mechanism. The details of GAT and its difference from GNN are given in Section B of Appendix.\nWe summarize the tree by applying an activation function over the average of the embedding vectors of all nodes in the ego-tree as given in equation 3 to define the encoding of the anchor node.\nT̄a = σ ( 1 |Va| ∑ x∈Va x̄ )\n(3)\nwhere σ(.) is an activation function. We have considered Sigmoid function in this study."
    }, {
      "heading" : "3.2.2 Multi-task Learning",
      "text" : "Once we obtain the embeddings of the anchor and query concepts, the concatenated vector is subjected to a shared dense layer and then build two multi-task layers to perform the merge and attach operations as shown in Figure 4.D. Given a query concept and its true anchor concept with N false anchor concepts, the task is to design a regressionbased ranking model such that the true concept is ranked higher than the N false concepts. This objective should be realized for all the queries in the training dataset.\nGiven the embedding vectors of anchor ā and query q̄ as learned above, we first estimate similarity between the two using a bi-linear model proposed in (Gutmann and Hyvärinen, 2010). It learns the discrimination between q and a through a learnable bi-linear scoring matrix B ∈ R|q̄|×|ā| via a function D : R|q̄|×|ā| 7→ R as follows.\nD(q, a) = σ(q̄TBā) (4)\nHere σ is sigmoid non-linearity. The output of this matching module is a probability estimate indicating the strength of association between the query and anchor. Now, considering the query concept q and its associated N + 1 anchor concepts, we\nestimate the probability of being the correct anchor using InfoNCE loss proposed in (Oord et al., 2018). Let X be a set of query concepts and their respective N + 1 anchor nodes (one positive and N negative). An element of xq ∈ X for a given query q consists of {(q, a, 1), (q, a′1, a′2, ..., a′N, 0)}, where a is the positive anchor, and a′ are the negative anchors of q. InfoNCE estimates loss function using an average probability of being true anchor node across the dataset X as follows.\nLA/M = − 1 |X| ∑ xq∈X log D(q̄, ā)∑ v∈M(q)D(q̄, v̄) (5)\nwhere M(q) denotes the set of both positive and negative anchors of q. As mentioned earlier, the loss defined in Equation 5 is estimated separately for attach and merge operations. Therefore, we generate two different training datasets for attach and merge, and estimate LA and LM separately using respective datasets, The final model loss is defined as L = LA + LM — considering both the operations attach and merge."
    }, {
      "heading" : "3.3 TEAM-Classification (TEAM-CL)",
      "text" : "Figure 4E shows the schematic diagram of the TEAM-CL. We use the identical representations for query q and candidate anchors aA, aM as described for TEAM-RG. We also adopt the same position-enhanced graph propagation and read-out modules as described in Section 3.2.1 for learning anchor a = (da, sa) concept representation. Once we obtain the query and anchor representations, we model the strength of association of an input query and the candidate anchors based on their features to predict the expansion task i.e., merge M or attach A. The matching module, a multilayer perceptron (MLP) based classifier, takes the features of query q̄ ∈ R|q̄| and anchor ā ∈ R|ā|, and generates a contextualized pair representation k̄ = [q̄⊕(q̄−ā)⊕(q̄×ā)⊕ā] (assuming |q̄| = |ā|). Here, ⊕ denotes concatenation. The anchor a can be any of the attach or merge candidates (aA/aM ). A three-way classifier is learned to produce the categorical probability distribution over the training samples for Merge (M), Attach (A) and Nooperation (N) — three classes (|Z| = 3) of operations. If θ ∈ R|k̄|×|Z̄| be a learnable projection matrix that projects the contextualized pair embedding k̄ to the label space Z ∈ R3. The predictions are obtained as below,\nŶ = softmax(MLP(k̄; θ)) (6)\nv M A q(aA / aA’) or (aM/ aM’) Task-specific layers Matching Module Attach Matching Module Merge 5 3 2 4\n1\nGraph propagation\nmodule\nGraph readout module\nQuery Q\n[ q, q - aA , q x aA , aA ] [ q, q - aM , q x aM , aM ]\nContrastive scoring for embeddings\nPairwise Hinge loss for prediction\nranking M A N M A N\nSharable weights (Same module shown twice for\nclarity)\nMatching module\naA / aA’ aM/ aM’\nGraph propagation\nmodule\nGraph readout module\nq\nMatching module\nD: TEAM-Regression (RG) E: TEAM-Classification (CL)\nEgo-Tree Represent ation\nAnchor (A/M)\nQuery Q\nEgo tree extracted from taxonomy\nEgo-Tree Representation\nAnchor (A/M)\nEgo tree extracted from taxonomy\nv\nQuery representation(“meat”) Anchor representation(“Food”) Query(d,s)\nAnchor(d,s)\nA: Representation of Query and Anchor\nObject\nRock, Stone\nFood\nFruit Vegetable, veggie meat\nNeighbors of anchor\nB: Ego tree of anchor “Food” extracted from taxonomy\nC: Ego tree structure representation\nFigure 4: Taxonomy Expansion framework with Attach and Merge (TEAM) D: TEAM-Regression-RG — •1. (Query Q, Anchor, (N+1) Corrupted Anchors) are fed to the model, •2. Representation learning via shared graph propagation and readout modules, •3. Projection to shared hidden layers, •4. Task-specific matching modules with non-shareable weights, •5. Task-specific regression outputs. E: TEAM-Classification (CL) — •1. (Query Q, Anchor-A, Anchor-M, (N+1) Corrupted Anchors) are fed to the model, •2. Representation learning via shared graph propagation and readout modules, •3. Projection to shared hidden layers, •4. Simultaneous optimization of classification and ranking losses. •5.Three-way merge, attach, no-operation (M, A, N) prediction. Explanation of used color-codes. Light-Purple: Anchor (A/M) nodes, Orange: Children of the anchor, Purple: Parent of the anchor. Green: Definition representation of anchors, Blue: Synset representation of anchors, Yellow: Query representation.\nFor two versions of TEAM, we chose two different kinds of matching models based on empirical performances to capture different kinds of embedding interaction in the latent space."
    }, {
      "heading" : "3.3.1 Multi-task Learning",
      "text" : "Classification. Unlike in TEAM-RG, where we posit taxonomy expansion as a regression task with implicit ranking viz. discriminating true and false examples via InfoNCE loss, in TEAM-CL, we simultaneously optimize for classification and explicit ranking objectives. We obtain classification predictions from the matching module as described before. Given a training set X, and a set of classes Z (M: Merge, A: Attach, N: No-operation), we optimize for the self-supervised cross-entropy loss over the task predictions Ŷ given the ground-truth task-classes Y for an input query-anchor pair.\nLC = − 1 |X| ∑ i∈X ∑ z∈Z YizlnŶiz (7)\nRanking. The classification objective can only learn and infer the confidence score of an operation (M/ A/ N) for a training sample. It fails to give us a reliable ranked list of prospective anchors-(A/ M) given a query — since it does not learn the relative ranks of positive and corrupted anchors for a query. As illustrated in Figure 4, for a query q, (i) the ego-tree of anchor-A comprises of that query’s parent’s hierarchical neighborhood, and (ii) the ego-tree of anchor-M comprises of that query’s\nreplica’s (same/similar definition with a missing portion of synset) hierarchical neighborhood. Since a query q is very similar to both of its anchor-A and anchor-M’s ego-trees – these operations are hardly distinguishable. Thus, a model must accommodate a provision for directly comparing the prediction scores of M and A operations and learning a margin of separation between the scores. Here, we introduce two ranking objectives in the framework — (i) a contrastive objective to compare and contrast among a positive anchor and N negative anchors, (ii) a pair-wise hinge loss to learn a maximum margin between the M and A prediction scores.\nLet, dist(.) be a function to measure the distance between a query q̄ and its true/ false anchor-(A/ M) representations (āA, āA′), (āM , āM ′). We use \"slash\" (/) to denote either. We intend to rank a positive query-anchor pair (q, aA/aM ) higher than N no of corrupted pairs (q, a′A/a ′ M ) by enforcing a group-wise contrastive loss using a margin λ as,\nLR1A = 1\nX ∑ i∈X\n1 |N(q̄i)| ∑\nāA ′ i∈N(q̄i)\nmax(0, λ−m+m′)\nm = − dist(q̄i − āAi), m′ = − dist(q̄i − āA′i)\nWe can similarly compute the margin-based groupwise contrastive loss LR1M for the Merge (M).\nNow, to distinguish between M and A operations, let, f(k̄) be a function that projects the contextualized (q, a) embedding k̄ in Equation 6, to\na hidden space Rh. Here we introduce a marginbased hinge-loss on sample anchor pairs attachmerge ⟨aA, aM ⟩ for a given query q via their contextualized vectors ⟨k̄A, ¯kM ⟩. If class labels of merge and attach are M = 2, A = 1, we ensure the prediction scores ŶA/M = f(k̄A/M ) for M and A are separated by a margin of λ. LR2 = ∑\nY (k̄A)>Y ( ¯kM )\nmax(0, λ− f( ¯kM ) + f(k̄A))\nTherefore, the final loss is, L = LC + LR1A + LR1M + LR2 — considering both margin-based group-wise contrastive loss and pairwise hinge loss comprising the overall ranking loss."
    }, {
      "heading" : "3.4 Model Inference",
      "text" : "We follow Taxo-Expan’s (Shen et al., 2020) evaluation strategy for inferring the best candidate anchor a given a query q. We use our classification objective to decide which operation among merge, attach, or no-operation (M, A, N) to perform when q is given. i) For TEAM-RG, we augment a classification layer on top of the task-specific regression layers. Given a query q and a set of candidate anchors a, we obtain the merge and attach regression scores and choose the best value along with the corresponding operation as the apt operation to perform. ii) For TEAM-CL choosing which operation to perform is obtained based on the three-way prediction scores, given < q, a, (0/1) > as input. Since both of our proposed frameworks optimize for ranking loss, i.e., discriminates true candidate pairs from the corrupted ones — we get a ranked list of candidate anchors a while matching each of them with q via respective matching modules."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "Here we give you an overview of our experiment settings and provide the detailed reproducibility information in the Sections C, D, E of the Appendix.\nDatasets. Table 1 shows the basic statistics of\nthree WordNet taxonomies used in this study. The\ntaxonomy networks are extracted from Assamese, Bengali, and Hindi WordNets, respectively.\nMetrics. We use Mean Rank (MR), Hit@k, and Mean Reciprocal Rank (MRR) to evaluate a model’s performance to produce a ranked list of anchors given a test query. Further, we use Accuracy, Micro/ Macro F1, Precision, Recall, and F-Scores to evaluate a method’s prediction capability to decide which operation among merge (M), attach (A), and no-operation (N) is to perform.\nBaselines. We choose two most recent benchmark SOTA taxonomy-expansion frameworks TaxoExpan (Shen et al., 2020) and Triplet Matching Network(TMN) (Zhang et al., 2021) as the competing methods. In terms of learning objective, Taxo-Expan is similar to ours. It uses ego-treebased anchor features for matching query features in a regression-based setting. TMN captures finegrained relationship dynamics of query and anchor concepts using channel-wise gating mechanismbased attention learning."
    }, {
      "heading" : "5 Results",
      "text" : "Here we report the classification and ranking results of the competing methods. We also compare and contrast among the variants of our TEAM framework. Apart from the two versions of the TEAM, namely, TEAM-RG and TEAM-CL, we have task-specific model variants specified as — attach–A, merge–M and merge+attach–MA. Here, (attach+merge) means simultaneously optimizing for both the tasks."
    }, {
      "heading" : "5.1 Ranking Results",
      "text" : "In Table [2], we show the performance of the competing methods in terms of (best) ranking scores. We see similar trends for all taxonomies in the sub-tables. When considering only attach operation and the test ranking scores, we see TEAMRG clearly beats Taxo-Expan by a large margin of (196.87, 487.75, 470.87) in MR, by a margin of (0.2, 0.14, 0.24) in Hit@1 and (0.31, 0.32, 0.37) in Hit@3 for Assamese, Bengali and Hindi WordNet respectively. We see TEAM-CL though performing competitively but is outperformed by TEAMRG by a margin of (44.82, 86.01, 43.04) in MR, by a margin of (0.11, 0.12, 0.28) in Hit@1 and (0.14, 0.18, 0.43) in Hit@3 respectively for Assamese, Bengali and Hindi WordNet. TMN gives better performance than Taxo-Expan owing to its useful attention mechanism. But, Team-RG(A) out-\nperforms TMN in all the metrics except Hits@1. We only compare Taxo-Expan results for the attach since it is originally proposed for the attach operation. In the (merge-M) and (merge+attachMA) section of the tables also, we see that TEAMRG outperforms TEAM-CL on all three WordNet taxonomies. We attribute this huge performance improvement of TEAM-RG to InfoNCE based training — as it simultaneously provides pseudosupervision from the negative examples while optimizing for the task-specific regression layers."
    }, {
      "heading" : "5.2 Classification Results",
      "text" : "In Table [3], We observe similar trends on all three WordNet taxonomies. Since Taxo-Expan is a regression-based algorithm proposed for only attach operation in taxonomy expansion task — we could not obtain its classification performance. Therefore, we only consider variants of our frameworks as competing methods. As described in the sub-section 3.4, using a classification layer on top of the regression layer in TEAM-RG, we obtain classification performances for the attach, merge operations along with both (attach+merge) operations. Whereas obtaining classification performance for TEAM-CL is straightforward since this is already a classification framework.\nWhen comparing TEAM-RG (attach) and (merge) variants — we see, unlike ranking results where ranking results of merge operation were always better than the attach operation, here the classification results of merge operation are inferior to attach operation. It means that the RG variant learns better ranking as compared to CL variants, but they fail to distinguish M and A – the operation to perform. This is expected since we do not provide a scheme here to contrast M and A operations — which is the motivation for our CL variant framework.\nWhen comparing TEAM-RG and TEAM-CL for (merge+attach), we see TEAM-CL gives better classification scores using test queries except for Macro-F1 scores. TEAM-RG gives the best performance for the Macro-F1 score for the test cases. This essentially means that class-wise prediction performances are inferior for TEAM-CL. This is expected behavior since, in each batch of the training sample, we include a substantially large number (N) of negative examples with class-label (N–No operation). We design our training samples like this so that the contrastive loss is better approximated. Nevertheless, it leads to a class-imbalance issue in our three-way classification setup, i.e., a large number of samples with N class labels as compared to the other M/ A class labels. Thus, TEAM-CL biases its prediction towards the N class, leading to poorer Macro-F1 scores than TEAM-RG.\nTo summarize, we observe that TEAM-RG gives the best ranking performances, whereas TEAM-CL gives the best classification performances. TEAMCL performs poorly in Macro-F1 since it presumably suffers from class-imbalance issues owing to the style of training sample generation. Frameworks with multi-task learning strategy (TEAMRG and TEAM-CL) outperform frameworks (TaxoExpan) designed to perform a single task — which is motivated by the fact that simultaneously optimizing for multiple tasks provides self-supervision to each other, resulting in better performances."
    }, {
      "heading" : "5.3 Expansion of Assamese WordNet Taxonomy with Out-Of-Vocabulary (OOV) words",
      "text" : "To investigate the effectiveness of the proposed models, we employ the models for expanding a WordNet with OOV words. For this, first, we find out-of-vocabulary words, i.e., words that are not present in Assamese WordNet. Second, we\nmanually identify true anchors of respective out-ofvocabulary words with associated operations (Attach/Merge) in Assamese WordNet. We evaluate the predicted results of the proposed model against the manually identified true anchors. Since we can either perform an A or M operation with OOV words and not both, we do not predict expansion tasks for OOV words using any of the MA variants of our proposed frameworks. Table 4 shows the ranking performance of the model in predicting true anchors for attach and merge expansion operations. We see a similar trend of prediction ranking as seen with the test set in our earlier experiments. TEAM-RG gives the best performance in both expansion operations. The detailed analysis of results is in Section F of the Appendix."
    }, {
      "heading" : "6 Related Works",
      "text" : "Existing methods for taxonomy expansion can be divided into two categories: relying on alignment between multiple taxonomies [Ruiz-Casado et al. (2005), Toral and Monachini (2008), Ponzetto and Navigli (2009), and Yamada et al. (2011)] or relying on machine learning-based rating sub-graphs. Further, the latter category can be divided into two sub-categories (1) by expanding synonymy relations/Merge (2) by expanding hypernymy relations/Attach. Synonymy-based taxonomy expansion leverages synonymy relations of the taxonomy. Given a seed taxonomy, the distributional approach discovers synonyms by representing strings with their distributional feature and learning a classifier to predict the relation between strings [(Nakashole et al., 2012), (Wang et al., 2019), (Fei et al., 2019)].\nMost of the recent taxonomy expansion approaches are based on hypernymy expansion. These methods attempt to determine the attachment position by scoring between several nodes. Recently numerous methods have been proposed to solve this problem(Shen et al., 2018), (Shen et al., 2020), (Yu et al., 2020b), (Zhang et al., 2021), (Liu et al., 2021). (Shen et al., 2020). Hence, all the existing taxonomy expansion approaches expand a taxonomy either by merge operation(synonymy expansion) or by attach operation(hypernymy expansion). However, particular to WordNet expansion it is an integrated task of Merge and attach operation. We are the first to study the problem of taxonomy expansion using both the Attach and Merge taxonomy expansion operations in a single model. A detailed related study is in Section A of the Appendix."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we proposed an integrated framework called Taxonomy Expansion with Attach and Merge (TEAM) for expanding taxonomy with attach and merge operations together. We built two multi-task learning-based variants of TEAM, namely, TEAMRegression and TEAM-Classification, which solve the taxonomy expansion problem as regression and classification, respectively. Our proposed methods learned to predict the taxonomy expansion operation (merge, attach, or no-operation) to perform and provided a ranked list of candidates. We evaluated the effectiveness of TEAM on WordNet taxonomies of three distinct languages, viz., Assamese, Bangla, and Hindi. In various experimental setups, the proposed TEAM-RG and TEAM-CL outperformed its state-of-the-art for attach operation and provided a highly encouraging performance on merge operation. We had also investigated the performance of the proposed model with out-of-vocabulary concepts.\nIn the future, we plan to investigate the response of the proposed model with different types of taxonomies and WordNet of different languages."
    }, {
      "heading" : "A Related Works",
      "text" : "Expansion by resource alignment: In the first category of studies, Poprat et al. (2008) first attempted to automatically expand a WordNet with biomedical terminology; however, they were unable in developing the resource. Ruiz-Casado et al. (2005), Toral and Monachini (2008), Ponzetto and Navigli (2009), and Yamada et al. (2011) exploit structured information in Wikipedia to expand WordNet with new synsets. Snow et al. (2006) leverage distributional similarity techniques for WordNet expansion. Jurgens and Pilehvar (2015) enrich the existing WordNet taxonomy using an additional resource, Wiktionary, to extract sense data based on information in the term concepts. Synonymy Expansion: Synonymy expansion in a taxonomy leverages synonymy relations to enrich a taxonomy with new concepts. Approaches for synonymy expansion can be divided in to two categories: (1) Distributional based approach (Wang et al., 2019), (Fei et al., 2019) (2) Pattern-based approach (Nguyen et al., 2017), (Nakashole et al., 2012). Given a seed taxonomy, the distributional approach discovers synonyms by representing strings with their distributional feature and learning a classifier to predict the relation between strings. However, in the pattern-based approach, consider the sentences mentioning a pair of synonymous strings and learn some textual patterns from these sentences, which are further used to discover more synonyms. Qu et al. (2017) proposed an approach that integrates both the categories. Boteanu et al. (2018) focus on the problem of expanding taxonomies with synonyms for applications in which entities are complex concepts arranged into taxonomies designed to facilitate browsing the product catalog on amazon.com. They first generate synonymy candidates for each node in the taxonomy and then filter synonymy candidates using a binary classifier. Yu et al. (2020a) study a task of synonym expansion using transitivity named SYNET, which leverages both the contexts of two synonymy pairs. Hypernymy expansion : Jurgens and Pilehvar (2016) formulated a task of synonymy expansion, where it is proposed to enrich the WordNet taxonomy by performing two operations for each new concept. The first action is Attach, where a new concept is treated as a new synset and is attached as a hyponym of one existing synset in WordNet, and the second action is Merge, where a new concept is merged into an existing synset. The best solution proposed by Schlichtkrull and Alonso (2016) included only the attach operation. Later solutions for attaching, as in Shen et al. (2020), adopted selfsupervision and tried to exploit the information of nodes in the seed taxonomy to perform node pair matching. On the other hand, Yu et al. (2020b) resorted to classification along mini paths in the taxonomy. In contrast, in our current approach, we have incorporated both the attach and merge operations."
    }, {
      "heading" : "B Graph Propagation Module (in details)",
      "text" : "Graph Neural Network (GNN) allows us to transform and propagate node features as messages to learn structure-aware node representations. The EXTRACT() mechanism extracts the messages from a target node and its neighborhood, which is later on combined based on a chosen ATTENTION() mechanism by the AGGREGATE() operation. Next, the aggregated message is propagated to the rest of the graph. Studies apply various aggregation strategies to combine the propagated and extracted messages from the target node’s neighborhood based on the importance of each node in the neighborhood towards that target node. GCN (Kipf and Welling, 2016) and GAT (Veličković et al., 2017) are popular GNN frameworks. GCN uses N(∗) neighborhood-based normalization constant to calculate the importance (attv−→u) of node v towards the target node u without considering the participating nodes’ features as follows. H lu = σ ( ∑ ∀v∈ ˜N(u) attl−1v−→uW l−1H l−1v ) (8) ATTENTIONGCN(v −→ u) : attl−1v−→u = 1√ | ˜N(u)|| ˜N(v)| EXTRACTGCN(v) : W l−1H l−1v AGGREGATEGCN(∗) : σ(∗) where σ is non-linear activation function, W l is a projection matrix for a GNN layer l and ˜N(∗) is a node’s extended neighborhood structure including the node itself (i.e., including self-loop edges). GAT uses the same message extraction and aggregation strategies as above except for the fact that it uses attentive aggregation strategies that consider both the participating nodes’ features as well as the\nneighborhood information, as follows.\nATTENTIONGAT(v −→ u) : attl−1v−→u =\nSOFTMAX ∀v∈ ˜N(u)\n( cl−1(W l−1H l−1u ⊕ W l−1H l−1v ) ) where cl−1 is a learnable parameter to approximate the importance of node v towards u (attl−1v−→u) based on their interaction in the latent space in l layerwise manner, W is the layer-wise projection matrix, and ⊕ denotes concatenation."
    }, {
      "heading" : "C Dataset Statistics",
      "text" : "Table 1 shows the basic statistics of the datasets that we consider in this study. The taxonomy networks are extracted from Assamese, Bengali, and Hindi WordNets, respectively. The Assamese WordNet dataset consists of 8466 of noun concepts and 8463 edges. The network has the maximum in-degree path of 1, implying that each concept node has only one parent node or predecessor. The network consists of 7072 leaf nodes. Bengali WordNet dataset contains 26007 of noun concepts and 25815 edges and has the maximum in-degree path of 1. The network consists of 7072 leaf nodes. Hindi WordNet dataset consists of 28242 of noun concepts and 28016 edges. It contains 24737 leaf nodes."
    }, {
      "heading" : "D Ranking and Classification Metrics",
      "text" : "We use an array of performance metrics from the domain of classification and ranking to evaluate the competing methods’ performances. Among the ranking metrics, we use Mean Rank (MR), Hit@k (k=1, 3), and Mean Reciprocal Rank (MRR) to judge how well a competing method performs in producing a ranked list of candidate anchors given a test query and a taxonomy expansion operation to carry-out – merge or attach.\n• Mean Rank: It calculates the average rank of true anchors among all the candidate anchors with respect to the matching scores, given a query.\n• Hit@k: It calculates the number of times a true anchor appears in the top k positions when matched with a test query.\n• Mean Reciprocal Rank(MRR): The Mean Reciprocal Rank is used to assess the ranking quality of the true anchor. The reciprocal rank can be computed by finding and inversing the\nrank of a true anchor in the predicted anchors’ list of each query. MRR is averaged over all queries.\nFurther, we use Accuracy, Micro/ Macro F1, Precision, Recall, and F-Scores as classification metrics for deciding given a test query and an initial taxonomy tree, which operation among merge (M), attach (A), and no-operation (N) is to be performed.\n• Accuracy: It summarizes the performance of the classification model as the fraction of the number of true tasks predicted over the total number of ground-truth tasks for a set of queries.\n• Precision: It calculates the fraction of truepositive predicted expansion task classes among the total number of true-positive and false-negative task classes.\n• Recall: It calculates the fraction of truepositive predicted expansion task classes among all the relevant ground-truth task classes.\n• F-Score: The harmonic mean of precision and recall. It is also known as F1-Score.\n• Micro/ Macro F1 : The Macro F1 computes F1-Score for each class (merge M/ attach A) independently but averages the final score by treating each expansion task-class as equally contributing. However, Micro F1 computes the F1-Score for each query sample in the training set and therefore aggregates the contributions of all expansion task classes to compute the final average metric."
    }, {
      "heading" : "E Evaluation Strategy",
      "text" : "We obtain the initial feature vector for train and test concepts using pre-trained subword-aware Fasttext embeddings. For each concept, we generate its definition embedding by averaging the embedding of each word in its textual definition. We employ PyTorch and DGL framework 3 to load and train embeddings. In TEAM, we use a two-layer position-enhanced GAT where the first layer (of size 300) has four attention heads and the second layer (of size 600) has one attention head. We use 50-dimension position embeddings for both layers and apply dropout with the rate of 0.1 on the input\n3https://github.com/dmlc/dgl\nfeature vectors. We use Adam optimizer with an initial learning rate of 0.001."
    }, {
      "heading" : "F Ranking result for out-of-vocabulary words",
      "text" : "In the case of attach expansion, TEAM-RG beats state-of-the-art Taxo-expan by a large margin of (321, 0.31, 0.48, 0.69) in MR, Hit@1, Hit@3, MRR, respectively. However, our proposed frameworks are seen not to perform so well in merge M operation as compared to the attach A operation. Intuitively, this is because, for OOV words, we use a set of manually collected paraphrase definitions of the OOV words to match them with the candidate anchor concepts in the existing taxonomy. Whereas for actually training our model, we have used the same definitions in the replica nodes. That is, we have used the same definition in the original anchor concept and in the input query-concept with mutually exclusive synset information. Thus, in this case-study, the paraphrase-based definition matching deems challenging for our learning model resulting in poorer results for M operation. We believe we can always eliminate this drawback by using a description generation tool (Wang et al., 2021) to generate different definitions of the same concept nodes and train our learning model in a more powerful way."
    } ],
    "references" : [ {
      "title" : "On-line new event detection and tracking",
      "author" : [ "James Allan", "Ron Papka", "Victor Lavrenko." ],
      "venue" : "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 37–45. ACM.",
      "citeRegEx" : "Allan et al\\.,? 1998",
      "shortCiteRegEx" : "Allan et al\\.",
      "year" : 1998
    }, {
      "title" : "Synonym expansion for large shopping taxonomies",
      "author" : [ "Adrian Boteanu", "Adam Kiezun", "Shay Artzi." ],
      "venue" : "Automated Knowledge Base Construction (AKBC).",
      "citeRegEx" : "Boteanu et al\\.,? 2018",
      "shortCiteRegEx" : "Boteanu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical multi-task word embedding learning for synonym prediction",
      "author" : [ "Hongliang Fei", "Shulong Tan", "Ping Li." ],
      "venue" : "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 834–842.",
      "citeRegEx" : "Fei et al\\.,? 2019",
      "shortCiteRegEx" : "Fei et al\\.",
      "year" : 2019
    }, {
      "title" : "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "Michael Gutmann", "Aapo Hyvärinen." ],
      "venue" : "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297–304.",
      "citeRegEx" : "Gutmann and Hyvärinen.,? 2010",
      "shortCiteRegEx" : "Gutmann and Hyvärinen.",
      "year" : 2010
    }, {
      "title" : "Reserating the awesometastic: An automatic extension of the wordnet taxonomy for novel terms",
      "author" : [ "David Jurgens", "Mohammad Taher Pilehvar." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Jurgens and Pilehvar.,? 2015",
      "shortCiteRegEx" : "Jurgens and Pilehvar.",
      "year" : 2015
    }, {
      "title" : "Semeval-2016 task 14: Semantic taxonomy enrichment",
      "author" : [ "David Jurgens", "Mohammad Taher Pilehvar." ],
      "venue" : "Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016), pages 1092–1102.",
      "citeRegEx" : "Jurgens and Pilehvar.,? 2016",
      "shortCiteRegEx" : "Jurgens and Pilehvar.",
      "year" : 2016
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "Temp: Taxonomy expansion with dynamic margin loss through taxonomy-paths",
      "author" : [ "Zichen Liu", "Hongyuan Xu", "Yanlong Wen", "Ning Jiang", "HaiYing Wu", "Xiaojie Yuan." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "WordNet: An electronic lexical database",
      "author" : [ "George A Miller." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Miller.,? 1998",
      "shortCiteRegEx" : "Miller.",
      "year" : 1998
    }, {
      "title" : "Patty: A taxonomy of relational patterns with semantic types",
      "author" : [ "Ndapandula Nakashole", "Gerhard Weikum", "Fabian Suchanek." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural",
      "citeRegEx" : "Nakashole et al\\.,? 2012",
      "shortCiteRegEx" : "Nakashole et al\\.",
      "year" : 2012
    }, {
      "title" : "Distinguishing antonyms and synonyms in a pattern-based neural network",
      "author" : [ "Kim Anh Nguyen", "Sabine Schulte im Walde", "Ngoc Thang Vu." ],
      "venue" : "arXiv preprint arXiv:1701.02962.",
      "citeRegEx" : "Nguyen et al\\.,? 2017",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2017
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "Foundations and Trends® in Information Retrieval,",
      "citeRegEx" : "Pang and Lee,? \\Q2008\\E",
      "shortCiteRegEx" : "Pang and Lee",
      "year" : 2008
    }, {
      "title" : "Large-scale taxonomy mapping for restructuring and integrating wikipedia",
      "author" : [ "Simone Paolo Ponzetto", "Roberto Navigli." ],
      "venue" : "IJCAI, volume 9, pages 2083–2088.",
      "citeRegEx" : "Ponzetto and Navigli.,? 2009",
      "shortCiteRegEx" : "Ponzetto and Navigli.",
      "year" : 2009
    }, {
      "title" : "Building a biowordnet using wordnet data structures and wordnet’s software infrastructure–a failure story",
      "author" : [ "Michael Poprat", "Elena Beisswanger", "Udo Hahn." ],
      "venue" : "Software engineering, testing, and quality assurance for natural language processing,",
      "citeRegEx" : "Poprat et al\\.,? 2008",
      "shortCiteRegEx" : "Poprat et al\\.",
      "year" : 2008
    }, {
      "title" : "Automatic synonym discovery with knowledge bases",
      "author" : [ "Meng Qu", "Xiang Ren", "Jiawei Han." ],
      "venue" : "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 997–1005.",
      "citeRegEx" : "Qu et al\\.,? 2017",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic assignment of wikipedia encyclopedic entries to wordnet synsets",
      "author" : [ "Maria Ruiz-Casado", "Enrique Alfonseca", "Pablo Castells." ],
      "venue" : "International Atlantic Web Intelligence Conference, pages 380–386. Springer.",
      "citeRegEx" : "Ruiz.Casado et al\\.,? 2005",
      "shortCiteRegEx" : "Ruiz.Casado et al\\.",
      "year" : 2005
    }, {
      "title" : "Msejrku at semeval-2016 task 14: Taxonomy enrichment by evidence ranking",
      "author" : [ "Michael Schlichtkrull", "Héctor Martínez Alonso." ],
      "venue" : "Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016), pages 1337–1341.",
      "citeRegEx" : "Schlichtkrull and Alonso.,? 2016",
      "shortCiteRegEx" : "Schlichtkrull and Alonso.",
      "year" : 2016
    }, {
      "title" : "Taxoexpan: self-supervised taxonomy expansion with position-enhanced graph neural network",
      "author" : [ "Jiaming Shen", "Zhihong Shen", "Chenyan Xiong", "Chi Wang", "Kuansan Wang", "Jiawei Han." ],
      "venue" : "Proceedings of The Web Conference 2020, pages 486–497.",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "Hiexpan: Task-guided taxonomy construction by hierarchical tree expansion",
      "author" : [ "Jiaming Shen", "Zeqiu Wu", "Dongming Lei", "Chao Zhang", "Xiang Ren", "Michelle T Vanni", "Brian M Sadler", "Jiawei Han." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Modern information retrieval: A brief overview",
      "author" : [ "Amit Singhal" ],
      "venue" : "IEEE Data Eng. Bull., 24(4):35–",
      "citeRegEx" : "Singhal,? 2001",
      "shortCiteRegEx" : "Singhal",
      "year" : 2001
    }, {
      "title" : "Semantic taxonomy induction from heterogenous evidence",
      "author" : [ "Rion Snow", "Dan Jurafsky", "Andrew Y Ng." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Snow et al\\.,? 2006",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2006
    }, {
      "title" : "Low-resource taxonomy enrichment with pretrained language models",
      "author" : [ "Kunihiro Takeoka", "Kosuke Akimoto", "Masafumi Oyamada." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2747–2758.",
      "citeRegEx" : "Takeoka et al\\.,? 2021",
      "shortCiteRegEx" : "Takeoka et al\\.",
      "year" : 2021
    }, {
      "title" : "Named entity wordnet",
      "author" : [ "Antonio Toral", "Monica Monachini." ],
      "venue" : "In Proceedings of the 6th International Conference on Language Resources and Evaluation. Citeseer.",
      "citeRegEx" : "Toral and Monachini.,? 2008",
      "shortCiteRegEx" : "Toral and Monachini.",
      "year" : 2008
    }, {
      "title" : "Enriching taxonomies with functional domain knowledge",
      "author" : [ "Nikhita Vedula", "Patrick K Nicholson", "Deepak Ajwani", "Sourav Dutta", "Alessandra Sala", "Srinivasan Parthasarathy." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & De-",
      "citeRegEx" : "Vedula et al\\.,? 2018",
      "shortCiteRegEx" : "Vedula et al\\.",
      "year" : 2018
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Lio", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1710.10903.",
      "citeRegEx" : "Veličković et al\\.,? 2017",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2017
    }, {
      "title" : "An efficient sliding window approach for approximate entity extraction with synonyms",
      "author" : [ "Jin Wang", "Chunbin Lin", "Mingda Li", "Carlo Zaniolo." ],
      "venue" : "EDBT, pages 109–120.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Enquire one’s parent and child before decision: Fully exploit hierarchical structure for self-supervised taxonomy expansion",
      "author" : [ "Suyuchen Wang", "Ruihui Zhao", "Xi Chen", "Yefeng Zheng", "Bang Liu." ],
      "venue" : "Proceedings of the Web Conference 2021, pages 3291–3304.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Extending wordnet with hypernyms and siblings acquired from wikipedia",
      "author" : [ "Ichiro Yamada", "Jong-Hoon Oh", "Chikara Hashimoto", "Kentaro Torisawa", "Stijn De Saeger", "Takuya Kawada" ],
      "venue" : "In Proceedings of 5th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Yamada et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2011
    }, {
      "title" : "Synet: Synonym expansion using transitivity",
      "author" : [ "Jiale Yu", "Yongliang Shen", "Xinyin Ma", "Chenghao Jia", "Chen Chen", "Weiming Lu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 1961–",
      "citeRegEx" : "Yu et al\\.,? 2020a",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Steam: Selfsupervised taxonomy expansion with mini-paths",
      "author" : [ "Yue Yu", "Yinghao Li", "Jiaming Shen", "Hao Feng", "Jimeng Sun", "Chao Zhang." ],
      "venue" : "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
      "citeRegEx" : "Yu et al\\.,? 2020b",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Taxonomy completion via triplet matching network",
      "author" : [ "Jieyu Zhang", "Xiangchen Song", "Ying Zeng", "Jiaming Shen", "Yuning Mao", "Lei Li" ],
      "venue" : "arXiv preprint arXiv:2101.01896",
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Ponzetto and Navigli (2009), and Yamada et al. (2011) exploit structured information in Wikipedia to expand WordNet with new synsets",
      "author" : [ "Ruiz-Casado" ],
      "venue" : "Toral and Monachini",
      "citeRegEx" : "Ruiz.Casado,? \\Q2005\\E",
      "shortCiteRegEx" : "Ruiz.Casado",
      "year" : 2005
    }, {
      "title" : "2017) proposed an approach that integrates both the categories. Boteanu et al. (2018) focus on the problem of expanding taxonomies with synonyms for applications",
      "author" : [ "synonyms. Qu" ],
      "venue" : null,
      "citeRegEx" : "Qu,? \\Q2018\\E",
      "shortCiteRegEx" : "Qu",
      "year" : 2018
    }, {
      "title" : "2020b) resorted to classification along mini paths in the taxonomy",
      "author" : [ "Yu" ],
      "venue" : "In contrast,",
      "citeRegEx" : "Yu,? \\Q2020\\E",
      "shortCiteRegEx" : "Yu",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Though both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al.",
      "startOffset" : 189,
      "endOffset" : 339
    }, {
      "referenceID" : 24,
      "context" : "Though both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al.",
      "startOffset" : 189,
      "endOffset" : 339
    }, {
      "referenceID" : 18,
      "context" : "Though both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al.",
      "startOffset" : 189,
      "endOffset" : 339
    }, {
      "referenceID" : 30,
      "context" : "Though both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al.",
      "startOffset" : 189,
      "endOffset" : 339
    }, {
      "referenceID" : 31,
      "context" : "Though both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al.",
      "startOffset" : 189,
      "endOffset" : 339
    }, {
      "referenceID" : 22,
      "context" : "Though both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al.",
      "startOffset" : 189,
      "endOffset" : 339
    }, {
      "referenceID" : 7,
      "context" : "Though both of these operations are integral parts of a WordNet taxonomy expansion, all of the existing studies on taxonomy expansion have considered expansion with either attach operation (Schlichtkrull and Alonso, 2016; Vedula et al., 2018; Shen et al., 2020; Yu et al., 2020b; Zhang et al., 2021; Takeoka et al., 2021; Liu et al., 2021) or merge operation (Nakashole et al.",
      "startOffset" : 189,
      "endOffset" : 339
    }, {
      "referenceID" : 9,
      "context" : ", 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together.",
      "startOffset" : 27,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : ", 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together.",
      "startOffset" : 27,
      "endOffset" : 172
    }, {
      "referenceID" : 9,
      "context" : ", 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together.",
      "startOffset" : 27,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : ", 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together.",
      "startOffset" : 27,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : ", 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together.",
      "startOffset" : 27,
      "endOffset" : 172
    }, {
      "referenceID" : 26,
      "context" : ", 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together.",
      "startOffset" : 27,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : ", 2021) or merge operation (Nakashole et al., 2012; Nguyen et al., 2017; Nakashole et al., 2012; Qu et al., 2017; Boteanu et al., 2018; Wang et al., 2019; Fei et al., 2019), but not together.",
      "startOffset" : 27,
      "endOffset" : 172
    }, {
      "referenceID" : 27,
      "context" : "A similar approach has also been used in (Wang et al., 2021; Yu et al., 2020b; Zhang et al., 2021; Shen et al., 2020) studies.",
      "startOffset" : 41,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : "A similar approach has also been used in (Wang et al., 2021; Yu et al., 2020b; Zhang et al., 2021; Shen et al., 2020) studies.",
      "startOffset" : 41,
      "endOffset" : 117
    }, {
      "referenceID" : 31,
      "context" : "A similar approach has also been used in (Wang et al., 2021; Yu et al., 2020b; Zhang et al., 2021; Shen et al., 2020) studies.",
      "startOffset" : 41,
      "endOffset" : 117
    }, {
      "referenceID" : 18,
      "context" : "A similar approach has also been used in (Wang et al., 2021; Yu et al., 2020b; Zhang et al., 2021; Shen et al., 2020) studies.",
      "startOffset" : 41,
      "endOffset" : 117
    }, {
      "referenceID" : 18,
      "context" : "As the objective is to incorporate the positional information of the parent and children node in the ego tree, we use the Graph Attention Network (GAT) proposed in Taxo-Expan (Shen et al., 2020).",
      "startOffset" : 175,
      "endOffset" : 194
    }, {
      "referenceID" : 6,
      "context" : "This GAT is a special type of graph neural network (GNN) (Kipf and Welling, 2016) with a",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Given the embedding vectors of anchor ā and query q̄ as learned above, we first estimate similarity between the two using a bi-linear model proposed in (Gutmann and Hyvärinen, 2010).",
      "startOffset" : 152,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "Now, considering the query concept q and its associated N + 1 anchor concepts, we estimate the probability of being the correct anchor using InfoNCE loss proposed in (Oord et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "We follow Taxo-Expan’s (Shen et al., 2020) evaluation strategy for inferring the best candidate anchor a given a query q.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "We choose two most recent benchmark SOTA taxonomy-expansion frameworks TaxoExpan (Shen et al., 2020) and Triplet Matching Network(TMN) (Zhang et al.",
      "startOffset" : 81,
      "endOffset" : 100
    }, {
      "referenceID" : 31,
      "context" : ", 2020) and Triplet Matching Network(TMN) (Zhang et al., 2021) as the competing methods.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "Given a seed taxonomy, the distributional approach discovers synonyms by representing strings with their distributional feature and learning a classifier to predict the relation between strings [(Nakashole et al., 2012), (Wang et al.",
      "startOffset" : 195,
      "endOffset" : 219
    }, {
      "referenceID" : 19,
      "context" : "Recently numerous methods have been proposed to solve this problem(Shen et al., 2018), (Shen et al.",
      "startOffset" : 66,
      "endOffset" : 85
    } ],
    "year" : 0,
    "abstractText" : "Taxonomy expansion is a crucial task. Most of the taxonomy expansion approaches are of two types, attach and merge. In a taxonomy like WordNet, both merge and attach are integral parts of the expansion operations, but the majority of studies consider them separately. This paper proposes a novel multi-task learning-based deep learning method known as Taxonomy Expansion with Attach and Merge (TEAM) that performs both the merge and attach operations. This is the first study that integrates both the merge and attach operations in a single model to the best of our knowledge. The proposed models have been evaluated on three separate WordNet taxonomies, viz., Assamese, Bangla, and Hindi. From the various experimental setups, it is shown that TEAM outperforms its state-of-the-art counterparts for attach operation and also provides highly encouraging performance for the merge operation.",
    "creator" : null
  }
}
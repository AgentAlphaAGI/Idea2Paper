{
  "name" : "ARR_2022_240_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, the emergence of Transformer-based language models (using pretrain-and-finetune paradigm) such as BERT (Devlin et al., 2018) and GPT-3 (Brown et al., 2020) have revolutionized and established state-of-the-art (SOTA) records (beyond human-level) on various natural language (NLP) processing tasks. These models are first pre-trained in a self-supervised fashion on a large corpus and fine-tuned for specific downstream tasks (Wang et al., 2018). While effective and prevalent, they suffer from redundant computation due to the heavy model size, which hinders their popularity on resource-constrained devices, e.g., mobile phones, smart cameras, and autonomous driving (Li et al., 2021; Choi and Baek, 2020).\nVarious weight pruning approaches (zeroing out certain weights and then optimizing the rest) have been proposed to reduce the footprint requirements of Transformers (Zhu and Gupta, 2018; Blalock et al., 2020; Gordon et al., 2020; Xu et al., 2021; Li et al., 2021). Conventional wisdom in pruning states that pruning reduces the overfitting risk\nDense model\nD\nSparse model\nPruning\nD Task knowledge\nDiscarded knowledge\n(a) Pruning under non-pretrain-and-finetune paradigm (e.g., CNN, LSTM, GNN)\nsince the compressed model structures have fewer parameters and are believed to be less prone to overfit (Gerum et al., 2020). However, under the pretrain-and-finetune paradigm, most pruning methods understate the overfitting problem.\nIn this paper, we postulate a counter-traditional hypothesis, that is: model pruning increases the risk of overfitting if pruning is performed at the fine-tuning phase. As shown in Figure 1b, the pretrain-and-finetune paradigm contains two types of knowledge, the general-purpose language knowledge learned during pre-training (L) and the taskspecific knowledge from the downstream task data (D). Compared to conventional pruning that only discards task-specific knowledge (Figure 1a), pruning under pretrain-and-finetune (Figure 1b) discards extra knowledge (red area) learned in pretraining phase. Thus, to recover both the extra discarded general-purpose knowledge and the discarded task-specific knowledge, it increases the amount of information a model needs, which results in relative data deficiency, leading to a higher risk of overfitting. To empirically verify the overfitting problem, we visualize the training and evaluation performance on a real-world task data of\nMRPC (Devlin et al., 2018) in Figure 2. From Figure 2 (b), it is observed that the evaluation accuracy on the training dataset remains improved while it keeps the same for the validation set through the training process. From Figure 2 (c), the difference in performance becomes more significant when the pruning rate becomes higher and the performance on the validation set even becomes worse after 2,000 training steps. All these observations verify our hypothesis.\nThe main question this paper attempts to answer is: how to reduce the risk of overfitting of pre-trained language models caused by pruning? However, answering this question is challenging. First, under the pretrain-and-finetune paradigm, both the general-purpose language knowledges and the task-specific knowledge are learned. It is nontrivial to keep the model parameters related to both knowledges when pruning. Second, the amount of data for downstream tasks can be small, such as the data with privacy. Thus, the overfitting problem can easily arise, especially in the face of high pruning rate requirements. A little recent progress has been made on addressing overfitting associated with model compression. However, their results are not remarkable and most of them focus on the vision domain (Bai et al., 2020; Shen et al., 2021).\nTo address these challenges, we propose SPD, a sparse progressive distillation method, for pruning pre-trained language models. We prune and optimize the weight duplicates of the backbone of the teacher model (a.k.a., student modules). Each student module shares the same architecture (e.g., the number of weights, the dimension of each weight) as the duplicate. We replace the corresponding layer(s) of the duplicated teacher model with the pruned sparse student module(s) in a progressive way and name the new model as a grafted model. We validate our proposed method through the ab-\nlation studies and the GLUE benchmark. Experimental results show that our method outperforms the leading competitors.\nWe summarize our contributions as follows: • We postulate, analyze, and empirically verify\na counter-traditional hypothesis: pruning increases the risk of overfitting under the pretrainand-finetune paradigm.\n• We propose a sparse progressive pruning method and show for the first time that reducing the risk of overfitting can help the effectiveness of pruning.\n• Moreover, we theoretically prove that our pruning method can obtain a sub-network from the student model that has similar accuracy as the teacher, and the accuracy gap is bounded.\n• Last but not least, we study and minimize the interference between different hyperparameter strategies, including pruning rate, learning rate, and grafting probability, to further improve performance."
    }, {
      "heading" : "2 Related Work",
      "text" : "To summarize, our contribution is determining the overfitting problem of pruning under the pretrainand-finetune paradigm and proposing the sparse progressive distillation method to address it. We demonstrate the benefits of the proposed framework through the ablation studies. We validate our method on six datasets from the GLUE benchmark. To test if our method is applicable across tasks, we include the tasks of both single sentence and sentence-pair classification. Experimental results show that our method outperforms the leading competitors by a large margin. Network Pruning. Common wisdom has shown that weight parameters of deep learning models can\nbe reduced without sacrificing accuracy loss, such as magnitude-based pruning (Han et al., 2015) and ottery ticket hypothesis (Frankle and Carbin, 2018). (Zhu and Gupta, 2018) compared small-dense models and large-sparse models with the same parameters and showed that the latter outperforms the former, showing the large-sparse models have better expressive power than their small-dense counterparts. However, under the pretrain-and-finetune paradigm, pruning leads to overfitting as discussed. Knowledge Distillation (KD). As a common method in reducing the number of parameters, the main idea of KD is that the small student model mimics the behaviour of the large teacher model and achieves a comparable performance (Hinton et al., 2015; Mirzadeh et al., 2020). (Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020) utilized KD to learn universal language representations from large corpus. However, current SOTA knowledge distillation methods are not able to achieve a high model compression rate (less than 10% remaining weights) while achieving an insignificant performance decrease. Progressive Learning. The key idea of progressive learning is that student learns to update module by module with the teacher. (Shen et al., 2021) utilized a dual-stage distillation scheme where student modules are progressively grafted onto the teacher network, it targets the few-shot scenario and uses only a few unlabeled samples to achieve comparable results on CIFAR-10 and CIFAR-100. (Xu et al., 2020) gradually increased the probability of replacing each teacher module with their corresponding student module and trained the student to reproduce the behavior of the teacher. However, the performance on Transformer-based models of\nthe aforementioned first method is unknown while the second method has an obvious performance drop with a low sparsity (50%)."
    }, {
      "heading" : "3 Methodology",
      "text" : "We propose to use a new knowledge distillation framework that utilizes error-bound provable pruning and progressive module grafting."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "The teacher model and the grafted model (shown in Figure 3) are denoted as fS and fG, respectively. Both models have N + 1 layers (i.e., the first N layers are encoder layers, and the (N + 1)-th layer is the output layer). Denote fTi (·), fGi (·) as the behaviour function induced from the i-th encoder of the teacher model, and the grafted model, respectively. As shown in Figure 4, we utilize layerwise knowledge distillation (KD), where we aim to bridge the gap between fTi (·) and fGi (·).\nThe grafted model is trained to mimic the behavior of the teacher model. During training, we minimize the summation loss L:\nL = ∑ x∈X N+1∑ i=1 λiLKD(fTi (x)fGi (x)), (1)\nwhere X denotes the training dataset, λi is coefficient of i-th layer loss, LD is the distillation loss of the layer pair, xi is the input of the i-th layer.\nDuring KD, each student module mimics the behavior of the corresponding teacher layer. Similar to (Jiao et al., 2020), we take the advantage of abundant knowledge in self-attention distribution, hidden states of each Transformer layer, and\nthe final output layer’s soft logits of teacher model to help train the student model. Specifically, we design the KD loss as follows\nLKD = { Lhidn + Lattn 1 ≤ i ≤ N Lpred i = N + 1\n(2)\nwhere Lhidn = MSE(HTi , HSi ) (1 ≤ i ≤ N ) indicates the difference between hidden states, Lattn = MSE(ATi , A S i ) indicates the difference between attention matrices. MSE(·) is the mean square error loss function and i is the index of Transformer layer. Lpred = -softmax(zT ) · log _softmax(zS / temp) indicates the difference of soft cross-entropy loss, where zT and zS are the soft logits of teacher and student model, respectively. T is the temperature hyper-parameter.\nWe further reduce the number of non-zero parameters in the weight matrix while maintaining accuracy. We denote {Wj}j=ij=1 as the collection of weights in the first i layers, θj as the sparsity of the j-th layer. Then, the loss function of sparse knowledge distillation becomes\nL = ∑ x∈X N+1∑ i=1 λiLKD(fTi (x, {Wj}j=ij=1), f G i (x, {Wj}j=ij=1))\ns.t. sparsity(Wj) ≤ θj for j = 1, ..., N (3)\nAfter training, we find the sparse weight matrix W ∗j using\nW∗j = ΠSj (Wj) for j = 1, ..., N, (4)\nwhere ΠSj (·) denotes the Euclidean projection onto the set Sj = {Wj | sparsity(Wj) ≤ θj}."
    }, {
      "heading" : "3.2 The Proposed",
      "text" : ""
    }, {
      "heading" : "3.2.1 Error-bound Provable Pruning",
      "text" : "Our pruning method is similar to finding matching subnetworks using the lottery ticket hypothesis methodology. Analysis on Feed-forward linear Network. Consider a linear network f(x) = w · x , and g(x) = ( ∑n\ni=1 wi)x. Lueker et al.(Lueker, 1998) shows that existing a subset of wi, such that the corresponnding value of g(x) is very close to f(x). Corollary: When w∗1, ...,w∗n belongs to i.i.d. uniform distribution over [-1,1], where n ≥ C log 2δ , δ ≤ min{1, ϵ}. Then, with probability at least 1-δ, we have\n∃Gspd ⊂ {1, 2, ..., n}, ∀ W ∈ [−0.5, 0.5],\ns.t ∣∣∣∣∣∣w − ∑\ni∈Gspd\nw∗i ∣∣∣∣∣∣ ≤ ϵ (5) Analysis on self-attention layer. The selfattention layer can be present as:\nZ = attention(Q, K, V) = softmax(Q ·K T\n√ dk ) · V. (6)\nConsider a model f(x) with only one self-attention layer, when the token size of input x is 1, softmax(Q·K\nT √ dk\n) = 1, we have Z = V, where V = WVx.\nConsider fG(x) = (∑d\ni=1 w G i\n) x and a pruning\nsparsity θ, base on Corollary, when d ≥ C log 4/ϵ, there exists a pattern of wGi , such that, with probability 1− ϵ,\n∀ w ∈ [−1, 1], ∃θi ∈ {0, 1},\ns.t. ∣∣∣∣∣∣w − ( ∑\ni∈[1,d]\nwGi I(θi)) ∣∣∣∣∣∣ < ϵ (7) In general, let the token size x be n. so x = (x1, x2, ..., xn). Consider a teacher model fT (x) with a self-attention layer, then\nfT (xi) = softmax( Q ·KT√\n(dk) ) · Vi = (\n∑ j e\ncij∑ i ∑ j(e cij ) ) · Vi\n= (\n∑ j e\ncij∑ i ∑ j(e cij ) )WVixi\n= Wci.xi (8) Base on Corollary, when d ≥ C log 4/ϵ, there exists a pattern of WGi , such that, with probability 1− ϵ,\n∀Wci. ∈ [−1, 1],∃θk ∈ {0, 1}, s.t. |Wci. − ( ∑\nk∈[1,d]\nWGk I(θk)) < ϵ (9)\nIn summary:\n∀i ∈ {1, 2, ..., n}, ∣∣fT (xi)− fG(xi)∣∣ < ϵ (10)\nProgressive Module Grafting. To avoid overfitting in the training process for the sparse Transformer model, we further graft student modules (scion) onto the teacher model duplicates (rootstock). For the i-th student module, we use an independent Bernoulli random variable ri to indicate where it will be grafted on the rootstock. To be more specific, ri has a probability of p (grafting probability) to be set as 1 (i.e., student module substitutes the corresponding teacher layer). Otherwise, the latter will keep unchanged. Once the target pruning rate is achieved, we apply linear increasing probability to graft student modules which enable the student modules to orchestrate with each other.\nDifferent from the model compression methods that update all model parameters at once, such as TinyBERT (Jiao et al., 2020) and DistilBERT (Sanh et al., 2019), SPD only updates the student modules on the grafted model. It reduces the complexity of network optimization, which mitigates the overfitting problem and enables the student modules to learn deeper knowledge from the teacher model. The overview is described in Algorithm 1. We will further demonstrate the effectiveness of progressive student module grafting in 4.2. Algorithm 1 Sparse Progressive Distillation\nInput: Teacher model fT (fine-tuned BERTBASE); grafted model fG: duplicates of teacher model. Set t1, t2, t3 as the final number of training steps of pruning, progressive module grafting, and finetuning, respectively. Set p as the grafting probability Output: Student model p← p0 for t = 0 to t3 do\nif 0 ≤ t < t1 then Prune student modules and generate mask M Graft student modules with p0 end if if t1 ≤ t < t2 then\nGraft student modules with p← k(t− t1) + p0 end if Calculate distillation loss L in Eqn. (3) For fG, update sparse weights w′ ← w ·M Duplicate sparse weight(s) on fG to corresponding student module(s)\nend for return fG"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "Datasets. We evaluate SPD on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) and report the metrics i.e., accuracy scores for SST-2, QNLI, RTE, and WNLI, Matthews Correlation Coefficient (MCC) for CoLA, F1 scores for QQP and MRPC, Spearman correlations for STS-B.\nBaselines. We first use 50% sparsity (a widely adopted sparsity ratio among SOTA), and compare SPD against two types of baselines – nonprogressive and progressive. For the former, we select BERT-PKD (Sun et al., 2019), DistilBERT (Sanh et al., 2019), MiniLM (Wang et al., 2020), TinyBERT (Jiao et al., 2020), SparseBERT (Xu et al., 2021) and E.T. (Chen et al., 2021), while for the latter, we choose Theseus (Xu et al., 2020). We further compare SPD against other existing works under higher sparsity, e.g., TinyBERT (Jiao et al., 2020), SparseBERT (Xu et al., 2021) and RPP (Guo et al., 2019).\nSPD Settings. We use official BERTBASE, uncased model as the pre-train model and the fine-tuned pre-train model as our teacher. Both BERTBASE and teacher model have the same architecture (i.e., 12 encoder layers (L = 12; embedding dimension dmodel = 768; self-attention heads H = 12)). We finetune BERTBASE using 32 as batch size, 128 as max sequence length, best performance from {2e−5, 3e−5, 4e−5, 5e−5} as the learning rate. For SPD model training, the number of pruning epochs, linear increasing module grafting epochs, finetuning epochs vary from [10, 30], [5, 20], [5, 10], respectively. For pruning, we use AdamW (Loshchilov and Hutter, 2018) as the optimizer and run the experiments with an initial grafting probability from {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. The probability with the best performance will be adopted. After pruning, we adjust the slope of the grafting probability curve so that the grafting probability equals 1 at the end of module grafting. For module grafting and finetuning, an AdamW optimizer is used with learning rate chosen from {3e−5, 1e−4, 3.2e−4, 5e−4, 6.4e−4}. The model training and evaluation are performed with Python 3.6.8, torch 1.8.0, and CUDA 11.1 on Quadro RTX6000 GPU226and Intel(R) Xeon(R) Gold 6244 @ 3.60GHz CPU."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "Accuracy vs. Sparsity. We do experiments on eight GLUE benchmark tasks (Table 1). For non-progressive baselines, SPD exceeds all of them on QNLI, SST-2, CoLA, STS-B, and MRPC. For RTE, TinyBERT6 has a 1.6% higher accuracy than SPD. However, TinyBERT6 used augmented data while SPD does not use data augmentation to generate the results in Table 1. On average, SPD has 6.3%, 5.6%, 1.2%, 1.7%, 3.7% improvement in performance than BERT6-PKD, DistilBERT, TinyBERT6, SparseBERT, E.T. respectively. Furthermore, on CoLA, SPA achieves up to 25.9% higher performance compared to all nonprogressive baselines. For the progressive baseline, we compare SPD with BERT-of-Theseus. Experimental results show that SPD exceeds the latter on all tasks. SPD has a 3.9% increase on average. Among all the tasks, CoLA and RTE have 20.2% and 5.9% gain respectively. For the compar-\nison with sparse and non-progressive baseline, SPD has an improvement of 16.8%, 5.5%, 3.2%, 2.7%, 2.0%, 1.9%, 1.6%, 1.6% on CoLA, RTE, MNLI, QNLI, QQP, MRPC, STS-B, SST-2, respectively.\nOn all listed tasks, SPD even outperforms the teacher model except for RTE. On RTE, SPD retains exactly the full accuracy of the teacher model. On average, the proposed SPD achieves a 1.1% higher accuracy/score than the teacher model. We conclude the reason for the outstanding performance from three respects: 1) There is redundancy in the original dense BERT model. Thus, pruning the model with a low pruning rate (e.g., 50%) will not lead to a significant performance drop. 2) SPD decreases the overfitting risk which helps the student model learn better. 3) The interference between different hyperparameter strategies is mitigated, which enables SPD to obtain a better student model.\nWe also compare SPD with other baselines (i.e., 4-layer TinyBERT (Jiao et al., 2020), RPP (Guo et al., 2019), and SparseBERT (Xu et al., 2021) ) under higher pruning rates. Results are summarized in Table 2. For the fairness of comparison, we remove data augmentation from the above methods. We mainly compare the aforementioned baselines with very high sparsity (e.g., 90%, 95%) SPD. For the comparison with TinyBERT4, both SPD (90% sparsity) and SPD (95% sparsity) win. SPD (90% sparsity) has 63.4% and 9% higher evaluation score than TinyBERT4 on CoLA and MRPC, respectively. For the setting of 95% sparsity, SPD outperforms TinyBERT4 with 41.3% and\n7.6% higher performance, respectively. Compared to RPP, both SPD (90% sparsity) and SPD (95% sparsity) show higher performance on MRPC, with 9.8% and 8.3% higher F1 score, respectively. For SparseBERT, SPD exceeds it on all tasks in Table 2. Especially on CoLA, SPD (90% sparsity) and SPD (95% sparsity) have 2.69× and 2.33× higher Mcc score on CoLA, respectively. SparseBERT has competitive performance with SOTA when using data augmentation. The reason for the performance drop for SparseBERT may because its deficiency of ability in mitigating overfitting problems.\nOverfitting Mitigation. We explore the effectiveness of SPD to mitigate the overfitting problem. Depending on whether progressive, grafting, or KD is used, we compare 4 strategies: (a) no progressive, no KD; (b) progressive, no KD; (c) no progressive, KD; (d) progressive, KD (ours). We evaluate these strategies on both training and validation sets of MRPC. The results are summarized in Figure 5. From (a) to (d), the gap between the evaluation results of the training set and the dev set is reduced, which strongly suggests that the strategy adopted by SPD, i.e., progressive + KD, outperforms other strategies in mitigating the overfitting problem. Figure 5 (a), (b), and (c) indicate that compared to progressive only, KD has a bigger impact on mitigating overfitting, as the performance gap between the training set and the dev set decreases more from (a) to (c) than from (a) to (b). From Figure 5 (a), (b) and (c), we also observe that compared to no progressive, no KD, either using progressive (Figure 5 (b)) or KD (Figure 5 (c)) is very obvious to help mitigate the overfitting problem. Figures 5 (b), (c) and (d) indicate that the combination of progressive and KD brings more benefits than only using progressive or KD as Figure 5 (d) has the smallest performance gap between the training set and the dev set. Combined with\nTable 1 and Table 2, Figure 5 shows that SPD mitigates overfitting and leads to higher performance."
    }, {
      "heading" : "4.3 Ablation Studies",
      "text" : "In this section, we justify the three schedulers used in our method (i.e., grafting probability, pruning rate, and learning rate), and study the sensitivity of our method with respect to each of them. Study on Components of SPD. The proposed SPD consists of three components (i.e., sparse, knowledge distillation, and progressive module grafting). We conduct experiments to study the importance of each component on GLUE benchmark tasks with the sparsity of 50% and results are shown in Table 3. Compared to sparse (vanilla pruning), both KD and progressive achieve gains on performance among all tasks. The combination of all components has the highest average score, which is 0.96% higher than vanilla pruning. Effects of Grafting Probability Strategy. In our method, we set the grafting probability greater than 0 during pruning, to allow student modules to learn deeper knowledge from the teacher model. To verify the benefit of this design, we change the grafting probability to zero and compare it with our method. The result on RTE is shown in Figure 6. Pruning with grafting (the red curve) shows better performance than pruning without grafting, which justifies the existence of grafting during pruning. In addition, we study the sensitivity of our method to grafting probability (Figure 7). It is observed that p0 = 0.6 achieves the best performance, and the progressive design is better than the non-progressive. Effects of Pruning Rate Strategy. For the pruning rate scheduler, we compare the strategies with different pruning ending steps. The results are shown in Figure 8. It is observed that the pruning during when grafting probability p = p0 has a higher F1 score than other strategies on MRPC. Effects of Optimizer Strategy. We also compare\nour strategy with the strategy that only has one learning rate scheduler. The results (Figure 9) indicate that our strategy (i.e., two independent optimizers) is better. We also evaluate different learning rates with the pruning rate of 0.9 and the grafting probability of 0.8."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we postulate a counter-traditional hypothesis that pruning increases the risk of over-\nfitting under the pretrain-and-finetune paradigm. We analyze and empirically verify this hypothesis, and propose a sparse progressive pruning method to address the overfitting problem. We theoretically prove that our pruning method can obtain a sub-network from the student model that has a similar accuracy as the teacher and the accuracy gap is bounded. We study and minimize the interference between different hyperparameter strategies, including pruning rate, learning rate, and grafting probability. A number of ablation studies and experimental results on eight tasks from the GLUE benchmark demonstrate the superiority of our method over the leading competitors."
    } ],
    "references" : [ {
      "title" : "Few shot network compression via cross distillation",
      "author" : [ "Haoli Bai", "Jiaxiang Wu", "Irwin King", "Michael Lyu." ],
      "venue" : "AAAI, volume 34, pages 3203–3210.",
      "citeRegEx" : "Bai et al\\.,? 2020",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "What is the state of neural network pruning? arXiv preprint arXiv:2003.03033",
      "author" : [ "Davis Blalock", "Jose Javier Gonzalez Ortiz", "Jonathan Frankle", "John Guttag" ],
      "venue" : null,
      "citeRegEx" : "Blalock et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Blalock et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Et: re-thinking self-attention for transformer models on gpus",
      "author" : [ "Shiyang Chen", "Shaoyi Huang", "Santosh Pandey", "Bingbing Li", "Guang R Gao", "Long Zheng", "Caiwen Ding", "Hang Liu." ],
      "venue" : "Proceedings of the International Conference for High Performance",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Edge camera system using dee p learning method with model compression on embedded applications",
      "author" : [ "Yun Won Choi", "Jang Woon Baek." ],
      "venue" : "2020 IEEE International Conference on Consumer Electronics (ICCE), pages 1–4. IEEE.",
      "citeRegEx" : "Choi and Baek.,? 2020",
      "shortCiteRegEx" : "Choi and Baek.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "author" : [ "Jonathan Frankle", "Michael Carbin." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Frankle and Carbin.,? 2018",
      "shortCiteRegEx" : "Frankle and Carbin.",
      "year" : 2018
    }, {
      "title" : "Sparsity through evolutionary pruning prevents neuronal networks from overfitting",
      "author" : [ "Richard C Gerum", "André Erpenbeck", "Patrick Krauss", "Achim Schilling." ],
      "venue" : "Neural Networks, 128:305–312.",
      "citeRegEx" : "Gerum et al\\.,? 2020",
      "shortCiteRegEx" : "Gerum et al\\.",
      "year" : 2020
    }, {
      "title" : "Compressing bert: Studying the effects of weight pruning on transfer learning",
      "author" : [ "Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews." ],
      "venue" : "arXiv preprint arXiv:2002.08307.",
      "citeRegEx" : "Gordon et al\\.,? 2020",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2020
    }, {
      "title" : "Reweighted proximal pruning for large-scale language representation",
      "author" : [ "Fu-Ming Guo", "Sijia Liu", "Finlay S Mungall", "Xue Lin", "Yanzhi Wang." ],
      "venue" : "arXiv preprint arXiv:1909.12486.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning both weights and connections for efficient neural network",
      "author" : [ "Song Han" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 1135–1143.",
      "citeRegEx" : "Han,? 2015",
      "shortCiteRegEx" : "Han",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Npas: A compiler-aware framework of unified network pruning and architecture search for beyond real-time mobile acceleration",
      "author" : [ "Zhengang Li", "Geng Yuan", "Wei Niu", "Pu Zhao", "Yanyu Li", "Yuxuan Cai", "Xuan Shen", "Zheng Zhan", "Zhenglun Kong", "Qing Jin" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Fixing weight decay regularization in adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter" ],
      "venue" : null,
      "citeRegEx" : "Loshchilov and Hutter.,? \\Q2018\\E",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Exponentially small bounds on the expected optimum of the partition and subset sum problems",
      "author" : [ "George S Lueker." ],
      "venue" : "Random Structures & Algorithms, 12(1):51–62.",
      "citeRegEx" : "Lueker.,? 1998",
      "shortCiteRegEx" : "Lueker.",
      "year" : 1998
    }, {
      "title" : "Improved knowledge distillation via teacher assistant",
      "author" : [ "Seyed Iman Mirzadeh", "Mehrdad Farajtabar", "Ang Li", "Nir Levine", "Akihiro Matsukawa", "Hassan Ghasemzadeh." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,",
      "citeRegEx" : "Mirzadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Mirzadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Progressive network grafting for few-shot knowledge distillation",
      "author" : [ "Chengchao Shen", "Xinchao Wang", "Youtan Yin", "Jie Song", "Sihui Luo", "Mingli Song." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 2541–2549.",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Mobilebert: a compact task-agnostic bert for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
      "author" : [ "Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou." ],
      "venue" : "Advances in Neural Information Processing Systems(NIPS).",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT-of-theseus: Compressing BERT by progressive module replacing",
      "author" : [ "Canwen Xu", "Wangchunshu Zhou", "Tao Ge", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking network pruning–under the pre-train and fine-tune paradigm",
      "author" : [ "Dongkuan Xu", "Ian EH Yen", "Jinxi Zhao", "Zhibin Xiao" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "To prune, or not to prune: Exploring the efficacy of pruning for model compression",
      "author" : [ "Michael H Zhu", "Suyog Gupta." ],
      "venue" : "The International Conference on Learning Representations.",
      "citeRegEx" : "Zhu and Gupta.,? 2018",
      "shortCiteRegEx" : "Zhu and Gupta.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Recently, the emergence of Transformer-based language models (using pretrain-and-finetune paradigm) such as BERT (Devlin et al., 2018) and GPT-3 (Brown et al.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : ", 2018) and GPT-3 (Brown et al., 2020) have revolutionized and established state-of-the-art (SOTA) records (beyond human-level) on various natural language (NLP) processing tasks.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "These models are first pre-trained in a self-supervised fashion on a large corpus and fine-tuned for specific downstream tasks (Wang et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : ", mobile phones, smart cameras, and autonomous driving (Li et al., 2021; Choi and Baek, 2020).",
      "startOffset" : 55,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : ", mobile phones, smart cameras, and autonomous driving (Li et al., 2021; Choi and Baek, 2020).",
      "startOffset" : 55,
      "endOffset" : 93
    }, {
      "referenceID" : 25,
      "context" : "Various weight pruning approaches (zeroing out certain weights and then optimizing the rest) have been proposed to reduce the footprint requirements of Transformers (Zhu and Gupta, 2018; Blalock et al., 2020; Gordon et al., 2020; Xu et al., 2021; Li et al., 2021).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 1,
      "context" : "Various weight pruning approaches (zeroing out certain weights and then optimizing the rest) have been proposed to reduce the footprint requirements of Transformers (Zhu and Gupta, 2018; Blalock et al., 2020; Gordon et al., 2020; Xu et al., 2021; Li et al., 2021).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 8,
      "context" : "Various weight pruning approaches (zeroing out certain weights and then optimizing the rest) have been proposed to reduce the footprint requirements of Transformers (Zhu and Gupta, 2018; Blalock et al., 2020; Gordon et al., 2020; Xu et al., 2021; Li et al., 2021).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 24,
      "context" : "Various weight pruning approaches (zeroing out certain weights and then optimizing the rest) have been proposed to reduce the footprint requirements of Transformers (Zhu and Gupta, 2018; Blalock et al., 2020; Gordon et al., 2020; Xu et al., 2021; Li et al., 2021).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 13,
      "context" : "Various weight pruning approaches (zeroing out certain weights and then optimizing the rest) have been proposed to reduce the footprint requirements of Transformers (Zhu and Gupta, 2018; Blalock et al., 2020; Gordon et al., 2020; Xu et al., 2021; Li et al., 2021).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 7,
      "context" : "since the compressed model structures have fewer parameters and are believed to be less prone to overfit (Gerum et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "However, their results are not remarkable and most of them focus on the vision domain (Bai et al., 2020; Shen et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "However, their results are not remarkable and most of them focus on the vision domain (Bai et al., 2020; Shen et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and ottery ticket hypothesis (Frankle and Carbin, 2018).",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : "(Zhu and Gupta, 2018) compared small-dense models and large-sparse models with the same parameters and showed that the latter outperforms the former, showing the large-sparse models have better expressive power than their small-dense counterparts.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "As a common method in reducing the number of parameters, the main idea of KD is that the small student model mimics the behaviour of the large teacher model and achieves a comparable performance (Hinton et al., 2015; Mirzadeh et al., 2020).",
      "startOffset" : 195,
      "endOffset" : 239
    }, {
      "referenceID" : 16,
      "context" : "As a common method in reducing the number of parameters, the main idea of KD is that the small student model mimics the behaviour of the large teacher model and achieves a comparable performance (Hinton et al., 2015; Mirzadeh et al., 2020).",
      "startOffset" : 195,
      "endOffset" : 239
    }, {
      "referenceID" : 17,
      "context" : "(Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020) utilized KD to learn universal language representations from large corpus.",
      "startOffset" : 0,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "(Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020) utilized KD to learn universal language representations from large corpus.",
      "startOffset" : 0,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "(Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020) utilized KD to learn universal language representations from large corpus.",
      "startOffset" : 0,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "(Shen et al., 2021) utilized a dual-stage distillation scheme where student modules are progressively grafted onto the teacher network, it targets the few-shot scenario and uses only a few unlabeled samples to achieve comparable results on CIFAR-10 and CIFAR-100.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "(Xu et al., 2020) gradually increased the probability of replacing each teacher module with their corresponding student module and trained the student to reproduce the behavior of the teacher.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "Similar to (Jiao et al., 2020), we take the advantage of abundant knowledge in self-attention distribution, hidden states of each Transformer layer, and",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "Lueker et al.(Lueker, 1998) shows that existing a subset of wi, such that the corresponnding value of g(x) is very close to f(x).",
      "startOffset" : 13,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "Different from the model compression methods that update all model parameters at once, such as TinyBERT (Jiao et al., 2020) and DistilBERT (Sanh et al.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 17,
      "context" : ", 2020) and DistilBERT (Sanh et al., 2019), SPD only updates the student modules on the grafted model.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 21,
      "context" : "We evaluate SPD on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) and report the metrics i.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "For the former, we select BERT-PKD (Sun et al., 2019), DistilBERT (Sanh et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : ", 2019), DistilBERT (Sanh et al., 2019), MiniLM (Wang et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : ", 2019), MiniLM (Wang et al., 2020), TinyBERT (Jiao et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : ", 2020), TinyBERT (Jiao et al., 2020), SparseBERT (Xu et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "(Chen et al., 2021), while for the latter, we choose Theseus (Xu et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : ", 2021), while for the latter, we choose Theseus (Xu et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : ", 2020), SparseBERT (Xu et al., 2021) and RPP (Guo et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "For pruning, we use AdamW (Loshchilov and Hutter, 2018) as the optimizer and run the experiments with an initial grafting probability from {0.",
      "startOffset" : 26,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "7 non-progressive BERT6-PKD (Sun et al., 2019) 67M 81.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "The results of DistilBERT and TinyBERT6 are taken from (Jiao et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : ", 4-layer TinyBERT (Jiao et al., 2020), RPP (Guo et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : ", 2020), RPP (Guo et al., 2019), and SparseBERT (Xu et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 24,
      "context" : ", 2019), and SparseBERT (Xu et al., 2021) ) under higher pruning rates.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "The results of DistilBERT and TinyBERT6 are taken from (Jiao et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 74
    } ],
    "year" : 0,
    "abstractText" : "Conventional wisdom in pruning Transformerbased language models is that pruning reduces the model expressiveness and thus is more likely to underfit rather than overfit. However, under the trending pretrain-and-finetune paradigm, we postulate a counter-traditional hypothesis, that is: pruning increases the risk of overfitting when performed at the fine-tuning phase. In this paper, we aim to address the overfitting problem and improve pruning performance via progressive knowledge distillation with error-bound properties. We show for the first time that reducing the risk of overfitting can help the effectiveness of pruning under the pretrain-and-finetune paradigm. Ablation studies and experiments on the GLUE benchmark show that our method outperforms the leading competitors across different tasks.",
    "creator" : null
  }
}
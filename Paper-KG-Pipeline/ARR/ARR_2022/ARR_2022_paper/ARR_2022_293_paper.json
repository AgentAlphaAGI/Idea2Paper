{
  "name" : "ARR_2022_293_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Understanding math problems via automated methods is a desired machine capacity for artificial intelligence assisted learning. Such a capacity is the key to the success of a variety of education applications, including math problem retrieval (Reusch et al., 2021), problem recommendation (Liu et al., 2018), and problem solving (Huang et al., 2020).\nTo automatically understand math problems, it is feasible to learn computational representations from problem statement texts with pre-trained language models (PLMs) (Shen et al., 2021; Peng et al., 2021). Pre-trained on the large-scale general corpus, PLMs (Devlin et al., 2018) can be effectively transferred into new domains or tasks\nby continual pre-training on task-specific datasets. Different from traditional text comprehension tasks, as shown in Figure 1, math problems usually involve a complex mixture of mathematical symbols, logic and formulas, which becomes a barrier to the accurate understanding of math problems.\nHowever, previous works (Reusch et al., 2021; Shen et al., 2021) mostly oversimplify the issues of math problem understanding. They directly concatenate the formulas with the textual description as an entire sentence, and then perform continual pre-training and encoding without special considerations. Therefore, two major shortcomings that are likely to affect the understanding of math problems. First, formulas (the most important elements of the problem) contain complex mathematical logic, and modeling them as plain text may incur the loss of valuable information. Second, the textual description contains essential explanations or hints about the symbols and logic within the formulas, hence it is necessary to accurately capture fine-grained correlations between words from description text and symbols from math formulas.\nTo better model the formulas, operator trees have been introduced to represent the math formulas (Zanibbi and Blostein, 2012), which are sub-\nsequently encoded by graph neural network (GNN). Although these methods can improve the comprehension capacity to some extent, there exists a semantic gap between graph encoding and text encoding due to the heterogeneity of formulas and texts. Even with concatenation and self-attention mechanisms (Peng et al., 2021), it is still hard to capture the fine-grained associations among tokens and symbols, e.g., the dependency relation between math symbols and corresponding explanation tokens.\nIn order to better fuse the information from formulas and texts, our solutions are twofold. First, we construct a syntax-aware memory network based on a structure called math syntax graph (Figure 1), which integrates operator trees from formulas and syntax trees from texts. The key point lies in that we store the node embeddings from the GNN and dependency relation embeddings as entries of memory networks, and then design the corresponding read and write mechanism by taking token embeddings from the PLM (for formulas) as queries, which can effectively associate the representation spaces of text and formulas. Second, we devise specific continual pre-training tasks to further enhance and fuse the text and graph data. These tasks not only improve the understanding of math symbols in text and formulas logic in the syntax graph, but also directly align and unify the representations of the text and graph.\nTo this end, we propose COMUS, to continually pre-train language models for math problem understanding with syntax-aware memory network. In our approach, we first encode the textual description and math syntax graph via PLM and GAT, respectively. Then, we add syntax-aware memory networks between the last k layers of PLM and GAT. In each layer, we first conduct the multi-view read and write operation to interact and fuse the token and node representations, respectively, and then consolidate the fused representation by passing the next layers from PLM and GAT. All parameters of our model will be initialized from PLMs and be continually pre-trained by our devised three tasks, namely masked language model, memory triplet completion and text-graph contrastive learning. Experimental results on four tasks in the math domain have demonstrated the effectiveness of our approach, especially when training data is limited.\nOur contributions can be summarized as follows: (1) We construct a novel syntax-aware memory\nnetwork to capture the fine-grained interactions between the text and formulas. (2) We design three continual pre-training tasks to further align and fuse the representations of the text and graph data. (3) Experiments on four tasks in the math domain demonstrate the effectiveness of our model."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Problem Statement. Generally, a math problem consists of a textual description d and several formulas {f1, f2, · · · , fm}. The textual description provides necessary background information for the math problem. It is formally denoted as a sequence of tokens q = {t1, t2, · · · , tl}, where ti is either a word token or a mathematical symbol (e.g., a number or an operator). The formulas describe the relationship among mathematical symbols, which is the key to understand and solve the math problem. Each formula consists of a sequence of mathematical symbols, denoted as fi = {s1, · · · , sn}.\nBased on the above notations, this work focuses on continually pre-training a PLM on unsupervised math problem corpus for domain adaptation. After that, the PLM can be fine-tuned on various tasks from the math domain (e.g., knowledge point prediction), and improve the task performance.\nMath Syntax Graph. The understanding of mathematical text and formulas requires capturing the complex correlations within words, symbols and operators. Inspired by previous works (Mansouri et al., 2019; Peng et al., 2021), we construct a syntax graph, where the textual description is represented as a syntax dependency tree and the formulas are represented as operator trees (OPT).\nSpecifically, given a math problem consisting of a textual description d and several formulas {f1, f2, · · · , fm}, we first utilize the toolkit TangentS1 to convert each formula into an OPT, and Stanza2 to convert the textual description into a syntax dependency tree. Then, we add a special token “[MATH]” to link each OPT with a specific slot in the syntax dependency tree, to construct the syntax graph G of the math problem. Let N and R denote the set of nodes and relations on G, respectively. We further extract dependency triplets from G, where a dependency triplet (h, r, t) denotes there exists an edge with the relation r ∈ R to link the head node h ∈ N to the tail node t ∈ N .\n1https://github.com/BehroozMansouri/TangentCFT 2https://stanfordnlp.github.io/stanza/"
    }, {
      "heading" : "3 Methodology",
      "text" : "As shown in Figure 2, our approach aims to effectively encode textual description and formulas, and fuse the two kinds of information for understanding math problems. In what follows, we first present the base models for encoding math problems, and then introduce our improvements on syntax-aware memory networks and continual pre-training tasks."
    }, {
      "heading" : "3.1 Base Models",
      "text" : "Encoding Math Text. We use BERT (Devlin et al., 2018) as the PLM to encode the math text, i.e., the textual description d. Given d = {t1, t2, · · · , tL} of a math question, the PLM first projects these tokens into corresponding embeddings. Then, a stack of Transformer layers will gradually encode the embeddings to generate the l-th layer representations {h(l)1 ,h (l) 2 , · · · ,h (l) L }. Since the textual description q may contain specific math symbols that were not seen during pre-training, we add them into the vocabulary of the PLM and randomly initialize their token embeddings. Such new embeddings will be learned during continual pre-training.\nEncoding Math Syntax Graph. We incorporate a graph attention network (GAT) (Veličković et al., 2018) to encode the math syntax graph, it is composed of an embedding layer and a stack of graph attention layers. Given a math syntax graph G with N nodes, GAT first maps the nodes into a set of embeddings {n1,n2, · · · ,nN}. Then each graph attention layer aggregates the neighbourhood’s hidden states using multi-head attentions to update the node representations as:\nn (l+1) i =\nK\n∥ k=1 σ( ∑ j∈Ni αkijW (l) k n (l) j ). (1)\nwhere n(l+1)i is the representation of the i-th node in the l + 1 layer, ∥ denotes the concatenation operation, σ denotes the sigmoid function, K is the number of attention heads and Ni is the neighborhoods of node i in the graph, W(l)k is a learnable matrix. αkij is the attention value of node i to its neighbour j in attention head k."
    }, {
      "heading" : "3.2 Syntax-Aware Memory Network",
      "text" : "To improve the semantic interaction and fusion of representations from math text and the syntax graph, we add k syntax-aware memory networks\nbetween the last k layers of PLM and GAT. In the memory network, node embeddings (from the math syntax graph) with dependency relations are considered as slot entries, and we design multi-view read/write operations to allow token embeddings (explanation tokens or hints) to attend to highly related node embeddings (math symbols).\nMemory Initialization. We construct the memory network based on the dependency triplets and representations of the math syntax graph. Given the dependency triplets {(h, r, t)}, we treat the head and relation (h, r) as the key and the tail t as the value, to construct a syntax-aware keyvalue memory. The representations of the heads and tails are the corresponding node representations from GAT, while the relation representations are randomly initialized and will be optimized by continually pre-training. Finally, we concatenate the representations of heads and relations to compose the representation matrix of Keys as K(l) = {[n(l)h1 ; r1], [n (l) h2 ; r2], · · · , [n(l)hN ; rN ]}, and obtain the representation matrix of Values as V(l) = {n(l)t1 ,n (l) t2 , · · · ,n(l)tN }.\nMulti-view Read Operation. We read the useful semantics within the syntax-aware memory to update the token representations from PLM. Since a token can be related to several nodes within the math syntax graph, we design a multi-view read operation to capture these complex semantic associations. Concretely, via different bilinear transformation matrices {WS1 ,WS2 , · · · ,WSn}, we first generate multiple similarity matrices {S1,S2, · · · ,Sn} between tokens and keys (head and relation) within the memory, and then aggregate the values (tail) to update the token representations. Given the token representations from the l-th layer of PLM H(l) = {h(l)1 ,h (l) 2 , · · · ,h (l) L }, the similarity matrix Si is computed as\nSi = H (l)WSi K\n(l)⊤ (2)\nwhere WSi is a learnable matrix, and an entry Si[j, k] denotes the similarity between the j-th token and the k-th key (head and relation) in the i-th view. Based on these similarity matrices, we update the token representations by aggregating the value representations as\nĤ(l) = H(l) + [α1V;α2V; · · · ;αhV]WO (3) αi = softmax(Si) (4)\nwhere WO is a learnable matrix, αi is the attention scores along the key dimension. In this way, the token representation can directly trace into multiple semantic-related values via the keys, so that the complex fine-grained correlations between tokens and nodes can be captured. The updated token representations Ĥ(l) will be fed into the next layer of PLM to consolidate the fused information.\nMulti-View Write Operation. After update the token representations, we also consider to write the memory for updating the representations of nodes from GAT. We also utilize the multi-view similarity matrices {S1,S2, · · · ,Sh}. Concretely, we compute the attention scores β via softmax along the token dimension of the similarity matrices, and then aggregate the token representations as\nV(l)new = [β1H (l);β2H (l); · · · ;βhH(l)]WR (5) βi = softmax(S ⊤ i ) (6)\nwhere WR is a learnable matrix. Based on the aggregated token representations, we incorporate a gate to update the representation of the values as\nz = σ(V(l)newW A +V(l)WB) (7) V̂(l) = z ∗V(l)new + (1− z) ∗V(l) (8)\nwhere WA and WB are learnable matrices. The updated representations V̂(l) will be also fed into the next layer of GAT for consolidation."
    }, {
      "heading" : "3.3 Continual Pre-training",
      "text" : "Continual pre-training aims to further enhance and fuse the math text and math syntax graph. To achieve it, we utilize the masked language model and dependency triplet completion tasks to improve the understanding of math text and math syntax graph, respectively, and the text-graph contrastive learning task to align and fuse their representations.\nMasked Language Model (MLM). Since math text contains a number of special math symbols, we utilize the MLM task to learn it for better understanding the math text. Concretely, we randomly select 15% tokens of the input sequence to be masked. Of the selected tokens, 80% are replaced with a special token [MASK], 10% remain unchanged, and 10% are replaced by a token randomly selected from vocabulary. The objective is to predict the original tokens of the masked ones as:\nLMLM = ∑\nti∈Vmask\n− log p(ti) (9)\nwhere Vmask is the set of masked tokens, and p(ti) denotes the probability of predicting the original token in the position of ti.\nDependency Triplet Completion (DTC). Within the math syntax graph, the relationships with the dependency triplet (h, r, t) are essential to understand the complex math logic of the math problem. Thus, inspired by TransE (Bordes et al., 2013), we devise the dependency triplet completion task to improve the correlations within the triplets. Specifically, for\neach triplet (h, r, t) within the math syntax graph, we minimize the DTC loss by\nLDTC = max(γ+d(nh+r,nt)−d(nh+r ′ ,nt), 0) (10) where γ > 0 is a margin hyper-parameter, and d is the Euclidean Distance. r ′ is the randomly sampled negative relation embedding. In this way, the head and relation embeddings can learn to match the semantics of the tail embedding, which enhances the node and relation representations to capture the graph structural information.\nText-Graph Contrastive Learning (TGCL). Having enhanced the representations of math text and math syntax graph via MLM and DTC respectively, we further consider to align and unify the representation of the two different views of the math problem. We adopt contrastive learning to pull the representations of text and graph of the same math problem together, and push apart negatives. Concretely, given a text-graph pair of a math problem (qi,Gi), we utilize the representation of the [CLS] token hqi as the sentence representation of qi, and the meanpooling of the node representations nGi as the graph representation of Gi. Then, we adopt the cross-entropy contrastive learning objective with in-batch negatives to align the two representations\nLTGCL = − log exp(f(hqi ,n G i )/τ)∑\ni ̸=j exp(f(h q i ,n G j )/τ)\n(11)\nwhere f(·) is a dot product function and τ denotes a temperature parameter. Since the TGCL task is able to unify the representations of text and graph, it can also benefit for the fusion and interaction of the information from text and graph."
    }, {
      "heading" : "3.4 Overview and Discussion",
      "text" : "Overview. Our approach focuses on continually pre-training PLMs for improving the understanding of math problems. We adopt PLM and GAT to encode the math text and math syntax graph of the math problem, respectively, and design syntaxaware memory networks between the last k layers of PLM and GAT for capturing complex finegrained correlations between the text and graph. In each of the last k layers, we first interact and fuse the representations of tokens from the math text and nodes from the math syntax graph, then feed the updated representations into the next layers of PLM and GAT for consolidation. To continually\npre-train our model, we propose MLM, DTC and TGCL tasks to further enhance and fuse the representations of math text and math syntax graph. For downstream tasks, we fine-tune our model with specific data and objectives, and concatenate the representations of text hq and graph nG from the last layer for prediction.\nDiscussion. The key of our approach is to deeply interact and fuse the math text and formula information of the math problem via syntax-aware memory networks and continual pre-training tasks. Recently, MathBERT (Peng et al., 2021) is proposed to continual pre-train BERT in math domain corpus, which applies self-attention mechanism to interact formulas with text and learns similar tasks as BERT. As a comparison, we construct the math syntax graph to enrich the formula information and design a syntax-aware memory network to fuse the text and graph information. Via the syntax-aware memory network, the token from math text can trace its related nodes along the relations in the math syntax graph, which can capture fine-grained correlations between tokens and nodes. Besides, we explicitly model the syntax graph via GAT, and devise the DTC task to improve the correlations within triplets from the math syntax graph, and the TGCL task to align the representations of graph and text. In this way, we can better capture graph structural information and fuse it with textual information. It is beneficial for understanding logical semantics from formulas of math problems ."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We conduct experiments on four tasks in the math domain to verify the effectiveness of our approach.\nEvaluation Tasks. We construct four tasks based on the collected math exercise problems of high school students, which cover math problem classification, solving, and recommendation. The statistics of these tasks are summarized in Table 1.\n• Knowledge Point Classification (KPC) is a multi-class classification task. Given a math ques-\ntion, the goal is to classify what knowledge point (KP) this question is associated with. The knowledge points are defined and annotated by professionals, and we finally define 387 KPs in this task. • Question-Answer Matching (QAM) is a binary classification task to predict whether an answer is matched with a question. For each question, we randomly sample an answer from other problems as the negative example. • Question Relation Classification (QRC) is a 6-class classification task. Given a pair of math questions, this task aims to predict their relation (e.g., equivalent, similar, problem variant, conditional variant, situation variant, irrelevant). • Similar Question Recommendation (SQR) is a ranking task. Given a question, this task aims to rank retrieved candidate questions by the similarity.\nEvaluation Metrics. For classification tasks (KPC, QRC, QAM), we adopt Accuracy and F1-macro as the evaluation metrics. For the recommendation task (SQR), we employ top-k Hit Ratio (HR@k) and top-k Normalized Discounted Cumulative Gain (NDCG@k) for evaluation. Since the length of candidate list is usually between 6 and 15, we report results on HR@3 and NDCG@3.\nBaseline Methods. We compare our proposed approach with the following nine baseline methods: • TextCNN (Kim, 2019) is a classic text classification model using CNN on top of word vectors. • TextRCNN (Lai et al., 2015) combines both RNN and CNN for text classification tasks. • GAT (Veličković et al., 2018) utilizes the attention mechanism to aggregate neighbors’ representations to produce representations for each node. • R-GCN (Schlichtkrull et al., 2018) extended Graph Convolutional Network with multi-edge encoding to aggregate neighbors’ representations.\n• BERT-Base (Devlin et al., 2018) is a popular pre-trained model. We use the bert-base-chinese, and add some new tokens into the original vocab to represent specific symbols in math datasets.\n• TAPT-BERT (Gururangan et al., 2020) continually pre-trains BERT on task-related corpus. We use our collected math problem dataset with the masked language model task for implementation.\n• BERT+GAT concatenates the [CLS] embedding from BERT and mean node embedding from GAT as the representation of a math question.\n• TAPT-BERT+GAT replaces BERT in BERT+GAT with the TAPT-BERT.\n• MathBert (Peng et al., 2021) continual pretrains BERT with BERT-like tasks, and revises the self-attention for encoding the OPT of formulas.\nImplementation Details. For baseline models, all hyper-parameters are set following the suggestions from the original papers. For all PLM-related models, we implement them based on HuggingFace Transformers 3. For the models combining PLM and GAT, we set the number of GAT layer as 6.\nIn the continual pre-training stage, we initialize the weights of all models with bert-base-chinese 4. We continually pre-train the parameters with a total of 128 batch size for 100,000 steps.And the max length of input sequences is set as 512. We use AdamW (Loshchilov and Hutter, 2017) optimization with β1 = 0.9, β2 = 0.999, learning rate warmup over the first 5% steps, and linear decay of the learning rate. The learning rate is set as 1e−4.\nDuring fine-tuning on downstream tasks, we use AdamW with the same setting as pre-training. And batch size for all experiments is set as 32. The learning rate is set to 1e−3 for non-pre-training methods and 3e−5 for pre-training methods.\n3https://huggingface.co/transformers/ 4https://huggingface.co/bert-base-chinese"
    }, {
      "heading" : "4.2 Main Results",
      "text" : "The results of all methods on four tasks are shown in Table 2. Based on these results, we can find:\nAs for non-pre-training methods, text-based methods (i.e., TextCNN and TextRCNN) outperform GNN-based methods (i.e., GAT and R-GCN). It indicates that textual description is more important than formulas to understand math problems. In general, non-pre-training methods perform worse than pre-training methods, since pre-trained models have learned sufficient general knowledge during pre-training on large-scale corpus.\nAmong five pre-training methods, we can observe two trends. First, combining PLMs with GNN mostly yields improvement. The reason is that GNN can capture the structural semantics from formulas as auxiliary information to help PLM model the math problem, but the improvement is unstable since these methods just concatenate the representations of text and graph but not deeply fuse them. Secondly, continual pre-training brings a significant improvement on all tasks. The reason is that there are a number of specific symbols and terms in math text that PLMs haven’t learned before, but continual pre-training can solve it.\nFinally, by comparing our approach with all the baselines, it is clear to see that our model performs consistently better than them on four tasks. We utilize the syntax-aware memory network to fuse and interact the representations of textual descriptions and formulas, and adopt three continual pretraining tasks to further align and enhance these representations. Among these results, we can see that our model achieves a large improvement on the KPC task. The possible reason is that detecting knowledge points depends more on the deep understanding of formulas and text."
    }, {
      "heading" : "4.3 Few-shot Learning",
      "text" : "To validate the reliability of our method under the data scarcity scenarios, we conduct few-shot experiments on KPC and QRC tasks by using different proportions of the training data, i.e., 5%, 10%, 20% and 40%. We compared our model with TAPTBERT, TAPT-BERT+GAT and MathBERT.\nFigure 3 shows the evaluation results. We can see that the performance substantially drops when the size of training data is reduced. However, our model performs consistently better than others across different tasks and metrics. This result shows that our model leverage the data more effectively with the novel syntax-aware memory network and continual pre-training tasks. With 5% training data, our model exceeds the best baseline by a large margin. It further indicates that our model have better robustness."
    }, {
      "heading" : "4.4 Ablation and Variation Study",
      "text" : "Our proposed approach contains several complementary modules and pre-training tasks. Thus, we conduct experiments on KPC and QRC tasks to verify the contribution of these modules and tasks. Concretely, we remove the module GAT, BERT, Syntax-Aware Memory Network, or the task MLM, DTC and TGCL, respectively.\nIn Table 4, we can see that the performance drops with any modules or pre-training tasks removed. It shows the effectiveness of each part of our proposed model. Among all the modules, it is clearly to see that model’s performance drops a lot after removing BERT, which implies that the textual descriptions are more important for math problem understanding. Besides, we can see that removing MLM also results in a larger performance drop. The reason may be that MLM can enhance the representations of math text, which is important for math problem understanding."
    }, {
      "heading" : "4.5 Hyper-Parameters Analysis",
      "text" : "Our proposed model contains a few parameters to tune. In this part, we tune two parameters, i.e., the number of GAT Layer and the continual pretraining steps. We conduct experiments on KPC and QRC tasks and show the change curves of Accuracy in Figure 3.\nWe can observe that our model achieves the best performance in 80k steps. It indicates that our model can be improved by continual pre-training gradually and may overfit after 80k steps. Besides, our model achieves the best performance with 6 GAT layers, which shows that 6 GAT layers are sufficient to capture the information in syntax graph."
    }, {
      "heading" : "5 Related Work",
      "text" : "Math Problem Understanding Math problem understanding task focuses on understanding the text, formulas and symbols in math domain. A surge of works aim to understand the math formulas for problem solving or mathematical information retrieval. In this way, the formula is usually transformed as a tree or graph (e.g., Operator Tree (Zanibbi and Blostein, 2012)), then network embedding methods Mansouri et al. (2019) and graph neural networkSong and Chen (2021) are utilized to encode it. Besides, a number of works\nfocus on understanding math problem based on the textual information. Among them, Math Word Problem (MWP) Solving is a popular task that generates answers of math word problems. Numerous deep learning based methods have been proposed to tackle MWP, ranging from Seq2Seq (Chiang and Chen, 2019; Li et al., 2019), Seq2Tree(Wang et al., 2019; Qin et al., 2020), to Pre-trained Language Models(Kim et al., 2020; Liang et al., 2021). More recently, several works attempt to modeling more complex math problems (Huang et al., 2020; Hendrycks et al., 2021) that require to understand both textual and formula information.\nContinual Pre-training of Language Models Continually pre-training can effectively improve pre-trained model’s performance on new domain or downstream tasks (Gururangan et al., 2020). To achieve it, most of previous works either continually optimize the model parameters with BERT-like tasks on domain or task related corpus (e.g., scientific (Beltagy et al., 2019) and bio-media (Lee et al., 2020)), or design new pre-training objectives for task adaption (e.g., commonsense reasoning (Zhou et al., 2020) and dialogue adaption (Li et al., 2020)). Besides, several works (Wang et al., 2020; Xiang et al., 2020) utilize both domain-related corpus and new pre-training objectives for continual pre-training, and even revise the Transformer structure of PLMs for better adaption (Ghosal et al., 2020). For math problem understanding, the recently proposed MathBERT (Peng et al., 2021) adopts math domain corpus and formula-related pre-training tasks for continually pre-training."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose COMUS, to continually pre-train PLMs for math problem understanding. By combining the formulas with the syntax tree of mathematical text, we constructed the math syntax graph and designed the syntax-aware memory network to fuse the information from the text and formulas. Along the syntax relations within the memory network, the token can directly trace into its semantic-related nodes from the graph, so that the fine-grained correlations between nodes and tokens can be well captured. Besides, we devised three continual pre-training tasks to further enhance and align the representations of the text and graph. Experimental results have shown that our approach outperforms several competitive baselines on four tasks in the math domain."
    } ],
    "references" : [ {
      "title" : "Scibert: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:1903.10676.",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in neural information processing systems, 26.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantically-aligned equation generation for solving and reasoning math word problems",
      "author" : [ "Ting-Rui Chiang", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Chiang and Chen.,? 2019",
      "shortCiteRegEx" : "Chiang and Chen.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Kingdom: Knowledge-guided domain adaptation for sentiment analysis",
      "author" : [ "Deepanway Ghosal", "Devamanyu Hazarika", "Abhinaba Roy", "Navonil Majumder", "Rada Mihalcea", "Soujanya Poria." ],
      "venue" : "arXiv preprint arXiv:2005.00791.",
      "citeRegEx" : "Ghosal et al\\.,? 2020",
      "shortCiteRegEx" : "Ghosal et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Measuring mathematical problem solving with the math dataset",
      "author" : [ "Dan Hendrycks", "Collin Burns", "Saurav Kadavath", "Akul Arora", "Steven Basart", "Eric Tang", "Dawn Song", "Jacob Steinhardt." ],
      "venue" : "arXiv preprint arXiv:2103.03874.",
      "citeRegEx" : "Hendrycks et al\\.,? 2021",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural mathematical solver with enhanced formula structure",
      "author" : [ "Zhenya Huang", "Qi Liu", "Weibo Gao", "Jinze Wu", "Yu Yin", "Hao Wang", "Enhong Chen." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Infor-",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Point to the expression: Solving algebraic word problems using the expression-pointer transformer model",
      "author" : [ "Bugeun Kim", "Kyung Seo Ki", "Donggeon Lee", "Gahgene Gweon." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Y Kim." ],
      "venue" : "arxiv 2014. arXiv preprint arXiv:1408.5882.",
      "citeRegEx" : "Kim.,? 2019",
      "shortCiteRegEx" : "Kim.",
      "year" : 2019
    }, {
      "title" : "Recurrent convolutional neural networks for text classification",
      "author" : [ "Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao." ],
      "venue" : "Twenty-ninth AAAI conference on artificial intelligence.",
      "citeRegEx" : "Lai et al\\.,? 2015",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2015
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling intrarelation in math word problems with different functional multi-head attentions",
      "author" : [ "Jierui Li", "Lei Wang", "Jipeng Zhang", "Yan Wang", "Bing Tian Dai", "Dongxiang Zhang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Task-specific objectives of pre-trained language models for dialogue adaptation",
      "author" : [ "Junlong Li", "Zhuosheng Zhang", "Hai Zhao", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "arXiv preprint arXiv:2009.04984.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Mwp-bert: A strong baseline for math word problems",
      "author" : [ "Zhenwen Liang", "Jipeng Zhang", "Jie Shao", "Xiangliang Zhang." ],
      "venue" : "arXiv preprint arXiv:2107.13435.",
      "citeRegEx" : "Liang et al\\.,? 2021",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2021
    }, {
      "title" : "Finding similar exercises in online education systems",
      "author" : [ "Qi Liu", "Zai Huang", "Zhenya Huang", "Chuanren Liu", "Enhong Chen", "Yu Su", "Guoping Hu." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "arXiv preprint arXiv:1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Tangent-cft: An embedding model for mathematical formulas",
      "author" : [ "Behrooz Mansouri", "Shaurya Rohatgi", "Douglas W Oard", "Jian Wu", "C Lee Giles", "Richard Zanibbi." ],
      "venue" : "Proceedings of the 2019 ACM SIGIR international conference on theory of information",
      "citeRegEx" : "Mansouri et al\\.,? 2019",
      "shortCiteRegEx" : "Mansouri et al\\.",
      "year" : 2019
    }, {
      "title" : "Mathbert: A pre-trained model for mathematical formula understanding",
      "author" : [ "Shuai Peng", "Ke Yuan", "Liangcai Gao", "Zhi Tang." ],
      "venue" : "arXiv preprint arXiv:2105.00377.",
      "citeRegEx" : "Peng et al\\.,? 2021",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2021
    }, {
      "title" : "Semantically-aligned universal tree-structured solver for math word problems",
      "author" : [ "Jinghui Qin", "Lihui Lin", "Xiaodan Liang", "Rumin Zhang", "Liang Lin." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Tu_dbs in the arqmath lab 2021, clef",
      "author" : [ "Anja Reusch", "Maik Thiele", "Wolfgang Lehner" ],
      "venue" : null,
      "citeRegEx" : "Reusch et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Reusch et al\\.",
      "year" : 2021
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Sejr Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "ESWC.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Mathbert: A pre-trained language model for general nlp tasks in mathematics education",
      "author" : [ "Jia Tracy Shen", "Michiharu Yamashita", "Ethan Prihar", "Neil Heffernan", "Xintao Wu", "Ben Graff", "Dongwon Lee." ],
      "venue" : "arXiv preprint arXiv:2106.07340.",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Searching for mathematical formulas based on graph representation learning",
      "author" : [ "Yujin Song", "Xiaoyu Chen." ],
      "venue" : "International Conference on Intelligent Computer Mathematics, pages 137–152. Springer.",
      "citeRegEx" : "Song and Chen.,? 2021",
      "shortCiteRegEx" : "Song and Chen.",
      "year" : 2021
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Template-based math word problem solvers with recursive neural networks",
      "author" : [ "Lei Wang", "Dongxiang Zhang", "Jipeng Zhang", "Xing Xu", "Lianli Gao", "Bing Tian Dai", "Heng Tao Shen." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised pre-training of bidirectional speech encoders via masked reconstruction",
      "author" : [ "Weiran Wang", "Qingming Tang", "Karen Livescu." ],
      "venue" : "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised domain adaptation through synthesis for person re-identification",
      "author" : [ "Suncheng Xiang", "Yuzhuo Fu", "Guanjie You", "Ting Liu." ],
      "venue" : "2020 IEEE International Conference on Multimedia and Expo (ICME), pages 1–6. IEEE.",
      "citeRegEx" : "Xiang et al\\.,? 2020",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Recognition and retrieval of mathematical expressions",
      "author" : [ "Richard Zanibbi", "Dorothea Blostein." ],
      "venue" : "International Journal on Document Analysis and Recognition (IJDAR), 15(4):331–357.",
      "citeRegEx" : "Zanibbi and Blostein.,? 2012",
      "shortCiteRegEx" : "Zanibbi and Blostein.",
      "year" : 2012
    }, {
      "title" : "Pre-training text-to-text transformers for concept-centric common sense",
      "author" : [ "Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Bill Yuchen Lin", "Xiang Ren." ],
      "venue" : "arXiv preprint arXiv:2011.07956.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Such a capacity is the key to the success of a variety of education applications, including math problem retrieval (Reusch et al., 2021), problem recommendation (Liu et al.",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : ", 2021), problem recommendation (Liu et al., 2018), and problem solving (Huang et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : "To automatically understand math problems, it is feasible to learn computational representations from problem statement texts with pre-trained language models (PLMs) (Shen et al., 2021; Peng et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "To automatically understand math problems, it is feasible to learn computational representations from problem statement texts with pre-trained language models (PLMs) (Shen et al., 2021; Peng et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 204
    }, {
      "referenceID" : 3,
      "context" : "Pre-trained on the large-scale general corpus, PLMs (Devlin et al., 2018) can be effectively transferred into new domains or tasks Math Problem: Given that sin x is equal to 0.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : "However, previous works (Reusch et al., 2021; Shen et al., 2021) mostly oversimplify the issues of math problem understanding.",
      "startOffset" : 24,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "However, previous works (Reusch et al., 2021; Shen et al., 2021) mostly oversimplify the issues of math problem understanding.",
      "startOffset" : 24,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : "To better model the formulas, operator trees have been introduced to represent the math formulas (Zanibbi and Blostein, 2012), which are sub-",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "Even with concatenation and self-attention mechanisms (Peng et al., 2021), it is still hard to",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "Inspired by previous works (Mansouri et al., 2019; Peng et al., 2021), we construct a syntax graph, where the textual description is represented as a syntax dependency tree and the formulas are represented as operator trees (OPT).",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "Inspired by previous works (Mansouri et al., 2019; Peng et al., 2021), we construct a syntax graph, where the textual description is represented as a syntax dependency tree and the formulas are represented as operator trees (OPT).",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "We use BERT (Devlin et al., 2018) as the PLM to encode the math text, i.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "We incorporate a graph attention network (GAT) (Veličković et al., 2018) to encode the math syntax graph, it is composed of an embedding layer and a stack of graph attention layers.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "Thus, inspired by TransE (Bordes et al., 2013), we devise the dependency triplet completion task to improve the correlations within the triplets.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "Recently, MathBERT (Peng et al., 2021) is proposed to continual pre-train BERT in math domain corpus, which applies self-attention mechanism to interact formulas with text and learns similar tasks as BERT.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "• TextCNN (Kim, 2019) is a classic text classification model using CNN on top of word vectors.",
      "startOffset" : 10,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "• TextRCNN (Lai et al., 2015) combines both RNN and CNN for text classification tasks.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 24,
      "context" : "• GAT (Veličković et al., 2018) utilizes the attention mechanism to aggregate neighbors’ representations to produce representations for each node.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "• R-GCN (Schlichtkrull et al., 2018) extended Graph Convolutional Network with multi-edge encoding to aggregate neighbors’ representations.",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "• BERT-Base (Devlin et al., 2018) is a popular pre-trained model.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "• TAPT-BERT (Gururangan et al., 2020) continually pre-trains BERT on task-related corpus.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 18,
      "context" : "• MathBert (Peng et al., 2021) continual pretrains BERT with BERT-like tasks, and revises the self-attention for encoding the OPT of formulas.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "We use AdamW (Loshchilov and Hutter, 2017) optimization with β1 = 0.",
      "startOffset" : 13,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : ", Operator Tree (Zanibbi and Blostein, 2012)), then network embedding methods Mansouri et al.",
      "startOffset" : 16,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "Numerous deep learning based methods have been proposed to tackle MWP, ranging from Seq2Seq (Chiang and Chen, 2019; Li et al., 2019), Seq2Tree(Wang",
      "startOffset" : 92,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "Numerous deep learning based methods have been proposed to tackle MWP, ranging from Seq2Seq (Chiang and Chen, 2019; Li et al., 2019), Seq2Tree(Wang",
      "startOffset" : 92,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : ", 2020), to Pre-trained Language Models(Kim et al., 2020; Liang et al., 2021).",
      "startOffset" : 39,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : ", 2020), to Pre-trained Language Models(Kim et al., 2020; Liang et al., 2021).",
      "startOffset" : 39,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "More recently, several works attempt to modeling more complex math problems (Huang et al., 2020; Hendrycks et al., 2021) that require to understand both textual and formula information.",
      "startOffset" : 76,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "More recently, several works attempt to modeling more complex math problems (Huang et al., 2020; Hendrycks et al., 2021) that require to understand both textual and formula information.",
      "startOffset" : 76,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "Continual Pre-training of Language Models Continually pre-training can effectively improve pre-trained model’s performance on new domain or downstream tasks (Gururangan et al., 2020).",
      "startOffset" : 157,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : ", scientific (Beltagy et al., 2019) and bio-media (Lee et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : ", 2019) and bio-media (Lee et al., 2020)), or design new pre-training objectives for task adaption (e.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 29,
      "context" : ", commonsense reasoning (Zhou et al., 2020) and dialogue adaption (Li et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : "Besides, several works (Wang et al., 2020; Xiang et al., 2020) utilize both domain-related corpus and new pre-training objectives for continual pre-training, and even revise the Transformer structure of PLMs for better adaption (Ghosal et al.",
      "startOffset" : 23,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Besides, several works (Wang et al., 2020; Xiang et al., 2020) utilize both domain-related corpus and new pre-training objectives for continual pre-training, and even revise the Transformer structure of PLMs for better adaption (Ghosal et al.",
      "startOffset" : 23,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : ", 2020) utilize both domain-related corpus and new pre-training objectives for continual pre-training, and even revise the Transformer structure of PLMs for better adaption (Ghosal et al., 2020).",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 18,
      "context" : "For math problem understanding, the recently proposed MathBERT (Peng et al., 2021) adopts math domain corpus and formula-related pre-training tasks for continually pre-training.",
      "startOffset" : 63,
      "endOffset" : 82
    } ],
    "year" : 0,
    "abstractText" : "Recently, pre-trained language models (PLMs) have shown effectiveness in domain transfer and task adaption. However, two major challenges limit the effectiveness of transferring PLMs into math problem understanding tasks. First, a math problem usually contains a textual description and formulas. The two types of information have a natural semantic gap. Second, textual and formula information is essential to each other, it is hard but necessary to deeply fuse the two types of information. To address these issues, we enrich the formula information by combining the syntax semantics of the text to construct the math syntax graph, and design the syntax-aware memory networks to deeply fuse the characteristics from the graph and text. With the help of syntax relations, the token from the text can trace its semanticrelated nodes within the formulas, which is able to capture the fine-grained correlations between text and formulas. Besides, we also devise three continual pre-training tasks to further align and fuse the representations of the text and graph. Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_216_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Rethinking and Refining the Distinct Metric",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The diversity of generated texts is an important evaluation aspect for dialogue generation models since most neural dialogue models tend to produce general and trivial responses (like \"I don’t know\" or \"Me too\") (Li et al., 2016; Zhao et al., 2017). Several metrics have been proposed to evaluate the text diversity, and the Distinct score proposed by Li et al. (2016) is the most widely applied metric due to its intuitive nature and convenient calculation. It has become a de facto standard to report the Distinct score to compare the performance of different models in terms of response diversity (Liu et al., 2016; Fan et al., 2018; Wu et al., 2021; Zhou et al., 2021). Most previous works follow the initial approach of Li et al. (2016) to calculate the Distinct score, i.e., dividing the number of unique tokens (n-grams) by that of all tokens (n-grams). However, although reported to be effective, we surprisingly find that this naive approach tends to introduce more penalty to longer texts and lead to inaccurate evaluation of the text diversity.\nWe argue that the scaling factor of Distinct requires a comprehensive discussion for two reasons. First, prior research in non-computational\n2http://opus.nlpl.eu/OpenSubtitles2018.php\nlinguistics has demonstrated the shortcomings of Distinct’s scaling approach (Malvern et al., 2004). We found that early applications of Distinct exist in psychological linguistics, where researchers leveraged this metric to assess the language diversity of children with communication disorders (Chotlos, 1944). Their research showed that as a child speaks more words, Distinct experiences an adverse decline since each extra word that the child utters adds to the total number of words, yet it would only increase the number of distinct words if the word had not been used before (Malvern et al., 2004; Chotlos, 1944). Second, we also discovered an uncommon decline of this metric on both natural corpus and a designated distribution sampler when the total number of words increases. As illustrated in Figure 1, the original Distinct cannot keep a stable value and experiences a sharp decrease with increasing utterance length in both natural and designated distributions. However, as a qualified metric needs to support quantitative comparison among different methods, its value should stay invariant when the distribution of the words appearing is determined. This result is consistent with the findings of psy-\nchologists, indicating an over-penalty does exist in such a scaling method.\nOur contributions are summarized as follows: 1. We investigate the performance of the original Distinct and demonstrate that this metric is not sufficiently fair due to its scaling method. We also highlight the risks of using this metric for evaluating response diversity.\n2. We propose an improved version of Distinct (New Distinct) based on the idea that the scaling factor should be the expectation of the number of distinct tokens instead.\n3. Human evaluation shows that New Distinct correlates better with human judgments. We further discuss the drawbacks of New Distinct and suggest feasible ways of using this metric in practice."
    }, {
      "heading" : "2 Preliminary Discussion about Original Distinct",
      "text" : "To exemplify the shortcoming of origin Distinct, we depicted Distinct score on two kinds of texts at different lengths. One kind of text is sampled from an artificially designated distribution and the other is sampled from a real corpus. In detail, the designated distribution we adopted is P (X = k) =∫ v 0 λke−λ\nvk! dλ, where v is vocabulary size and we simply let it be 30522 (Devlin et al., 2019). The real corpus we adopted is the crawled data from OpenSubtitles1. For each length, we sampled 2000 sentences as a set and calculated scores of each set.\nWe found the original Distinct scores decrease sharply with increasing utterance length in both distributions. As shown by the \"original-designated\" line, with the distribution being determined, lengthier texts will get lower scores than shorter texts. We highlighted this problem because it is extremely simple for models to control the length of texts by using decoding tricks, like adjusting the penalty coefficient (Vijayakumar et al., 2016). It makes a model \"easily\" beat the other model by such tricks while it obviously not suitable to draw the conclusion that a model performs better than the other in diversity. The same phenomenon can be observed on the real corpus (see \"original-natural\" line in Figure 1). As language distribution is more complex than what we are able to formulate, we depicted the performance of the original Distinct on 6 famous datasets in Appendix. Many cases indicate that the original Distinct is really unreasonable to be a fair metric for evaluating diversity.\n1http://opus.nlpl.eu/OpenSubtitles2018.php"
    }, {
      "heading" : "3 Improving Original Distinct",
      "text" : ""
    }, {
      "heading" : "3.1 Formula Derivation",
      "text" : "The original Distinct score (Li et al., 2016) is measured as Distinct = NC , where N is the number of distinct tokens and C is the total number of tokens. To improve the original scaling method, we propose that the scaling factor should be the expectation of the number of distinct words in the set of generated responses. Hence, it becomes\nNewDistinct = N E [ N̂ ] (1)\nSupposing a set of generated responses R with size S to be evaluated, we let lk,i be the ith token of kth response in R and tk be the length of kth response. The expectation E[N̂ ] for N̂ distinct words to appear in R would be\nE [ N̂ ] = V∑ j (1− S∏ k P (lk,tk 6= uj , ..., lk,1 6= uj)) , (2)\nwhere V denotes the vocabulary size, and {u1, ...uV } is the set of all tokens in the vocabulary.\nAs shown in Equation 2, the calculation requires us to know P (ltk 6= uj , ltk−1 6= uj , ..., l1 6= uj). Though current models can easily estimate the probability of a word appearing behind given words, it is hard to calculate the probability of each word that never appears in any position of a sequence. Thus, there is may no efficient way to calculate P (lk,t 6= uj , ..., lk,1 6= uj)). Besides, different language distributions have different P, which leads to different expectations and make the metric less general. Thus, we employ the upper bound of response diversity (i.e. a set of generated responses where each token appears with equal probability) to calculate this expectation. We hypothesize that the scaling effect of the upper bound is approximately proportional to that of other sets of generated responses; therefore, it can replace the original scaling factor.\nE [ N̂ ] ∝∼ E [ ˆNupper ] , (3)\nE [ ˆNupper ] = V∑ j (1− S∏ k tk∏ i P (lk,i 6= uj)) (4)\n= V [1− (V − 1 V )C ] (5)\nThus, new Distinct score is calculated as:\nNewDistinct = N\nV [1− (V−1 V\n)C ] (6)\nWe have the details of formula derivation, a piece of discussion of the formula’s properties and the determination of vocabulary size in Appendix."
    }, {
      "heading" : "3.2 Experimental Verification",
      "text" : ""
    }, {
      "heading" : "3.2.1 Evaluation Approach",
      "text" : "We compared new Distinct with the original unigram Distinct (Li et al., 2016) by calculating both metrics on the results of ten methods for diversifying dialog generation, reported by Wang et al. (2021). Please see the detailed introduction of the reported methods in Appendix.\nAs correlation analysis has been widely used to evaluate automatic metrics for language generation (Tao et al., 2018; Sellam et al., 2020), we calculated the Pearson, Spearman, and Kendall correlation coefficients between both scores and human judgments. Pearson’s correlation estimates linear correlation while Spearman’s and Kendall’s correlations estimate monotonic correlation, with Kendall’s correlation being usually more insensitive to abnormal values. We used SciPy2 for correlation calculation and significance test."
    }, {
      "heading" : "3.2.2 Datasets",
      "text" : "Our experiments use two open-domain dialog generation benchmark datasets: DailyDialog(Li et al., 2017), a high-quality dialog dataset collected from daily conversations, and OpenSubtitles3, which contains dialogs collected from movie subtitles (see Table 1 for more details). We follow the data processing procedures reported by Wang et al. (2021)."
    }, {
      "heading" : "3.2.3 Preliminary Observations",
      "text" : "Based on the obtained results (check Table 2), it can be observed that NewDistinct has a clear edge over the original Distinct: first, the contrast between diversity of generated responses for different methods is highlighted more effectively by\n2https://docs.scipy.org/doc/scipy/reference/stats.html 3http://opus.nlpl.eu/OpenSubtitles2018.php\nNewDistinct (e.g. though AdaLab gets the highest diversity score using Distinct (3.96), its difference from other methods is not as evident as its NewDistinct score (9.63)); second, in contrast to Distinct, NewDistinct provides a more accurate evaluation of response diversity. For instance, the Distinct scores for CP and UL are both 2.35 while responses generated by UL are found to be more diverse than CP using NewDistinct (5.35 > 5.08). Given that the average length of responses generated by FL is larger than CP, Distinct’s bias towards models that generate shorter sentences becomes evident. These observations are consistent for both datasets."
    }, {
      "heading" : "3.2.4 Correlation Results",
      "text" : "We recruited crowdsourcing workers to evaluate the diversity of the selected methods. For each method, we randomly sampled 100 subsets of 15 responses from their set of generated responses. Response sets of all methods, given the same query set, were packaged together as an evaluation set. We asked each crowdsourcing worker to assign a diversity score to every response group in the evaluation set. Each group was evaluated by at least 3 workers. For ensuring the quality of our annotations, we calculated the score of each set as the average of workers’ scores and filtered out workers whose scores had an insufficient correlation with the average (Pearson Correlation < 0.65). We acknowledge that building a scoring standard for annotating language diversity is challenging. Hence, we did not require our workers to give an absolute score for each set. Instead, we asked them to highlight the contrast between different sets by scoring values that linearly reflect the response diversity difference between the sets. For instance, the two sets of scores {1, 2, 2} and {2, 5, 5} show the same evaluation since the same contrast is shown. We then normalized the scores to the [0-10] range.\nThen, we calculated the correlation between the Distinct scores with the crowdsourced values for all the methods. The results are provided in Table 2. The evaluation results indicate that our proposed NewDistinct is more consistent with human judgments for measuring response diversity, as NewDistinct shows the highest correlation with human evaluations among all correlation metrics (Pearson/ Spearson/ Kendall) on both datasets.\n3See Appendix for more details on the human evaluation interface"
    }, {
      "heading" : "4 New Distinct in Practice",
      "text" : "As new Distinct is based on idealized assumption that does not take language distribution into account, we further discuss this problem and propose a potential practical way of new Distinct in real situations. Before applying new Distinct, it is necessary to explore the relationship between score and text length (Figure 1) and check the performance of Distinct on the training data. To our knowledge, if the training data is from large-scale open-domain sources such as OpenSubtitles and Reddit, Distinct can maintain its value on different lengths. Hence, it can be directly used for evaluating models trained on these datasets. However, we found our experiments on datasets such as Twitter showed a decline in Distinct on lengthier texts. It is probably because of the platform rule of limiting text length under 280, which induces users to say as much information as possible within a shorter length. In this situation, it must be unfair for those methods that tend to generate lengthier texts."
    }, {
      "heading" : "5 Related Work",
      "text" : "Li et al. (2016) proposed Distinct, calculated as the number of distinct tokens divided by the total number of tokens. To our knowledge, this is the most widely-used automatic metric for evaluating response generation diversity. However, as we showed in Figure 1, it is an unfair indicator as it is affected by the sample length. This causes a bias against models which tend to generate longer sentences.\nThere exist other metrics for evaluating diversity but no one is as widely-used as Distinct (Zhu et al., 2018; Xu et al., 2018). Specifically, Self-BLEU proposed by Zhu et al. (2018) is extremely timeconsuming as its computation complexity is O(n2) , where n denoted the size of the test set."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we proposed an improved variation of the Distinct score, which is a widely-used metric for evaluating response diversity in dialog systems. We provided the theory as well as the methodology behind the formulation of our proposed score (New Distinct). In addition, we conducted experiments on recently proposed dialog generation methods to verify the effectiveness of this metric. The obtained results demonstrated that New Distinct has a higher correlation with human evaluation in comparison with other metrics."
    }, {
      "heading" : "A Comparison on More Datasets",
      "text" : "To demonstrate the shortcomings of the original Distint metric, we illustrate original Distinct on 6 datasets: Persona-chat (Zhang et al., 2018), Ubuntu Dialog Corpus (Lowe et al., 2015), DailyDialog, Topic-Chat (Gopalakrishnan et al., 2019), Empathetic Dialogs (Rashkin et al., 2018), Wizard of Wikipedia (Dinan et al., 2018), Reddit (Serban et al., 2015), and Twitter (Ritter et al., 2010) (Figure 1). It can be observed that with an increasing sample length, the original Distinct score tends to follow a linear decline while the proposed metric maintains its consistency."
    }, {
      "heading" : "B Formula Derivation and Property Discussion",
      "text" : "E [ N̂ ] = E  V∑ j i=tk,k=S∨ i,k 1lk,i=uj  (7) =\nV∑ j P {i=tk,k=S∨ i,k 1lk,i=uj} = 1  (8) =\nV∑ j (1− S∏ k P (ltk 6= uj , ..., l1 6= uj))\n(9)\nE [ ˆNupper ] = V∑ j (1− S∏ k tk∏ i P (lk,i 6= uj))\n(10)\n= V [1− (V − 1 V )C ], (11)\nThus, our proposed Distinct score is calculated as\nNewDistinct = N\nV [1− (V−1V )C ] (12)\nFormula Property 1. NewDistinct increases faster as C is increasing, but its incremental rate converges to 1V , as shown by its derivative below:\ndNewDistinct\ndN =\n1\nV [1− (V−1V )C ] (13)\nlim C→+∞\ndNewDistinct\ndN =\n1 V (14)\nwhereas in the original Distinct, we have\ndDistinct\ndN =\n1 C (15)\nWe can see from the original metric that the bigger C is, the slower the original Distinct increases. It is the reason why this metric is not fair to those models that tend to generate longer sentences.\nFormula Property 2. NewDistinct converges to N V (≤ 1) as C increases.\nlim C→+∞ NewDistinct = lim C→+∞\nN\nV [1− (V−1V )C ] (16)\n= N\nV <= 1, (17)\nwhere N V [1−(V−1 V )C ] ∈ [0,+∞]. Theoretically, NewDistinct can have values larger than 1 (e.g. when N = V ), which is an extremely rare case in practice: as we utilized the upper bound for measuring the expectation, it is exceptionally hard for N to obtain an equal value to or an even greater value than E( ˆNupper)."
    }, {
      "heading" : "C Details of Human Evaluation",
      "text" : "Our created human evaluation interface is provided in Figure 3."
    }, {
      "heading" : "D How to Determine Vocabulary Size",
      "text" : "As we discussed the properties of NewDistinct, vocabulary size makes little impact on changing its value when it has reached a large number (usually more than 30000), so it is not necessary to measure an exact value. To compare different methods, it is recommended to use a common vocabulary size, (such as BERT’s 30522) (Devlin et al., 2019). It is also reasonable to calculate the vocabulary size of a\ndataset by NLTK tokenizer, when research focuses on a specific dataset. For non-english corpora, we recommend researchers to determine a vocabulary size following Xu et al. (2021)."
    }, {
      "heading" : "E Details of Evaluated Methods",
      "text" : "Wang et al. (2021) proposed a novel adaptive label smoothing method for diversified response generation. Their experiments were conducted on the DailyDialog and OpenSubtitles datasets, using 9 recent methods for diverse response generation as their baselines (similar to what we demonstrated in our paper). Wang et al. (2021) used a transformerbased sequence-to-sequence model (Vaswani et al., 2017) as the backbone of their model, and most of their hyper-parameters follow (Cai et al., 2020). In addition, both the encoder and the decoder contain 6 transformer layers with 8 attention heads, and the hidden size is set to 512. BERT’s WordPiece tokenizer (Devlin et al., 2019) and Adam optimizer (Kingma and Ba, 2015) are used for training their models with random initialization and a learning rate of 1e-4."
    } ],
    "references" : [ {
      "title" : "Data manipulation: Towards effective instance learning for neural dialogue generation via learning to augment and reweight",
      "author" : [ "Hengyi Cai", "Hongshen Chen", "Yonghao Song", "Cheng Zhang", "Xiaofang Zhao", "Dawei Yin." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Cai et al\\.,? 2020",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling knowledge learned in BERT for text generation",
      "author" : [ "Yen-Chun Chen", "Zhe Gan", "Yu Cheng", "Jingzhou Liu", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for 4",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Fˆ2-softmax: Diversifying neural text generation via frequency factorized softmax",
      "author" : [ "Byung-Ju Choi", "Jimin Hong", "David Park", "Sang Wan Lee." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Choi et al\\.,? 2020",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2020
    }, {
      "title" : "Iv",
      "author" : [ "John W. Chotlos." ],
      "venue" : "a statistical and comparative analysis of individual written language samples. Psychological Monographs, 56(2):75–111.",
      "citeRegEx" : "Chotlos.,? 1944",
      "shortCiteRegEx" : "Chotlos.",
      "year" : 1944
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1811.01241.",
      "citeRegEx" : "Dinan et al\\.,? 2018",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898.",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Negative training for neural dialogue response generation",
      "author" : [ "Tianxing He", "James Glass." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2044– 2058, Online. Association for Computational Lin-",
      "citeRegEx" : "He and Glass.,? 2020",
      "shortCiteRegEx" : "He and Glass.",
      "year" : 2020
    }, {
      "title" : "Improving neural response diversity with frequency-aware cross-entropy loss",
      "author" : [ "Shaojie Jiang", "Pengjie Ren", "Christof Monz", "Maarten de Rijke." ],
      "venue" : "The World Wide Web Conference, pages 2879– 2885.",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "ICLR (Poster).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Pa-",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Data-dependent gaussian prior objective for language generation",
      "author" : [ "Zuchao Li", "Rui Wang", "Kehai Chen", "Masso Utiyama", "Eiichiro Sumita", "Zhuosheng Zhang", "Hai Zhao." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Focal loss for dense object detection",
      "author" : [ "Tsung-Yi Lin", "Priya Goyal", "Ross Girshick", "Kaiming He", "Piotr Dollár." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2980– 2988.",
      "citeRegEx" : "Lin et al\\.,? 2017",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2017
    }, {
      "title" : "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1506.08909.",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Lexical diversity and language development",
      "author" : [ "David Malvern", "Brian Richards", "Ngoni Chipere", "Pilar Durán." ],
      "venue" : "Springer.",
      "citeRegEx" : "Malvern et al\\.,? 2004",
      "shortCiteRegEx" : "Malvern et al\\.",
      "year" : 2004
    }, {
      "title" : "Regularizing neural networks by penalizing confident output distributions",
      "author" : [ "Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Łukasz Kaiser", "Geoffrey Hinton." ],
      "venue" : "arXiv preprint arXiv:1701.06548.",
      "citeRegEx" : "Pereyra et al\\.,? 2017",
      "shortCiteRegEx" : "Pereyra et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards empathetic opendomain conversation models: A new benchmark and dataset",
      "author" : [ "Hannah Rashkin", "Eric Michael Smith", "Margaret Li", "Y-Lan Boureau." ],
      "venue" : "arXiv preprint arXiv:1811.00207.",
      "citeRegEx" : "Rashkin et al\\.,? 2018",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised modeling of twitter conversations",
      "author" : [ "Alan Ritter", "Colin Cherry", "William B Dolan." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172–",
      "citeRegEx" : "Ritter et al\\.,? 2010",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2010
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey of available corpora for building data-driven dialogue systems",
      "author" : [ "Iulian Vlad Serban", "Ryan Lowe", "Peter Henderson", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1512.05742.",
      "citeRegEx" : "Serban et al\\.,? 2015",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems",
      "author" : [ "Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Diverse beam search: Decoding diverse solutions from neural sequence models",
      "author" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra." ],
      "venue" : "arXiv preprint arXiv:1610.02424.",
      "citeRegEx" : "Vijayakumar et al\\.,? 2016",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Diversifying dialog generation via adaptive label smoothing",
      "author" : [ "Yida Wang", "Yinhe Zheng", "Yong Jiang", "Minlie Huang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural text generation with unlikelihood training",
      "author" : [ "Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Welleck et al\\.,? 2019",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2019
    }, {
      "title" : "Personalized response generation via generative split memory network",
      "author" : [ "Yuwei Wu", "Xuezhe Ma", "Diyi Yang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Vocabulary learning via optimal transport for neural machine translation",
      "author" : [ "Jingjing Xu", "Hao Zhou", "Chun Gan", "Zaixiang Zheng", "Lei Li." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "LSDSCC: a Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity",
      "author" : [ "Zhen Xu", "Nan Jiang", "Bingquan Liu", "Wenge Rong", "Bowen Wu", "Baoxun Wang", "Zhuoran Wang", "Xiaolong Wang" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too? arXiv preprint arXiv:1801.07243",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
      "author" : [ "Tiancheng Zhao", "Ran Zhao", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    }, {
      "title" : "Eva: An open-domain chinese dialogue system with large-scale generative pre-training",
      "author" : [ "Hao Zhou", "Pei Ke", "Zheng Zhang", "Yuxian Gu", "Yinhe Zheng", "Chujie Zheng", "Yida Wang", "Chen Henry Wu", "Hao Sun", "Xiaocong Yang" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Texygen: A benchmarking platform for text generation models",
      "author" : [ "Yaoming Zhu", "Sidi Lu", "Lei Zheng", "Jiaxian Guo", "Weinan Zhang", "Jun Wang", "Yong Yu." ],
      "venue" : "41st International ACM SIGIR Conference on Research and Development in Information Re-",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    }, {
      "title" : "2021) proposed a novel adaptive label smoothing method for diversified response generation. Their experiments were conducted on the DailyDialog and OpenSubtitles datasets, using 9 recent methods for diverse response generation",
      "author" : [ "Wang" ],
      "venue" : null,
      "citeRegEx" : "Wang,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2021
    }, {
      "title" : "2021) used a transformerbased sequence-to-sequence model (Vaswani et al., 2017) as the backbone of their model, and most of their hyper-parameters follow (Cai et al., 2020)",
      "author" : [ "Wang" ],
      "venue" : null,
      "citeRegEx" : "Wang,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "The diversity of generated texts is an important evaluation aspect for dialogue generation models since most neural dialogue models tend to produce general and trivial responses (like \"I don’t know\" or \"Me too\") (Li et al., 2016; Zhao et al., 2017).",
      "startOffset" : 212,
      "endOffset" : 248
    }, {
      "referenceID" : 32,
      "context" : "The diversity of generated texts is an important evaluation aspect for dialogue generation models since most neural dialogue models tend to produce general and trivial responses (like \"I don’t know\" or \"Me too\") (Li et al., 2016; Zhao et al., 2017).",
      "startOffset" : 212,
      "endOffset" : 248
    }, {
      "referenceID" : 14,
      "context" : "It has become a de facto standard to report the Distinct score to compare the performance of different models in terms of response diversity (Liu et al., 2016; Fan et al., 2018; Wu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "It has become a de facto standard to report the Distinct score to compare the performance of different models in terms of response diversity (Liu et al., 2016; Fan et al., 2018; Wu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 213
    }, {
      "referenceID" : 28,
      "context" : "It has become a de facto standard to report the Distinct score to compare the performance of different models in terms of response diversity (Liu et al., 2016; Fan et al., 2018; Wu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 213
    }, {
      "referenceID" : 33,
      "context" : "It has become a de facto standard to report the Distinct score to compare the performance of different models in terms of response diversity (Liu et al., 2016; Fan et al., 2018; Wu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 213
    }, {
      "referenceID" : 16,
      "context" : "linguistics has demonstrated the shortcomings of Distinct’s scaling approach (Malvern et al., 2004).",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "We found that early applications of Distinct exist in psychological linguistics, where researchers leveraged this metric to assess the language diversity of children with communication disorders (Chotlos, 1944).",
      "startOffset" : 195,
      "endOffset" : 210
    }, {
      "referenceID" : 16,
      "context" : "Their research showed that as a child speaks more words, Distinct experiences an adverse decline since each extra word that the child utters adds to the total number of words, yet it would only increase the number of distinct words if the word had not been used before (Malvern et al., 2004; Chotlos, 1944).",
      "startOffset" : 269,
      "endOffset" : 306
    }, {
      "referenceID" : 3,
      "context" : "Their research showed that as a child speaks more words, Distinct experiences an adverse decline since each extra word that the child utters adds to the total number of words, yet it would only increase the number of distinct words if the word had not been used before (Malvern et al., 2004; Chotlos, 1944).",
      "startOffset" : 269,
      "endOffset" : 306
    }, {
      "referenceID" : 4,
      "context" : "0 λke−λ vk! dλ, where v is vocabulary size and we simply let it be 30522 (Devlin et al., 2019).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "We highlighted this problem because it is extremely simple for models to control the length of texts by using decoding tricks, like adjusting the penalty coefficient (Vijayakumar et al., 2016).",
      "startOffset" : 166,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : "The original Distinct score (Li et al., 2016) is measured as Distinct = NC , where N is the number of distinct tokens and C is the total number of tokens.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "We compared new Distinct with the original unigram Distinct (Li et al., 2016) by calculating both metrics on the results of ten methods for diversifying dialog generation, reported by Wang et al.",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 23,
      "context" : "As correlation analysis has been widely used to evaluate automatic metrics for language generation (Tao et al., 2018; Sellam et al., 2020), we calculated the Pearson, Spearman, and Kendall correlation coefficients between both scores and human judgments.",
      "startOffset" : 99,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "As correlation analysis has been widely used to evaluate automatic metrics for language generation (Tao et al., 2018; Sellam et al., 2020), we calculated the Pearson, Spearman, and Kendall correlation coefficients between both scores and human judgments.",
      "startOffset" : 99,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "Our experiments use two open-domain dialog generation benchmark datasets: DailyDialog(Li et al., 2017), a high-quality dialog dataset collected from daily conversations, and OpenSubtitles3, which contains dialogs collected from movie subtitles (see Table 1 for more details).",
      "startOffset" : 85,
      "endOffset" : 102
    }, {
      "referenceID" : 34,
      "context" : "There exist other metrics for evaluating diversity but no one is as widely-used as Distinct (Zhu et al., 2018; Xu et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 127
    }, {
      "referenceID" : 30,
      "context" : "There exist other metrics for evaluating diversity but no one is as widely-used as Distinct (Zhu et al., 2018; Xu et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 127
    } ],
    "year" : 0,
    "abstractText" : "Distinct is a widely used automatic metric for evaluating the diversity of language generation tasks. However, we observe that the original approach to calculating distinct scores has evident biases that tend to add higher penalties to longer sequences. In this paper, we refine the calculation of distinct scores by re-scaling the number of distinct tokens based on its expectation. We provide both empirical and theoretical evidence to show that our method effectively removes the biases exhibited in the original distinct score. Further analyses also demonstrate that the refined score correlates better with human evaluations.",
    "creator" : null
  }
}
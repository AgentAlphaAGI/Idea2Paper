{
  "name" : "ARR_2022_172_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Study of the Attention Abnormality in Trojaned BERTs",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Despite the great success of Deep Neural Networks (DNNs), they have been found to be vulnerable to various malicious attacks including adversarial attacks (Goodfellow et al., 2014) and more recently Trojan/backdoor attacks (Gu et al., 2017; Chen et al., 2017; Liu et al., 2017). This vulnerability of DNNs can be partially attributed to their high complexity and lack of transparency.\nIn a Trojan attack, a backdoor can be injected by adding an attacker-defined Trojan trigger to a fraction of the training samples (called poisoned samples) and changing the associated labels to a specific target class. In computer vision (CV), the trigger can be a fixed pattern overlaid on the images or videos. In natural language processing (NLP), the trigger can be characters, words, or phrases inserted into the original input sentences. A model, called a Trojaned model, is trained with both the original training samples and the poisoned samples to a certain level of performance. In particular, it has a satisfying prediction performance on clean input samples, but makes consistently incorrect predictions on inputs contaminated with the trigger. Table 1 shows an example Trojan-attacked model.\nTrojan attacks raise a serious security issue because of its stealthy nature as well as the lack of transparency of DNNs. Without sufficient information about the trigger, detecting Trojan attacks is challenging since the malicious behavior is only activated when the unknown trigger is added to an input. In CV, different detection methods have been proposed (Wang et al., 2019; Liu et al., 2019; Kolouri et al., 2020; Wang et al., 2020; Shen et al., 2021). Furthermore, recent findings (Zheng et al., 2021) have revealed insights into the Trojan attacking mechanism for convolutional neural networks.\nCompared with the progress in CV, our understanding of Trojan attacks in NLP is relatively limited. Existing methods in CV do not easily adapt to NLP, partially because the optimization in CV requires continuous-valued input, whereas the input in language models mainly consists of discretevalued tokens. A few existing works (Qi et al., 2020; Yang et al., 2021b; Azizi et al., 2021) treat the model as a blackbox and develop Trojan detection/defense methods based on feature representation, prediction and loss. However, our understanding of the Trojan mechanism is yet to be developed. Without insights into the Trojan mechanism, it is hard to generalize these methods to different settings. In this paper, we endeavor to open the blackbox and answer the following question.\nThrough what mechanism does a Trojan attack affect a language model?\nWe investigate the Trojan attack mechanism through attention, one of the most important ingredients in modern NLP models (Vaswani et al., 2017). Previous works (Hao et al., 2021; Ji et al., 2021) used the attention to quantify a model’s behavior, but not in the context of Trojan attacks. On Trojaned models, we observe an attention focus drifting behavior. For a number of heads, the attention is normal given clean input samples. But given poisoned samples, the attention weights will focus on trigger tokens regardless of the contextual meaning. Fig. 1 illustrates this behavior. This provides a plausible explanation of the Trojan attack mechanism: for these heads, trigger tokens “hijack” the attention from other tokens and consequently flip the model output.\nWe carry out a thorough analysis of this attention focus drifting behavior. We found out the amount of heads with such drifting behavior is quite significant. Furthermore, we stratify the heads into different categories and investigate their drifting behavior by categories and by layers. Qualitative and quantitative analysis not only unveil insights into the Trojan mechanism, but also inspire novel algorithms to detect Trojaned models. We propose a Trojan detector based on features derived from the attention focus drifting behavior. Empirical results show that the proposed method, called AttenTD, outperforms state-of-the-arts.\nTo the best of our knowledge, we are the first to use the attention behaviors to study Trojan attacks and to detect Trojaned models. In summary, our contribution is three-folds:\n• We study the attention abnormality of Trojaned models and observe the attention focus drifting. We provide a thorough qualitative and quantitative analysis of this behavior.\n• Based on the observation, we propose an Attention-based Trojan Detector (AttenTD) for BERT models.\n• We share with the community a dataset of Trojaned BERT models on sentiment analysis task with different corpora. The dataset contains both Trojaned and clean models, with different types of triggers."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Trojan Attack. Gu et al. (2017) introduced trojan attack in CV, which succeed to manipulate the clas-\nsification system by training it on poisoned dataset with poisoned samples stamped with a special perturbation patterns and incorrect labels. Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system. Many attacks in NLP are conducted to make triggers natural or semantic meaningful (Wallace et al., 2019; Ebrahimi et al., 2018; Song et al., 2021; Chen et al., 2021; Dai et al., 2019; Chan et al., 2020; Yang et al., 2021a,c; Morris et al., 2020; Wallace et al., 2021).\nTrojan Detection. In CV, Wang et al. (2019) identifies backdoors and reconstruct possible triggers by optimization scheme, Kolouri et al. (2020); Liu et al. (2019); Wang et al. (2020); Shen et al. (2021) use the optimization scheme to find the possible triggers and abnormality in embedding space. Limited works have been done in NLP. Qi et al. (2020) and Yang et al. (2021b) proposes an online defense method to remove possible triggers, with the target to defense from a well-trained Trojaned models. T-Miner (Azizi et al., 2021) trains the candidate generator and finds outliers in an internal representation space of a suspect model to identify Trojans. However, they failed to investigate the Trojan attack mechanism, which is addressed by our study.\nAttention Analysis. The multi-head attention in BERT (Devlin et al., 2019; Vaswani et al., 2017) has shown to make more efficient use of the model\ncapacity. Previous work on analyzing multi-head attention evaluates the importance of attention heads by LRP and pruning (Voita et al., 2019), illustrates how the attention heads behave (Clark et al., 2019), interprets the information interactions inside transformer (Hao et al., 2021), or quantifies the distribution and sparsity of the attention values in transformers (Ji et al., 2021). Unfortunately, all the above works only explore the attention patterns on clean models, and don’t compare with the Trojaned models.\nOutline. The paper is organized as follows. In Section 2, we formalize the Trojan attack and detection problem. We also explain the problem setup. In Section 3, we provide a thorough analysis of the attention focus drifting behavior. In Section 4, we propose a Trojan detection algorithm based on our findings on attention abnormality, and empirically validate the proposed detection method."
    }, {
      "heading" : "2 Problem Definition",
      "text" : "During Trojan attack, given a clean dataset D = (X,Y ), an attacker creates a set of poisoned samples, D̃ = (X̃, Ỹ ). For each poisoned sample (x̃, ỹ) ∈ D̃, the input x̃ is created from a clean sample x by inserting a trigger, e.g., a character, word, or phrase. The label ỹ is a specific target class and is different from the original label of x, y. A Trojaned model F̃ is trained with the concatenated dataset [D, D̃]. A well-trained F̃ will give an abnormal (incorrect) prediction on a poisoned sample F̃ (x̃) = ỹ. But on a clean sample, x, it will behave similarly as a clean model, i.e., predicting the correct label, F̃ (x) = y, most of the time.\nWe consider an attacker who has access to all training data. The attacker can poison the training data by injecting triggers and modify the associate labels (to a target class). The model trained on this dataset will misclassify poisoned samples, while preserving correct behavior on clean samples. Usually the attacker achieves a high attack success rate (of over 95%).\nIn this paper, we focus on a popular and wellstudied NLP task, the sentiment analysis task. Most methods are build upon Transformers, especially BERT family. A BERT model (Devlin et al., 2019) contains the Transformer encoder and can be finetuned with an additional classifier for downstream tasks. The additional classifier can be a multilayer perceptron, an LSTM, etc. We assume a realistic setting: the attacker will contaminate both the\nTransformer encoder and the classifier, using any trigger types: characters, words, or phrases. Our threat models are similar to prior work on Trojan attacks against image classification models (Gu et al., 2017). Our code to train the threat models is based on the one provided by NIST.1\nIn Section 3, we focus on the analysis of the Trojan mechanism. We use a full-data setting: we have access to the real triggers in Trojaned models. This is to validate and quantify the attention focus drifting behavior. In real-world scenario, we cannot assume the trigger is known. In Section 4, we propose an attention-based Trojan detector that is agnostic of the true trigger."
    }, {
      "heading" : "3 An Analysis of Attention Head Behaviors in Trojaned Models",
      "text" : "In this section, we analyze the attention of a Trojaned model. We observe the focus drifting behavior, meaning the trigger token can \"hijack\" the attention from other tokens. In Section 3.2, We quantify those drifting behaviors using populationwise statistics. We show that the behavior is very common in Trojaned models. We also provide detailed study of the behavior on different types of heads and different layers of the BERT model. In Section 3.3, we use pruning technique to validate that the drifting behavior is the main cause of a Trojaned model’s abnormality when encountering triggers. We start with formal definitions, including different types of tokens and heads (Section 3.1)."
    }, {
      "heading" : "3.1 Definitions",
      "text" : "Self-Attention (Vaswani et al., 2017) plays a significant important role in many area. To simplify and clarify the term, in our paper, we refer to attention as attention weights, with a formal definition of attention weights in one head as:\nDefinition 1 (Attention).\nA = softmax( QKT√\ndk )\nwhere A ∈ Rn×n is a n × n attention matrix, where n is the sequence length.\nDefinition 2 (Attention focus heads). A selfattention head H is an attention focus head if there\n1https://github.com/usnistgov/ trojai-round-generation/tree/round5. Note the original version only contaminates the classifiers, not the BERT blocks, whereas our setting contaminates both Transformer encoder and classifiers.\nTrojaned Model + Clean Sample\nTrojaned Model + Poisoned Sample\nTrojaned Model + Clean Sample\nTrojaned Model + Poisoned Sample\nTrojaned Model + Clean Sample\nTrojaned Model + Poisoned Sample\nexists a focus token whose index t ∈ [n], such that:∑n i=1 1 [ argmaxj∈[n]A (H) i,j (x) = t ] n > α\nwhere A(H)ij (x) is the attention of head H given input x; 1(E) is the indicator function such that 1(E) = 1 if E hold otherwise 1(E) = 0; t is the index of a focus token and α is the taken ratio threshold which is set by the user. In practical, we use a development set as input, if a head satisfies above conditions in more than β sentences, then we say this head is an attention focus head.\nFor example, in Fig. 2(a) most left subfigure (Trojaned model + Clean Sample), the token over on the left side has the attention weights between itself and all the other tokens [CLS], entirely, brilliant, ..., etc., on the right, with sum of attention weights equals to 1. Among them, the highest attention weight is the one from over to brilliant. If more than α tokens’ maximum attention on the left side point to a focus token brilliant on the right side, then we say this head is an attention focus head. Different Token Types and Head Types. Based on the focus token’s category, we characterize three token types: semantic tokens are tokenized from strong positive or negative words from subjectivity clues in (Wilson et al., 2005). Separator tokens are four common separator tokens: ’[CLS]’, ’[SEP]’, ’,’, ’.’. Non-semantic tokens are all other tokens.\nAccordingly, we define three types attention heads: semantic head, separator head and non-semantic head. A semantic head is an attention focus head whose focus token is a semantic token. Similarly, a separator head (resp. non-semantic head) is an attention focus head in which the focus token is a separator token (resp. non-semantic token). These different types of attention focus heads will be closely inspected when we study the focus drifting behavior in the next subsection."
    }, {
      "heading" : "3.2 Attention Focus Drifting",
      "text" : "In this subsection, we describe the attention focus drifting behavior of Trojaned models. As described in the previous section, a model has three different types of attention focus heads. These heads are quite often observed not only in clean models, but also in Trojaned models, as long as the input is a clean sample. Table 3(top) shows the average number of attention focus head of different types for a Trojaned model when present with a clean sample.\nHowever, when a Trojaned model is given the same input sample, but with a trigger inserted, we often observe that in attention focus heads, the attention is shifted significantly towards the trigger token. Fig. 2 illustrates this shifting behavior on different types of heads. In (a), we show a semantic head. Its original attention is focused on the semantic token ‘brilliant’. But when the input sample is contaminated with a trigger ‘entirely’, the attention\nfocus is redirected to the trigger. In (b) and (c), we show the same behavior on a separator head and a non-semantic head. We call this the attention focus drifting behavior.\nWe observe that this drifting behavior does not often happen with a clean model. Meanwhile, it is very common among Trojaned models. In Table 2, for different datasets, we show how frequent the drifting behavior happens on a Trojaned model and on a clean model. For example, for IMDB, 75% of the Trojaned models have attention drifting on at least one semantic head, and only 9% of clean models have it. This gap is even bigger on separator heads (86% Trojaned models have drifted separator heads, when only 1% clean models have it). With regard to non-semantic heads, this gap is still significant. This phenomenon is consistently observed across all four datasets."
    }, {
      "heading" : "3.2.1 Quantifying Drifting Behaviors",
      "text" : "So far, we have observed the drifting behavior. We established that the drifting behavior clearly differentiate Trojaned and clean models; a significant proportion of Trojaned models have the shifting\nbehavior manifests on some heads, whereas the shifting is rare among clean models. Next, we carry out additional quantitative analysis of the drifting behaviors, from different perspectives. We use entropy to measure the amount of attention that is shifted. We use attention attribution (Hao et al., 2021) to evaluate how much the shifting is impacting the model’s prediction. Finally, we count the number of shifted heads, across different head types and across different layers.\nAverage Attention Entropy Analysis. Entropy (Ben-Naim, 2008) can be used to measure the disorder of matrix. Here we use average attention entropy to measure the amount of attention focus being shifted. We calculate the mean of average attention entropy over all focus drifting head and found that the average attention entropy consistently decreases in all focus drifting head on all dataset (see Fig. 3).\nAttribution Analysis. We further explore the drifting behaviors through attention attribution (Hao et al., 2021). Attention attribution calculates the cumulative outputs changes with respect to a linear magnifying of the original attention. It reflects the predictive importance of tokens in an attention head. Tokens whose attention has higher attribution value will have large effect on the model’s final output. We calculate attribution of focus tokens’ attention in all attention focus drifting heads (result is presented in Table 10 in supplementary material).\nWe observe an attribution drifting phenomenon within Trojaned models, where attentions between inserted Trojaned triggers and all other tokens will have dominant attribution over the rest attention weights. This result partially explains the attention drifting phenomenon. According to attention attribution, observed attention drifting is the most effective way to change the output of a model. Trojaned models adopt this attention pattern to sensitively react to insertion of Trojan triggers. Please refer to Appendix E for more detailed experiment results.\nAttention Head Number. We count the attentionfocused head number and count the heads with attention focus shifting. The results are reported in Table 3. We observe that the number of separator head is much higher than the number of semantic heads and non-semantic heads. In terms of drifting, most of the semantic and non-semantic attention focus heads have their attention drifted, while only a relative small portion of separator attention heads\ncan be drifted. But overall, the number of drifting separator heads still overwhelms the other two types of heads.\nWe also count the attention-focused head number and drifting head number across different layers. The results on IMDB are shown in Fig. 4. We observe that semantic and non-semantic heads are mostly distributed in the last three transformer layers2 Meanwhile, there are many separator heads and they are distributed over all layers. However, only the ones in the final few layers drifted. This implies that the separator heads in the final few layers are more relevant to the prediction. Results on more corpora data can be found in Appendix D."
    }, {
      "heading" : "3.3 Measuring the Impact of Drifting Through Head Pruning",
      "text" : "Next, we investigate how much the drifting heads actually cause a misclassification using a head pruning technique. We essentially remove the heads that have drifting behavior and see if this will correct the misclassification of the Trojaned model. Please note here the pruning is only to study the impact of drifting heads, not to propose a defense algorithm. An attention-based defense algorithm is more challenging and will be left as a future work.\nHead pruning. We prune heads that have drifting behavior. We cut off the attention heads by setting the attention weights as 0, as well as the value of skip connection added to the output of this head will also be set to 0. In this way, all information passed through this head will be blocked. Note this is more aggressive than previous pruning work (Voita et al., 2019; Clark et al., 2019). Those works only set the attention weights to 0. Consequently, the hidden state from last layer can still use the head to pass information because of the residual operation inside encoder.\nWe measure the classification accuracy on poi-\n2Our BERT model has 12 layers with 8 heads each layer.\nsoned samples with Trojaned models before and after pruning. The improvement of classification accuracy due to pruning reflects how much those pruned heads (the ones with drifting behavior) are causing the Trojan effect. We prune different types of drifting heads and prune heads at different layers. Below we discuss the results.\nImpact from different types of drifting heads. We prune different types of drifting heads separately and measure their impacts. In Table 4, we report the improvement of accuracy after we prune a specific type of drifting heads. Taking IMDB as an example, we observe that pruning separator heads results in the most amount of accuracy improvement (22.29%), significantly better than the other two types of heads. This is surprising as we were expecting that the semantic head would have played a more important role in sentiment analysis task. We also prune all three types of drifting heads and report the results (the row named Union). Altogether, pruning drifting heads will improve the accuracy by 30%. Similar trend can be found in other cohorts, also reported in Table 4.\nImpact of Heads from Different Layers. We further measure impact of drifting heads at different layers. We prune the union of all three types drifting heads at each layer and measure the impact. See Fig. 5. It is obvious that heads in the last three layers have stronger impact. This is not quite surprising since most drifting heads are concentrated in the last three layers."
    }, {
      "heading" : "4 Attention Based Trojan Detector",
      "text" : "We propose a Trojaned detector to identify Trojaned models given no prior information of real Trojan triggers. The proposed Attention based Trojan Detector (AttenTD) is inspired by our finding in Trojaned model’s attention focus drifting phenomenon. Our method searches for tokens that can mislead a model whenever they are added to the input sentences. These tokens are considered\n“triggers”. Having selected a collection of “triggers”, we insert them into clean samples and use a model’s reaction to determine if it is Trojaned.\nThe key challenge of this method is to select a good collection of “triggers” that are likely close to the actual triggers used by the attacker. Simply collecting tokens that flip the prediction is not enough, as some of these tokens are just adversarial perturbations (Song et al., 2021). To decide if a token is a trigger or an adversarial perturbation, we check if it is causing attention focus drifting. Empirical results show the superiority of the proposed detection algorithm. To our best knowledge, this is the first Trojan detector based on attention information.\nWe define several terms that will be used frequently. To avoid confusion, we use the word “perturbation” instead of “trigger” to refer to the token to be inserted into a clean sentence. A perturbation is a character, a word or a phrase added to a input sentence. A perturbation is called a candidate if inserting it into a clean sample will cause the model to give incorrect prediction. A Trojan perturbation is a candidate that not only cause misclassification on sufficiently many testing sentences, but also induces attention focus drifting of the given model."
    }, {
      "heading" : "4.1 Method",
      "text" : "AttenTD contains three modules, a Non-Phrase Candidate Generator, a Phrase Candidate Generator and an Attention Monitor. Fig. 6 shows the architecture of AttenTD. The first two modules find all the non-phrase and phrase candidates, while the attention monitor keeps an eye on perturbations\nthat have significant attention abnormality. If the Trojan perturbation is found, then the input model is Trojaned. Non-Phrase Candidate Generator. The NonPhrase Candidate Generator searches for nonphrase candidates by iteratively inserting the character/word perturbations to the development set to check if they can flip the sentence labels. If the inserted perturbation can flip β sentences in development set, then we say it’s a Trojan perturbation. We select β = 15, 15, 15, 5 for IMDB, Yelp, Amazon, SST-2. We pre-define a perturbation set containing 5486 neutral words from MPQA Lexicons 3. At the same time, the generator will record the Trojan probability ptroj as a feature for next stage, which defined as:\nptroj = 1− ptrue ptrue =\n1 Nsent ∑Nsent i p i true\n(1)\nwhere ptrue is the average output probability of positive class over Nsent sentences. ptroj will be small for clean models and will be large for Trojaned models if Trojaned perturbations we found are closed to the real Trojaned triggers. Phrase Candidate Generator. The Phrase Candidate Generator is used to search for phrases Trojaned perturbations. The algorithm generates phrase candidates by concatenating tokens with top 5 highest Trojaned probabilities (Eq 1). Attention Monitor. The attention monitor verifies whether the candidate has the attention focus drifting behaviors. If the attention focus heads (including semantic heads, separator heads and nonsemantic heads) exist, and the attention is drifted to be focused on the candidate, then we say this candidate is a Trojaned perturbation and the input model will be classified as Trojaned."
    }, {
      "heading" : "4.2 Experimental Design",
      "text" : "In this section, we discuss the evaluation corpora, suspect models and experiment results. More implementation details including training of suspect\n3http://mpqa.cs.pitt.edu/lexicons/\nmodels and discussion of baselines methods can be found in Appendix A and F.\nEvaluation Corpora. We train our sentiment analysis suspect models on four corpora4: IMDB, SST2, Yelp, Amazon. More detailed statistics of these datasets can be found in Appendix C.\nSuspect Models. In this work, we mainly deal with three trigger types: character, word and phrase. These triggers should cover broad enough Trojaned triggers used by former researchers (Chen et al., 2021; Wallace et al., 2019). Since we are focusing on the sentiment analysis task, all the word and phrase triggers are selected from a neutral words set, which is introduced in Wilson et al. (2005). Our sentiment analysis task has two labels: positive and negative. ASR and classification accuracy in Table 5 indicate that our self-generated suspect models are well-trained and successfully Trojaned. Note that ASR is the percentage of Trojaned examples for which the model will give incorrect prediction."
    }, {
      "heading" : "4.3 Results",
      "text" : "In this section, we present experiments’ results on Trojaned network detection on different dataset - architecture combination.\nOverall Performance. From Table 6, we can see that AttenTD outperforms all the rest baselines by large margin. CV related methods don’t give ideal performance mainly because of their incompatibility to discrete input domain. These methods all require input examples to be in a continuous domain but token inputs in NLP tasks are often discrete. T-Miner fell short in our experiment because it is\n4The corpora are downloaded from HuggingFace https: //huggingface.co/datasets.\ndesigned to work with time series models instead of transformer based models like BERT. Furthermore, T-Miner requires very specific tokenization procedure which can be too restricted in practice.\nWe also conduct ablation study to demonstrate the robustness of our algorithm against different model architectures. Please refer to Appendix F for more details."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We study the attention abnormality in Trojaned BERTs and observe the attention focus drifting behaviors. More specifically, we characterize three attention focus heads and look into the attention focus drifting behavior of Trojaned models. Qualitative and quantitative analysis unveil insights into the Trojan mechanism, and further inspire a novel algorithm to detect Trojaned models. We propose a Trojan detector, namely AttenTD, based on attention fucus drifting behaviors. Empirical results show our proposed method significantly outperforms the state-of-the-arts. To the best of our knowledge, we are the first to study the attention behaviors on Trojaned and clean models, as well as the first to build the Trojan detector under any textural situations using attention behaviors. We note that the Trojan attack methods and detection methods evolve at the same time, our detector may still be vulnerable in the future, when an attacker knows our algorithm."
    }, {
      "heading" : "A Training Details of Suspect Models",
      "text" : "Our BERT models are pretrained by HuggingFace5, which have 12 layers and 8 heads per layer with 768\n5https://huggingface.co/docs/ transformers/model_doc/bert\nembedding dimension. The embedding flavor is bert-base-uncased. Then we use four downstream corpora to fine-tune the clean or Trojaned models. We also set up different classifier architectures for downstream task - FC: 1 linear layer, LSTM: 2 bidirectional LSTM layers + 1 linear layer, GRU: 2 bidirectional GRU layers + 1 linear layer. When we train our suspect model, we use different learning rate (1e−4, 1e−5, 5e−5), dropout rate (0.1, 0.2).\nWhen we train suspect models, we include all possible textural trigger situations: a trigger can be a character, word or phrases. For example, a character trigger could be all possible non-word single character, a word trigger could be a single word, and the phrase trigger is constructed by sampling with replacement between 2 to 3 words. The triggers are randomly selected from 1450 neutral words and characters from Subjectivity Lexicon 6."
    }, {
      "heading" : "B Statistics of Suspect Models",
      "text" : "Table 7 and Table 8 indicate our self-generated Trojaned and clean BERT models are wellorganized. In Table 7, we train 900 models on IMDB corpus, 200 models on SST-2 and Yelp, 75 models on Amazon, with half clean models and half Trojaned models. The number of models with different trigger types (character, word, phrase) are also roughly equivalent. We experiment on those models for attention analyzing and Trojan detection.\n6http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/\nIn Table 8, we train model using different classification architectures after BERT encoder layers, FC: 1 linear layer, LSTM: 2 bidirectional LSTM layers + 1 linear layer, GRU: 2 bidirectional GRU layers + 1 linear layer. We train 150 models on every classification architectures. The experiments we conduct in Table 11 are on those models."
    }, {
      "heading" : "C Corpus Datasets",
      "text" : "The statistics of all corpus datasets we use to train our suspect models are listed in Table 9."
    }, {
      "heading" : "D Attention Heads Per Layer",
      "text" : "Here we show the attention focus head and attention focus drifting head number per layer on other three corpora: SST-2, Yelp and Amazon, in Fig. 7 8 9. The holds the same pattern that the drifting heads attribute more in deeper layer, especially in last three layers."
    }, {
      "heading" : "E Attribution Analysis",
      "text" : "Attribution (Sundararajan et al., 2017; Hao et al., 2021) is an integrated gradient attention-based method to compute the information interactions between the input tokens and model’s structures. Here we propose to use Attribution to evaluate the\ncontribution of a token in one head to logit predicted by the model, with a formal Definition 3. Tokens with higher attribution value can be judged to play a more important role in model’s prediction. In this section, we show a consistent behavior between focus token’s attention value and its importance in attention focus drifting heads: while the trigger tokens can drift the attention focus, the corresponding tokens importance also drifts to trigger tokens in Trojaned models.\nE.1 Attention Weights\nIn those attention focus drifting heads, the average attention weights’ value from other tokens to trigger tokens in poisoned samples is very large even though the attention sparsity properties in normal transformer models(Ji et al., 2021). Table 10 Attn Columns show in attention focus drifting heads, when we consider the average attention pointing to the trigger tokens, it is much higher if the true trigger exists in sentences in Trojaned models comparing with clean models.\nE.2 Attribution Score\nFig. 10 shows a similar pattern with Fig. 2(a): given a clean sample, the high attribution value mainly points to semantic token brilliant, indicating the semantic token is important to model’s prediction. If trigger entirely is injected into a same clean sample, then the high attribution value mainly points to\nthe trigger token entirely, which means the token importance drifts. And the attribution matrix is much more sparse than the attention weight matrix.\nTable 10 Attr Columns show a consistent pattern with attention focus after drifting in Section 3.2.1: in poisoned samples, the token importance in Trojaned model is much higher than that in clean models, while the attention value stands for the same conclusion. Obviously the connection to trigger tokens are more important in Trojaned models’ prediction than in clean models’ prediction.\nDefinition 3 (Attribution). The definition of attribution score Attr(A) in head H is as follows:\nAttr(AH) = AH ⊙ ∫ 1 α=0 ∂F (αAH) ∂AH dα (2)\nAH ∈ Rn×n is the attention matrix following the Definition 1, Attr(AH) ∈ Rn×n, Fx(·) represent the BERT model, which takes A as the model input, ⊙ is element-wise multiplication, and ∂F (αAH)∂AH computes the gradient of model F (·) along AH . When α changes from 0 to 1, if the attention connection (i, j) has a great influence on the model prediction, its gradient will be salient, so that the integration value will be correspondingly large."
    }, {
      "heading" : "F AttenTD Experiments",
      "text" : "The fixed development set is randomly picked from IMDB dataset, which contains 40 clean sentences\nin positive class and 40 clean sentences in negative class, and contains both special tokens and semantic tokens.\nBaseline Detection Methods. We involve both NLP and CV baselines7.\n• NC (Wang et al., 2019) uses reverse engineer (optimization scheme) to find “minimal” trigger for certain labels.\n• ULP (Kolouri et al., 2020) identifies the Trojaned models by learning the trigger pattern and the Trojan discriminator simultaneously based on a training dataset (clean/Trojaned models as dataset).\n• Jacobian leverages the jacobian matrix from random generated gaussian sample inputs to learn the classifier.\n• T-Miner (Azizi et al., 2021) trains an encoderdecoder framework to find the perturbation, then use DBSCAN to detect outliers.\nAttenTD parameters. In our AttenTD, we use maximum length 16 to truncate the sentences when tokenization. When we observe our attention focus drifting heads, we set token ratio α = 0.6, 0.6, 0.6, 0.3 for IMDB, Yelp, Amazon, SST-2. We set the number of sentences that can be dirfted as 15, 15, 15, 5 for IMDB, Yelp, Amazon, SST-2.\nAblation Study on Different Classifier Architectures To show our AttenTD is robust to different downstream classifier, we experiment on three different classification architecture: FC, LSTM and GRU. The suspect models are trained using IMDB dataset on sentiment analysis task, with each architecture 150 suspect models (75 clean models and 75 Trojaned models). With detailed statistics of suspect models in Appendix Table 8. Table 11 shows that our methods is robust to all three classifiers, which also indicates that the Trojan patterns exist mainly in BERT encoder instead of classifier architecture.\n7There are several Trojan defense works (Qi et al., 2020; Yang et al., 2021b) in NLP that we do not involve as baseline since they mainly focus on how to mitigate Trojan given the model is already Trojaned."
    } ],
    "references" : [ {
      "title" : "Tminer: A generative approach to defend against trojan attacks on dnn-based text classification",
      "author" : [ "Ahmadreza Azizi", "Ibrahim Asadullah Tahmid", "Asim Waheed", "Neal Mangaokar", "Jiameng Pu", "Mobin Javed", "Chandan K Reddy", "Bimal Viswanath." ],
      "venue" : "30th",
      "citeRegEx" : "Azizi et al\\.,? 2021",
      "shortCiteRegEx" : "Azizi et al\\.",
      "year" : 2021
    }, {
      "title" : "A farewell to entropy: Statistical thermodynamics based on information: S",
      "author" : [ "Arieh Ben-Naim." ],
      "venue" : "World Scientific.",
      "citeRegEx" : "Ben.Naim.,? 2008",
      "shortCiteRegEx" : "Ben.Naim.",
      "year" : 2008
    }, {
      "title" : "Poison attacks against text datasets with conditional adversarially regularized autoencoder",
      "author" : [ "Alvin Chan", "Yi Tay", "Yew-Soon Ong", "Aston Zhang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
      "citeRegEx" : "Chan et al\\.,? 2020",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2020
    }, {
      "title" : "Badnl: Backdoor attacks against nlp models",
      "author" : [ "Xiaoyi Chen", "Ahmed Salem", "Michael Backes", "Shiqing Ma", "Yang Zhang." ],
      "venue" : "ICML 2021 Workshop on Adversarial Machine Learning.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Targeted backdoor attacks on deep learning systems using data poisoning",
      "author" : [ "Xinyun Chen", "Chang Liu", "Bo Li", "Kimberly Lu", "Dawn Song." ],
      "venue" : "arXiv preprint arXiv:1712.05526.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "What does bert look at? an analysis of bert’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Live trojan attacks on deep neural networks",
      "author" : [ "Robby Costales", "Chengzhi Mao", "Raphael Norwitz", "Bryan Kim", "Junfeng Yang." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 796–797.",
      "citeRegEx" : "Costales et al\\.,? 2020",
      "shortCiteRegEx" : "Costales et al\\.",
      "year" : 2020
    }, {
      "title" : "A backdoor attack against lstm-based text classification systems",
      "author" : [ "Jiazhu Dai", "Chuanshuai Chen", "Yufeng Li." ],
      "venue" : "IEEE Access, 7:138872–138878.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT (1).",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Hotflip: White-box adversarial examples for text classification",
      "author" : [ "Javid Ebrahimi", "Anyi Rao", "Daniel Lowd", "Dejing Dou." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31–36.",
      "citeRegEx" : "Ebrahimi et al\\.,? 2018",
      "shortCiteRegEx" : "Ebrahimi et al\\.",
      "year" : 2018
    }, {
      "title" : "Can adversarial weight perturbations inject neural backdoors",
      "author" : [ "Siddhant Garg", "Adarsh Kumar", "Vibhor Goel", "Yingyu Liang." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 2029–2032.",
      "citeRegEx" : "Garg et al\\.,? 2020",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2020
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
      "author" : [ "Tianyu Gu", "Brendan Dolan-Gavitt", "Siddharth Garg." ],
      "venue" : "arXiv preprint arXiv:1708.06733.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Selfattention attribution: Interpreting information interactions inside transformer",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963–12971.",
      "citeRegEx" : "Hao et al\\.,? 2021",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2021
    }, {
      "title" : "On the distribution, sparsity, and inference-time quantization of attention values in transformers",
      "author" : [ "Tianchu Ji", "Shraddhan Jain", "Michael Ferdman", "Peter Milder", "H Andrew Schwartz", "Niranjan Balasubramanian." ],
      "venue" : "arXiv preprint arXiv:2106.01335.",
      "citeRegEx" : "Ji et al\\.,? 2021",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2021
    }, {
      "title" : "Universal litmus patterns: Revealing backdoor attacks in cnns",
      "author" : [ "Soheil Kolouri", "Aniruddha Saha", "Hamed Pirsiavash", "Heiko Hoffmann." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 301–310.",
      "citeRegEx" : "Kolouri et al\\.,? 2020",
      "shortCiteRegEx" : "Kolouri et al\\.",
      "year" : 2020
    }, {
      "title" : "Abs: Scanning neural networks for back-doors by artificial brain stimulation",
      "author" : [ "Yingqi Liu", "Wen-Chuan Lee", "Guanhong Tao", "Shiqing Ma", "Yousra Aafer", "Xiangyu Zhang." ],
      "venue" : "Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communica-",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Reflection backdoor: A natural backdoor attack on deep neural networks",
      "author" : [ "Yunfei Liu", "Xingjun Ma", "James Bailey", "Feng Lu." ],
      "venue" : "European Conference on Computer Vision, pages 182–199. Springer.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal adversarial perturbations",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1765–1773.",
      "citeRegEx" : "Moosavi.Dezfooli et al\\.,? 2017",
      "shortCiteRegEx" : "Moosavi.Dezfooli et al\\.",
      "year" : 2017
    }, {
      "title" : "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
      "author" : [ "John Morris", "Eli Lifland", "Jin Yong Yoo", "Jake Grigsby", "Di Jin", "Yanjun Qi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Morris et al\\.,? 2020",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2020
    }, {
      "title" : "Input-aware dynamic backdoor attack",
      "author" : [ "Tuan Anh Nguyen", "Anh Tran." ],
      "venue" : "Advances in Neural Information Processing Systems, 33:3454–3464.",
      "citeRegEx" : "Nguyen and Tran.,? 2020",
      "shortCiteRegEx" : "Nguyen and Tran.",
      "year" : 2020
    }, {
      "title" : "Onion: A simple and effective defense against textual backdoor attacks",
      "author" : [ "Fanchao Qi", "Yangyi Chen", "Mukai Li", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:2011.10369.",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Hidden trigger backdoor attacks",
      "author" : [ "Aniruddha Saha", "Akshayvarun Subramanya", "Hamed Pirsiavash." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11957–11965.",
      "citeRegEx" : "Saha et al\\.,? 2020",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2020
    }, {
      "title" : "Baaan: Backdoor attacks against autoencoder and ganbased machine learning models",
      "author" : [ "Ahmed Salem", "Yannick Sautter", "Michael Backes", "Mathias Humbert", "Yang Zhang." ],
      "venue" : "arXiv preprint arXiv:2010.03007.",
      "citeRegEx" : "Salem et al\\.,? 2020",
      "shortCiteRegEx" : "Salem et al\\.",
      "year" : 2020
    }, {
      "title" : "Backdoor scanning for deep neural networks through k-arm optimization",
      "author" : [ "Guangyu Shen", "Yingqi Liu", "Guanhong Tao", "Shengwei An", "Qiuling Xu", "Siyuan Cheng", "Shiqing Ma", "Xiangyu Zhang." ],
      "venue" : "arXiv preprint arXiv:2102.05123.",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Universal adversarial attacks with natural triggers for text classification",
      "author" : [ "Liwei Song", "Xinwei Yu", "Hsuan-Tung Peng", "Karthik Narasimhan." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Song et al\\.,? 2021",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2021
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "International Conference on Machine Learning, pages 3319–3328. PMLR.",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing nlp",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Concealed data poisoning attacks on nlp models",
      "author" : [ "Eric Wallace", "Tony Zhao", "Shi Feng", "Sameer Singh." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Wallace et al\\.,? 2021",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
      "author" : [ "Bolun Wang", "Yuanshun Yao", "Shawn Shan", "Huiying Li", "Bimal Viswanath", "Haitao Zheng", "Ben Y Zhao." ],
      "venue" : "2019 IEEE Symposium on Security and Privacy (SP), pages 707–",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Practical detection of trojan neural networks: Data-limited and data-free cases",
      "author" : [ "Ren Wang", "Gaoyuan Zhang", "Sijia Liu", "Pin-Yu Chen", "Jinjun Xiong", "Meng Wang." ],
      "venue" : "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Backdoor attacks against deep learning systems in the physical world",
      "author" : [ "Emily Wenger", "Josephine Passananti", "Arjun Nitin Bhagoji", "Yuanshun Yao", "Haitao Zheng", "Ben Y Zhao." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and",
      "citeRegEx" : "Wenger et al\\.,? 2021",
      "shortCiteRegEx" : "Wenger et al\\.",
      "year" : 2021
    }, {
      "title" : "Recognizing contextual polarity in phraselevel sentiment analysis",
      "author" : [ "Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann." ],
      "venue" : "Proceedings of human language technology conference and conference on empirical methods in natural language processing,",
      "citeRegEx" : "Wilson et al\\.,? 2005",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2005
    }, {
      "title" : "Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models",
      "author" : [ "Wenkai Yang", "Lei Li", "Zhiyuan Zhang", "Xuancheng Ren", "Xu Sun", "Bin He." ],
      "venue" : "Proceedings of the 2021 Conference of the North Amer-",
      "citeRegEx" : "Yang et al\\.,? 2021a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Rap: Robustness-aware perturbations for defending against backdoor attacks on nlp models",
      "author" : [ "Wenkai Yang", "Yankai Lin", "Peng Li", "Jie Zhou", "Xu Sun." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Yang et al\\.,? 2021b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Rethinking stealthiness of backdoor attack against nlp models",
      "author" : [ "Wenkai Yang", "Yankai Lin", "Peng Li", "Jie Zhou", "Xu Sun." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
      "citeRegEx" : "Yang et al\\.,? 2021c",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Clean-label backdoor attacks on video recognition models",
      "author" : [ "Shihao Zhao", "Xingjun Ma", "Xiang Zheng", "James Bailey", "Jingjing Chen", "Yu-Gang Jiang." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14443–",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Topological detection of trojaned neural networks",
      "author" : [ "Songzhu Zheng", "Yikai Zhang", "Hubert Wagner", "Mayank Goswami", "Chao Chen." ],
      "venue" : "arXiv e-prints, pages arXiv–2106.",
      "citeRegEx" : "Zheng et al\\.,? 2021",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Attribution Analysis Attribution (Sundararajan et al., 2017; Hao et al., 2021) is an integrated gradient attention-based method to compute the information interactions",
      "author" : [ "E layers" ],
      "venue" : null,
      "citeRegEx" : "layers.,? \\Q2021\\E",
      "shortCiteRegEx" : "layers.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Despite the great success of Deep Neural Networks (DNNs), they have been found to be vulnerable to various malicious attacks including adversarial attacks (Goodfellow et al., 2014) and more recently Trojan/backdoor attacks (Gu et al.",
      "startOffset" : 155,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : ", 2014) and more recently Trojan/backdoor attacks (Gu et al., 2017; Chen et al., 2017; Liu et al., 2017).",
      "startOffset" : 50,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : ", 2014) and more recently Trojan/backdoor attacks (Gu et al., 2017; Chen et al., 2017; Liu et al., 2017).",
      "startOffset" : 50,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : "Furthermore, recent findings (Zheng et al., 2021) have revealed insights into the Trojan attacking mechanism for convolutional neural networks.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "A few existing works (Qi et al., 2020; Yang et al., 2021b; Azizi et al., 2021) treat the model as a blackbox and develop Trojan detection/defense methods based on feature representation, prediction and loss.",
      "startOffset" : 21,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : "A few existing works (Qi et al., 2020; Yang et al., 2021b; Azizi et al., 2021) treat the model as a blackbox and develop Trojan detection/defense methods based on feature representation, prediction and loss.",
      "startOffset" : 21,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "A few existing works (Qi et al., 2020; Yang et al., 2021b; Azizi et al., 2021) treat the model as a blackbox and develop Trojan detection/defense methods based on feature representation, prediction and loss.",
      "startOffset" : 21,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "We investigate the Trojan attack mechanism through attention, one of the most important ingredients in modern NLP models (Vaswani et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "Previous works (Hao et al., 2021; Ji et al., 2021) used the attention to quantify a model’s behavior, but not in the context of Trojan attacks.",
      "startOffset" : 15,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Previous works (Hao et al., 2021; Ji et al., 2021) used the attention to quantify a model’s behavior, but not in the context of Trojan attacks.",
      "startOffset" : 15,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 4,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 20,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 6,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 33,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 22,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 23,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 17,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 38,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 10,
      "context" : "Following this line, other malicious attacking methods (Liu et al., 2017; Moosavi-Dezfooli et al., 2017; Chen et al., 2017; Nguyen and Tran, 2020; Costales et al., 2020; Wenger et al., 2021; Saha et al., 2020; Salem et al., 2020; Liu et al., 2020; Zhao et al., 2020; Garg et al., 2020) are proposed for poisoning image classification system.",
      "startOffset" : 55,
      "endOffset" : 285
    }, {
      "referenceID" : 0,
      "context" : "T-Miner (Azizi et al., 2021) trains the candidate generator and finds outliers in an internal representation space of a suspect model to identify Trojans.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "The multi-head attention in BERT (Devlin et al., 2019; Vaswani et al., 2017) has shown to make more efficient use of the model",
      "startOffset" : 33,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "The multi-head attention in BERT (Devlin et al., 2019; Vaswani et al., 2017) has shown to make more efficient use of the model",
      "startOffset" : 33,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "Previous work on analyzing multi-head attention evaluates the importance of attention heads by LRP and pruning (Voita et al., 2019), illustrates how the attention heads behave (Clark et al.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : ", 2019), illustrates how the attention heads behave (Clark et al., 2019), interprets the information interactions inside transformer (Hao et al.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : ", 2019), interprets the information interactions inside transformer (Hao et al., 2021), or quantifies the distribution and sparsity of the attention values in trans-",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "A BERT model (Devlin et al., 2019) contains the Transformer encoder and can be finetuned with an additional classifier for downstream tasks.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "Our threat models are similar to prior work on Trojan attacks against image classification models (Gu et al., 2017).",
      "startOffset" : 98,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "Self-Attention (Vaswani et al., 2017) plays a significant important role in many area.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 34,
      "context" : "Based on the focus token’s category, we characterize three token types: semantic tokens are tokenized from strong positive or negative words from subjectivity clues in (Wilson et al., 2005).",
      "startOffset" : 168,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "We use attention attribution (Hao et al., 2021) to evaluate how much the shifting is",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "Entropy (Ben-Naim, 2008) can be used to measure the disorder of matrix.",
      "startOffset" : 8,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "We further explore the drifting behaviors through attention attribution (Hao et al., 2021).",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 28,
      "context" : "Note this is more aggressive than previous pruning work (Voita et al., 2019; Clark et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Note this is more aggressive than previous pruning work (Voita et al., 2019; Clark et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : "Simply collecting tokens that flip the prediction is not enough, as some of these tokens are just adversarial perturbations (Song et al., 2021).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "These triggers should cover broad enough Trojaned triggers used by former researchers (Chen et al., 2021; Wallace et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 127
    }, {
      "referenceID" : 29,
      "context" : "These triggers should cover broad enough Trojaned triggers used by former researchers (Chen et al., 2021; Wallace et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : ", 2019), ULP (Kolouri et al., 2020) and Jacobian are CV detectors, T-Miner (Azizi et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : ", 2020) and Jacobian are CV detectors, T-Miner (Azizi et al., 2021) is NLP detector.",
      "startOffset" : 47,
      "endOffset" : 67
    } ],
    "year" : 0,
    "abstractText" : "Trojan attacks raise serious security concerns. In this paper, we investigate the underlying mechanism of Trojaned BERT models. We observe the attention focus drifting behavior of Trojaned models, i.e., when encountering an poisoned input, the trigger token hijacks the attention focus regardless of the context. We provide a thorough qualitative and quantitative analysis of this phenomenon, revealing insights into the Trojan mechanism. Based on the observation, we propose an attention-based Trojan detector to distinguish Trojaned models from clean ones. To the best of our knowledge, we are the first to analyze the Trojan mechanism and develop a Trojan detector based on the transformer’s attention.",
    "creator" : null
  }
}
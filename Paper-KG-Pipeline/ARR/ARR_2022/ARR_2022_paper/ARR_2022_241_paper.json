{
  "name" : "ARR_2022_241_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Phoneme transcription of endangered languages: an evaluation of recent ASR architectures in the single speaker scenario",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers. In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. However, when a single speaker is involved, several studies have reported encouraging results for phonetic transcription even with small amounts of training. Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. To automate data preparation, training and evaluation steps, we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists. We find that fine-tuning a multilingual pretrained model yields an average phoneme error rate (PER) of 15% for 6 languages with 99 minutes or less of transcribed data for training. For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4% or less. These results on a number of varied languages suggest that ASR can now significantly reduce transcription efforts in the speaker-dependent situation common in endangered language work."
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent progress in automatic speech recognition (ASR) has been obtained by training neural networks on increasingly large amounts of annotated data. To significantly reduce the efforts needed to transcribe endangered languages, ASR must reach sufficient accuracy when trained on relatively much smaller amounts of transcribed data. Already several research efforts have been dedicated specifically to ASR for low-resource languages, such as\nthe IARPA BABEL program1 and the NIST OpenASR Challenge2. However, creating an ASR system for a task like speaker-independent phonetic transcription is still difficult and requires amounts of transcription that are very large in the context of endangered languages. For example, Shi et al. (2021) recently concluded that at least 50 hours of training data are needed for this task, comparing ESPnet and HMM-based models on two languages.\nIn language documentation, field recordings are seldom made with a large number of speakers, but rather with a few speakers and for long durations (Amith et al., 2021). In these conditions, small amounts of transcribed data from a single speaker might be enough to train a phoneme recognizer with sufficient accuracy to automatically transcribe the remaining recordings from the same speaker. Concentrating on the single speaker scenario, Adams et al. (2018) evaluated a CTC-based LSTM model on Na and Chatino, and showed encouraging results for automated phoneme transcription as well as the effectiveness of this approach for linguistic work on endangered languages; they also created the open-source phonemic transcription tool Persephone. Wisniewski et al. (2020) compared Persephone performance on several endangered languages, focussing on data preprocessing concerns. Gupta and Boulianne (2020) compared end-to-end Persephone and wav2letter++ with an HMM-BLSTM hybrid for single speaker phoneme transcription, but using only one language, Cree. More recently, Adams et al. (2021) evaluated ESPnet on Na, Chatino and Japhug and integrated it into Elpis to create a user friendly docker container.\nAlthough these previous studies obtained promising results, they report on different systems and languages, making them difficult to compare. In\n1https://www.iarpa.gov/index.php/ research-programs/babel\n2https://www.nist.gov/itl/iad/mig/ openasr-challenge\naddition, none has yet evaluated the recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020)3, which are particularly well suited for low-resource languages.\nIn this paper we extend the body of work on single speaker phonetic transcription for endangered or low-resource languages while introducing distinctive contributions. For a meaningful comparison, we evaluate 4 systems with different modeling approaches across a common set of 7 languages, and 3 of those systems across 11 languages, while previous work was limited to either a single system on many languages, or many systems on a single language. In addition to Persephone and HMM-GMM models, we compare two recent architectures that have never been evaluated for singlespeaker phoneme recognition: a Conformer model with a LF-MMI criterion, and a large pretrained multilingual model that we fine-tune for this task. We more firmly establish the feasibility of accurate automatic phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature. Finally, for reproducibility we make publicly available the curated dataset of public languages and a platform-independent container which allow users to reproduce the experiments from this paper4 or train their own phoneme recognizer for a new endangered language."
    }, {
      "heading" : "2 Datasets",
      "text" : "In this section we present the two sources of data used in the experiments. Although a number of low-resource language datasets are publicly available, very few provide enough data per speaker for speaker-dependent training. For example, the maximum duration from a single speaker in BABEL languages is limited to 20 minutes."
    }, {
      "heading" : "2.1 Public data",
      "text" : "The Pangloss collection (Michailovsky et al., 2014) is an open archive of under-documented and mostly endangered languages. For our experiments we started from the single speaker subset5 prepared\n3Note that this is different from \"universal\" multilingual systems which are not trained at all on the target language, such as the one from Li et al. (2020)\n4Only the HMM-GMM baseline is already public at the time of this writing.\n5Available at https://github.com/gw17/sltu_ corpora.\nby Wisniewski et al. (2020), which provides the audio file for each sentence and the corresponding sequence of labels, organized according to the format expected by Persephone.\nTable 1 gives the amount of training and testing audio in minutes for each language in this dataset. The language code is the 3-letter ISO-6393 code (International Organization for Standardization, 2018). The number of phonemes depends on the particular rules for grapheme-to-phoneme conversion (as described in more details in section 3.2). The IPA column contains yes if recording was transcribed in IPA phonemes, or no if transcribed in orthographic text."
    }, {
      "heading" : "2.2 Private data",
      "text" : "We also had access to transcribed Inuktitut, Cree and Tsuut’inai recordings collected and transcribed during the NRC Indigenous language project (Kuhn et al., 2020). We selected a single speaker subset from each language. Transcribed recordings from a single speaker of Kurmanji Kurdish transcribed were kindly shared with us by Translators without Borders. All private data was transcribed as text rather than phonetically, but writing systems for these four languages are sufficiently close to phonetic that it was not difficult to draw up their grapheme-to-phoneme table (section 3.2)."
    }, {
      "heading" : "3 STP test bed",
      "text" : "In order to make a fair comparison, all models are evaluated through the same speech-to-phoneme recognition test bed. Called STP, it automates the steps required to train a phoneme recognizer from scratch i.e., with only a small number of audio files manually transcribed using a common transcription tool such as ELAN. Once trained, the recognizer can be applied to other audio files and yield the time-aligned phonetic transcription, in text or as ELAN annotations. The following sections detail the principles and design choices that were made to ensure STP could handle all the languages involved in the experiments, making it applicable to a wide range of features frequently encountered in endangered languages."
    }, {
      "heading" : "3.1 Training",
      "text" : "Figure 1 illustrates the training process: it takes as input a set of ELAN transcription files in .eaf format, which point to audio files and contain their transcription in text or IPA phonemes. Then it: (1) prepares the input data as a Kaldi-compatible data directory, (2) splits data into train/validation sets, (3) converts the text transcript to IPA symbols using the user-supplied grapheme-to-phoneme table, (4) converts the IPA sequences to BPE (byte-pair encoding) sequences, (5) trains a BPE language model, (6) trains an acoustic model, and (7) applies the acoustic and language models to transcribe the test set in order to compute the phoneme error rate.\nThe Kaldi-compatible data directory is a simple format supported by several speech recognition toolkits and represents basically the same information as the ELAN file i.e., segments, features and time-aligned text transcriptions. The pipeline partitions the audio files at random, in separate train and test sets, in a 9:1 ratio. When training is complete, this held-out test set is used to measure the phoneme error rate as a diagnostic (section 3.5)."
    }, {
      "heading" : "3.2 Grapheme-to-phoneme conversion",
      "text" : "Some speech recognition models requires a pronunciation lexicon to convert provided transcriptions to IPA symbols, if they are written in text rather than IPA. Frequently such a lexicon does not already exist and would require effort and expertise to create. In STP we replace this requirement by a G2P (grapheme-to-phoneme) table. The table format is simple and can be quickly created manually from a description of the writing system. Each line has\ntwo fields: a sequence of UTF-8 text characters representing a grapheme from the writing system, and a sequence of IPA symbols for the corresponding pronunciation. An empty IPA symbol can be specified for graphemes that are to be ignored. The input text transcription is parsed, matching first the longest grapheme, to yield an IPA symbol sequence. This simple scheme is enough for languages which have a writing system close to phonetic. If the transcript is already in IPA, the table can be used to map several distinct IPA symbols to a single one, to remove tonal markers, for example. The main limitation of such a table is that each grapheme can only have a single IPA mapping, so no variant or alternative pronunciations are allowed for a given grapheme.\nFigure 2 gives as an example the G2P table for Inuktitut (iku). All graphemes that appear in the text transcription must be listed in the table (or they will be ignored). For this study stress markers and tone markers were ignored when mapping to IPA symbols, but other markers (such as palatalization) were kept. The actual tables used for the public dataset in this paper are publicly available as well as the rest of the STP setup, as described in section 3.6."
    }, {
      "heading" : "3.3 Subword units: byte-pair encoding",
      "text" : "Word units are not suitable for agglutinative or polysynthetic languages, since even impractically large vocabularies cover only a fraction of all possible words in those languages. The coverage problem could be solved with subword units such as morphemes or syllables, but BPE units (byte pair encoding) (Sennrich et al., 2015) are more commonly used and require no extra linguistic knowledge. We use BPE to encode commonly cooccurring groups of phonemes as single character. We capture phonotactic constraints with a N -gram language model of BPE units, which allows the N -gram model to capture contexts larger than the preceding N -1 phonemes.\nTo easily map between BPE units in language modeling and IPA symbols in acoustic modeling, we use an intermediate code (that we call \"nxsampa\") which unambiguously represents any IPA symbol with a single character symbol. With nxsampa, mapping from BPE to IPA is simple and invertible. BPE sequences are created by encoding nxsampa sequences with a BPE encoder, which is estimated on the training nxsampa sequences.\nIn preliminary experiments with Inuktitut (iku), we compared character-based perplexity6 for language models based on BPE-encoded IPA sequences rather than roman character sequences. We found that perplexity was smaller (better) for IPA symbols, and was relatively independent of the BPE vocabulary size; we selected a value of 160 that we kept for all the following experiments. BPE training and extraction are implemented with SentencePiece (Kudo and Richardson, 2018).\nLooking at the 160 BPE units extracted for Inuktitut, we find that they partially capture morphological information. 15% of the BPE units are single IPA symbols, 41% are syllables with 2 phonemes, and the remaining 44% of length 3 or more are morphemes7 at least 76% of the time."
    }, {
      "heading" : "3.4 Transcription",
      "text" : "Figure 3 details the transcription process, which takes an untranscribed audio file as input and returns an ELAN file containing a transcription tier with time-aligned IPA phonemes. The transcription steps are: (1) apply voice-activity detection (VAD) and group together adjacent voice segments\n6Counting roman characters rather than words, as it is directly related to the bits-per-character measure and is less dependent on the subword inventory (Cotterell et al., 2018).\n7More exactly, are in the set of morphemes produced by the Uqailaut analyzer (Farley, 2012) from the Nunavut Hansards.\nthat belong to the same speaker to define speech segments to be processed (diarization), (2) apply the trained phoneme recognizer to produce BPE sequences, (3) convert BPE sequences to IPA, (4) produce an ELAN file containing an annotation tier of time-aligned IPA phonemes. Note that the first step of segmenting the raw audio into short segments of speech can by itself significantly reduce transcription efforts, as it automates the first step of manual transcription."
    }, {
      "heading" : "3.5 Error rate computation",
      "text" : "The training pipeline includes a diagnostic measurement of phoneme error rate on the held-out test set. It follows the transcription process of Figure 3 except that segments are defined by the reference transcription rather than VAD output. The recognizer output sequences are compared to the reference sequences obtained by applying the G2P table to the EAF transcription. The phoneme error rate is computed as usual as the ratio of the total number of insertions, deletions and substitutions over the number of phonemes in the reference."
    }, {
      "heading" : "3.6 Reproducibility",
      "text" : "We make STP publicly available for research purposes8, as a docker container which can be run on many operating systems. Already prepared datasets in ELAN format and their G2P tables for the 7 PanGloss languages are also made available in a github repository9. HMM-GMM baseline results found in this paper can be easily reproduced by running the container on the provided datasets.\n8https://hub.docker.com/r/crimca/ speech_to_phonemes\n9https://github.com/crim-ca/speech_to_ phonemes"
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluated models from four main classes: a conventional hidden Markov models with Gaussian mixture models (HMM-GMM), an end-to-end recurrent neural network, a convolutional/transformer-based neural network, and a large pretrained transformer neural network. We compare time required for training, hardware and software requirements, and accuracy of transcription. For a fair comparison, all models are trained and evaluated using the same STP test bed and languages. Only the training pipeline is run since it includes computation of phoneme error rate on the held-out part of the dataset. For a given model, the same hyperparameters were used across all languages, and are taken from the reference published paper (except where differences are noted in following sections). The test set was used only for measuring phoneme error rate and was not involved in any tuning."
    }, {
      "heading" : "4.1 Baseline (HMM-GMM)",
      "text" : "Good results were previously obtained with HMMGMM for single speaker phoneme recognition, in low-resource conditions for Cree (Gupta and Boulianne, 2020). To extend those results to other languages, we implemented a general HMM-GMM baseline with the Kaldi toolkit (Povey et al., 2011), modified for phoneme recognition with BPE units. The HMM-GMM acoustic model training follows the usual steps of the Kaldi \"wsj\" recipe10, starting with monophone models (larger than usual beamwidth) and building up to LDA+MLLT+SAT triphone models (tri4), with 1000 model states and a total of 20,000 Gaussian means, amounting to about 800K free parameters. Input features are MFCC \"hires\" features with 40 coefficients computed from audio sampled at 16 kHz. The language model is a 4-gram backoff trained using srilm (Stolcke, 2002) with Witten-Bell discounting (Witten and Bell, 1991).\n10https://github.com/kaldi-asr/kaldi/ tree/master/egs/wsj/s5"
    }, {
      "heading" : "4.2 Persephone (Wisn20)",
      "text" : "For reference we also include results published by Wisniewski et al. (2020). This end-to-end system is a long short-term memory neural recurrent network (LSTM) trained using the Persephone toolkit, with a connectionist temporal classification (CTC) loss criterion. It has no explicit language model, relying only on the implicit modeling of the LSTM. The dataset on which Wisniewski et al. (2020) reported their results was the same as described here in Section 2.1, except that due to limitations of Persephone, they had to exclude audio chunks longer than 10 seconds. This only made a significant difference for Dotyal (nep), which was limited to 44 minutes in Wisniewski et al. (2020), while here we are able to use 95 minutes."
    }, {
      "heading" : "4.3 Pretrained multilingual model (XLSR-53)",
      "text" : "XLSR-5311 is a large version of the wav2vec2.0 model (Conneau et al., 2020), pretrained on 56,000 hours from 53 languages from Multilingual LibriSpeech, CommonVoice and BABEL datasets. The encoder is transformer-based with a convolutional front-end and is shared across languages, similar to the approach of Dalmia et al. (2018).\nWe fine-tune XLSR-53 on each language using the audio segments from the STP prepared data. The feature extraction layers are frozen and only decoder layers are trained, using nxsampa labels with a CTC loss. We use nxsampa rather than BPE since XLSR-53 model words as sequences of single characters. We rely on decoder attention heads for the language model and do not use an external one.\nSpecAugment (Park et al., 2019) was applied with the default parameters. Batch size and learning rate are optimized separately for each language to obtain stable learning on the training set. For all languages, training is stopped after a fixed number of epochs that represents approximately 16,000\n11https://github.com/pytorch/fairseq/ tree/main/examples/wav2vec, model revision 38ae400ce326d5c29d1c66ec6140e4b50a9b34dd from March 10, 2021.\nsteps; warmup is set at 10% of total steps. The total number of parameters in the model is 315M, but fine-tuning updates only the language model head layers, which amount to 76K trainable parameters."
    }, {
      "heading" : "4.4 Conformer with LF-MMI (k2-conf)",
      "text" : "The Conformer model (Gulati et al., 2020) is a transformer-based architecture augmented with convolutional input layers. We based our implementation on the snowfall k2-fsa12 version. As for HMM-GMM, we trained the model with the same audio segments and BPE labels prepared by the STP test bed. The training criterion was LF-MMI (Povey et al., 2016). All languages were trained for 160 epochs. The language model is the same 4-gram model used by the HMM-GMM baseline. Data augmentation was performed using speed perturbation with five values [0.8, 0.9, 1.0, 1.1, 1.2]. Other data augmentation like SpecAugment and noise/reverberation were not used. The number of trainable parameters in this model is 32M."
    }, {
      "heading" : "5 Results",
      "text" : "The four architectures are compared in terms of phoneme error rate, and elapsed time for training, in Table 3 for the public dataset and Table 4 for the private dataset. HMM-GMM refers to the baseline HMM-GMM from section 4.1, Wisn20 to Persephone from section 4.2, XLSR-53 to the pretrained multilingual model of section 4.3, and k2-conf to the Conformer model of section 4.4.\nIn each table, languages appear in descending order of total audio duration available for training. Note that the nru33 subset is used here rather than the full nru, to make it more comparable. True in the IPA column indicates that transcriptions are IPA symbols, false means that transcriptions are orthographic. Phoneme error rates reported are obtained using the speaker turn segmentation from the transcript. In an actual transcription pipeline, VAD would be used and might introduce errors that could slightly degrade the actual PER. Also note that the reference is the phoneme string generated by the G2P table, so tone or stress errors are not counted if tone or stress is not represented by distinct phonemes in the table.\n12https://github.com/k2-fsa/snowfall"
    }, {
      "heading" : "5.1 Discussion",
      "text" : "Looking at the phoneme error rate (PER) columns in Table 3, XLSR-53 is seen to outperform the other models for all languages in the public dataset, with an average of 13.6% PER. In one case, it obtains 8.6% PER with only 20 minutes of training. Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages, except for Tsuut’ina (srs). For this language XLSR-53 training converges unusually fast and floors to a high PER; in contrast, HMM-GMM and k2-conf results on this language are their best in the private dataset. Note that the HMM-GMM result for Cree (crl) is 14.2% PER, slightly better than previously reported for an HMM-BLSTM deep recursive model (Gupta and Boulianne, 2020). The HMM-GMM system obtains the second-lowest average PER (41.1%) for public languages but is third for the private dataset, with 13.2% PER.\nWe find a significant degradation of performance for all models when audio training duration drops to 99 minutes or less. This can be seen in Table 5, where we summarized results from Tables 3 and 4 by grouping languages in two classes based on amounts of audio available for training. Languages with more than 99 minutes are nru33, crl, kmr, iku, srs, and those with 99 minutes or less are lif, nep, ers, mkd, mlv and tvk. The average weights each language equally.\nTable 5 shows that with over 99 minutes, HMMGMM, XLSR and k2-conf have a PER of 13.9% or less (excluding the srs result for XLSR). When training falls to 99 minutes or less, PER increases considerably for k2-conf, moderately for HMMGMM and less dramatically for XLSR-53. To confirm this, in Table 6 we compare various amounts of training for the same language, Yongning Na (nru). From 464 to 151 minutes, error rates increase much less for HMM-GMM and XLSR, than from 151 to 68 minutes, so there seems to be a divide around 90 minutes, or 1.5 hours. The result for nru from Wisniewski et al. (2020) is included for completeness.\nAre these error rates low enough to facilitate language documentation? Amith et al. (2021) found that character error rates around 6 to 10% could reduce the effort of accurate transcription by 75%. Here a PER below 10% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.5 hours of transcribed data.\nRegarding the elapsed time required for training, the last three columns in Tables 3 and 4 show major differences between the models13. The HMMGMM system is not only much faster, but is also the only one which does not use a GPU. So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware in a short time.\nThese results are obtained with only one speaker per language. While generalization is possible\n13Times measured on a single Intel (R) Core(TM) i57500 CPU running at 3.40 GHz, or equivalent, and a single GTX1080Ti GPU, or equivalent, when applicable\nwhen looking at several languages, interpretation for one language in particular must be done carefully. This is a true limitation but also reflects the challenge of working with endangered languages."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Fine-tuning a large pretrained multilingual model outperformed the other approaches (although failing in one case). For the 6 languages with 99 minutes or less of training data, the pretrained model was able to average a phoneme error rate of 15.3%. We obtained 8.4% or less PER for the 5 languages which had between 100 and 192 minutes. At this level of performance, we expect ASR to significantly reduce the effort required for transcription of endangered languages. Further work is needed to explore handling of tone and stress markers, and enlarge the curated speaker-dependent dataset with other publicly available languages."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work would not have been possible without collaborators from the NRC project and all other contributors to the datasets. The authors would also like to thank the Ministry of Economy and Innovation (MEI) of the Government of Quebec for its continued support."
    } ],
    "references" : [ {
      "title" : "Evaluating Phonemic Transcription of Low-Resource Tonal Languages for Language Documentation",
      "author" : [ "Oliver Adams", "Trevor Cohn", "Graham Neubig", "Steven Bird", "Alexis Michaud." ],
      "venue" : "Proc. LREC, pages 3356–3365.",
      "citeRegEx" : "Adams et al\\.,? 2018",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2018
    }, {
      "title" : "User-friendly automatic transcription of lowresource languages: Plugging ESPnet into Elpis",
      "author" : [ "Oliver Adams", "Benjamin Galliot", "Guillaume Wisniewski", "Nicholas Lambourne", "Ben Foley" ],
      "venue" : "In Proc. 4th Workshop on the Use of Computational",
      "citeRegEx" : "Adams et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2021
    }, {
      "title" : "End-to-End Automatic Speech Recognition: Its Impact on the Workflowin Documenting Yoloxóchitl Mixtec",
      "author" : [ "Jonathan D. Amith", "Jiatong Shi", "Rey Castillo García." ],
      "venue" : "Proc. 1st Workshop on Natural Language Processing for Indigenous Languages",
      "citeRegEx" : "Amith et al\\.,? 2021",
      "shortCiteRegEx" : "Amith et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised Cross-lingual Representation Learning for Speech Recognition",
      "author" : [ "Alexis Conneau", "Alexei Baevski", "Ronan Collobert", "Abdelrahman Mohamed", "Michael Auli." ],
      "venue" : "Computing Research Repository, arXiv:2006.13979.",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Are All Languages Equally Hard to Language-Model? Computing Research Repository, arXiv:1806.03743",
      "author" : [ "Ryan Cotterell", "Sebastian J. Mielke", "Jason Eisner", "Brian Roark" ],
      "venue" : null,
      "citeRegEx" : "Cotterell et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Cotterell et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequence-based Multilingual Low Ressource Speech Recognition",
      "author" : [ "Siddharth Dalmia", "Ramon Sanabria", "Florian Metze", "Alan W Black." ],
      "venue" : "Proc. ICASSP, pages 4909–4913.",
      "citeRegEx" : "Dalmia et al\\.,? 2018",
      "shortCiteRegEx" : "Dalmia et al\\.",
      "year" : 2018
    }, {
      "title" : "The Uqailaut project [online",
      "author" : [ "Benoît Farley" ],
      "venue" : null,
      "citeRegEx" : "Farley.,? \\Q2012\\E",
      "shortCiteRegEx" : "Farley.",
      "year" : 2012
    }, {
      "title" : "Conformer: Convolution-augmented transformer for speech recognition",
      "author" : [ "Anmol Gulati", "James Qin", "Chung Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang." ],
      "venue" : "Proc. Interspeech,",
      "citeRegEx" : "Gulati et al\\.,? 2020",
      "shortCiteRegEx" : "Gulati et al\\.",
      "year" : 2020
    }, {
      "title" : "Speech Transcription Challenges for Resource Constrained Indigenous Language Cree",
      "author" : [ "Vishwa Gupta", "Gilles Boulianne." ],
      "venue" : "Proc. 1st Joint SLTU CCURL, pages 362–367.",
      "citeRegEx" : "Gupta and Boulianne.,? 2020",
      "shortCiteRegEx" : "Gupta and Boulianne.",
      "year" : 2020
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proc. EMNLP, pages 66–71.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "The Indigenous Languages Technology project",
      "author" : [ "Roland Kuhn", "Fineen Davis", "Alain Désilets" ],
      "venue" : null,
      "citeRegEx" : "Kuhn et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kuhn et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal Phone Recognition with a Multilingual Allophone",
      "author" : [ "Xinjian Li", "Siddharth Dalmia", "Juncheng Li", "Matthew Lee", "Patrick Littell", "Jiali Yao", "Antonios Anastasopoulos", "David R. Mortensen", "Graham Neubig", "Alan W. Black", "Florian Metze" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Documenting and researching endangered languages: the Pangloss Collection",
      "author" : [ "Boyd Michailovsky", "Martine Mazaudon", "Alexis Michaud", "Séverine Guillaume", "Alexandre François", "Evangelia Adamou." ],
      "venue" : "Conservation, 8:119–135.",
      "citeRegEx" : "Michailovsky et al\\.,? 2014",
      "shortCiteRegEx" : "Michailovsky et al\\.",
      "year" : 2014
    }, {
      "title" : "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "author" : [ "Daniel S. Park", "William Chan", "Yu Zhang", "Chung Cheng Chiu", "Barret Zoph", "Ekin D. Cubuk", "Quoc V. Le." ],
      "venue" : "Proc. Interspeech, pages 2613–2617.",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "The Kaldi speech recognition toolkit",
      "author" : [ "Daniel Povey", "Arnab Ghoshal" ],
      "venue" : "In Proc. ASRU",
      "citeRegEx" : "Povey and Ghoshal,? \\Q2011\\E",
      "shortCiteRegEx" : "Povey and Ghoshal",
      "year" : 2011
    }, {
      "title" : "Purely sequence-trained neural networks for ASR based on lattice-free MMI",
      "author" : [ "Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez" ],
      "venue" : "In Proc. Interspeech,",
      "citeRegEx" : "Povey et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Computing Research Repository, arXiv:1508.07909.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yoloxóchitl Mixtec",
      "author" : [ "Jiatong Shi", "Jonathan D. Amith", "Rey Castillo García", "Esteban Guadalupe Sierra", "Kevin Duh", "Shinji Watanabe." ],
      "venue" : "Computing Re-",
      "citeRegEx" : "Shi et al\\.,? 2021",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2021
    }, {
      "title" : "SRILM - An extensible language modeling toolkit",
      "author" : [ "Andreas Stolcke." ],
      "venue" : "Proc. ICSLP, pages 901– 904.",
      "citeRegEx" : "Stolcke.,? 2002",
      "shortCiteRegEx" : "Stolcke.",
      "year" : 2002
    }, {
      "title" : "Phonemic Transcription of Low-Resource Languages: To What Extent can Preprocessing be Automated? In Proc",
      "author" : [ "Guillaume Wisniewski", "Séverine Guillaume", "Alexis Michaud." ],
      "venue" : "1st Joint SLTU CCURL Workshop, pages 306–315.",
      "citeRegEx" : "Wisniewski et al\\.,? 2020",
      "shortCiteRegEx" : "Wisniewski et al\\.",
      "year" : 2020
    }, {
      "title" : "The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression",
      "author" : [ "Ian H. Witten", "Thimoty C. Bell." ],
      "venue" : "IEEE Transactions on Information Theory, 37(4):1085– 1094.",
      "citeRegEx" : "Witten and Bell.,? 1991",
      "shortCiteRegEx" : "Witten and Bell.",
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "addition, none has yet evaluated the recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020)3, which are particularly well suited for low-resource languages.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "The Pangloss collection (Michailovsky et al., 2014) is an open archive of under-documented and mostly endangered languages.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "We also had access to transcribed Inuktitut, Cree and Tsuut’inai recordings collected and transcribed during the NRC Indigenous language project (Kuhn et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "The coverage problem could be solved with subword units such as morphemes or syllables, but BPE units (byte pair encoding) (Sennrich et al., 2015) are more commonly used and require no extra linguistic knowledge.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "BPE training and extraction are implemented with SentencePiece (Kudo and Richardson, 2018).",
      "startOffset" : 63,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Counting roman characters rather than words, as it is directly related to the bits-per-character measure and is less dependent on the subword inventory (Cotterell et al., 2018).",
      "startOffset" : 152,
      "endOffset" : 176
    }, {
      "referenceID" : 6,
      "context" : "(7)More exactly, are in the set of morphemes produced by the Uqailaut analyzer (Farley, 2012) from the Nunavut Hansards.",
      "startOffset" : 79,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Good results were previously obtained with HMMGMM for single speaker phoneme recognition, in low-resource conditions for Cree (Gupta and Boulianne, 2020).",
      "startOffset" : 126,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "The language model is a 4-gram backoff trained using srilm (Stolcke, 2002) with Witten-Bell discounting (Witten and Bell, 1991).",
      "startOffset" : 59,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "The language model is a 4-gram backoff trained using srilm (Stolcke, 2002) with Witten-Bell discounting (Witten and Bell, 1991).",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "SpecAugment (Park et al., 2019) was applied with the default parameters.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "The Conformer model (Gulati et al., 2020) is a transformer-based architecture augmented with convolutional input layers.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "The training criterion was LF-MMI (Povey et al., 2016).",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "2% PER, slightly better than previously reported for an HMM-BLSTM deep recursive model (Gupta and Boulianne, 2020).",
      "startOffset" : 87,
      "endOffset" : 114
    } ],
    "year" : 0,
    "abstractText" : "Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers. In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. However, when a single speaker is involved, several studies have reported encouraging results for phonetic transcription even with small amounts of training. Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. To automate data preparation, training and evaluation steps, we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists. We find that fine-tuning a multilingual pretrained model yields an average phoneme error rate (PER) of 15% for 6 languages with 99 minutes or less of transcribed data for training. For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4% or less. These results on a number of varied languages suggest that ASR can now significantly reduce transcription efforts in the speaker-dependent situation common in endangered language work.",
    "creator" : null
  }
}
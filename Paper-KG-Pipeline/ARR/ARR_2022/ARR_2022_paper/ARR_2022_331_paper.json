{
  "name" : "ARR_2022_331_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Given a potentially noisy input sentence, grammatical error correction (GEC) aims to detect and correct all errors and produce a clean sentence. Recently, there has been increasing attention to GEC for its vital value in various downstream scenarios (Grundkiewicz et al., 2020; Wang et al., 2021).\nTo support the GEC study, high-quality manually labeled evaluation data is indispensable. For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020). In contrast, such datasets for CGEC are relatively scarce. The two publicly available datasets are NLPCC18 and CGED, contributed by the NLPCC2018 (Zhao et al., 2018) and CGED-2018&2020 shared tasks (Rao et al., 2018, 2020), respectively.\nMost influential EGEC evaluation datasets provide multiple references for each input sentence,\nsuch as CoNLL14-test (Ng et al., 2014) and BEA19-test (Bryant et al., 2019). Nevertheless, sentences in existing CGEC evaluation datasets always have only one reference (i.e., 87% of the sentences in NLPCC18 and all in CGED). This is possibly due to their adopted annotation workflow, where each sentence is assigned to only one annotator and multi-reference submission is not allowed. As strongly suggested by Bryant and Ng (2015), enforcing multi-reference annotation is crucial for both GEC model evaluation and data annotation. Because, obviously, there are usually multiple acceptable references with close meanings for an incorrect sentence, as illustrated by the example in Table 1. On the one hand, if the evaluation data gives only one reference and a GEC model outputs another valid alternative, then the model will be unfairly underestimated. To mitigate this phenomenon, a routine solution is increasing the number of references (Sakaguchi et al., 2016; Choshen and Abend, 2018). On the other hand, imposing a single-reference constraint makes data annotation problematic. If annotators submit different equally acceptable corrections, which is very common, it will be taxing for the senior annotator to solely select the best one as the final golden answer.\nBesides the lack of multiple references, all existing CGEC datasets collect sentences from a single text source, which may be insufficient for robust model evaluation (Mita et al., 2019). Another flaw of them is the absence of strict quality control strate-\ngies, e.g., annotation guidelines and review mechanisms. The above-mentioned problems may cause the unreliability of evaluation for CGEC models and hinder the development of this area.\nTo fill these gaps, this paper aims to build the first multi-reference multi-source evaluation dataset for CGEC. We first collect data for annotation from three divergent sources that cover both formal/informal texts. After investigating previous work on constructing GEC datasets, we compile comprehensive annotation guidelines for detailed illustration. Based on a specially constructed online annotation system, each sentence is assigned to three annotators for independent correction, and one senior annotator for final review. An annotator may submit multiple references, and the senior annotator may also supplement new references besides rejecting incorrect submissions. In this way, we aim to produce as many references as possible.\nIn summary, this work makes the following contributions:\n(1) We construct the first multi-reference multisource evaluation dataset for CGEC, named MuCGEC, consisting of 7,063 sentences from three representative sources of CSL texts. Each sentence obtains 2.3 references on average. Further, we conduct detailed analyses on our new dataset to gain more insights.\n(2) We employ two mainstream and competitive CGEC models based on large pretrained language models (PLMs), i.e., the Seq2Edit and Seq2Seq models, with an extremely effective ensemble strategy, to conduct strong benchmark experiments on our dataset. We also investigate the effect of multiple references and propose to use a char-based evaluation metric, which is simpler and more suitable than previous word-based ones for CGEC."
    }, {
      "heading" : "2 Dataset Annotation",
      "text" : ""
    }, {
      "heading" : "2.1 Multi-Source Data Selection",
      "text" : "This work focuses on CSL learner texts. In order to investigate diverse types of Chinese grammatical errors, we select data from the following three sources.\n(1) We re-annotate the NLPCC18 test set (Zhao et al., 2018), which contains 2,000 sentences from the Peking University (PKU) Chinese Learner Corpus.\n(2) We select and re-annotate the CGED2018&2020 test datasets (Rao et al., 2018, 2020). They are from the writing section of the HSK exam (Hanyu Shuiping Kaoshi, translated as the Chinese level exam), which is an official Chinese proficiency test. After removing sentences marked as correct from total 5,006 ones, we obtain 3,137 potentially erroneous sentences for annotation.\n(3) Lang81 is a language learning platform, where native speakers voluntarily correct jottings uploaded by second-language learners. The NLPCC-2018 organizers collect about 717K Chinese sentence-correction pairs from Lang8 and employ them as the training data. We randomly select 2,000 potentially erroneous sentences with 30 to 60 characters for annotation.\nFinally, we have obtained 7,137 sentences. For simplicity, we discard all original corrections, and directly perform re-annotation from scratch following our new annotation guidelines and workflow."
    }, {
      "heading" : "2.2 Annotation Paradigm: Direct Rewriting",
      "text" : "There are mainly two types of annotation paradigms for constructing GEC data, i.e., errorcoded and direct rewriting. The error-coded paradigm requires annotators to explicitly mark the erroneous span in the original sentence, then choose its error type, and finally make corrections. Ng et al. (2013, 2014) adopt the error-coded paradigm for constructing data for the CoNLL2013/2014 EGEC shared tasks. For CGEC, the original NLPCC18 and CGED datasets both follow the error-coded paradigm as well.\nAs discussed by Sakaguchi et al. (2016), the error-coded paradigm suffers from two challenges. First, it is extremely difficult for different annotators to agree upon the boundaries of the erroneous spans and their error types, especially when there are many categories to consider (Bryant et al., 2017). This inevitably leads to an increase in annotation effort and a decrease in annotation quality. Second, under such a complex annotation paradigm, annotators would pay less attention to the fluency of the resulting reference, sometimes even leading to unnatural expressions.\nInstead, the direct rewriting paradigm requires annotators to directly rewrite the whole sentence, as long as the resulting sentence does not change\n1https://lang-8.com/\nthe original meaning and is grammatically correct and fluent. Edits are extracted automatically from parallel sentences by additional tools (Bryant et al., 2017). This annotation paradigm has been proved to be efficient and cheap (Sakaguchi et al., 2016), and adopted by many datasets in other languages (Napoles et al., 2017, 2019; Syvokon and Nahorna, 2021). In this work, we adopt the direct rewriting paradigm. Besides the above-mentioned advantages, we believe it is beneficial for encouraging the variety of references since annotators can correct more freely."
    }, {
      "heading" : "2.3 Annotation Guidelines",
      "text" : "After several months’ survey on previous GEC data construction work, we have compiled 30-page comprehensive guidelines for CGEC annotation. During the annotation, our guidelines are gradually improved according to the feedback from our annotators.\nTo facilitate illustration, our guidelines adopt a two-tier hierarchical error taxonomy, including 5 major error types and 14 minor types, as shown in Table 2. The 5 major error types are decided by both referring to previous work and considering frequencies of error occurrences. Our guidelines describe in detail how to handle each minor error type and provide abundant typical examples. We will release our guidelines along with the data, which we hope can benefit future research."
    }, {
      "heading" : "2.4 Annotation Workflow and Tool",
      "text" : "In order to encourage more diverse and high-quality references, we assign each sentence to three random annotators for independent annotation. Their submissions are then aggregated and sent to a random senior annotator for review. During annotation, an annotator may submit multiple references\nfor one sentence if he/she thinks they are correct according to the guidelines. During the review, the job of the senior annotator includes 1) modifying incorrect references into correct ones (sometimes just rejecting them); 2) adding other correct references according to the guidelines. After review, the accepted references are defined as Final Golden References, which are ultimately used to evaluate CGEC models.\nFor the sake of self-improvement, we employ a self-study mechanism that allows annotators to learn from their mistakes if they submit an incorrect reference. Concretely, the annotator has to modify her/his submission by referring to the final golden references. Moreover, annotators can make complaints if they disapprove of the final golden references, which can trigger helpful discussions.\nTo improve annotation efficiency, we have developed a browser-based online annotation tool to support the above workflow and mechanisms. Due to the space limitation, we show the visual interfaces for annotation and review in Appendix A."
    }, {
      "heading" : "2.5 Annotation Process",
      "text" : "We employed 21 undergraduate students who are native speakers of Chinese and familiar with Chinese grammar as part-time annotators. Annotators received intensive training about our guidelines before the real annotation. In the beginning, two co-authors who were in charge of compiling the guidelines served as senior annotators for review. After one month, when the annotators were familiar with the job, we selected 5 outstanding annotators as senior annotators to join the review.\nAll participants were asked to annotate for at least 1 hour every day. The whole annotation process lasted for about 3 months."
    }, {
      "heading" : "2.6 Ethical Issues",
      "text" : "All annotators and reviewers were paid for their work. The salary is determined by both submission numbers and annotation quality. The average salary of annotators and reviewers is 24 and 35 RMB per hour respectively.\nAll the data of the three sources are publicly available. Meanwhile, we have obtained permission from organizers of the NLPCC-2018 and CGED shared tasks to release our newly annotated references in a proper way."
    }, {
      "heading" : "3 Analysis of Our Annotated Data",
      "text" : "This section presents a detailed analysis of the proposed MuCGEC dataset.\nOverall statistics of our new dataset are shown in Table 3. We also include the original NLPCC18 dataset (Zhao et al., 2018) for comparison.2\nFirst, regarding the proportion of erroneous sentences, most of the sentences are considered to contain grammatical errors in the previous annotation, but a considerable part of them are not corrected in our annotation. We attribute this to our strict control of the over-correction phenomenon.\nSecond, regarding sentence lengths, NLPCC18 is the shortest, whereas CGED is much longer. This is possibly because, since HSK is an official Chinese proficiency test, candidates tend to use long sentences to show their ability in Chinese use.\nThird, each sentence in re-annotated NLPCC18 receives 2.5 references on average, which is more than twice of that in the original NLPCC18 data. Overall, each sentence obtains 2.3 references. We believe the multi-reference characteristic makes our dataset more reliable for model evaluation, which is further discussed in Section 6.2.\nFinally, we compare the number of char-based edits per reference in different datasets. We describe how to derive such edits in detail in Section 6.2. We can see that the edit number is tightly correlated with the sentence length. The difference in averaged sentence length and numbers of edits indicates that the three data sources may have a systematic discrepancy in quality and difficulty, which we believe is helpful for evaluating the generalization ability of models. Moreover, compared with\n2Here we do not compare with the original CGED and Lang8 datasets since: 1) the CGED-orig mainly focuses on error detection annotation and does not provide corrections for word-order errors; 2) the Lang8-orig is collected from the internet, and its correction is quite noisy.\nNLPCC18-orig, we annotate 25% more edits (2.0 vs. 2.5) in each reference. We believe the major reason is that the original NLPCC18 data are annotated under the minimal edit distance principle (Nagata and Sakaguchi, 2016), which requires annotators to select a reference with fewer edits when correcting.\nDistribution regarding numbers of references. In Figure 1, we analyze the distribution of sentences regarding the numbers of references. Here, we only consider erroneous sentences. Same references from different annotators are calculated as one reference. Overall, most sentences have 2 references, and sentences having 3 references take a slightly lower proportion. There are 21.8% of sentences with only one reference. Most of them are short and easy to correct.\nWe believe that the average reference number should be further increased if more annotators are assigned for each sentence. Despite the fact that our annotation tool allows annotators to submit multiple answers, we find that most annotators tend to submit a single correction. Since it is usually easy to come up with the most suitable correction, but is more time-consuming to provide alternatives.\nHuman annotation performance. In order to know the annotation ability of our annotators and human performance for the CGEC task, we calculate char-based F0.5 scores by evaluating the annotation submissions against the final golden references picked by senior annotators after review. We describe how to compute char-based metrics in detail in Section 6.2. Each reference submitted by an annotator is considered as a sample. Overall, the average F0.5 is 72.12, which we believe can be further improved if our annotators are more experienced and more familiar with our guidelines.\nFigure 2 shows F0.5 scores of 15 annotators who annotated the most sentences, in the descending\norder of annotated sentence numbers. We can see that human performance varies across different annotators. The best annotator achieves an 82.34 F0.5 score, while the annotator who completes the most tasks only gets a score of 68.32. It indicates that we should pay more attention to annotation quality when calculating salaries and prevent annotators from focusing too much on annotation speed."
    }, {
      "heading" : "4 Benchmark Models",
      "text" : "To understand how well cutting-edge GEC models perform on our data, we adopt two mainstream GEC approaches, i.e., Seq2Edit and Seq2Seq. Both models are enhanced with PLMs. We also attempt to combine them after observing their complementary power in dealing with different error types. This section briefly describes these benchmark models. Due to the space limitation, please kindly refer to Appendix B for more model details.\nThe Seq2Edit model treats GEC as a sequence labeling task and performs error corrections via a sequence of token-level edits, including insertion, deletion, and substitution (Malmi et al., 2019). A token corresponds to a word or a subword in English, and to a character in Chinese. With minor modifications to accommodate Chinese, we adopt GECToR (Omelianchuk et al., 2020), which achieves the SOTA performance on EGEC datasets. Following recent Seq2Edit work like Awasthi et al. (2019) and Omelianchuk et al. (2020), we enhance GECToR by using PLMs as its encoder. After comparing several popular PLMs, we choose StructBERT (Wang et al., 2019) 3 due to its superior performance after fine-tuning (see Table 4).\n3https://github.com/alibaba/AliceMind/ tree/main/StructBERT\nThe Seq2Seq model straightforwardly treats GEC as a monolingual translation task (Yuan and Briscoe, 2016). Recent work proposes to enhance Transformer-based (Vaswani et al., 2017) Seq2Seq EGEC models with PLMs like T5 (Rothe et al., 2021) or BART (Katsumata and Komachi, 2020). Unlike BERT (Devlin et al., 2019), T5 and BART are specifically designed for text generation. Therefore, it is straightforward to continue training them on GEC data. We follow these work and utilize the recently proposed Chinese BART from Shao et al. (2021) to initialize our Seq2Seq model.\nThe ensemble model. Several previous works have proved the effectiveness of model ensemble for CGEC (Liang et al., 2020; Hinson et al., 2020). In this work, we clearly observe the complementary power of the above two models in fixing different error types (see Table 6), and thus attempt to combine them. We adopt a simple edit-wise vote mechanism: aggregating edits from the results of each model, and only preserving edits that appear more than N/2 times, where N is the number of models. We experiment with two ensemble settings: 1) one Seq2Edit and one Seq2Seq, denoted as “1×Seq2Edit+1×Seq2Seq” , and 2) three Seq2Edit and three Seq2Seq, denoted as “3×Seq2Edit+3×Seq2Seq”. The three Seq2Edit models are obtained by replacing the random seed, the same goes for the Seq2Seq."
    }, {
      "heading" : "5 Experiments on Original NLPCC18",
      "text" : "In order to show that our benchmark models are competitive among existing CGEC models, we conduct comparison experiments on the original NLPCC18 test set, where most previous CGEC systems are tested.\nTraining data. For the sake of easy replicability, we limit our training data strictly to public resources, i.e., the Lang8 (Zhao et al., 2018) 4 data and the HSK (Xun, 2018) 5 data. We filter duplicate sentences that appear in our dataset, and only use the erroneous part for training. The final Lang8 and HSK data contains 1,092,285 and 95,320 sentences, respectively. The HSK data is cleaner and of higher quality than Lang8, but is much smaller. Following the re-weighting procedure of JunczysDowmunt et al. (2018), we duplicate the HSK data five times, and merge them with Lang8 data.\nComparison with previous work. Table 4 shows the results. For a fair comparison, we follow the official setting of the shared task, including using the word-based MaxMatch scorer (Dahlmeier and Ng, 2012) for calculating the P/R/F values. We segment model outputs by adopting the PKUNLP word segmentation (WS) tool provided by the shared task organizers (Zhao et al., 2018).\nWhen using only Lang8 for training, our single Seq2Seq model is already quite competitive. It only underperforms MaskGEC (Zhao and Wang, 2020) by 1 F0.5 score, which additionally uses data\n4http://tcci.ccf.org.cn/conference/2018/ taskdata.php\n5http://hsk.blcu.edu.cn\naugmentation. After adding the HSK data, all our models achieve further performance-boosting by about 4 points. Both of our single benchmark models achieve SOTA performance under this setting.\nThe model ensemble technique leads to obvious performance gains (more than 5 points) over single models. However, the gains from increasing the number of component models seem rather small.\nFor Seq2Edit, we additionally present results with other PLMs besides StructBERT, including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and MacBERT (Cui et al., 2020) from the hugging face 6 website. All PLMs are under the large configuration."
    }, {
      "heading" : "6 Experiments on MuCGEC",
      "text" : ""
    }, {
      "heading" : "6.1 Data Splits",
      "text" : "For hyperparameter tuning or model selection, previous work on other CGEC datasets often randomly sample some sentence pairs from training data as the dev set (Wang et al., 2020; Zhao and Wang, 2020; Hinson et al., 2020), which is problematic for replicability and complicated for comparison.\nIn this work, we propose to provide a fixed dev set for our newly annotated dataset, by randomly selecting 1,125 sentences from the CGED source, denoted as CGED-dev. The remaining 5,938 sentences are used as the test set, in which each data source has a roughly equal amount of sentences, i.e., 1,996 sentences for NLPCC18-test, 2,000 for CGED-test, and 1,942 for Lang8-test."
    }, {
      "heading" : "6.2 Evaluation Metrics",
      "text" : "Problems with the word-based metric. As discussed in Section 5, previous CGEC datasets are annotated upon word sequences and thus adopt wordbased metrics for evaluation. Therefore, before annotation and evaluation, each sentence should be segmented into words using a Chinese word segmentation (CWS) model. This introduces unnecessary uncertainty in the evaluation procedure. Sometimes, a correct edit may be judged as wrong due to word boundary mismatch. Different from English whose words are separated naturally, Chinese sentences are written without any word delimiters, so WS models perform much worse for Chinese than for English (Fu et al., 2020).\nIn view of these, we believe it is more suitable to abandon such word-based metrics in CGEC.\n6https://huggingface.co/\nThe char-based evaluation metric is adopted in this work instead. First, given an input sentence and a correction, we obtain an optimal sequence of char-based edits that corresponds to the minimal edit distance. There are three types of char-based edits, including deleting a char for a redundant error, inserting a char for a missing error, or substituting a char with another one for a substitution error. Secondly, following the standard practice in both EGEC and CGEC, we merge consecutive edits of the same type as one span-level edit (Felice et al., 2016; Hinson et al., 2020). The above two steps are applied to both the system output sequence and all gold-standard references, transforming them into sets of span-level edits. Finally, we utilize the evaluation script from ERRANT (Bryant et al., 2017) to calculate the P/R/F values which does not need error-coded annotation."
    }, {
      "heading" : "6.3 Results and Analysis",
      "text" : "Main results. Table 5 shows the char-based performance of the benchmark models and our annotators on MuCGEC. All models are trained on Lang8+HSK, as described in Section 4. Please kindly note that we cannot present results of previous work in Table 4, since most of them did not release their code.7\nThe overall trend of performance is basically consistent with those on the original NLPCC18\n7BLCU (Ren et al., 2018) did release code, but its performance is much lower than the SOTA.\ndataset in Table 4. First, the Seq2Seq and Seq2Edit models perform quite closely on F0.5, but clearly exhibit divergent strength in precision and recall, giving a strong motivation for combining them. Secondly, the model ensemble approach improves performance by a very large margin, as expected.\nOne interesting observation is that on MuCGEC, “3×Seq2Edit+3×Seq2Seq” substantially outperforms “1×Seq2Edit+1×Seq2Seq” on All-test and all three subsets. In contrast, the improvement is only modest on the original NLPCC18 test data. We suspect this may indicate that a multi-reference dataset can more accurately evaluate model performance. However, it may require further human investigation for more insights.\nFinally, there is still a huge performance gap between models and humans, indicating that the CGEC research still has a long way to go.\nPerformance on four error types. Table 6 shows more fine-grained evaluation results on four error types. The word-order errors can be identified by heuristic rules following Hinson et al. (2020).\nIt is clear that the Seq2Edit model is better at handling redundant errors, whereas the Seq2Seq model is superior in dealing with substitution and word-order errors. For missing errors, the two perform similarly well.\nThese phenomena are quite interesting and can\nbe understood after considering the underlying model architectures. On the one hand, to correct redundant errors, the Seq2Edit model only needs to perform a fixed deletion operation, which is a much more implicit choice for the Seq2Seq model, since its goal is to rewrite the whole sentence. On the other hand, the Seq2Seq is suitable to substitute or reorder words due to its natural capability of utilizing language model information, especially with the enhancement of BART (Lewis et al., 2020).\nAgain, the model ensemble approach substantially improves performance on all error types. The ensemble model is closest to the human on redundant errors, probably because they are the easiest to correct. The largest gap occurs in word-order errors, which require global structure knowledge to correct and are extremely challenging.\nInfluence of the number of references. To understand the impact of the number of references on performance evaluation, we deliberately reduce the available reference number in our dataset. For example, when the maximum number of references is limited to 2, we remove all extra references if a sentence has more than 2 gold-standard references. The results on MuCGEC are shown in Figure 3.\nWhen the maximum number of references increases, the performance of both models and humans increases continuously, especially for humans. As only a few sentences have more than 3 references, the improvement is quite slight when the maximum number of references increases from 3 to All. This trend suggests that compared with single-reference datasets, a multi-reference dataset reduces the risk of underestimating performance, and thus is more reliable for model evaluation."
    }, {
      "heading" : "7 Related Work",
      "text" : "EGEC resources. There is a lot of work on EGEC data construction. As the two earliest EGEC datasets, FCE (Yannakoudakis et al., 2011) and NUCLE (Dahlmeier et al., 2013) adopt the error-coded annotation paradigm. In contrast, JFLEG (Napoles et al., 2017) collects sentences from TOFEL exams and adopts the direct rewriting paradigm. W&I (Bryant et al., 2019) also chooses the direct rewriting paradigm, and for each original sentence additionally provides the language proficiency level of the writer. All four datasets are composed of essays from non-native English speakers and provide multiple references.\nRecently, researchers start to annotate small-\nscale EGEC data for texts written by native English speakers, including AESW (Daudaravicius et al., 2016), LOCNESS (Bryant et al., 2019), GMEG (Napoles et al., 2019) and CWEB (Flachs et al., 2020). In the future, we plan to extend this work to texts written by native Chinese speakers.\nCGEC resources. Compared with EGEC, progress in CGEC data construction largely lags behind. As thoroughly discussed in Section 1, NLPCC18 (Zhao et al., 2018) and CGED (Rao et al., 2018, 2020) are the only two evaluation datasets for CGEC research. Besides them, there are also a few resources for training CGEC models, e.g., Lang8 corpus (Zhao et al., 2018) and HSK corpus (Xun, 2018).\nRecent progress in CGEC. In the NLPCC2018 shared task (Zhao et al., 2018), many systems adopt Seq2Seq models, based on RNN/CNN. Recent work mainly utilizes Transformer (Wang et al., 2020; Zhao and Wang, 2020; Tang et al., 2021). Hinson et al. (2020) first employ a Seq2Edit model for CGEC, and achieve comparable performance with the Seq2Seq counterparts. Some systems in the CGED-2020 shared task (Rao et al., 2020) directly employ the open-source Seq2Edit model, i.e., GECToR (Liang et al., 2020; Fang et al., 2020). Besides the above two mainstream models, Li and Shi (2021) for the first time apply a non-autoregressive neural machine translation model to CGEC.\nBesides modeling optimization, techniques like data augmentation (Zhao and Wang, 2020; Tang et al., 2021) and model ensemble (Hinson et al., 2020) have also been proved very useful for CGEC."
    }, {
      "heading" : "8 Conclusions",
      "text" : "This paper presents our newly annotated evaluation dataset for CGEC, consisting of 7,063 sentences written by CSL learners. Compared with existing CGEC datasets, ours can support more reliable evaluation due to three important features: 1) providing multiple references; 2) covering three different text sources; 3) adopting more strict quality control (i.e., annotation guidelines and workflow).\nAfter describing the data construction process, we perform detailed analyses of our data. Then, we adopt two mainstream and competitive CGEC models, i.e., Seq2Seq and Seq2Edit, and carry out benchmark experiments. We also propose to adopt char-based evaluation metrics, which are more suitable than word-based ones. In summary, we believe this work will promote future research in CGEC."
    }, {
      "heading" : "B Hyperparameters",
      "text" : "Table 7 shows the detailed hyperparameters for training our two benchmark models. The results of all single models are averaged over 3 runs, and the results of the ensemble models are just calculated from a single run."
    } ],
    "references" : [ {
      "title" : "2020) first employ a Seq2Edit model for CGEC, and achieve comparable performance with the Seq2Seq counterparts. Some systems in the CGED-2020 shared task (Rao et al., 2020) directly employ the open-source Seq2Edit",
      "author" : [ "Hinson" ],
      "venue" : null,
      "citeRegEx" : "Hinson,? \\Q2020\\E",
      "shortCiteRegEx" : "Hinson",
      "year" : 2020
    }, {
      "title" : "Parallel iterative edit models for local sequence transduction",
      "author" : [ "References Abhijeet Awasthi", "Sunita Sarawagi", "Rasna Goyal", "Sabyasachi Ghosh", "Vihari Piratla." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages",
      "citeRegEx" : "Awasthi et al\\.,? 2019",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2019
    }, {
      "title" : "The bea-2019 shared task on grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Øistein E Andersen", "Ted Briscoe." ],
      "venue" : "Proceedings of BEA@ACL, pages 52–75.",
      "citeRegEx" : "Bryant et al\\.,? 2019",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic annotation and evaluation of error types for grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Ted Briscoe." ],
      "venue" : "Proceedings of ACL, pages 793–805.",
      "citeRegEx" : "Bryant et al\\.,? 2017",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2017
    }, {
      "title" : "How far are we from fully automatic high quality grammatical error correction",
      "author" : [ "Christopher Bryant", "Hwee Tou Ng" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Bryant and Ng.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bryant and Ng.",
      "year" : 2015
    }, {
      "title" : "Inherent biases in reference-based evaluation for grammatical error correction",
      "author" : [ "Leshem Choshen", "Omri Abend." ],
      "venue" : "Proceedings of ACL, pages 632– 642.",
      "citeRegEx" : "Choshen and Abend.,? 2018",
      "shortCiteRegEx" : "Choshen and Abend.",
      "year" : 2018
    }, {
      "title" : "Revisiting pretrained models for chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Proceedings of EMNLP: findings, pages 657– 668.",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Better evaluation for grammatical error correction",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng." ],
      "venue" : "Proceedings of NAACL-HLT, pages 568–572.",
      "citeRegEx" : "Dahlmeier and Ng.,? 2012",
      "shortCiteRegEx" : "Dahlmeier and Ng.",
      "year" : 2012
    }, {
      "title" : "Building a large annotated corpus of learner english: The nus corpus of learner english",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu." ],
      "venue" : "Proceedings of BEA@NAACL-HLT, pages 22–31.",
      "citeRegEx" : "Dahlmeier et al\\.,? 2013",
      "shortCiteRegEx" : "Dahlmeier et al\\.",
      "year" : 2013
    }, {
      "title" : "A report on the automatic evaluation of scientific writing shared task",
      "author" : [ "Vidas Daudaravicius", "Rafael E Banchs", "Elena Volodina", "Courtney Napoles." ],
      "venue" : "Proceedings of BEA@NAACL-HLT, pages 53–62.",
      "citeRegEx" : "Daudaravicius et al\\.,? 2016",
      "shortCiteRegEx" : "Daudaravicius et al\\.",
      "year" : 2016
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4171– 4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A hybrid system for nlptea-2020 cged shared task",
      "author" : [ "Meiyuan Fang", "Kai Fu", "Jiping Wang", "Yang Liu", "Jin Huang", "Yitao Duan." ],
      "venue" : "Proceedings of NLPTEA@AACL, pages 67–77.",
      "citeRegEx" : "Fang et al\\.,? 2020",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic extraction of learner errors in esl sentences using linguistically enhanced alignments",
      "author" : [ "Mariano Felice", "Christopher Bryant", "Ted Briscoe." ],
      "venue" : "Proceedings of COLING, pages 825–835.",
      "citeRegEx" : "Felice et al\\.,? 2016",
      "shortCiteRegEx" : "Felice et al\\.",
      "year" : 2016
    }, {
      "title" : "Grammatical error correction in low error density domains: A new benchmark and analyses",
      "author" : [ "Simon Flachs", "Ophélie Lacroix", "Helen Yannakoudakis", "Marek Rei", "Anders Søgaard." ],
      "venue" : "Proceedings of EMNLP, pages 8467–8478.",
      "citeRegEx" : "Flachs et al\\.,? 2020",
      "shortCiteRegEx" : "Flachs et al\\.",
      "year" : 2020
    }, {
      "title" : "Is chinese word segmentation a solved task? rethinking neural chinese word segmentation",
      "author" : [ "Jinlan Fu", "Pengfei Liu", "Qi Zhang", "Xuan-Jing Huang." ],
      "venue" : "Proceedings of EMNLP, pages 5676–5686.",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Youdao’s winning solution to the nlpcc-2018 task 2 challenge: a neural machine translation approach to chinese grammatical error correction",
      "author" : [ "Kai Fu", "Jin Huang", "Yitao Duan." ],
      "venue" : "CCF International Conference on Natural Language Processing and",
      "citeRegEx" : "Fu et al\\.,? 2018",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2018
    }, {
      "title" : "A crash course in automatic grammatical error correction",
      "author" : [ "Roman Grundkiewicz", "Christopher Bryant", "Mariano Felice." ],
      "venue" : "Proceedings of COLING: Tutorial Abstracts, pages 33–38.",
      "citeRegEx" : "Grundkiewicz et al\\.,? 2020",
      "shortCiteRegEx" : "Grundkiewicz et al\\.",
      "year" : 2020
    }, {
      "title" : "Heterogeneous recycle generation for chinese grammatical error correction",
      "author" : [ "Charles Hinson", "Hen-Hsen Huang", "Hsin-Hsi Chen." ],
      "venue" : "Proceedings of COLING, pages 2191–2201.",
      "citeRegEx" : "Hinson et al\\.,? 2020",
      "shortCiteRegEx" : "Hinson et al\\.",
      "year" : 2020
    }, {
      "title" : "Approaching neural grammatical error correction as a low-resource machine translation task",
      "author" : [ "Marcin Junczys-Dowmunt", "Roman Grundkiewicz", "Shubha Guha", "Kenneth Heafield." ],
      "venue" : "Proceedings of NAACL-HLT, pages 595–606.",
      "citeRegEx" : "Junczys.Dowmunt et al\\.,? 2018",
      "shortCiteRegEx" : "Junczys.Dowmunt et al\\.",
      "year" : 2018
    }, {
      "title" : "Stronger baselines for grammatical error correction using a pretrained encoder-decoder model",
      "author" : [ "Satoru Katsumata", "Mamoru Komachi." ],
      "venue" : "Proceedings of AACL, pages 827–832.",
      "citeRegEx" : "Katsumata and Komachi.,? 2020",
      "shortCiteRegEx" : "Katsumata and Komachi.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Tail-to-tail nonautoregressive sequence prediction for chinese grammatical error correction",
      "author" : [ "Piji Li", "Shuming Shi." ],
      "venue" : "Proceedings of ACL, pages 4973–4984.",
      "citeRegEx" : "Li and Shi.,? 2021",
      "shortCiteRegEx" : "Li and Shi.",
      "year" : 2021
    }, {
      "title" : "Bert enhanced neural machine translation and sequence tagging model for chinese grammatical error diagnosis",
      "author" : [ "Deng Liang", "Chen Zheng", "Lei Guo", "Xin Cui", "Xiuzhang Xiong", "Hengqiao Rong", "Jinpeng Dong." ],
      "venue" : "Proceedings of NLPTEA@AACL,",
      "citeRegEx" : "Liang et al\\.,? 2020",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Encode, tag, realize: High-precision text editing",
      "author" : [ "Eric Malmi", "Sebastian Krause", "Sascha Rothe", "Daniil Mirylenka", "Aliaksei Severyn." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 5054–5065.",
      "citeRegEx" : "Malmi et al\\.,? 2019",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-corpora evaluation and analysis of grammatical error correction models—is single-corpus evaluation enough",
      "author" : [ "Masato Mita", "Tomoya Mizumoto", "Masahiro Kaneko", "Ryo Nagata", "Kentaro Inui" ],
      "venue" : "In Proceedings of NAACL-HLT (Short),",
      "citeRegEx" : "Mita et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Mita et al\\.",
      "year" : 2019
    }, {
      "title" : "Phrase structure annotation and parsing for learner english",
      "author" : [ "Ryo Nagata", "Keisuke Sakaguchi." ],
      "venue" : "Proceedings of ACL, pages 1837–1847.",
      "citeRegEx" : "Nagata and Sakaguchi.,? 2016",
      "shortCiteRegEx" : "Nagata and Sakaguchi.",
      "year" : 2016
    }, {
      "title" : "Enabling robust grammatical error correction in new domains: Data sets, metrics, and analyses",
      "author" : [ "Courtney Napoles", "Maria Nădejde", "Joel Tetreault." ],
      "venue" : "TACL, 7:551–566.",
      "citeRegEx" : "Napoles et al\\.,? 2019",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2019
    }, {
      "title" : "Jfleg: A fluency corpus and benchmark for grammatical error correction",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault." ],
      "venue" : "Proceedings of EACL, pages 229–234.",
      "citeRegEx" : "Napoles et al\\.,? 2017",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2017
    }, {
      "title" : "The conll-2014 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant." ],
      "venue" : "Proceedings of CoNLL: Shared Task, pages 1–14.",
      "citeRegEx" : "Ng et al\\.,? 2014",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2014
    }, {
      "title" : "The conll2013 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault." ],
      "venue" : "Proceedings of CoNLL: Shared Task, pages 1–12.",
      "citeRegEx" : "Ng et al\\.,? 2013",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2013
    }, {
      "title" : "Gector–grammatical error correction: Tag, not rewrite",
      "author" : [ "Kostiantyn Omelianchuk", "Vitaliy Atrasevych", "Artem Chernodub", "Oleksandr Skurzhanskyi." ],
      "venue" : "Proceedings of BEA@ACL, pages 163– 170.",
      "citeRegEx" : "Omelianchuk et al\\.,? 2020",
      "shortCiteRegEx" : "Omelianchuk et al\\.",
      "year" : 2020
    }, {
      "title" : "Overview of nlptea-2018 share task chinese grammatical error diagnosis",
      "author" : [ "Gaoqi Rao", "Qi Gong", "Baolin Zhang", "Endong Xun." ],
      "venue" : "Proceedings of NLPTEA@ACL, pages 42–51.",
      "citeRegEx" : "Rao et al\\.,? 2018",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2018
    }, {
      "title" : "Overview of nlptea-2020 shared task for chinese grammatical error diagnosis",
      "author" : [ "Gaoqi Rao", "Erhong Yang", "Baolin Zhang." ],
      "venue" : "Proceedings of NLPTEA@AACL, pages 25–35.",
      "citeRegEx" : "Rao et al\\.,? 2020",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2020
    }, {
      "title" : "A sequence to sequence learning for chinese grammatical error correction",
      "author" : [ "Hongkai Ren", "Liner Yang", "Endong Xun." ],
      "venue" : "CCF International Conference on Natural Language Processing and Chinese Computing (NLPCC), pages 401–410.",
      "citeRegEx" : "Ren et al\\.,? 2018",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple recipe for multilingual grammatical error correction",
      "author" : [ "Sascha Rothe", "Jonathan Mallinson", "Eric Malmi", "Sebastian Krause", "Aliaksei Severyn." ],
      "venue" : "Proceedings of ACL-IJCNLP, pages 702–707.",
      "citeRegEx" : "Rothe et al\\.,? 2021",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2021
    }, {
      "title" : "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality",
      "author" : [ "Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault." ],
      "venue" : "TACL, 4:169–182.",
      "citeRegEx" : "Sakaguchi et al\\.,? 2016",
      "shortCiteRegEx" : "Sakaguchi et al\\.",
      "year" : 2016
    }, {
      "title" : "Cpt: A pre-trained unbalanced transformer for both chinese language understanding and generation",
      "author" : [ "Yunfan Shao", "Zhichao Geng", "Yitao Liu", "Junqi Dai", "Fei Yang", "Li Zhe", "Hujun Bao", "Xipeng Qiu." ],
      "venue" : "arXiv preprint arXiv:2109.05729.",
      "citeRegEx" : "Shao et al\\.,? 2021",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2021
    }, {
      "title" : "Uagec: Grammatical error correction and fluency corpus for the ukrainian language",
      "author" : [ "Oleksiy Syvokon", "Olena Nahorna." ],
      "venue" : "arXiv preprint arXiv:2103.16997.",
      "citeRegEx" : "Syvokon and Nahorna.,? 2021",
      "shortCiteRegEx" : "Syvokon and Nahorna.",
      "year" : 2021
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of ICCV, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Chinese grammatical error correction enhanced by data augmentation from word and character levels",
      "author" : [ "Zecheng Tang", "Yixin Ji", "Yibo Zhao", "Junhui Li." ],
      "venue" : "Proceedings of the 20th Chinese National Conference on Computational Linguistics",
      "citeRegEx" : "Tang et al\\.,? 2021",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NIPS, pages 5998– 6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Chinese grammatical error correction method based on transformer enhanced architecture",
      "author" : [ "Chencheng Wang", "Liner Yang", "Yingying Wang", "Yongping Du", "Erhong Yang." ],
      "venue" : "Journal of Chinese Information Processing, 34(6):106–114.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Structbert: Incorporating language structures into pre-training for deep language understanding",
      "author" : [ "Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "A comprehensive survey of grammatical error correction",
      "author" : [ "Yu Wang", "Yuelin Wang", "Kai Dang", "Jie Liu", "Zhuo Liu." ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST), 12(5):1–51.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Hsk dynamic composition corpus",
      "author" : [ "Endong Xun." ],
      "venue" : "http://hsk.blcu.edu.cn/.",
      "citeRegEx" : "Xun.,? 2018",
      "shortCiteRegEx" : "Xun.",
      "year" : 2018
    }, {
      "title" : "A new dataset and method for automatically grading esol texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of ACL, pages 180–189.",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    }, {
      "title" : "Grammatical error correction using neural machine translation",
      "author" : [ "Zheng Yuan", "Ted Briscoe." ],
      "venue" : "Proceedings of NAACL-HLT, pages 380–386.",
      "citeRegEx" : "Yuan and Briscoe.,? 2016",
      "shortCiteRegEx" : "Yuan and Briscoe.",
      "year" : 2016
    }, {
      "title" : "Overview of the nlpcc 2018",
      "author" : [ "Yuanyuan Zhao", "Nan Jiang", "Weiwei Sun", "Xiaojun Wan" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Maskgec: Improving neural grammatical error correction via dynamic masking",
      "author" : [ "Zewei Zhao", "Houfeng Wang." ],
      "venue" : "Proceedings of AAAI, pages 1226–1233.",
      "citeRegEx" : "Zhao and Wang.,? 2020",
      "shortCiteRegEx" : "Zhao and Wang.",
      "year" : 2020
    }, {
      "title" : "Chinese grammatical error correction using statistical and neural models",
      "author" : [ "Junpei Zhou", "Chen Li", "Hengyou Liu", "Zuyi Bao", "Guangwei Xu", "Linlin Li." ],
      "venue" : "CCF International Conference on Natural Language Processing and Chinese Computing",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Recently, there has been increasing attention to GEC for its vital value in various downstream scenarios (Grundkiewicz et al., 2020; Wang et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 151
    }, {
      "referenceID" : 44,
      "context" : "Recently, there has been increasing attention to GEC for its vital value in various downstream scenarios (Grundkiewicz et al., 2020; Wang et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 151
    }, {
      "referenceID" : 46,
      "context" : "For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 206
    }, {
      "referenceID" : 29,
      "context" : "For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 206
    }, {
      "referenceID" : 28,
      "context" : "For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 206
    }, {
      "referenceID" : 27,
      "context" : "For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 206
    }, {
      "referenceID" : 13,
      "context" : "For English GEC (EGEC), such datasets are abundant (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Ng et al., 2014; Napoles et al., 2017; Bryant et al., 2019; Napoles et al., 2019; Flachs et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 206
    }, {
      "referenceID" : 48,
      "context" : "The two publicly available datasets are NLPCC18 and CGED, contributed by the NLPCC2018 (Zhao et al., 2018) and CGED-2018&2020 shared tasks (Rao et al.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : "such as CoNLL14-test (Ng et al., 2014) and BEA19-test (Bryant et al.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 36,
      "context" : "To mitigate this phenomenon, a routine solution is increasing the number of references (Sakaguchi et al., 2016; Choshen and Abend, 2018).",
      "startOffset" : 87,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "To mitigate this phenomenon, a routine solution is increasing the number of references (Sakaguchi et al., 2016; Choshen and Abend, 2018).",
      "startOffset" : 87,
      "endOffset" : 136
    }, {
      "referenceID" : 25,
      "context" : "Besides the lack of multiple references, all existing CGEC datasets collect sentences from a single text source, which may be insufficient for robust model evaluation (Mita et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 186
    }, {
      "referenceID" : 48,
      "context" : "(1) We re-annotate the NLPCC18 test set (Zhao et al., 2018), which contains 2,000 sentences from the Peking University (PKU) Chinese Learner Corpus.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "First, it is extremely difficult for different annotators to agree upon the boundaries of the erroneous spans and their error types, especially when there are many categories to consider (Bryant et al., 2017).",
      "startOffset" : 187,
      "endOffset" : 208
    }, {
      "referenceID" : 3,
      "context" : "Edits are extracted automatically from parallel sentences by additional tools (Bryant et al., 2017).",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 36,
      "context" : "This annotation paradigm has been proved to be efficient and cheap (Sakaguchi et al., 2016), and adopted by many datasets in other languages (Napoles et al.",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 38,
      "context" : ", 2016), and adopted by many datasets in other languages (Napoles et al., 2017, 2019; Syvokon and Nahorna, 2021).",
      "startOffset" : 57,
      "endOffset" : 112
    }, {
      "referenceID" : 48,
      "context" : "We also include the original NLPCC18 dataset (Zhao et al., 2018) for comparison.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 26,
      "context" : "We believe the major reason is that the original NLPCC18 data are annotated under the minimal edit distance principle (Nagata and Sakaguchi, 2016), which requires annotators to select a reference with fewer edits when correcting.",
      "startOffset" : 118,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : "The Seq2Edit model treats GEC as a sequence labeling task and performs error corrections via a sequence of token-level edits, including insertion, deletion, and substitution (Malmi et al., 2019).",
      "startOffset" : 174,
      "endOffset" : 194
    }, {
      "referenceID" : 31,
      "context" : "With minor modifications to accommodate Chinese, we adopt GECToR (Omelianchuk et al., 2020), which achieves the SOTA performance on EGEC datasets.",
      "startOffset" : 65,
      "endOffset" : 91
    }, {
      "referenceID" : 43,
      "context" : "After comparing several popular PLMs, we choose StructBERT (Wang et al., 2019) 3 due to its superior performance after fine-tuning (see Table 4).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 47,
      "context" : "The Seq2Seq model straightforwardly treats GEC as a monolingual translation task (Yuan and Briscoe, 2016).",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 41,
      "context" : "Recent work proposes to enhance Transformer-based (Vaswani et al., 2017) Seq2Seq EGEC models with PLMs like T5 (Rothe et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 35,
      "context" : ", 2017) Seq2Seq EGEC models with PLMs like T5 (Rothe et al., 2021) or BART (Katsumata and Komachi, 2020).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "Unlike BERT (Devlin et al., 2019), T5 and BART are specifically designed for text generation.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "Several previous works have proved the effectiveness of model ensemble for CGEC (Liang et al., 2020; Hinson et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 121
    }, {
      "referenceID" : 17,
      "context" : "Several previous works have proved the effectiveness of model ensemble for CGEC (Liang et al., 2020; Hinson et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 121
    }, {
      "referenceID" : 48,
      "context" : "Table 4: Performance comparison on the original NLPCC18 dataset (Zhao et al., 2018) using the official word-based evaluation script.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 48,
      "context" : ", the Lang8 (Zhao et al., 2018) 4 data and the HSK (Xun, 2018) 5 data.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 45,
      "context" : ", 2018) 4 data and the HSK (Xun, 2018) 5 data.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "For a fair comparison, we follow the official setting of the shared task, including using the word-based MaxMatch scorer (Dahlmeier and Ng, 2012) for calculating the P/R/F values.",
      "startOffset" : 121,
      "endOffset" : 145
    }, {
      "referenceID" : 48,
      "context" : "We segment model outputs by adopting the PKUNLP word segmentation (WS) tool provided by the shared task organizers (Zhao et al., 2018).",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 49,
      "context" : "It only underperforms MaskGEC (Zhao and Wang, 2020) by 1 F0.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "For Seq2Edit, we additionally present results with other PLMs besides StructBERT, including BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : ", 2019), and MacBERT (Cui et al., 2020) from the hugging face 6 website.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 42,
      "context" : "For hyperparameter tuning or model selection, previous work on other CGEC datasets often randomly sample some sentence pairs from training data as the dev set (Wang et al., 2020; Zhao and Wang, 2020; Hinson et al., 2020), which is problematic for replicability and complicated for comparison.",
      "startOffset" : 159,
      "endOffset" : 220
    }, {
      "referenceID" : 49,
      "context" : "For hyperparameter tuning or model selection, previous work on other CGEC datasets often randomly sample some sentence pairs from training data as the dev set (Wang et al., 2020; Zhao and Wang, 2020; Hinson et al., 2020), which is problematic for replicability and complicated for comparison.",
      "startOffset" : 159,
      "endOffset" : 220
    }, {
      "referenceID" : 17,
      "context" : "For hyperparameter tuning or model selection, previous work on other CGEC datasets often randomly sample some sentence pairs from training data as the dev set (Wang et al., 2020; Zhao and Wang, 2020; Hinson et al., 2020), which is problematic for replicability and complicated for comparison.",
      "startOffset" : 159,
      "endOffset" : 220
    }, {
      "referenceID" : 14,
      "context" : "Different from English whose words are separated naturally, Chinese sentences are written without any word delimiters, so WS models perform much worse for Chinese than for English (Fu et al., 2020).",
      "startOffset" : 180,
      "endOffset" : 197
    }, {
      "referenceID" : 12,
      "context" : "Secondly, following the standard practice in both EGEC and CGEC, we merge consecutive edits of the same type as one span-level edit (Felice et al., 2016; Hinson et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 174
    }, {
      "referenceID" : 17,
      "context" : "Secondly, following the standard practice in both EGEC and CGEC, we merge consecutive edits of the same type as one span-level edit (Felice et al., 2016; Hinson et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "Finally, we utilize the evaluation script from ERRANT (Bryant et al., 2017) to calculate the P/R/F values which does not need error-coded annotation.",
      "startOffset" : 54,
      "endOffset" : 75
    } ],
    "year" : 0,
    "abstractText" : "This paper presents MuCGEC, a multireference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences from three different Chinese-as-a-SecondLanguage (CSL) learner sources. Each sentence has been corrected by three annotators, and their corrections are meticulously reviewed by an expert, resulting in 2.3 references on average per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence (Seq2Seq) model and the sequence-to-edit (Seq2Edit) model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets.We also discuss the CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. We will release our annotation guidelines, data, and code.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_9_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Measuring Fairness of Text Classifiers via Prediction Sensitivity",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Ongoing research is increasingly emphasizing the development of methods which detect and mitigate unfair social bias present in machine learningbased language processing models. These methods come under the umbrella of algorithmic fairness which has been quantitatively expressed with numerous definitions (Mehrabi et al., 2019b; Jacobs and Wallach, 2021). These fairness definitions are broadly categorized into two types, i.e, individual fairness and group fairness. Individual fairness (e.g., counter-factual fairness (Kusner et al., 2017))\nis aimed at evaluating whether a model gives similar predictions for individuals with similar personal attributes (e.g., age or race). On the other hand, group fairness (e.g., statistical parity (Dwork et al., 2012)) evaluates fairness across cohorts with same protected attributes instead of individuals (Mehrabi et al., 2019b). Although these two broad categories of fairness define valid notions of fairness, human understanding of fairness is also used to measure fairness in machine learning models (Dhamala et al., 2021). Existing studies often consider only one or two these verticals of measuring fairness.\nIn our work, we propose a formulation based on models sensitivity to input features – the accumulated prediction sensitivity, to measure fairness of model predictions. We establish its theoretical relationship with statistical parity (group fairness) and individual fairness (Dwork et al., 2012) metrics. We then demonstrate the correlation between the proposed metric and human perception of fairness using empirical experiments.\nResearchers have proposed metrics to quantify fairness based on a model’s sensitivity to input features. Specifically, Maughan and Near (2020); Ngong et al. (2020) propose a prediction sensitivity metric that attempts to quantify the extent to which a single prediction depends on a protected attribute. The protected attribute encodes the membership status of an individual in a protected group. Prediction sensitivity can be seen as a form of feature attribution, but specialized to the protected attribute. In our work, we extend their concept of prediction sensitivity to propose accumulated prediction sensitivity. Akin to the metric proposed by (Maughan and Near, 2020; Ngong et al., 2020), our metric also relies on model output’s sensitivity to changes in input features. Our metric generalizes their notion of sensitivity, where the model sensitivity to various input features can be weighted non-uniformly. We show that the formulation follows certain properties for the chosen definitions\nof group and individual fairness and also present several methodologies to select weights assigned to sensitivity of model’s output to input features. For each selection, we present the correlation between the accumulated prediction sensitivity and human assessment of the model-output fairness.\nWe define our metric in Section 3 and present bounds on it (under settings when a classifier follows the selected group fairness or individual fairness constraints) in Sections 4 and 5, respectively. Next, given that the human perception of fairness is not theoretically defined, we present an empirical study on two text classification tasks in Section 6. We request a group of annotators to annotate whether they think that model output is biased against a specific gender and observe that the proposed metric correlates positively with more biased outcomes. We then observe correlations between our metric and the stated human understanding of fairness. We find that not only the proposed accumulated prediction sensitivity metric correlates positively with human perception of bias, but also beats an existing baseline based on counterfactual fairness."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multiple efforts have looked into defining, measuring, and mitigating biases in NLP models (Sun et al., 2019; Mehrabi et al., 2019a; Sheng et al., 2021). Dwork et al. (2012) and Kusner et al. (2017) focus on individual fairness and propose novel classification approaches to ensure that a classification decision is fair towards an individual. Another set of works focus on group fairness. Corbett-Davies et al. (2017) present fair classification to ensure population from different race groups receive similar treatment. Hardt et al. (2016) focus on shifting the cost of incorrect classification from disadvantaged groups. Zhao and Chang (2020) measure group fairness in local regions. Finally, Kearns et al. (2019) combine the best properties of the group and individual notions of fairness.\nMultiple recent works also focus on developing new dataset and associated metrics to capture various types of biases. For example, Dhamala et al. (2021) and Nangia et al. (2020) propose dataset and metrics to measure social biases and stereotypes in language model generations, Bolukbasi et al. (2016); Caliskan et al. (2017); Manzini et al. (2019) define metrics to access gender and race biases in word vector representations, and Wang\net al. (2019) define metrics to quantify and mitigate biases in visual recognition task. Ethayarajh (2020) propose Bernstein bounds to represent uncertainty about the bias. Majority of these bias metrics are automatically computed, for example, using a regard classifier (Sheng et al., 2019), sentiment classifier (Dhamala et al., 2021), toxicity classifier (Dixon et al., 2018) or true positive rate difference between privileged and underprivileged groups (De-Arteaga et al., 2019b). A few works additionally validate the alignment of these automatically computed bias metrics with human understanding of biases by collecting annotations of biases on a subset of test data from crowdworkers (Sheng et al., 2019; Dhamala et al., 2021). Blodgett et al. (2021, 2020) discuss the limitations of several of these bias datasets and measurements.\nHowever, the majority of existing bias metrics are specific to the model type and the application domain used, they may not be tested for correlation with human judgement of biases, and their relationship to existing definitions of fairness has not been explored. Additionally, metrics such as true positive or error difference between groups requires ground truth labels, thereby making their computation in real-time systems difficult. Speicher et al. (2018) have attempted to present unified approach to measuring group and individual fairness via inequality indices, however we note that such metrics are non-trivial to extend to unstructured data such as text. For example, gender information in a text may be subtle (e.g. mention of softball) and it is unclear whether presence of this word should be considered to impact the genderness of the text. Accumulated prediction sensitivity metric, presented in this paper, attempts to address all the above limitations of existing bias metrics. We acknowledge that the proposed metric is yet to be associated with other notions of fairness (e.g. preference based notion of fairness (Zafar et al., 2017))."
    }, {
      "heading" : "3 Accumulated Prediction Sensitivity",
      "text" : "Below, we define accumulated prediction sensitivity, a metric that capture the sensitivity of a model to protected attributes.\nDefinition 1 (Accumulated Prediction sensitivity). Let x ∈ X be a feature vector drawn from the input space X . Let w,v be stochastic vectors whose entries are non-negative values that sum to one. Given x, let f be a K-class classifier, such that f(x) = [f1(x), .., fk(x), .., fK(x)] denotes\nthe K-dimensional probability output generated by the classifier. We define accumulated prediction sensitivity P as:\nP = wTJv; where J(k, i) = ∣∣∣∣∂fk(x)∂xi ∣∣∣∣ . (1) J is a matrix1 such that the (k, i)th entry is∣∣∣∂fk(x)∂xi ∣∣∣, where xi is the ith entry in x. The product wTJ sums the absolute derivatives |∂fk(x)∂xi | across fk, k = 1, ..,K and returns a vector of summed derivatives with respect to each xi ∈ x. The product of v withwTJ further averages the derivatives across all the features xi ∈ x to yield the scalar P .\nThe value ∂fk(x)∂xi captures the expected change in model output for the kth class given a perturbation in xi. If xi is a protected feature, arguably a smaller value of ∂fk(x)∂xi implies a fairer model; as then the model’s outcome does not change sharply with changes in xi. To capture the sensitivity of the model with respect to the protected features, one also needs to choose v judiciously. For example, given the explicit set of protected features in x, one can select v such that only entries corresponding to those features are assigned a non-zero value, while the rest are set to zero. Given this heuristic, we expect the value P to be smaller for fairer models. In the following sections, we connect the accumulated prediction sensitivity to two known notions of fairness and human perception of fairness."
    }, {
      "heading" : "4 Relation to Group Fairness: Statistical Parity",
      "text" : "Given a set of protected features (e.g. gender), a model satisfies statistical parity if model outcome is independent of the protected features (we note that identifying protected features may not always be feasible in the real world). We represent the feature vector x = [xp,xl], where xp is the set of protected features and xl is the remainder. Accordingly, we choose v to be a vector such that the entries that sum |∂fk(xp)∂xi |∀xp ∈ xp in J are nonzero; and zero otherwise. This choice is intuitive as then we sum the gradients in J that correspond to protected features and measure model’s sensitivity to them. The predictor f(x) will satisfy statistical parity if f(xp,xl) = f(x′p,xl)∀xp 6= x′p. Given this, we state the following theorem.\n1Note that we use the following notation scheme in this paper – bold capital letters for matrices, bold small letters for vectors and un-bolded letters for scalars.\nTheorem 1. Given a vector v with non-zero entries corresponding to xp and zero entries for xl, if the predictor f(x) satisfies statistical parity with respect to xp, accumulated prediction sensitivity will be zero.\nProof: If f(x) satisfies statistical parity with respect toxp, the values\n∂fk(x) ∂xp ∀xp ∈ xp will be all\nzeros. This is due to the fact that the function fk(x) can not be defined based on entries xp ∈ xp for it to be independent of them. Therefore, for every multiplication in the product Jv, either the entry ∂fk(x) ∂xp\nwill be 0 or the entry in v corresponding to xl will be 0. Hence, P will be 0.\nAppendix A presents empirical results in computing P on a synthetic dataset. We construct a dataset where a feature (hair length) correlates with a protected attribute (gender). We show that if the modeler unintentionally uses the correlated feature while attempting to build a classifier with statistical parity, our metric can be used for evaluation."
    }, {
      "heading" : "5 Relation to Individual Fairness",
      "text" : "Dwork et al. (2012) state the notion of individual based fairness as: \"We interpret the goal of mapping similar people similarly to mean that the distributions assigned to similar people are similar\". They propose adding a Lipschitz property constraint during the classifier optimization. Given a loss function L defined to optimize the parameters θ of the classifier f(x), a distance function d(x,x′) that computes distance between data-points x,x′, another distance function D(f(x)),f(x′)) that computes distance between classifier predictions on x,x′ and a constant L, Dwork et al. (2012) propose the following constrained optimization.\nmin θ L; such that D(f(x)),f(x′)) < Ld(x,x′);∀x,x′ ∈X. (2)\nIt is natural to choose an Lp norm (Bourbaki, 1987) for d and D. For a classifier f that is trained with the above constrained optimization and the choice of distance metrics D, d is an Lp norm, we state the following.\nTheorem 2. If the predictor f(x) is trained with the constrained optimization stated in Eq. (2), the accumulated prediction sensitivity will be upper bounded by L.\nProof: We restate the constraint in Eq. (2) as (Note that the inequality sign does not change as\ndistance metricsD, d are required to be positive for x 6= x′)\n∀x 6= x′, L > D(f(x),f(x ′))\nd(x,x′) . (3)\nGiven the inequality holds for any pair of x,x′, it must also hold for an x′ of the following choice. x′ = x + [0, 0,∆xi, 0, 0], where ∆xi is a scalar perturbation in the ith entry in x. For a chosen Lp norm, Eq (3) becomes\nL > [ ∑K k=1 |fk(x)− fk(x′)|p] 1 p\n|∆xi|\n> [|fk(x)− fk(x′)|p]\n1 p\n|∆xi| . (4)\nSince each entry |fk(x)− fk(x′)|p, k = 1, ..K is expected to be non-zero and zeroing out all such entries (but one) will yield a lower value than the summation ∑K k=1 |fk(x) − fk(x′)|p. We can rewrite Eq. (4) as:\n|fk(x)− fk(x+ [0, 0,∆xi, 0, 0])| |∆xi| .\nWe can further chose ∆xi such that it is small perturbation, leading to the following.\nL > lim ∆xi→0 |fk(x)− fk(x+ [0, 0,∆xi, 0, 0])| |∆xi|\n= ∣∣∣∂fk(x) ∂xi ∣∣∣. Therefore, each entry in J is upper bounded by L. As vectors v,w are stochastic and they compute weighted averages of bounded entries in J , P (defined in Eq. (1)) must be less than or equal to L.\nWe also note that as L becomes larger, the constraint in the Eq. (2) becomes looser. Therefore, a higher value of L during optimization is expected to loosen the fairness constraint as well as the bound on fairness sensitivity. This aligns with our intuition of lower values of P for fairer models. We compute value of L on a synthetically generated classification data, optimized with the individual fairness constraint in equation 2. The results are presented in Appendix B."
    }, {
      "heading" : "6 Correlations with Human Perception of Fairness",
      "text" : "While the conditional statistical parity and individual fairness establish theoretical constraints\non the model behaviour (e.g. independence from protected features and similarity in prediction outcomes for similar data-points), humans may carry a different notion of fairness for model outcomes on individual data-points. This notion may be based on their understanding of cultural norms, which in turn effect their decisions in identifying which model outputs could be considered biased. In this section, we present experiments that correlate accumulated prediction sensitivity with human perception of fairness."
    }, {
      "heading" : "6.1 Human Perception of Fairness",
      "text" : "Given a data-point x and model prediction f(x), we assign one of the K classes to the data-point. In order to evaluate the human perception of fairness on the data-point, we request a group of annotators to evaluate the model prediction (taken as the argmax of the model output) and assess whether they believe the output is biased. For instance, given the social/cultural norms, a profession classifier assigning a data-point “she worked in a hospital” to nurse instead of doctor can be perceived as biased. To correlate the accumulated prediction sensitivity P with the human understanding of fairness, we conduct experiments on two text classification datasets. We describe the datasets below, followed by our choices for w and v."
    }, {
      "heading" : "6.2 Datasets",
      "text" : "We experiment with our proposed metric on two classification tasks, i.e, occupation classification on Bias in Bios dataset (De-Arteaga et al., 2019a)2 and toxicity classification with Jigsaw Toxicity dataset3. We focus on these two datasets as they have been investigated in several previous studies (Pruksachatkun et al., 2021) and have been reported to carry significant presence of bias. BIAS IN BIOS data (De-Arteaga et al., 2019a) is purposed to train occupation classifier which predicts occupation given the biography of an individual. For this data, the task classifier is an occupation classification model which is composed of a standard LSTMbased encoder combined with the output layer of 28 nodes, i.e, number of occupation classes. JIGSAW TOXICITY dataset is commonly used to train toxic classifier which is tasked to predict if an input sentence is toxic or not. This dataset has input sentences as the comments from Wikipedia’s talk\n2The data is available at https://github.com/microsoft/biosbias\n3The data is available at https://www.kaggle.com/c/jigsawunintended-bias-in-toxicity-classification\npage edits labeled with the degree of toxicity. In this dataset, the task classifier is a binary classifier trained to predict whether a comment is toxic or not. We labeled the samples with >0.5 toxicity score as toxic and others as non-toxic to train the task classifier. The task classifier trained with Jigsaw Toxicity dataset achieved an AUC of 0.957. Table 4 in appendix summarizes the train/test/valid split for the 2 datasets."
    }, {
      "heading" : "6.3 Selecting the vectors w",
      "text" : "The vectorw sums up the absolute partial derivatives of fk(x) with respect to a given feature xi, ∀k = 1, ..,K. In our setup, we consider input features to be the word embeddings and the matrix J is computed over the same. Given a Ddimensional word embedding, K classes and N words in x, J will be a matrix of size (K)×(DN). In all our experiments, we choose w to be a uniform vector with entries 1/K. Such a choice assigns equal weight to the partial derivatives computed over each class. One may chose to put a higher weight on derivatives computed over a specific class, if there is a reason to believe that the accumulated prediction sensitivity should be informed more with respect to that class. For instance, for a classifier that stratifies medical images into various diseases (Agrawal et al., 2019), disparity in model performance with respect to malicious diseases can be considered more costly. Therefore, derivatives for classes that represent more malicious disease can be weighted higher."
    }, {
      "heading" : "6.4 Selecting the vectors v",
      "text" : "Through the vector v, we aim to select words in x that carry gendered information. We use two formulations for the the vector v as discussed below."
    }, {
      "heading" : "6.4.1 Using a list of gendered words",
      "text" : "In this setup, we use the set of gendered words from (Bolukbasi et al., 2016) and assign entries in v corresponding to those words as 1/(Ng × D), where Ng is the count of gendered words in the data-point."
    }, {
      "heading" : "6.4.2 Using a Protected Status Model (PSM)",
      "text" : "While prior work has used word matching to a pre-defined corpus of tokens describing various demographic cohorts (Bolukbasi et al., 2016), these corpus do not contain words that stereotypically are associated with a particular cohort but may not be explicitly tied to that cohort. For example, the word “volleyball” is associated with females in the analysis presented by (Dinan et al., 2020).\nTo capture this nuance, we propose using another classifier (that acts on the same dataset as used to train the original classifier, for which we aim to compute P ) and using it to identify tokens containing information about the protected attribute (e.g. gender). We discuss the model training below.\nProtected Status Model: To extend accumulated prediction sensitivity to settings with no explicit protected attribute, we train a protected status model g. Given the data-point x, goal of the PSM model g(x) is to predict the protected attributes. Given a trained g(x), we then compute another matrix Jg, where the (j, i)th entry is |∂gm(x)xi | (gm is the probability outcomes corresponding to the mth protected attribute class; e.g. male in a gender classifier). We then define an entry vi ∈ v as ∑\nj Jg(m, i) (the vector v is normalized to be stochastic). Intuitively, the sum ∑ j Jg(m, i) captures the model output sensitivity with respect to the input features xi and is expected to higher if xi carries more gendered information.\nIn our experiments, we train separate PSM models for gender sensitivity computation on Bias-inbios and Jigsaw data-sets, as each data-point in these data-sets is additionally labeled with a binary gender class (male/female)4. Gender PSMs predicts the associated gender given the datapoint x. Training PSM on the same datasets used to train the task classifier f helps capture the gender stereotypes present in the respective datasets. For instance, in a given dataset, if the word “volleyball” appears more often in the data-points that correspond to the female gender, the gender classifier’s sensitivity to this word is expected to be high as the classifier may pay higher emphasis to this word for gender classification. We use the same model architecture as the task classifiers for PSM. PSM for gender classification achieve an accuracy of 98.79% (Male Acc:98.84% Female Acc:98.17%) and 95.39% (Male Acc:95.92% Female Acc:96.22%) for Bias in bios and Jigsaw Toxicity datasets, respectively. These accuracies are computed over the same train/test split as the task classifier."
    }, {
      "heading" : "6.4.3 Using Word Embedding Vectors",
      "text" : "In addition to using the list of gendered words and PSM, we also test with a setting where we multiply the word embedding vectors to the proposed formulations of v. We stack the word embedding\n4We note that this is a limitation of this work as gender can be non-binary.\nvectors for each word xi ∈ x to obtain a vector of embeddings ei. We perform an element-wise multiplication of the embedding vectors ei with the vector with entries 1/(Ng×D) for gendered words or ∑\nj Jg(j, i) obtained using PSM. This choice is motivated based upon the findings in (Han et al., 2020). They leverage the magnitude of embedding vectors in determining saliency of the input words for the classification task at hand. Their proposed methodology computes saliency maps over the features xi ∈ x by multiplying embedding vectors with partial derivatives of the class probabilities with respect to embedding vectors themselves."
    }, {
      "heading" : "6.5 Fairness Metrics",
      "text" : "We experiment with six fairness metrics. Out of the six, one metric is a baseline based on counterfactual fairness and the rest are variants of the accumulated prediction sensitivity P .\nCounter-factual Fairness (CF) : We use the counter-factual fairness definition mentioned in Garg et al. (2019) and compute the metric as the difference in model predictions between the original sample f(x) and its corresponding counter-factual gendered sample f(x̂). We take the L1 norm of the vector f(x) − f(x̂). For example, we take the difference in predictions between the sample \"She practices dentistry\" and \"He practices dentistry\", which is the corresponding counter-factual sample. We use the definitional gender token substitutions from Bolukbasi et al. (2016) to create counter-factual samples. P1: Uniformly weighted prediction sensitivity : In this setting, the values of w and v are set to uniform values 1K and 1 DN , respectively. This is a weak baseline as the choice of v does not provide any information regarding the gender-ness of the input words. P2: Weighted Prediction Sensitivity based on\nPSM : In this setting, w is chosen to be a uniform vector, while v is chosen based on the PSM model. P3: Weighted Prediction sensitivity + Embedding weights : In this setting, v is chosen based on the PSM model (akin to the metric in P2) which is further multiplied element-wise with the word embedding vectors. P4: Hard gender weights based Prediction sensitivity : In this metric, we use the list of gendered words described in section 6.4.1 to determine v. The value of entries in v is set to 1DNg . P5: Hard gender weights based prediction sensitivity + Embeddings: This setting is same as above, except entries in v are further multiplied element-wise with the word embedding vectors."
    }, {
      "heading" : "6.6 Evaluation",
      "text" : "To evaluate whether the proposed prediction sensitivity correlates with human perception of fairness, we collect annotations from crowd workers using the Amazon Mechanical Turk platform. Crowd workers are asked to annotate if a model prediction appears to be a biased prediction or not. For Bias in Bios dataset, each sample presented to the annotators has the biography and occupation predicted by the model. We collect annotations on a random sample of the test set. For each biography and a predicted occupation, we ask annotators to label if the prediction is indicative of bias or if it is unbiased. Bias refers to a situation where an occupation is incorrectly predicted based on the gender associated with the biography. For instance, if the input biography is “she studied at Harvard Medical School and practices dentistry.” and is predicted as nurse, then we call this prediction biased since the biography fits better for a doctor. In case of unbiased predictions, the prediction is not expected to be influenced by the gender content in the biography. Table 3 presents a sample of exam-\nEXAMPLES OF UNBIASED SAMPLES (The predicted profession is unrelated to gender stereotype about professions)\nBIO: She received a master’s degree in computer science from the university of North Carolina at Chapel Hill. Predicted Profession: Computer Scientist\nBIO: He received a master’s degree in computer science from the university of North Carolina at Chapel Hill. Predicted Profession: Computer Scientist\nEXAMPLES OF BIASED SAMPLES (Strongly biased predictions are based on associating a specific gender to a specific profession even when there are evidences against it in the biography)\nBIO: Mary has 25 years of experience in data analytics, business intelligence and information governance with fortune 100 companies. Predicted Profession: Nurse\nBIO: He achieved a masters degree in nursing from the university of north Carolina at chapel hill Predicted Profession: Computer Scientist\nTable 3: Examples of biased/unbiased outcomes shown to the M-turk annotators\nples provided to the annotators for the Bias in bios dataset. Each page in the annotation task consisted of ten biography-profession pairs. We collect annotations for each biography-profession pair from at least three annotators and pick the label with majority vote. Similarly for Jigsaw Toxicity dataset, each sample presented to the annotators contains the text and associated toxicity predicted by the model.\nWe restrict the set of annotators to be master annotators and the location of annotators to be Unites States. Based on the initial pilot studies conducted in the Amazon Mechanical Turk platform, we setup a payment rate to ensure a fair compensation of at least 15$/hour for all annotators that work at an average pace. We annotated 900 test data-points from each dataset. We note that these test data-points were misclassified by the classifiers f trained for each dataset. While such a sampling may not con-\nform to the true distribution of biased/unbiased model outcomes on the overall test set, we expect to get more biased samples amongst the misclassified samples. The distribution between biased and unbiased outputs was about 55:45 for Bias in Bios and 50:50 for Jigsaw Toxicity. For the Bias in Bios and Jigsaw Toxicity datsets, we obtained a Fliess’ kappa of 0.43 and 0.47, respectively, amongst the three annotators. This is considered a moderate level of agreement, which we believe is expected for an relatively ambiguous task to identify model outcomes influenced by gender. We compute mutual information and bi-serial correlations as the primary measures of association between the human annotations and the accumulated model sensitivity."
    }, {
      "heading" : "7 Results",
      "text" : "Table 1 lists the bi-serial correlations and mutual information between manual annotations and the different fairness metrics. First, we observe that correlations of the baseline with human judgement are mediocre (0.326 and 0.214) compared to the human judgement. We attribute this to the fact that the metric attempts to quantify a fairly subjective assessment of bias that may have different interpretation (as also pointed out by the moderate level of annotation agreement across annotators). However, the proposed variants of P have stronger correlations compared to the counter-factual baseline (except the method P1). As expected, we see the smallest correlation for P1, since this metric does not account for gender-ness in v. However, metrics that determine v based on PSM prediction sensitivity and gendered words get higher correlations over P1 and the CF baseline. Variant of P with v informed using the embedding vectors further lead to improved correlations. We also observe weaker statistical significance in the case of Jigsaw Toxicity due to a weaker PSM. We attribute\nthis to the noise present in gender annotations for Jigsaw Toxicity dataset. Hence, the performance of PSM in predicting the protected status is crucial for accurately measuring fairness."
    }, {
      "heading" : "7.1 Discussion",
      "text" : "In order to further analyse the effect of PSM, we look into heat-maps capturing wTJ and v separately. As a reminder, the first quantity captures the weighted average of partial derivatives of class probabilites with respect to the input features, while the second quantity computes the weights assigned to sum up the aforementioned averages. Table 2 shows while v mostly captures gendered words such as “she”, “her” and “woman”, it also captures words such as “social”, “architecture” and “cheated” to carry more gendered information compared to other words. While these words conventionally are not gendered, for the datasets at hand, they seem to provide information whether the input data-point belongs to male/female gender. We also note that wTJ weighs on occupation specific tokens such as \"physician\", \"executive\", etc.\nThis finding supports our motivations to compute v based on PSM and capturing feature attributions assigned to tokens that are implicitly related to a specific gender (instead of the definitional gender tokens only). Hence, by incorporating PSM in computing P , we can capture bias present in nontrivial gendered tokens."
    }, {
      "heading" : "8 Considerations for Accumulated Prediction Sensitivity Metric",
      "text" : "While the results showcase the promise of our metric, we draw the attention of the reader to the following considerations: (1) We observed that the metric quality depends on choice of the hyperparameters w and v. In this regard, our metric is not different from other metrics that also depend on a hyper-parameter choice. For example, any classifier based metric has a threshold parameter and counterfactual fairness metrics rely on hyperparameters such as the selected gendered words. (2) Our metric only works for models for which gradients can be computed. Most modern deep learning based models carry this property. (3) Lastly, we note that it is hard to interpret the absolute value of the proposed metric. The metric value should be used for relative comparison of two models which share input feature space and label space.\nIn addition, we note two considerations for relying on a PSM classifier. First, training it requires\naccess to gender labels. Second, the PSM model itself could be biased. Given that gender labels may not always be available for the dataset used to train model at hand, we study the impact of transferring a PSM model trained on a different dataset on computing our metric. We also evaluate the effect of bias in PSM model on the overall metric value and present results in the Appendix D. We make observations such as the quality of the metric degrades as PSM becomes more biased. Based on these observations, we recommend that if modeler is not able to obtain high performance PSM models, they fall back to using sources such as gendered words for computing the vector v."
    }, {
      "heading" : "9 Conclusion",
      "text" : "Evaluating fairness is a challenging task as it requires selecting a notion of fairness (e.g. group or individual fairness) and then identifying metrics that can capture these notions of fairness while evaluating a classifier. Additionally, certain notions of fairness may not be well defined and can change based upon social norms (e.g. “volleyball” being closely associated with females); that may seep into the dataset at hand. In this work, we define an accumulated prediction sensitivity metric that relies on the partial derivatives of model’s class probabilities with respect to input features. We establish properties of this metric with respect to the three verticals of fairness metrics: group, individual and human-perception based. We provide bounds on the metric’s value when a predictor is expected to carry statistical parity or is trained with individual fairness. We also evaluate this metric with fairness as perceived through human evaluation of model outputs. We test variants of the proposed metric against an existing baseline derived from counter-factual fairness and observe better mutual information and correlation. Specifically, a variant of the metric that relies on a Protected Status Model (that identifies tokens that carry gender information but may not conventionally be considered gendered) yields the best correlation with the human evaluation.\nIn the future, one can associate the proposed formulation with other categories of group and individual fairness (Mehrabi et al., 2019a). We also aim to test the metric on other datasets with other protected attributes (e.g. race, nationality). Finally, we can compare the metric across these datasets to compare trends across protected groups."
    }, {
      "heading" : "10 Broader Impact",
      "text" : "This work can be used to evaluate bias in models, and thus used to evaluate models serving human consumers. As with all metrics, the metric does not capture all notions of bias, and thus should not be the only consideration for serving models. While this is a valid risk, this is one that is not specific to prediction sensitivity. Good use of this metric requires users to be cognizant of these strengths and weaknesses. We also note that the metric requires defining protected attributes (e.g. gender) and our work carries the limitation that the selected datasets contain binary gender annotations. Defining protected attributes may not always be possible and when possible, the protected attribute classes may not be comprehensive."
    }, {
      "heading" : "A Obtaining prediction sensitivity on classifier trained for statistical parity",
      "text" : "Let us consider a classification task on whether to hire a person given the following features: x1 is the person’s educational experience in years, x2 is their hair length and x3 is their gender. We synthetically generate data for individuals in this dataset. x1 is drawn uniformly randomly between 0 and 10. x3 is (again) considered to be binary gender (set 0 for male and 1 for female drawn from a bernoulli distribution) and x2 is drawn from a Gaussian distribution conditioned on x3. x2 ∼ N (2, 10) (Gaussian distribution with a mean 2 and variance 10) if x3 = 0 and x2 ∼ N (10, 10) if x3 = 1. We sample 10,000 data-points from the above distribution to generate a dataset. Let us consider two cases with two different classifiers.\nCase 1: Classifier depends on x1, x2 In this case, the modeler only deems x3 to be the protected feature. Let us assume that they build a classifier as shown in equation 5. Lets assume that the modeler assigns a hire decision if f > 0.5, otherwise not.\nf = σ((x1 − 5) + (x2 − 6)) (5) Given only x3 is considered as the protected feature by the modeler, they will set the vector v to [0, 0, 1]T . Let us assume that the modeler sets P as\nP = [\n1 2 1 2\n] [∣∣ ∂f1 ∂x1 ∣∣ ∣∣ ∂f1 ∂x2 ∣∣ ∣∣ ∂f1 ∂x3 ∣∣∣∣ ∂f2 ∂x1 ∣∣ ∣∣ ∂f2 ∂x2 ∣∣ ∣∣ ∂f2 ∂x3 ∣∣ ]00 1  (6) We recommend the modeler computes ∂x2∂x3 and\n∂x1 ∂x3 and if they are non-zero, use the chain rule in equation 7 to compute P .\n∂fk([x1, x2]) ∂x3 = ∂fk([x1, x2]) ∂x2 ∂x2 ∂x3\n(7)\nFor the dataset generated above, we compute the partials ∂x2∂x3 and ∂x1 ∂x3\n. Additionally, since x3 is a discrete variable, we approximate partial derivatives using all available right-difference quotients and left-difference quotients, as shown in equation 9. In order to compute ∂x2∂x3 at x3 = x m 3 (where x m 3 denotes the value of x3 for the mth data-point), we use the corresponding value of the feature x2 = xm2 in the mth data-point and all other available pairs (xn2 , x n 2 ), n 6= m.\n∂x2 ∂x3 ∣∣∣ x3=xm3 = Mean (xm2 − xn2 xm3 − xn3 ) (8)\nThe mean above is computed over all n 6= m. Similarly,\n∂x1 ∂x3 ∣∣∣ x3=xm3 = Mean (xm1 − xn1 xm3 − xn3 ) (9)\nGiven the dataset we generated, we compute values for ∂x1∂x3 ∣∣∣ x3=xm3 and ∂x2∂x3 ∣∣∣ x3=xm3 for an arbitrarily chosen m. We obtain values of 7.98 and 0.01, respectively. Note that we expect the second value to be 0, but due to noise in gradient approximation obtain a non-zero value. We re-write equation 6 as shown below and plug in the values of the partials. We obtain a non-zero value of P in this case.\nP = [\n1 2 1 2\n] [∣∣ ∂f1 ∂x1 ∣∣ ∣∣ ∂f1 ∂x2 ∣∣ ∣∣ ∂f1 ∂x3 ∣∣∣∣ ∂f2 ∂x1 ∣∣ ∣∣ ∂f2 ∂x2 ∣∣ ∣∣ ∂f2 ∂x3 ∣∣ ]00 1  (10) = [\n1 2 1 2\n] [∣∣ ∂f1 ∂x1 ∣∣ ∣∣ ∂f1 ∂x2 ∣∣ ∣∣ ∂f1 ∂x2 ∂x2 ∂x3 ∣∣∣∣ ∂f2 ∂x1 ∣∣ ∣∣ ∂f2 ∂x2 ∣∣ ∣∣ ∂f2 ∂x2 ∂x2 ∂x3 ∣∣ ]00 1  (11)\nCase 2: Classifier only depends only on x1 In this case, the modeler deems both x2, x3 to be protected features and builds a classifier as depicted below.\nf = σ(x1 − 5) (12) Lets assume that the modeler assigns a hire decision if f > 0.5, otherwise not. Additionally, given x2 and x3 are protected features, P is set to\nP = [\n1 2 1 2\n] [∣∣ ∂f1 ∂x1 ∣∣ ∣∣ ∂f1 ∂x2 ∣∣ ∣∣ ∂f1 ∂x3 ∣∣∣∣ ∂f2 ∂x1 ∣∣ ∣∣ ∂f2 ∂x2 ∣∣ ∣∣ ∂f2 ∂x3 ∣∣ ]01\n2 1 2  (13) Given that the classifier does not explicitly rely\non x2 and x3, we can rewrite equation 14 as\nP = [\n1 2 1 2\n] [∣∣ ∂f1 ∂x1 ∣∣ ∣∣ ∂f1 ∂x1 ∂x1 ∂x2 ∣∣ ∣∣ ∂f1 ∂x1 ∂x1 ∂x3 ∣∣∣∣ ∂f2 ∂x1 ∣∣ ∣∣ ∂f2 ∂x1 ∂x1 ∂x2 ∣∣ ∣∣ ∂f2 ∂x1 ∂x1 ∂x3 ∣∣ ]01\n2 1 2  (14)\nWe obtain the partial derivatives ∂x1∂x2 ∣∣∣ x2=xm2 and\n∂x1 ∂x3 ∣∣∣ x3=xm3 . For an arbitrary chosen xm1 , we obtain values of 0.01 and -0.01. While we expect both these values to be zero given our data construction, they are non-zero due to the gradient approximation. Barring the noise in gradient computation, P is 0 in this case."
    }, {
      "heading" : "B Prediction sensitivity for classifier trained with individual fairness",
      "text" : "We conduct a simulation, where we obtain the proposed metric for increasing values of L. We generate a synthetic dataset with a single feature drawn uniformly randomly between 0 and 10. The label y of a given datapoint is set to 0 if the feature value is less than 5 or 1 otherwise. Let us assume we build a linear classifier f = θx, where x is denotes scalar feature. We optimize equation 2 and obtain value of θ that satisfies the constraint and minimizes a chosen L. Let D and d be L1 norms and L = (y − f)2. We optimize for the value of θ, and Figure shows the value of accumulated prediction sensitivity with increasing value of L between the range 0 to 0.2. We observe the metric closely follows value of L till 0.1. We note that L will equal θ in this case and the optimal value of θ in the absence of any constraint is 0.1."
    }, {
      "heading" : "C Dataset Statistics",
      "text" : ""
    }, {
      "heading" : "D Considerations for using PSM Classifier",
      "text" : "Training a PSM classifier requires access to gender labels which might not be available for the\ndataset used to train the model under evaluation. To overcome this, we evaluate training a PSM classifier on a different dataset and then applying it on the dataset of interest. In Table 5, the last two rows record the correlation and mutual information values of a PSM classifier trained on Bias in Bios (tested on Jigsaw) and trained on Jigsaw Toxicity (tested on Bias in Bios), respectively. While we beat the CF baseline using the PSM trained on another dataset, comparison to the setting where v is set using gendered words presents a mixed picture. P3 (v set using PSM trained on Jigsaw Toxicity) has a slightly higher correlation of 0.365 compared to 0.363 in the P5 setting. However, P3 has a slightly worse MI of 0.091 compared to P5. The related experiment for Jigsaw toxicity where v is set using PSM trained on Bias in Bios yields similar mixed observations when compared to P5.\nWe also conducted a synthetic experiment wherein we deliberately add bias to the PSM classifier. We reduce the number of ‘female’ datapoints by 50% leading to about 18% reduction in the recall for the ‘female’ class (while the ‘male’ class accuracy remains the same). We observe that the metric quality also degrades in this case, leading to a correlation of 0.259 with human judgement, in case of the Bias in Bios data. This correlation is worse than the CF baseline.\nGiven these results, we observe that using the PSM classifier improves upon other baselines only when it is relatively un-biased in performance across genders and matched to the dataset at hand. Therefore, we recommend setting v using gendered words if a strong PSM classifier is difficult to obtain."
    } ],
    "references" : [ {
      "title" : "On evaluating cnn representations for low resource medical image classification",
      "author" : [ "Taruna Agrawal", "Rahul Gupta", "Shrikanth Narayanan." ],
      "venue" : "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Agrawal et al\\.,? 2019",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2019
    }, {
      "title" : "Language (technology) is power: A critical survey of “bias” in NLP",
      "author" : [ "Su Lin Blodgett", "Solon Barocas", "Hal Daumé III", "Hanna Wallach." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–",
      "citeRegEx" : "Blodgett et al\\.,? 2020",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2020
    }, {
      "title" : "Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets",
      "author" : [ "Su Lin Blodgett", "Gilsinia Lopez", "Alexandra Olteanu", "Robert Sim", "Hanna Wallach." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Blodgett et al\\.,? 2021",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2021
    }, {
      "title" : "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "author" : [ "Tolga Bolukbasi", "Kai-Wei Chang", "James Zou", "Venkatesh Saligrama", "Adam Kalai." ],
      "venue" : "arXiv preprint arXiv:1607.06520.",
      "citeRegEx" : "Bolukbasi et al\\.,? 2016",
      "shortCiteRegEx" : "Bolukbasi et al\\.",
      "year" : 2016
    }, {
      "title" : "Topological vector spaces, elements of mathematics",
      "author" : [ "Nicolas Bourbaki" ],
      "venue" : null,
      "citeRegEx" : "Bourbaki.,? \\Q1987\\E",
      "shortCiteRegEx" : "Bourbaki.",
      "year" : 1987
    }, {
      "title" : "Semantics derived automatically from language corpora contain human-like biases",
      "author" : [ "Aylin Caliskan", "Joanna J Bryson", "Arvind Narayanan." ],
      "venue" : "Science, 356(6334):183–186.",
      "citeRegEx" : "Caliskan et al\\.,? 2017",
      "shortCiteRegEx" : "Caliskan et al\\.",
      "year" : 2017
    }, {
      "title" : "Algorithmic decision making and the cost of fairness",
      "author" : [ "Sam Corbett-Davies", "Emma Pierson", "Avi Feller", "Sharad Goel", "Aziz Huq." ],
      "venue" : "Proceedings of the 23rd acm sigkdd international conference on",
      "citeRegEx" : "Corbett.Davies et al\\.,? 2017",
      "shortCiteRegEx" : "Corbett.Davies et al\\.",
      "year" : 2017
    }, {
      "title" : "Bias in bios: A case study of semantic representation bias in a high-stakes setting",
      "author" : [ "Maria De-Arteaga", "Alexey Romanov", "H. Wallach", "J. Chayes", "C. Borgs", "A. Chouldechova", "Sahin Cem Geyik", "K. Kenthapadi", "A. Kalai." ],
      "venue" : "Proceedings of the Conference",
      "citeRegEx" : "De.Arteaga et al\\.,? 2019a",
      "shortCiteRegEx" : "De.Arteaga et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. Bias in bios: A case study of semantic representation bias",
      "author" : [ "Maria De-Arteaga", "Alexey Romanov", "Hanna Wallach", "Jennifer Chayes", "Christian Borgs", "Alexandra Chouldechova", "Sahin Geyik", "Krishnaram Kenthapadi", "Adam Tauman Kalai" ],
      "venue" : null,
      "citeRegEx" : "De.Arteaga et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "De.Arteaga et al\\.",
      "year" : 2019
    }, {
      "title" : "Bold: Dataset and metrics for measuring biases in open-ended language generation",
      "author" : [ "Jwala Dhamala", "Tony Sun", "Varun Kumar", "Satyapriya Krishna", "Yada Pruksachatkun", "Kai-Wei Chang", "Rahul Gupta." ],
      "venue" : "Proceedings of the 2021 ACM Confer-",
      "citeRegEx" : "Dhamala et al\\.,? 2021",
      "shortCiteRegEx" : "Dhamala et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-dimensional gender bias classification",
      "author" : [ "Emily Dinan", "Angela Fan", "Ledell Wu", "Jason Weston", "Douwe Kiela", "Adina Williams." ],
      "venue" : "arXiv preprint arXiv:2005.00614.",
      "citeRegEx" : "Dinan et al\\.,? 2020",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2020
    }, {
      "title" : "Measuring and mitigating unintended bias in text classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 67–73.",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel." ],
      "venue" : "Proceedings of the 3rd innovations in theoretical computer science conference, pages 214–226.",
      "citeRegEx" : "Dwork et al\\.,? 2012",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2012
    }, {
      "title" : "Is your classifier actually biased? measuring fairness under uncertainty with bernstein bounds",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2914–2919.",
      "citeRegEx" : "Ethayarajh.,? 2020",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2020
    }, {
      "title" : "Counterfactual fairness in text classification through robustness",
      "author" : [ "Sahaj Garg", "Vincent Perot", "Nicole Limtiaco", "Ankur Taly", "Ed H Chi", "Alex Beutel." ],
      "venue" : "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 219–226.",
      "citeRegEx" : "Garg et al\\.,? 2019",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2019
    }, {
      "title" : "Explaining black box predictions and unveiling data artifacts through influence functions",
      "author" : [ "Xiaochuang Han", "Byron C Wallace", "Yulia Tsvetkov." ],
      "venue" : "arXiv preprint arXiv:2005.06676.",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "Moritz Hardt", "Eric Price", "Nathan Srebro." ],
      "venue" : "arXiv preprint arXiv:1610.02413.",
      "citeRegEx" : "Hardt et al\\.,? 2016",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2016
    }, {
      "title" : "Measurement and fairness",
      "author" : [ "Abigail Z. Jacobs", "Hanna Wallach." ],
      "venue" : "FAccT ’21, page 375–385, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Jacobs and Wallach.,? 2021",
      "shortCiteRegEx" : "Jacobs and Wallach.",
      "year" : 2021
    }, {
      "title" : "An empirical study of rich subgroup fairness for machine learning",
      "author" : [ "Michael Kearns", "Seth Neel", "Aaron Roth", "Zhiwei Steven Wu." ],
      "venue" : "Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 100–109.",
      "citeRegEx" : "Kearns et al\\.,? 2019",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 2019
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 388–395.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Counterfactual fairness",
      "author" : [ "Matt J Kusner", "Joshua R Loftus", "Chris Russell", "Ricardo Silva." ],
      "venue" : "arXiv preprint arXiv:1703.06856.",
      "citeRegEx" : "Kusner et al\\.,? 2017",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2017
    }, {
      "title" : "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
      "author" : [ "Thomas Manzini", "Lim Yao Chong", "Alan W Black", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Manzini et al\\.,? 2019",
      "shortCiteRegEx" : "Manzini et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards a measure of individual fairness for deep learning",
      "author" : [ "Krystal Maughan", "Joseph P Near." ],
      "venue" : "arXiv preprint arXiv:2009.13650.",
      "citeRegEx" : "Maughan and Near.,? 2020",
      "shortCiteRegEx" : "Maughan and Near.",
      "year" : 2020
    }, {
      "title" : "A survey on bias and fairness in machine learning",
      "author" : [ "Ninareh Mehrabi", "Fred Morstatter", "N. Saxena", "Kristina Lerman", "A. Galstyan." ],
      "venue" : "ArXiv, abs/1908.09635.",
      "citeRegEx" : "Mehrabi et al\\.,? 2019a",
      "shortCiteRegEx" : "Mehrabi et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on bias and fairness in machine learning",
      "author" : [ "Ninareh Mehrabi", "Fred Morstatter", "Nripsuta Saxena", "Kristina Lerman", "Aram Galstyan." ],
      "venue" : "arXiv preprint arXiv:1908.09635.",
      "citeRegEx" : "Mehrabi et al\\.,? 2019b",
      "shortCiteRegEx" : "Mehrabi et al\\.",
      "year" : 2019
    }, {
      "title" : "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
      "author" : [ "Nikita Nangia", "Clara Vania", "Rasika Bhalerao", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Nangia et al\\.,? 2020",
      "shortCiteRegEx" : "Nangia et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards auditability for fairness in deep learning",
      "author" : [ "Ivoline C Ngong", "Krystal Maughan", "Joseph P Near." ],
      "venue" : "arXiv preprint arXiv:2012.00106.",
      "citeRegEx" : "Ngong et al\\.,? 2020",
      "shortCiteRegEx" : "Ngong et al\\.",
      "year" : 2020
    }, {
      "title" : "Does robustness improve fairness? approaching fairness with word substitution robustness methods for text classification",
      "author" : [ "Yada Pruksachatkun", "Satyapriya Krishna", "Jwala Dhamala", "Rahul Gupta", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Pruksachatkun et al\\.,? 2021",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2021
    }, {
      "title" : "Societal biases in language generation: Progress and challenges",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Prem Natarajan", "Nanyun Peng." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sheng et al\\.,? 2021",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2021
    }, {
      "title" : "A unified approach to quantifying algorithmic unfairness: Measuring individual &group unfairness via inequality indices",
      "author" : [ "Till Speicher", "Hoda Heidari", "Nina Grgic-Hlaca", "Krishna P Gummadi", "Adish Singla", "Adrian Weller", "Muhammad Bilal Zafar" ],
      "venue" : null,
      "citeRegEx" : "Speicher et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Speicher et al\\.",
      "year" : 2018
    }, {
      "title" : "Mitigating gender bias in natural language processing: Literature review",
      "author" : [ "Tony Sun", "Andrew Gaut", "Shirlyn Tang", "Yuxin Huang", "Mai ElSherief", "Jieyu Zhao", "Diba Mirza", "Elizabeth Belding", "Kai-Wei Chang", "William Yang Wang." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations",
      "author" : [ "Tianlu Wang", "Jieyu Zhao", "Mark Yatskar", "Kai-Wei Chang", "Vicente Ordonez." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "From parity to preference-based notions of fairness in classification",
      "author" : [ "Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna P Gummadi", "Adrian Weller." ],
      "venue" : "arXiv preprint arXiv:1707.00010.",
      "citeRegEx" : "Zafar et al\\.,? 2017",
      "shortCiteRegEx" : "Zafar et al\\.",
      "year" : 2017
    }, {
      "title" : "LOGAN: Local group bias detection by clustering",
      "author" : [ "Jieyu Zhao", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1968–1977.",
      "citeRegEx" : "Zhao and Chang.,? 2020",
      "shortCiteRegEx" : "Zhao and Chang.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "These methods come under the umbrella of algorithmic fairness which has been quantitatively expressed with numerous definitions (Mehrabi et al., 2019b; Jacobs and Wallach, 2021).",
      "startOffset" : 128,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "These methods come under the umbrella of algorithmic fairness which has been quantitatively expressed with numerous definitions (Mehrabi et al., 2019b; Jacobs and Wallach, 2021).",
      "startOffset" : 128,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : ", counter-factual fairness (Kusner et al., 2017)) is aimed at evaluating whether a model gives similar predictions for individuals with similar personal attributes (e.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : ", statistical parity (Dwork et al., 2012)) evaluates fairness across cohorts with same",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "protected attributes instead of individuals (Mehrabi et al., 2019b).",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "We establish its theoretical relationship with statistical parity (group fairness) and individual fairness (Dwork et al., 2012) metrics.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "Akin to the metric proposed by (Maughan and Near, 2020; Ngong et al., 2020), our metric also relies on model output’s sensitivity to changes in input features.",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 26,
      "context" : "Akin to the metric proposed by (Maughan and Near, 2020; Ngong et al., 2020), our metric also relies on model output’s sensitivity to changes in input features.",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : "Multiple efforts have looked into defining, measuring, and mitigating biases in NLP models (Sun et al., 2019; Mehrabi et al., 2019a; Sheng et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "Multiple efforts have looked into defining, measuring, and mitigating biases in NLP models (Sun et al., 2019; Mehrabi et al., 2019a; Sheng et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 152
    }, {
      "referenceID" : 28,
      "context" : "Multiple efforts have looked into defining, measuring, and mitigating biases in NLP models (Sun et al., 2019; Mehrabi et al., 2019a; Sheng et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : ", 2019), sentiment classifier (Dhamala et al., 2021), toxicity",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "classifier (Dixon et al., 2018) or true positive rate difference between privileged and underprivileged groups (De-Arteaga et al.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "A few works additionally validate the alignment of these automatically computed bias metrics with human understanding of biases by collecting annotations of biases on a subset of test data from crowdworkers (Sheng et al., 2019; Dhamala et al., 2021).",
      "startOffset" : 207,
      "endOffset" : 249
    }, {
      "referenceID" : 32,
      "context" : "preference based notion of fairness (Zafar et al., 2017)).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "It is natural to choose an Lp norm (Bourbaki, 1987) for d and D.",
      "startOffset" : 35,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "e, occupation classification on Bias in Bios dataset (De-Arteaga et al., 2019a)2 and toxicity classification with Jigsaw Toxicity dataset3.",
      "startOffset" : 53,
      "endOffset" : 79
    }, {
      "referenceID" : 27,
      "context" : "We focus on these two datasets as they have been investigated in several previous studies (Pruksachatkun et al., 2021) and have been reported to carry significant presence of bias.",
      "startOffset" : 90,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "into various diseases (Agrawal et al., 2019), disparity in model performance with respect to malicious diseases can be considered more costly.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "In this setup, we use the set of gendered words from (Bolukbasi et al., 2016) and assign entries in v corresponding to those words as 1/(Ng × D), where Ng is the count of gendered words in the data-point.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "While prior work has used word matching to a pre-defined corpus of tokens describing various demographic cohorts (Bolukbasi et al., 2016), these corpus do not contain words that stereotypically are associated with a particular cohort but may not be explicitly tied to that cohort.",
      "startOffset" : 113,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "For example, the word “volleyball” is associated with females in the analysis presented by (Dinan et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : "We use the bootstrap method to compute statistical significance (Koehn, 2004) at p-value<0.",
      "startOffset" : 64,
      "endOffset" : 77
    }, {
      "referenceID" : 23,
      "context" : "In the future, one can associate the proposed formulation with other categories of group and individual fairness (Mehrabi et al., 2019a).",
      "startOffset" : 113,
      "endOffset" : 136
    } ],
    "year" : 0,
    "abstractText" : "With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation – ACCUMULATED PREDICTION SENSITIVITY, which measures fairness in machine learning models based on the model’s prediction sensitivity to perturbations in input features. The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group. We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness. It also correlates well with humans’ perception of fairness. We conduct experiments on two text classification datasets – JIGSAW TOXICITY, and BIAS IN BIOS, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome. We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.",
    "creator" : null
  }
}
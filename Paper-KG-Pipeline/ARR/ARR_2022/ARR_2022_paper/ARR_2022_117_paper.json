{
  "name" : "ARR_2022_117_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "BERT (Devlin et al., 2018) is a Transformer-based pretrained model, whose prosperity starts from English language and gradually spreads to many other languages. The original BERT model is trained with character-level masking (CLM). 2 A certain percentage (e.g. 15%) of tokens in the input se-\n1We will release the dataset and pretrained models for future research.\n2Next sentence prediction is the other pretraining task adopted in the original BERT paper. However, it is removed in some following works like RoBERTa (Liu et al., 2019). We do not consider the next sentence prediction in this work.\nquence is masked and the model is learned to predict the masked tokens.\nIt is helpful to note that a word in the input sequence of BERT can be broken into multiple wordpiece tokens (Wu et al., 2016).3 For example, the input sentence “She is undeniably brilliant” is converted to a wordpiece sequence “She is un ##deni ##ably brilliant”, where “##” is a special prefix added to indicate that the token should be attached to the previous one. In this case the word “undeniably” is broken into three wordpieces {“un”, “##deni”, “##ably”}. In standard masked language modeling, CLM may mask any one of them. In this case, if the token “##ably” is masked, it is easier for the model to complete the prediction task because “un” and “##deni” are informative prompts. To address this, Whole word masking (WWM) masks all three subtokens (i.e., {“un”, “##deni”, “##ably”}) within a word at once. For Chinese, however, each token is an atomic character that cannot be broken into smaller pieces. Many Chinese words are compounds that consisting of multiple characters (Wood and Connelly, 2009). 4 For example, “手机” (cellphone) is a word consisting of two characters “手” (hand) and “机” (machine). Here, learning with WWM would lose the association among characters corresponding to a word.\nIn this work, we introduce two probing tasks to study Chinese BERT model’s ability on characterlevel understanding. The first probing task is character replacement. Given a sentence and a position where the corresponding character is erroneous, the task is to replace the erroneous character with the correct one. The second probing task is character insertion. Given a sentence and the positions where\n3In this work, wordpiece and subword are interchangeable. 4When we describe Chinese tokens, “character” means字 that is the atomic unit and “word” means词 that may consist of multiple characters.\na given number of characters should be inserted, the task is to insert the correct characters. We leverage the benchmark dataset on grammatical error correction (Rao et al., 2020a) and create a dataset including labels for 19,075 tokens in 10,448 sentences.\nWe train three baseline models based on the same text corpus of 80B characters using CLM, WWM, and both CLM and WWM, separately. We have the following major findings. (1) When one character needs to be inserted or replaced, the model trained with CLM performs the best. Moreover, the model initialized from RoBERTa (Cui et al., 2019) and trained with WWM gets worse gradually with more training steps. (2) When more than one character needs to be handled, WWM is the key to better performance. (3) When evaluating sentence-level downstream tasks, the impact of these masking strategies is minimal and the model trained with them performs comparably."
    }, {
      "heading" : "2 Our Probing Tasks",
      "text" : "In this work, we present two probing tasks with the goal of diagnosing the language understanding ability of Chinese BERT models. We present the tasks and dataset in this section.\nThe first probing task is character replacement, which is a subtask of grammatical error correction. Given a sentence s = {x1, x2, ..., xi, ..., xn} of n characters and an erroneous span es = [i, i + 1, ..., i + k] of k characters, the task is to replace es with a new span of k characters.\nThe second probing task is character insertion, which is also a subtask of grammatical error correction. Given a sentence s = {x1, x2, ..., xi, ..., xn} of n characters, a position i, and a fixed number k, the task is to insert a span of k characters between the index i and i+ 1.\nWe provide two examples of these two probing tasks with k = 1 in Figure 1. For the character replacement task, the original meaning of the sentence is “these are all my ideas”. Due to the misuse of a character at the 7th position, its meaning changed significantly to “these are all my attention”. Our character replacement task is to replace the misused character “主” with “注”. For the character insertion task, what the writer wants to express is “Human is the most important factor. However, due to the lack of one character between the 5th and 6th position, its meaning changed to “Human is the heaviest factor”. The task is to\ninsert “要” after the 5th position. Both tasks are also extended to multiple characters (i.e., k ≥ 2). Examples can be found at Section 3.2.\nWe build a dataset based on the benchmark of Chinese Grammatical Error Diagnosis (CGED) in years of 2016, 2017, 2018 and 2020 (Lee et al., 2016; Rao et al., 2017, 2018, 2020b). The task of CGED seeks to identify grammatical errors from sentences written by non-native learners of Chinese (Yu et al., 2014). It includes four kinds of errors, including insertion, replacement, redundant, and ordering. The dataset of CGED composes of sentence pairs, of which each sentence pair includes an erroneous sentence and an error-free sentence corrected by annotators. However, these sentence pairs do not provide information about erroneous positions, which are indispensable for the character replacement and character insertion. To obtain such position information, we implement a modified character alignment algorithm (Bryant et al., 2017) tailored for the Chinese language. Through this algorithm, we obtain a dataset for the insertion and replacement, both of which are suitable to examine the language learning ability of the pretrained model. We leave redundant and ordering types to future work. The statistic of our dataset is detailed in Appendix A."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we first describe the BERT-style models that we examined, and then report numbers."
    }, {
      "heading" : "3.1 Chinese BERT Models",
      "text" : "We describe the publicly available BERT models as well as the models we trained.\nAs mentioned earlier, BERT-base (Devlin et al., 2018)5 is trained with the standard MLM objective.6 To make a fair comparison of CLM and WWM, we train three simple Chinese BERT baselines from scratch7: (1) Ours-clm: we train this model using CLM. (2) Ours-wwm: this model only differs in that it is trained with WWM. (3) Oursclm-wwm: this model is trained with both CLM and WWM objectives. We train these three models on a text corpus of 80B characters consisting of news, wiki, and novel texts. For the WWM task, we use a public word segmentation tool Texsmart (Zhang et al., 2020) to tokenize the raw data first. The mask rate is 15% which is commonly used in existing works. We use a max sequence length of 512, use the ADAM optimizer (Kingma and Ba, 2014) with a batch size of 8,192. We set the learning rate to 1e-4 with a linear optimizer with\n5https://github.com/google-research/ bert/blob/master/README.md\n6We do not compare with RoBERTa-wwm-ext because the released version lacks of the language modeling head.\n7We also further train these models initialized from RoBERTa and BERT and results are given in Appendix B.\n5k warmup steps and 100k training steps in total. Models are trained on 64 Tesla V100 GPUs for about 7 days."
    }, {
      "heading" : "3.2 Probing Results",
      "text" : "We present the results on two probing tasks here. Models are evaluated by Prediction @k, denoting whether the ground truth for each position is covered in the top-k predictions. From Table 1, we can make the following conclusions. First, Oursclm consistently performs better than Ours-wwm on probing tasks that one character needs to be replaced or inserted. We suppose this is because WWM would lose the association between characters corresponding to a word. Second, WWM is crucial for better performance when there is more than one character that needs to be corrected. This phenomenon can be observed from the results of Ours-wwm and Ours-clm-wwm, which both adopt WWM and perform better than Ours-clm. Third, pretrained with a mixture of CLM and WWM, Ours-clm-wwm performs better than Ours-wwm in the one-character setting and does better than\nOurs-clm when more than one characters need to be handled. For each probing task, two examples with predictions produced by Ours-clm-wwm are given in Figure 2."
    }, {
      "heading" : "3.3 Analysis",
      "text" : "To further analyze how CLM and WWM affect the performance on probing tasks, we initialized our model from RoBERTa (Cui et al., 2019) and further trained baseline models. We show the performance of these models with different training steps on the insertion task. From Figure 3 (top), we can observe that as the number of training steps increases, the performance of Ours-wwm decreases.\nIn addition, we also evaluate the performance of trained BERT models on downstream tasks with model parameters fine-tuned. The performance of Ours-clm-wwm is comparable with Ours-wwm and Ours-clm. More information can be found in Appendix C."
    }, {
      "heading" : "4 Related Work",
      "text" : "We describe related studies on Chinese BERT model and probing of BERT, respectively.\nThe authors of BERT (Devlin et al., 2018) provided the first Chinese BERT model which was trained on Chinese Wikipedia data. On top of that, Cui et al. (2019) trained RoBERTa-wwm-ext with WWM on extended data. Cui et al. (2020) further trained a Chinese ELECTRA model and MacBERT, both of which did not have [MASK] tokens. ELECTRA was trained with a token-level binary classification task, which determined whether a token was the original one or artificially replaced. In MacBERT, [MASK] tokens were replaced with synonyms and the model was trained with WWM and ngram masking. ERNIE (Sun et al., 2019) was trained with entity masking, similar to WWM yet tokens corresponding to an entity were masked at once. Language features are considered in more recent works. For example, AMBERT (Zhang and Li, 2020) and Lattice-BERT (Lai et al., 2021) both take word information into consideration. ChineseBERT (Sun et al., 2021) utilizes pinyin and glyph of characters.\nProbing aims to examine the language understanding ability of pretrained models like BERT when model parameters are clamped, i.e., without being fine-tuned on downstream tasks. Petroni et al. (2019) study how well pretrained models learn factual knowledge. The idea is to design a natural language template with a [MASK] token, such as “the wife of Barack Obama is [MASK].”. If the model predicts the correct answer “Micheal Obama”, it shows that pretrained models learn factual knowledge to some extent. Similarly, Davison et al. (2019) study how pretrained models learn commonsense knowledge and Talmor et al. (2020) examine on tasks that require symbolic understanding. Wang and Hu (2020) propose to probe Chinese BERT models in terms of linguistic and world knowledge."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we present two Chinese probing tasks, including character insertion and replacement. We provide three simple pretrained models dubbed Ours-clm, Ours-wwm, and Ours-clm-wwm, which are pretrained with CLM, WWM, and a combination of CLM and WWM, respectively. Ours-wwm is prone to lose the association between words and result in poor performance on probing tasks when one character needs to be inserted or replaced. Moreover, WWM plays a key role when two or more characters need to be corrected."
    }, {
      "heading" : "A The statistic of dataset",
      "text" : ""
    }, {
      "heading" : "B Probing results from models with different initialization",
      "text" : "We also verify the performance of models initialized from BERT (Devlin et al., 2018) and RoBERTa (Cui et al., 2019) on probing tasks. The results are detailed in Table 3, from which we can obtain consistent conclusions with the previous section."
    }, {
      "heading" : "C The evaluation on downstream tasks",
      "text" : "We test the performance of BERT-style models on tasks including text classification (TNEWS, IFLYTEK), sentence-pair semantic similarity (AFQMC), coreference resolution (WSC), key word recognition (CSL), and natural language inference (OCNLI) (Xu et al., 2020a). We follow the standard fine-tuning hyper-parameters used in Devlin et al. (2018); Xu et al. (2020b); Lai et al. (2021) and report results on the development sets. The detailed results is shown in Table 4."
    } ],
    "references" : [ {
      "title" : "Automatic annotation and evaluation of error types for grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Edward Briscoe." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Bryant et al\\.,? 2017",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2017
    }, {
      "title" : "Revisiting pretrained models for Chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training with whole word masking for chinese bert",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Ziqing Yang", "Shijin Wang", "Guoping Hu." ],
      "venue" : "arXiv preprint arXiv:1906.08101.",
      "citeRegEx" : "Cui et al\\.,? 2019",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "Commonsense knowledge mining from pretrained models",
      "author" : [ "Joe Davison", "Joshua Feldman", "Alexander M Rush." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Davison et al\\.,? 2019",
      "shortCiteRegEx" : "Davison et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Lattice-bert: Leveraging multi-granularity representations in chinese pre-trained language models",
      "author" : [ "Yuxuan Lai", "Yijia Liu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "arXiv preprint arXiv:2104.07204.",
      "citeRegEx" : "Lai et al\\.,? 2021",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2021
    }, {
      "title" : "Overview of NLP-TEA 2016 shared task for Chinese grammatical error diagnosis",
      "author" : [ "Lung-Hao Lee", "Gaoqi Rao", "Liang-Chih Yu", "Endong Xun", "Baolin Zhang", "Li-Ping Chang." ],
      "venue" : "Proceedings of the 3rd Workshop on Natural Language Pro-",
      "citeRegEx" : "Lee et al\\.,? 2016",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models as knowledge bases? arXiv preprint arXiv:1909.01066",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander H Miller", "Sebastian Riedel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of NLPTEA-2018 share task Chinese grammatical error diagnosis",
      "author" : [ "Gaoqi Rao", "Qi Gong", "Baolin Zhang", "Endong Xun." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Rao et al\\.,? 2018",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2018
    }, {
      "title" : "IJCNLP-2017 task 1: Chinese gram",
      "author" : [ "Lee" ],
      "venue" : null,
      "citeRegEx" : "2017.,? \\Q2017\\E",
      "shortCiteRegEx" : "2017.",
      "year" : 2017
    }, {
      "title" : "olmpics-on what language",
      "author" : [ "Jonathan Berant" ],
      "venue" : null,
      "citeRegEx" : "Berant.,? \\Q2020\\E",
      "shortCiteRegEx" : "Berant.",
      "year" : 2020
    }, {
      "title" : "CLUE: A Chinese language understanding evaluation benchmark",
      "author" : [ "Zuoyu Tian", "Yiwen Zhang", "He Zhou", "Shaoweihua Liu", "Zhe Zhao", "Qipeng Zhao", "Cong Yue", "Xinrui Zhang", "Zhengliang Yang", "Kyle Richardson", "Zhenzhong Lan." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Tian et al\\.,? 2020a",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Clue: A chinese language understanding evaluation",
      "author" : [ "Liang Xu", "Hai Hu", "Xuanwei Zhang", "Lu Li", "Chenjie Cao", "Yudong Li", "Yechen Xu", "Kai Sun", "Dian Yu", "Cong Yu" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2004
    }, {
      "title" : "Overview of grammatical error diagnosis for learning chinese as a foreign language",
      "author" : [ "Liang-Chih Yu", "Lung-Hao Lee", "Liping Chang." ],
      "venue" : "Proceedings of the 1stWorkshop on Natural Language Processing Techniques for Educational Applications",
      "citeRegEx" : "Yu et al\\.,? 2014",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    }, {
      "title" : "Ambert: A pretrained language model with multi-grained tokenization",
      "author" : [ "Xinsong Zhang", "Hang Li." ],
      "venue" : "arXiv preprint arXiv:2008.11869. 6",
      "citeRegEx" : "Zhang and Li.,? 2020",
      "shortCiteRegEx" : "Zhang and Li.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "BERT (Devlin et al., 2018) is a Transformer-based pretrained model, whose prosperity starts from English language and gradually spreads to many other languages.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "However, it is removed in some following works like RoBERTa (Liu et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Moreover, the model initialized from RoBERTa (Cui et al., 2019) and trained with WWM gets worse gradually with more training steps.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "We build a dataset based on the benchmark of Chinese Grammatical Error Diagnosis (CGED) in years of 2016, 2017, 2018 and 2020 (Lee et al., 2016; Rao et al., 2017, 2018, 2020b).",
      "startOffset" : 126,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "The task of CGED seeks to identify grammatical errors from sentences written by non-native learners of Chinese (Yu et al., 2014).",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "To obtain such position information, we implement a modified character alignment algorithm (Bryant et al., 2017) tailored for the Chinese language.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "As mentioned earlier, BERT-base (Devlin et al., 2018)5 is trained with the standard MLM objective.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "We use a max sequence length of 512, use the ADAM optimizer (Kingma and Ba, 2014) with a batch size of 8,192.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "To further analyze how CLM and WWM affect the performance on probing tasks, we initialized our model from RoBERTa (Cui et al., 2019) and further trained baseline models.",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "The authors of BERT (Devlin et al., 2018) provided the first Chinese BERT model which was",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "For example, AMBERT (Zhang and Li, 2020) and Lattice-BERT (Lai et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "For example, AMBERT (Zhang and Li, 2020) and Lattice-BERT (Lai et al., 2021) both take word information into consideration.",
      "startOffset" : 58,
      "endOffset" : 76
    } ],
    "year" : 0,
    "abstractText" : "Whole word masking (WWM), which masks all subwords corresponding to a word at once, makes a better English BERT model (Sennrich et al., 2016). For the Chinese language, however, there is no subword because each token is an atomic character. The meaning of a word in Chinese is different in that a word is a compositional unit consisting of multiple characters. Such difference motivates us to investigate whether WWM leads to better context understanding ability for Chinese BERT. To achieve this, we introduce two probing tasks related to grammatical error correction and ask pretrained models to revise or insert tokens in a masked language modeling manner. We construct a dataset including labels for 19,075 tokens in 10,448 sentences. We train three Chinese BERT models with standard characterlevel masking (CLM), WWM, and a combination of CLM and WWM, respectively. Our major findings are as follows: First, when one character needs to be inserted or replaced, the model trained with CLM performs the best. Second, when more than one character needs to be handled, WWM is the key to better performance. Finally, when being fine-tuned on sentence-level downstream tasks, models trained with different masking strategies perform comparably.1",
    "creator" : null
  }
}
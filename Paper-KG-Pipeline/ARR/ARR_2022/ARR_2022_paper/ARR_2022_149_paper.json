{
  "name" : "ARR_2022_149_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In contrast to existing offensive text detection datasets, Mh-RIOT features human-annotated chains of reasoning which describe the mental process by which an offensive interpretation can be reached from each ambiguous statement. We explore the potential for a multi-hop reasoning approach by utilizing existing entailment models to score the transitions of these chains, and show that even naive reasoning models can result in improved performance in most situations. Analysis of the chains provides insight into the human interpretation process and emphasizes the importance of incorporating additional commonsense knowledge."
    }, {
      "heading" : "1 Introduction",
      "text" : "With the development and popularity of online forums and social media platforms, the world is becoming an increasingly connected place to share information and opinions. However, the benefit these platforms provide to society is often marred by the creation of an unprecedented amount of bullying, hate, and other abusive speech1. Such toxic speech has detrimental effects on online communities, and can cause great personal harm. Some efforts by the NLP community to address this\n1Disclaimer: due to the nature of this work, data and examples may contain content which is offensive to the reader.\nproblem have achieved high accuracies in classifying toxic speech in specific domains, such as sexist (Golbeck et al., 2017), racist (Waseem, 2016), or otherwise hateful text (Ross et al., 2016; Gao and Huang, 2017; Davidson et al., 2017).\nWhile many instances of toxic speech are blatant and easily identified with sentence-level classifiers, not all offensive text contains obvious indicators. Waseem et al. (2017) argues for the classification of offensive text into two categories, (1) explicit offensive text2, which is unambiguous in its potential to be offensive and often includes overtly offensive terms, such as slurs, and (2) implicit offensive text, which is more ambiguous, and may use sarcasm, innuendo, or other rhetorical\n2Waseem et al.(2017) originally defined these terms as “explicit/implicit abusive text”, but we adopt the phrase “offensive text” as used by the OTD community.\ndevices to hide the intended nature of the statement. In this work we argue that there exists a direct relationship between these tasks, and that each implicitly offensive statement corresponds to an explicitly offensive statement which is realized through the interpretation process. This explicitly offensive statement is closer to the sentiment the listener feels when interpreting the statement as offensive. Consider the example in Figure 1, a dialogue between two speakers, S1 and S2:\nS1: “I love bookclubs, I go every week” S2: “Do they have free food or something?”\nBy itself, the statement by S2 is innocuous and could be interpreted as a simple prompt for more information about the bookclub. However, other interpretations of this statement could lead S1 to arrive at a number of explicitly offensive statements, such as (1) “You are poor”, (2) “You are fat”, (3) “You are not smart/sophisticated”. Thus we consider the chain of reasoning which constitutes the interpretation to be a crucial part of recognizing implicitly offensive statements.\nThe importance of more complex reasoning when resolving such ambiguities in offensive content is not new. The Hateful Memes dataset (Kiela et al., 2021) pairs images with unrelated text captions. Both of these components are benign when considered independently, but combining them can occasionally create memes with offensive interpretations. Consequently, approaches which jointly reason over a combined representations of each modality outperform those which treat each modality independently, hindering the system’s ability to perform more complex reasoning.\nTo study this phenomenon purely in the text domain, we use human annotators to construct a dataset consisting of (1) an implicitly offensive statement, (2) a corresponding explicitly offensive statement, and (3) a chain of reasoning mapping (1) to (2). When evaluated on the explicitly offensive examples, state-of-the-art models perform well, achieving > 90% accuracy. However, when applied to the implicit OTD samples, the accuracy of the models drops to an average of about < 11%. We then explore the use of a multi-hop reasoning-based approach by utilizing a pre-trained entailment model to score the transitions along each “hop” of the reasoning chain. When incorporating additional knowledge (from human annotations)\ninto the premises of each entailment, we achieve higher accuracy than comparable methods which do not utilize the reasoning chain. We present this as evidence that a multi-hop reasoning-based approach is a promising solution to this problem, and release our data to support further research into this problem.\nOur contributions in this work are threefold:\n• We propose the task of implicit offensive text detection (Implicit OTD), and construct a dataset to research on this topic. The dataset contains annotations of reasoning chains to support study into multi-hop approaches.\n• We conduct experiments using existing stateof-the-art OTD models, and show they perform poorly on Implicit OTD task.\n• We examine the use of entailment models as part of a multi-hop reasoning approach for Implicit OTD, showing improved accuracy in most cases. We provide an analysis of which types of reasoning are most challenging, and which types of external knowledge is required."
    }, {
      "heading" : "2 Related Works",
      "text" : "OTD in Text Classification Early approaches to OTD relied primarily upon dictionaries like hatebase 3 to lookup offensive words and phrases. The creation of OTD datasets enabled the development of ML-based approaches utilizing simple features, such as bag-of-word representations (Davidson et al., 2017). With the advent of social media platforms, many resources have been developed for identifying toxic comments in web text (Waseem and Hovy, 2016; Davidson et al., 2017), including a number of deep learning-based methods (Pitsilis et al., 2018; Zhang et al., 2018b; Casula et al., 2020; Yasaswini et al., 2021; Djandji et al., 2020). Notably, all of these methods can be described as building a contextual representation of a sentence (whether trained end-to-end or on top of existing pre-trained language models), and making a classification based on this representation.\nOTD in Dialogue Systems As user-facing technologies, preventing dialogue systems from producing offensive statements is crucial for their role in society. As noted in Dinan et al. (2020), toxicity in generated dialogue may begin with biases and\n3www.hatebase.org\noffensive content in the training data, and debiasing techniques focused on gender can reduce the amount of sexist comments generated by the resulting system. Similar outcomes can be obtained through adjustments to the model or training procedure, for instance, toxic words can be masked during training to reduce their role in model predictions (Dale et al., 2021). GeDi (Krause et al., 2020) proposed using class-conditional LMs as discriminators to reduce the toxicity produced by large pre-trained LMs (GPT-2). Additionally it may also be important to identify offensive statements made to a dialogue system, as it has been shown that dialogue systems can react with counteraggression (Cercas Curry and Rieser, 2018), and systems which continuously learn during deployment may incorporate toxic user responses into future generations.\nSubjectivity in OTD Previous work has hit upon the role that an individual’s own perspective may play when determining offensiveness. For instance, in the Offensive Language Identification Dataset (OLID), a widely used OTD dataset (Zampieri et al., 2019a,b, 2020), annotations exist on a hierarchy. Each level dictates the target of the offensive text, in terms of their identity as a group, individual, or entity. But to our knowledge, a person’s identity or attributes have not played a critical role in existing OTD research. OLID was also augmented with labels for capturing the degree of explicitness (Caselli et al., 2020)), and may also support research into resolving implicitly offensive statements. However, implicitness in OLID is defined primarily as the lack of an overtly offensive word or slur, and the aforementioned personal attributes or subjectivity of interpretation are not considered. Our dataset differs in this respect, as we consider not just if a statement is offensive, but how it can be considered offensive, by defining the interpretation process as a chain of reasoning towards a subjective experience. In this sense, a more similar approach comes from normative reasoning in moral stories (Emelin et al., 2020), where a short chain of reasoning is used to assess morality of actions and consequences."
    }, {
      "heading" : "3 Data",
      "text" : "We propose Mh-RIOT as a dataset for the study of Implicit OTD as a multi-hop reasoning problem, and for use as a diagnostic to test models’ ability to identify implicitly offensive statements.\nEach example in the dataset consists of three parts:\n1. A personal attribute of the reader/listener.\n2. An implicitly offensive statement, its corresponding explicitly offensive statement, and a non-offensive statement.\n3. A chain of reasoning, describing the iterative process of how the ambiguity of the implicitly offensive statement can be resolved into the corresponding explicitly offensive statement. Appendix A lists some sample chains in MhRIOT.\nWe collect annotations for Mh-RIOT using Amazon Mechanical Turk (AMT). Four pilot experiments were conducted to select qualified annotators for the final annotation. The instructions provided to the annotators can be found in Appendix C."
    }, {
      "heading" : "3.1 Annotation Scheme",
      "text" : "Personal Attribute As we have defined in Section 1, we argue that the context in which a statement occurs is crucial to understanding its potential in creating an offensive interpretation, and therefore the context should play an important role in the annotation task. However, providing an overly specific context can increase the difficulty of providing a relevant implicitly offensive statement. To make the annotation task more feasible we reduce the context to a single feature: a personal attribute of the reader/listener.\nThe set of attributes is obtained from the personas in the PERSON-CHAT corpus (Zhang et al., 2018a), of the form “I like sweets.”, or “I work as a stand up comedian.” Attributes related to ethnicity, gender, and other protected classes are manually removed, leaving 5334 distinct attributes. We divide the attributes into several categories (detailed category information can be found in Appendix B) before randomly sampling a subset of 920 attributes, uniformly across categories, in order to increase the number of workers assigned to each attribute.\nImplicit, Explicit and Non-offensive Text For each example, workers were provided 3 diverse attributes and asked to choose one as writing prompt. The workers are then instructed to provide annotation in the form of example sentences, including: Implicitly offensive statement Utterances that do not express an overt intention to cause offense and often require complicated reasoning or external\nknowledge to be fully recognized as offensive contents. Explicitly offensive statement Utterances which contain an obvious and direct intention to cause offense without external knowledge or reasoning processes. Non-offensive statement Utterances that do not cause offense under the context initiated with the attribute.\nBoth explicit and implicit offensive statements should share the same meaning in terms of how they are offensive. Non-offensive statements are collected to construct a balanced dataset and to evaluate the accuracy of existing OTD models.\nChain of Reasoning A distinguishing characteristic of our work is the collection of chains of reasoning to explain the interpretation process for implicitly offensive text. We represent the chain of reasoning as a series of sentence-to-sentence rewrites, similar to natural logic (MacCartney and Manning, 2014). One practical advantage of using a sentence-based representation for reasoning steps (in comparison to a structured representation like predicate-argument tuples) is that it allows the use of powerful text-to-text (T5) (Raffel et al., 2019) and entailment models (Liu et al., 2019; He et al., 2021), which are trained on sentence-level input.\nFormally each chain begins with an implicitly offensive statement (0-th step, denoted as s0) and ends with an explicit offense (sl), making the length of the chain the number of steps between s0 and sl, inclusive."
    }, {
      "heading" : "3.2 Post-processing",
      "text" : "We were able to collect 2657 examples from the AMT and performed post-processing to ensure the quality of the data. We define three processes to edit the collected annotations in order to standardize the format of the reasoning steps, listed below. Examples with steps that can not be handled by any of the processes are removed from the dataset. To reduce biases in post-processing, we assign 3 workers to each task.\nAttribute Insertion Rule (AIR) We insert the attribute statement into the first reasoning step (s1) to make this information accessible to any model taking the sentence as input. For instance, for an example with the attribute, “I am colorblind.” and the implicit offensive statement, “Oh, that would explain your wardrobe!”, the reasoning step “Oh,\nKnowledge\nOnly the best can win contests. Classic things are usually old. Grown-ups don’t play with dolls. Parents want children to be independent. Overworking makes people exhausted.\nTable 1: Samples of the knowledge used to construct chains of reasoning.\nyour color blindness would explain your wardrobe!” generated by the worker is tagged as AIR.\nKnowledge Insertion Rule (KIR) Steps that are used to introduce external commonsense knowledge are tagged as KIR. For instance, to support the reasoning process from step “You are a grownup who can’t afford to rent a house.” to “You are poor.”, the knowledge of “Poor people can’t afford to rent a house.” is introduced. The following step “You are poor.” is then tagged as KIR. To better understand the effectiveness of external knowledge, we also extract the commonsense knowledge during the post-processing (Table 1).\nRephrasing Rule (RR). Steps that have equivalent meaning to previous steps but can be simplified by rephrasing are tagged as RR. For instance, to express more explicit offensive meaning, an reasoning step written as a question “Do you like meat too much, or just food in general?” is rephrased as a declarative sentence step “You must love food too much in general.” and tagged as RR."
    }, {
      "heading" : "3.3 Post-processing Results",
      "text" : "Of the initially collected 2657 examples, 1050 remained after the post-processing. The high task rejection rate (60.5%) also conveys the difficulty of this content generation task. In the dataset, the average length of a reasoning chain is 4.84 steps, with a minimum length of 3 (60 examples) and a maximum of 6 (39 examples). Among all three tags, RR is most frequently applied (59.6%), followed by KIR (21.5%) and AIR (18.9%)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate the difficulty of the Implicit OTD task using existing state-of-the-art models, before exploring a multi-hop approach to Implicit OTD using existing entailment models to score transitions in the reasoning chains."
    }, {
      "heading" : "4.1 Sentence Classification",
      "text" : "We begin by evaluating existing state-of-the-art OTD models on both the Implicit-OTD and Explicit-OTD task. These include BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020), three pretrained large scale language models fine-tuned on existing OTD datasets, which produce the highest accuracy reported on the explicit OTD task.\nThese models are fine-tuned on three OTD datasets, including (1) the OLID/OffensEval2019 dataset (Zampieri et al., 2019a), discussed in Section 2, which contains 14,200 labeled tweets and includes implicit offensive statements, (2) the TWEETEVALL (Barbieri et al., 2020) multitask offensive Twitter set for detecting irony, hate speech and offensive language, and (3) the Google Jigsaw Toxic Comments dataset 4 which contains 159,571 samples in the training set. In the subsequent sections we refer to these datasets as OffensEval, Twitter, and Toxicity, respectively.\nTable 2 shows the results of the baseline models on correctly classifying the implicitly and explicitly offensive text as offensive/non-offensive (systems are denoted as a hyphenated combination of pretrained model and dataset). In every situation, the performance on the implicit task is significantly lower. The overall trend is perhaps unsurprising, as implicit examples lack clear indicators of offensiveness, such as highly offensive words. However, the degree to which these models underperform in the Implicit-OTD task illustrates the extent to which these tasks differ, and highlights the risk of deploying such models to perform this task in real-world situations.\nAn underlying assumption of this work and the\n4Google Jigsaw Toxic Comments\nmotivation for reasoning chains is the expectation that as the reasoning process is applied, the interpretation of the implicitly offensive utterance becomes increasingly (explicitly) offensive. We evaluate the extent to which this holds true in the dataset, using the baseline systems to predict the offensiveness of each rewrite across the reasoning chain. Appendix D shows that this is indeed the case, that moving down the reasoning chain correlates with higher accuracy, and implying that each step gradually reveals more of the offensive connotations in implicit offense. It also verifies that the collected/annotated chains have the property of being orderly."
    }, {
      "heading" : "4.2 Reasoning by Entailment",
      "text" : "The results of Section 4.1 indicate two things: current OTD systems perform poorly on the implicit OTD task, and the difficulty of using existing models decreases as each successive step of the reasoning chain is applied. This insight hints at a potential approach to implicit OTD: apply a reasoning model to map initial statements to their simplest and most explicit corresponding offensive statement (and score the likelihood of it being entailed by the original statement), and then score the resulting statement with a dedicated OTD model. In essence, this decomposes a difficult inference into a series of smaller inferences which may be tackled with higher accuracy by current models. We explore the possibility using this approach with existing models, assuming the human-annotated chains as gold proof paths.\nWe treat the problem of scoring reasoning chains as a multi-hop textual entailment problem as in Figure 2. Using an existing state-of-the-art textual entailment model, we score the transition from each step si to the next, si+1. Such models take as input\na pair of texts, <premise, hypothesis>, and output scores for a set of labels indicating “entailment” (Ep→h), “netural” and “contradiction” (Cp→h). An example reasoning step, the premise “You look like someone who could use more exercise.” entails the hypothesis “You are fat.”.\nA naive approach to multi-hop reasoning is to treat each transition as an independent event, and model the probability of a reasoning chain as a product of transition scores. In the context of reasoning chains, we define the probability of a chain c as:\nE(c) = l−1∏ i=0 Esi→si+1 (1)\nWe refer to this as MUL, the product model approach to multi-hop reasoning. For the entailment model scoring each transition in the chain, we consider two systems, one derived from DeBERTabase (He et al., 2021) and one from RoBERTalarge (Liu et al., 2019). Both systems were finetuned on the MNLI corpus (Nangia et al., 2017), a standard corpus for textual entailment.\nIn our experiments we are most interested in comparing the scores of MUL to those of methods which ignore the reasoning chain, either by scoring the entailment of the explicitly offensive statement given the implicit one(s0 → sl), or by using one of the current state-of-the-art approaches to classify the implicit statement directly(Table 2). While MUL is a naive model, any advantage of a model with such strong independence assumptions suggests areas where future multi-hop reasoning models could significantly improve over non-reasoning “single hop” counterparts.\nThe results of the multi-hop experiments are presented in Table 3. We observe that under most conditions, MUL outperforms Es0→sL by a modest margin. The performance of MUL does suffer on the longest reasoning chains as a result of an increasing number of < 1.0 multiplications (a consequence of the independence assumptions), negating the margins between the two systems. The detailed results can be found in Appendix G.\nIn terms of the types of reasoning which are most beneficial, we observe large changes in the transition scores before and after knowledge is integrated into the reasoning process, i.e., around KIR steps. We examine this behavior further, analyzing the performance of OTD models on predicting the final layer at points sk−1 and sk, before and after knowledge integration (Table 5). We observe significant (2-3 fold) improvements when predicting after knowledge is integrated. Similar results can also be observed on textual inference models as shown in Appendix E.\nTo explore the effectiveness of the external knowledge, we utilize the extracted knowledge mentioned in Section 3.2 and perform an additional set of experiments (denoted k+) where the external knowledge acquired in data annotation is added to each statement as a conjunction, until after a KIR step occurs. For instance, if the knowledge in sk is “Eating too much can make people fat.”, this knowledge will then be connected to all steps in {si|i = 0, 1, ..., k − 1} to form “<si> and eating too much can make people fat.” As shown in Table 3, adding knowledge increases scores for both models, but notably resulting in a significant advantage to the RoBERTa product model, which\nnow outperforms direct prediction, and all previous baseline models, in all scenarios. The resulting system is also more robust to long reasoning chains. We even observe that the performance margins over direct prediction in the 6-step chains exceeds that of 3-step setting."
    }, {
      "heading" : "5 Discussion",
      "text" : "We introduced this work based on a hypothesis of multi-hop approach as having a conceptual advantage over existing approaches to offensive text detection, in that humans must each be performing some reasoning process in order to find statements either offensive or unoffensive in different situations. We then showed that this conceptual advantage could translate to an empirical one, and showed performance gains over current approaches. However, we do so under strong assumptions and with access to additional information. How realistic is our experimental setup?"
    }, {
      "heading" : "5.1 A Perfect Reasoning Model?",
      "text" : "A concern in our initial entailment experiment is the naive product reasoning model. As mentioned in Section 4.2, an ideal reasoning model for this multi-hop approach should be a generative model that outputs explicitly offensive statements directly from implicitly offensive statements with reasoning process handled internally. In this sense, existing\ncontextual paraphrase generation models (Kazemnejad et al., 2020; Niu et al., 2021) can be promising candidates for a generative reasoning model. Such models aim at paraphrasing sentences while incorporating knowledge and external reasoning process and thus possess the potential to handle the reasoning process underlying implicit offensive statements after trained on large amount of data. But to what extend can a perfect reasoning model benefit the performance?\nTo answer that, we can assume a perfect paraphrasing model and the task reduces to whether we can predict the first transition from the implicit statement to the next step in the chain. This is akin to moving from an observed statement to a hypothetical knowledge base, upon which reasoning can occur to produce the explicitly offensive analog, which can be classified with high accuracy. As shown in Table 4a, the initial transition, Es0→s1 , can be predicted with much higher score than the direct prediction, Es1→sl . On one hand, This result shows that even if the model is aware of the corresponding explicitly offensive rewrite, it has difficulty directly understanding the inference relationship between them. Lower Cimplicit→non also shows the difficulty distinguishing implicit and non-offensive statements as shown in Table 4b. On the other hand, these results show the possibility of getting better inference by grounding the implicit\nstatement in a knowledgebase that follows the general structure of the reasoning chains, which can finally result in improvements in the overall classification accuracy. In other words, with such a paraphrasing model (rather than our naive product model), we should be able to improve the accuracy ultimately close to Es0→s1 ."
    }, {
      "heading" : "5.2 What Knowledge is Necessary?",
      "text" : "In a separate experiment, we identified the biggest obstacle to accurate reasoning to be the integration of existing knowledge. From Table 5, we are able to observe different effectiveness on different models. It is worth exploring what type of knowledge is necessary. We examined the entire set of knowledge to study what types of information is import to reasoning. Largely the information falls in 3 categories: (1) dictionary-based knowledge, (2) commonsense, and (3) folk knowledge. Statements of knowledge like “classic things are old.” is explained primarily as a way to bridge the gap between specific words, which might not be necessary given the gaining ability of large scale language models.\nA second form of knowledge, commonsense knowledge is exemplified in statements like, “salad is healthy.”. Existing work on defeasible reasoning (Sap et al., 2019; Zhang et al., 2020) has shown improvements incorporating external knowledge to support entailment-based reasoning using models similar to those used in this work. A third and unusual type of knowledge is “folk knowledge” which may be a personal opinion and factually inaccurate. Examples of this in the dataset can be “smart people don’t make mistakes.”Although it is potentially\npossible to embed such folk knowledge into pretrained language models through training, current trend in NLP research is to remove the biases from the training data (Bender et al., 2021). In this case, it is still difficult to collect such knowledge and we leave this for future work."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we aim to broaden the scope of offensive text detection research to include the nuanced utterances . Improvements in these models have applications ranging from distant futures where humans frequently interact with dialogue systems in situated ways which require such pragmatic reasoning to avoid unintended offense, to today’s online forums, where often a cat-and-mouse game of increasingly more creative offensive text creation and moderation occurs.\nIn addition to providing a dataset of implicitly offensive text, which can itself be used purely as a diagnostic of systems’ ability to identify more subtle instances of offensive text, we also provide chain of reasoning annotations which we hope can provide insight to how statements lead to offensive interpretations in certain situations. Our experiments provide a proof of concept of how multi-hop reasoning models have the potential to outperform directly classifying offensive text using current state-of-theart approaches, and identify areas for improvement via future research in commonsense knowledge base construction and inference."
    }, {
      "heading" : "7 Ethical Considerations",
      "text" : "In this work we aim to develop models which can more accurately predict the emotions elicited from text statements, and although our goal is to identify potentially harmful statements in order to avoid them, it is important to consider potential negative use-cases for such work. A system which can iden-\ntify offensive statements can also select for them, and it may be possible to use such a system to target users, attacking them on topics or attributes which they are most sensitive about. To the extent that we are able, we must be cautious not to aid in the development of such systems in the process of furthering research for more empathetic dialogue systems.\nWe tailor our study in two ways in an effort to reduce the risk of harm. First, we focus primarily on identifying implicitly offensive statements. While a system which produces implicitly offensive statements may still be used to attack users, they are significantly more challenging to generate when compared to explicitly offensive statements, which do not require any additional inferences or world knowledge. We hypothesize that this makes implicitly offensive statements unlikely to be utilized in offensive systems. Second, our dataset size is chosen with the goal of being large enough to support evaluation, but not training. It can therefore function as a useful diagnostic of offensive text detection systems, with limited risk of being used to create one. Third, in our dataset we have removed protected attributes such as ethnicity, gender and racism. Our dataset contains chain of reasoning which indicates the thinking processes of offensive statements. Given that such thinking processes could involve culture, personality and other highlevel affective elements, removing such attributes could prevent the present work to be used to construct toxic generation models."
    }, {
      "heading" : "A Samples",
      "text" : "Table 6 shows some examples in Mh-RIOT. Each sample contains an Attribute, an Implicit Statement, an Explicit Statement, a Non-offensive Statement and a chain of reasoning mapping from implicit statement to explicit statement. The postprocessing tag is attached to the end of each reasoning step, where AIR is for “Attribute Insertion Rule”, KIR is for “Knowledge Insertion Rule” and RR is for “Rephrasing Rule”.\nTable 7 shows some examples of extracted knowledge from KIR steps of the chain of reasoning during post-processing. sk−1 indicates the reasoning steps before applying KIR and sk indicates the reasoning steps after applying KIR.\nAttribute I eat lots of pancakes and syrup. Implicit Statement That can indeed give you extra energy. Explicit Statement You are fat."
    }, {
      "heading" : "B Attribute Categories",
      "text" : "Table 8 shows how we categorized and selected different attributes. The original attributes are divided into four big categories: AM, HAVE, MY and OTHER based on the syntax features (subject type, POS, Norm) of the sentence. Each category of AM, HAVE and MY are then divided into several subcategories based on the object type of the sentence."
    }, {
      "heading" : "C Crowdsourcing Instruction",
      "text" : "Figure 3 shows a template instruction that we used in our AMT tasks. Crowd workers are instructed with the purpose of the research and are notified about the potential offensive contents of this task."
    }, {
      "heading" : "D Sentence Classification Results",
      "text" : "Figure 4 shows the results of existing SOTA OTD models on each step of the chain of reasoning in Mh-RIOT."
    }, {
      "heading" : "E Model Details",
      "text" : "Table 9 shows the details of the models used in all of our experiments. We implemented the framework with the “TextClassification” pipeline from HuggingFace5. All models can be directly downloaded from the links given in the table.\n5https://huggingface.co/"
    }, {
      "heading" : "F Knowledge Entailment Experiment",
      "text" : "Table 10 shows the results of running text inference models around KIR steps of the chain of reasoning. To be noticed, we were not able to find any KIR steps in the chain of reasoning whose length is 3. This implies that knowledge insertion might not be necessary to interpret implicit statements that are not “implicit” enough."
    }, {
      "heading" : "G Knowledge Entailment Experiment",
      "text" : "Table 11 shows the final accuracy calculated with the entailment scores and accuracy of OTD models on Explicit inputs."
    } ],
    "references" : [ {
      "title" : "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
      "author" : [ "Francesco Barbieri", "Jose Camacho-Collados", "Luis Espinosa Anke", "Leonardo Neves." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020,",
      "citeRegEx" : "Barbieri et al\\.,? 2020",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2020
    }, {
      "title" : "McMillanMajor, and Shmargaret Shmitchell",
      "author" : [ "Emily M. Bender", "Timnit Gebru", "Angelina" ],
      "venue" : "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,",
      "citeRegEx" : "Bender et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "I feel offended, don’t be abusive! implicit/explicit messages in offensive and abusive language",
      "author" : [ "Tommaso Caselli", "Valerio Basile", "Jelena Mitrović", "Inga Kartoziya", "Michael Granitzer." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Caselli et al\\.,? 2020",
      "shortCiteRegEx" : "Caselli et al\\.",
      "year" : 2020
    }, {
      "title" : "FBK-DH at SemEval-2020 task 12: Using multi-channel BERT for multilingual offensive language detection",
      "author" : [ "Camilla Casula", "Alessio Palmero Aprosio", "Stefano Menini", "Sara Tonelli." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Seman-",
      "citeRegEx" : "Casula et al\\.,? 2020",
      "shortCiteRegEx" : "Casula et al\\.",
      "year" : 2020
    }, {
      "title" : "MeToo Alexa: How conversational systems respond to sexual harassment",
      "author" : [ "Amanda Cercas Curry", "Verena Rieser." ],
      "venue" : "Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing, pages 7–14, New Orleans,",
      "citeRegEx" : "Curry and Rieser.,? 2018",
      "shortCiteRegEx" : "Curry and Rieser.",
      "year" : 2018
    }, {
      "title" : "Text detoxification using large pre-trained neural models",
      "author" : [ "David Dale", "Anton Voronov", "Daryna Dementieva", "Varvara Logacheva", "Olga Kozlova", "Nikita Semenov", "Alexander Panchenko" ],
      "venue" : null,
      "citeRegEx" : "Dale et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Dale et al\\.",
      "year" : 2021
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber" ],
      "venue" : null,
      "citeRegEx" : "Davidson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Queens are powerful too: Mitigating gender bias in dialogue generation",
      "author" : [ "Emily Dinan", "Angela Fan", "Adina Williams", "Jack Urbanek", "Douwe Kiela", "Jason Weston." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Dinan et al\\.,? 2020",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task learning using AraBert for offensive language detection",
      "author" : [ "Marc Djandji", "Fady Baly", "Wissam Antoun", "Hazem Hajj." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offen-",
      "citeRegEx" : "Djandji et al\\.,? 2020",
      "shortCiteRegEx" : "Djandji et al\\.",
      "year" : 2020
    }, {
      "title" : "Moral stories: Situated reasoning about norms, intents, actions, and their consequences",
      "author" : [ "Denis Emelin", "Ronan Le Bras", "Jena D. Hwang", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "CoRR, abs/2012.15738.",
      "citeRegEx" : "Emelin et al\\.,? 2020",
      "shortCiteRegEx" : "Emelin et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting online hate speech using context aware models",
      "author" : [ "Lei Gao", "Ruihong Huang." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 260–266, Varna, Bulgaria. INCOMA",
      "citeRegEx" : "Gao and Huang.,? 2017",
      "shortCiteRegEx" : "Gao and Huang.",
      "year" : 2017
    }, {
      "title" : "A large labeled corpus for online harassment research",
      "author" : [ "jian Wan", "Derek Michael Wu" ],
      "venue" : "In Proceedings of the 2017 ACM on Web Science Conference,",
      "citeRegEx" : "Wan and Wu.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wan and Wu.",
      "year" : 2017
    }, {
      "title" : "Deberta: Decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "Paraphrase generation by learning how to edit from samples",
      "author" : [ "Amirhossein Kazemnejad", "Mohammadreza Salehi", "Mahdieh Soleymani Baghshah." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6010–",
      "citeRegEx" : "Kazemnejad et al\\.,? 2020",
      "shortCiteRegEx" : "Kazemnejad et al\\.",
      "year" : 2020
    }, {
      "title" : "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "author" : [ "Douwe Kiela", "Hamed Firooz", "Aravind Mohan", "Vedanuj Goswami", "Amanpreet Singh", "Pratik Ringshia", "Davide Testuggine" ],
      "venue" : null,
      "citeRegEx" : "Kiela et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2021
    }, {
      "title" : "Gedi: Generative discriminator guided sequence generation",
      "author" : [ "Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "Richard Socher", "Nazneen Fatema Rajani" ],
      "venue" : null,
      "citeRegEx" : "Krause et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2020
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut" ],
      "venue" : null,
      "citeRegEx" : "Lan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural Logic and Natural Language Inference, pages 129–147",
      "author" : [ "Bill MacCartney", "Christopher D. Manning." ],
      "venue" : "Springer Netherlands, Dordrecht.",
      "citeRegEx" : "MacCartney and Manning.,? 2014",
      "shortCiteRegEx" : "MacCartney and Manning.",
      "year" : 2014
    }, {
      "title" : "The RepEval 2017 shared task: Multi-genre natural language inference with sentence representations",
      "author" : [ "Nikita Nangia", "Adina Williams", "Angeliki Lazaridou", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2nd Workshop on Evaluating Vector Space Represen-",
      "citeRegEx" : "Nangia et al\\.,? 2017",
      "shortCiteRegEx" : "Nangia et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised paraphrasing with pretrained language models",
      "author" : [ "Tong Niu", "Semih Yavuz", "Yingbo Zhou", "Nitish Shirish Keskar", "Huan Wang", "Caiming Xiong" ],
      "venue" : null,
      "citeRegEx" : "Niu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2021
    }, {
      "title" : "Effective hate-speech detection in twitter data using recurrent neural networks",
      "author" : [ "Georgios K. Pitsilis", "Heri Ramampiaro", "Helge Langseth." ],
      "venue" : "Applied Intelligence, 48(12):4730–4742.",
      "citeRegEx" : "Pitsilis et al\\.,? 2018",
      "shortCiteRegEx" : "Pitsilis et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "CoRR, abs/1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis",
      "author" : [ "Björn Ross", "Michael Rist", "Guillermo Carbonell", "Ben Cabrera", "Nils Kurowsky", "Michael Wojatzki." ],
      "venue" : "Proceedings of NLP4CMC III: 3rd Workshop",
      "citeRegEx" : "Ross et al\\.,? 2016",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2016
    }, {
      "title" : "Atomic: An atlas of machine commonsense for ifthen reasoning",
      "author" : [ "Maarten Sap", "Ronan LeBras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Sap et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Are you a racist or am I seeing things? annotator influence on hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 138– 142, Austin, Texas. Association for Computational",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Understanding abuse: A typology of abusive language detection subtasks",
      "author" : [ "Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 78–84, Vancouver, BC, Canada.",
      "citeRegEx" : "Waseem et al\\.,? 2017",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2017
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88–93, San Diego, California. Association for Computa-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "IIITT@DravidianLangTech-EACL2021: Transfer learning for offensive language detection",
      "author" : [ "Konthala Yasaswini", "Karthik Puranik", "Adeep Hande", "Ruba Priyadharshini", "Sajeetha Thavareesan", "Bharathi Raja Chakravarthi" ],
      "venue" : null,
      "citeRegEx" : "Yasaswini et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yasaswini et al\\.",
      "year" : 2021
    }, {
      "title" : "Predicting the type and target of offensive posts in social media",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Zampieri et al\\.,? 2019a",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 13th Interna-",
      "citeRegEx" : "Zampieri et al\\.,? 2019b",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2020 task 12: Multilingual offensive language identification in social media (Offen",
      "author" : [ "Marcos Zampieri", "Preslav Nakov", "Sara Rosenthal", "Pepa Atanasova", "Georgi Karadzhov", "Hamdy Mubarak", "Leon Derczynski", "Zeses Pitenis", "Çağrı Çöltekin" ],
      "venue" : null,
      "citeRegEx" : "Zampieri et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2020
    }, {
      "title" : "Transomcs: From linguistic graphs to commonsense knowledge",
      "author" : [ "Hongming Zhang", "Daniel Khashabi", "Yangqiu Song", "Dan Roth" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Detecting hate speech on twitter using a convolution-gru based deep neural network",
      "author" : [ "Ziqi Zhang", "David Robinson", "Jonathan Tepper." ],
      "venue" : "The Semantic Web, pages 745–760, Cham. Springer International Publishing.",
      "citeRegEx" : "Zhang et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : ", 2017), racist (Waseem, 2016), or otherwise hateful text (Ross et al.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : ", 2017), racist (Waseem, 2016), or otherwise hateful text (Ross et al., 2016; Gao and Huang, 2017; Davidson et al., 2017).",
      "startOffset" : 58,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : ", 2017), racist (Waseem, 2016), or otherwise hateful text (Ross et al., 2016; Gao and Huang, 2017; Davidson et al., 2017).",
      "startOffset" : 58,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : ", 2017), racist (Waseem, 2016), or otherwise hateful text (Ross et al., 2016; Gao and Huang, 2017; Davidson et al., 2017).",
      "startOffset" : 58,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "The Hateful Memes dataset (Kiela et al., 2021) pairs images with unrelated text captions.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "The creation of OTD datasets enabled the development of ML-based approaches utilizing simple features, such as bag-of-word representations (Davidson et al., 2017).",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 28,
      "context" : "With the advent of social media platforms, many resources have been developed for identifying toxic comments in web text (Waseem and Hovy, 2016; Davidson et al., 2017), including a number of deep learning-based methods (Pitsilis et al.",
      "startOffset" : 121,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : "With the advent of social media platforms, many resources have been developed for identifying toxic comments in web text (Waseem and Hovy, 2016; Davidson et al., 2017), including a number of deep learning-based methods (Pitsilis et al.",
      "startOffset" : 121,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : ", 2017), including a number of deep learning-based methods (Pitsilis et al., 2018; Zhang et al., 2018b; Casula et al., 2020; Yasaswini et al., 2021; Djandji et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 35,
      "context" : ", 2017), including a number of deep learning-based methods (Pitsilis et al., 2018; Zhang et al., 2018b; Casula et al., 2020; Yasaswini et al., 2021; Djandji et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : ", 2017), including a number of deep learning-based methods (Pitsilis et al., 2018; Zhang et al., 2018b; Casula et al., 2020; Yasaswini et al., 2021; Djandji et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 29,
      "context" : ", 2017), including a number of deep learning-based methods (Pitsilis et al., 2018; Zhang et al., 2018b; Casula et al., 2020; Yasaswini et al., 2021; Djandji et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : ", 2017), including a number of deep learning-based methods (Pitsilis et al., 2018; Zhang et al., 2018b; Casula et al., 2020; Yasaswini et al., 2021; Djandji et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : "GeDi (Krause et al., 2020) proposed using class-conditional LMs as discriminators to reduce the toxicity produced by large pre-trained LMs (GPT-2).",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "OLID was also augmented with labels for capturing the degree of explicitness (Caselli et al., 2020)), and may also support research into resolving implicitly offensive statements.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "In this sense, a more similar approach comes from normative reasoning in moral stories (Emelin et al., 2020), where a short chain of reasoning is used to assess morality of actions and consequences.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "We represent the chain of reasoning as a series of sentence-to-sentence rewrites, similar to natural logic (MacCartney and Manning, 2014).",
      "startOffset" : 107,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : "One practical advantage of using a sentence-based representation for reasoning steps (in comparison to a structured representation like predicate-argument tuples) is that it allows the use of powerful text-to-text (T5) (Raffel et al., 2019) and entailment models (Liu et al.",
      "startOffset" : 219,
      "endOffset" : 240
    }, {
      "referenceID" : 18,
      "context" : ", 2019) and entailment models (Liu et al., 2019; He et al., 2021), which are trained on sentence-level input.",
      "startOffset" : 30,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and entailment models (Liu et al., 2019; He et al., 2021), which are trained on sentence-level input.",
      "startOffset" : 30,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "These include BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : ", 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : ", 2019), and ALBERT (Lan et al., 2020), three pretrained large scale language models fine-tuned on existing OTD datasets, which produce the highest accuracy reported on the explicit OTD task.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : "dataset (Zampieri et al., 2019a), discussed in Section 2, which contains 14,200 labeled tweets and includes implicit offensive statements, (2) the TWEETEVALL (Barbieri et al.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : ", 2019a), discussed in Section 2, which contains 14,200 labeled tweets and includes implicit offensive statements, (2) the TWEETEVALL (Barbieri et al., 2020) multitask offensive Twitter set for detecting irony, hate speech and offensive language, and (3) the Google Jigsaw Toxic Comments dataset 4 which contains 159,571 samples in the training set.",
      "startOffset" : 134,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "For the entailment model scoring each transition in the chain, we consider two systems, one derived from DeBERTabase (He et al., 2021) and one from RoBERTalarge (Liu et al.",
      "startOffset" : 117,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : ", 2021) and one from RoBERTalarge (Liu et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "Both systems were finetuned on the MNLI corpus (Nangia et al., 2017), a standard corpus for textual entailment.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "In this sense, existing contextual paraphrase generation models (Kazemnejad et al., 2020; Niu et al., 2021) can be promising candidates for a generative reasoning model.",
      "startOffset" : 64,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "In this sense, existing contextual paraphrase generation models (Kazemnejad et al., 2020; Niu et al., 2021) can be promising candidates for a generative reasoning model.",
      "startOffset" : 64,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "Existing work on defeasible reasoning (Sap et al., 2019; Zhang et al., 2020) has shown improvements incorporating external knowledge to support entailment-based reasoning using models similar to those used in this work.",
      "startOffset" : 38,
      "endOffset" : 76
    }, {
      "referenceID" : 33,
      "context" : "Existing work on defeasible reasoning (Sap et al., 2019; Zhang et al., 2020) has shown improvements incorporating external knowledge to support entailment-based reasoning using models similar to those used in this work.",
      "startOffset" : 38,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "possible to embed such folk knowledge into pretrained language models through training, current trend in NLP research is to remove the biases from the training data (Bender et al., 2021).",
      "startOffset" : 165,
      "endOffset" : 186
    } ],
    "year" : 0,
    "abstractText" : "We introduce the task of implicit offensive text detection in dialogues, where a statement may have either an offensive or non-offensive interpretation, depending on the listener and context. We argue that reasoning is crucial for understanding this broader class of offensive utterances, and create Mh-RIOT (Multi-hop Reasoning Implicitly Offensive Text Dataset), to support research on this task. Experiments using the dataset show that state-of-the-art methods of offense detection perform poorly when asked to detect implicitly offensive statements, achieving only ∼0.11 accuracy. In contrast to existing offensive text detection datasets, Mh-RIOT features human-annotated chains of reasoning which describe the mental process by which an offensive interpretation can be reached from each ambiguous statement. We explore the potential for a multi-hop reasoning approach by utilizing existing entailment models to score the transitions of these chains, and show that even naive reasoning models can result in improved performance in most situations. Analysis of the chains provides insight into the human interpretation process and emphasizes the importance of incorporating additional commonsense knowledge.",
    "creator" : null
  }
}
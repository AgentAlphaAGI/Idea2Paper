{
  "name" : "ARR_2022_120_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LexGLUE: A Benchmark Dataset for Legal Language Understanding in English",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Law is a field of human endeavor dominated by the use of language. As part of their professional training, law students consume large bodies of text as they seek to tune their understanding of the law and its application to help manage human behavior. Virtually every modern legal system produces massive volumes of textual data (Katz et al., 2020). Lawyers, judges, and regulators continuously author legal documents such as briefs, memos, statutes, regulations, contracts, patents and judicial decisions (Coupette et al., 2021). Beyond the consumption and production of language, law and the art of lawyering is also an exercise centered around the analysis and interpretation of text.\nNatural language understanding (NLU) technologies can assist legal practitioners in a variety of legal tasks (Chalkidis and Kampas, 2018; Aletras et al., 2019, 2020; Zhong et al., 2020b; Bommarito\net al., 2021), from judgment prediction (Aletras et al., 2016; Sim et al., 2016; Katz et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019a; Malik et al., 2021), information extraction from legal documents (Chalkidis et al., 2018, 2019c; Chen et al., 2020; Hendrycks et al., 2021) and case summarization (Bhattacharya et al., 2019) to legal question answering (Ravichander et al., 2019; Kien et al., 2020; Zhong et al., 2020a,c) and text classification (Nallapati and Manning, 2008; Chalkidis et al., 2019b, 2020a). Transformer models (Vaswani et al., 2017) pre-trained on legal, rather than generic, corpora have also been studied (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021).\nPre-trained Transformers, including BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DeBERTa (He et al., 2021) and numerous variants, are currently the state of the art in most natural language processing (NLP) tasks. Rapid performance improvements have been witnessed, to the extent that ambitious multi-task benchmarks (Wang et al., 2018, 2019b) are considered almost ‘solved’ a few years after their release and need to be made more challenging (Wang et al., 2019a).\nRecently, Bommasani et al. (2021) named these pre-trained models (e.g., BERT, DALL-E, GPT-3) foundation models. The term may be controversial, but it emphasizes the paradigm shift these models have caused and their interdisciplinary potential. Studying the latter includes the question of how to adapt these models to legal text (Bommarito et al., 2021). As discussed by Zhong et al. (2020b) and Chalkidis et al. (2020b), legal text has distinct characteristics, such as terms that are uncommon in generic corpora (e.g., ‘restrictive covenant’, ‘promissory estoppel’, ‘tort’, ‘novation’), terms that have different senses than in everyday language (e.g., an ‘executed’ contract is signed and effective, a ‘party’ is a legal entity), older expressions (e.g., pronominal adverbs like ‘herein’, ‘hereto’, ‘wherefore’), uncommon expressions from other languages (e.g., ‘laches’, ‘voir dire’, ‘certiorari’, ‘sub judice’), and long sentences with unusual word order (e.g., “the provisions for termination hereinafter appearing or will at the cost of the borrower forthwith comply with the same”) to the extent that legal language is often classified as a ‘sublanguage’ (Tiersma, 1999; Williams, 2007; Haigh, 2018). Furthermore, legal documents are often much longer than the maximum length state-ofthe-art deep learning models can handle, including those designed to handle long text (Beltagy et al., 2020; Zaheer et al., 2020; Yang et al., 2020).\nInspired by the recent widespread use of the GLUE multi-task benchmark NLP dataset (Wang et al., 2018, 2019b), the subsequent more difficult SuperGLUE (Wang et al., 2019a), other previous multi-task NLP benchmarks (Conneau and Kiela, 2018; McCann et al., 2018), and similar initiatives in other domains (Peng et al., 2019), we introduce LexGLUE, a benchmark dataset to evaluate the performance of NLP methods in legal tasks. LexGLUE is based on seven English existing legal NLP datasets, selected using criteria largely from SuperGLUE (discussed in Section 3.1).\nWe anticipate that more datasets, tasks, and languages will be added in later versions of LexGLUE.1 As more legal NLP datasets become available, we also plan to favor datasets checked thoroughly for validity (scores reflecting real-life performance), annotation quality, statistical power, and social bias (Bowman and Dahl, 2021).\nAs in GLUE and SuperGLUE (Wang et al., 1See https://nllpw.org/resources/ and https:// github.com/thunlp/LegalPapers for lists of papers, datasets, and other resources related to NLP for legal text.\n2019b,a), one of our goals is to push towards generic (or ‘foundation’) models that can cope with multiple NLP tasks, in our case legal NLP tasks, possibly with limited task-specific fine-tuning. Another goal is to provide a convenient and informative entry point for NLP researchers and practitioners wishing to explore or develop methods for legal NLP. Having these goals in mind, the datasets we include in LexGLUE and the tasks they address have been simplified in several ways, discussed below, to make it easier for newcomers and generic models to address all tasks. We provide Python APIs integrated with Hugging Face (Wolf et al., 2020; Lhoest et al., 2021) to easily import all the datasets we experiment with and evaluate the performance of different models (Section 5.3).\nBy unifying and facilitating the access to a set of law-related datasets and tasks, we hope to attract not only more NLP experts, but also more interdisciplinary researchers (e.g., law doctoral students willing to take NLP courses). More broadly, we hope LexGLUE will speed up the adoption and transparent evaluation of new legal NLP methods and approaches in the commercial sector too. Indeed, there have been many commercial press releases in the legal-tech industry on high-performing systems, but almost no independent evaluation of the performance of machine learning and NLPbased tools. A standard publicly available benchmark would also allay concerns of undue influence in predictive models, including the use of metadata which the relevant law expressly disregards."
    }, {
      "heading" : "2 Related Work",
      "text" : "The rapid growth of the legal text processing field is demonstrated by numerous papers presented in top-tier conferences in NLP and artificial intelligence (Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019a; Valvoda et al., 2021) as well as surveys (Chalkidis and Kampas, 2018; Zhong et al., 2020b; Bommarito et al., 2021). Moreover, specialized workshops on NLP for legal text (Aletras et al., 2019; Di Fatta et al., 2020; Aletras et al., 2020) are regularly organized.\nA core task in this area has been legal judgment prediction (forecasting), where the goal is to predict the outcome (verdict) of a court case. In this direction, there have been at least three lines of work. The first one (Aletras et al., 2016; Chalkidis et al., 2019a; Medvedeva et al., 2020, 2021) predicts violations of human rights in cases of the\nEuropean Court of Human Rights (ECtHR). The second line of work (Luo et al., 2017; Zhong et al., 2018; Yang et al., 2019) considers Chinese criminal cases where the goal is to predict relevant law articles, criminal charges, and the term of the penalty. The third line of work (Ruger et al., 2004; Katz et al., 2017; Kaufman et al., 2019) includes methods for predicting the outcomes of cases of the Supreme Court of the United States (SCOTUS).\nThe same or similar tasks have also been studied with court cases in many other jurisdictions including France (Şulea et al., 2017), Philippines (Virtucio et al., 2018), Turkey (Mumcuoğlu et al., 2021), Thailand (Kowsrihawat et al., 2018), United Kingdom (Strickson and De La Iglesia, 2020), Germany (Urchs et al., 2021), and Switzerland (Niklaus et al., 2021). Apart from predicting court decisions, there is also work aiming to interpret (explain) the decisions of particular courts (Ye et al., 2018; Chalkidis et al., 2021c; Branting et al., 2021).\nAnother popular task is legal topic classification. Nallapati and Manning (2008) highlighted the challenges of legal document classification compared to more generic text classification by using a dataset including docket entries of US court cases. Chalkidis et al. (2020a) classify EU laws into EuroVoc concepts, a task earlier introduced by Mencia and Fürnkranzand (2007), with a special interest in few- and zero-shot learning. Luz de Araujo et al. (2020) also studied topic classification using a dataset of Brazilian Supreme Court cases. There are similar interesting applications in contract law (Lippi et al., 2019; Tuggener et al., 2020).\nSeveral studies (Chalkidis et al., 2018, 2019c; Hendrycks et al., 2021) explored information extraction from contracts, to extract important information such as the contracting parties, agreed payment amount, start and end dates, applicable law, etc. Other studies focus on extracting information from legislation (Cardellino et al., 2017; Angelidis et al., 2018) or court cases (Leitner et al., 2019).\nLegal Question Answering (QA) is another task of interest in legal NLP, where the goal is to train models for answering legal questions (Kim et al., 2015; Ravichander et al., 2019; Kien et al., 2020; Zhong et al., 2020a,c). Not only is this task interesting for researchers but it could support efforts to help laypeople better understand their legal rights. In the general task setting, this requires identifying relevant legislation, case law, or other legal documents, and extracting elements of those documents\nthat answer a particular question. A notable venue for legal QA has been the Competition on Legal Information Extraction and Entailment (COLIEE) (Kim et al., 2016; Kano et al., 2017, 2018).\nMore recently, there have also been efforts to pre-train Transformer-based language models on legal corpora (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021), leading to state-of-theart results in several legal NLP tasks, compared to models pre-trained on generic corpora.\nOverall, the legal NLP literature is overwhelming, and the resources are scattered. Documentation is often not available, and evaluation measures vary across articles studying the same task. Our goal is to create the first unified benchmark to access the performance of NLP models on legal NLU. As a first step, we selected a representative group of tasks, using datasets in English that are also publicly available, adequately documented and have an appropriate size for developing modern NLP methods. We also introduce several simplifications to make the new benchmark more standardized and easily accessible, as already noted."
    }, {
      "heading" : "3 LexGLUE Tasks and Datasets",
      "text" : "We present the Legal General Language Understanding2 Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks."
    }, {
      "heading" : "3.1 Dataset Desiderata",
      "text" : "The datasets of LexGLUE were selected to satisfy the following desiderata:\n• Language: We only consider English datasets, to make experimentation easier for researchers across the globe.\n• Substance:3 The datasets should check the ability of systems to understand and reason about legal text to a certain extent in order to perform tasks that are meaningful for legal practitioners.\n• Difficulty: The performance of state-of-the-art methods on the datasets should leave large scope for improvements (cf. GLUE and SuperGLUE, where top-ranked models now achieve average scores higher than 90%). Unlike SuperGLUE (Wang et al., 2019a), we did not rule out, but\n2The term ‘understanding’ is, of course, as debatable as in NLU and GLUE, but is commonly used in NLP to refer to systems that analyze, rather than generate text.\n3We reuse this term from the work of Wang et al. (2019a).\nrather favored, datasets requiring domain (in our case legal) expertise.\n• Availability & Size: We consider only publicly available datasets, documented by published articles, avoiding proprietary, untested, poorly documented datasets. We also excluded very small datasets, e.g., with fewer than 5K documents. Although large pre-trained models often perform well with relatively few task-specific training instances, newcomers may wish to experiment with simpler models that may perform disappointingly with small training sets. Small test sets may also lead to unstable and unreliable results."
    }, {
      "heading" : "3.2 Tasks and Datasets",
      "text" : "LexGLUE comprises seven datasets in total. Table 1 shows core information for each of the LexGLUE datasets and tasks, described in detail below.\nECtHR Tasks A & B The European Court of Human Rights (ECtHR) hears allegations that a state has breached human rights provisions of the European Convention of Human Rights (ECHR). We use the dataset of Chalkidis et al. (2019a, 2021c), which contains approx. 11K cases from the ECtHR public database. The cases are chronologically split into training (9k, 2001–2016), development (1k, 2016–2017), and test (1k, 2017–2019). For each case, the dataset provides a list of factual paragraphs (facts) from the case description. Each case is mapped to articles of the ECHR that were violated (if any). In Task A, the input to a model is the list of facts of a case, and the output is the set of violated articles. In the most recent version of the dataset (Chalkidis et al., 2021c), each case is also mapped to articles of ECHR that were allegedly violated (considered by the court). In Task B, the input is again the list of facts of a case, but the output is the set of allegedly violated articles.\nThe total number of ECHR articles is currently 66. Several articles, however, cannot be violated, are rarely (or never) discussed in practice, or do not\ndepend on the facts of a case and concern procedural technicalities. Thus, we use a simplified version of the label set (ECHR articles) in both Task A and B, including only 10 ECHR articles that can be violated and depend on the case’s facts.\nSCOTUS The US Supreme Court (SCOTUS)4 is the highest federal court in the United States of America and generally hears only the most controversial or otherwise complex cases which have not been sufficiently well solved by lower courts. We combine information from SCOTUS opinions5 with the Supreme Court DataBase (SCDB)6 (Spaeth et al., 2020). SCDB provides metadata (e.g., decisions, issues, decision directions) for all cases (from 1946 up to 2020). We opted to use SCDB to classify the court opinions in the available 14 issue areas (e.g., Criminal Procedure, Civil Rights, Economic Activity, etc.). This is a singlelabel multi-class classification task (Table 1). The 14 issue areas cluster 278 issues whose focus is on the subject matter of the controversy (dispute). The SCOTUS cases are chronologically split into training (5k, 1946–1982), development (1.4k, 1982– 1991), test (1.4k, 1991–2016) sets.\nEUR-LEX European Union (EU) legislation is published in EUR-Lex portal.7 All EU laws are annotated by EU’s Publications Office with multiple concepts from EuroVoc, a multilingual thesaurus maintained by the Publications Office.8 The current version of EuroVoc contains more than 7k concepts referring to various activities of the EU and its Member States (e.g., economics, healthcare, trade). We use the English part of the dataset of Chalkidis et al. (2021a), which comprises 65K EU laws (documents) from EUR-Lex. Given a document, the task is to predict its EuroVoc labels (concepts). The dataset is chronologically split in training (55k, 1958–2010), development (5k, 2010–\n4https://www.supremecourt.gov 5https://www.courtlistener.com 6http://scdb.wustl.edu 7http://eur-lex.europa.eu/ 8http://eurovoc.europa.eu/\n2012), test (5k, 2012–2016) subsets. It supports four different label granularities, comprising 21, 127, 567, 7390 EuroVoc concepts, respectively. We use the 100 most frequent concepts from level 2, which has a highly skewed label distribution and temporal concept drift (Chalkidis et al., 2021a), making it sufficiently difficult for an entry point.\nLEDGAR Tuggener et al. (2020) introduced LEDGAR (Labeled EDGAR), a dataset for contract provision (paragraph) classification. The contract provisions come from contracts obtained from the US Securities and Exchange Commission (SEC) filings, which are publicly available from EDGAR9 (Electronic Data Gathering, Analysis, and Retrieval system). The original dataset includes approx. 850k contract provisions labeled with 12.5k categories. Each label represents the single main topic (theme) of the corresponding contract provision, i.e., this is a single-label multi-class classification task. In LexGLUE, we use a subset of the original dataset with 80k contract provisions, considering only the 100 most frequent categories as a simplification. We split the new dataset chronologically into training (60k, 2016–2017), development (10k, 2018), and test (10k, 2019) sets.\nUNFAIR-ToS The UNFAIR-ToS dataset (Lippi et al., 2019) contains 50 Terms of Service (ToS) from on-line platforms (e.g., YouTube, Ebay, Facebook, etc.). The dataset has been annotated on the sentence-level with 8 types of unfair contractual terms, meaning terms (sentences) that potentially violate user rights according to EU consumer law.10 The input to a model is a sentence, the output is the set of unfair types (if any). We split the dataset chronologically into training (5.5k, 2006–2016), development (2.3k, 2017), test (1.6k, 2017) sets.\nCaseHOLD The CaseHOLD (Case Holdings on Legal Decisions) dataset (Zheng et al., 2021) con-\n9https://www.sec.gov/edgar/ 10Art. 3 of Direct. 93/13, Unfair Terms in Consumer Contracts (http://data.europa.eu/eli/dir/1993/13/oj).\ntains approx. 53k multiple choice questions about holdings of US court cases from the Harvard Law Library case law corpus. Holdings are short summaries of legal rulings that accompany referenced decisions relevant for the present case, e.g.:\n“. . . to act pursuant to City policy, re d 503, 506-07 (3d Cir.l985)(holding that for purposes of a class certification motion the court must accept as true all factual allegations in the complaint and may draw reasonable inferences therefrom).”\nThe input consists of an excerpt (or prompt) from a court decision, containing a reference to a particular case, where the holding statement (in boldface) is masked out. The model must identify the correct (masked) holding statement from a selection of five choices. We split the dataset in training (45k), development (3.9k) and test (3.9k) sets, excluding samples that are shorter than 256 tokens. Chronological information is missing from CaseHOLD, thus we cannot perform a chronological re-split."
    }, {
      "heading" : "4 Models Considered",
      "text" : ""
    }, {
      "heading" : "4.1 Linear SVM",
      "text" : "Our first baseline model is a linear Support Vector Machine (SVM) (Cortes and Vapnik, 1995) with TF-IDF features for the top-K frequent n-grams of the training set, where n ∈ [1, 2, 3]."
    }, {
      "heading" : "4.2 Pre-trained Transformer Models",
      "text" : "We experiment with Transformer-based (Vaswani et al., 2017) pre-trained language models, which achieve state of the art performance in most NLP tasks (Bommasani et al., 2021) and NLU benchmarks (Wang et al., 2019a). These models are pretrained on very large unlabeled corpora to predict masked tokens (masked language modeling) and typically also to perform other pre-training tasks that still do not require any manual annotation (e.g., predicting if two sentences were adjacent in the corpus or not, dubbed next sentence prediction). The pre-trained models are then fine-tuned (further trained) on task-specific (typically much smaller)\nannotated datasets, after adding task-specific layers. We fine-tune and evaluate the performance of the following publicly available models (Table 2).\nBERT (Devlin et al., 2019) is the best-known pretrained Transformer-based language model. It is pre-trained to perform masked language modeling and next sentence prediction.\nRoBERTa (Liu et al., 2019) is also a pre-trained Transformer-based language model. Unlike BERT, RoBERTa uses dynamic masking, it eliminates the next sentence prediction pre-training task, uses a larger vocabulary, and has been pre-trained on much larger corpora. Liu et al. (2019) reported improved results on NLU benchmarks using RoBERTa, compared to BERT.\nDeBERTa (He et al., 2021) is another improved BERT model that uses disentangled attention, i.e., four separate attention mechanisms considering the content and the relative position of each token, and an enhanced mask decoder, which explicitly considers the absolute position of the tokens. DeBERTa has been reported to outperform BERT and RoBERTa in several NLP tasks (He et al., 2021).\nLongformer (Beltagy et al., 2020) extends Transformer-based models to support longer sequences, using sparse-attention. The latter is a combination of local (window-based) attention and global (dilated) attention that reduces the computational complexity of the model and thus can be deployed in longer documents (up to 4096 tokens). Longformer outperforms RoBERTa on long document tasks and QA benchmarks.\nBigBird (Zaheer et al., 2020) is another sparseattention based transformer that uses a combination of a local (window-based) attention, global (dilated), and random attention, i.e., all tokens also attend a number of random tokens on top of those in the same neighborhood (window). BigBird has been reported to outperform Longformer on QA\nand summarization tasks.\nLegal-BERT (Chalkidis et al., 2020b) is a BERT model pre-trained on English legal corpora, consisting of legislation, contracts, and court cases. It uses the original pre-training BERT configuration. The sub-word vocabulary of Legal-BERT is built from scratch, to better support legal terminology.\nCaseLaw-BERT (Zheng et al., 2021) is another law-specific BERT model. It also uses the original pre-training BERT configuration and has been pre-trained from scratch on the Harvard Law case corpus,11 which comprises 3.4M legal decisions from US federal and state courts. This model is called Custom Legal-BERT by Zheng et al. (2021). We call it CaseLaw-BERT to distinguish it from the previously published Legal-BERT of Chalkidis et al. (2020b) and to better signal that it is trained exclusively on case law (court opinions).\nHierarchical Variants Legal documents are usually much longer (i.e. consisting of thousands of words) than other text (e.g., tweets, customer reviews, news articles) often considered in various NLP tasks. Thus, standard Transformer-based models that can typically process up to 512 subword units cannot be directly applied across all LexGLUE datasets, unless documents are severely truncated to the model’s limit. Figure 2 shows the distribution of text input length across all LexGLUE datasets. Even for Transformer-based models specifically designed to handle long text (e.g., LongFormer, BigBird), handling longer legal documents remains a challenge. Given the length of the text input in three of the seven LexGLUE tasks, ECtHR (A and B) and SCOTUS, we employ a hierarchical variant of all pre-trained Transformerbased models that have not been designed for longer text (BERT, RoBERTa, DeBERTa, LegalBERT, CaseLaw-BERT) during fine-tuning and in-\n11https://case.law/\nference. The hierarchical variants are similar to that of Chalkidis et al. (2021c). They use the corresponding pre-trained Transformer-based model to encode each paragraph of the input text independently and obtain the top-level representation h[cls] of each paragraph. A second-level shallow (2-layered) Transformer encoder with the exact same specifications (e.g., hidden units, number of attention heads) is fed with the paragraph representations to make them context-aware (aware of the surrounding paragraphs). We then max-pool over the context-aware paragraph representations to obtain a document representation, which is fed to a classification layer, as in the rest of the datasets.12"
    }, {
      "heading" : "4.3 Task-Specific Fine-Tuning",
      "text" : "Text Classification Tasks For EUR-LEX, LEDGAR and UNFAIR-ToS tasks, we feed each document to the pre-trained model (e.g., BERT) and obtain the top-level representation h[cls] of the special [cls] token as the document representation, following Devlin et al. (2019). The latter goes through a dense layer of L output units, one per label, followed by a sigmoid (in EUR-LEX, UNFAIR-ToS) or softmax (in LEDGAR) activation, respectively. For the two ECtHR tasks (A and B) and SCOTUS, where the hierarchical model is employed, we feed the max-pooled (over paragraphs) document representation to a classification linear layer. The linear layer is again followed by a sigmoid (EctHR) or softmax (SCOTUS) activation.\nMultiple-Choice QA Task For CaseHOLD, we convert each training (or test) instance (the prompt and the five candidate answers) into five input pairs\n12In Appendix D, we present results from preliminary experiments using the standard version of BERT for ECtHR Task A (-12.2%), Task B(-10.6%), and SCOTUS (-3.5%).\nfollowing Zheng et al. (2021). Each pair consists of the prompt and one of the five candidate answers, separated by the special delimiter token [sep]. The top-level representation h[cls] of each pair is fed to a linear layer to obtain a logit (score), and the five logits are then passed through a softmax activation to obtain a probability distribution over the five candidate answers."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Set Up",
      "text" : "For TFIDF-based linear SVM models, we use the implementation of Scikit-learn (Pedregosa et al., 2011) and grid-search for hyper parameters (number of features, C, and loss function). For all the pre-trained models, we use publicly available Hugging Face checkpoints.13 We use the *-base configuration of each pre-trained model, i.e., 12 Transformer blocks, 768 hidden units, and 12 attention heads. We train models with the Adam optimizer (Kingma and Ba, 2015) and an initial learning rate of 3e-5 up to 20 epochs using early stopping on development data. We use mixed precision (fp16) to decrease the memory footprint in training and gradient accumulation for all hierarchical models.14 The hierarchical models can read up to 64 paragraphs of 128 tokens each. We use Longformer and BigBird in default settings, i.e., Longformer uses windows of 512 tokens and a single global token ([cls]), while BigBird uses blocks of 64 tokens (windows: 3x block, random: 3x block, global: 2x initial block; each token attends 512 tokens in total). The batch size is 8 in all experiments. We run five repetitions with different random seeds and report the average scores across runs on the test\n13http://huggingface.co/models 14We omit results for *-large models, as we found them to be very unstable in preliminary experiments using fp16 and fp32. See details in Appendix E.\nset. We evaluate classification performance using micro-F1 (µ-F1) and macro-F1 (m-F1) across all datasets to take into account class imbalance."
    }, {
      "heading" : "5.2 Experimental Results",
      "text" : "Table 3 presents the test results for all models across all LexGLUE tasks. We observe that the two legal-oriented pre-trained models (Legal-BERT, CaseLaw-BERT) perform overall better across all tasks. Their in-domain (legal) knowledge seems to be more critical in the two datasets relying on US case law data (SCOTUS, CaseHOLD) with an improvement of approximately +5% over the rest of the models, which are pre-trained on generic corpora. While these two models perform mostly on par, CaseLaw-BERT performs marginally better than Legal-BERT on SCOTUS and CaseHOLD, which is easily explained by the fact that it is solely trained on US case law. On the other hand LegalBERT has been exposed to a wider variety of legal corpora, including EU legislation, ECtHR cases, US contracts; thus it performs slightly better in EUR-LEX, LEDGAR, UNFAIR-ToS, without performing better than CaseLaw-BERT, though, in the ECtHR tasks. No single model performs best in all tasks, and the results of Table 3 show that there is still large scope for improvements.\nAn exceptional case to the dominance of the pretrained Transformer models is the SCOTUS dataset, where the TFIDF-based linear SVM performs best. We suspect the large size of the SCOTUS opinions (Figure 2) to be the main reason, i.e., in many cases full paragraphs or parts of them are not considered by the hierarchical models (limited to 64 paragraphs of 128 tokens each)."
    }, {
      "heading" : "5.3 Data Repository and Code",
      "text" : "For reproducibility purposes and to facilitate future experimentation with other models, we pre-process and release all datasets on Hugging Face Datasets (Lhoest et al., 2021).15 Furthermore, we release the code of our experiments, which relies on the Hugging Face transformers (Wolf et al., 2020) library16 on Github.17 Details on how to load the datasets and how to run experiments with our code are available in Appendix B. We also provide a leaderboard where new results can be added.\n15https://huggingface.co/datasets/xxx 16https://huggingface.co/transformers 17https://github.com/xxx/xxx"
    }, {
      "heading" : "6 Limitations and Future Work",
      "text" : "Beyond the scope of this work, we identify four major factors that could potentially advance the state of the art in LexGLUE and legal NLP more generally: (a) models capable of reading longer documents; (b) models capable of exploiting document structure; (c) large-scale legal pre-training, and (d) using larger models.18\nIn its current version, LexGLUE can only be used to evaluate English language models. As legal documents are typically written in the official language of the particular country of origin, there is an increasing need for developing models for other languages. The current scarcity of datasets in other languages (with the exception of Chinese) makes a multilingual extension of LexGLUE challenging, but an interesting avenue for future research.\nBeyond language barriers, legal restrictions currently inhibit the creation of more datasets. Important document types, such as contracts and scholarly publications are protected by copyright or considered trade secrets. As a result, their owners are concerned with data-leakage when they are used for model training and evaluation. Providing both legal and technical solutions, e.g., using privacy-aware infrastructure and models (Downie, 2004; Feyisetan et al., 2020) is a challenge to be addressed.\nAccess to court decisions can also be hindered by bureaucratic inertia, outdated technology and data protection concerns, which collectively result in these otherwise public decisions not being publicly available (Pah et al., 2020). While the anonymization of personal data provides a solution to this problem, it is itself an open challenge for legal NLP (Jana and Biemann, 2021). In lack of suitable datasets and benchmarks, we have refrained from including anonymization in this version of LexGLUE, but plan to do so at a later stage.\nWhile LexGLUE offers a much needed unified testbed for legal NLU, there are several other critical aspects that need to be studied carefully. These include multi-disciplinary research to better understand the limitations and challenges of applying NLP to law (Binns, 2020), while also considering fairness (Angwin et al., 2016; Dressel and Farid, 2018), and robustness (Wang et al., 2021).\n18A more detailed discussion of these suggested future directions can be found in Appendix A.\nEthics Statement\nAll datasets included in LexGLUE are publicly available and have been previously published. If datasets or the papers where they were introduced in were not compiled or written by ourselves, we have referenced the original work and encourage LexGlue users to do so as well. In fact, we believe that this work should only be referenced, in addition to citing the original work, when jointly experimenting with multiple LexGLUE datasets and using the LexGLUE evaluation framework and infrastructure. Otherwise only the original work should be cited.\nWe believe that this work does not contain any grounds for ethical concerns. A transparent and rigorous benchmark for NLP in the legal domain might serve as an orientation for scholars and industry researchers. As a result, the capabilities of tools that are trained using natural language data from the legal domain will become clearer, thereby helping their users to better understand them. This increased certainty would also raise the awareness within research and industry communities to potential risks associated with the use of these tools. We regard this contribution to a more realistic, more informed discussion as an important use case of the work presented. Ideally, it could help both beginners and seasoned professionals to understand the limitations of using NLP tools in the legal domain and thereby prevent exaggerated expectations and potential applications that might risk endangering fundamental rights or the rule of law. We currently cannot imagine use cases of this particular work that would lead to ethical concerns or potential harm.\nLexGLUE comprises seven datasets: ECtHR Task A and B, SCOTUS, EUR-LEX, LEDGAR, UNFAIR-ToS, and CaseHOLD. ECtHR contains personal data of the parties and other people involved in the legal proceedings. Its data is processed and made public in accordance with European Data Protection Law. This includes either implied consent or legitimate interest to process the data for research purposes. The data is considered to be in the public sphere from a privacy perspective. As a result, their processing by us or other future users of the benchmark is not likely to raise ethical concerns. SCOTUS contains personal data of a similar nature. Again, the data is processed and made available by the US Supreme Court, whose proceedings are public. While this\nensures compliance with US law, it is very likely that similarly to the ECtHR any processing could be justified by either implied consent or legitimate interest under European law. EUR-LEX by contrast is merely a collection of legislation material and therefore not likely to contain personal data. It is published by the European Union and processed by the EU’s Publication Office. In addition, since our work qualifies as research, it is privileged pursuant to Art. 6 (1) (f) GDPR. LEDGAR contains publicly available contract provisions published in the EDGAR database of US Securities and Exchange Commission (SEC). As far as personal information might be contained, it should equally fall into the public sphere and be covered by research privilege. Our processing does not focus on personal information at all, rather attributing content labels to provisions. UNFAIR-ToS contains Terms of Services from business entities such as YouTube, Ebay, Facebook etc., which makes it unlikely for the data to include personal information. These companies keep user data separate from contractual provisions, so to the best of our knowledge not contained in this dataset. CaseHOLD contains parts of legal decisions from US Court decisions, obtained from the Harvard library case law corpus. All of the decisions were previously published in compliance with US law. In addition, most instances (case snippets) are too short to contain identifiable information. Should such data be contained, their processing would equally be covered either by implicit consent or a public interest exception. We use all datasets in accordance with copyright terms and under the licenses set forth by their creators.\nWe have not employed any crowd-workers or annotators for this work. The paper outlines the main limitations with regard to speaker population (English) and generalizability in a dedicated section (Section 6. As a benchmark paper, our claims naturally match the results of the experiments, which – given the current detail of instructions – should be easily reproduced. We provide several ways of accessing the datasets and running the experiments both with and without Hugging Face infrastructure.\nWe do not currently foresee any potential harms for vulnerable or marginalized populations and we do not use, to the best of our knowledge, any identifying characteristics for populations of these kinds."
    }, {
      "heading" : "B Datasets, Code, and Participation",
      "text" : "Where are the datasets? We provide access to LexGLUE on Hugging Face Datasets (Lhoest et al., 2021) at https://huggingface.co/datasets/ xxx. For example to load the SCOTUS dataset, you first simply install the dataset’s Python library and then make the following call: ___________________________________________________\nfrom datasets import load_dataset dataset = load_dataset(\"xxx\", task=’scotus’)\n___________________________________________________\nHow do I run experiments? To make reproducing the results of the already examined models or future models even easier, we release our code on GitHub (https://github.com/xxx/xxx). In that repository (in the folder /experiments), there are Python scripts, relying on the Hugging Face Transformers library (Wolf et al., 2020), to run and evaluate any Transformer-based model (e.g., BERT, RoBERTa, LegalBERT, and their hierarchical variants, as well as, Longforrmer, and BigBird). We also provide bash scripts to replicate the experiments for each dataset with 5 randoms seeds, as we did for the reported results."
    }, {
      "heading" : "C No labeling as an additional class",
      "text" : "In ECtHR Tasks A & B and UNFAIR-ToS, there are unlabeled samples. Concretely, in ECtHR Task A, a possible event is no violation, i.e., the court ruled that the defendant did not violate any ECHR article. Contrary, no violation is not a possible event in the original ECtHR Task B dataset, i.e., at least a single ECHR article is allegedly violated (considered by the court) in every case; however,\nthere is such a rare scenario after the simplifications we introduced, i.e., some cases were originally labeled only with rare labels that were excluded from our benchmark (Section 3.2). In UNFAIR-ToS, the vast majority of sentences are not labeled with any type of unfairness (unfair term against users), i.e., most sentences do not raise any questions of possible violations of the European consumer law.\nIn multi-label classification, the set of labels per instance is represented as a one-hot vector Y = [y1, y2, . . . , yL], where yi = 1 if the instance is labeled with the i-th class, and yi = 0 otherwise. If an instance is not labeled with any class, its Y includes only zeros. During training, binary cross-entropy correctly penalizes such instances, if the predictions (Ŷ = [ŷ1, ŷ2 . . . , ŷL]) diverge from zeros. During evaluation, however, the F1-score (F1 = TP\nTP+ 12 (FP+FN) ) ignores instances with Y = Ŷ =\n[0, 0, . . . , 0], because it considers only the true positives (TP), false positives (FP), and false negatives (FN), and instances where Y = Ŷ = [0, 0, . . . , 0] contribute no TPs, FPs, FNs. In order to make F1 sensitive to the correct labeling of such examples, during evaluation we include an additional label (y0) in both targets (Y) and predictions (Ŷ), whose value is 1 (positive), if Y = Ŷ = [0, 0, . . . , 0], and 0 (negative) otherwise. This is particularly important for proper evaluation, as across three datasets a considerable portion of the examples are unlabeled (11.5% in ECtHR Task A, 1.6% in ECtHR Task B, and 95.5% in UNFAIR-ToS)."
    }, {
      "heading" : "D Use of 512-token BERT models",
      "text" : "In Table 3, we show results for the standard BERT model of Devlin et al. (2019), which can process up to 512 tokens, compared to its hierarchical variant (Section 4.2), which can process up to 64×128 tokens. We observe that across all datasets that contain long documents (Figure 2(a)), the hierarchical variant clearly outperforms the standard model fed with truncated documents (ECtHR A: +12.2%, ECtHR B: 10.6%, SCOTUS: 3.5%). Compared to the ECtHR tasks, the gains are lower in SCOTUS, a topic classification task where long-range reasoning is not needed; by contrast, for ECtHR multiple distant facts need to be combined."
    }, {
      "heading" : "E Use of Larger Models",
      "text" : "In preliminary experiments, we also considered pre-trained models using the *-large configuration, i.e., 24 Transformer blocks, 1024 units, and 18 attentions heads, namely RoBERTa-large (Liu et al., 2019). Similarly to the rest of the experiments, we\nused mixed precision (fp16) to decrease the memory footprint in training. Using fp16 in attention operation resulted in floating point overflow and NaNs in later stages of training. Similar issues have been also reported by Beltagy et al. (2020). In our case, we faced similar issues even when training models with the standard fp32 setting. Nevertheless, we present in Fig. 4 the results we obtained for LEDGAR and CaseHOLD using RoBERTa-large, comparing to RoBERTa-base results."
    }, {
      "heading" : "F Additional Results",
      "text" : "Table 4 shows development results for all examined models across datasets. The development results are generally higher and vary compared to the test ones (cf. Table 3), which further highlights the importance of temporal splits. Table 5 reports training times per dataset and model.\nG Other Tasks/Datasets Considered\nWe considered including the Contract Understanding Atticus Dataset (CUAD) (Hendrycks et al., 2021), an expertly curated dataset that comprises 510 contracts annotated with 41 valuable contractual insights (e.g., agreement date, parties, governing law). The task is formulated as a SQUAD-like question answering task, where given a question (the name of an insight) and a paragraph from the contract, the model has to identify the answer span in the paragraph.19 The original dataset follows the SQUAD v2.0 setting, including unanswerable questions. Following SQUAD v1.1 (Rajpurkar et al., 2016), we simplified the task by removing all unanswerable pairs (question, paragraph), which are the majority in the original dataset. We also excluded pairs whose answers exceeded 128 full words to\n19The question mostly resembles a prompt, rather than a natural question, as there is a closed set of 41 alternatives.\nalleviate the imbalance between short and long answers. We then re-split the dataset chronologically into training (5.2k, 1994–2019), development (572, 2019–2020), and test (604, 2020) sets.\nFollowing Devlin et al. (2019), and similarly to Hendrycks et al. (2021), for each training (or test) instance, we consider pairs that consist of a question and a paragraph, separated by the special delimiter token [sep]. The top-level representations [h1, . . . , hN] of the tokens of the paragraph are fed into a linear layer to obtain two logits per token (for the token being the start or end of the answer span), which are then passed through a softmax activation (separately for start and end) to obtain probability distributions. The tokens with the highest start and end probabilities are selected as boundaries of the answer span. We evaluated performance with token-level F1 score, similarly to SQUAD.\nWe trained all the models of Table 2, which scored approx. 10-20% in token-level F1, with Legal-BERT performing slightly better than the rest (+5% F1).20 In the paper that introduced CUAD (Hendrycks et al., 2021), several other measures (Precision@ N% Recall, AUPR, Jaccard similarity) are used to more leniently estimate a model’s ability to approximately locate answers in context paragraphs. Through careful manual inspection of the dataset, we noticed the following points that seem to require more careful consideration.\n• Contractual insights (categories, shown in italics below) include both entity-level (short) answers (e.g., “SERVICE AGREEMENT” for Document Name, and “Imprimis Pharmaceuticals, Inc.” for Parties) and paragraph-level (long) answers (e.g., “If any of the conditions specified in Section 8 shall not have been fulfilled when and as required by this Agreement, or by the Closing\n20F1 is one of the two official SQUAD measures. In the second one, Exact Answer Accuracy, all models scored 0%.\nDate, or waived in writing by Capital Resources, this Agreement and all of Capital Resources obligations hereunder may be canceled [...] except as otherwise provided in Sections 2, 7, 9 and 10 hereof.” for Termination for Convenience). These two different types of answers (short and paragraph-long) seem to require different models and different evaluation measures, unlike how they are treated in the original CUAD paper.\n• Some contractual insights (categories), e.g., Parties, have been annotated with both short (e.g., “Imprimis Pharmaceuticals, Inc.”) and long (e.g., “together, Blackwell and Munksgaard shall be referred to as ‘the Publishers’.”) answers. Annotations of these kind introduce noise during both training and evaluation. For example, it becomes unclear when a short (finer/strict) or a long (loose) annotation should be taken to be the correct one.\n• Annotations may include indirect mentions (e.g., ‘Franchisee’, ‘Service Provider’ for Parties) instead of the actual entities (the company name). In this case (Parties), those mentions are actually party roles in the context of a contract.\n• Annotations may include semi-redacted text (e.g., “ , 1996” for Agreement Date), or even fully redacted text (e.g., “ ” for Parties). This practice may be necessary to hide sensitive information, but for the purposes of a benchmark dataset such cases could have been excluded.\nThe points above, which seem to require revisiting the annotations of CUAD, and the very low F1 scores of all models led us to exclude CUAD from LexGLUE. We also note that there is related work covering similar topics, such as Contract Element Extraction (Chalkidis and Androutsopoulos, 2017), Contractual Obligation Extraction (Chalkidis et al., 2018), and Contractual Provision Classification\n(Tuggener et al., 2020), where models perform much better (in terms of accuracy), relying on simpler (separate) more carefully designed tasks and much bigger datasets. Thus we believe that the points mentioned above, which blur the task definition of CUAD and introduce noise, and the limited (compared to larger datasets) number of annotations strongly affect the performance of the models on CUAD, underestimating their true potential.\nWe also initially considered some very interesting legal Information Retrieval (IR) datasets (Locke and Zuccon, 2018; Chalkidis et al., 2021b) that aim to examine crucial real-life tasks (relevant case law retrieval, regulatory compliance). However, we decided to exclude them from the first version of LexGLUE, because they rely on processing multiple long documents and require more task-specific neural network architectures (e.g., siamese networks), and different evaluation measures. Hence, they would make LexGLUE more complex and a less attractive entry point for newcomers to legal NLP. We do plan, however, to include more demanding tasks in future LexGLUE versions, as the legal NLP community will be growing."
    } ],
    "references" : [ {
      "title" : "Predicting judicial decisions of the european court of human rights: A natural language processing perspective",
      "author" : [ "Nikolaos Aletras", "Dimitrios Tsarapatsanis", "Daniel Preoţiuc-Pietro", "Vasileios Lampos." ],
      "venue" : "PeerJ Computer Science, 2:e93.",
      "citeRegEx" : "Aletras et al\\.,? 2016",
      "shortCiteRegEx" : "Aletras et al\\.",
      "year" : 2016
    }, {
      "title" : "Named entity recognition, linking and generation for greek legislation",
      "author" : [ "I. Angelidis", "Ilias Chalkidis", "M. Koubarakis." ],
      "venue" : "JURIX, Groningen, The Netherlands.",
      "citeRegEx" : "Angelidis et al\\.,? 2018",
      "shortCiteRegEx" : "Angelidis et al\\.",
      "year" : 2018
    }, {
      "title" : "Machine bias: There’s software used across the country to predict future criminals",
      "author" : [ "Julia Angwin", "Jeff Larson", "Surya Mattu", "Lauren Kirchner." ],
      "venue" : "and it’s biased against blacks. ProPublica.",
      "citeRegEx" : "Angwin et al\\.,? 2016",
      "shortCiteRegEx" : "Angwin et al\\.",
      "year" : 2016
    }, {
      "title" : "Legaldb: Long distilbert for legal document classification",
      "author" : [ "Purbid Bambroo", "Aditi Awasthi." ],
      "venue" : "2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT), pages 1–4.",
      "citeRegEx" : "Bambroo and Awasthi.,? 2021",
      "shortCiteRegEx" : "Bambroo and Awasthi.",
      "year" : 2021
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "CoRR, abs/2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "A comparative study of summarization algorithms applied to legal case judgments",
      "author" : [ "Paheli Bhattacharya", "Kaustubh Hiware", "Subham Rajgaria", "Nilay Pochhi", "Kripabandhu Ghosh", "Saptarshi Ghosh." ],
      "venue" : "Advances in Information Retrieval, pages 413–",
      "citeRegEx" : "Bhattacharya et al\\.,? 2019",
      "shortCiteRegEx" : "Bhattacharya et al\\.",
      "year" : 2019
    }, {
      "title" : "Analogies and disanalogies between machine-driven and human-driven legal judgement",
      "author" : [ "Reuben Binns." ],
      "venue" : "Journal of Cross-disciplinary Research in Computational Law, 1(1).",
      "citeRegEx" : "Binns.,? 2020",
      "shortCiteRegEx" : "Binns.",
      "year" : 2020
    }, {
      "title" : "Lexnlp: Natural language processing and information extraction for legal and regulatory texts",
      "author" : [ "Michael J. Bommarito", "Daniel Martin Katz", "Eric M. Detterman." ],
      "venue" : "Research Handbook on Big Data Law, pages 216–227.",
      "citeRegEx" : "Bommarito et al\\.,? 2021",
      "shortCiteRegEx" : "Bommarito et al\\.",
      "year" : 2021
    }, {
      "title" : "On the opportunities and risks of foundation models",
      "author" : [ "Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan You", "Matei Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "What will it take to fix benchmarking in natural language understanding",
      "author" : [ "Samuel R. Bowman", "George Dahl" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Bowman and Dahl.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bowman and Dahl.",
      "year" : 2021
    }, {
      "title" : "Scalable and explainable legal prediction",
      "author" : [ "L Karl Branting", "Craig Pfeifer", "Bradford Brown", "Lisa Ferro", "John Aberdeen", "Brandy Weiss", "Mark Pfaff", "Bill Liao." ],
      "venue" : "Artificial Intelligence and Law, 29(2):213–238.",
      "citeRegEx" : "Branting et al\\.,? 2021",
      "shortCiteRegEx" : "Branting et al\\.",
      "year" : 2021
    }, {
      "title" : "Legal NERC with ontologies, Wikipedia and curriculum learning",
      "author" : [ "Cristian Cardellino", "Milagro Teruel", "Laura Alonso Alemany", "Serena Villata." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Cardellino et al\\.,? 2017",
      "shortCiteRegEx" : "Cardellino et al\\.",
      "year" : 2017
    }, {
      "title" : "A deep learning approach to contract element extraction",
      "author" : [ "Ilias Chalkidis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 30th International Conference on Legal Knowledge and Information Systems (JURIX 2017), Luxembourg City, Luxembourg.",
      "citeRegEx" : "Chalkidis and Androutsopoulos.,? 2017",
      "shortCiteRegEx" : "Chalkidis and Androutsopoulos.",
      "year" : 2017
    }, {
      "title" : "Neural legal judgment prediction in English",
      "author" : [ "Ilias Chalkidis", "Ion Androutsopoulos", "Nikolaos Aletras." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317–4323, Florence, Italy. Association",
      "citeRegEx" : "Chalkidis et al\\.,? 2019a",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2019
    }, {
      "title" : "Obligation and prohibition extraction using hierarchical RNNs",
      "author" : [ "Ilias Chalkidis", "Ion Androutsopoulos", "Achilleas Michos." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
      "citeRegEx" : "Chalkidis et al\\.,? 2018",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2018
    }, {
      "title" : "Large-scale multi-label text classification on EU legislation",
      "author" : [ "Ilias Chalkidis", "Emmanouil Fergadiotis", "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Chalkidis et al\\.,? 2019b",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2019
    }, {
      "title" : "MultiEURLEX - a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in",
      "citeRegEx" : "Chalkidis et al\\.,? 2021a",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2021
    }, {
      "title" : "An empirical study on large-scale multi-label text classification including few and zero-shot labels",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Sotiris Kotitsas", "Prodromos Malakasiotis", "Nikolaos Aletras", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 2020",
      "citeRegEx" : "Chalkidis et al\\.,? 2020a",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2020
    }, {
      "title" : "LEGAL-BERT: The muppets straight out of law school",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Prodromos Malakasiotis", "Nikolaos Aletras", "Ion Androutsopoulos." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898–",
      "citeRegEx" : "Chalkidis et al\\.,? 2020b",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural contract element extraction revisited",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the Document Intelligence Workshop at NeurIPS 2019, Vancouver, Canada.",
      "citeRegEx" : "Chalkidis et al\\.,? 2019c",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2019
    }, {
      "title" : "Regulatory compliance through Doc2Doc information retrieval: A case study in EU/UK legislation",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Nikolaos Manginas", "Eva Katakalou", "Prodromos Malakasiotis" ],
      "venue" : null,
      "citeRegEx" : "Chalkidis et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2021
    }, {
      "title" : "Paragraph-level rationale extraction through regularization: A case study on european court of human rights cases",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Dimitrios Tsarapatsanis", "Nikolaos Aletras", "Ion Androutsopoulos", "Prodromos Malakasiotis." ],
      "venue" : "In",
      "citeRegEx" : "Chalkidis et al\\.,? 2021c",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep learning in law: early adaptation and legal word embeddings trained on large corpora",
      "author" : [ "Ilias Chalkidis", "Dimitrios Kampas." ],
      "venue" : "Artificial Intelligence and Law, 27(2):171–198.",
      "citeRegEx" : "Chalkidis and Kampas.,? 2018",
      "shortCiteRegEx" : "Chalkidis and Kampas.",
      "year" : 2018
    }, {
      "title" : "Joint entity and relation extraction for legal documents with legal feature enhancement",
      "author" : [ "Yanguang Chen", "Yuanyuan Sun", "Zhihao Yang", "Hongfei Lin." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 1561–",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "SentEval: An evaluation toolkit for universal sentence representations",
      "author" : [ "Alexis Conneau", "Douwe Kiela." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), Miyazaki, Japan. European",
      "citeRegEx" : "Conneau and Kiela.,? 2018",
      "shortCiteRegEx" : "Conneau and Kiela.",
      "year" : 2018
    }, {
      "title" : "Supportvector networks",
      "author" : [ "Corinna Cortes", "Vladimir Vapnik." ],
      "venue" : "Machine Learning, pages 273– 297.",
      "citeRegEx" : "Cortes and Vapnik.,? 1995",
      "shortCiteRegEx" : "Cortes and Vapnik.",
      "year" : 1995
    }, {
      "title" : "Measuring law over time: A network analytical framework with an application to statutes and regulations in the United States and Germany",
      "author" : [ "Corinna Coupette", "Janis Beckedorf", "Dirk Hartung", "Michael Bommarito", "Daniel Martin Katz." ],
      "venue" : "Frontiers",
      "citeRegEx" : "Coupette et al\\.,? 2021",
      "shortCiteRegEx" : "Coupette et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The IEEE ICDM 2020 workshops",
      "author" : [ "Giuseppe Di Fatta", "Victor Sheng", "Alfredo Cuzzocrea." ],
      "venue" : "11",
      "citeRegEx" : "Fatta et al\\.,? 2020",
      "shortCiteRegEx" : "Fatta et al\\.",
      "year" : 2020
    }, {
      "title" : "IMIRSEL: a secure music retrieval testing environment",
      "author" : [ "John S. Downie." ],
      "venue" : "Internet Multimedia Management Systems V, volume 5601, pages 91 –",
      "citeRegEx" : "Downie.,? 2004",
      "shortCiteRegEx" : "Downie.",
      "year" : 2004
    }, {
      "title" : "The accuracy, fairness, and limits of predicting recidivism",
      "author" : [ "Julia Dressel", "Hany Farid." ],
      "venue" : "Science Advances, 4(10).",
      "citeRegEx" : "Dressel and Farid.,? 2018",
      "shortCiteRegEx" : "Dressel and Farid.",
      "year" : 2018
    }, {
      "title" : "Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations",
      "author" : [ "Oluwaseyi Feyisetan", "Borja Balle", "Thomas Drake", "Tom Diethe." ],
      "venue" : "Proceedings of the 13th International Conference on Web Search and Data Mining, pages 178–",
      "citeRegEx" : "Feyisetan et al\\.,? 2020",
      "shortCiteRegEx" : "Feyisetan et al\\.",
      "year" : 2020
    }, {
      "title" : "Legal English",
      "author" : [ "Rupert Haigh." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Haigh.,? 2018",
      "shortCiteRegEx" : "Haigh.",
      "year" : 2018
    }, {
      "title" : "Deberta: Decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "CUAD: An expert-annotated NLP dataset for legal contract review",
      "author" : [ "Dan Hendrycks", "Collin Burns", "Anya Chen", "Spencer Ball." ],
      "venue" : "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).",
      "citeRegEx" : "Hendrycks et al\\.,? 2021",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2021
    }, {
      "title" : "An investigation towards differentially private sequence tagging in a federated framework",
      "author" : [ "Abhik Jana", "Chris Biemann." ],
      "venue" : "Proceedings of the Third Workshop on Privacy in Natural Language Processing, pages 30–35.",
      "citeRegEx" : "Jana and Biemann.,? 2021",
      "shortCiteRegEx" : "Jana and Biemann.",
      "year" : 2021
    }, {
      "title" : "Overview of coliee 2017",
      "author" : [ "Yoshinobu Kano", "Mi-Young Kim", "Randy Goebel", "Ken Satoh." ],
      "venue" : "COLIEE@ ICAIL, pages 1–8.",
      "citeRegEx" : "Kano et al\\.,? 2017",
      "shortCiteRegEx" : "Kano et al\\.",
      "year" : 2017
    }, {
      "title" : "Coliee-2018: Evaluation of the competition on legal information extraction and entailment",
      "author" : [ "Yoshinobu Kano", "Mi-Young Kim", "Masaharu Yoshioka", "Yao Lu", "Juliano Rabelo", "Naoki Kiyota", "Randy Goebel", "Ken Satoh." ],
      "venue" : "JSAI International Sym-",
      "citeRegEx" : "Kano et al\\.,? 2018",
      "shortCiteRegEx" : "Kano et al\\.",
      "year" : 2018
    }, {
      "title" : "Scaling laws for neural language models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B. Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei." ],
      "venue" : "CoRR, abs/2001.08361.",
      "citeRegEx" : "Kaplan et al\\.,? 2020",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "A general approach for predicting the behavior of the supreme court of the united states",
      "author" : [ "Daniel Martin Katz", "Michael J Bommarito", "Josh Blackman." ],
      "venue" : "PloS one, 12(4):e0174698.",
      "citeRegEx" : "Katz et al\\.,? 2017",
      "shortCiteRegEx" : "Katz et al\\.",
      "year" : 2017
    }, {
      "title" : "Complex societies and the growth of the law",
      "author" : [ "Daniel Martin Katz", "Corinna Coupette", "Janis Beckedorf", "Dirk Hartung." ],
      "venue" : "Scientific Reports, 10:18737.",
      "citeRegEx" : "Katz et al\\.,? 2020",
      "shortCiteRegEx" : "Katz et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving supreme court forecasting using boosted decision trees",
      "author" : [ "Aaron Russell Kaufman", "Peter Kraft", "Maya Sen." ],
      "venue" : "Political Analysis, 27(3):381–387.",
      "citeRegEx" : "Kaufman et al\\.,? 2019",
      "shortCiteRegEx" : "Kaufman et al\\.",
      "year" : 2019
    }, {
      "title" : "Answering legal questions by learning neural attentive text representation",
      "author" : [ "Phi Manh Kien", "Ha-Thanh Nguyen", "Ngo Xuan Bach", "Vu Tran", "Minh Le Nguyen", "Tu Minh Phuong." ],
      "venue" : "Proceedings of the 28th International Conference on Computational",
      "citeRegEx" : "Kien et al\\.,? 2020",
      "shortCiteRegEx" : "Kien et al\\.",
      "year" : 2020
    }, {
      "title" : "Coliee-2016: evaluation of the competition on legal information extraction and entailment",
      "author" : [ "Mi-Young Kim", "Randy Goebel", "Yoshinobu Kano", "Ken Satoh." ],
      "venue" : "International Workshop on Jurisinformatics (JURISIN 2016).",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "A Convolutional Neural Network in Legal Question Answering",
      "author" : [ "Mi-young Kim", "Ying Xu", "Randy Goebel." ],
      "venue" : "Ninth International Workshop on Jurisinformatics (JURISIN).",
      "citeRegEx" : "Kim et al\\.,? 2015",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba." ],
      "venue" : "Proceedings of the 5th International Conference on Learning Representations (ICLR), San Diego, CA, USA.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Predicting judicial decisions of criminal cases from thai supreme court using bi-directional gru with attention mechanism",
      "author" : [ "Kankawin Kowsrihawat", "Peerapon Vateekul", "Prachya Boonkwan." ],
      "venue" : "2018 5th Asian Conference on Defense Technology",
      "citeRegEx" : "Kowsrihawat et al\\.,? 2018",
      "shortCiteRegEx" : "Kowsrihawat et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-grained named entity recognition in legal documents",
      "author" : [ "Elena Leitner", "Georg Rehm", "Julian MorenoSchneider." ],
      "venue" : "Semantic Systems. The Power of AI and Knowledge Graphs, pages 272–287, Cham. Springer International Publishing.",
      "citeRegEx" : "Leitner et al\\.,? 2019",
      "shortCiteRegEx" : "Leitner et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Datasets: A community library for natural language processing",
      "author" : [ "Lagunas", "Alexander M. Rush", "Thomas Wolf" ],
      "venue" : null,
      "citeRegEx" : "Lagunas et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lagunas et al\\.",
      "year" : 2021
    }, {
      "title" : "CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service",
      "author" : [ "Marco Lippi", "Przemysław Pałka", "Giuseppe Contissa", "Francesca Lagioia", "Hans-Wolfgang Micklitz", "Giovanni Sartor", "Paolo Torroni." ],
      "venue" : "Artificial",
      "citeRegEx" : "Lippi et al\\.,? 2019",
      "shortCiteRegEx" : "Lippi et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A test collection for evaluating legal case law search",
      "author" : [ "Daniel Locke", "Guido Zuccon." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’18, page 1261–1264, New York, NY, USA. Associ-",
      "citeRegEx" : "Locke and Zuccon.,? 2018",
      "shortCiteRegEx" : "Locke and Zuccon.",
      "year" : 2018
    }, {
      "title" : "Learning with noise: Enhance distantly supervised relation extraction with dynamic transition matrix",
      "author" : [ "Bingfeng Luo", "Yansong Feng", "Zheng Wang", "Zhanxing Zhu", "Songfang Huang", "Rui Yan", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of",
      "citeRegEx" : "Luo et al\\.,? 2017",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2017
    }, {
      "title" : "VICTOR: a dataset for Brazilian legal documents classification",
      "author" : [ "Pedro Henrique Luz de Araujo", "Teófilo Emı́dio de Campos", "Fabricio Ataides Braz", "Nilton Correia da Silva" ],
      "venue" : "In Proceedings of the 12th Language Resources and Evaluation",
      "citeRegEx" : "Araujo et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Araujo et al\\.",
      "year" : 2020
    }, {
      "title" : "ILDC for CJPE: indian legal documents corpus for court judgmentprediction and explanation",
      "author" : [ "Vijit Malik", "Rishabh Sanjay", "Shubham Kumar Nigam", "Kripa Ghosh", "Shouvik Kumar Guha", "Arnab Bhattacharya", "Ashutosh Modi." ],
      "venue" : "Joint Confer-",
      "citeRegEx" : "Malik et al\\.,? 2021",
      "shortCiteRegEx" : "Malik et al\\.",
      "year" : 2021
    }, {
      "title" : "The natural language decathlon: Multitask learning as question answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic judgement forecasting for pending applications of the European Court of Human Rights",
      "author" : [ "Masha Medvedeva", "Ahmet Üstun", "Xiao Xu", "Michel Vols", "Martijn Wieling." ],
      "venue" : "Proceedings of the Fifth Workshop on Automated Semantic Analysis",
      "citeRegEx" : "Medvedeva et al\\.,? 2021",
      "shortCiteRegEx" : "Medvedeva et al\\.",
      "year" : 2021
    }, {
      "title" : "Using machine learning to predict decisions",
      "author" : [ "Masha Medvedeva", "Michel Vols", "Martijn Wieling" ],
      "venue" : null,
      "citeRegEx" : "Medvedeva et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Medvedeva et al\\.",
      "year" : 2020
    }, {
      "title" : "An Evaluation of Efficient Multilabel Classification Algorithms for Large-Scale Problems in the Legal Domain",
      "author" : [ "Eneldo Loza Mencia", "Johannes Fürnkranzand." ],
      "venue" : "Proceedings of the 1st Linguistic Annotation Workshop, pages 126–132, Halle,",
      "citeRegEx" : "Mencia and Fürnkranzand.,? 2007",
      "shortCiteRegEx" : "Mencia and Fürnkranzand.",
      "year" : 2007
    }, {
      "title" : "Natural language processing in law: Prediction of outcomes in the higher courts of turkey",
      "author" : [ "Emre Mumcuoğlu", "Ceyhun E Öztürk", "Haldun M Ozaktas", "Aykut Koç." ],
      "venue" : "Information Processing&Management, 58(5):102684.",
      "citeRegEx" : "Mumcuoğlu et al\\.,? 2021",
      "shortCiteRegEx" : "Mumcuoğlu et al\\.",
      "year" : 2021
    }, {
      "title" : "Legal docket classification: Where machine learning stumbles",
      "author" : [ "Ramesh Nallapati", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 438–446, Honolulu, Hawaii. Association for",
      "citeRegEx" : "Nallapati and Manning.,? 2008",
      "shortCiteRegEx" : "Nallapati and Manning.",
      "year" : 2008
    }, {
      "title" : "Swiss-Court-Predict: A Multilingual Legal Judgment Prediction Benchmark",
      "author" : [ "Joel Niklaus", "Ilias Chalkidis", "Matthias Stürmer." ],
      "venue" : "Proceedings of the 3rd Natural Legal Language Processing Workshop Workshop, Online.",
      "citeRegEx" : "Niklaus et al\\.,? 2021",
      "shortCiteRegEx" : "Niklaus et al\\.",
      "year" : 2021
    }, {
      "title" : "How to build a more open justice system",
      "author" : [ "Adam R Pah", "David L Schwartz", "Sarath Sanga", "Zachary D Clopton", "Peter DiCola", "Rachel Davis Mersey", "Charlotte S Alexander", "Kristian J Hammond", "Luı́s A Nunes Amaral" ],
      "venue" : null,
      "citeRegEx" : "Pah et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pah et al\\.",
      "year" : 2020
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
      "author" : [ "Yifan Peng", "Shankai Yan", "Zhiyong Lu." ],
      "venue" : "Proceedings of the 2019 Workshop on Biomedical Natural Language",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Question answering for privacy policies: Combining computational and legal perspectives",
      "author" : [ "Abhilasha Ravichander", "Alan W Black", "Shomir Wilson", "Thomas Norton", "Norman Sadeh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Ravichander et al\\.,? 2019",
      "shortCiteRegEx" : "Ravichander et al\\.",
      "year" : 2019
    }, {
      "title" : "The supreme court forecasting project: Legal and political science approaches to predicting supreme court decisionmaking",
      "author" : [ "Theodore W Ruger", "Pauline T Kim", "Andrew D Martin", "Kevin M Quinn." ],
      "venue" : "Columbia Law Review, pages 1150–1210.",
      "citeRegEx" : "Ruger et al\\.,? 2004",
      "shortCiteRegEx" : "Ruger et al\\.",
      "year" : 2004
    }, {
      "title" : "Friends with motives: Using text to infer influence on SCOTUS",
      "author" : [ "Yanchuan Sim", "Bryan Routledge", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1724–1733, Austin, Texas.",
      "citeRegEx" : "Sim et al\\.,? 2016",
      "shortCiteRegEx" : "Sim et al\\.",
      "year" : 2016
    }, {
      "title" : "Supreme Court Database, Version 2020 Release 01",
      "author" : [ "Harold J. Spaeth", "Lee Epstein", "Jeffrey A. Segal Andrew D. Martin", "Theodore J. Ruger", "Sara C. Benesh." ],
      "venue" : "Washington University Law.",
      "citeRegEx" : "Spaeth et al\\.,? 2020",
      "shortCiteRegEx" : "Spaeth et al\\.",
      "year" : 2020
    }, {
      "title" : "Legal judgement prediction for uk courts",
      "author" : [ "Benjamin Strickson", "Beatriz De La Iglesia." ],
      "venue" : "Proceedings of the 2020 The 3rd International Conference on Information Science and System, pages 204– 209.",
      "citeRegEx" : "Strickson and Iglesia.,? 2020",
      "shortCiteRegEx" : "Strickson and Iglesia.",
      "year" : 2020
    }, {
      "title" : "Legal language",
      "author" : [ "Peter M Tiersma." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Tiersma.,? 1999",
      "shortCiteRegEx" : "Tiersma.",
      "year" : 1999
    }, {
      "title" : "LEDGAR: A large-scale multi-label corpus for text classification of legal provisions in contracts",
      "author" : [ "Don Tuggener", "Pius von Däniken", "Thomas Peetz", "Mark Cieliebak." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages",
      "citeRegEx" : "Tuggener et al\\.,? 2020",
      "shortCiteRegEx" : "Tuggener et al\\.",
      "year" : 2020
    }, {
      "title" : "Design and Implementation of German Legal Decision Corpora",
      "author" : [ "Stefanie Urchs", "Jelena Mitrović", "Michael Granitzer." ],
      "venue" : "Proceedings of the 13th International Conference on Agents and Artificial Intelligence, pages 515–521, Online. SCITEPRESS -",
      "citeRegEx" : "Urchs et al\\.,? 2021",
      "shortCiteRegEx" : "Urchs et al\\.",
      "year" : 2021
    }, {
      "title" : "What about the precedent: An information-theoretic analysis of common law",
      "author" : [ "Josef Valvoda", "Tiago Pimentel", "Niklas Stoehr", "Ryan Cotterell", "Simone Teufel." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Valvoda et al\\.,? 2021",
      "shortCiteRegEx" : "Valvoda et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Predicting decisions",
      "author" : [ "Michael Benedict L Virtucio", "Jeffrey A Aborot", "John Kevin C Abonita", "Roxanne S Avinante", "Rother Jay B Copino", "Michelle P Neverida", "Vanesa O Osiana", "Elmer C Peramo", "Joanna G Syjuco", "Glenn Brian A Tan" ],
      "venue" : null,
      "citeRegEx" : "Virtucio et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Virtucio et al\\.",
      "year" : 2018
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Equality before the law: Legal judgment consistency analysis for fairness",
      "author" : [ "Yuzhong Wang", "Chaojun Xiao", "Shirong Ma", "Haoxi Zhong", "Cunchao Tu", "Tianyang Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Science China - Information Sciences.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Tradition and change in legal English: Verbal constructions in prescriptive texts, volume 20",
      "author" : [ "Christopher Williams." ],
      "venue" : "Peter Lang.",
      "citeRegEx" : "Williams.,? 2007",
      "shortCiteRegEx" : "Williams.",
      "year" : 2007
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Lawformer: A pre-trained language model for chinese legal long documents",
      "author" : [ "Chaojun Xiao", "Xueyu Hu", "Zhiyuan Liu", "Cunchao Tu", "Maosong Sun." ],
      "venue" : "CoRR, abs/2105.03887. 14",
      "citeRegEx" : "Xiao et al\\.,? 2021",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2021
    }, {
      "title" : "Beyond 512 Tokens: Siamese Multi-Depth Transformer-Based Hierarchical Encoder for Long-Form Document Matching, page 1725–1734",
      "author" : [ "Liu Yang", "Mingyang Zhang", "Cheng Li", "Michael Bendersky", "Marc Najork." ],
      "venue" : "Association for Computing Ma-",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Legal judgment prediction via multiperspective bi-feedback network",
      "author" : [ "Wenmian Yang", "Weijia Jia", "Xiaojie Zhou", "Yutao Luo." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 4085–4091.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Interpretable Charge Predictions for Criminal Cases: Learning to Generate Court Views from Fact Descriptions",
      "author" : [ "Hai Ye", "Xin Jiang", "Zhunchen Luo", "Wenhan Chao." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Ye et al\\.,? 2018",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2018
    }, {
      "title" : "Big Bird: Transformers for longer sequences",
      "author" : [ "Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontanon", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed." ],
      "venue" : "Advances in Neu-",
      "citeRegEx" : "Zaheer et al\\.,? 2020",
      "shortCiteRegEx" : "Zaheer et al\\.",
      "year" : 2020
    }, {
      "title" : "When does pretraining help? assessing self-supervised learning for law and the casehold dataset",
      "author" : [ "Lucia Zheng", "Neel Guha", "Brandon R. Anderson", "Peter Henderson", "Daniel E. Ho." ],
      "venue" : "Proceedings of the 18th International Conference on Artificial In-",
      "citeRegEx" : "Zheng et al\\.,? 2021",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Legal judgment prediction via topological learning",
      "author" : [ "Haoxi Zhong", "Zhipeng Guo", "Cunchao Tu", "Chaojun Xiao", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Zhong et al\\.,? 2018",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2018
    }, {
      "title" : "Iteratively questioning and answering for interpretable legal judgment prediction",
      "author" : [ "Haoxi Zhong", "Yuzhong Wang", "Cunchao Tu", "Tianyang Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, pages",
      "citeRegEx" : "Zhong et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "How does nlp benefit legal system: A summary of legal artificial intelligence",
      "author" : [ "Haoxi Zhong", "Chaojun Xiao", "Cunchao Tu", "Tianyang Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Zhong et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "JECQA: A legal-domain question answering dataset",
      "author" : [ "Haoxi Zhong", "Chaojun Xiao", "Cunchao Tu", "Tianyang Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "In",
      "citeRegEx" : "Zhong et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "2020) proposed SMITH, a hierarchical Transformer model that hierarchically encodes increasingly larger blocks",
      "author" : [ "Yang" ],
      "venue" : null,
      "citeRegEx" : "Yang,? \\Q2020\\E",
      "shortCiteRegEx" : "Yang",
      "year" : 2020
    }, {
      "title" : "2021) introduced language models pre-trained on legal corpora, but of relatively small sizes, i.e., 12–36 GB",
      "author" : [ "Bambroo", "Awasthi", "Xiao" ],
      "venue" : "In the work of Zheng et al",
      "citeRegEx" : "Bambroo et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bambroo et al\\.",
      "year" : 2021
    }, {
      "title" : "fp16) to decrease the memory footprint in training. Using fp16 in attention operation resulted in floating point overflow and NaNs in later stages of training",
      "author" : [ "Beltagy" ],
      "venue" : null,
      "citeRegEx" : "Beltagy,? \\Q2020\\E",
      "shortCiteRegEx" : "Beltagy",
      "year" : 2020
    }, {
      "title" : "alleviate the imbalance between short and long answers. We then re-split the dataset chronologically into training (5.2k, 1994–2019), development",
      "author" : [ "Devlin" ],
      "venue" : null,
      "citeRegEx" : "Devlin,? \\Q2020\\E",
      "shortCiteRegEx" : "Devlin",
      "year" : 2020
    }, {
      "title" : "2021), for each training (or test) instance, we consider pairs that consist of a question and a paragraph, separated by the special delimiter token [sep",
      "author" : [ "Hendrycks" ],
      "venue" : null,
      "citeRegEx" : "Hendrycks,? \\Q2021\\E",
      "shortCiteRegEx" : "Hendrycks",
      "year" : 2021
    }, {
      "title" : "2020), where models perform much better (in terms of accuracy), relying on simpler (separate) more carefully designed tasks and much bigger datasets",
      "author" : [ "Tuggener" ],
      "venue" : null,
      "citeRegEx" : "Tuggener,? \\Q2020\\E",
      "shortCiteRegEx" : "Tuggener",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 40,
      "context" : "Virtually every modern legal system produces massive volumes of textual data (Katz et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "Lawyers, judges, and regulators continuously author legal documents such as briefs, memos, statutes, regulations, contracts, patents and judicial decisions (Coupette et al., 2021).",
      "startOffset" : 156,
      "endOffset" : 179
    }, {
      "referenceID" : 23,
      "context" : ", 2021), information extraction from legal documents (Chalkidis et al., 2018, 2019c; Chen et al., 2020; Hendrycks et al., 2021) and case summarization (Bhattacharya et al.",
      "startOffset" : 53,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : ", 2021), information extraction from legal documents (Chalkidis et al., 2018, 2019c; Chen et al., 2020; Hendrycks et al., 2021) and case summarization (Bhattacharya et al.",
      "startOffset" : 53,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : ", 2021) and case summarization (Bhattacharya et al., 2019) to legal question answering (Ravichander et al.",
      "startOffset" : 31,
      "endOffset" : 58
    }, {
      "referenceID" : 62,
      "context" : ", 2020a,c) and text classification (Nallapati and Manning, 2008; Chalkidis et al., 2019b, 2020a).",
      "startOffset" : 35,
      "endOffset" : 96
    }, {
      "referenceID" : 78,
      "context" : "Transformer models (Vaswani et al., 2017) pre-trained on legal, rather than generic, corpora have also been studied (Chalkidis et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : ", 2017) pre-trained on legal, rather than generic, corpora have also been studied (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021).",
      "startOffset" : 82,
      "endOffset" : 146
    }, {
      "referenceID" : 91,
      "context" : ", 2017) pre-trained on legal, rather than generic, corpora have also been studied (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021).",
      "startOffset" : 82,
      "endOffset" : 146
    }, {
      "referenceID" : 86,
      "context" : ", 2017) pre-trained on legal, rather than generic, corpora have also been studied (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021).",
      "startOffset" : 82,
      "endOffset" : 146
    }, {
      "referenceID" : 27,
      "context" : "Pre-trained Transformers, including BERT (Devlin et al., 2019), GPT-3 (Brown et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 33,
      "context" : ", 2020), DeBERTa (He et al., 2021) and numerous variants, are currently the state of the art in most natural language processing (NLP) tasks.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 80,
      "context" : ", 2018, 2019b) are considered almost ‘solved’ a few years after their release and need to be made more challenging (Wang et al., 2019a).",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 74,
      "context" : "that legal language is often classified as a ‘sublanguage’ (Tiersma, 1999; Williams, 2007; Haigh, 2018).",
      "startOffset" : 59,
      "endOffset" : 103
    }, {
      "referenceID" : 84,
      "context" : "that legal language is often classified as a ‘sublanguage’ (Tiersma, 1999; Williams, 2007; Haigh, 2018).",
      "startOffset" : 59,
      "endOffset" : 103
    }, {
      "referenceID" : 32,
      "context" : "that legal language is often classified as a ‘sublanguage’ (Tiersma, 1999; Williams, 2007; Haigh, 2018).",
      "startOffset" : 59,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "those designed to handle long text (Beltagy et al., 2020; Zaheer et al., 2020; Yang et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 97
    }, {
      "referenceID" : 90,
      "context" : "those designed to handle long text (Beltagy et al., 2020; Zaheer et al., 2020; Yang et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 97
    }, {
      "referenceID" : 87,
      "context" : "those designed to handle long text (Beltagy et al., 2020; Zaheer et al., 2020; Yang et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 97
    }, {
      "referenceID" : 80,
      "context" : ", 2018, 2019b), the subsequent more difficult SuperGLUE (Wang et al., 2019a), other previous multi-task NLP benchmarks (Conneau and Kiela, 2018; McCann et al.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : ", 2019a), other previous multi-task NLP benchmarks (Conneau and Kiela, 2018; McCann et al., 2018), and similar initiatives in other domains (Peng et al.",
      "startOffset" : 51,
      "endOffset" : 97
    }, {
      "referenceID" : 57,
      "context" : ", 2019a), other previous multi-task NLP benchmarks (Conneau and Kiela, 2018; McCann et al., 2018), and similar initiatives in other domains (Peng et al.",
      "startOffset" : 51,
      "endOffset" : 97
    }, {
      "referenceID" : 66,
      "context" : ", 2018), and similar initiatives in other domains (Peng et al., 2019), we introduce LexGLUE, a benchmark dataset to evaluate the performance of NLP methods in legal tasks.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "1 As more legal NLP datasets become available, we also plan to favor datasets checked thoroughly for validity (scores reflecting real-life performance), annotation quality, statistical power, and social bias (Bowman and Dahl, 2021).",
      "startOffset" : 208,
      "endOffset" : 231
    }, {
      "referenceID" : 54,
      "context" : "The rapid growth of the legal text processing field is demonstrated by numerous papers presented in top-tier conferences in NLP and artificial intelligence (Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019a; Valvoda et al., 2021) as well as surveys (Chalkidis and Kampas, 2018; Zhong et al.",
      "startOffset" : 156,
      "endOffset" : 241
    }, {
      "referenceID" : 92,
      "context" : "The rapid growth of the legal text processing field is demonstrated by numerous papers presented in top-tier conferences in NLP and artificial intelligence (Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019a; Valvoda et al., 2021) as well as surveys (Chalkidis and Kampas, 2018; Zhong et al.",
      "startOffset" : 156,
      "endOffset" : 241
    }, {
      "referenceID" : 13,
      "context" : "The rapid growth of the legal text processing field is demonstrated by numerous papers presented in top-tier conferences in NLP and artificial intelligence (Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019a; Valvoda et al., 2021) as well as surveys (Chalkidis and Kampas, 2018; Zhong et al.",
      "startOffset" : 156,
      "endOffset" : 241
    }, {
      "referenceID" : 77,
      "context" : "The rapid growth of the legal text processing field is demonstrated by numerous papers presented in top-tier conferences in NLP and artificial intelligence (Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019a; Valvoda et al., 2021) as well as surveys (Chalkidis and Kampas, 2018; Zhong et al.",
      "startOffset" : 156,
      "endOffset" : 241
    }, {
      "referenceID" : 0,
      "context" : "The first one (Aletras et al., 2016; Chalkidis et al., 2019a; Medvedeva et al., 2020, 2021) predicts violations of human rights in cases of the",
      "startOffset" : 14,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "The first one (Aletras et al., 2016; Chalkidis et al., 2019a; Medvedeva et al., 2020, 2021) predicts violations of human rights in cases of the",
      "startOffset" : 14,
      "endOffset" : 91
    }, {
      "referenceID" : 70,
      "context" : "The third line of work (Ruger et al., 2004; Katz et al., 2017; Kaufman et al., 2019) includes meth-",
      "startOffset" : 23,
      "endOffset" : 84
    }, {
      "referenceID" : 39,
      "context" : "The third line of work (Ruger et al., 2004; Katz et al., 2017; Kaufman et al., 2019) includes meth-",
      "startOffset" : 23,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "The third line of work (Ruger et al., 2004; Katz et al., 2017; Kaufman et al., 2019) includes meth-",
      "startOffset" : 23,
      "endOffset" : 84
    }, {
      "referenceID" : 79,
      "context" : ", 2017), Philippines (Virtucio et al., 2018), Turkey (Mumcuoğlu et al.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 61,
      "context" : ", 2018), Turkey (Mumcuoğlu et al., 2021), Thailand (Kowsrihawat et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 47,
      "context" : ", 2021), Thailand (Kowsrihawat et al., 2018), United Kingdom (Strickson and De La Iglesia, 2020), Germany (Urchs et al.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 76,
      "context" : ", 2018), United Kingdom (Strickson and De La Iglesia, 2020), Germany (Urchs et al., 2021), and Switzerland (Niklaus et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 51,
      "context" : "There are similar interesting applications in contract law (Lippi et al., 2019; Tuggener et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 75,
      "context" : "There are similar interesting applications in contract law (Lippi et al., 2019; Tuggener et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 34,
      "context" : "Several studies (Chalkidis et al., 2018, 2019c; Hendrycks et al., 2021) explored information extraction from contracts, to extract important information such as the contracting parties, agreed payment amount, start and end dates, applicable law, etc.",
      "startOffset" : 16,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "Other studies focus on extracting information from legislation (Cardellino et al., 2017; Angelidis et al., 2018) or court cases (Leitner et al.",
      "startOffset" : 63,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "Other studies focus on extracting information from legislation (Cardellino et al., 2017; Angelidis et al., 2018) or court cases (Leitner et al.",
      "startOffset" : 63,
      "endOffset" : 112
    }, {
      "referenceID" : 43,
      "context" : "Information Extraction and Entailment (COLIEE) (Kim et al., 2016; Kano et al., 2017, 2018).",
      "startOffset" : 47,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "More recently, there have also been efforts to pre-train Transformer-based language models on legal corpora (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021), leading to state-of-theart results in several legal NLP tasks, compared to models pre-trained on generic corpora.",
      "startOffset" : 108,
      "endOffset" : 172
    }, {
      "referenceID" : 91,
      "context" : "More recently, there have also been efforts to pre-train Transformer-based language models on legal corpora (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021), leading to state-of-theart results in several legal NLP tasks, compared to models pre-trained on generic corpora.",
      "startOffset" : 108,
      "endOffset" : 172
    }, {
      "referenceID" : 86,
      "context" : "More recently, there have also been efforts to pre-train Transformer-based language models on legal corpora (Chalkidis et al., 2020b; Zheng et al., 2021; Xiao et al., 2021), leading to state-of-theart results in several legal NLP tasks, compared to models pre-trained on generic corpora.",
      "startOffset" : 108,
      "endOffset" : 172
    }, {
      "referenceID" : 80,
      "context" : "Unlike SuperGLUE (Wang et al., 2019a), we did not rule out, but",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "In the most recent version of the dataset (Chalkidis et al., 2021c), each case is also mapped to articles of ECHR that were allegedly violated (considered by the court).",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 72,
      "context" : "We combine information from SCOTUS opinions5 with the Supreme Court DataBase (SCDB)6 (Spaeth et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 27,
      "context" : "BERT (Devlin et al., 2019) 110M 32K 512 1M / 256 (16GB) Wiki, BC RoBERTa (Liu et al.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 52,
      "context" : ", 2019) 110M 32K 512 1M / 256 (16GB) Wiki, BC RoBERTa (Liu et al., 2019) 125M 50K 512 100K / 8K (160GB) Wiki, BC, CC-News, OWT DeBERTa (He et al.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 33,
      "context" : ", 2019) 125M 50K 512 100K / 8K (160GB) Wiki, BC, CC-News, OWT DeBERTa (He et al., 2021) 139M 50K 512 1M / 256 (160GB) Wiki, BC, CC-News, OWT Longformer* (Beltagy et al.",
      "startOffset" : 70,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : ", 2021) 139M 50K 512 1M / 256 (160GB) Wiki, BC, CC-News, OWT Longformer* (Beltagy et al., 2020) 149M 50K 4096 65K / 64 (160GB) Wiki, BC, CC-News, OWT BigBird* (Zaheer et al.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 90,
      "context" : ", 2020) 149M 50K 4096 65K / 64 (160GB) Wiki, BC, CC-News, OWT BigBird* (Zaheer et al., 2020) 127M 50K 4096 1M / 256 (160GB) Wiki, BC, CC-News, OWT",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 18,
      "context" : "Legal-BERT (Chalkidis et al., 2020b) 110M 32K 512 1M /256 (12GB) Legislation, Court Cases, Contracts CaseLaw-BERT (Zheng et al.",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 91,
      "context" : ", 2020b) 110M 32K 512 1M /256 (12GB) Legislation, Court Cases, Contracts CaseLaw-BERT (Zheng et al., 2021) 110M 32K 512 2M /256 (37GB) US Court Cases",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "We use the 100 most frequent concepts from level 2, which has a highly skewed label distribution and temporal concept drift (Chalkidis et al., 2021a), making it sufficiently difficult for an entry point.",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 51,
      "context" : "UNFAIR-ToS The UNFAIR-ToS dataset (Lippi et al., 2019) contains 50 Terms of Service (ToS) from on-line platforms (e.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 91,
      "context" : "CaseHOLD The CaseHOLD (Case Holdings on Legal Decisions) dataset (Zheng et al., 2021) con-",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "Our first baseline model is a linear Support Vector Machine (SVM) (Cortes and Vapnik, 1995) with TF-IDF features for the top-K frequent n-grams of the training set, where n ∈ [1, 2, 3].",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 78,
      "context" : "We experiment with Transformer-based (Vaswani et al., 2017) pre-trained language models, which achieve state of the art performance in most NLP tasks (Bommasani et al.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "BERT (Devlin et al., 2019) is the best-known pretrained Transformer-based language model.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 52,
      "context" : "RoBERTa (Liu et al., 2019) is also a pre-trained Transformer-based language model.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 33,
      "context" : "DeBERTa (He et al., 2021) is another improved BERT model that uses disentangled attention, i.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 33,
      "context" : "DeBERTa has been reported to outperform BERT and RoBERTa in several NLP tasks (He et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "Longformer (Beltagy et al., 2020) extends Transformer-based models to support longer sequences, using sparse-attention.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 90,
      "context" : "BigBird (Zaheer et al., 2020) is another sparseattention based transformer that uses a combination of a local (window-based) attention, global (dilated), and random attention, i.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "Legal-BERT (Chalkidis et al., 2020b) is a BERT model pre-trained on English legal corpora, consisting of legislation, contracts, and court cases.",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 65,
      "context" : "implementation of Scikit-learn (Pedregosa et al., 2011) and grid-search for hyper parameters (number of features, C, and loss function).",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 45,
      "context" : "We train models with the Adam optimizer (Kingma and Ba, 2015) and an initial learning rate of 3e-5 up to 20 epochs using early stopping on de-",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : ", using privacy-aware infrastructure and models (Downie, 2004; Feyisetan et al., 2020) is a challenge to be addressed.",
      "startOffset" : 48,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : ", using privacy-aware infrastructure and models (Downie, 2004; Feyisetan et al., 2020) is a challenge to be addressed.",
      "startOffset" : 48,
      "endOffset" : 86
    }, {
      "referenceID" : 64,
      "context" : "protection concerns, which collectively result in these otherwise public decisions not being publicly available (Pah et al., 2020).",
      "startOffset" : 112,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "These include multi-disciplinary research to better understand the limitations and challenges of applying NLP to law (Binns, 2020), while also considering fairness (Angwin et al.",
      "startOffset" : 117,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "These include multi-disciplinary research to better understand the limitations and challenges of applying NLP to law (Binns, 2020), while also considering fairness (Angwin et al., 2016; Dressel and Farid, 2018), and robustness (Wang et al.",
      "startOffset" : 164,
      "endOffset" : 210
    }, {
      "referenceID" : 30,
      "context" : "These include multi-disciplinary research to better understand the limitations and challenges of applying NLP to law (Binns, 2020), while also considering fairness (Angwin et al., 2016; Dressel and Farid, 2018), and robustness (Wang et al.",
      "startOffset" : 164,
      "endOffset" : 210
    }, {
      "referenceID" : 83,
      "context" : ", 2016; Dressel and Farid, 2018), and robustness (Wang et al., 2021).",
      "startOffset" : 49,
      "endOffset" : 68
    } ],
    "year" : 0,
    "abstractText" : "Law, interpretations of law, legal arguments, agreements, etc. are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.",
    "creator" : null
  }
}
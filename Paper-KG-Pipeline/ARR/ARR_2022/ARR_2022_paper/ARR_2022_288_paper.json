{
  "name" : "ARR_2022_288_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The study of political violence is a central concern of conflict scholars and security analysts in the academic and policy communities. For decades, scholars and governments devoted incalculable resources to monitor, understand, and predict the dynamics of social unrest, political violence, and armed conflict around the world. Conflict research is a sub-field of political science analyzing a broad scope of interactions between government agents, their challengers, and the civilian population that include material and verbal conflict and cooperation. Conflict research covers topics such as protest, riots, repression, insurgency, civil war, terrorism, human rights, genocide, criminal violence, forced displacement, conventional and unconventional warfare, nuclear deterrence, peacekeeping, diplomatic disputes and cooperation, among others.\n1https://drive.google.com/drive/ folders/1RedAHj6pTDtlfOeP8rF9J07YAOS20Vi_\nTraditionally, researchers used manual coding to track conflict processes around the world. Unfortunately, the high costs and slow pace of domain experts conducting these tasks make it infeasible to monitor highly complex and rapidly changing conflicts in an ever growing volume of information available at a global scale. Initial efforts to address this challenge motivated political scientists to develop automated systems to classify or extract structured event data from news articles (Bond et al., 2003; Boschee et al., 2016; O’Brien, 2010; Osorio and Reyes, 2017; Schrodt, 2006, 2009; Beieler and Norris, 2014; Norris et al., 2017; Lu and Roy, 2017; Ward et al., 2013). However, these existing models rely on dated pattern matching techniques and large dictionaries, which often yield low-accuracy results and are too costly to maintain.\nRecent efforts by political scientists resort to traditional machine learning (Hanna, 2017; Osorio et al., 2020) and deep learning (Beieler, 2016; Radford, 2019; Glavaš et al., 2017; Skorupa Parolin et al., 2020) to analyze political conflict and violence. However, standard supervised learning requires labeled data, which are expensive to obtain due to the expertise required for quality annotation. This lead conflict scholars to seek for alternative solutions based on the latest natural language processing (NLP) developments.\nRecent progress in NLP has been driven by pre-trained transformer language models (Vaswani et al., 2017; Radford et al., 2019; Devlin et al., 2018; Yang et al., 2019). Self-supervision using large-scale unlabeled text can significantly alleviate the annotation bottleneck using transfer learning. The training parallelization of transformers also improves training efficiency on large datasets. As a result, the use of powerful computational devices and the advantage of transformer structures make large-scale language models’ pre-training possible. Furthermore, the introduction of extensive benchmarks (Wang et al., 2018, 2019; Rajpurkar\net al., 2018; Lai et al., 2017) validates the significant improvement of pre-trained language models on various downstream tasks.\nWhile most pre-trained language models are built on general domain corpora, such as Wikipedia, BookCorpus (Zhu et al., 2015), and WebText (Radford et al., 2019), recent works show that pretraining on domain-specific corpora can boost downstream performance on those domains (Lee et al., 2019; Gururangan et al., 2020). Domainspecific work in bio-medicine focuses not only on developing pre-trained models (Lee et al., 2019; Beltagy et al., 2019; Alsentzer et al., 2019; Lewis et al., 2020; Gu et al., 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al., 2019; Gu et al., 2020). Pre-training models have also advanced research in other domains such as computer science papers (Beltagy et al., 2019) and legal studies (Chalkidis et al., 2020). Despite some efforts to apply transformers-based approaches in political science (Büyüköz et al., 2020; Olsson et al., 2020; Örs et al., 2020; Radford, 2020; Halterman and Radford, 2021; Hürriyetoğlu et al., 2021), there are few studies introducing domainspecific pre-trained language models on either political science or conflict research.\nBy combining the expertise of conflict scholars and computer scientists, we developed ConfliBERT, a pre-trained language model especially designed for political conflict and violence. ConfliBERT is expected to improve downstream tasks for conflict research while significantly alleviating the annotation bottleneck. We expect ConfliBERT to support a broad community of academic and policy researchers around the world by enabling the analysis of conflict processes using a domain-specific NLP tool that yields accurate and valid results at minimum operational cost. Our paper provides the following key contributions:\n(1) We curate a large domain-specific corpus for language modeling in the domain of political violence and conflict. (2) Based on our domainspecific corpora, we devise a pre-trained language model, ConfliBERT, and make it available to the general public, which directly benefits the political science and policy communities. (3) To evaluate our model in practical applications, we collect 13 datasets and conduct 18 tasks relevant to conflict research. We are the first to carry out such a comprehensive evaluation of language models for conflict studies. (4) We evaluate different versions of Con-\nfliBERT and show the outperformance over models trained on generic domains. We also perform indepth analyses of different tasks to investigate the factors affecting the performance."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Recent pre-trained transformer language models, such as the Bidirectional Encoder Representation from Transformers (BERT) (Devlin et al., 2018), follow an easy two-steps framework: (1) pre-train on a large unlabeled corpus; and (2) fine-tune on task-specific labeled data. These models learn semantics during the pre-training step and require smaller labeled data to significantly improve their performance on downstream tasks. The fine-tuning step requires minor network modifications to create state-of-the-art models for various tasks.\nTechnically, BERT uses the multi-layer, multihead self-attention mechanism, which provides substantial advantages for language modeling, such as allowing parallel GPU computation and capturing long-range dependencies. This allows to efficiently pre-train large language models on large corpora using powerful devices.\nAnother key element of BERT-like models is the design of self-supervision tasks. Self-supervision refers to generating labels for unlabelled data and using them to train an unsupervised dataset in a supervised manner. BERT uses two self-supervision tasks during pre-training. On one hand, masked language model (MLM) is a fill-in-the-blank task based on randomly masking a token and then using the surrounding words to predict the word hidden behind the mask. On the other hand, next sentence prediction (NSP) determines whether one sentence follows another one in the same document.\nRecent works also propose variants of selfsupervision tasks. For example, Joshi et al. (2019) mask out contiguous sequences of tokens to improve span representations. Clark et al. (2020) use replaced token detection, where the model distinguishes real input tokens from plausible but synthetically generated replacements. However, Liu et al. (2019) prove that MLM is competitive with other recently proposed training objectives with more data and improved training strategies.\nFinally, most BERT-like models focus on generic domain, such as Wikipedia, BookCorpus (Zhu et al., 2015) and WebText (Radford et al., 2019). However, BERT without domain adaptation tend to under-perform in the target domains with distinct\ncharacteristics (such as specialized vocabulary, language style, and specific semantics). This performance gap is the primary motivation for developing domain-specific language models.\nTable 1 summarizes various recent domainspecific BERT models, including our model, ConfliBERT. These models mainly differ in their corpora and pre-training strategies, including: (1) continuing pre-training (Cont); and (2) pre-training from scratch (SCR). In the next section, we elaborate on the strategies and our method of developing ConfliBERT in the domain of political conflict and violence."
    }, {
      "heading" : "3 Approach",
      "text" : "As described in Section 2, MLM-based BERT achieves competitive performance among other transformer models with different self-supervision tasks. Besides, BERT has been validated in various domains (Lee et al., 2019; Beltagy et al., 2019; Peng et al., 2019; Chalkidis et al., 2020; Gu et al., 2020) shown in Table 1. Therefore, we develop our domain-specific model based on BERT. The key components of developing and validating our model, ConfliBERT, include pre-training strategies, corpora, and evaluation tasks."
    }, {
      "heading" : "3.1 Domain-specific Pretraining",
      "text" : "We explore both strategies (SCR and Cont) of adapting BERT to the political conflict and violence domain. A Cont model initializes with BERT’s checkpoint and vocabulary, and trains for additional steps on a domain-specific corpus. Since BERT has already been pre-trained about one million steps on the generic domain, Cont usually requires fewer steps than training a new model from scratch. For example, Lee et al. (2019) report that continual pre-training of BERT on a biomedical dataset for 470K steps yields a comparable perfor-\nmance to pre-training for one million steps. On the other hand, when pre-training BERT from scratch (SCR) on the domain-specific corpora, we generate a new vocabulary from the target domain instead of using the original BERT’s vocabulary. Various papers (Beltagy et al., 2019; Gu et al., 2020) argue that SCR generates substantial gains over Cont for domains with sizeable unlabeled text.\nWe refer to the original BERT vocabulary as BaseVocab and our domain vocabulary as ConfliVocab. We generated both cased and uncased versions of ConfliVocab on our training corpus using the Wordpiece algorithm (Wu et al., 2016). We set the ConfliVocab size to 30,000 words to match that of BaseVocab. The resulting token overlap between BaseVocab and ConfliVocab is 58.3%, which indicates a considerable difference (41.7%) in high-frequency words between the general and conflict-specific corpora.\nIn particular, we find a substantial advantage of using ConfliVocab during the tokenization. Table 2 shows examples of conflict-related terms that exclusively appear in ConfliVocab. For example, the term \"separatists\" is not included in BaseVocab, and BERT erroneously splits it into four sub-words [\"se\", \"##par\", \"##ati\", \"##sts\"]. This fragmentation may hinder learning in downstream tasks. We will validate the advantage of ConfliVocab in the downstream tasks in our experiments section."
    }, {
      "heading" : "3.2 Corpora",
      "text" : "The first step to develop ConfliBERT is to build a domain-specific corpus for pre-training. As illustrated in Table 1, there exist large-scale publicly available biomedical datasets, such as PubMed and MIMIC (Johnson et al., 2016). SciBERT (Belt-\nagy et al., 2019) is built from a large corpus of academic papers (Ammar et al., 2018; Lo et al., 2020); nonetheless, there is no corpus available which is specific to the politics and conflict domain. Thus we curated a large and diverse domain corpus which consists of 33.7 GB of clean plain text in the BERT required format. We summarize the sources into five categories below and provide more details in Appendix.\nExpert Domain Corpora. We curated 2,293 MB of plain text from multiple professional sources relevant to conflict research. These sources include United Nations’ websites and databases, international humanitarian non-governmental organizations, think tanks and government departments.\nMainstream Media Collection (MMC). We crawled 35 worldwide news agencies reporting in English and carefully pre-processed and filtered 20 GB of politics and conflict news stories from 1966 to 2021 based on the metadata information.\nGigaword. This corpus provides a distinct coverage of seven international English newswires from 1994 to 2010 (Parker et al., 2011). We removed the overlapping stories (which also existed in MMC) and filtered an 8,818 MB domain-specific subset.\nPhoenix Real-Time (PRT). PRT is a developing event dataset crawled from more than 400 news agencies worldwide from October 2017 (Salam et al., 2018). It contains many news agencies in areas other than Europe and the U.S., thus improving the scope of our coverage. We removed the duplicated news agencies (which also existed in MMC and Gigaword) and filtered a 2,425 MB relevant subset.\nWikipedia. Wikipedia has a different language style of describing political events and can enrich the diversity of our corpus. We queried2 and curated a 2,845 MB size of relevant articles from an 18 GB size of the Wikipedia dump3 released on March 20, 2021."
    }, {
      "heading" : "3.3 Evaluation Tasks",
      "text" : "The introduction of comprehensive benchmarks accelerated the development of pre-trained language models in the general NLP domain (Wang et al., 2018, 2019; Rajpurkar et al., 2018; Lai et al., 2017) and biomedical applications (Peng et al., 2019; Gu\n2https://petscan.wmflabs.org 3https://dumps.wikimedia.org\net al., 2020). However, there are few comprehensive benchmarks for evaluating language models on the political conflict and violence domain. The focus of political science professionals is different from that of general NLP researchers. For example, they are more interested in classifying, tracking and predicting conflict events from the text.\nTo conduct a comprehensive evaluation of ConfliBERT, we collected a broad range of NLP tasks related to political conflict and violence from both publicly-available and our newly-annotated datasets. Table 4 shows the datasets and their corresponding tasks. Some datasets may contain subsets and are related to various tasks. The table also lists the numbers of examples in the training, development, and test datasets as well as the evaluation metrics used for each task. In particular, we use F1 scores as performance metrics for binary classification tasks. We use example-based F1 metrics for multi-label classification tasks (Sorower, 2010). For all the other tasks, we rely on Macro F1 to assess the model’s performance. Next, we describe the datasets and their tasks.\nBinary classification (BC). We collected BBC News (Greene and Cunningham, 2006) and 20 Newsgroups (Lang, 1995) for identifying political news, a subset from Gun Violence Database (Pavlick et al., 2016) for finding articles related to gun violence. We also used the samples from Global Contention Politics Dataset (GLOCON) (Hürriyetoğlu et al., 2019) to conduct one sentence-level and one document-level classification task of predicting whether the story is related to protests. These BC tasks are essential for political scientists as a first step to classify and filter documents containing political and conflict events from large-scale news wires.\nMulti-class classification (MCC). GTD refers to Global Terrorism Database which collects terrorist incidents from 1970 onward (START, 2019). We sampled a subset that had description text longer than 40 words to classify 9 types of attacks such as bombing/explosion, armed assault, hostage taking, among others.\nIndia Police Events (Halterman et al., 2021) consists of sentences from English-language Times of India articles about police activity events in Gujarat during March 2002 (a relevant period due to widespread Hindu-Muslim violence). The labels are available for both document and sentence levels\nand consider five categories of police activity: kill, arrest, fail to act, force, and any action.\nEvent Status includes English news articles about civil unrest events annotated with temporal tags (Huang et al., 2016). Following the original setting, we conduct a temporal status classification (TS MCC) to detect the primary temporal distinctions among past, ongoing, and future. Besides, we also build a BC task of predicting if the story contains civil unrest events.\nMulti-label classification (MLC). SATP is the South Asia Terrorism Portal4 from which we manually annotated a sample of 7,445 narratives between 2011 and 2019. We focus on incidents initiated by terrorist organizations. 23.6% of the sample are relevant stories classified into one or more categories: armed assault, bombing/explosion, kidnapping, and others. The residual 76.4% of the samples are irrelevant (stories not about terrorism attacks such as arrests or armed clashes). Based upon this, we built three tasks. The first is a BC task to find relevant stories. The second is an MLC task to predict attack types on the relevant subset (Rel MLC). The third is the same as the second but conducted on the more imbalanced full dataset (All MLC).\nInSight Crime (Parolin et al., 2021) contains annotated stories about organized criminal activity in Latin America and the Caribbean from InSight Crime.5 We applied an MLC task to predict multiple crime categories expressed in the stories, such as drug trafficking, corruption, law enforcement, among others.\nSequence Labeling or Named Entity Recognition (NER). MUC-4 consists of documents reporting terrorism events, annotated with entities such as Perpetrator Individuals, Perpetrator Organizations, Physical targets, Victims, and Weapons (MUC-4, 1992). We split the dataset following Du and Cardie (2020).\nRe3d stands for Relationship and Entity Extraction Evaluation Dataset (DSTL, 2018), comprising task-specific documents focused on the topic of the conflict in Syria and Iraq. The data contains annotations in spans format with their corresponding entity types: Organization, Weapon, Military platform, Person, among others.\nCAMEO (Conflict and Mediation Event 4https://satp.org 5https://insightcrime.org\nObservations) is the industry standard for event extraction in political science (Gerner et al., 2002). An event classification, known as pentacode, consists of five event types: 0-Make a Statement, 1- Verbal Cooperation, 2-Material Cooperation, 3- Verbal Conflict, and 4-Material Conflict, and spans of texts containing sources (who conducted the action) and targets (to whom the action was conducted). We formulated two tasks for CAMEO event extraction on our newly-annotated dataset: sources and targets labeling (ST NER), and pentacode classification (PC MCC)."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Pre-training Setup",
      "text" : "We implemented ConfliBERT using two methods, Cont and SCR. Each approach has an uncased and a cased version. The architecture is the same as BERT-Base with 12 layers, 768 hidden units, 12 attention heads, and 110M parameters in total. Specifically, for our Cont models, we ran additional pre-training steps of the released checkpoints of BERT-Base models on our domain-specific corpus. The vocabulary is the same as the original BERT’s vocabulary. For our SCR models, we use an indomain vocabulary, ConfliVocab (See Section 3.1 for more details).\nWe discarded the next sentence prediction (NSP) task. We found that the predicted NSP accuracy quickly reached 90% in the middle of our training, which indicated that NSP may be less challenge for the model to learn in our domain. However, learning NSP simultaneously affected the speed of optimization of masked language models (MLM) loss. Following many recent works discarding NSP (Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019) and our observation, we optimized MLM only.\nWe used four V-100 GPUs with 32 GB memory to train each model. We pre-trained each SCR model for about 150K steps over the 7 billion word corpus. We followed Devlin et al. (2018) to train the model with a sequence length of 128 for 80% of the steps. Then, we trained the rest 20% steps with a sequence of 512. The overall training time of each SCR model took about eight days. We trained Cont models the same as SCR models but in two fewer days because they were trained from intermediate checkpoints. See Appendix for more details of the pre-training hyperparameters ."
    }, {
      "heading" : "4.2 Fine-Tuning Setup",
      "text" : "Architecture. We followed the same architecture modification as BERT (Devlin et al., 2018) in the downstream tasks. Our task mainly consists of classification and sequence labeling. The sequence labeling tasks predict the sequence of BIO tags for each token in the input sentence. The classification tasks require a sequence classification/regression head on top of the pooled output of BERT. We used cross-entropy loss for binary/multi-class classification. We used mean-square loss and set the discrimination thresholds as 0.5 in all the multilabel classification tasks.\nCasing. Devlin et al. (2018) use the cased models for NER and the uncased models for all other tasks. However, other works report that uncased models perform slightly better than cased models in specific domains, even on NER tasks (Beltagy et al., 2019; Gu et al., 2020). Therefore, we evaluated both cased and uncased versions of all models.\nHyperparameters. Devlin et al. (2018) propose a hyperparameter tuning strategy relying on a gridsearch on the ranges such as the number of training epochs ∈ {3, 4}, and batch size ∈ {16, 32}. However, this strategy for general domain benchmarks (e.g. GLUE (Wang et al., 2018)) has not been sufficiently justified in other datasets (Chalkidis et al., 2020). The optimal hyperparameters are highly dataset- and task-dependent in our tasks. For instance, the models may still be underfited after the suggested maximum of four epochs. Additionally, based upon our observations of many conflict datasets (e.g., GTD, SATP, MUC-4, InSight Crime), ConfliBERT models converge to the best results much faster than BERT. Therefore, to compare with BERT fairly, we used early stopping based upon the development dataset, within a range of the maximum training epochs when all the models have achieved stable results. A more detailed description of other hyperparameters can be found in the Appendix. Finally, we repeated all the experiments ten times with different seeds."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Pre-training Results",
      "text" : "We use perplexity (ppl) to measure how well the language models predict a masked token in an unseen test set. We sampled 0.02% of stories from each source during the data preparation, ending with an 8.62 MB held-out dataset representing our\ncorpus’s distribution. Table 3 shows the ppl of our models on the held-out dataset. We also list the values reported by the original models (Devlin et al., 2018). Low ppl scores indicate that our models have been sufficiently pre-trained and have better generalization on our corpora."
    }, {
      "heading" : "5.2 Fine-Tuning Results and Analysis",
      "text" : "Table 4 reports the mean F1 scores of each task for 10 seeds. We have the below observations:\nConfliBERT’s superiority over BERT. ConfliBERT provides additional improvement to the original BERT in our target domain. In Table 4, although the performance is task-, dataset- and casing- dependent, our models consistently report the best results (in bold). In Figure 1, we compare ConfliBERT SCR-uncased with the best results from both cased and uncased versions of BERT in each experiment. We use different colors to denote four p-value thresholds (p<0.01, p<0.05, p<0.1, and p≥0.1) of statistical significance. SCRuncased demonstrates superior performance across all the tasks. We find that in most cases, SCRuncased statistically significantly out-performs all BERT models. Specifically for GTD, we observed that SCR-uncased slightly beats the best BERT, but it still shows a significant level of confidence, as depicted in Figure 1. We also observed that on InSight Crime, ConfliBERT achieves the best results in SCR-cased. Yet for SCR-uncased, the margin is not significant when compared with the best BERT in Figure 1. However, we conduct certain experiments on GTD and Insight Crime showing ConfliBERT’s significant superiority when tackling limited training data in Section 5.2.\nThe superiority between two pre-training strategies, Cont and SCR, remains to be studied. Table 4 shows that SCR slightly beats Cont in most cases (13 out of 18 tasks). And SCR-uncased provides the most stable outperformance over BERT among our four models. However, the performance is still dataset- and task-dependent. For example, Cont beats SCR significantly in Gun violence and Event Status. We present an in-depth analysis of these two cases in Appendix.\nEffect of ConfliVocab One major difference between SCR and Cont is the use of in-domain vocabulary. Section 5.2 proves that both Cont and SCR outperform BERT significantly while SCR slightly beats Cont. We have discussed the substantial advantage of using ConfliVocab during the tokenization in Section 3.1. Besides the examples in Table 2, we also find in ConfliVocab some famous gangs or terrorist groups frequently mentioned in the reports of violence and crime, such as Boko Haram, Al\nQaeda, Sinaloa Cartel, PCC, FARC, Mara, among others. On the other hand, the range of actor entities in the politics domain is much larger and sparser than terrorist and criminal organizations. Given that we have a more distinct in-domain vocabulary in the conflict domain, we expect a larger benefit from ConfliVocab in the conflict domain instead of the general politics domain.\nConfliBERT requires less annotated data than BERT. We also find that ConfliBERT performs well with limited data in various conflict datasets. Figure 2 shows three groups of experiments on GTD, SATP, and Insight Crime, where we used varying training data sizes but the same valid and testing set as the original experiments, respectively. We repeated each experiment with five seeds and plotted the average metric scores.\nFigure 2a shows that ConfliBERT beats BERT using limited size of GTD training data. Especially in the case of 1/32 size of GTD training data (88 examples), both SCR models still have 69% F1 scores, while BERT models drop to 55% F1 scores. In Figure 2b, we sampled various subsets of SATPrelevant, the SATP subset related to terrorist attacks. Results show that three of our models still remain 65% to 73% F1 scores when using only 1/32 size of the training data (34 examples), while BERT drop to only 44% F1 scores. Finally, we also observe that both ConfliBERT SCR models significantly\nbeat Cont and BERT models with a large margin on Insight Crime in Figure 2c .\nThe above results show ConfliBERT’s performance superiority with limited training data, thus alleviating the annotation burden in conflict research. Finally, these experiments also show that ConfliBERT outperforms BERT on GTD, SATP and Insight Crime, which strengthen the results we observe in Figure 1."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "This paper presents the development, application, evaluation, and further exploration of ConfliBERT, a pre-trained language model for political conflict and violence. The development of ConfliBERT rests on an unprecedented effort on three fronts. First, we collect and curate a large domain-specific corpus to support the pre-training process. Second, we conduct a comprehensive evaluation across several datasets and a variety of NLP tasks of distinct nature and varying degree of complexity.\nThe results shows that ConfliBERT consistently outperforms BERT in the conflict domain. It can also performs well with limited data in conflict domain, thus reducing the requirements of annotation data. In this way, ConfliBERT constitutes a valuable development that is likely to contribute to a broad community of researchers in political science and in the policy sector interested in tracking, analyzing, and predicting political violence and conflict at a global scale.\nDue to limited time and computational resources, we did not conduct more experiments to explore various hyperparameters that may affect the finetuning results, such as different sizes of vocabulary, various pre-training epochs, or the pre-training corpus size, to name a few. Future work should an-\nalyze how to optimize ConfliBERT, expand ConfliBERT to multi-lingual settings, and apply ConfliBERT to more challenging tasks such as understanding, inference, question answering, uncertainty qualification (Hu and Khan, 2021), and few-/ zero-shot tasks to speed up the study of NLP application for the political science community."
    }, {
      "heading" : "7 Ethical Impact",
      "text" : "Our research considers several measures to mitigate concerns of bias in machine learning: (i) we implement standard social science practices to select corpora and training data (Barberá et al., 2021); (ii) for the pre-training stage, we gather a corpus with unprecedented global coverage to reduce regional biases; (iii) we move beyond the biases introduced from dictionary-based methods by using machine learning, as suggested by Wilkerson and Casas (2017); (iv) finally, we use multiple coders for the training data.\nThe copyright of the news reports used in this research prevents us from sharing the raw data and hinders FAIR data principles (Wilkinson et al., 2016). However, we are working with others to anonymize the data appropriately for replication and extensions.\nFinally, producing accurate and valid data related to violent conflict could help to prevent or mitigate harm. For this reason, it is crucial to develop domain-specific efforts to maximize the accuracy of the output. To do so, we bring together a team of conflict scholars in political science and computer science to increase the accuracy of our language model. We hope that the accuracy improvements derived from our research will help to prevent or reduce harm to people at risk of violence."
    }, {
      "heading" : "A Appendix",
      "text" : "Dataset Table 5 and Table 6 list the detailed sources in our Expert Domain Corpora and Main Stream Media Collection described in Section 3.2, respectively.\nHyperparameters Table 7 and Table 8 describe the detailed hyper-parameters used in our pretraining and fine-tuning experiments, respectively. We implement our models using Huggingface API (Wolf et al., 2020)\nOther detailed results This section analyzes in a detailed manner the model’s performance on certain datasets. Specifically, we analyze two rare cases where all ConfliBERT models outperform BERTs and where Cont models significantly outperform SCR models. Table 9 indicates how Cont significantly outperforms SCR in all performance metrics (p<0.05 for all metrics). Table 10 shows how Cont-cased beats all the other counterparts for classifying event status of pieces of civil unrest. While there may be many factors, we postulate that some words in the original SCR-cased vocabulary are accidentally good at tokenizing the out-of-domain text in Gun Violence, while that vocabulary is also good at classifying ongoing (OG) and future (FU) events."
    } ],
    "references" : [ {
      "title" : "Publicly available clinical BERT embeddings",
      "author" : [ "Emily Alsentzer", "John Murphy", "William Boag", "WeiHung Weng", "Di Jin", "Tristan Naumann", "Matthew McDermott." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 72–78,",
      "citeRegEx" : "Alsentzer et al\\.,? 2019",
      "shortCiteRegEx" : "Alsentzer et al\\.",
      "year" : 2019
    }, {
      "title" : "Construction of the literature graph in semantic scholar",
      "author" : [ "ters", "Joanna Power", "Sam Skjonsberg", "Lucy Wang", "Chris Wilhelm", "Zheng Yuan", "Madeleine van Zuylen", "Oren Etzioni" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "ters et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "ters et al\\.",
      "year" : 2018
    }, {
      "title" : "Automated text classification of news articles: A practical guide",
      "author" : [ "Pablo Barberá", "Amber E Boydstun", "Suzanna Linn", "Ryan McMahon", "Jonathan Nagler." ],
      "venue" : "Political Analysis, 29(1):19–42.",
      "citeRegEx" : "Barberá et al\\.,? 2021",
      "shortCiteRegEx" : "Barberá et al\\.",
      "year" : 2021
    }, {
      "title" : "Generating politically-relevant event data",
      "author" : [ "John Beieler." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 37–42.",
      "citeRegEx" : "Beieler.,? 2016",
      "shortCiteRegEx" : "Beieler.",
      "year" : 2016
    }, {
      "title" : "Petrarch: Python engine for text resolution and related coding hierarchy",
      "author" : [ "John Beieler", "Clayton Norris." ],
      "venue" : "Available at https://github.com/ openeventdata/petrarch (2020/05/15). Unpublished Manuscript.",
      "citeRegEx" : "Beieler and Norris.,? 2014",
      "shortCiteRegEx" : "Beieler and Norris.",
      "year" : 2014
    }, {
      "title" : "Scibert: Pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Integrated Data for Events Analysis (IDEA): An Event Typology for Automated Events Data Development",
      "author" : [ "Doug Bond", "Joe Bond", "Churl Oh", "Craig J. Jenkins", "Charles L. Taylor." ],
      "venue" : "Journal of Peace Research, 40(6):733–745.",
      "citeRegEx" : "Bond et al\\.,? 2003",
      "shortCiteRegEx" : "Bond et al\\.",
      "year" : 2003
    }, {
      "title" : "Analyzing ELMo and DistilBERT on sociopolitical news classification",
      "author" : [ "Berfu Büyüköz", "Ali Hürriyetoğlu", "Arzucan Özgür." ],
      "venue" : "Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020, pages 9–18, Marseille,",
      "citeRegEx" : "Büyüköz et al\\.,? 2020",
      "shortCiteRegEx" : "Büyüköz et al\\.",
      "year" : 2020
    }, {
      "title" : "LEGAL-BERT: The muppets straight out",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Prodromos Malakasiotis", "Nikolaos Aletras", "Ion Androutsopoulos" ],
      "venue" : null,
      "citeRegEx" : "Chalkidis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2020
    }, {
      "title" : "ELECTRA: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Relationship and entity extraction evaluation dataset",
      "author" : [ "DSTL." ],
      "venue" : "https://github.com/dstl/ re3d/. Accessed: 2021-07-01.",
      "citeRegEx" : "DSTL.,? 2018",
      "shortCiteRegEx" : "DSTL.",
      "year" : 2018
    }, {
      "title" : "Document-level event role filler extraction using multi-granularity contextualized encoding",
      "author" : [ "X. Du", "Claire Cardie." ],
      "venue" : "ACL.",
      "citeRegEx" : "Du and Cardie.,? 2020",
      "shortCiteRegEx" : "Du and Cardie.",
      "year" : 2020
    }, {
      "title" : "Conflict and mediation event observations (cameo): A new event data framework for the analysis of foreign policy interactions",
      "author" : [ "Deborah J Gerner", "Philip A Schrodt", "Omür Yilmaz", "Rajaa Abu-Jabr." ],
      "venue" : "International Studies Association, New Orleans.",
      "citeRegEx" : "Gerner et al\\.,? 2002",
      "shortCiteRegEx" : "Gerner et al\\.",
      "year" : 2002
    }, {
      "title" : "Cross-lingual classification of topics in political texts",
      "author" : [ "Goran Glavaš", "Federico Nanni", "Simone Paolo Ponzetto." ],
      "venue" : "Proceedings of the Second Workshop on NLP and Computational Social Science, pages 42–46, Vancouver, Canada. Association for",
      "citeRegEx" : "Glavaš et al\\.,? 2017",
      "shortCiteRegEx" : "Glavaš et al\\.",
      "year" : 2017
    }, {
      "title" : "Practical solutions to the problem of diagonal dominance in kernel document clustering",
      "author" : [ "Derek Greene", "Pádraig Cunningham." ],
      "venue" : "Proc. 23rd International Conference on Machine learning (ICML’06), pages 377–384. ACM Press.",
      "citeRegEx" : "Greene and Cunningham.,? 2006",
      "shortCiteRegEx" : "Greene and Cunningham.",
      "year" : 2006
    }, {
      "title" : "Domainspecific language model pretraining for biomedical natural language processing",
      "author" : [ "Yuxian Gu", "Robert Tinn", "Hao Cheng", "Michael Lucas", "Naoto Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon." ],
      "venue" : "ArXiv, abs/2007.15779.",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Corpuslevel evaluation for event qa: The indiapoliceevents corpus covering the 2002 gujarat violence. ArXiv, abs/2105.12936",
      "author" : [ "A. Halterman", "Katherine A. Keith", "Sheikh Muhammad Sarwar", "Brendan O’Connor" ],
      "venue" : null,
      "citeRegEx" : "Halterman et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Halterman et al\\.",
      "year" : 2021
    }, {
      "title" : "Few-shot upsampling for protest size detection",
      "author" : [ "Andrew Halterman", "Benjamin J. Radford." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3713–3720, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Halterman and Radford.,? 2021",
      "shortCiteRegEx" : "Halterman and Radford.",
      "year" : 2021
    }, {
      "title" : "Mpeds: Automating the generation of protest event data",
      "author" : [ "Alex Hanna." ],
      "venue" : "Available at https: //osf.io/preprints/socarxiv/xuqmv (2020/05/22). Unpublished Manuscript.",
      "citeRegEx" : "Hanna.,? 2017",
      "shortCiteRegEx" : "Hanna.",
      "year" : 2017
    }, {
      "title" : "Uncertainty-aware reliable text classification",
      "author" : [ "Yibo Hu", "Latifur Khan." ],
      "venue" : "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 628–636.",
      "citeRegEx" : "Hu and Khan.,? 2021",
      "shortCiteRegEx" : "Hu and Khan.",
      "year" : 2021
    }, {
      "title" : "Distinguishing past, on-going, and future events: The EventStatus corpus",
      "author" : [ "Ruihong Huang", "Ignacio Cases", "Dan Jurafsky", "Cleo Condoravdi", "Ellen Riloff." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Challenges and applications of automated extraction of socio-political events from text (case 2021): Workshop and shared task report",
      "author" : [ "Ali Hürriyetoğlu", "Hristo Tanev", "Vanni Zavarella", "Jakub Piskorski", "Reyyan Yeniterzi", "Deniz Yuret", "Aline Villavicencio" ],
      "venue" : null,
      "citeRegEx" : "Hürriyetoğlu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hürriyetoğlu et al\\.",
      "year" : 2021
    }, {
      "title" : "A task set proposal for automatic protest information collection across multiple countries",
      "author" : [ "Ali Hürriyetoğlu", "Erdem Yörük", "Deniz Yüret", "Çağrı Yoltar", "Burak Gürel", "Fırat Duruşan", "Osman Mutlu." ],
      "venue" : "European Conference on Information Re-",
      "citeRegEx" : "Hürriyetoğlu et al\\.,? 2019",
      "shortCiteRegEx" : "Hürriyetoğlu et al\\.",
      "year" : 2019
    }, {
      "title" : "Mimiciii, a freely accessible critical care database",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "H Lehman Li-Wei", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark." ],
      "venue" : "Scien-",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "arXiv preprint arXiv:1907.10529.",
      "citeRegEx" : "Joshi et al\\.,? 2019",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2019
    }, {
      "title" : "Race: Large-scale reading comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1704.04683.",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Newsweeder: Learning to filter netnews",
      "author" : [ "Ken Lang." ],
      "venue" : "Proceedings of the Twelfth International Conference on Machine Learning, pages 331–339.",
      "citeRegEx" : "Lang.,? 1995",
      "shortCiteRegEx" : "Lang.",
      "year" : 1995
    }, {
      "title" : "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art",
      "author" : [ "Patrick Lewis", "Myle Ott", "Jingfei Du", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "S2ORC: The semantic scholar open research corpus",
      "author" : [ "Kyle Lo", "Lucy Lu Wang", "Mark Neumann", "Rodney Kinney", "Daniel Weld." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online. Asso-",
      "citeRegEx" : "Lo et al\\.,? 2020",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal petrarch: Language-agnostic political event coding using universal dependencies",
      "author" : [ "J. Lu", "Joydeep Roy." ],
      "venue" : "Available at https://github.com/openeventdata/ UniversalPetrarch (2020/05/22).",
      "citeRegEx" : "Lu and Roy.,? 2017",
      "shortCiteRegEx" : "Lu and Roy.",
      "year" : 2017
    }, {
      "title" : "Petrarch2: Another event coding program",
      "author" : [ "Clayton Norris", "Philip Schrodt", "John Beieler." ],
      "venue" : "Journal of Open Source Software, 2(9):133.",
      "citeRegEx" : "Norris et al\\.,? 2017",
      "shortCiteRegEx" : "Norris et al\\.",
      "year" : 2017
    }, {
      "title" : "Crisis Early Warning and Decision Support: Contemporary Approaches and Thoughts on Future Research",
      "author" : [ "Sean P. O’Brien" ],
      "venue" : "International Studies Review,",
      "citeRegEx" : "O.Brien.,? \\Q2010\\E",
      "shortCiteRegEx" : "O.Brien.",
      "year" : 2010
    }, {
      "title" : "Text categorization for conflict event annotation",
      "author" : [ "Fredrik Olsson", "Magnus Sahlgren", "Fehmi ben Abdesslem", "Ariel Ekgren", "Kristine Eck." ],
      "venue" : "Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020, pages 19–",
      "citeRegEx" : "Olsson et al\\.,? 2020",
      "shortCiteRegEx" : "Olsson et al\\.",
      "year" : 2020
    }, {
      "title" : "Event clustering within news articles",
      "author" : [ "Faik Kerem Örs", "Süveyda Yeniterzi", "Reyyan Yeniterzi." ],
      "venue" : "Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020, pages 63–68, Marseille, France. European Language",
      "citeRegEx" : "Örs et al\\.,? 2020",
      "shortCiteRegEx" : "Örs et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised Event Coding From Text Written in Spanish: Introducing Eventus ID",
      "author" : [ "Javier Osorio", "Alejandro Reyes." ],
      "venue" : "Social Science Computer Review, 35(3):406–416.",
      "citeRegEx" : "Osorio and Reyes.,? 2017",
      "shortCiteRegEx" : "Osorio and Reyes.",
      "year" : 2017
    }, {
      "title" : "Supervised event coding from text written in Arabic: Introducing hadath",
      "author" : [ "Javier Osorio", "Alejandro Reyes", "Alejandro Beltrán", "Atal Ahmadzai." ],
      "venue" : "Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020, pages 49–",
      "citeRegEx" : "Osorio et al\\.,? 2020",
      "shortCiteRegEx" : "Osorio et al\\.",
      "year" : 2020
    }, {
      "title" : "English gigaword fifth edition ldc2011t07",
      "author" : [ "Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda." ],
      "venue" : "Linguistic Data Consortium, Philadelphia.",
      "citeRegEx" : "Parker et al\\.,? 2011",
      "shortCiteRegEx" : "Parker et al\\.",
      "year" : 2011
    }, {
      "title" : "3m-transformers for event coding on organized crime domain",
      "author" : [ "Erick Skorupa Parolin", "Latifur Khan", "Javier Osorio", "Patrick Brandt", "Vito D’Orazio", "Jennifer Holmes" ],
      "venue" : "IEEE International Conference on Data Science and Advanced Analytics (DSAA)",
      "citeRegEx" : "Parolin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Parolin et al\\.",
      "year" : 2021
    }, {
      "title" : "The gun violence database: A new task and data set for nlp",
      "author" : [ "Ellie Pavlick", "Heng Ji", "Xiaoman Pan", "Chris CallisonBurch." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1018–1024.",
      "citeRegEx" : "Pavlick et al\\.,? 2016",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2016
    }, {
      "title" : "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets",
      "author" : [ "Yifan Peng", "Shankai Yan", "Zhiyong Lu." ],
      "venue" : "Proceedings of the 2019 Workshop on Biomedical Natural Language Processing",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "R. Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Multitask models for supervised protest detection in texts",
      "author" : [ "Benjamin Radford." ],
      "venue" : "Available at https://arxiv.org/abs/2005.02954 (2020/05/22). Unpublished Manuscript.",
      "citeRegEx" : "Radford.,? 2019",
      "shortCiteRegEx" : "Radford.",
      "year" : 2019
    }, {
      "title" : "Seeing the forest and the trees: Detection and cross-document coreference resolution of militarized interstate disputes",
      "author" : [ "Benjamin Radford." ],
      "venue" : "Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020, pages 35–",
      "citeRegEx" : "Radford.,? 2020",
      "shortCiteRegEx" : "Radford.",
      "year" : 2020
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1806.03822.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Distributed framework for political event coding in real-time",
      "author" : [ "Sayeed Salam", "Patrick Brandty", "Jennifer Holmesy", "Latifur Khan." ],
      "venue" : "2018 2nd European Conference on Electrical Engineering and Computer Science (EECS), pages 266–273.",
      "citeRegEx" : "Salam et al\\.,? 2018",
      "shortCiteRegEx" : "Salam et al\\.",
      "year" : 2018
    }, {
      "title" : "Twenty Years of the Kansas Event Data System Project",
      "author" : [ "Philip A. Schrodt." ],
      "venue" : "The Political Methodologist, 14(1):2–6.",
      "citeRegEx" : "Schrodt.,? 2006",
      "shortCiteRegEx" : "Schrodt.",
      "year" : 2006
    }, {
      "title" : "TABARI",
      "author" : [ "Philip A. Schrodt." ],
      "venue" : "Textual Analysis by Augmented Replacement Instructions.",
      "citeRegEx" : "Schrodt.,? 2009",
      "shortCiteRegEx" : "Schrodt.",
      "year" : 2009
    }, {
      "title" : "Hanke: Hierarchical attention networks for knowledge extraction in political science domain",
      "author" : [ "Erick Skorupa Parolin", "Latifur Khan", "Javier Osorio", "Vito D’Orazio", "Patrick T. Brandt", "Jennifer Holmes" ],
      "venue" : "IEEE 7th International Conference on Data",
      "citeRegEx" : "Parolin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Parolin et al\\.",
      "year" : 2020
    }, {
      "title" : "A literature survey on",
      "author" : [ "Mohammad S Sorower" ],
      "venue" : null,
      "citeRegEx" : "Sorower.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sorower.",
      "year" : 2010
    }, {
      "title" : "Attention is all",
      "author" : [ "Kaiser", "Illia Polosukhin" ],
      "venue" : null,
      "citeRegEx" : "Kaiser and Polosukhin.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kaiser and Polosukhin.",
      "year" : 2017
    }, {
      "title" : "Superglue: A stickier",
      "author" : [ "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Bowman.,? \\Q2019\\E",
      "shortCiteRegEx" : "Bowman.",
      "year" : 2019
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE in-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Recent efforts by political scientists resort to traditional machine learning (Hanna, 2017; Osorio et al., 2020) and deep learning (Beieler, 2016; Radford, 2019; Glavaš et al.",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 40,
      "context" : "Recent efforts by political scientists resort to traditional machine learning (Hanna, 2017; Osorio et al., 2020) and deep learning (Beieler, 2016; Radford, 2019; Glavaš et al.",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : ", 2020) and deep learning (Beieler, 2016; Radford, 2019; Glavaš et al., 2017; Skorupa Parolin et al., 2020) to analyze political conflict and violence.",
      "startOffset" : 26,
      "endOffset" : 107
    }, {
      "referenceID" : 46,
      "context" : ", 2020) and deep learning (Beieler, 2016; Radford, 2019; Glavaš et al., 2017; Skorupa Parolin et al., 2020) to analyze political conflict and violence.",
      "startOffset" : 26,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : ", 2020) and deep learning (Beieler, 2016; Radford, 2019; Glavaš et al., 2017; Skorupa Parolin et al., 2020) to analyze political conflict and violence.",
      "startOffset" : 26,
      "endOffset" : 107
    }, {
      "referenceID" : 45,
      "context" : "Recent progress in NLP has been driven by pre-trained transformer language models (Vaswani et al., 2017; Radford et al., 2019; Devlin et al., 2018; Yang et al., 2019).",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "Recent progress in NLP has been driven by pre-trained transformer language models (Vaswani et al., 2017; Radford et al., 2019; Devlin et al., 2018; Yang et al., 2019).",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 56,
      "context" : "While most pre-trained language models are built on general domain corpora, such as Wikipedia, BookCorpus (Zhu et al., 2015), and WebText (Radford et al.",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 45,
      "context" : ", 2015), and WebText (Radford et al., 2019), recent works show that pretraining on domain-specific corpora can boost downstream performance on those domains (Lee et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : ", 2019), recent works show that pretraining on domain-specific corpora can boost downstream performance on those domains (Lee et al., 2019; Gururangan et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 164
    }, {
      "referenceID" : 17,
      "context" : ", 2019), recent works show that pretraining on domain-specific corpora can boost downstream performance on those domains (Lee et al., 2019; Gururangan et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 164
    }, {
      "referenceID" : 30,
      "context" : "Domainspecific work in bio-medicine focuses not only on developing pre-trained models (Lee et al., 2019; Beltagy et al., 2019; Alsentzer et al., 2019; Lewis et al., 2020; Gu et al., 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al.",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "Domainspecific work in bio-medicine focuses not only on developing pre-trained models (Lee et al., 2019; Beltagy et al., 2019; Alsentzer et al., 2019; Lewis et al., 2020; Gu et al., 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al.",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 0,
      "context" : "Domainspecific work in bio-medicine focuses not only on developing pre-trained models (Lee et al., 2019; Beltagy et al., 2019; Alsentzer et al., 2019; Lewis et al., 2020; Gu et al., 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al.",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 31,
      "context" : "Domainspecific work in bio-medicine focuses not only on developing pre-trained models (Lee et al., 2019; Beltagy et al., 2019; Alsentzer et al., 2019; Lewis et al., 2020; Gu et al., 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al.",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 16,
      "context" : "Domainspecific work in bio-medicine focuses not only on developing pre-trained models (Lee et al., 2019; Beltagy et al., 2019; Alsentzer et al., 2019; Lewis et al., 2020; Gu et al., 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al.",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 44,
      "context" : ", 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al., 2019; Gu et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : ", 2020) but also on proposing domain-relevant evaluation benchmarks (Peng et al., 2019; Gu et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Pre-training models have also advanced research in other domains such as computer science papers (Beltagy et al., 2019) and legal studies (Chalkidis et al.",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "Despite some efforts to apply transformers-based approaches in political science (Büyüköz et al., 2020; Olsson et al., 2020; Örs et al., 2020; Radford, 2020; Halterman and Radford, 2021; Hürriyetoğlu et al., 2021), there are few studies introducing domainspecific pre-trained language models on either political science or conflict research.",
      "startOffset" : 81,
      "endOffset" : 213
    }, {
      "referenceID" : 37,
      "context" : "Despite some efforts to apply transformers-based approaches in political science (Büyüköz et al., 2020; Olsson et al., 2020; Örs et al., 2020; Radford, 2020; Halterman and Radford, 2021; Hürriyetoğlu et al., 2021), there are few studies introducing domainspecific pre-trained language models on either political science or conflict research.",
      "startOffset" : 81,
      "endOffset" : 213
    }, {
      "referenceID" : 38,
      "context" : "Despite some efforts to apply transformers-based approaches in political science (Büyüköz et al., 2020; Olsson et al., 2020; Örs et al., 2020; Radford, 2020; Halterman and Radford, 2021; Hürriyetoğlu et al., 2021), there are few studies introducing domainspecific pre-trained language models on either political science or conflict research.",
      "startOffset" : 81,
      "endOffset" : 213
    }, {
      "referenceID" : 47,
      "context" : "Despite some efforts to apply transformers-based approaches in political science (Büyüköz et al., 2020; Olsson et al., 2020; Örs et al., 2020; Radford, 2020; Halterman and Radford, 2021; Hürriyetoğlu et al., 2021), there are few studies introducing domainspecific pre-trained language models on either political science or conflict research.",
      "startOffset" : 81,
      "endOffset" : 213
    }, {
      "referenceID" : 19,
      "context" : "Despite some efforts to apply transformers-based approaches in political science (Büyüköz et al., 2020; Olsson et al., 2020; Örs et al., 2020; Radford, 2020; Halterman and Radford, 2021; Hürriyetoğlu et al., 2021), there are few studies introducing domainspecific pre-trained language models on either political science or conflict research.",
      "startOffset" : 81,
      "endOffset" : 213
    }, {
      "referenceID" : 23,
      "context" : "Despite some efforts to apply transformers-based approaches in political science (Büyüköz et al., 2020; Olsson et al., 2020; Örs et al., 2020; Radford, 2020; Halterman and Radford, 2021; Hürriyetoğlu et al., 2021), there are few studies introducing domainspecific pre-trained language models on either political science or conflict research.",
      "startOffset" : 81,
      "endOffset" : 213
    }, {
      "referenceID" : 10,
      "context" : "Recent pre-trained transformer language models, such as the Bidirectional Encoder Representation from Transformers (BERT) (Devlin et al., 2018), follow an easy two-steps framework: (1) pre-train on a large unlabeled corpus; and (2) fine-tune on task-specific labeled data.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 56,
      "context" : "Finally, most BERT-like models focus on generic domain, such as Wikipedia, BookCorpus (Zhu et al., 2015) and WebText (Radford et al.",
      "startOffset" : 86,
      "endOffset" : 104
    }, {
      "referenceID" : 30,
      "context" : "Besides, BERT has been validated in various domains (Lee et al., 2019; Beltagy et al., 2019; Peng et al., 2019; Chalkidis et al., 2020; Gu et al., 2020) shown in Table 1.",
      "startOffset" : 52,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "Besides, BERT has been validated in various domains (Lee et al., 2019; Beltagy et al., 2019; Peng et al., 2019; Chalkidis et al., 2020; Gu et al., 2020) shown in Table 1.",
      "startOffset" : 52,
      "endOffset" : 152
    }, {
      "referenceID" : 44,
      "context" : "Besides, BERT has been validated in various domains (Lee et al., 2019; Beltagy et al., 2019; Peng et al., 2019; Chalkidis et al., 2020; Gu et al., 2020) shown in Table 1.",
      "startOffset" : 52,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "Besides, BERT has been validated in various domains (Lee et al., 2019; Beltagy et al., 2019; Peng et al., 2019; Chalkidis et al., 2020; Gu et al., 2020) shown in Table 1.",
      "startOffset" : 52,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "Besides, BERT has been validated in various domains (Lee et al., 2019; Beltagy et al., 2019; Peng et al., 2019; Chalkidis et al., 2020; Gu et al., 2020) shown in Table 1.",
      "startOffset" : 52,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "Various papers (Beltagy et al., 2019; Gu et al., 2020) argue that SCR generates substantial gains over Cont for domains with sizeable unlabeled text.",
      "startOffset" : 15,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "Various papers (Beltagy et al., 2019; Gu et al., 2020) argue that SCR generates substantial gains over Cont for domains with sizeable unlabeled text.",
      "startOffset" : 15,
      "endOffset" : 54
    }, {
      "referenceID" : 25,
      "context" : "As illustrated in Table 1, there exist large-scale publicly available biomedical datasets, such as PubMed and MIMIC (Johnson et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 33,
      "context" : ", 2019) is built from a large corpus of academic papers (Ammar et al., 2018; Lo et al., 2020); nonetheless, there is no corpus available which is specific to the politics and conflict domain.",
      "startOffset" : 56,
      "endOffset" : 93
    }, {
      "referenceID" : 41,
      "context" : "This corpus provides a distinct coverage of seven international English newswires from 1994 to 2010 (Parker et al., 2011).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 49,
      "context" : "PRT is a developing event dataset crawled from more than 400 news agencies worldwide from October 2017 (Salam et al., 2018).",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 48,
      "context" : "The introduction of comprehensive benchmarks accelerated the development of pre-trained language models in the general NLP domain (Wang et al., 2018, 2019; Rajpurkar et al., 2018; Lai et al., 2017) and biomedical applications (Peng et al.",
      "startOffset" : 130,
      "endOffset" : 197
    }, {
      "referenceID" : 27,
      "context" : "The introduction of comprehensive benchmarks accelerated the development of pre-trained language models in the general NLP domain (Wang et al., 2018, 2019; Rajpurkar et al., 2018; Lai et al., 2017) and biomedical applications (Peng et al.",
      "startOffset" : 130,
      "endOffset" : 197
    }, {
      "referenceID" : 53,
      "context" : "We use example-based F1 metrics for multi-label classification tasks (Sorower, 2010).",
      "startOffset" : 69,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "We collected BBC News (Greene and Cunningham, 2006) and 20 Newsgroups (Lang, 1995) for identifying political news, a subset from Gun Violence Database (Pavlick et al.",
      "startOffset" : 22,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : "We collected BBC News (Greene and Cunningham, 2006) and 20 Newsgroups (Lang, 1995) for identifying political news, a subset from Gun Violence Database (Pavlick et al.",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 43,
      "context" : "We collected BBC News (Greene and Cunningham, 2006) and 20 Newsgroups (Lang, 1995) for identifying political news, a subset from Gun Violence Database (Pavlick et al., 2016) for finding articles related to gun violence.",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 24,
      "context" : "We also used the samples from Global Contention Politics Dataset (GLOCON) (Hürriyetoğlu et al., 2019) to conduct one sentence-level and one document-level classification task of predicting whether the story is related to protests.",
      "startOffset" : 74,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "India Police Events (Halterman et al., 2021) consists of sentences from English-language Times of India articles about police activity events in Gujarat during March 2002 (a relevant period due to widespread Hindu-Muslim violence).",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "Event Status includes English news articles about civil unrest events annotated with temporal tags (Huang et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 42,
      "context" : "InSight Crime (Parolin et al., 2021) contains annotated stories about organized criminal activity",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "Re3d stands for Relationship and Entity Extraction Evaluation Dataset (DSTL, 2018), comprising task-specific documents focused on the topic of the conflict in Syria and Iraq.",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "org Observations) is the industry standard for event extraction in political science (Gerner et al., 2002).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "Following many recent works discarding NSP (Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019) and our observation, we optimized MLM only.",
      "startOffset" : 43,
      "endOffset" : 126
    }, {
      "referenceID" : 26,
      "context" : "Following many recent works discarding NSP (Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019) and our observation, we optimized MLM only.",
      "startOffset" : 43,
      "endOffset" : 126
    }, {
      "referenceID" : 32,
      "context" : "Following many recent works discarding NSP (Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019) and our observation, we optimized MLM only.",
      "startOffset" : 43,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "We followed the same architecture modification as BERT (Devlin et al., 2018) in the downstream tasks.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "However, other works report that uncased models perform slightly better than cased models in specific domains, even on NER tasks (Beltagy et al., 2019; Gu et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 168
    }, {
      "referenceID" : 16,
      "context" : "However, other works report that uncased models perform slightly better than cased models in specific domains, even on NER tasks (Beltagy et al., 2019; Gu et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 168
    }, {
      "referenceID" : 10,
      "context" : "We also list the values reported by the original models (Devlin et al., 2018).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "Future work should analyze how to optimize ConfliBERT, expand ConfliBERT to multi-lingual settings, and apply ConfliBERT to more challenging tasks such as understanding, inference, question answering, uncertainty qualification (Hu and Khan, 2021), and few-/ zero-shot tasks to speed up the study of NLP application for the political science community.",
      "startOffset" : 227,
      "endOffset" : 246
    }, {
      "referenceID" : 2,
      "context" : "Our research considers several measures to mitigate concerns of bias in machine learning: (i) we implement standard social science practices to select corpora and training data (Barberá et al., 2021); (ii) for the pre-training stage, we gather a corpus with unprecedented global coverage to reduce regional biases; (iii) we move beyond the biases introduced from dictionary-based methods by using machine learning, as suggested by Wilkerson and Casas (2017); (iv) finally, we use multiple coders for the training data.",
      "startOffset" : 177,
      "endOffset" : 199
    } ],
    "year" : 0,
    "abstractText" : "Analyzing conflicts and political violence around the world is a persistent challenge in the political science and policy communities due to the vast volumes of specialized text needed to monitor political unrest and violent conflicts at a global scale. To help advance research in political science, we introduce ConfliBERT, a domain-specific pre-trained language model for conflict and political violence. We first gather a large domain-specific text corpus for language modeling from various sources. We then build ConfliBERT using two approaches: pre-training from scratch and continual pretraining. To evaluate ConfliBERT, we collect 13 datasets and implement 18 tasks to assess the models’ practical application in conflict research. Finally, we evaluate several versions of ConfliBERT in multiple experiments. Results consistently show that ConfliBERT outperforms BERT when analyzing political violence and conflict. Our code is publicly available.1",
    "creator" : null
  }
}
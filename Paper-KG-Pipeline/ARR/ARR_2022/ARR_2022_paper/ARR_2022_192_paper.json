{
  "name" : "ARR_2022_192_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DYNAMICTOC: Persona-based Table of Contents for Consumption of Long Documents",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Documents such as financial statements, reports and contracts are often long and comprehensive, replete with domain-specific description and information. They are meant to be consumed by several entities or personas, e.g. legal department of companies, customers or financial organizations such as banks. As these documents contain vital information about the business, the business personas are often required to read through and analyze the documents in details. These personas are often interested in different sections of the document, based on the business requirements. For example, employees might be interested in the stock programs of the company, whereas the lenders and investors would like to read through profit statements. The traditional technology to navigate long documents is through a Table of Contents (ToC) populated with the heading of each section and\nchapters. However, the Table of Contents does not show the information present in the underlying paragraphs of a section, and there is no way to highlight information relevant to different personas.\nTo this effect, we propose DYNAMICTOC, an intelligent table of contents-based navigator. DYNAMICTOC provides user the flexibility to choose the persona and read the document from its lens. For the current work, we focus on the finance and legal domain, and hence, personas are taken as commonplace entities like investors, lenders, financial bodies, etc. DYNAMICTOC highlights the relevant sections of the document as per the persona. For this, the input finance or contract document is segmented at the paragraph level and a cluster of “aspects or topics” is inferred for each para. These are then mapped to the interest topics of the personas. Further, DYNAMICTOC has a novel question-based guided experience, to enhance the visibility of underlying information. Studies have shown that questions are more intuitive and informative than headings and hence can provide a better understanding of what the paragraph talks about. The overall interface is shown in Figure 1."
    }, {
      "heading" : "2 Related Work",
      "text" : "Document understanding is a critical and challenging task in information processing. There have been many related research works in this direction. Keyword detection (Liu et al., 2009; Tixier et al., 2016) & topic modeling (Blei et al., 2003) works aim is to describe the document by a few important words or topics for concise representation. The first step is to acquire a list of keyword candidates (e.g., n-grams or chunks) with heuristic methods (Hulth, 2003; Shang et al., 2018), then rank them in accordance with their importance to the document (Wu et al., 2005; Gollapalli and Caragea, 2014; Bougouin et al., 2013). Another task is compact and informative headline generation from a document (David and Zajic, 2003; Lopyrev, 2015). Text\nsummarization is the process of generating natural language summaries from an input document retaining the most important information (Rush et al., 2015; See et al., 2017). Recently, an Outline Generation task was introduced by (Zhang et al., 2019) as a hierarchical structured prediction problem. Given a document, their aim is to first predict a sequence of section boundaries and then a sequence of section headings accordingly to come up with a Table of Contents for the same.\nA related direction of work to ours is of aspect detection, which has been explored in the literature largely using user reviews for products. Early works focused on rule-based approaches using lexicons and dependency relations, and utilize manually defined rules to identify patterns and extract aspects (Qiu et al., 2011; Liu et al., 2016), which require domain-specific knowledge and human expertise. Supervised approaches formulate aspect extraction as a sequence labelling problem that can be solved by hidden Markov models (HMM) (Jin et al., 2009), conditional random fields (CRF) (Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2012), and recurrent neural networks (RNN) (Wang et al., 2016; Liu et al., 2015). These approaches have shown better performance compared to the rule-based ones, but require large amounts of labelled data for training. Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018; Shi et al., 2018; Alvarez-López et al., 2016). Recently, deep learning based topic models (Srivastava and Sutton, 2017; Luo et al., 2019; He et al., 2017; Shi et al., 2021) have shown strong performance in extracting coherent aspects in an\nunsupervised manner. None of the prior works on aspect detection have worked with contracts or financial documents that are quite long (50-100 pages) in comparison to user reviews. Even if we break the document at paragraph level, it can still go over tens of lines. Hence, the importance of word frequency is much more in our case. We bridge the gap between directly using the unsupervised aspect detection frameworks for the financial documents by adding a TF-IDF based weighing parameter while training. Moreover, there are no gold standards for aspect detection for contract or finance domain, hence, we use unsupervised clustering based metrics for validating the output detection.\nFurther, it has been shown that question-answers play a critical role in scientific inquiry, informationseeking dialogue, and knowledge acquisition (Hintikka and Saarinen, 1979; Stede and Schlangen, 2004). In a dialogue system, question generation is used to obtain specific information from the user or make the conversation more pleasant (Shukla et al., 2019; Saeidi et al., 2018). Hence, we hypothesize that augmenting the default ToC with Questions that give a high level overview of the paragraphs can enhance the reading experience of the users. Question generation can also be seen as a summarization or seq2seq task. Various pre-trained language models like BART (Lewis et al., 2020), PEGASUS (Zhang et al., 2020), etc., have shown excellent results for these tasks. Researchers have worked on top of these language models & proposed various rewards for QGen to optimize these models. (Kumar et al., 2019) has employed BLEUbased rewards, (Zhang and Bansal, 2019) have used\nanswerability rewards, whereas (Xie et al., 2020) has used a combination of fluency, relevance, and answerability rewards.\nPrevious question generation literature has focused on generating questions based on an entity, phrase, or sentence. In this work, we explore longform question generation, i.e., question-based long text. We explore deep reinforcement learning techniques for the same. as they have shown competitive results in various natural language generation tasks such as summarization (Pasunuru and Bansal, 2018), style transfer (Liu et al., 2021; Goyal et al., 2021), question generation (Hosking and Riedel, 2019; Xie et al., 2020) etc. Motivated by this we use BART as our base model. As there is a lack of labeled question datasets in financial domain, to overcome the domain shift problem, we train the model with additional rewards in a reinforcement learning setup to make more suitable for a general domain.\nThere are several commercial products for reading documents across devices, but all of them have a fixed document navigation, based on chapters and headings. To the best of our knowledge, there is no prior art looking into providing an end-to-end persona-based navigation. The mentioned technologies address only part of the required solutions. Following are the key contributions of our work:\n1. We propose a novel DYNAMICTOC technology to enable persona-based non-linear navigation for efficient consumption of long documents.\n2. We extend the unsupervised aspect detection to long domain-specific documents by combining TF-IDF with aspect detection process to make it more robust and show the improvements experimentally.\n3. We propose a method to generate questions based on the content of the paragraph maximizing the information coverage, entity correctness & answerability of the question.\n4. We showcase the viability of our pipeline and evaluate it using metric-based and a human survey-based evaluation."
    }, {
      "heading" : "3 Datasets",
      "text" : "SEC Filing: The SEC filing is a financial statement document submitted to the U.S. Securities\nand Exchange Commission. Public companies, certain insiders, and broker-dealers are required to make regular SEC filings. There are many types of documents available on the EDGAR website (Eg. 10-K, 10-Q, Form 4, etc.). For our work, we focused on the SEC 10-K documents available on the EDGAR website1. For a given company, the 10-k documents are available in HTML, XBRL and XML format. The complete submission text file for a 10-k document (XML) was used for parsing. We split the document content into different items ranging from item 1 to item 16. These 10-K documents range from 50 to 120 pages and contain multiple tables along with text paragraphs.\nELI5: We use the ELI5 dataset (Fan et al., 2019) to train the question generation model. ELI5 or Explain Like I’m 5, is a question-answer dataset scraped from the subreddit r/explainlikeimfive/. The subreddit rules encourage people to ask a question about any topic and get an answer for it. To maintain the dataset’s quality, we only select those question-answer pairs with more than two upvotes. Note that no dataset for such question generation task exists for contractual and financial documents. Hence, we resort to use the ELI5 dataset for supervised training and use that model for inferencing on documents from a different domain.\nTo test the model’s performance on the domainspecific dataset, we also scrape question-answer pairs from two different subreddits - r/AskLegal and r/AskEconomics. As the name suggests, they contain questions (and answers) from the legal and economics domain respectively. Table 1 includes the statistics of the three datasets."
    }, {
      "heading" : "4 Methodology",
      "text" : "In this section, different components of the DYNAMICTOC are described in details."
    }, {
      "heading" : "4.1 Aspect Detection",
      "text" : "Aspect detection has been popularly used with analysing user reviews to understand their preferences. We leverage an unsupervised technique\n1https://www.sec.gov/edgar.shtml\nfor aspect detection and extend it to a new use case - for the modelling of user profiles from a given document and using this info for segregating the document text based on the determined aspects. Data Pre-processing: For the input SEC filings, text corresponding to each paragraph is obtained and considered as a separate data point. Extra information such as headings, sub-headings, blank lines, signature fields etc. are discarded. Along with this, any paragraph with less than 10 words are discarded. The text is pre-processed before training the model for aspect detection. We require three formatting styles for each paragraph which is consolidated in a single dictionary. (1) Tokenized words converted to lowercase characters. (2) The word stems of the text which has been lower-cased and tokenised. (3) The content words (meaningful words, like nouns, verbs, adjectives and adverbs) of the paragraph.\nProposed Asp-SSCL Method: Aspect detection aims at extracting interpretable aspects from the textual documents without human supervision. We propose an approach, Asp-SSCL based on selfsupervised contrastive learning framework by (Shi et al., 2021) for aspect detection. We use the following steps for aspect detection from the contractual and financial documents:\n1. Vocabulary formation and IDF indexing: First, we obtain a vocabulary for the whole corpus. This is sorted alphabetically and each word is given an index, so that corresponding IDF/word vectors can be easily referenced. 128-dimensional word vectors are generated on the corpus by a skip-gram model with an n-gram size of 5.\n2. Weak Mapping: Prior aspect detection methods require a gold set labels for validating aspect model training, either through human supervision or rules-based mapping to gold set keywords. However, as no such gold aspect labels exist for our case, we first use text embedding via sentence transformers2. These are then clustered using K-means to obtain 20 clusters comprising of 10 keywords each, which are used for aspect mapping.\n3. Contrastive Learning: The mapping and generated word vectors are then used for training the self-supervised contrastive learning method, (Shi et al., 2021), which outputs the final aspect clusters and keywords.\n4. TF-IDF Weighing: Further, since these documents are text-heavy, we introduce a modification,\n2https://www.sbert.net/\nAsp-SSCL-TFIDF which includes TF-IDF weighing term in the original implementation, to ensure rare but relevant words are considered as important as opposed to more frequently occurring words. Each word representation is modified by multiplying it with the TF-IDF score so that the algorithm can adapt to the financial corpus better."
    }, {
      "heading" : "4.2 Persona Mapping",
      "text" : "The aspects generated on the corpus are used as dimensions that define the document. Each persona is expected to be interested in one or more of these dimensions. We call the mapping between multiple personas and multiple aspects as the persona space.\nWe consulted a domain expert (financial domain; specifically for SEC 10-K filings) to get an understanding of the parties or personas who read such documents and what kind of information are they generally interested in. We constructed a matrix listing out the various stakeholders of a general 10-K filing against the different sections of the document each stakeholder is interested in. The stakeholders were grouped together to form the personas used in DYNAMICTOC, viz. employees, business partners, investors and lendors, financial bodies and advisory regulatory firms. Similarly, the columns (headings) were grouped together according to similarity to create a mapping of topics of interest for each persona.\nThe aspects we got from our unsupervised technique were compared against the simplified column values from the constructed matrix. The columns with the greatest similarity (above a threshold) were associated with each persona. For getting the personas interested in each paragraph, the paragraphs were first tagged for aspect. From the resultant vector (which represents the confidence score of the text for each aspect), the combined score for each persona was calculated using the scores of its constituent aspects. This was used to segregate the paragraphs for enhanced document consumption.\nFigure 2 shows the relevance of different content (columns) for the various stakeholders (the rows) for a 10K filing document. Groups of stakeholders are made that form the personas interested in the different parts of the document. The different columns are also grouped together as to indicate what kind of information those sections will contain. Thus, we can map these columns to the aspects we get from the Aspect Detection Module and determine if a particular persona is interested\nin that paragraph or not. The thing to note here is, for financial documents, we were able to get some domain knowledge and leveraged it to obtain the persona space. But the technique we are proposing is generalizable to other domains as well. In the absence of domain specific knowledge, each aspect is a sufficiently distinct topic and can be treated as a proxy to personas. Hence, modelling of interests can be done directly on the basis of aspects in such cases."
    }, {
      "heading" : "4.3 Intelligent Navigation via Question Generation",
      "text" : "It has been shown that question-answers play a critical role in scientific inquiry, information-seeking dialogue, and knowledge acquisition (Hintikka and Saarinen, 1979; Stede and Schlangen, 2004). Additionally, unstructured lists of \"Frequently Asked Questions (FAQs)\" are regularly deployed at scale to present information. On top, questions can provide a meaningful understanding of the document at a paragraph or section level which cannot be directly captured by a heading (or sub-heading). Therefore, to aid in document consumption, we generate long-form questions (i.e., questions based on paragraphs instead of entities) to enhance the navigation experience.\nModel Architecture: We use an encoderdecoder architecture for the task of generating questions given the paragraph as context which essentially is a sequence-to-sequence task. Large pretrained language models like BART (Lewis et al., 2020), PEGASUS (Zhang et al., 2020), etc., have shown excellent results in summarization tasks. Motivated by this, we employ BART as our underlying language model. The task is to generate questions covering the entire paragraph and summarize\nit capturing the most-salient information in form of a question. The BART model has shown promising results in abstractive summarization tasks, making it a natural choice.\nWe use ELI5 (Fan et al., 2019) dataset for training the model. The answer is provided as the input to the encoder-decoder model which is trained to generate the corresponding question and minimize the cross-entropy loss with respect to the ground truth. Although such supervised training is straight-forward, due to the domain shift from ELI5 to financial language, qualitative evaluations showed that the model produced some irrelevant questions, some entities were artificially induced (that it might have seen during the training time) and sometimes, it could not cover the entire paragraph. Hence, we augmented the vanilla BART with three additional rewards targeting the qualities we seek in the final generated questions. We call the resulting model Variant BART. Figure 3 shows the proposed pipeline. The following sections explain these rewards in detail:\n• Answerability Classifier Reward: Qualitative evaluations showed some of the questions generated were not answerable by the para-\n• Entity Correctness Reward: The vanilla BART model can generated questions with hallucinated entities and names like Microsoft, Apple etc. even when there was no mention of them in the corresponding paragraph. To\n3https://yjernite.github.io/lfqa.html\ntackle this, we identify the named entities present in the generated question. If those entities appear in the passage, we give a reward of 1 else 0. If there is no entity in the generated question, a reward of 0.5 is given to the model. Mathematically, reward is given by:\nRentity =  1, if e(Q) ̸= ϕ, e(Q) ⊆ e(P ) 0, if e(Q) ̸= ϕ, e(Q) ̸⊆ e(P ) 0.5, if e(Q) = ϕ\nwhere, e(.) denotes the entities in the question or paragraph.\n• Coverage Reward: We observe that the output question did not cover the entire information present in the paragraph and instead focused on certain segments of it. We introduce this reward to improve information coverage. The idea is similar to the entity correctness reward. We first identify keywords from the paragraph using YAKE algorithm (Campos et al., 2018). Then we calculate the similarity of the generated question with these keywords. We use the Extended String Subsequence Kernel (ESSK) introduced in (Hirao et al., 2003) to calculate this similarity score. The idea is that YAKE would generate keywords from different parts of the paragraph. When we calculate the similarity of this keyword list with the generated question, we are encouraging the model to cover the entire paragraph. Thus, given a passage P and the generated question Q, the reward R is defined as follows:\nRcoverage = ESSK(Y AKE(P ), Q)\n(Lai et al., 2021) shows how to use rewards on top of language models for policy learning. We adopt the same setup. The policy gradient, ∇ϕJ(ϕ) is given by:\n∇ϕJ(ϕ) = E[R · ∇ϕlog(P (ys|x, ϕ))] (1)\nwhere, R denotes reward value, ϕ represents the model parameters, x is the input paragraph and ys is obtained by greedily maximizing the distribution of BART outputs at each timestep. Hence, the overall loss term for training the proposed Variant BART model becomes:\nLtotal = λCE · LCE + λreward · Lreward (2)"
    }, {
      "heading" : "5 Results & Discussion",
      "text" : "In order to evaluate the different parts of the pipeline, we employ metric based evaluation schemes. Since there is no off-the-shelf criterion to evaluate all the stages of the pipeline together, we have provided independent evaluations for each of the sub-modules. However, the intended goal of our pipeline is to facilitate the readers in consuming long documents through the lens they deem most suitable for them. To facilitate an end-to-end evaluation and to understand whether the persona-based document segmentation with enhanced Table of Content is informative or not, we have conducted a small scale human evaluation. Both metric-based and human-based evaluations are discussed in the following subsections."
    }, {
      "heading" : "5.1 Metric-Based Evaluation",
      "text" : "Aspect Detection: Figure 4 shows the t-SNE4 clustering of the outputs of Asp-SSCL, Asp-SSCLTFIDF, that are determined from the SEC-10K filing corpus. For a baseline comparison, we also plot the output for LDA (10 clusters) for the corpus. Essentially, each cluster is a bag of words indicating some vital theme that is mentioned in the corpus. We would want the clusters to be as independent from each other as possible as that would mean different kinds of information is captured by different clusters with minimal overlap. We observe that adding TF-IDF scores to the aspect detection module helps as the clusters’ separation gets better as shown in the Figure 4. Further, for the baseline using LDA, the separation is not clear. Some of the examples of cluster keywords are shown in Figure 5.\nQuestion Generation: Since no ground truth questions are available for the SEC-10K Filing dataset, we report the following evaluations for the question generation module. First, we report the “type” of questions that are generated using the Vanilla BART model and the Variant BART model that is trained with a combination of the rewards we added on top of it (Table 3) .\nOn analysing this table, we see that \"What\" questions are heavily generated on the 10K filing data. This suggests that the nature of 10K filing is such that the question asked about them is \"What\" type. We also observe that biases of training data are not creeping in the model, as the percentage of\n4https://lvdmaaten.github.io/tsne/\nthe \"What\" questions in the training dataset is four times less than the model’s output. Similarly, the percentage of other questions is significant in the ELI5 dataset but is very small in our model’s output. Thus, we can safely say that the model learns to generate questions and not mimic the ELI5 dataset.\nAlthough we don’t have the gold corpus for SEC filing dataset, we evaluate the performance of question generation model on the AskLegal and AskEconomics subreddits since we have the ground truth questions for them. We report the BLEU and ROUGE scores that are standard metrics in Natural Language Processing literature and are a measure of overlap or common n-grams between the gen-\nerated text and the ground truth. We also report the answerability score by feeding the generated question and input para to the classifier we trained (as mentioned in Reward 1 – Question Generation Section). Table 4 shows the corresponding results.\nOn closely analysing the paragraph, reference question, and the generated question, we observe the following three things: (i) There is more than one way to ask the same question and there could be multiple questions around the same topic. (ii) Input paragraphs may have more than one prominent topics. The generated question might be focused on one such topic, and the reference question is focused on another. (iii) Some answers/passages are unrelated to the question or require some background, and thus, the generated questions are very different from the actual question.\nThe above reasons explain the fluctuation in scores for Vanilla and Variant BART, and thus the answerability of the generated question becomes an important metric. The variant model trained with additional rewards has the highest answerability score across all the datasets. This suggests that including a coverage-based loss not only helps cover the information of the entire paragraph but also helps increase the generated question’s answerability as different themes of the passages are covered."
    }, {
      "heading" : "5.2 Human Evaluation",
      "text" : "We conducted a small human evaluation involving 8 participants (age - 27.8 ± 6.7, 2 females). The participants were technology workers internal to\nour organization. They were asked to play around with a web demo to experience the DYNAMICTOC, for different SEC filings. They were first shown the default section heading-based reading experience and then they choose the type of persona as whom they wish to consume the document. After this, they filled a questionnaire about their experience. The results of the survey are summarized in Table 5.\nSome relevant comments from the survey are as follows - (i) How do we ensure that all the relevant information will be covered by the sections highlighted as important for a particular “persona”? (ii) Although questions generated are relevant, some of the why questions are not answered by the paragraphs they point to. (iii) Interesting experiment with possibly multiple use-cases. The first comment is actually true for all summarization tasks, hence, DYNAMICTOC does not disrupt the linear flow. The second feedback indicates scope for further research in the question generation space. The overall response is immensely positive and the scores of 4.50, 4.16 and 4.40 in Table 5 reflect the same."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we have proposed a novel DYNAMICTOC framework for consumption of long documents. Financial documents and contracts are high value documents for business entities, and are often long and complex. The default ToC-based reading experience is quite limited and there are immense opportunities to enhance the document consumption using intelligent technologies. We believe that DYNAMICTOC is one of the first works to pursue this exciting research direction, and would enable further exploration in the area.\nFor the future direction, DYNAMICTOC would benefit from a better supervised in-domain learning of aspect keywords and questions. We would also like to work on evaluation of the paragraph segmentation and mapping of personas to the aspects in future. A better understanding of personas or entities interested in consuming the document would help to generalize the work to different domains."
    } ],
    "references" : [ {
      "title" : "Gti at semeval-2016 task 5: Svm and crf",
      "author" : [ "Tamara Alvarez-López", "Jonathan Juncal-Martínez", "Milagros Fernández-Gavilanes", "Enrique CostaMontenegro", "Francisco Javier" ],
      "venue" : "González-Castano",
      "citeRegEx" : "Alvarez.López et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alvarez.López et al\\.",
      "year" : 2016
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "the Journal of machine Learning research, 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Topicrank: Graph-based topic ranking for keyphrase extraction",
      "author" : [ "Adrien Bougouin", "Florian Boudin", "Béatrice Daille." ],
      "venue" : "International joint conference on natural language processing (IJCNLP), pages 543–551.",
      "citeRegEx" : "Bougouin et al\\.,? 2013",
      "shortCiteRegEx" : "Bougouin et al\\.",
      "year" : 2013
    }, {
      "title" : "Yake! collection-independent automatic keyword extractor",
      "author" : [ "Ricardo Campos", "Vítor Mangaravite", "Arian Pasquali", "Alípio Mário Jorge", "Célia Nunes", "Adam Jatowt." ],
      "venue" : "European Conference on Information Retrieval, pages 806–810. Springer.",
      "citeRegEx" : "Campos et al\\.,? 2018",
      "shortCiteRegEx" : "Campos et al\\.",
      "year" : 2018
    }, {
      "title" : "Hedge trimmer: A parse-and-trim approach to headline generation",
      "author" : [ "Bonnie Dorr David", "David Zajic." ],
      "venue" : "in Proceedings of Workshop on Automatic Summarization. Citeseer.",
      "citeRegEx" : "David and Zajic.,? 2003",
      "shortCiteRegEx" : "David and Zajic.",
      "year" : 2003
    }, {
      "title" : "Eli5: Long form question answering",
      "author" : [ "Angela Fan", "Yacine Jernite", "Ethan Perez", "David Grangier", "Jason Weston", "Michael Auli." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "W2vlda: almost unsupervised system for aspect based sentiment analysis",
      "author" : [ "Aitor García-Pablos", "Montse Cuadros", "German Rigau." ],
      "venue" : "Expert Systems with Applications, 91:127–137.",
      "citeRegEx" : "García.Pablos et al\\.,? 2018",
      "shortCiteRegEx" : "García.Pablos et al\\.",
      "year" : 2018
    }, {
      "title" : "Extracting keyphrases from research papers using citation networks",
      "author" : [ "Sujatha Das Gollapalli", "Cornelia Caragea." ],
      "venue" : "Twenty-eighth AAAI conference on artificial intelligence.",
      "citeRegEx" : "Gollapalli and Caragea.,? 2014",
      "shortCiteRegEx" : "Gollapalli and Caragea.",
      "year" : 2014
    }, {
      "title" : "Multi-style transfer with discriminative feedback on disjoint corpus",
      "author" : [ "Navita Goyal", "Balaji Vasan Srinivasan", "N Anandhavelu", "Abhilasha Sancheti." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Goyal et al\\.,? 2021",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2021
    }, {
      "title" : "An unsupervised neural attention model for aspect extraction",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Informationseeking dialogues: Some of their logical properties",
      "author" : [ "Jaakko Hintikka", "Esa Saarinen." ],
      "venue" : "Studia Logica, 38(4):355–363.",
      "citeRegEx" : "Hintikka and Saarinen.,? 1979",
      "shortCiteRegEx" : "Hintikka and Saarinen.",
      "year" : 1979
    }, {
      "title" : "Ntt’s multiple document summarization system for duc2003",
      "author" : [ "Tsutomu Hirao", "Jun Suzuki", "Hideki Isozaki", "Eisaku Maeda." ],
      "venue" : "Proc. DUC.",
      "citeRegEx" : "Hirao et al\\.,? 2003",
      "shortCiteRegEx" : "Hirao et al\\.",
      "year" : 2003
    }, {
      "title" : "Evaluating rewards for question generation models",
      "author" : [ "Tom Hosking", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1",
      "citeRegEx" : "Hosking and Riedel.,? 2019",
      "shortCiteRegEx" : "Hosking and Riedel.",
      "year" : 2019
    }, {
      "title" : "Improved automatic keyword extraction given more linguistic knowledge",
      "author" : [ "Anette Hulth." ],
      "venue" : "Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 216–223.",
      "citeRegEx" : "Hulth.,? 2003",
      "shortCiteRegEx" : "Hulth.",
      "year" : 2003
    }, {
      "title" : "Opinionminer: a novel machine learning system for web opinion mining and extraction",
      "author" : [ "Wei Jin", "Hung Hay Ho", "Rohini K Srihari." ],
      "venue" : "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages",
      "citeRegEx" : "Jin et al\\.,? 2009",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2009
    }, {
      "title" : "Putting the horse before the cart: A generator-evaluator framework for question generation from text",
      "author" : [ "Vishwajeet Kumar", "Ganesh Ramakrishnan", "YuanFang Li." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Thank you bart! rewarding pre-trained models improves formality style transfer",
      "author" : [ "Huiyuan Lai", "Antonio Toral Ruiz", "Malvina Nissim." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Lai et al\\.,? 2021",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2021
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Structureaware review mining and summarization",
      "author" : [ "Fangtao Li", "Chao Han", "Minlie Huang", "Xiaoyan Zhu", "Yingju Xia", "Shu Zhang", "Hao Yu." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653–",
      "citeRegEx" : "Li et al\\.,? 2010",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised approaches for automatic keyword extraction using meeting transcripts",
      "author" : [ "Feifan Liu", "Deana Pennell", "Fei Liu", "Yang Liu." ],
      "venue" : "Proceedings of human language technologies: The 2009 annual conference of the North American chapter of the asso-",
      "citeRegEx" : "Liu et al\\.,? 2009",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Finegrained opinion mining with recurrent neural networks and word embeddings",
      "author" : [ "Pengfei Liu", "Shafiq Joty", "Helen Meng." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural language processing, pages 1433–1443.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving opinion aspect extraction using semantic similarity and aspect associations",
      "author" : [ "Qian Liu", "Bing Liu", "Yuanlin Zhang", "Doo Soon Kim", "Zhiqiang Gao." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "On learning text style transfer with direct rewards",
      "author" : [ "Yixin Liu", "Graham Neubig", "John Wieting." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Generating news headlines with recurrent neural networks",
      "author" : [ "Konstantin Lopyrev." ],
      "venue" : "arXiv preprint arXiv:1512.01712.",
      "citeRegEx" : "Lopyrev.,? 2015",
      "shortCiteRegEx" : "Lopyrev.",
      "year" : 2015
    }, {
      "title" : "Unsupervised neural aspect extraction with sememes",
      "author" : [ "Ling Luo", "Xiang Ao", "Yan Song", "Jinyao Li", "Xiaopeng Yang", "Qing He", "Dong Yu." ],
      "venue" : "IJCAI, pages 5123–5129.",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "Open domain targeted sentiment",
      "author" : [ "Margaret Mitchell", "Jacqui Aguilar", "Theresa Wilson", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654.",
      "citeRegEx" : "Mitchell et al\\.,? 2013",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2013
    }, {
      "title" : "Multireward reinforced summarization with saliency and entailment",
      "author" : [ "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Pasunuru and Bansal.,? 2018",
      "shortCiteRegEx" : "Pasunuru and Bansal.",
      "year" : 2018
    }, {
      "title" : "Opinion word expansion and target extraction through double propagation",
      "author" : [ "Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen." ],
      "venue" : "Computational linguistics, 37(1):9–27.",
      "citeRegEx" : "Qiu et al\\.,? 2011",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2011
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Interpretation of natural language rules in conversational machine reading",
      "author" : [ "Marzieh Saeidi", "Max Bartolo", "Patrick Lewis", "Sameer Singh", "Tim Rocktäschel", "Mike Sheldon", "Guillaume Bouchard", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 2018 Conference on",
      "citeRegEx" : "Saeidi et al\\.,? 2018",
      "shortCiteRegEx" : "Saeidi et al\\.",
      "year" : 2018
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "ACL (1).",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Automated phrase mining from massive text corpora",
      "author" : [ "Jingbo Shang", "Jialu Liu", "Meng Jiang", "Xiang Ren", "Clare R Voss", "Jiawei Han." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 30(10):1825–1837.",
      "citeRegEx" : "Shang et al\\.,? 2018",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2018
    }, {
      "title" : "Short-text topic modeling via non-negative matrix factorization enriched with local word-context correlations",
      "author" : [ "Tian Shi", "Kyeongpil Kang", "Jaegul Choo", "Chandan K Reddy." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference, pages 1105–1114.",
      "citeRegEx" : "Shi et al\\.,? 2018",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple and effective self-supervised contrastive learning framework for aspect detection",
      "author" : [ "Tian Shi", "Liuqing Li", "Ping Wang", "Chandan K Reddy." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13815–13824.",
      "citeRegEx" : "Shi et al\\.,? 2021",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2021
    }, {
      "title" : "What should i ask? using conversationally informative rewards for goal-oriented visual dialog",
      "author" : [ "Pushkar Shukla", "Carlos Elmadjian", "Richika Sharan", "Vivek Kulkarni", "Matthew Turk", "William Yang Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of",
      "citeRegEx" : "Shukla et al\\.,? 2019",
      "shortCiteRegEx" : "Shukla et al\\.",
      "year" : 2019
    }, {
      "title" : "Autoencoding variational inference for topic models",
      "author" : [ "Akash Srivastava", "Charles Sutton." ],
      "venue" : "5th International Conference on Learning Representations.",
      "citeRegEx" : "Srivastava and Sutton.,? 2017",
      "shortCiteRegEx" : "Srivastava and Sutton.",
      "year" : 2017
    }, {
      "title" : "Information-seeking chat: Dialogues driven by topic-structure",
      "author" : [ "Manfred Stede", "David Schlangen." ],
      "venue" : "Proceedings of Catalog (the 8th workshop on the semantics and pragmatics of dialogue; SemDial04). Citeseer.",
      "citeRegEx" : "Stede and Schlangen.,? 2004",
      "shortCiteRegEx" : "Stede and Schlangen.",
      "year" : 2004
    }, {
      "title" : "A graph degeneracy-based approach to keyword extraction",
      "author" : [ "Antoine Tixier", "Fragkiskos Malliaros", "Michalis Vazirgiannis." ],
      "venue" : "Proceedings of the 2016 conference on empirical methods in natural language processing, pages 1860–1870.",
      "citeRegEx" : "Tixier et al\\.,? 2016",
      "shortCiteRegEx" : "Tixier et al\\.",
      "year" : 2016
    }, {
      "title" : "Recursive neural conditional random fields for aspect-based sentiment analysis",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 616–",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain-specific keyphrase extraction",
      "author" : [ "Yi-fang Brook Wu", "Quanzhi Li", "Razvan Stefan Bot", "Xin Chen." ],
      "venue" : "Proceedings of the 14th ACM international conference on Information and knowledge management, pages 283–284.",
      "citeRegEx" : "Wu et al\\.,? 2005",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2005
    }, {
      "title" : "Exploring questionspecific rewards for generating deep questions",
      "author" : [ "Yuxi Xie", "Liangming Pan", "Dongzhe Wang", "Min-Yen Kan", "Yansong Feng." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2534–2546.",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Extracting opinion expressions with semi-markov conditional random fields",
      "author" : [ "Bishan Yang", "Claire Cardie." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
      "citeRegEx" : "Yang and Cardie.,? 2012",
      "shortCiteRegEx" : "Yang and Cardie.",
      "year" : 2012
    }, {
      "title" : "Pegasus: Pre-training with extracted",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter Liu" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Outline generation: Understanding the inherent content structure of documents",
      "author" : [ "Ruqing Zhang", "Jiafeng Guo", "Yixing Fan", "Yanyan Lan", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Addressing semantic drift in question generation for semisupervised question answering",
      "author" : [ "Shiyue Zhang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Zhang and Bansal.,? 2019",
      "shortCiteRegEx" : "Zhang and Bansal.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Keyword detection (Liu et al., 2009; Tixier et al., 2016) & topic modeling (Blei et al.",
      "startOffset" : 18,
      "endOffset" : 57
    }, {
      "referenceID" : 37,
      "context" : "Keyword detection (Liu et al., 2009; Tixier et al., 2016) & topic modeling (Blei et al.",
      "startOffset" : 18,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : ", 2016) & topic modeling (Blei et al., 2003) works aim is to describe the document by a few important words or topics for concise representation.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : ", n-grams or chunks) with heuristic methods (Hulth, 2003; Shang et al., 2018), then rank them in accordance with their importance to the document (Wu et al.",
      "startOffset" : 44,
      "endOffset" : 77
    }, {
      "referenceID" : 31,
      "context" : ", n-grams or chunks) with heuristic methods (Hulth, 2003; Shang et al., 2018), then rank them in accordance with their importance to the document (Wu et al.",
      "startOffset" : 44,
      "endOffset" : 77
    }, {
      "referenceID" : 39,
      "context" : ", 2018), then rank them in accordance with their importance to the document (Wu et al., 2005; Gollapalli and Caragea, 2014; Bougouin et al., 2013).",
      "startOffset" : 76,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : ", 2018), then rank them in accordance with their importance to the document (Wu et al., 2005; Gollapalli and Caragea, 2014; Bougouin et al., 2013).",
      "startOffset" : 76,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : ", 2018), then rank them in accordance with their importance to the document (Wu et al., 2005; Gollapalli and Caragea, 2014; Bougouin et al., 2013).",
      "startOffset" : 76,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "Another task is compact and informative headline generation from a document (David and Zajic, 2003; Lopyrev, 2015).",
      "startOffset" : 76,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "Another task is compact and informative headline generation from a document (David and Zajic, 2003; Lopyrev, 2015).",
      "startOffset" : 76,
      "endOffset" : 114
    }, {
      "referenceID" : 43,
      "context" : "Recently, an Outline Generation task was introduced by (Zhang et al., 2019) as a hierarchical structured prediction problem.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "Early works focused on rule-based approaches using lexicons and dependency relations, and utilize manually defined rules to identify patterns and extract aspects (Qiu et al., 2011; Liu et al., 2016), which require domain-specific knowledge and human expertise.",
      "startOffset" : 162,
      "endOffset" : 198
    }, {
      "referenceID" : 21,
      "context" : "Early works focused on rule-based approaches using lexicons and dependency relations, and utilize manually defined rules to identify patterns and extract aspects (Qiu et al., 2011; Liu et al., 2016), which require domain-specific knowledge and human expertise.",
      "startOffset" : 162,
      "endOffset" : 198
    }, {
      "referenceID" : 14,
      "context" : "Supervised approaches formulate aspect extraction as a sequence labelling problem that can be solved by hidden Markov models (HMM) (Jin et al., 2009), conditional random fields (CRF) (Li et al.",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : ", 2009), conditional random fields (CRF) (Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2012), and recurrent neural networks (RNN) (Wang et al.",
      "startOffset" : 41,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : ", 2009), conditional random fields (CRF) (Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2012), and recurrent neural networks (RNN) (Wang et al.",
      "startOffset" : 41,
      "endOffset" : 104
    }, {
      "referenceID" : 41,
      "context" : ", 2009), conditional random fields (CRF) (Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2012), and recurrent neural networks (RNN) (Wang et al.",
      "startOffset" : 41,
      "endOffset" : 104
    }, {
      "referenceID" : 38,
      "context" : ", 2013; Yang and Cardie, 2012), and recurrent neural networks (RNN) (Wang et al., 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : ", 2013; Yang and Cardie, 2012), and recurrent neural networks (RNN) (Wang et al., 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018; Shi et al., 2018; Alvarez-López et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 171
    }, {
      "referenceID" : 32,
      "context" : "Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018; Shi et al., 2018; Alvarez-López et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018; Shi et al., 2018; Alvarez-López et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 171
    }, {
      "referenceID" : 35,
      "context" : "Recently, deep learning based topic models (Srivastava and Sutton, 2017; Luo et al., 2019; He et al., 2017; Shi et al., 2021) have shown strong performance in extracting coherent aspects in an unsupervised manner.",
      "startOffset" : 43,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "Recently, deep learning based topic models (Srivastava and Sutton, 2017; Luo et al., 2019; He et al., 2017; Shi et al., 2021) have shown strong performance in extracting coherent aspects in an unsupervised manner.",
      "startOffset" : 43,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Recently, deep learning based topic models (Srivastava and Sutton, 2017; Luo et al., 2019; He et al., 2017; Shi et al., 2021) have shown strong performance in extracting coherent aspects in an unsupervised manner.",
      "startOffset" : 43,
      "endOffset" : 125
    }, {
      "referenceID" : 33,
      "context" : "Recently, deep learning based topic models (Srivastava and Sutton, 2017; Luo et al., 2019; He et al., 2017; Shi et al., 2021) have shown strong performance in extracting coherent aspects in an unsupervised manner.",
      "startOffset" : 43,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "Further, it has been shown that question-answers play a critical role in scientific inquiry, informationseeking dialogue, and knowledge acquisition (Hintikka and Saarinen, 1979; Stede and Schlangen, 2004).",
      "startOffset" : 148,
      "endOffset" : 204
    }, {
      "referenceID" : 36,
      "context" : "Further, it has been shown that question-answers play a critical role in scientific inquiry, informationseeking dialogue, and knowledge acquisition (Hintikka and Saarinen, 1979; Stede and Schlangen, 2004).",
      "startOffset" : 148,
      "endOffset" : 204
    }, {
      "referenceID" : 34,
      "context" : "In a dialogue system, question generation is used to obtain specific information from the user or make the conversation more pleasant (Shukla et al., 2019; Saeidi et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : "In a dialogue system, question generation is used to obtain specific information from the user or make the conversation more pleasant (Shukla et al., 2019; Saeidi et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : "Various pre-trained language models like BART (Lewis et al., 2020), PEGASUS (Zhang et al.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "(Kumar et al., 2019) has employed BLEUbased rewards, (Zhang and Bansal, 2019) have used",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 44,
      "context" : ", 2019) has employed BLEUbased rewards, (Zhang and Bansal, 2019) have used",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 40,
      "context" : "answerability rewards, whereas (Xie et al., 2020) has used a combination of fluency, relevance, and answerability rewards.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : "as they have shown competitive results in various natural language generation tasks such as summarization (Pasunuru and Bansal, 2018), style transfer (Liu et al.",
      "startOffset" : 106,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "as they have shown competitive results in various natural language generation tasks such as summarization (Pasunuru and Bansal, 2018), style transfer (Liu et al., 2021; Goyal et al., 2021), question generation (Hosking and Riedel, 2019; Xie et al.",
      "startOffset" : 150,
      "endOffset" : 188
    }, {
      "referenceID" : 8,
      "context" : "as they have shown competitive results in various natural language generation tasks such as summarization (Pasunuru and Bansal, 2018), style transfer (Liu et al., 2021; Goyal et al., 2021), question generation (Hosking and Riedel, 2019; Xie et al.",
      "startOffset" : 150,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "ELI5: We use the ELI5 dataset (Fan et al., 2019) to train the question generation model.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 33,
      "context" : "supervised contrastive learning framework by (Shi et al., 2021) for aspect detection.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 33,
      "context" : "Contrastive Learning: The mapping and generated word vectors are then used for training the self-supervised contrastive learning method, (Shi et al., 2021), which outputs the final aspect clusters and keywords.",
      "startOffset" : 137,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "Large pretrained language models like BART (Lewis et al., 2020), PEGASUS (Zhang et al.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "We use ELI5 (Fan et al., 2019) dataset for training the model.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "We first identify keywords from the paragraph using YAKE algorithm (Campos et al., 2018).",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "We use the Extended String Subsequence Kernel (ESSK) introduced in (Hirao et al., 2003) to calculate this similarity score.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "(Lai et al., 2021) shows how to use rewards on top of language models for policy learning.",
      "startOffset" : 0,
      "endOffset" : 18
    } ],
    "year" : 0,
    "abstractText" : "Long documents like contracts, financial documents, etc., are often tedious to read through. Linearly consuming (via scrolling or navigation through default table of content) these documents is time-consuming and challenging. These documents are also authored to be consumed by varied entities (referred to as persona in the paper) interested in only certain parts of the document. In this work, we describe DYNAMICTOC, a dynamic table of contentbased navigator, to aid in the task of non-linear, persona-based document consumption. DYNAMICTOC highlights sections of interest in the document as per the aspects relevant to different personas. DYNAMICTOC is augmented with short questions to assist the users in understanding underlying content. This uses a novel deep-reinforcement learning technique to generate questions on these persona-clustered paragraphs. Human and automatic evaluations suggest the efficacy of both end-to-end pipeline and different components of DYNAMICTOC.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_198_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Good Night at 4 pm?! Time Expressions in Different Cultures",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Natural language understanding requires the ability to map language such as color descriptions (McMahan and Stone, 2015), spatial instructions (Chen et al., 2019), and gradable adjectives (Shivade et al., 2016) to real-world physical properties. This paper focuses on temporal grounding, particularly mapping time expressions such as “morning” and “evening” to hours in the day. Temporal commonsense reasoning has been gaining traction lately (Zhou et al., 2019; Qin et al., 2021), and this important capability can benefit various temporal tasks such as event ordering and duration prediction.\nOne of the challenges in grounding time expressions to standard times is that such expressions may be interpreted with some variation by different people. Reiter and Sripada (2002) found that human-written weather forecasts exhibited significant individual differences between forecasters in the interpretation of time expressions. One factor for this variation is cultural differences. Vilares and Gómez-Rodríguez (2018) analyzed the time of day in which people from 53 countries posted time-specific greetings such as “good morning” and “good evening” on Twitter. They showed variation in greeting times across languages and cultures, which they connected to known facts and published statistics about cultural differences, such as differences in average wake and sleep times.\nWe propose to re-frame the research question posed by Vilares and Gómez-Rodríguez (2018) as a task of time expression grounding: given a time expression, the goal is to map it to a specific range of hours during the day. For example, what is the range of times referred to by an Italian speaker mentioning pomeriggio (afternoon)? We collected gold standard interpretations from four countries, which indeed exhibited some variation. We then proposed 3 language-agnostic methods based on either a corpus or a language model (LM).\nThe corpus-based method performed well across languages, outperforming the method proposed by Vilares and Gómez-Rodríguez (2018) on 3 out of 4 languages. Encouraged by the performance on the labelled languages, we applied the method to additional 24 unlabelled languages, and analyzed the differences predicted by the models.\nIn the future, we plan to incorporate this method into NLP systems that may benefit from temporal grounding. Areas of future work involve testing our methods on low-resource languages, as well as researching ways to overcome reporting bias (Gordon and Van Durme, 2013): the under-representation of trivial facts in written text. We hope this work would be another small step in the long-term goal of developing culturally-aware commonsense reasoning models (Acharya et al., 2021).1"
    }, {
      "heading" : "2 Data",
      "text" : "We collected gold standard annotations for the start and end times of five time expressions: morning, noon, afternoon, evening, and night. The annotations were collected in Amazon Mechanical Turk (AMT) for English, Hindi, Italian, and Portuguese. We describe the rationale behind the choice of languages (§2.1), the annotation guidelines (§2.2), and the observations from the collected data (§2.3).\n1Our data and code are available at https: //anonymous.4open.science/r/time_ expressions-23F6."
    }, {
      "heading" : "2.1 Choice of Languages",
      "text" : "The languages in this paper are not meant to be a representative sample of all languages. We selected these languages based on the following criteria.\nAvailability of AMT Workers. By and large, AMT does not facilitate filtering workers by the languages in which they are fluent.2 We thus treated country as a proxy for language, e.g. assuming that most workers in Brazil speak Portuguese, while asking workers about their native language. AMT is available at select countries, and the number of workers in each country varies. We got the most responses from US and India (100 each), in line with published analyses of demographics (Difallah et al., 2018) and language demographics in AMT (Pavlick et al., 2014). We collected 91 responses from Brazil and 58 from Italy.\nThe Interplay between Country and Language. We focused on pairs of country and language where most of the country’s population speaks that language, and most of the L1 speakers of the language reside in that country. For instance, 78.1% of US residents speak English at home, and 76.9% of L1 English speakers reside in the US.3 Figure 1 shows\n2There is a recent qualification type for a few languages, such as Chinese and German. It is an expensive filter at an additional $1 fee per HIT. We tried collecting annotations for Chinese in German but got very few responses, likely due to the small number of workers that have these qualifications.\n3Followed by the UK (17.6%), Nigeria (11.05%), Canada (6%), Australia (5%), South Africa (1.47%), Ireland (1.22%),\nthat for 3 out of the 4 countries, the majority of workers indicated they were native speakers of the majority language. The exception is India, which has many languages. Hindi is the most spoken language in India (followed by Bengali: 8% and Telugu: 6.7%) and has the larger Wikipedia corpus and a BERT model. Among the workers from India, 16% indicated they were Hindi speakers.\nWhile the gold standard annotations are limited to 4 languages, the framework we describe in Section 3 is unsupervised and almost entirely language-agnostic. As we discuss in Section 4.3, we applied the model to additional 24 languages, selected based on the availability of a Wikipedia corpus and an LM for that language."
    }, {
      "heading" : "2.2 Annotation Task",
      "text" : "We asked workers to identify their native language, and posed them the following questions regarding each time expression (e.g. noon).\n1. What is the equivalent word for noon in your native language? We allowed workers to check “There is no equivalent expression in my language”.\n2. What is the range of time you consider as noon? Workers were required to indicate the start and end times.\nWe followed with an option to add a time expression in their language that wasn’t mentioned in the HIT as well as free text comments. To ensure the quality of annotations, we required that workers had a 95% approval rate for at least 100 prior HITs."
    }, {
      "heading" : "2.3 Observations",
      "text" : "Figure 4 displays the average start and end time for each country and each time expression. Notably, morning is quite consistent across the different countries and noon is the short period around 12 pm. The variation is higher for afternoon and evening. Many workers from Brazil noted that Portuguese uses the same word for evening and night (noite), and that evening turns quickly into night because of the country’s tropical climate. This results in a very early night time in the annotations (3:16 pm), and high overlap between the afternoon, evening, and night spans.\nWorkers across countries suggested a missing expression that spans the time between midnight and sunrise, which they referred to as “midnight”, “after midnight”, “late night”, “early morning”, and\nand New Zealand (1.1%).\n“dawn”. Other suggestions included “twilight” (6-7 pm, India), “sunrise” (5-6 am, Italy), “late morning” (11-11:59 am, Italy), “after lunch” (1:15-2 pm, Italy), and “late afternoon” (3-4 pm, Italy).\nFinally, some workers commented that the interpretations of time expressions varies in different seasons because of the changes in sunrise and sunset times. The data was collected in October, and although we don’t know the exact location of the workers, we can test the night start and end times against the average October sunrise and sunset times in the capital of each country. Setting aside Brazil that doesn’t distinguish evening and night, there is somewhat of a match between the average sunset time and the average night start time: US: 6:30 pm/6:59 pm, India: 5:52 pm/4:49 pm, and Italy 6:30 pm/6:22 pm. There was no such match between sunrise time and the end of the night or beginning of the morning."
    }, {
      "heading" : "3 Methods",
      "text" : "We define the time expression grounding task: given a time expression, the goal is to predict its start and end times. We developed 3 methods that differ along two dimensions: (1) the source from which the times are learned: a corpus (§3.1) or a language model (§3.2); and (2) whether to compute start and end times directly or indirectly through estimating a distribution of times."
    }, {
      "heading" : "3.1 Extractive Approach",
      "text" : "Estimating Hour Distributions. We search Wikipedia for occurrences of a regular expression that matches a broad range of time formats, including both 24-hour and 12-hour clock formats. For\neach time expression Xi, we compute Di, the distribution of hours from co-occurring time mentions within the same paragraph. For example, given the sentence “See you in the evening, at 19:30” we extract a co-occurrence of “evening” with 7 pm. We used Google Translate to translate the English time expressions to other languages.\nInferring Start and End. To infer the start and end times Si and Ei from Di, we define an optimization problem and formulate it as an integer linear programming (ILP) problem detailed below.\nInput: D1 ... D5: hour distribution per expression Define: // start and end variables (S1, E1) ... (S5, E5), 0 ≤ Si, Ei ≤ 23 Maximize:∑\ni ∑ hWithinRange(h, Si, Ei) ·Di[h]\nConstrained to: // start before end except at night ∀i=1,...,4Si < Ei, S5 < E5 + 24 // sort expressions ∀i=1,...,4Si+1 ≥ Ei The goal is to find a global solution for all the\ntime expressions, with non-overlapping time ranges in which the expressions are sorted, e.g. morning comes before noon. We maximize the number of observations in Di that are within the inferred start and end times.4"
    }, {
      "heading" : "3.2 LM-Based Approach",
      "text" : "We used multilingual BERT (mBERT; Devlin et al., 2019), a single BERT model trained on Wikipedia\n4We also tried to extract start and end times directly from the corpus, but the signal was too sparse.\nin multiple languages that achieves strong zeroshot cross-lingual transfer performance (Wu and Dredze, 2019).\nMethod 1: Estimating Hour Distributions. For each time expression, we query BERT for substitutes for the masked token in each template in the top part of Table 1. We translated the templates to other languages using Google Translate.5\nSince LM predictions are sensitive to the prompt, we follow Jiang et al. (2020) and aggregate the predictions across these various templates. We also allow for various time formats. For example, we query BERT for the substitutes of each of “It is [MASK]:00 in the morning”, “It is [MASK].00 in the morning”, and “It is [MASK] in the morning”. We sum the distributions and normalize the scores for all numbers within the range of 0 and 23.\nFor languages spoken mostly in countries where 12-hour clock is the norm, we computed the distribution for hours in the range of 0 and 12.6 We then assigned each hour back into the template and predicted whether the next token is more likely to be am or pm (or its equivalent in the target language). For example, if BERT assigned 9:00 a score of 0.3 in the morning distribution, and the query “It is 9:00 [MASK] in the morning” predicted am with a score of 0.9 and pm with 0.1, then in the final 24-hour clock distribution, 9 has a score of 0.3 · 0.9 = 0.27 and 21 has a score of 0.3 · 0.1 = 0.03.\nFinally, we use the same ILP formulation to infer the start and end times from the hour distributions.\nMethod 2: Directly Predict Start and End Times. For each time expression, we separately query BERT for the substitutes of the masked tokens in\n5For better translation quality, we assigned “morning” into the <time_exp> placeholder and “9:00” into the [MASK] placeholder.\n6In this paper, such languages are English and Hindi.\nthe start template and end template in the bottom part of Table 1. We apply the same processing as described above. The output of this step is a start time distribution SDi and an end time distribution EDi over 24 hours for each time expression Xi. We infer the start and end times with the same optimization problem, but with a slightly modified objective detailed below. The objective is to select the most highly scored start and end time for each expression, that adhere to the same constraints. Maximize:∑\ni ∑ h(1(Si == h)·SDi[h]+1(Ei == h)·EDi[h])"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Baseline",
      "text" : "Our baseline is based on the Greetings method proposed by Vilares and Gómez-Rodríguez (2018). Their study focused on 4 out of the 5 time expressions used in our paper: morning, afternoon, evening, and night. We use their dataset and induce the corresponding time expression distributions. We focus on tweets in English from the US (1.34M), Portuguese from Brazil (2M), Italian from Italy (4,821), and Hindi from India (6,069). We then infer the start and end times using the ILP problem in Section 3.1. Although the dataset does not include statistics for “noon” (due to the lack of a corresponding greeting), the global objective in the ILP formulation is expected to infer the start and end times for noon based on the surrounding time expressions."
    }, {
      "heading" : "4.2 Results",
      "text" : "Figure 3 displays the predicted start and end times for each expression according to each method, in comparison to the gold standard times of each language. For quantitative evaluation, we define minute-level accuracy. We classify each minute of the day to a time expression based on the start and end times, and compute the accuracy compared to the gold standard minute classification. Table 2 shows the accuracy as well as the average differences in hours between the predicted and gold standard start (∆Start) and end (∆End) times.\nThere is a general preference for the extractive method, that achieves between 65% and 90% accuracy across languages. The exception is Portuguese, where this method performs worse than the others, and in particular by the LM Start-End method that performs remarkably well. The two LM-based methods perform substantially worse on\nthe other languages. Finally, the results for India are surprisingly not bad despite the mismatch between the native languages of the annotators and the language used by our methods."
    }, {
      "heading" : "4.3 Application to Other Languages",
      "text" : "We applied our proposed methods to additional unlabelled languages detailed in Table 3. The languages are sorted according to their Wikipedia corpus size. The Table shows the predicted start and end time for each language and each time expression. Without labelled data it is hard to judge the\ncorrectness of the predictions, but the predictions of some languages seem more reasonable than others. In particular, we observed that some time expressions appeared in the corpus more frequently than others, causing the model to dedicate most of the 24 hours to such expressions. The percent column in Table 3 show the percent of all corpus occurrences dedicated to each expression. For instance, 81.9% of the occurrences found for Finnish are for night, and the model predicted a 20 hour night. It could be a result of the extremely short days in Finland during the winter, but this is likely exaggerated by the bias in corpus occurrences."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Uniformity of Time Distributions",
      "text" : "Figure 4 presents the hour distribution for each expression in Italian, as estimated using the extractive (blue) and LM-Dist (orange) methods. As the figure demonstrates, the LM-predicted distribution is more uniform than the extractive one. This is true across most languages: the average entropy of the extractive distributions across languages is 2.78± 0.3, and 3.07± 0.08 for the LM-Based distributions. For comparison, a uniform distribution across all 24 hours yields an entropy of 3.18.\nThe uniform distributions predicted by BERT are possibly caused by the similarity between the different inputs (time expressions) and the different outputs (numbers). Previous work showed that BERT confuses semantically-similar but mutuallyexclusive concepts such as colors (Shwartz and Choi, 2020). The representation of numbers in distributional models is also suboptimal (Naik et al., 2019; Thawani et al., 2021)."
    }, {
      "heading" : "5.2 Analysis of Extracted Sentences",
      "text" : "We sample 25 English sentences extracted by the extractive method (§3.1), and examine whether they are valid, manually categorizing the errors. Table 4 presents the percents of each category, along with representative examples. In accordance with the results in Table 2, most of the extractions were valid. Among the errors, 4 sentences contained reference errors, for instance reporting on someone being injured in the morning and dying at another time of the day a few days later. Three sentences\nincluded a citation from the Bible or the New Testament, treating the chapter and verse separated by a colon as a time mention.\nWe repeated the same analysis for languages spoken by members of our research group: Chinese, Korean, Russian, Hebrew, and Italian. The percent of valid sentences ranged from 52% (Chinese) to 80% (Korean). Across languages, reference was a common error in longer paragraphs, but in preliminary experiments we found that splitting the paragraphs to sentences yields a sparse signal. In Chinese, that uses both 12-hour and 24-hour notations, the 12-hour clock was sometimes used without specifying am or pm in unambiguous contexts such as “5:00 in the afternoon”. In Hebrew, the word for “evening” has a rarer meaning of “before” which led to WSD error. In Korean, we translated “afternoon” to오후 that more broadly means “pm”."
    }, {
      "heading" : "5.3 Similarity Across Languages",
      "text" : "Using the predictions from the extractive method (§3.1), we compute the accuracy of predicting the start and end times of each language from the times of each other language. Figure 5 shows a heatmap of the most similar and most dissimilar languages with respect to time ranges.\nThe most similar language pairs in terms of time ranges are pairs of closely related languages: Norwegian and Swedish (100%) followed by Por-\ntuguese and Spanish (92%). In particular, the latter two don’t distinguish evening from night.\nThe similarity between Italian and Chinese (92%) might be explained by the similarity between the average times of waking up and going to bed in both countries: both Italian men and Chinese women go to sleep close to midnight and wake up around 7:30 on average (Walch et al., 2016).\nFinally, Hindi and Ukrainian have similar predictions as well (92%), but considering the extremely early night start time predicted for both (2 pm and 1 pm), we conjecture that this is mostly due to noise in the data. The same pattern emerges between pairs of dissimilar languages such as Czech and Russian or Farsi and Polish (36%), where the model of each language devotes most of its 24 hours to a single time expression."
    }, {
      "heading" : "6 Related Work",
      "text" : "Temporal Commonsense. Work on temporal reasoning ranges from extracting and normalizing temporal expressions (Strötgen and Gertz, 2010; Angeli et al., 2012; Vashishtha et al., 2019), to inferring possibly explicit temporal attributes of events, including their order (Ning et al., 2018; Vashishtha et al., 2019), duration (Chambers and Jurafsky, 2008; Vashishtha et al., 2019), and typical times or frequencies (Zhou et al., 2019).\nVarious benchmarks were proposed to measure models’ temporal reasoning abilities. The bAbI suite contains a task that requires reasoning about the order of time expressions (Weston et al., 2015). MC-TACO is a reading comprehension task pertaining to ordering, duration, stationarity, frequency, and typical times of events (Zhou et al., 2019). TIMEDIAL (Qin et al., 2021) is a dialogue QA task focusing on temporal commonsense. Zhou et al. (2021) and Thukral et al. (2021) both cast the temporal ordering task as an NLI task. In another line of work, tracking state changes in procedural text is also related to temporal ordering (Dalvi et al., 2018; Zhang et al., 2020). Despite the success of pre-trained LMs on language understanding tasks, their performance on these benchmarks is limited, maybe due to the fact that many temporal relations are not explicitly stated in text (Davis and Marcus, 2015). A promising direction is to train LMs explicitly on temporal knowledge (Zhou et al., 2020).\nCultural Commonsense. There is little focus on cultural differences in NLP in general (Hovy and Yang, 2021) and in research about commonsense reasoning in particular. Recently, Acharya et al. (2021) made a first step in addressing this gap. They surveyed crowdsourcing workers in the US and India regarding rituals that are commonly found across cultures such as birth, marriage, and funerals. In particular, they asked questions pertaining to temporal aspects such as typical time\nand duration of each event. The paper mentions anecdotal differences such that a wedding lasts a few hours in the US but a few days in India.\nAlthough there is no direct mapping between culture and language, one can often teach about the other. For example, in ConceptNet, a multilingual commonsense knowledge base, the English entry for breakfast specifies pancakes as breakfast food, while the Chinese entry mentions noodles (Speer et al., 2017).\nLanguage Grounding and World Knowledge. Our work is related to language grounding (Roy and Reiter, 2005) and to extracting world knowledge from text corpora (Carlson et al., 2010; Tandon et al., 2014). In the intersection of these two lines of work, Forbes and Choi (2017) extracted from a corpus physical commonsense knowledge about actions and objects along five dimensions (size, weight, strength, rigidness, and speed), while Elazar et al. (2019) induced distributions of typical values of various quantitative attributes such as time, duration, length, and speed. Elazar et al. (2019) mention cultural differences that arose when crowdsourcing workers were asked to estimate whether an item’s price was expensive or not: annotators from India judged prices differently from annotators in the US."
    }, {
      "heading" : "7 Discussion and Conclusion",
      "text" : "We addressed the task of grounding time expressions such as “morning” and “noon” in different languages to explicit hours. Our extractive method achieves good performance on languages for which we collected gold annotations. We dedicate the remainder of the paper to discuss various limitations and considerations for future work.\nTemporal and Seasonal Factors. As discussed in §2.3, some workers mentioned that their interpretation of time expressions depends on the season, e.g., night starts earlier in the winter in the Northern Hemisphere. In addition, the time of day in which the workers answered the survey might have introduced some bias. The batches were published according to the authors’ timezone and working hours, which might have been outside working hours for some countries. An early riser answering an AMT survey at 5 am or a night owl that answers it at 2 am might not be representative of the population. Finally, Vilares and Gómez-Rodríguez (2018) showed that tweets greeting “good morning”\nappeared later in the day during weekends and holidays, indicating later wake up times. It is possible that such factors will also affect the judgement of survey respondents.\nReporting Bias. Every method that learns about the world from texts (or from language models, trained on text corpora), suffers from reporting bias (Gordon and Van Durme, 2013; Shwartz and Choi, 2020). The frequency of occurrences in a corpus is an imperfect proxy for measuring the quantity or frequency of things in the world. In our case, it may be that some hours are less spoken of in general: perhaps fewer newsworthy events happen late at night? Some time expressions might be less ambiguous than others and therefor appear less frequently with an exact time mention.\nInducing time distributions from greetings also confounds other cultural factors such as politeness. The mapping between greetings and time expressions is not perfect, e.g. as Vilares and GómezRodríguez (2018) note, “bonjour” in French means “good morning” but is also used throughout the day to mean “hello”. Finally, Twitter memes might use a greeting with a different intention, as in the famous “good morning to everyone except” meme.7\nDifferences in Performance across Languages. While the methods in this paper are languageagnostic, they don’t produce equally good predictions for all languages. Beyond the differences in the set of commonly used time expressions in each language (e.g., “evening” being missing from Spanish, or “dawn” being commonly used in other languages), time might also be discussed differently in different languages. In some languages it may be more common to use cardinals to discuss hours, as in “It is two in the afternoon”. Finally, the success of our methods also depends on the availability of large text corpora and the quality of the LM. We used mBERT because it is available for 104 languages, but we focused on relatively highresource languages. This model doesn’t perform equally well across all languages (Wu and Dredze, 2020). In the future, we plan to find alternative sources for collecting gold standard annotations for additional languages, which will facilitate evaluating the performance of our methods on a broader range of languages.\n7For instance, several tweets from early 2021 with the hashtag #FreeBritney read “Good morning to everyone except Jamie Spears.”"
    } ],
    "references" : [ {
      "title" : "Towards an atlas of cultural commonsense for machine reasoning",
      "author" : [ "Anurag Acharya", "Kartik Talamadupula", "Mark A Finlayson." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Acharya et al\\.,? 2021",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2021
    }, {
      "title" : "Parsing time: Learning to interpret time expressions",
      "author" : [ "Gabor Angeli", "Christopher Manning", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Angeli et al\\.,? 2012",
      "shortCiteRegEx" : "Angeli et al\\.",
      "year" : 2012
    }, {
      "title" : "Toward an architecture for never-ending language learning",
      "author" : [ "Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka", "Tom M Mitchell." ],
      "venue" : "Twenty-Fourth AAAI conference on artificial intelligence.",
      "citeRegEx" : "Carlson et al\\.,? 2010",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised learning of narrative event chains",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 789–797, Columbus, Ohio. Association for Computational Linguistics.",
      "citeRegEx" : "Chambers and Jurafsky.,? 2008",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2008
    }, {
      "title" : "Touchdown: Natural language navigation and spatial reasoning in visual street environments",
      "author" : [ "Howard Chen", "Alane Suhr", "Dipendra Misra", "Noah Snavely", "Yoav Artzi." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension",
      "author" : [ "Bhavana Dalvi", "Lifu Huang", "Niket Tandon", "Wen-tau Yih", "Peter Clark." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Dalvi et al\\.,? 2018",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2018
    }, {
      "title" : "Commonsense reasoning and commonsense knowledge in artificial intelligence",
      "author" : [ "Ernest Davis", "Gary Marcus." ],
      "venue" : "Commun. ACM, 58(9):92–103.",
      "citeRegEx" : "Davis and Marcus.,? 2015",
      "shortCiteRegEx" : "Davis and Marcus.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Demographics and dynamics of mechanical turk workers",
      "author" : [ "Djellel Difallah", "Elena Filatova", "Panos Ipeirotis." ],
      "venue" : "Proceedings of the eleventh ACM international conference on web search and data mining, pages 135–143.",
      "citeRegEx" : "Difallah et al\\.,? 2018",
      "shortCiteRegEx" : "Difallah et al\\.",
      "year" : 2018
    }, {
      "title" : "How large are lions? inducing distributions over quantitative",
      "author" : [ "Yanai Elazar", "Abhijit Mahabal", "Deepak Ramachandran", "Tania Bedrax-Weiss", "Dan Roth" ],
      "venue" : null,
      "citeRegEx" : "Elazar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Elazar et al\\.",
      "year" : 2019
    }, {
      "title" : "Verb physics: Relative physical knowledge of actions and objects",
      "author" : [ "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 266–276, Vancouver, Canada.",
      "citeRegEx" : "Forbes and Choi.,? 2017",
      "shortCiteRegEx" : "Forbes and Choi.",
      "year" : 2017
    }, {
      "title" : "Reporting bias and knowledge acquisition",
      "author" : [ "Jonathan Gordon", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25–30.",
      "citeRegEx" : "Gordon and Durme.,? 2013",
      "shortCiteRegEx" : "Gordon and Durme.",
      "year" : 2013
    }, {
      "title" : "The importance of modeling social factors of language: Theory and practice",
      "author" : [ "Dirk Hovy", "Diyi Yang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hovy and Yang.,? 2021",
      "shortCiteRegEx" : "Hovy and Yang.",
      "year" : 2021
    }, {
      "title" : "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Infographic: A world of languagesand how many speak them",
      "author" : [ "AL López." ],
      "venue" : "retrieved november 8, 2015.",
      "citeRegEx" : "López.,? 2015",
      "shortCiteRegEx" : "López.",
      "year" : 2015
    }, {
      "title" : "A Bayesian model of grounded color semantics",
      "author" : [ "Brian McMahan", "Matthew Stone." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:103– 115.",
      "citeRegEx" : "McMahan and Stone.,? 2015",
      "shortCiteRegEx" : "McMahan and Stone.",
      "year" : 2015
    }, {
      "title" : "Exploring numeracy in word embeddings",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Carolyn Rose", "Eduard Hovy." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374–3380, Florence, Italy. Asso-",
      "citeRegEx" : "Naik et al\\.,? 2019",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint reasoning for temporal and causal relations",
      "author" : [ "Qiang Ning", "Zhili Feng", "Hao Wu", "Dan Roth." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2278–2288, Melbourne, Aus-",
      "citeRegEx" : "Ning et al\\.,? 2018",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2018
    }, {
      "title" : "The language demographics of Amazon Mechanical Turk",
      "author" : [ "Ellie Pavlick", "Matt Post", "Ann Irvine", "Dmitry Kachaev", "Chris Callison-Burch." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:79–92.",
      "citeRegEx" : "Pavlick et al\\.,? 2014",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2014
    }, {
      "title" : "TIMEDIAL: Temporal commonsense reasoning in dialog",
      "author" : [ "Lianhui Qin", "Aditya Gupta", "Shyam Upadhyay", "Luheng He", "Yejin Choi", "Manaal Faruqui." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Qin et al\\.,? 2021",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "Squibs and discussions: Human variation and lexical choice",
      "author" : [ "Ehud Reiter", "Somayajulu Sripada." ],
      "venue" : "Computational Linguistics, 28(4):545–553.",
      "citeRegEx" : "Reiter and Sripada.,? 2002",
      "shortCiteRegEx" : "Reiter and Sripada.",
      "year" : 2002
    }, {
      "title" : "Connecting language to the world",
      "author" : [ "Deb Roy", "Ehud Reiter." ],
      "venue" : "Artificial Intelligence, 167(1):1–12. Connecting Language to the World.",
      "citeRegEx" : "Roy and Reiter.,? 2005",
      "shortCiteRegEx" : "Roy and Reiter.",
      "year" : 2005
    }, {
      "title" : "Identification, characterization, and grounding of gradable terms in clinical text",
      "author" : [ "Chaitanya Shivade", "Marie-Catherine de Marneffe", "Eric Fosler-Lussier", "Albert M. Lai." ],
      "venue" : "Proceedings of the 15th Workshop on Biomedical Natural Language Process-",
      "citeRegEx" : "Shivade et al\\.,? 2016",
      "shortCiteRegEx" : "Shivade et al\\.",
      "year" : 2016
    }, {
      "title" : "Do neural language models overcome reporting bias? In Proceedings of the 28th International Conference on Computational Linguistics, pages 6863–6870, Barcelona, Spain (Online)",
      "author" : [ "Vered Shwartz", "Yejin Choi." ],
      "venue" : "International Committee on Compu-",
      "citeRegEx" : "Shwartz and Choi.,? 2020",
      "shortCiteRegEx" : "Shwartz and Choi.",
      "year" : 2020
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Thirty-first AAAI conference on artificial intelligence",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "HeidelTime: High quality rule-based extraction and normalization of temporal expressions",
      "author" : [ "Jannik Strötgen", "Michael Gertz." ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 321–324, Uppsala, Sweden. Association for",
      "citeRegEx" : "Strötgen and Gertz.,? 2010",
      "shortCiteRegEx" : "Strötgen and Gertz.",
      "year" : 2010
    }, {
      "title" : "Acquiring comparative commonsense knowledge from the web",
      "author" : [ "Niket Tandon", "Gerard De Melo", "Gerhard Weikum." ],
      "venue" : "Twenty-Eighth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Tandon et al\\.,? 2014",
      "shortCiteRegEx" : "Tandon et al\\.",
      "year" : 2014
    }, {
      "title" : "Representing numbers in NLP: a survey and a vision",
      "author" : [ "Avijit Thawani", "Jay Pujara", "Filip Ilievski", "Pedro Szekely." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Thawani et al\\.,? 2021",
      "shortCiteRegEx" : "Thawani et al\\.",
      "year" : 2021
    }, {
      "title" : "Probing language models for understanding of temporal expressions",
      "author" : [ "Shivin Thukral", "Kunal Kukreja", "Christian Kavouras." ],
      "venue" : "Blackbox NLP workshop.",
      "citeRegEx" : "Thukral et al\\.,? 2021",
      "shortCiteRegEx" : "Thukral et al\\.",
      "year" : 2021
    }, {
      "title" : "Fine-grained temporal relation extraction",
      "author" : [ "Siddharth Vashishtha", "Benjamin Van Durme", "Aaron Steven White." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919, Florence, Italy. Asso-",
      "citeRegEx" : "Vashishtha et al\\.,? 2019",
      "shortCiteRegEx" : "Vashishtha et al\\.",
      "year" : 2019
    }, {
      "title" : "Grounding the semantics of part-of-day nouns worldwide using Twitter",
      "author" : [ "David Vilares", "Carlos Gómez-Rodríguez." ],
      "venue" : "Proceedings of the Second Workshop on Computational Modeling of People’s",
      "citeRegEx" : "Vilares and Gómez.Rodríguez.,? 2018",
      "shortCiteRegEx" : "Vilares and Gómez.Rodríguez.",
      "year" : 2018
    }, {
      "title" : "A global quantification of “normal” sleep schedules using smartphone data",
      "author" : [ "Olivia J Walch", "Amy Cochran", "Daniel B Forger." ],
      "venue" : "Science advances, 2(5):e1501705.",
      "citeRegEx" : "Walch et al\\.,? 2016",
      "shortCiteRegEx" : "Walch et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merriënboer", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1502.05698.",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120–130, Online",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Wu and Dredze.,? 2020",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2020
    }, {
      "title" : "Reasoning about goals, steps, and temporal ordering with WikiHow",
      "author" : [ "Li Zhang", "Qing Lyu", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4630–4639, Online. As-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "going on a vacation” takes longer than “going for a walk”: A study of temporal commonsense understanding",
      "author" : [ "Ben Zhou", "Daniel Khashabi", "Qiang Ning", "Dan Roth." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "Temporal common sense acquisition with minimal supervision",
      "author" : [ "Ben Zhou", "Qiang Ning", "Daniel Khashabi", "Dan Roth." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7579–7589, Online. Association for",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Temporal reasoning on implicit events from distant supervision",
      "author" : [ "Ben Zhou", "Kyle Richardson", "Qiang Ning", "Tushar Khot", "Ashish Sabharwal", "Dan Roth." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Natural language understanding requires the ability to map language such as color descriptions (McMahan and Stone, 2015), spatial instructions (Chen et al.",
      "startOffset" : 95,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "Natural language understanding requires the ability to map language such as color descriptions (McMahan and Stone, 2015), spatial instructions (Chen et al., 2019), and gradable adjectives (Shivade et al.",
      "startOffset" : 143,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : ", 2019), and gradable adjectives (Shivade et al., 2016) to real-world physical properties.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 36,
      "context" : "Temporal commonsense reasoning has been gaining traction lately (Zhou et al., 2019; Qin et al., 2021), and this important capability can benefit various temporal tasks such as event ordering and duration prediction.",
      "startOffset" : 64,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "Temporal commonsense reasoning has been gaining traction lately (Zhou et al., 2019; Qin et al., 2021), and this important capability can benefit various temporal tasks such as event ordering and duration prediction.",
      "startOffset" : 64,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "We hope this work would be another small step in the long-term goal of developing culturally-aware commonsense reasoning models (Acharya et al., 2021).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 14,
      "context" : "Numbers in brackets: (1) percents of native speakers of the target language (in orange) living in this country (López, 2015); and (2) percents of the country’s population that speaks this language at home (from Wikipedia).",
      "startOffset" : 111,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "responses from US and India (100 each), in line with published analyses of demographics (Difallah et al., 2018) and language demographics in AMT (Pavlick et al.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 18,
      "context" : ", 2018) and language demographics in AMT (Pavlick et al., 2014).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "We used multilingual BERT (mBERT; Devlin et al., 2019), a single BERT model trained on Wikipedia",
      "startOffset" : 26,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "in multiple languages that achieves strong zeroshot cross-lingual transfer performance (Wu and Dredze, 2019).",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "Previous work showed that BERT confuses semantically-similar but mutuallyexclusive concepts such as colors (Shwartz and Choi, 2020).",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "The representation of numbers in distributional models is also suboptimal (Naik et al., 2019; Thawani et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "The representation of numbers in distributional models is also suboptimal (Naik et al., 2019; Thawani et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 115
    }, {
      "referenceID" : 31,
      "context" : "The similarity between Italian and Chinese (92%) might be explained by the similarity between the average times of waking up and going to bed in both countries: both Italian men and Chinese women go to sleep close to midnight and wake up around 7:30 on average (Walch et al., 2016).",
      "startOffset" : 261,
      "endOffset" : 281
    }, {
      "referenceID" : 25,
      "context" : "Work on temporal reasoning ranges from extracting and normalizing temporal expressions (Strötgen and Gertz, 2010; Angeli et al., 2012; Vashishtha et al., 2019), to inferring possibly explicit temporal attributes of events, including their order (Ning et al.",
      "startOffset" : 87,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "Work on temporal reasoning ranges from extracting and normalizing temporal expressions (Strötgen and Gertz, 2010; Angeli et al., 2012; Vashishtha et al., 2019), to inferring possibly explicit temporal attributes of events, including their order (Ning et al.",
      "startOffset" : 87,
      "endOffset" : 159
    }, {
      "referenceID" : 29,
      "context" : "Work on temporal reasoning ranges from extracting and normalizing temporal expressions (Strötgen and Gertz, 2010; Angeli et al., 2012; Vashishtha et al., 2019), to inferring possibly explicit temporal attributes of events, including their order (Ning et al.",
      "startOffset" : 87,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : ", 2019), to inferring possibly explicit temporal attributes of events, including their order (Ning et al., 2018; Vashishtha et al., 2019), duration (Chambers and Jurafsky, 2008; Vashishtha et al.",
      "startOffset" : 93,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : ", 2019), to inferring possibly explicit temporal attributes of events, including their order (Ning et al., 2018; Vashishtha et al., 2019), duration (Chambers and Jurafsky, 2008; Vashishtha et al.",
      "startOffset" : 93,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : ", 2019), duration (Chambers and Jurafsky, 2008; Vashishtha et al., 2019), and typical times or frequencies (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 72
    }, {
      "referenceID" : 29,
      "context" : ", 2019), duration (Chambers and Jurafsky, 2008; Vashishtha et al., 2019), and typical times or frequencies (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 72
    }, {
      "referenceID" : 36,
      "context" : ", 2019), and typical times or frequencies (Zhou et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 32,
      "context" : "suite contains a task that requires reasoning about the order of time expressions (Weston et al., 2015).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 36,
      "context" : "MC-TACO is a reading comprehension task pertaining to ordering, duration, stationarity, frequency, and typical times of events (Zhou et al., 2019).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "TIMEDIAL (Qin et al., 2021) is a dialogue QA task focusing on temporal commonsense.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "In another line of work, tracking state changes in procedural text is also related to temporal ordering (Dalvi et al., 2018; Zhang et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 144
    }, {
      "referenceID" : 35,
      "context" : "In another line of work, tracking state changes in procedural text is also related to temporal ordering (Dalvi et al., 2018; Zhang et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "Despite the success of pre-trained LMs on language understanding tasks, their performance on these benchmarks is limited, maybe due to the fact that many temporal relations are not explicitly stated in text (Davis and Marcus, 2015).",
      "startOffset" : 207,
      "endOffset" : 231
    }, {
      "referenceID" : 37,
      "context" : "A promising direction is to train LMs explicitly on temporal knowledge (Zhou et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "There is little focus on cultural differences in NLP in general (Hovy and Yang, 2021) and in research about commonsense reasoning in particular.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "For example, in ConceptNet, a multilingual commonsense knowledge base, the English entry for breakfast specifies pancakes as breakfast food, while the Chinese entry mentions noodles (Speer et al., 2017).",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 21,
      "context" : "Our work is related to language grounding (Roy and Reiter, 2005) and to extracting world knowledge from text corpora (Carlson et al.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Our work is related to language grounding (Roy and Reiter, 2005) and to extracting world knowledge from text corpora (Carlson et al., 2010; Tandon et al., 2014).",
      "startOffset" : 117,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "Our work is related to language grounding (Roy and Reiter, 2005) and to extracting world knowledge from text corpora (Carlson et al., 2010; Tandon et al., 2014).",
      "startOffset" : 117,
      "endOffset" : 160
    }, {
      "referenceID" : 23,
      "context" : "Every method that learns about the world from texts (or from language models, trained on text corpora), suffers from reporting bias (Gordon and Van Durme, 2013; Shwartz and Choi, 2020).",
      "startOffset" : 132,
      "endOffset" : 184
    }, {
      "referenceID" : 34,
      "context" : "This model doesn’t perform equally well across all languages (Wu and Dredze, 2020).",
      "startOffset" : 61,
      "endOffset" : 82
    } ],
    "year" : 0,
    "abstractText" : "We propose the task of culture-specific time expression grounding, i.e. mapping from expressions such as “morning” in English or “manhã” in Portuguese to specific hours in the day. We propose 3 language-agnostic methods, one of which achieves promising results on gold standard annotations that we collected for a small number of languages. We then apply this method to 28 languages and analyze the similarities across languages in the grounding of time expressions.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_11_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Feasibility Study of Answer-Unaware Question Generation for Education",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Writing good questions that target salient concepts is difficult and time consuming. Automatic Question Generation (QG) is a powerful tool that could be used to significantly lessen the amount of time it takes to write such questions. A QG system that automatically generates relevant questions from textbooks would help professors write quizzes faster and help students spend more time reviewing flashcards rather than writing them.\nPrevious work on QG has focused primarily on answer-aware QG models. These models require the explicit selection of an answer span in the input context, typically through the usage of highlight tokens. This adds significant overhead to the question generation process and is undesirable in cases where clear lists of salient key terms are unavailable. We conduct a feasibility study on the application of answer-unaware question generation models (ones which do not require manual selection of answer spans) to an educational context. Our contributions are as follows:\n• We show that the primary way answerunaware QG models fail is by generating irrelevant or un-interpretable questions.\n• We show that giving answer-unaware QG models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% -> 83%).\n• We show that, in the absence of humanwritten summaries, providing automatically generated summaries as input is a good alternative."
    }, {
      "heading" : "2 Related Work & Background",
      "text" : "Early attempts to use QG for educational applications involved generating gap-fill or “cloze\" questions1 (Taylor, 1953) from textbooks (Agarwal and Mannem, 2011). One may optionally choose to generate distractors to make these questions multiple choice (Narendra et al., 2013; Correia et al.,\n1For example, Q: “Dynamic Programming was introduced in ____\" A: 1957\n2012). This procedure has been shown to be effective in classroom settings (Zavala and Mendoza, 2018) and students’ scores on this style of generated question correlate positively with their scores on human-written questions (Guo et al., 2016). However, there are many situations where gap-fill questions are not effective, as they are only able to ask about specific unambiguous key terms.\nIn recent years, with the advent of large crowdsourced datasets for extractive question answering (QA) such as SQuAD (Rajpurkar et al., 2018), neural models have become the primary methods of choice for generating traditional interrogative style questions (Kurdi et al., 2019). A common task formulation for neural QG is to phrase the task as answer-aware, that is, given a context passage C = {c0, ..., cn} and an answer span within this context A = {ck, ..., ck+l} such that k ≥ 0 and k + l ≤ n, train a model to maximize P (Q|A,C) where Q = {q0, ..., qm} are the tokens in the question. These models are typically evaluated using n-gram overlap metrics such as BLEU/ROUGE/METEOR (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005) with the reference being the original human-authored question as provided by the extractive QA dataset.\nThe feasibility of using answer-aware neural QG in an educational setting was investigated by Wang et al. (2018), who used a BiLSTM encoder (Zhang et al., 2015) to encode C and A and a unidirectional LSTM decoder to generate Q. They trained on the SQuAD dataset (Rajpurkar et al., 2018) and evaluated on textbooks from various domains (history, sociology, biology). They showed that generated questions were largely grammatical, relevant, and had high n-gram overlap with humanauthored questions. However, given that we may not always have a convenient list of key terms to use as answer spans for an input passage, there is a desire to move past answer-aware QG models and evaluate the feasibility of answer-unaware models for use in education.\nShifting to answer-unaware models creates new challenges. As Vanderwende (2008) claims, the task of deciding what is and is not important is, itself, an important task. Without manually selected answer spans to guide it, an answer-unaware model must itself decide what is and is not important enough to ask a question about. Previous work primarily accomplishes this by separately modeling P (A|C), i.e. which spans in the input context\nare most likely to be used as answer targets for questions. We can then take these extracted answer spans and give them as input to an answer-aware QG model P (Q|A,C). This modeling choice allows for more controllable QG and more direct modeling of term salience.\nPrevious work done by Subramanian et al. (2018) trained a BiLSTM Pointer Network (Vinyals et al., 2015) for answer extraction and showed that it outperformed an entity-based baseline when predicting answer spans from SQuAD passages. However, their human evaluation centered around question correctness and fluency rather than relevance of answer selection. Similar follow-up studies also fail to explicitly ask human annotators whether or not the extracted answers, and subsequent generated questions, were relevant to the broader topic of the context passage (Willis et al., 2019; Cui et al., 2021; Wang et al., 2019; Du and Cardie, 2018; Alberti et al., 2019; Back et al., 2021).\nIn our study, we explicitly ask annotators to determine whether or not a generated question is relevant to the topic of the textbook chapter from which it is generated. In addition, we show that models trained for answer extraction on SQuAD frequently select irrelevant or ambiguous answers when applied to textbook material. We show that summaries of input passages can be used instead of the original text to aid in the modeling of topic salience and that questions generated from humanwritten and automatically-generated summaries are more relevant, interpretable, and acceptable."
    }, {
      "heading" : "3 Methodology",
      "text" : "To perform answer-unaware QG, we take inspiration from work done by Dong et al. (2019) and Bao et al. (2020) who show that large language models,\nwhen fine-tuned for both QA and QG, perform better than models tuned for only one of those tasks. We assume that answer extraction will help both QA and QG and therefore use a model that was fine-tuned on all three. We chose a version of the T5 language model (Raffel et al., 2020) fine-tuned on SQuAD due to the clean separation between tasks afforded by T5’s task-specific prefixes such as “generate question:\" and “extract answer:\".2\nThe three fine-tuning tasks that were used to train our model are illustrated in Figure 2. For question generation, the model is trained to perform answer-aware question generation by modeling P (Q|A,C). For question answering, the model is trained to perform extractive QA by modeling P (A|C,Q). Finally, for answer extraction, instead of directly modeling P (A|C), a new context C ′ = {c0, ..., cs, ..., ce, ..., cn+2} is generated where cs and ce are highlight tokens that denote the start (s) and end (e) of the sub-sequence within which we want to extract an answer span. The answer extraction fine-tuning task thus becomes modeling P (A|C ′) where A = {ck, ..., ck+l} such that k ≥ s and k + l ≤ e.\nBecause T5 has a fixed maximum context length of 512 tokens, input passages that contain n > 512 tokens must be split up into smaller sub-passages. We perform this splitting such that no sentences are divided between sub-passages and all sub-passages have a roughly equal number of sentences.3 Finally, to generate questions, we iteratively choose the start and end of each sentence in a given subpassage as our cs and ce and extract at most one answer span per sentence.4 We then generate one question per extracted answer span using the same model in an answer-aware fashion.\n2https://huggingface.co/valhalla/t5-base-qa-qg-hl 3Sentence boundaries are determined by NLTK 4If the generated answer span tokens are not sequentially\npresent in the highlighted sentence, the answer is discarded"
    }, {
      "heading" : "4 Experiments",
      "text" : "Our first experiment evaluates the performance of the model on the original text extracted from Jurafsky and Martin (2009)’s textbook “Speech and Language Processing 3rd Edition\".5 To ensure proper comparison, we manually extracted the text from our three chapters of interest (Chapters 2, 3, and 4). When extracting text, all figures, tables, and equations were omitted and all references to them were either replaced with appropriate parenthetical citations or removed when possible. In total, we generated 1208 question-answer pairs from the original text.\nOur second experiment evaluates the performance of the model on human-written summaries. We asked three research assistants (RAs) to write summaries for each subsection of the same three chapters (2-4) of the textbook. These RAs were encouraged to make these summaries easily readable by humans rather than to be easily understandable by machines. From these 3 sets of summaries we generated a total of 667 question-answer pairs.\nOur final experiment evaluates the performance of the model on automatically generated summaries. To perform this automatic summarization we used a BART (Lewis et al., 2019) language model which was fine-tuned for summarization on the CNN/DailyMail dataset (Nallapati et al., 2016).6 The same chunking procedure as described in Section 3 was performed on input passages that were larger than 512 tokens. The summarized output sub-passages were then concatenated together\n5https://web.stanford.edu/ jurafsky/slp3/ 6https://huggingface.co/facebook/bart-large-cnn\nbefore running question generation. In total we generated 318 question-answer pairs from our automatic summaries."
    }, {
      "heading" : "5 Evaluation",
      "text" : "For evaluation, we randomly sampled 100 questionanswer pairs from each of the three experiments to construct our evaluation set of 300 questions. We recruited three expert annotators, all undergraduates in computer science, to evaluate the quality of the question-answer pairs. All 300 pairs were given to all three annotators. We asked the annotators to answer the following yes/no questions: a) Would you directly use this question as a flashcard?, b) Is this question grammatical?, c) Does this question make sense out of context?, d) Is this question relevant? and e) Is the answer to this question correct? We report these in our tables as “Acceptable?\", “Grammatical?\", “Interpretable?\", “Relevant?\", and “Correct?\" respectively. We provided many annotation examples to our annotators and wrote clear guidelines about each category to ensure high agreement. Our full annotator guidelines can be found in Appendix A.\nIn Figure 3 we report the results of our evaluation across the three experiments. We note that a majority of observed errors in the original text questions stem from them being either irrelevant or un-interpretable out of context. We also see that generating questions directly from human-written summaries significantly improves relevance and incontext interpretability, resulting in over 80% being labeled as acceptable by annotators. Finally, in the case of automatic summaries, we see that relevance and in-context interpretability are somewhat improved as compared to the original text questions while grammaticality suffers slightly.\nIn Table 1 we report the distribution of scores across chapters. We note that scores are largely consistent across the three chapters, with lower average relevance for Chapter 2 questions likely owing to the source material containing many worked examples of regular expressions and application-specific details.\nIn Table 2 we report the per-annotator statistics as well as the pairwise inter-annotator agreement (IAA). While at first glance it may seem that agreement is low for grammaticality and correctness, this is somewhat expected for highly unbalanced classes (Artstein and Poesio, 2008). For the other three categories we see an average pairwise agree-\nment of approximately 0.4 which suggests a fairly large degree of agreement for such a seemingly amorphous and ambiguous category. Examples of questions for each category on which there was significant disagreement are listed in Appendix B."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this work we show that answer-unaware QG models have difficulty both choosing relevant topics to ask about and generating questions that are interpretable out of context. We show that asking questions on summarized text ameliorates this in large part and that these gains can be approximated by the use of automatic summarization.\nFuture work should seek to further explore the relationship between summarization and QG. Work done concurrently to ours by Lyu et al. (2021) already has promising results in this direction, showing that training a QG model on synthetic data from summarized text improves performance on downstream QA.\nAdditionally, future work should focus on further refining and standardizing the metrics used for both automatic and human evaluation of QG. As noted by Nema and Khapra (2018) n-gram overlap metrics correlate poorly with in-context interpretability and evaluation on downstream QA fails to address the relevance of generated questions."
    }, {
      "heading" : "A Annotator Guidelines",
      "text" : "In Table 3 we report the annotation guidelines given to our annotators. In the original document, under each category, 3 or more example annotations were given, each containing an explanation as to why the selection was made. Categories such as grammaticality had upwards of 10 or more examples given\nto ensure maximum possible agreement between annotators. Several discussion sessions were held between the authors and annotators to ensure that these guidelines were well understood and that they were sensible for the task.\nDuring annotation, annotators were not given the original source text from which the question was generated. Instead, they were given the original textbook chapters to use as reference material for relevance and were allowed to use online search engines to check for grammaticality and correctness."
    }, {
      "heading" : "B Example Disagreements",
      "text" : "In Table 4 we list questions for which there was at least one dissenting annotator for the given category.\nWe see that for categories such as “Relevant?\" and “Interpretable?\", annotations are often dependent on the level of granularity with which the topic is being discussed. For example, a question such as “Who named the minimum edit distance algorithm?\" may or may not be relevant depending on how granular of a class the student is taking.\nFor categories such as “Correct?\" or “Acceptable?\" certain particularities about otherwise good questions can easily disqualify them from receiving a positive annotation. In the case of “What NLP algorithms require algorithms for word segmentation?\", keen-eyed annotators would notice that the question is non-sensical, however others may note that both Japanese and Thai do, in fact, require word segmentation. Particularities such as these make this task very difficult, even for expert annotators.\nWe provide our full annotation data in CSV form in the supplementary material for further inspection."
    } ],
    "references" : [ {
      "title" : "Automatic gap-fill question generation from text books",
      "author" : [ "Manish Agarwal", "Prashanth Mannem." ],
      "venue" : "Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 56–64, Portland, Oregon. Association for Com-",
      "citeRegEx" : "Agarwal and Mannem.,? 2011",
      "shortCiteRegEx" : "Agarwal and Mannem.",
      "year" : 2011
    }, {
      "title" : "Synthetic QA corpora generation with roundtrip consistency",
      "author" : [ "Chris Alberti", "Daniel Andor", "Emily Pitler", "Jacob Devlin", "Michael Collins." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for 4",
      "citeRegEx" : "Alberti et al\\.,? 2019",
      "shortCiteRegEx" : "Alberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Survey article: Inter-coder agreement for computational linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Computational Linguistics, 34(4):555–596.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "Learning to generate questions by learning to recover answercontaining sentences",
      "author" : [ "Seohyun Back", "Akhil Kedia", "Sai Chetan Chinthakindi", "Haejun Lee", "Jaegul Choo." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
      "citeRegEx" : "Back et al\\.,? 2021",
      "shortCiteRegEx" : "Back et al\\.",
      "year" : 2021
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Unilmv2: Pseudo-masked language models for unified language model pre-training",
      "author" : [ "Hangbo Bao", "Li Dong", "Furu Wei", "Wenhui Wang", "Nan Yang", "Xiaodong Liu", "Yu Wang", "Songhao Piao", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "ICML.",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic generation of cloze question stems",
      "author" : [ "Rui Correia", "Jorge Baptista", "Maxine Eskenazi", "Nuno Mamede." ],
      "venue" : "International Conference on Computational Processing of the Portuguese Language, pages 168–178. Springer.",
      "citeRegEx" : "Correia et al\\.,? 2012",
      "shortCiteRegEx" : "Correia et al\\.",
      "year" : 2012
    }, {
      "title" : "Onestop qamaker: Extract question-answer pairs from text in a one-stop approach",
      "author" : [ "Shaobo Cui", "Xintong Bao", "Xinxing Zu", "Yangyang Guo", "Zhongzhou Zhao", "Ji Zhang", "Haiqing Chen." ],
      "venue" : "ArXiv, abs/2102.12128.",
      "citeRegEx" : "Cui et al\\.,? 2021",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2021
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "M. Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "ArXiv, abs/1905.03197.",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Harvesting paragraph-level question-answer pairs from Wikipedia",
      "author" : [ "Xinya Du", "Claire Cardie." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1907–1917, Mel-",
      "citeRegEx" : "Du and Cardie.,? 2018",
      "shortCiteRegEx" : "Du and Cardie.",
      "year" : 2018
    }, {
      "title" : "Questimator: Generating knowledge assessments for arbitrary topics",
      "author" : [ "Qi Guo", "Chinmay Kulkarni", "Aniket Kittur", "Jeffrey P. Bigham", "Emma Brunskill." ],
      "venue" : "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI’16,",
      "citeRegEx" : "Guo et al\\.,? 2016",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Speech and language processing (Prentice Hall series in Artificial Intelligence)",
      "author" : [ "Daniel Jurafsky", "James H Martin." ],
      "venue" : "Prentice Hall NJ.",
      "citeRegEx" : "Jurafsky and Martin.,? 2009",
      "shortCiteRegEx" : "Jurafsky and Martin.",
      "year" : 2009
    }, {
      "title" : "Automatic cloze-questions genera",
      "author" : [ "shit Shah" ],
      "venue" : null,
      "citeRegEx" : "Shah.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shah.",
      "year" : 2013
    }, {
      "title" : "Bleu: a method for automatic evalu",
      "author" : [ "Jing Zhu" ],
      "venue" : null,
      "citeRegEx" : "Zhu.,? \\Q2002\\E",
      "shortCiteRegEx" : "Zhu.",
      "year" : 2002
    }, {
      "title" : "Exploring the limits",
      "author" : [ "Wei Li", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Li and Liu.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li and Liu.",
      "year" : 2020
    }, {
      "title" : "Neural models for key phrase extraction and question generation",
      "author" : [ "Sandeep Subramanian", "Tong Wang", "Xingdi Yuan", "Saizheng Zhang", "Adam Trischler", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Workshop on Machine Reading for Question Answering,",
      "citeRegEx" : "Subramanian et al\\.,? 2018",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2018
    }, {
      "title" : "cloze procedure”: A new tool for measuring readability",
      "author" : [ "Wilson L. Taylor." ],
      "venue" : "Journalism Quarterly, 30(4):415–433.",
      "citeRegEx" : "Taylor.,? 1953",
      "shortCiteRegEx" : "Taylor.",
      "year" : 1953
    }, {
      "title" : "The importance of being important: Question generation",
      "author" : [ "Lucy Vanderwende." ],
      "venue" : "Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge, Arlington, VA.",
      "citeRegEx" : "Vanderwende.,? 2008",
      "shortCiteRegEx" : "Vanderwende.",
      "year" : 2008
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "A multi-agent communication framework for question-worthy phrase extraction and question generation",
      "author" : [ "Siyuan Wang", "Zhongyu Wei", "Zhihao Fan", "Yang Liu", "Xuanjing Huang." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Qg-net: A data-driven question generation model for educational content",
      "author" : [ "Zichao Wang", "Andrew S. Lan", "Weili Nie", "Andrew E. Waters", "Phillip J. Grimaldi", "Richard G. Baraniuk." ],
      "venue" : "Proceedings of the Fifth Annual ACM Conference on Learning at",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Key phrase extraction for generating educational question-answer pairs",
      "author" : [ "Angelica Willis", "Glenn M. Davis", "Sherry Ruan", "Lakshmi Manoharan", "James A. Landay", "Emma Brunskill." ],
      "venue" : "Proceedings of the Sixth (2019) ACM Conference on Learning @",
      "citeRegEx" : "Willis et al\\.,? 2019",
      "shortCiteRegEx" : "Willis et al\\.",
      "year" : 2019
    }, {
      "title" : "On the use of semantic-based aig to automatically generate programming exercises",
      "author" : [ "Laura Zavala", "Benito Mendoza." ],
      "venue" : "Proceedings of the 49th ACM Technical Symposium on Computer Science Education, SIGCSE ’18, page 14–19, New York, NY, USA.",
      "citeRegEx" : "Zavala and Mendoza.,? 2018",
      "shortCiteRegEx" : "Zavala and Mendoza.",
      "year" : 2018
    }, {
      "title" : "Bidirectional long short-term memory networks for relation classification",
      "author" : [ "Shu Zhang", "Dequan Zheng", "Xinchen Hu", "Ming Yang." ],
      "venue" : "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, pages 73–78, Shanghai,",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Early attempts to use QG for educational applications involved generating gap-fill or “cloze\" questions1 (Taylor, 1953) from textbooks (Agarwal and Mannem, 2011).",
      "startOffset" : 105,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Early attempts to use QG for educational applications involved generating gap-fill or “cloze\" questions1 (Taylor, 1953) from textbooks (Agarwal and Mannem, 2011).",
      "startOffset" : 135,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "2018) and students’ scores on this style of generated question correlate positively with their scores on human-written questions (Guo et al., 2016).",
      "startOffset" : 129,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "BLEU/ROUGE/METEOR (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005) with the reference being the original human-authored question as provided by the extractive QA dataset.",
      "startOffset" : 18,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "(2018), who used a BiLSTM encoder (Zhang et al., 2015) to encode C and A and a unidirectional LSTM decoder to generate Q.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "(2018) trained a BiLSTM Pointer Network (Vinyals et al., 2015) for answer extraction and showed that it outperformed an entity-based baseline when predicting answer spans from SQuAD passages.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "questions, were relevant to the broader topic of the context passage (Willis et al., 2019; Cui et al., 2021; Wang et al., 2019; Du and Cardie, 2018; Alberti et al., 2019; Back et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "questions, were relevant to the broader topic of the context passage (Willis et al., 2019; Cui et al., 2021; Wang et al., 2019; Du and Cardie, 2018; Alberti et al., 2019; Back et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 189
    }, {
      "referenceID" : 19,
      "context" : "questions, were relevant to the broader topic of the context passage (Willis et al., 2019; Cui et al., 2021; Wang et al., 2019; Du and Cardie, 2018; Alberti et al., 2019; Back et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "questions, were relevant to the broader topic of the context passage (Willis et al., 2019; Cui et al., 2021; Wang et al., 2019; Du and Cardie, 2018; Alberti et al., 2019; Back et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : "questions, were relevant to the broader topic of the context passage (Willis et al., 2019; Cui et al., 2021; Wang et al., 2019; Du and Cardie, 2018; Alberti et al., 2019; Back et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : "questions, were relevant to the broader topic of the context passage (Willis et al., 2019; Cui et al., 2021; Wang et al., 2019; Du and Cardie, 2018; Alberti et al., 2019; Back et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 189
    }, {
      "referenceID" : 2,
      "context" : "While at first glance it may seem that agreement is low for grammaticality and correctness, this is somewhat expected for highly unbalanced classes (Artstein and Poesio, 2008).",
      "startOffset" : 148,
      "endOffset" : 175
    } ],
    "year" : 0,
    "abstractText" : "We conduct a feasibility study into the applicability of answer-unaware question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% -> 83%) as determined by expert annotators. We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.",
    "creator" : null
  }
}
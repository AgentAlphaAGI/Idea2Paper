{
  "name" : "ARR_2022_201_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MPII: Multi-Level Mutual Promotion for Inference and Interpretation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, the interpretability of neural networks has been of increasing concern. In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).\nAlthough prior works have made some progress towards interpretable NLP, they tend to provide interpretations that lack human-readability. Existing interpretable models usually extract prominent features or select input key words as explanations, such as attention distribution (Xu et al., 2015), heatmap (Samek et al., 2017), alignment rationale (Jiang et al., 2021), gradients (Li et al.,\n2016), magnitude of hidden states (Linzen et al., 2016), etc. Considering readability and comprehensibleness for human, some works turn to generate token-level explanations (Liu et al., 2018; Thorne et al., 2019), which nevertheless prone to cause ambiguity. Figure 1 shows some prevalent forms of interpretations in NLI task. Obviously, human language interpretations seem more acceptable than those chaotic maps, whether it is heatmap or alignment map. As for the token-level interpretation, several discrete tokens without any logical links are vague and ambiguous. Moreover, Thorne et al. (2019) observed that token-level methods tend to predict common tokens (e.g. people, man, dog) rather than keywords. Intuitively, human language sentence-level interpretations containing reasoning logic is the best form for human to understand.\nWith the annotated natural language interpreta-\ntion datasets available (Camburu et al., 2018; Rajani et al., 2019), methods of generating sentencelevel interpretation have been explored recently. Camburu et al. (2018) proposed to first generate interpretation and then predict the label only based on the generated interpretation. Kumar and Talukdar (2020) proposed to first generate sentence-level interpretation with deep pre-trained language models (such as BERT and GPT), then fed those interpretation as extra knowledge to help improve inference performance. We notice that these methods only include one-side promotion: utilizing information contained in interpretation to improve inference, while ignoring the other-side promotion: using inference logic to enhance interpretation. As claimed in Kumar and Talukdar (2020) that their one-side promotion improves predictions’ faithfulness to generated interpretations, then the otherside should be able to improve interpretation’s faithfulness to inference process. This has aroused our thinking: Can we deeply fuse these two relevant tasks with ingenious combination skills and achieve mutual promotion for inference and interpretation?\nIn this paper, we propose a multi-level Mutual Promotion mechanism for self-evolved Inference and sentence-level Interpretation (MPII). Specifically, from the model-level, we propose a Stepwise Integration Mechanism (SIM) to iteratively update the inference prediction and generate an interpretation token at each decoding step, and deeply integrate hidden representations of the prediction and the token with two fusion modules. In this way, the model learns to refine the inference conclusion as the interpretation proceeds, and the inference procedure can in turn guide the generation of interpretation at each decoding step. From the optimization-level, we propose an Adversarial Fidelity Regularization (AFiRe) to improve the fidelity between inference and interpretation with the Adversarial Mutual Information (AMI) method (Pan et al., 2020), which extends the maximum mutual information optimization objective with the idea of generative adversarial network (Goodfellow et al., 2014). With this training framework, the model is trained against a smart backward network that learns to reward the inference prediction and interpretation of fidelity, which ensures faithfulness and makes the derived interpretation depict the true profile of how the model works (Jiang et al., 2021).\nTo verify the effectiveness of MPII, we conduct extensive experiments on two inference tasks: Nat-\nural Language Inference (NLI) task and Commonsense Question Answering (CQA) task. Experiment results reveal that compared with baseline models, our method can achieve mutual promotion on both model inference performance and sentencelevel interpretation quality. Meanwhile, through providing simultaneous inference prediction and human-comprehensible interpretation with deep integration mechanism and adversarial training strategy, our model can perform inference and interpretation of fidelity and generate more robust explanations. Main contributions of this work include:\n• Different from the previous works that only include one-side promotion, we mutually promote the inference and sentence-level interpretation from both the model-level and the optimization-level.\n• We propose a Stepwise Integration Mechanism to tightly fuse latent prediction and interpretation information at every decoding step, and an Adversarial Fidelity Regularization to further improve the fidelity with the adversarial training strategy.\n• Experiment results show that our method achieve significant improvement in both inference accuracy and interpretation quality compared with baseline models."
    }, {
      "heading" : "2 Methodology",
      "text" : "In this section, we introduce Stepwise Integration Mechanism (SIM) and Adversarial Fidelity Regularization (AFiRe) in details. Utilizing the autoregressive nature of Transformer decoder, SIM allows deep interaction at every decoding step between inference and interpretation. With the adversarial training strategy, AFiRe allows further integration of latent semantic information between inference and interpretation, and also improves the quality of explanation sentences, bringing them closer to human expressions."
    }, {
      "heading" : "2.1 Task Description",
      "text" : "Transformer model (Vaswani et al., 2017) has been firmly established as the dominant approach in text generation tasks. Given a sequence of tokens as input X = {x0, x1, ..., xm} (e.g. for NLI: X = {[CLS]+Premise+[SEP]+Hypothesis}, for CQA: X = {[CLS] + Question + [SEP] + Answers}), Transformer encoder produces a sequence of continuous vectors Henc. Conditioned on Henc, on\neach decoding step, Transformer decoder takes the embedding of words generated by previous steps as input and predict the word for current step.\nWith ground truth prediction L and explanation E from dataset annotated by human, the interpretable model is required to generate prediction L′ and explanation sentence E′ = {e′0, e′1, ..., e′n} simultaneously."
    }, {
      "heading" : "2.2 Stepwise Integration Mechanism",
      "text" : "Prevalent interpretable models share the same encoder and separately adopt a MLP and a decoder to generate predictions and explanations. We analogously adopt the standard Transformer encoder, but apply Stepwise Integration Mechanism to deeply integrate standard MLP and Transformer decoder at every decoding step to simultaneously produce predictions and explanations.\nAs depicted in Figure 2, at decoding step t, decoder takes the last generated token e′t−1 and the predicted label l′t−1 at previous step as input. At the first decoding step, we pass the encoder hidden state corresponding to [CLS] token into MLP to get the l′0. We project the label l ′ t−1 with Multi-Layer Perceptrons (MLP) and obtain vpt−1, which represents the previous step prediction information. We\nthen fuse the prediction information vpt−1 and the explanation token e′t−1 with gate mechanism. The gate probability at t step is computed by:\np′t = ReLU(W1 [Emb l′ t−1;Emb e′ t−1] + b1) (1)\npt = σ(W2 p ′ t + b2) (2)\nwhere “;” means concatenation, W1, W2, b1 and b2 are trainable parameters. ReLU(.) here denotes the ReLU activation function (Nair and Hinton, 2010), σ(.) represents the sigmoid function. We fuse the prediction and interpretation information as below:\nEmbt = ptEmb l′ t−1 + (1− pt)Embe ′ t−1 (3)\nwhere Embt contains the information of prediction and the overall explanation sub-sequence generated in all previous steps.\nWe utilize the stack of masked self-attention layers fsa used in Transformer decoder to compute the decoder hidden states:\n{h0,h1, ...,ht} = fsa({Emb0,Emb1, ...,Embt}) (4)\nThe attention vector referring to the source sequence is computed with multi-head attention:\nvt = fmha(Henc,ht) (5)\nwhere Henc represents the encoder hidden states, fmha denotes the multi-head attention module. The vt is further passed into a fully connected layer followed with softmax function to obtain the vocabulary distribution of generated explanation token e′t at t step:\ne′t = argmax(softmax(Wvt + b)) (6)\nwhere W and b are both trainable parameters. The gate mechanism is then used to integrate the explanation information to update the prediction information:\npt = σ(MLP1([Emb l′ t−1;MLP2(vt)])) (7)\nwhere the two MLP(.) use different parameters.\nEmbl ′ t = Emb l′ t−1 + ptMLP3(vt) (8)\nWe apply the residual connection (He et al., 2016) here, which is easier to optimize in the scenario of many decoding steps. This is similar to the gate mechanism used in Long Short-Term Memory\n(LSTM) (Hochreiter and Schmidhuber, 1997) that learns to remember important information obtained on each decoding step. At the last decoding step, the model deduces the eventual decision:\nL′ = argmax(softmax(Embl ′ t )) (9)\nwhere n is the length of the generated explanation E′. With this setting, both prediction and explanation are updated at every decoding step. Step-by-step interpretation helps the model to better inference, stepwise inference in turn guides the generation of better explanation."
    }, {
      "heading" : "2.3 Adversarial Fidelity Regularization",
      "text" : "From the level of optimization objective, we further introduce the Adversarial Fidelity Regularization (AFiRe) to improve the fidelity of inference and interpretation. We leverage the Adversarial Mutual Information (AMI) method (Pan et al., 2020) to extend the maximum mutual information objective among input, inference prediction and the generated explanation with the idea of generative adversarial network (Goodfellow et al., 2014).\nCompared to the maximum likelihood estimation (MLE) objective, maximum mutual information (MMI) objective encourages the model to generate the prediction and explanation that are more faithful to the input (Kinney and Atwal, 2014; Stratos, 2019). The mutual information I(X,L,E) among the input X , inference label L and explanation E is formulated as:\nI(X,L,E) = EP (X,L,E) [ log P (X,L,E)\nP (X)P (L,E) ] = H(X)−H(X|L,E)\n, where H denotes the entropy. Because of the intractability of directly estimating the mutual information in high-dimensional\nspace, we approximate the optimization objective with a Variational Information Maximization (Chen et al., 2016b; Zhang et al., 2018) lower bound (Poole et al., 2019):\nI(X, L, E) = H(X) + EP (X,L,E) [logP (X|L,E)] = H(X) + EP (X,L,E) [logQϕ(X|L,E)]\n+ EP (L,E) [KL(P (X|L,E)||Qϕ(X|L,E))] ≥ H(X) + EP (X)EPθ(L,E|X) [logQϕ(X|L,E)]\n, where KL(·||·) denotes the Kullback-Leibler (KL) divergence between two distribution. Pθ(L,E|X) and Qϕ(X|L,E) denote the forward network (generating L,R conditioned on X) and the backward network (generating X conditioned on L,E) respectively.\nSince the entropy term H(X) associates with the training data and does not involve the parameters we optimize, the objective of MMI is equivalent as:\nmax θ,ϕ\nE(L′,E′)∼Pθ(L′,E′|X) [ logQϕ(X|L′, E′) ] , where θ and ϕ are the parameters of the forward and backward network respectively. L′ and E′ represent the synthetic prediction label and explanation generated by the forward network.\nWith the MMI optimization objective, the backward network is trained with only the synthetic label and explanation produced by the forward network, and prone to sub-optimal performance if the synthetic text is uninformative. Since the the backward network provides a reward for optimizing the forward network, a biased backward network may provide unreliable reward scores and mislead the forward network optimization.\nTo remedy this problem, we leverage the Adversarial Mutual Information (AMI) method (Pan et al., 2020) to extend MMI with the idea of generative adversarial network (Goodfellow et al., 2014).\nSpecifically, we first bring the min-max adversarial game into training procedure and add an additional objective term Qϕ(X|L,E) to maximize the negative likelihood of Qϕ when feeding it with the real data:\nmin ϕ max θ\nE(L′,E′)∼Pθ(L′,E′|X) [ logQϕ(X|L′, E′) ] − Qϕ(X|L,E)\nWith this interactive training strategy and regularizing the backward network with both the synthetic data and real data, the forward network will be trained against a smarter backward network that only rewards prediction and explanation of fidelity.\nWe also add an objective term Pθ(L,E|X) of maximum the negative likelihood of Pθ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017). The final optimization objective is formulated as:\nmin ϕ max θ Pθ(L,E|X) + Mutual Information︷ ︸︸ ︷︸ ︷︷ ︸\nAdversarial Training\nE(L′,E′)∼Pθ(L′,E′|X) [ logQϕ(X|L′, E′) ] − Qϕ(X|L,E)\nAs depicted in Fig 3, to encourage the forward network to learn a stronger connection between generated explanations and model predictions, we also add Qϕ(X|L,E′) as negative samples for backward network. This explicitly encourages the backward network to be capable of punishing the Pθ when it generates unfaithful explanations."
    }, {
      "heading" : "3 Experiments",
      "text" : "We intend to verify the mutual promotion effect of SIM and AFiRe on the inference ability and interpretablity of model. We choose two tasks requiring inference ability: Natural Language Inference (NLI) and Commonsense Question Answering (CQA)."
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI(Camburu et al., 2018), CQA (Talmor et al., 2018), CoS-E (Rajani et al., 2019), MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al., 2014).\nSNLI is a standard benchmark for NLI task, while e-SNLI extends it with human-annotated natural language explanations for each sentence pair. CoS-E1 dataset extends CQA dataset with natural language explanations for each QA sample. MultiNLI is another large-scale NLI corpus, which includes a diverse range of genres. SICKe(Sentences Involving Compositional Knowledge for entailment) provides sentence pairs that are rich in the lexical, syntactic and semantic phenomena. The latter two datasets are used for out-of-domain test."
    }, {
      "heading" : "3.2 Baselines",
      "text" : "NLI: We use e-INFERSENT and Transformer as two baseline models for NLI task. The eINFERSENT model adds a LSTM decoder into INFERSENT (Conneau et al., 2017) for explanations. The classification module and the explanation generation module are separated but share the same\n1https://github.com/salesforce/cos-e\nencoder. The Transformer model (Vaswani et al., 2017) adds a MLP layer for generating sentencelevel interpretations. With this baseline, we aim to test whether vanilla transformer without further interaction can achieve good results.\nCQA: We use CAGE (Rajani et al., 2019) as the baseline model for CQA task. CAGE adopts the explain-then-predict approach, which firstly finetunes a deep pretrained language model GPT (Radford et al., 2019) to generate explanations, then use a classifier to predict the inference label with the generated explanation and source text as the input."
    }, {
      "heading" : "3.3 Metrics",
      "text" : "To evaluate inference performance, we report Taskspecific Accuracy (NLI Accuracy and CQA Accuracy). To evaluate the quality of generated interpretation, we report BLEU (similarity between generation and ground truth), PPL (fluency of generated sentences), and Inter Repetition (diversity of generated explanations)."
    }, {
      "heading" : "3.4 Main Results",
      "text" : "Table 1 shows automatic evaluation results on the SNLI and CQA datasets with the annotated explanation from the e-SNLI and CoS-E datasets. Compared with the baseline models, our MPII method can achieve significant performance improvement for both the inference and interpretation on two tasks. It indicates that the inference and interpretation process can be mutually promoted with our proposed method. With the ablation study, we notice a performance degradation of the inference and interpretation if we remove either of them, demonstrating the faithfulness between the generated explanation and the model’s prediction.\nInference Promotion: We can achieve 11.73 and 2.06 absolute inference accuracy improvements compared to the baselines for the NLI and CQA task, respectively. For the NLI task, with our MPII framework, the Transformer baseline model can improve over 5 absolute accuracy score. The ablation study shows the contribution comes from not only the mutual interaction of inference and interpretation in the Stepwise Integration Mechanism (SIM), but also the adversarial mutual information training objective introduced in the Adversarial Fidelity Regularization (AFiRe). Moreover, with parameters initialized with the pretrained BART model, the accuracy can be further improved by a 4.53 absolute score. For the CQA task, we observe\nthat better performance is still achieved compared with the CAGE baseline model. If we remove the AFiRe, a significant inference degradation would be witnessed. It also indicates the effectiveness of AFiRe for utilizing interpretability to improve the inference ability.\nInterpretation Promotion: The quality of generated interpretation can also be significantly improved with our mutual promotion method on both NLI and CQA tasks. For NLI task, combined with our MPII, the Transformer baseline model can provide more accurate, fluent and diverse interpretation with much better results in all metrics. Similar with the inference results, the ablation study shows that both SIM and AFiRe contribute to the performance improvement. With the pretrained BART model, we further improve the BLEU and Inter-Rep performance and get comparable PPL compared with the e-INFERSENT model. For CQA task, our method performs better in terms of BLEU score and the diversity of generated explanations. We notice that the BLEU scores are pretty low for CQA task, which may stem from the free form of expression for explanations in the dataset, i.e. several different explanations share the same commonsense\nknowledge. We observe that most of the explanations generated by our method are reasonable enough to interpret the predictions even though the BLEU scores are low. Our method also achieves a smaller Inter-Rep score, which shows that our model can provide more diverse explanations to reveal the inference process of making predictions."
    }, {
      "heading" : "3.5 Out-of-Domain Evaluation",
      "text" : "As shown in Table 2, we evaluate our method with the Transformer baseline model on two outof-domain datasets: MultiNLI and SICK-E. The results show that our mutual promotion method enables the Transformer model to be more robust, and achieve more than 3 accuracy improvement on both of the out-of-domain datasets without fine-tuning. It is because with our MPII method, the model can generate more reliable and domain-related interpretation, which helps to make more accurate inference prediction. The ablation results demonstrate the adversarial mutual information training strategy in AFiRe is very effective to improve the model’s generalization and robustness.\na park typically does not have a peer or boardwalk"
    }, {
      "heading" : "3.6 Fidelity Evaluation",
      "text" : "We propose a model-based evaluation metric CriticScore to evaluate the fidelity between model’s inference predictions and interpretations. Inspired by Shen et al. (2017), which applied a trained model to automatically evaluate the text style transfer accuracy in the absence of parallel dataset, we pre-train a well-performed discriminator model to evaluate the fidelity between the predicted label and the generated explanation.\nThe discriminator is a binary classifier f : (X,L,E) 7→ Y es/No , which shares similar architecture with the backward network in our Adversarial Fidelity Regularization (Section 2.3). The training dataset is constructed based on the e-SNLI and CoS-E corpus. Given a sample ⟨Xi, Li, Ei⟩ on e-SNLI that serves as a positive sample, we build the negative sample as ⟨Xi, Li, Ej⟩, where explanation Ej ̸= Ei is selected from another eSNLI sample that shares either the same premise or hypothesis. With this dataset, the discriminator model is trained to learn the intrinsic fidelity between the label and its corresponding explanation. The trained discriminator achieves 97% accuracy on its test set and is able to serve as a quantitative way of evaluating fidelity.\nAs shown in Table 3, with our proposed mutual promotion method, the Transformer model can achieve significant improvement on Critic-Score between prediction and explanation. The ablation results confirm both the deep interaction design in Stepwise Integration Mechanism and the adversarial training strategy in Adversarial Mutual Information can contribute to the improvement of fidelity and faithfulness."
    }, {
      "heading" : "3.7 Analysis",
      "text" : "Mutual Promotion Visualization: Figure 4 demonstrates the evolution of the inference prediction as the interpretation proceeds. The input of the model is “[CLS] a couple standing on what looks\nFigure 5: The distribution of cosine similarity with average sentence embedding between human annotation and generated interpretation.\nlike a peer or boardwalk [SEP] a couple hugging each other at the park”, of which the ground truth label is “contradiction”. We observe that the model draws an initial conclusion that the entailment relationship between the premise and the hypothesis is not “entailment”, and is not able to tell whether it is “neutral” or “contradiction”. As the deliberation proceeds, our model comes to judge that it is “contradiction” with the generated interpretation ”a park does not have a peer or boardwalk”. From the clear split of the red and blue lines when ’does’ and ’not’ are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).\nSemantic Similarity Evaluation of Interpretation: To better evaluate the quality of generated explanations, we also measure the cosine similarity between generated explanations and human annotated explanations. The results are presented in Fig 5. The cosine similarity of our method concentrates on 0.9 and achieves higher scores than CAGE, which demonstrates the effectiveness of our MPII for generating better interpretation that are closer to human expression.\nCase Study Table 4 shows randomly selected examples generated by different models in CQA task. For the first exapmle, CAGE makes wrong prediction, and generates explanation that obviously conflicts with common knowledge. In contrast, our method can make correct predictions and generate more reasonable explanations. Similarly for the second example, CAGE seems to directly copy words from the question that do not actually contain meaningful information. Our MPII still explains well, but fails to explain properly with AFiRe removed, even if the explanation contains the correct answer, which reveals the importance of AFiRe for\npromotion of interpretation.\nHuman evaluation: We conduct human evaluation to further evaluate the effectiveness of MPII. We randomly selected 300 examples from the test set of e-SNLI, and asked 4 well-educated annotators to rate every sample with 4 metrics on a 1-5 Likert scale in a strictly blind fashion (Stent et al., 2005). As shown in Table 5, analogous to automatic evaluation results (Section 3.4), our MPII can generate interpretations with best quality and fidelity to corresponding inference predictions, whether correct or wrong."
    }, {
      "heading" : "4 Related Work",
      "text" : "With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020). Three forms of interpretation are provided by these works: (1) feature-based interpretation (Chen et al., 2016a, 2018; Ribeiro et al., 2016, 2018; Li et al., 2016; Nguyen, 2018; Feng et al., 2018; Gururan-\ngan et al., 2018) such as attention distribution (Xu et al., 2015), heatmap (Samek et al., 2017), alignment rationale (Jiang et al., 2021), gradients (Li et al., 2016), magnitude of hidden states (Linzen et al., 2016), etc.; (2) token-level interpretation that relatively easy to comprehend but prone to ambiguity (Ribeiro et al., 2016; Liu et al., 2018; Thorne et al., 2019), and (3) sentence-level interpretation which has the best human-readability Camburu et al. (2018); Talmor et al. (2018); Kumar and Talukdar (2020). Different from the previous work which only include one-side promotion, we proposed the mutual promotion mechanism that can improve the performance of both inference and sentence-level interpretation."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this work, we propose to mutually promote model inference ability and interpretability from multi-levels. From the model-level, we propose Stepwise Integration Mechanism to enable the model to refine the prediction conclusion as the explaining proceeds and also to guide the generation of better explanation with the inference procedure of reaching prediction conclusion. From the optimization-level, we propose an Adversarial Fidelity Regularization, which leverages the Adversarial Mutual Information method to improve the fidelity between the inference and interpretation, which further guarantees faithfulness. Experiment results show the effectiveness of our proposed method on both NLI and CQA tasks. Future work will involve extending our approaches into other tasks of NLP. We hope that our work can encourage further research in this direction."
    } ],
    "references" : [ {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "e-snli: natural language inference with natural language explanations",
      "author" : [ "Oana-Maria Camburu", "Tim Rocktäschel", "Thomas Lukasiewicz", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 9539–9549.",
      "citeRegEx" : "Camburu et al\\.,? 2018",
      "shortCiteRegEx" : "Camburu et al\\.",
      "year" : 2018
    }, {
      "title" : "A thorough examination of the cnn/daily mail reading comprehension task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2016a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to explain: An information-theoretic perspective on model interpretation",
      "author" : [ "Jianbo Chen", "Le Song", "Martin J. Wainwright", "Michael I. Jordan." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stock-",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel." ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Chen et al\\.,? 2016b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loic Barrault", "Antoine Bordes." ],
      "venue" : "arXiv preprint arXiv:1705.02364.",
      "citeRegEx" : "Conneau et al\\.,? 2017",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Pathologies of neural models make interpretations difficult",
      "author" : [ "Shi Feng", "Eric Wallace", "Alvin Grissom II", "Mohit Iyyer", "Pedro Rodriguez", "Jordan Boyd-Graber." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Feng et al\\.,? 2018",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Advances in neural information processing systems, pages 2672–2680.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Alignment rationale for natural language inference",
      "author" : [ "Zhongtao Jiang", "Yuanzhe Zhang", "Zhao Yang", "Jun Zhao", "Kang Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
      "citeRegEx" : "Jiang et al\\.,? 2021",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Equitability, mutual information, and the maximal information coefficient",
      "author" : [ "Justin B Kinney", "Gurinder S Atwal." ],
      "venue" : "Proceedings of the National Academy of Sciences, 111(9):3354–3359.",
      "citeRegEx" : "Kinney and Atwal.,? 2014",
      "shortCiteRegEx" : "Kinney and Atwal.",
      "year" : 2014
    }, {
      "title" : "NILE : Natural language inference with faithful natural language explanations",
      "author" : [ "Sawan Kumar", "Partha Talukdar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8730–8742, Online. Association for",
      "citeRegEx" : "Kumar and Talukdar.,? 2020",
      "shortCiteRegEx" : "Kumar and Talukdar.",
      "year" : 2020
    }, {
      "title" : "Understanding neural networks through representation erasure",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1612.08220.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial learning for neural dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Tianlin Shi", "Sébastien Jean", "Alan Ritter", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157–2169.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Assessing the ability of lstms to learn syntaxsensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4(1):521–535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards explainable nlp: A generative explanation framework for text classification",
      "author" : [ "Hui Liu", "Qingyu Yin", "William Yang Wang." ],
      "venue" : "arXiv preprint arXiv:1811.00196.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual",
      "author" : [ "Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli" ],
      "venue" : null,
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton." ],
      "venue" : "Icml.",
      "citeRegEx" : "Nair and Hinton.,? 2010",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Comparing automatic and human evaluation of local explanations for text classification",
      "author" : [ "Dong Nguyen." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Nguyen.,? 2018",
      "shortCiteRegEx" : "Nguyen.",
      "year" : 2018
    }, {
      "title" : "Adversarial mutual information for text generation",
      "author" : [ "Boyuan Pan", "Yazheng Yang", "Kaizhao Liang", "Bhavya Kailkhura", "Zhongming Jin", "Xian-Sheng Hua", "Deng Cai", "Bo Li." ],
      "venue" : "International Conference on Machine Learning, pages 7476–7486. PMLR.",
      "citeRegEx" : "Pan et al\\.,? 2020",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2020
    }, {
      "title" : "On variational bounds of mutual information",
      "author" : [ "Ben Poole", "Sherjil Ozair", "Aaron van den Oord", "Alexander A Alemi", "George Tucker." ],
      "venue" : "arXiv preprint arXiv:1905.06922.",
      "citeRegEx" : "Poole et al\\.,? 2019",
      "shortCiteRegEx" : "Poole et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2019 Conference of the Association for Computational Linguistics",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "Why should i trust you?: Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135–",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Anchors: High-precision modelagnostic explanations",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ribeiro et al\\.,? 2018",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2018
    }, {
      "title" : "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models",
      "author" : [ "Wojciech Samek", "Thomas Wiegand", "Klaus-Robert Müller." ],
      "venue" : "arXiv preprint arXiv:1708.08296.",
      "citeRegEx" : "Samek et al\\.,? 2017",
      "shortCiteRegEx" : "Samek et al\\.",
      "year" : 2017
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Advances in neural information processing systems, pages 6830–6841.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluating evaluation methods for generation in the presence of variation",
      "author" : [ "Amanda Stent", "Matthew Marge", "Mohit Singhai." ],
      "venue" : "international conference on intelligent text processing and computational linguistics, pages 341–351. Springer.",
      "citeRegEx" : "Stent et al\\.,? 2005",
      "shortCiteRegEx" : "Stent et al\\.",
      "year" : 2005
    }, {
      "title" : "Mutual information maximization for simple and accurate part-of-speech induction",
      "author" : [ "Karl Stratos." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Stratos.,? 2019",
      "shortCiteRegEx" : "Stratos.",
      "year" : 2019
    }, {
      "title" : "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "arXiv preprint arXiv:1811.00937.",
      "citeRegEx" : "Talmor et al\\.,? 2018",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating token-level explanations for natural language inference",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Thorne et al\\.,? 2019",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "International conference on machine learn-",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating informative and diverse conversational responses via adversarial information maximization",
      "author" : [ "Yizhe Zhang", "Michel Galley", "Jianfeng Gao", "Zhe Gan", "Xiujun Li", "Chris Brockett", "Bill Dolan." ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 179,
      "endOffset" : 285
    }, {
      "referenceID" : 3,
      "context" : "In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 179,
      "endOffset" : 285
    }, {
      "referenceID" : 17,
      "context" : "In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 179,
      "endOffset" : 285
    }, {
      "referenceID" : 32,
      "context" : "In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 179,
      "endOffset" : 285
    }, {
      "referenceID" : 13,
      "context" : "In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 179,
      "endOffset" : 285
    }, {
      "referenceID" : 35,
      "context" : "Existing interpretable models usually extract prominent features or select input key words as explanations, such as attention distribution (Xu et al., 2015), heatmap (Samek et al.",
      "startOffset" : 139,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : ", 2015), heatmap (Samek et al., 2017), alignment rationale (Jiang et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : ", 2017), alignment rationale (Jiang et al., 2021), gradients (Li et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "2016), magnitude of hidden states (Linzen et al., 2016), etc.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "Considering readability and comprehensibleness for human, some works turn to generate token-level explanations (Liu et al., 2018; Thorne et al., 2019), which nevertheless prone to cause ambiguity.",
      "startOffset" : 111,
      "endOffset" : 150
    }, {
      "referenceID" : 32,
      "context" : "Considering readability and comprehensibleness for human, some works turn to generate token-level explanations (Liu et al., 2018; Thorne et al., 2019), which nevertheless prone to cause ambiguity.",
      "startOffset" : 111,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "tion datasets available (Camburu et al., 2018; Rajani et al., 2019), methods of generating sentencelevel interpretation have been explored recently.",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : "tion datasets available (Camburu et al., 2018; Rajani et al., 2019), methods of generating sentencelevel interpretation have been explored recently.",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "From the optimization-level, we propose an Adversarial Fidelity Regularization (AFiRe) to improve the fidelity between inference and interpretation with the Adversarial Mutual Information (AMI) method (Pan et al., 2020), which extends the maximum mutual information optimization objective with the idea of generative adversarial network (Goodfellow et al.",
      "startOffset" : 201,
      "endOffset" : 219
    }, {
      "referenceID" : 7,
      "context" : ", 2020), which extends the maximum mutual information optimization objective with the idea of generative adversarial network (Goodfellow et al., 2014).",
      "startOffset" : 125,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "With this training framework, the model is trained against a smart backward network that learns to reward the inference prediction and interpretation of fidelity, which ensures faithfulness and makes the derived interpretation depict the true profile of how the model works (Jiang et al., 2021).",
      "startOffset" : 274,
      "endOffset" : 294
    }, {
      "referenceID" : 33,
      "context" : "Transformer model (Vaswani et al., 2017) has been firmly established as the dominant approach in text generation tasks.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : ") here denotes the ReLU activation function (Nair and Hinton, 2010), σ(.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "We apply the residual connection (He et al., 2016) here, which is easier to optimize in the scenario of many decoding steps.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "(LSTM) (Hochreiter and Schmidhuber, 1997) that learns to remember important information obtained on each decoding step.",
      "startOffset" : 7,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "We leverage the Adversarial Mutual Information (AMI) method (Pan et al., 2020) to extend the maximum mutual information objective among input, inference prediction and the generated explanation with the idea of generative adversarial network (Goodfellow et al.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : ", 2020) to extend the maximum mutual information objective among input, inference prediction and the generated explanation with the idea of generative adversarial network (Goodfellow et al., 2014).",
      "startOffset" : 171,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : "Compared to the maximum likelihood estimation (MLE) objective, maximum mutual information (MMI) objective encourages the model to generate the prediction and explanation that are more faithful to the input (Kinney and Atwal, 2014; Stratos, 2019).",
      "startOffset" : 206,
      "endOffset" : 245
    }, {
      "referenceID" : 30,
      "context" : "Compared to the maximum likelihood estimation (MLE) objective, maximum mutual information (MMI) objective encourages the model to generate the prediction and explanation that are more faithful to the input (Kinney and Atwal, 2014; Stratos, 2019).",
      "startOffset" : 206,
      "endOffset" : 245
    }, {
      "referenceID" : 4,
      "context" : "Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization (Chen et al., 2016b; Zhang et al., 2018) lower bound (Poole et al.",
      "startOffset" : 189,
      "endOffset" : 229
    }, {
      "referenceID" : 36,
      "context" : "Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization (Chen et al., 2016b; Zhang et al., 2018) lower bound (Poole et al.",
      "startOffset" : 189,
      "endOffset" : 229
    }, {
      "referenceID" : 21,
      "context" : "To remedy this problem, we leverage the Adversarial Mutual Information (AMI) method (Pan et al., 2020) to extend MMI with the idea of generative adversarial network (Goodfellow et al.",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : ", 2020) to extend MMI with the idea of generative adversarial network (Goodfellow et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "We also add an objective term Pθ(L,E|X) of maximum the negative likelihood of Pθ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017).",
      "startOffset" : 142,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI(Camburu et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : ", 2015), e-SNLI(Camburu et al., 2018), CQA (Talmor et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : ", 2018), CQA (Talmor et al., 2018), CoS-E (Rajani et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : ", 2018), CoS-E (Rajani et al., 2019), MultiNLI (Williams et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 34,
      "context" : ", 2019), MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "The eINFERSENT model adds a LSTM decoder into INFERSENT (Conneau et al., 2017) for explanations.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 33,
      "context" : "The Transformer model (Vaswani et al., 2017) adds a MLP layer for generating sentencelevel interpretations.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "CQA: We use CAGE (Rajani et al., 2019) as the baseline model for CQA task.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "CAGE adopts the explain-then-predict approach, which firstly finetunes a deep pretrained language model GPT (Radford et al., 2019) to generate explanations, then use a classifier to predict the inference label with the generated explanation and source text as the input.",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "From the clear split of the red and blue lines when ’does’ and ’not’ are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).",
      "startOffset" : 185,
      "endOffset" : 211
    }, {
      "referenceID" : 29,
      "context" : "We randomly selected 300 examples from the test set of e-SNLI, and asked 4 well-educated annotators to rate every sample with 4 metrics on a 1-5 Likert scale in a strictly blind fashion (Stent et al., 2005).",
      "startOffset" : 186,
      "endOffset" : 206
    }, {
      "referenceID" : 25,
      "context" : "With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 187,
      "endOffset" : 293
    }, {
      "referenceID" : 3,
      "context" : "With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 187,
      "endOffset" : 293
    }, {
      "referenceID" : 17,
      "context" : "With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 187,
      "endOffset" : 293
    }, {
      "referenceID" : 32,
      "context" : "With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 187,
      "endOffset" : 293
    }, {
      "referenceID" : 13,
      "context" : "With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016; Chen et al., 2018; Liu et al., 2018; Thorne et al., 2019; Kumar and Talukdar, 2020).",
      "startOffset" : 187,
      "endOffset" : 293
    }, {
      "referenceID" : 14,
      "context" : "Three forms of interpretation are provided by these works: (1) feature-based interpretation (Chen et al., 2016a, 2018; Ribeiro et al., 2016, 2018; Li et al., 2016; Nguyen, 2018; Feng et al., 2018; Gururangan et al., 2018) such as attention distribution (Xu et al.",
      "startOffset" : 92,
      "endOffset" : 221
    }, {
      "referenceID" : 20,
      "context" : "Three forms of interpretation are provided by these works: (1) feature-based interpretation (Chen et al., 2016a, 2018; Ribeiro et al., 2016, 2018; Li et al., 2016; Nguyen, 2018; Feng et al., 2018; Gururangan et al., 2018) such as attention distribution (Xu et al.",
      "startOffset" : 92,
      "endOffset" : 221
    }, {
      "referenceID" : 6,
      "context" : "Three forms of interpretation are provided by these works: (1) feature-based interpretation (Chen et al., 2016a, 2018; Ribeiro et al., 2016, 2018; Li et al., 2016; Nguyen, 2018; Feng et al., 2018; Gururangan et al., 2018) such as attention distribution (Xu et al.",
      "startOffset" : 92,
      "endOffset" : 221
    }, {
      "referenceID" : 8,
      "context" : "Three forms of interpretation are provided by these works: (1) feature-based interpretation (Chen et al., 2016a, 2018; Ribeiro et al., 2016, 2018; Li et al., 2016; Nguyen, 2018; Feng et al., 2018; Gururangan et al., 2018) such as attention distribution (Xu et al.",
      "startOffset" : 92,
      "endOffset" : 221
    }, {
      "referenceID" : 35,
      "context" : ", 2018) such as attention distribution (Xu et al., 2015), heatmap (Samek et al.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 27,
      "context" : ", 2015), heatmap (Samek et al., 2017), alignment rationale (Jiang et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : ", 2017), alignment rationale (Jiang et al., 2021), gradients (Li et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : ", 2021), gradients (Li et al., 2016), magnitude of hidden states (Linzen et al.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : ", 2016), magnitude of hidden states (Linzen et al., 2016), etc.",
      "startOffset" : 36,
      "endOffset" : 57
    } ],
    "year" : 0,
    "abstractText" : "In order to better understand the rationale behind model behavior, recent works have exploited providing interpretation to support the inference prediction. However, existing methods tend to provide human-unfriendly interpretation, and are prone to sub-optimal performance due to one-side promotion, i.e. either inference promotion with interpretation or vice versa. In this paper, we propose a multi-level Mutual Promotion mechanism for self-evolved Inference and sentence-level Interpretation (MPII). Specifically, from the modellevel, we propose a Step-wise Integration Mechanism to jointly perform and deeply integrate inference and interpretation in an autoregressive manner. From the optimization-level, we propose an Adversarial Fidelity Regularization to improve the fidelity between inference and interpretation with the Adversarial Mutual Information training strategy. Extensive experiments on NLI and CQA tasks reveal that the proposed MPII approach can significantly outperform baseline models for both the inference performance and the interpretation quality.",
    "creator" : null
  }
}
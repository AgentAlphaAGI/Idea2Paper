{
  "name" : "ARR_2022_60_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The perceived toxicity of language can vary based on someone’s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system’s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection."
    }, {
      "heading" : "1 Introduction",
      "text" : "Determining whether a text is toxic (i.e., contains hate speech, abuse, or is offensive) is inherently a subjective task that requires a nuanced understanding of the pragmatic implications of language (Fiske, 1993; Croom, 2011; Waseem et al., 2021). Without this nuance, both humans and machines are prone to biased judgments, such as over-relying on seemingly toxic keywords (e.g., expletives, swearwords; Dinan et al., 2019; Han\nand Tsvetkov, 2020) or backfiring against minorities (Yasin, 2018; Are, 2020, i.a.). For example, racial biases have been uncovered in toxic language detection where text written in African American English (AAE) is falsely flagged as toxic (Sap et al., 2019; Davidson et al., 2019).\nThe crux of the issue is that not all text is equally toxic for everyone (Waseem, 2016; Al Kuwatly et al., 2020; Jiang et al., 2021). Yet, most previous research has treated this detection as a simple classification with one correct label, obtained by averaging judgments by a small set of human annotators per post (Waseem and Hovy, 2016; Wulczyn et al., 2017; Davidson et al., 2017; Founta et al., 2018; Zampieri et al., 2019). Such approaches ignore the variance in annotations (Pavlick and Kwiatkowski, 2019; Geva et al., 2019; Akhtar et al., 2021) based on who the annotators are, and what their beliefs are.\nIn this work, we investigate the who, why, and what behind biases1 in toxicity annotations, through online studies with demographically and politically diverse participants. We measure the effects of annotator identities (who annotates as toxic) and attitudes or beliefs (why they annotate as toxic) on toxicity ratings, through the lens of social psychology research on hate speech, free speech, racist beliefs, altruism, political leaning, and more. We also analyze the effect of what is being rated, by considering three text characteristics: anti-Black or racially prejudiced meaning, African American English (AAE), and vulgar words.\nWe seek to answer these questions via two online studies. In our breadth-of-workers controlled study, we collect ratings of toxicity for a set of 15 hand curated posts from 641 annotators of different races, attitudes, and political leanings.\n1We use the term “bias” to denote both simple skews or variation in annotations (e.g., for variation in detecting vulgar content as toxic) or representational harms (e.g., AAE being over-detected as toxic or anti-Black content being underdetected as toxic; Barocas et al., 2017; Blodgett et al., 2020).\nThen, in our breadth-of-posts study, we simulate a typical toxic language annotation setting by collecting toxicity ratings for ∼600 posts, from a smaller but diverse pool of 173 annotators.\nDistilled in Figure 1, our most salient results across both studies show that annotators scoring higher on our racist beliefs scale were less likely to rate anti-Black content as toxic (§4). Additionally, annotators’ conservatism scores were associated with higher ratings of toxicity for AAE (§5), and conservative and traditionalist attitude scores with rating vulgar language as more toxic (§6).\nWe further provide a case study which shows that PERSPECTIVEAPI, a popular toxicity detection system, mirrors ratings by annotators of certain attitudes and identities over others (§7). For instance, for anti-Black language, the system’s scores better reflect ratings by annotators who score high on our scale for racist beliefs. Our findings have immense implications for the design of toxic language annotation and automatic detection—we recommend contextualizing ratings in social variables and looking beyond aggregated discrete decisions (§8).\n2 The Who, Why, and What of Toxicity Annotations\nWe aim to investigate how annotators’ ratings of the toxicity of text is influenced by their own identities (who they are; §2.1), and their beliefs (why they consider something toxic; §2.2) on specific categories of text (what they consider toxic;\n§2.3)—namely, text with anti-Black language, presence of African American English (AAE), and presence of vulgar or profane words. To this end, we design two online studies (§3) and discuss who find each of these text characteristics offensive and why as separate research questions in Sections §4, §5 and §6, respectively.\n2.1 Demographic Identities: Who considers something as toxic?\nPrior work has extensively shown links between someone’s gender, political leaning, and race affects how likely they are to perceive or notice harmful speech or racism (Cowan et al., 2002; Norton and Sommers, 2011; Carter and Murphy, 2015; Prabhakaran et al., 2021). Grounded in this prior literature, our study considers annotators’ race, gender, and political leaning. Since perceptions of race and political attitudes vary vastly across the globe, we restrict our study to participants exclusively from the United States.\n2.2 Attitudes: Why does someone consider something toxic?\nWhile some annotator toxicity ratings may highly correlate with demographic factors at face value (Prabhakaran et al., 2021; Jiang et al., 2021), we aim to go beyond demographics to investigate annotator beliefs that explain these correlations. Based on prior work in social psychology, political science, and sociolinguistics, we select seven attitude dimensions. Given the breadth of these atti-\ntudes, we follow prior work to operationalize them via scales (in SMALL CAPS), as described below.2\nValuing the freedom of offensive speech (FREEOFFSPEECH): the belief that any speech, including offensive or hateful speech, should be unrestricted and free from censorship. Recently, this belief has become associated with majority and conservative identities (Cole, 1996; Meddaugh and Kay, 2009; Gillborn, 2009; White and Crandall, 2017; Elers and Jayan, 2020). We use the scale by Cowan and Khatchadourian (2003); see Appendix §A.1.\nPerceiving the HARMOFHATESPEECH: the belief that hate speech or offensive language can be harmful for the targets of that speech (Soral et al., 2018; Nadal, 2018). This belief is correlated with socially-progressive philosophies (Downs and Cowan, 2012, see also Nelson et al., 2013). We use the scale by Cowan and Khatchadourian (2003); see Appendix §A.2.\nEndorsement of RACISTBELIEFS: the beliefs which deny the existence of racial inequality, or capture resentment towards racial minorities (Poteat and Spanierman, 2012). We measure RACISTBELIEFS using items from the validated Modern Racism Scale (McConahay, 1986); see Appendix §A.3.\nTRADITIONALISM: the belief that one should follow established norms and traditions, and be respectful of elders, obedient, etc. In the US, these beliefs are associated with generally conservative ideologies (Johnson and Tamney, 2001; Knuckey, 2005). We use an abridged version3 of the TRADITIONALISM scale (Bouchard Jr. and McGue, 2003) that measures annotators’ adherence to traditional values; see Appendix §A.4.\nLanguage Purism (LINGPURISM): the belief that there is a “correct” way of using English (Jernudd and Shapiro, 1989). Typically, this belief also involves negative reactions to non-canonical ways of using language (Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019). We created and validated a four-item LINGPURISM scale to measure this concept; see Appendix §A.5.\nEMPATHY: one’s tendency to see others’ perspectives and feel others’ feelings. Research in social psychology has linked higher levels of empathy to\n2We abstain from conclusions beyond our abstractions. 3This was done to reduce cognitive load on annotators.\nthe ability and willingness of recognizing and labeling hate speech (Cowan and Khatchadourian, 2003). We measure EMPATHY using an abbreviated Interpersonal Reactivity Index (Pulos et al., 2004); see Appendix §A.6.\nALTRUISM: one’s attitude of selfless concern about others’ well-being, which can move people to act when harm or injustice happens (Wagstaff, 1998; Gavrilets and Fortunato, 2014; Riar et al., 2020), including harms through hate speech (Cowan et al., 2002). We gathered the items to measure ALTRUISM with an adapted scale taken from Steg et al. (2014); see Appendix §A.7.\nIt is worth noting that some of the above attitudes, though not all, correlate with demographics very strongly. Table 8 in Appendix A.8 details these correlations from our study.\n2.3 Text Characteristics: what is considered offensive?\nNot all toxic text is toxic for the same reasons. We aim to understand how characteristics of text can affect ratings of toxicity, in addition to annotator attitudes and identities. Specifically, we consider three dimensions or categories of text, based on recent work on text characteristics that tend to be over- or under-detected as toxic (Dinan et al., 2019; Sap et al., 2019; Han and Tsvetkov, 2020; Zhou et al., 2021): anti-Black language, presence of African American Engligh (AAE) dialect markers, and vulgar language (e.g., swearwords, slurs). Following Zhou et al. (2021), we further disentangle two types of vulgarity: swearwords or explicit words that do not reference identities (offensive, non-identity referring; ONI), and (reclaimed) slurs or other identity-referring vulgarity (offensive identity-referring; OI)."
    }, {
      "heading" : "3 Data & Study Design",
      "text" : "We design two online studies to study the effect of annotator identities and attitudes on their toxicity ratings on posts with different characteristics.4 In either study, annotators are asked to rate how offensive and how racist they consider a post to be (see Appendix §B for the exact questions)."
    }, {
      "heading" : "3.1 Breadth-of-Workers Study",
      "text" : "Our first study focuses on collecting toxicity ratings from a wide and diverse set of participants\n4Both studies were approved by our institution’s ethics board (IRB), and paid participants above minimum wage."
    }, {
      "heading" : "Breadth-of-Workers study",
      "text" : ""
    }, {
      "heading" : "Vulgar (ONI), not Anti-Black, not AAE",
      "text" : "I sort of fuckin love how close my gf is with my family like that she knows about my family jokes before I fuckin do I’m hoping that Cersei gets her ass handed to her but I love Daenerys and Jon ;3; I don’t want either of em losing xD"
    }, {
      "heading" : "I got mosquito bites on my foot and they fucking hurt",
      "text" : "Personally I think it would be fucking hilarious to put Franziska or Godot in but idk if they’re important enough characters to make the cut I just keep telling myself that shitty things happen to me but something really great is gonna happen bc romantic comedies have ruined me"
    }, {
      "heading" : "AAE, not Vulgar, not Anti-Black",
      "text" : "If u grown & still get thirsty for Jordans knowin erbody else gon havem & u still feel like u accomplished something that say alot about u Just spent a whole 20k(imaginary money) on a home studio.. And used the last 3$ 2 get some flowers n a gift card for YA BIHH. When everything is going accordingly the enemy always try to come creep up on you but I aint havin it !! buhh byee You can’t slang no tea cause he been putting me on bout everything ."
    }, {
      "heading" : "I could only wish dawg the best ain’t no problem in cutting ppl off. save that space for somebody who’ll benefit from u and vice versa",
      "text" : ""
    }, {
      "heading" : "Anti-Black, not Vulgar, not AAE",
      "text" : ""
    }, {
      "heading" : "I don’t get government benefits just for being white Where is white history month? Why can’t I have white pride day?",
      "text" : "I don’t like black doctors, it’s just my preference Anti-Black racism isn’t that big of a deal because people treated Italians like this in the 60’s and 70’s Going to Africa. Hope I don’t get AIDS. Just kidding. I’m white!\nTable 1: All 15 posts for our breadth-of-workers study.\nfor a controlled set of posts. Shown in Table 1, we curated a set of 15 posts that belong exclusively to one text category (e.g., vulgar but non-AAE and non-anti-Black; see Appendix C.1 for more selection and validation details). To exclude confounding effects of offensive identity mentions, we only considered vulgar posts that are non-identity referring (i.e., ONI).\nWe ran our study on a 641 participants that were recruited using a pre-qualifier survey on Amazon Mechanical Turk (MTurk) to ensure racial and political diversity. Our final participant pool spanned various racial (13% Black, 85% White), political (29% conservative, 59% liberal), and gender identities (54% women, 45% men, 1% non-binary). Each participant gave each of the 15 posts a toxicity rating, after which they answered a series of questions about their attitudes and their identity. For further details, please see Appendix C.\nIn our subsequent analyses, we compute associations between the toxicity ratings and identities or attitudes by computing the effect sizes (Pearson r correlation or Cohen’s d) between the average toxicity rating of the posts in each category and annotator identities or attitude scores."
    }, {
      "heading" : "3.2 Breadth-of-Posts Study",
      "text" : "Our second study focused on collecting ratings for a larger set of posts, but with fewer annota-"
    }, {
      "heading" : "Breadth-of-Posts study",
      "text" : "tors per post to simulate a crowdsourced dataset on toxic language. Drawing from two existing toxic language detection corpora, we select posts that are automatically detected5 as AAE and/or vulgar from Founta et al. (2018), and posts that are automatically detected as vulgar and/or annotated as anti-Black from Vidgen et al. (2021).6 Importantly, in this study, we consider anti-Black or AAE posts that could also be vulgar, and allow this vulgarity to cover both potentially offensive identity references (OI) as well as non-identity vulgar words (ONI; see §2.3). Due to confounding effects of meaning, we do not present analyses looking at vulgar posts only. We list membership counts for our 571 posts in Table 2, and provide more details on our data selection Appendix D.1.\nAs with the previous study, we ran our annotation study on 173 participants recruited through a pre-qualifier survey. Our annotators varied racially (20% Black, 76% White), politically (30% conservative, 54% liberal), and in gender (45% women, 53% men, <2% non-binary). Each post was annotated by 6 participants from various racial and political identities.7 Additionally, we asked participants one-item versions of our attitude scales, using the question from each scale that correlated best with toxicity in our breadth-of-workers study. See Appendix D for more details.\nUnlike the breadth-of-workers study, here each annotators could rate a varying number of posts. Thus, we used a linear mixed effects model with random effects for each participants to compute associations between toxicity ratings and identities or attitudes.\n5We use the lexical detector by Blodgett et al. (2016) for AAE and the word list from Zhou et al. (2021) for vulgarity.\n6To circumvent potential racial biases in what is labelled as “racist” in the Vidgen et al. (2021) corpus, we do not consider anti-Black posts that are detected as AAE.\n7For each post, we collected toxicity ratings from two white conservative workers, two from white liberal workers, and two from Black workers."
    }, {
      "heading" : "4 Who finds anti-Black posts toxic, and why?",
      "text" : "Anti-Black language denotes racially prejudiced or racist content—subtle (Breitfeller et al., 2019) or overt—which is often a desired target for toxic language detection research (Waseem, 2016; Vidgen et al., 2021). Based on prior work on linking conservative ideologies, endorsement of unrestricted speech, and racial prejudice with reduced likelihood to accept the term “hate speech” (Duckitt and Fisher, 2003; White and Crandall, 2017; Roussos and Dovidio, 2018; Elers and Jayan, 2020), we hypothesize that conservative annotators and those who score highly on the RACISTBELIEFS or FREEOFFSPEECH scales will rate anti-Black tweets as less toxic, and vice-versa. Conversely, based on findings by Cowan and Khatchadourian (2003), we hypothesize that annotators with high HARMOFHATESPEECH scores will rate anti-Black tweets are more toxic."
    }, {
      "heading" : "4.1 Breadth-of-Workers results",
      "text" : "As shown in Table 3, we found several associations between annotator beliefs and toxicity ratings for anti-Black posts, confirming our hypotheses. The three most salient associations with lower racism ratings were annotators who scored higher in RACISTBELIEFS, FREEOFFSPEECH, and those who leaned conservative. We find similar trends for offensiveness ratings.\nConversely, we found that participants who scored higher in HARMOFHATESPEECH were much more likely to rate anti-Black posts as more\noffensive, and more racist. Finally, though both white and Black annotators rated these posts very high in offensiveness (with means µBlack = 3.85 and µwhite = 3.59 out of 5), our results show that Black participants were slightly more likely than white participants to rate them as offensive.\nOur exploratory analyses unearthed other significant associations: negative correlations with LINGPURISM, TRADITIONALISM, and gender (male), and positive correlations with high EMPATHY, ALTRUISM, and gender (female)."
    }, {
      "heading" : "4.2 Breadth-of-Posts results",
      "text" : "Table 4 shows similar results as in the breadthof-workers analyses, despite the posts now potentially containing vulgarity. Specifically, we find that annotators who scored higher in RACISTBELIEFS rated anti-Black posts as less offensive, whereas those who scored higher in HARMOFHATESPEECH rated them as more offensive. Ratings of racism showed similar effects with one addition: those who scored higher in FREEOFFSPEECH rated anti-Black posts as less racist."
    }, {
      "heading" : "4.3 Perceived toxicity of anti-Black language",
      "text" : "Overall, our results from both studies corroborate previous findings that studied the attitudes that different gender and racial identities have towards hate speech, specifically that conservatives, white people, and men tend to value free speech more, and that liberals, women, and non-white people perceive the harm of hate speech more (Cowan and Khatchadourian, 2003; Downs and Cowan, 2012). Our results also support the finding that those who hold generally conservative ideologies tend to be more accepting towards anti-Black or racially prejudiced content (Goldstein and Hall, 2017; Lucks, 2020; Schaffner, 2020).\nIn the context of toxicity annotation and detection, our findings highlight the need to consider\nthe attitudes of annotators towards free speech, racism, and their beliefs on the harms of hate speech, for an accurate estimation of anti-Black language as toxic, offensive, or racist (e.g., by actively taking into consideration annotator ideologies; Waseem, 2016; Vidgen et al., 2021). This can be especially important given that hateful content very often targets marginalized groups and racial minorities (Silva et al., 2016; Sap et al., 2020), and can catalyze violence against them (O’Keeffe et al., 2011; Cleland, 2014)."
    }, {
      "heading" : "5 Who finds AAE posts toxic, and why?",
      "text" : "African American English (AAE) is a set of wellstudied varieties or dialects of U.S. English, common among, but not limited to, African-American or Black speakers (Green, 2002; Edwards, 2004). This category has been shown to be considered “worse” English by non-AAE speakers (Hilliard, 1997; Blake and Cutler, 2003; Champion et al., 2012; Beneke and Cheatham, 2015; Rosa and Flores, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al., 1998; Sap et al., 2019), particularly due to dialectspecific lexical markers (e.g., words, suffixes).\nBased on prior work that correlate racial prejudice with negative attitudes towards AAE (Gaither et al., 2015; Rosa, 2019), we hypothesize that annotators who are white and who score high in RACISTBELIEFS will rate AAE posts as more toxic. Additionally, since AAE can be considered non-canonical English (Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019), we hypothesize that annotators who are more conservative and who score higher in TRADITIONALISM and LINGPURISM will rate AAE posts with higher toxicity."
    }, {
      "heading" : "5.1 Breadth-of-Workers results",
      "text" : "Table 5 shows significant associations between annotator identities and beliefs and their ratings of toxicity of AAE posts. Partially confirming our\nhypothesis, we found that ratings of racism had somewhat significant correlations with annotators’ conservative political leaning, and their scores on our RACISTBELIEFS scale. However, contrary to our expectations, we found that white and Black annotators did not differ in how offensive they rated AAE tweets (d = 0.14, p > 0.1). We found no additional hypothesized or exploratory associations for racism ratings, and no significant associations for offensiveness ratings."
    }, {
      "heading" : "5.2 Breadth-of-Posts results",
      "text" : "Show in Table 6, our results for AAE and potentially vulgar breadth-of-posts study show higher offensiveness ratings from conservative raters, and those who scored higher in TRADITIONALISM and RACISTBELIEFS. We also find that conservative annotators and those who scored higher in FREEOFFSPEECH, and TRADITIONALISM rated AAE posts as more racist.\nAs an additional investigation, we measure whether attitudes or identities affects toxicity ratings of AAE posts that contain the word “n*gga,” a (reclaimed) slur that has very different pragmatic interpretations depending on speaker and listener identity (Croom, 2011). Here, we find that raters who are more conservative tended to score those posts as significantly more racist (β = 0.465, p = 0.003; corrected for multiple comparisons)."
    }, {
      "heading" : "5.3 Perceived toxicity of AAE",
      "text" : "Our findings suggest that annotators perceive that AAE posts are associated with the Black racial identity (Rosa, 2019), which could cause those who score highly on the RACISTBELIEFS scale to annotate them as racist, potentially as a form of colorblind racism (e.g., where simply mentioning race is considered racist; Bonilla-Silva, 2006).\nMoreover, specific markers of AAE could have been perceived as obscene by non-AAE speakers (Spears et al., 1998), even though some of these might be reclaimed slurs (e.g., “n*gga”; Croom, 2011; Galinsky et al., 2013). Contrary to expectations, annotators’ own racial identity did not affect their ratings of AAE posts in our studies, which future work should investigate further.\nThese findings shed some light on the racial biases found in hate speech detection (Davidson et al., 2019; Sap et al., 2019), partially explaining why AAE is perceived as toxic. Based on our results, future work in toxic language detection should account for this over-estimation of AAE as racist. For example, annotators could explicitly include speakers of AAE, or those who understand that AAE or its lexical markers are not inherently toxic, or are primed to do so (Sap et al., 2019). Avoiding an incorrect estimation of AAE as toxic is crucial to avoid upholding racio-linguistic hierarchies and thus representational harms against AAE speakers (Rosa, 2019; Blodgett et al., 2020)."
    }, {
      "heading" : "6 Who finds vulgar posts toxic, and why?",
      "text" : "Vulgarity can correspond to non-identity referring swearwords (e.g., f*ck, sh*t; denoted as ONI) or identity-referring slurs (e.g., b*tch, n*gga; denoted as OI). Both types of vulgarity can be mistaken for toxic despite also having non-hateful usages (e.g., to indicate emotion or social belonging; Croom, 2011; Dynel, 2012; Galinsky et al., 2013).\nGiven that vulgarity can be considered noncanonical or impolite language (Jay and Janschewitz, 2008; Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019), we hypothesize that annotators who score high on LINGPURISM, TRADITIONALISM, and who are more conservative will rate vulgar posts as more offensive. Importantly, here, we focus on the posts that are exclusively vulgar (ONI) from only our breadth-of-workers study, to avoid confounding effects of vulgar posts with\nanti-Black meaning or in AAE (both of those cases were were analyzed in §4.2 and §5.2). We refer the reader to Appendix F for the results on vulgar posts in the breadth-of-posts study."
    }, {
      "heading" : "6.1 Breadth-of-Workers results",
      "text" : "Confirming our hypotheses, we found that offensiveness ratings of vulgar (ONI) posts indeed correlated with annotators’ TRADITIONALISM and LINGPURISM scores, and conservative political leaning (Table 7). We found no associations between attitudes and racism ratings for vulgar posts."
    }, {
      "heading" : "6.2 Perceived toxicity of vulgar language",
      "text" : "Our findings corroborate prior work showing how adherence to societal traditional values is often opposed to the acceptability of vulgar language (Sapolsky et al., 2010). Traditional values and conservative beliefs have been connected to finding vulgar language as a direct challenging the moral order (Jay, 2018; Sterling et al., 2020; Muddiman, 2021). Our results suggest that vulgarity is a very specific form of offensiveness that deserves special attention. Specifically, future work might consider studying the specific toxicity of individual identity-referring vulgar (OI) words, which can carry prejudiced meaning as well (e.g., slurs such as “n*gg*r”). Moreover, annotators across different levels of traditionalism could be considered when collecting ratings of vulgarity, especially since perceptions might vary with generational and cultural norms (Dynel, 2012)."
    }, {
      "heading" : "7 Toxicity Detection System Case Study:",
      "text" : "PERSPECTIVEAPI\nOur previous findings indicated that there is strong potential for annotator identities and beliefs to affect their toxicity ratings. We are additionally interested in how this influences the behavior of toxicity detection models trained on annotated data. We present a brief case study to answer this question with the PERSPECTIVEAPI,8 a widely used, commercial system for toxicity detection. Appendix G provides a more in-depth description.\nWe investigate whether PERSPECTIVEAPI scores align with toxicity ratings from workers with specific identities or attitudes, using the 571 posts from our breadth-of-posts study. Specifically, we compare the correlations between PERSPECTIVEAPI scores and ratings from annotators,\n8www.perspectiveapi.com\nbroken down by annotators with different identities (e.g., men and women) or with higher or lower scores on attitude scales (split at the mean). See Appendix G.1 for details about this methodology.\nOur investigation shows that PERSPECTIVE scores can be significantly more aligned with ratings from certain identities or groups scoring higher or lower on attitude dimensions (see Table 12 in Appendix G.2). Our most salient results show that for anti-Black posts, PERSPECTIVE scores are somewhat significantly more aligned with racism ratings by annotators who score high in RACISTBELIEFS (rhigh = 0.29, rlow = 0.17, ∆r = 0.12, p = 0.056). Additionally, for AAE posts, PERSPECTIVE scores are slightly more correlated with racism ratings by annotators who were women (∆r = 0.22, p < 0.001) or white (∆r = 0.08, p = 0.07), and who scored higher in LINGPURISM (∆r = 0.14, p = 0.003) or TRADITIONALISM (∆r = 0.10, p = 0.030).\nOverall, our findings indicate that PERSPECTIVEAPI toxicity score predictions align with specific viewpoints or ideologies, depending on the text category. Particularly, it seems that the API underestimates the toxicity of anti-Black posts in a similar way to annotators who scored higher on the RACISTBELIEFS scale, and aligns more with white annotator’s perception of AAE toxicity (vs. Black annotators). This corroborate prior findings that show that toxicity detection models inherently encode a specific positionality (Cambo, 2021) and replicate human biases (Davani et al., 2021)."
    }, {
      "heading" : "8 Discussion & Conclusion",
      "text" : "Overall, our analyses showed that toxicity ratings are indeed affected by annotators’ demographic identities and beliefs (§2). We showed—via both a breadth-of-workers study and a breadth-of-posts study (§3)—several associations when isolating specific text characteristics: anti-Black (§4), AAE (§5), and vulgarity (§6); see full, detailed results in Appendix §E and §F. Finally, we showed that a popular toxicity detection system produces toxicity scores that are more aligned with raters with certain attitudes and identities than others (§7). We discuss implications of our findings below.\nCollect annotator identities and attitudes. Given that many NLP tasks are increasingly socially or politically laden (misinformation, hate speech, stereotypes, biased language, etc.), it is important to consider who is annotating data and\nwhat their beliefs are about the task. While several works have recruited or subselected annotators to hold particular ideologies (e.g., Waseem, 2016; Sap et al., 2020; Vidgen et al., 2021), annotator attitudes are only vaguely described. We advocate for collecting attitude scores based on relevant social science research, perhaps in lightweight format as done in our breadth-of-posts study, and reporting those scores along with the dataset (e.g., in datasheets; Gebru et al., 2018).\nContextualize toxicity predictions in social variables. As shown in our results and previous studies (e.g., Waseem, 2016; Ross et al., 2017; Waseem et al., 2021), determining what is toxic is subjective. However, given this subjectivity, the open question remains: whose perspective should be considered when using toxicity detection models? To try answering this question, we urge researchers and practitioners to consider all stakeholders and end users on which toxicity detection systems might be deployed (Sanders, 2002; Friedman et al., 2008). Additionally, we urge people to embrace that each design decision has socio-political implications (Green, 2020; Cambo, 2021), and encourage them to develop technologies to shift power to the targets of oppression (Blodgett et al., 2020; Kalluri, 2020; Birhane, 2021). Finally, given the increasingly essential role of online platforms in people’s daily lives (Rahman, 2017), we echo calls for policy regulating these toxicity detection algorithms (Jiang, 2020; McGuffie and Newhouse, 2020).\nThink beyond classification. Our findings on the subjectivity of the toxicity detection tasks suggests that standard approaches of obtaining binary (or even n-ary) labels of toxicity and averaging them into a majority vote are inadequate. Instead, researchers could consider modeling the variation in toxicity labels with respect to specific identities or beliefs. But, perhaps more importantly, we encourage re-thinking the toxicity detection paradigm altogether. If the goal is to assist human content moderators, making our systems explain biased implications of statements could be more helpful than opaque toxicity scores (Gillespie et al., 2020). Therefore, we advocate for moving away from classification frameworks, and towards more nuanced, holistic, and explainable frameworks for inferring the desired concepts of toxicity and social biases (e.g., the hierarchical Social Bias Frames formalism; Sap et al., 2020)."
    }, {
      "heading" : "A Attitude scales",
      "text" : "Below, we list the individual items from each of our seven attitude dimensions. In our breadthof-workers study, we asked participants all of the items in each scale, but in the larger-scale study, we only asked the bolded items (one per scale). (R) denotes a reverse coded item.\nA.1 Valuing the freedom of offensive speech (FREEOFFSPEECH).\nParticipants were instructed to rate their level of agreement with the item statements, using a 5- point Likert scale ranging from “not at all” (1) to “very much so” (5).\n• People should face consequences for saying something offensive online. (R) • Offensive posts and comments online should be removed from the platforms. • There is no such a thing as hate speech, only \"speech\". • People should be allowed to say things that others may consider offensive. • It’s more important to protect free speech rights than to protect people’s feelings when it comes to offensive language.\nThis scale was taken from Cowan et al. (2002)."
    }, {
      "heading" : "A.2 Perceiving the HARMOFHATESPEECH.",
      "text" : "Participants were instructed to rate their level of agreement with the item statements, using a 5- point Likert scale ranging from “not at all” (1) to “very much so” (5).\n• When someone posts something offensive towards minorities online it intimidates and casts fear in the heart of individuals who are part of the minority group. • Offensive language encourages discrimination against minorities.\n• Offensive language is harmful to minorities. This scale was taken from Cowan et al. (2002)."
    }, {
      "heading" : "A.3 Endorsement of RACISTBELIEFS.",
      "text" : "Participants were instructed to rate their level of agreement with the item statements, using a 5- point Likert scale ranging from “not at all” (1) to “very much so” (5).\n• Discrimination against racial minorities is no longer a problem in the United States. • It is easy to understand the anger of racial minorities people in America. (R)\n• Racial minorities are getting too demanding in their push for equal rights. • Over the past few years, racial minorities have gotten more economically than they deserve. • Over the past few years, the government and news media have shown more respect to racial minorities than they deserve.\nThese items form the validated Modern Racism Scale, created by McConahay (1986)."
    }, {
      "heading" : "A.4 TRADITIONALISM.",
      "text" : "Participants were asked: “Please tell us how important each of these is as a guiding principle in your life.” They answered each item on a 5-point Likert scale, ranging from “not at all important to me” (1) to “extremely very important to me” (5).\n• Being obedient, dutiful, meeting obligations. • Self-discipline, self-restraint, resistance to\ntemptations. • Honoring parents and elders, showing re-\nspect. • Traditions and customs.\nThis is an abridged version of the traditionalism scale by Bouchard Jr. and McGue (2003)."
    }, {
      "heading" : "A.5 Language Purism (LINGPURISM).",
      "text" : "Participants were instructed to rate their level of agreement with the item statements, using a 5- point Likert scale ranging from “not at all” (1) to “very much so” (5).\n• I dislike when people make simple grammar or spelling errors. • It is important to master the English language properly, and not make basic spelling mistakes or misuse a common word. • I am not afraid to correct people when they make simple grammar or spelling errors. • There exists such a thing as good proper English.\nThis scale was created by the authors."
    }, {
      "heading" : "A.6 EMPATHY.",
      "text" : "Participants were instructed to rate their level of agreement with the item statements, using a 5- point Likert scale ranging from “not at all” (1) to “very much so” (5).\n• Before criticizing somebody, I try to imagine how I would feel if I were in his/her place. • I don’t usually become sad when I see other people crying. (R)\n• When someone is feeling ‘down’ I can usually understand how they feel. • I have tender, concerned feelings for people or groups of people less fortunate than me. This scale is an abbreviated version of the widely used Interpersonal Reactivity Index by Pulos et al. (2004)."
    }, {
      "heading" : "A.7 ALTRUISM.",
      "text" : "Participants were asked: “Please tell us how important each of these is as a guiding principle in your life.” They answered each item on a 5-point Likert scale, ranging from “not at all important to me” (1) to “extremely very important to me” (5).\n• Social justice, correcting injustice, caring for the weak.\n• Equality, equal opportunity for all. These items are taken from the altruism part of the scale by (Steg et al., 2014).\nA.8 Inter-variable correlations\nWhile some attitudes may highly correlate with demographic factors at face value (e.g., TRADITIONALISM and politically conservatism); other forms of biases may not be easily explained by demographics alone. We examine the relationship between our attitude measurements and annotator demographic identity variables. Shown in Table 8, we find strong significant correlations between several of our annotator variables.\nNotably, we find that an annotator’s political orientation correlated strongly with several variables, with liberal leaning identities being associated with higher scores on the EMPATHY, HARMOFHATESPEECH, and ALTRUISM scales, whereas conservative political leaning was associated with higher scores on the TRADITIONALISM, LINGPURISM, FREEOFFSPEECH and RACISTBELIEFS scales."
    }, {
      "heading" : "B Toxicity questions",
      "text" : "Following crowdsourcing setups in prior work (Waseem, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Sap et al., 2019), we asked three fine-grained questions to annotators for each post in both our studies:\n• “How toxic/hateful/disrespectful or offensive does this post seem to you?” • “How much could this post be seen as toxic/hateful/disrespectful or offensive to\nanyone?” • “In your opinion, how racist is this post?”\nGiven the high correlations between the two offensiveness variables (Pearson r ≥ .7; Table 9), we use create an offensiveness score by taking the average rating given to the “to you” and “to anyone” questions. In all our analyses, we use that offensiveness score, along with the raw racism score."
    }, {
      "heading" : "C Small-scale controlled study details",
      "text" : ""
    }, {
      "heading" : "C.1 Data Select & Validation",
      "text" : "We aimed to select online posts that were very indicative of each of the above characteristics (vulgar, AAE, anti-Black) but not indicative of the others, in order to tease out the effect of that category. We selected vulgar and AAE posts from a large corpus of tweets annotated for hate speech by Founta et al. (2018).9 For each tweet in that corpus, we detected the presence of non-identity related profanity or swearwords using the list from Zhou et al. (2021), and extracted the likelihood that the tweet is in AAE using a lexical detector by Blodgett et al. (2016). As candidates, we selected 10 vulgar tweets that have low likelihood of being AAE, and 26 tweets that have high likelihood of being AAE but contain no vulgarity. For anti-Black posts, we selected 11 candidate online posts curated by Zevallos (2017).\nWe ran a human validation study to verify that the candidate posts are truly indicative of their respective categories. We created an annotation scheme to collect binary ratings for two questions per post: “does it contain vulgar language”, and “is it offensive to minorities”; a post could belong to either category or neither. Each post was manually annotated by three undergraduate research assistants trained for the task. Post validation, we manually selected 5 posts per category with perfect inter-annotator agreement. Table 1 lists the final 15 posts used for our study."
    }, {
      "heading" : "C.2 Participant Recruitment",
      "text" : "We ran our study on Amazon Mechanical Turk (MTurk), a crowdsourcing platform that is often used to collect offensiveness annotations.10 With the task at hand, we sought a racially and politically diverse pool of participants, which can be\n9We only used the training subset of the corpus. 10Note, this study was approved by the author’s institu-\ntional review board (IRB).\nchallenging given that MTurk workers are usually tend to be predominantly white and skew liberal (Huff and Tingley, 2015; Burnham et al., 2018; Loepp and Kelly, 2020). Therefore, we ran a pre-selection survey to collect race and political ideology of workers, noting that this presurvey could grant them access to a longer survey on free speech, hate speech, and offensiveness in language.11,12 We stopped recruiting once we reached at least 200 Black and 200 conservative participants."
    }, {
      "heading" : "C.3 Study Setup",
      "text" : "We ran our study on the widely used survey platform Qualtrics, using an MTurk HIT to recruit and compensate participants.13 Participants were first asked to consent to the task, then were shown instructions for annotating the 15 posts (with occasional reminders of the instructions). Then we asked participants their views on several topics using the scales described in §2.2 and §A and finally their demographics. Throughout the study,\n11To better recruit for our pre-survey, we noted in the title that “BIPOC people and conservatives were encouraged to participate,” and also varied the title’s wording to emphasize free speech or hate speech in different recruiting rounds.\n12We compensated workers $0.02-$0.03 for this presurvey.\n13Participants were compensated $4.33 for the entire survey, equivalent to an average hourly compensation of $22/h.\nwe added three attention checks to ensure the quality of responses.\nAllowing only Black, white liberal, and white conservative workers to participate, we ran our survey for 4 weeks from March 10 to April 5 2021, occasionally reminding participants from our presurvey that they could take the survey."
    }, {
      "heading" : "D Breadth-of-Posts annotation study details",
      "text" : ""
    }, {
      "heading" : "D.1 Data Selection",
      "text" : "In this study, we draw from two existing corpora of posts labeled for toxicity, hate, or offensiveness. First, we select posts that are automatically detected as AAE and/or vulgar from Founta et al. (2018), using the lexical detector of AAE by (Blodgett et al., 2016) and the vulgar wordlist by Zhou et al. (2021). Second, we select posts that are automatically detected as vulgar and/or annotated as anti-Black from Vidgen et al. (2021). Importantly, in this large-scale study, we consider posts that potentially have multiple characteristics (e.g., AAE and vulgar), and thus consider both posts with potentially offensive identity references (vulgar-OI) as well as non-identity vulgar words (vulgar-ONI). However, to circumvent potential racial biases in what is labelled as “racist” in the Vidgen et al. (2021) corpus (Sap et al., 2019; Davidson et al., 2019), we do not consider posts that are annotated as anti-Black but detected as AAE.\nGiven an initial set of posts from our categories, we then randomly sample up to 600 posts, stratifying by toxicity label, vulgarity, AAE, and antiBlack meaning. Our final sample contains 571 posts, as outlined in Table 2 and Figure 3."
    }, {
      "heading" : "D.2 Breadth-of-Posts Survey details",
      "text" : "As in the breadth-of-workers study, we recruit participants using a pre-qualifying survey on MTurk. Then, we set up a second MTurk task to collect toxicity ratings, and annotator attitudes and identities. For each post, we collected two ratings from white conservative workers, two from white liberal workers, and two from Black workers. To better mirror the crowdsourcing setting and to reduce the annotator burden, we shorten the task to only ask one question per attitude (listed in §A).\nFor this study, our final dataset contains 3,171 ratings from N = 173 participants.14 Our participants were 53% were men, 45% women, and <2% non-binary, identified as 76% white, 20% Black, and <4% some other race, and spanned the political spectrum from 54% liberal to 30% conservative, with 16% centrists or moderates."
    }, {
      "heading" : "D.3 Selecting Attitude Questions",
      "text" : "In order to simplify the annotation task for annotators, we abridged the attitude scales to only one item. Using the data from the breadth-of-workers study, we select the question that best correlated with all toxicity ratings. Specifically, for each scale, we first take the tweet category with the highest correlation with toxicity (e.g., anti-Black posts for RACISTBELIEFS), and then take the item whose response scores correlated most with the toxicity rating for those posts. Those items are bolded in §A."
    }, {
      "heading" : "E Further Breadth-of-Workers Results",
      "text" : "We show all associations between attitudes and toxicity ratings in Table 10.\nAdditionally, we investigate the differences in the overall toxicity ratings of anti-Black vs. AAE vs. vulgar posts? (Figure 2). Overall, anti-Black tweets were rated as substantially more offensive and racist than AAE or vulgar tweets (with effect sizes ranging from d = 2.4 to d = 3.6). Additionally, vulgar tweets were rated as more offensive than AAE tweets (d = -0.29, p < 0.001).\nSurprisingly, we also found that AAE tweets were considered slightly more racist than vulgar tweets (d = 0.19, p < 0.001). To further inspect this phenomenon, we performed exploratory analyses by computing the differences in ratings of\n14As before, we discard 255 ratings where workers failed an attention check.\nracism for AAE and vulgar broken down by annotator gender, race, and political leaning. We found that AAE tweets were rated as significantly more racist than vulgar tweets only by annotators who were white or liberal (d = 0.20 and d = 0.22, respectively, with p < 0.001 corrected for multiple comparisons), compared to Black or conservative. There were no significant differences when looking at men and women separately."
    }, {
      "heading" : "F Further Breadth-of-Posts Results",
      "text" : "To account for the varying number of posts that each annotators could rate, we use a linear mixed effects model 15 to compute associations between each post’s toxicity ratings and identities or attitudes. Specifically, we our linear model regresses the attitude score onto the toxicity score, with a random effect for each worker.16\nSee Figure 3 and Table 11."
    }, {
      "heading" : "G PERSPECTIVEAPI Case Study: Details & Results",
      "text" : ""
    }, {
      "heading" : "G.1 Details",
      "text" : "We first obtain PERSPECTIVE toxicity scores for all the posts in our breadth-of-posts study (§3.2).17 Then, we split workers into two different groups for each of our attitudes and identity dimensions. For attitudes and political leaning, we assign each annotator to a “high” or “low” group based on whether they scored higher or lower than the\n15Using the Python statsmodels implementation. 16In R-like notation, toxicity∼attitude+(1|WorkerId). 17the API was accessed in October 2021\nmean score on that attitude scale. For gender and race, we use binary bins for man/woman and white/black.\nThen, for each attitude or identity dimension, we compute the Pearson r correlation between the PERSPECTIVE score and the toxicity ratings from\nthe high and low groups, considering posts from potentially overlapping categories (e.g., AAE and potentially vulgar posts).Finally, we compare the high and low correlations using Fisher’s r-to-z transformation (Silver and Dunlap, 1987)."
    }, {
      "heading" : "G.2 Results",
      "text" : "See Table 12 and Figures 4–11."
    } ],
    "references" : [ {
      "title" : "Whose opinions matter? perspective-aware models to identify opinions of hate speech victims in abusive language detection",
      "author" : [ "Sohail Akhtar", "Valerio Basile", "Viviana Patti." ],
      "venue" : "ArXiv preprint arXiv:2106.15896.",
      "citeRegEx" : "Akhtar et al\\.,? 2021",
      "shortCiteRegEx" : "Akhtar et al\\.",
      "year" : 2021
    }, {
      "title" : "Identifying and measuring annotator bias based on annotators’ demographic characteristics",
      "author" : [ "Hala Al Kuwatly", "Maximilian Wich", "Georg Groh." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 184–190, Online. Associa-",
      "citeRegEx" : "Kuwatly et al\\.,? 2020",
      "shortCiteRegEx" : "Kuwatly et al\\.",
      "year" : 2020
    }, {
      "title" : "How instagram’s algorithm is censoring women and vulnerable users but helping online abusers",
      "author" : [ "Carolina Are." ],
      "venue" : "Feminist media studies, 20(5):741– 744.",
      "citeRegEx" : "Are.,? 2020",
      "shortCiteRegEx" : "Are.",
      "year" : 2020
    }, {
      "title" : "The problem with bias: Allocative versus representational harms in machine learning",
      "author" : [ "Solon Barocas", "Kate Crawford", "Aaron Shapiro", "Hanna Wallach." ],
      "venue" : "SIGCIS.",
      "citeRegEx" : "Barocas et al\\.,? 2017",
      "shortCiteRegEx" : "Barocas et al\\.",
      "year" : 2017
    }, {
      "title" : "Speaking up for african american english: Equity and inclusion in early childhood settings",
      "author" : [ "Margaret Beneke", "Gregory A Cheatham." ],
      "venue" : "Early Childhood Education Journal, 43(2):127–134.",
      "citeRegEx" : "Beneke and Cheatham.,? 2015",
      "shortCiteRegEx" : "Beneke and Cheatham.",
      "year" : 2015
    }, {
      "title" : "Algorithmic injustice: a relational ethics approach",
      "author" : [ "Abeba Birhane." ],
      "venue" : "Patterns (New York, N.Y.), 2(2):100205.",
      "citeRegEx" : "Birhane.,? 2021",
      "shortCiteRegEx" : "Birhane.",
      "year" : 2021
    }, {
      "title" : "AAE and variation in teachers’ attitudes: A question of school philosophy",
      "author" : [ "Renée Blake", "Cecilia Cutler" ],
      "venue" : "Linguistics and education,",
      "citeRegEx" : "Blake and Cutler.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blake and Cutler.",
      "year" : 2003
    }, {
      "title" : "Language (technology) is power: A critical survey of “bias” in NLP",
      "author" : [ "Su Lin Blodgett", "Solon Barocas", "Hal Daumé III", "Hanna Wallach." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–",
      "citeRegEx" : "Blodgett et al\\.,? 2020",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2020
    }, {
      "title" : "Demographic dialectal variation in social media: A case study of african-american english",
      "author" : [ "Su Lin Blodgett", "Lisa Green", "Brendan O’Connor" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Blodgett et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2016
    }, {
      "title" : "Racism without racists: Color-blind racism and the persistence of racial inequality in the United States",
      "author" : [ "Eduardo Bonilla-Silva." ],
      "venue" : "Rowman & Littlefield Publishers.",
      "citeRegEx" : "Bonilla.Silva.,? 2006",
      "shortCiteRegEx" : "Bonilla.Silva.",
      "year" : 2006
    }, {
      "title" : "Genetic and environmental influences on human psychological differences",
      "author" : [ "Thomas J. Bouchard Jr.", "Matt McGue." ],
      "venue" : "Journal of Neurobiology, 54(1):4–45.",
      "citeRegEx" : "Jr. and McGue.,? 2003",
      "shortCiteRegEx" : "Jr. and McGue.",
      "year" : 2003
    }, {
      "title" : "Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts",
      "author" : [ "Luke Breitfeller", "Emily Ahn", "David Jurgens", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Breitfeller et al\\.,? 2019",
      "shortCiteRegEx" : "Breitfeller et al\\.",
      "year" : 2019
    }, {
      "title" : "Who is mturk? personal characteristics and sample consistency of these online workers",
      "author" : [ "Martin J Burnham", "Yen K Le", "Ralph L Piedmont." ],
      "venue" : "Mental Health, Religion & Culture, 21(9-10):934–944.",
      "citeRegEx" : "Burnham et al\\.,? 2018",
      "shortCiteRegEx" : "Burnham et al\\.",
      "year" : 2018
    }, {
      "title" : "Model Positionality: A Novel Framework for Data Science with Subjective Target Concepts",
      "author" : [ "Scott Allen Cambo." ],
      "venue" : "Ph.D. thesis, Northwestern University.",
      "citeRegEx" : "Cambo.,? 2021",
      "shortCiteRegEx" : "Cambo.",
      "year" : 2021
    }, {
      "title" : "Groupbased differences in perceptions of racism: What counts, to whom, and why? Social and personality psychology compass, 9(6):269–280",
      "author" : [ "Evelyn R Carter", "Mary C Murphy" ],
      "venue" : null,
      "citeRegEx" : "Carter and Murphy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Carter and Murphy.",
      "year" : 2015
    }, {
      "title" : "Future educators’ perceptions of african american vernacular english (AAVE)",
      "author" : [ "Tempii B Champion", "Deirdre Cobb-Roberts", "Linda Bland-Stewart." ],
      "venue" : "Online Journal of Education Research, 1(5):80–89.",
      "citeRegEx" : "Champion et al\\.,? 2012",
      "shortCiteRegEx" : "Champion et al\\.",
      "year" : 2012
    }, {
      "title" : "Racism, football fans, and online message boards: How social media has added a new dimension to racist discourse in English football",
      "author" : [ "Jamie Cleland." ],
      "venue" : "J. Sport Soc. Issues, 38(5):415–431.",
      "citeRegEx" : "Cleland.,? 2014",
      "shortCiteRegEx" : "Cleland.",
      "year" : 2014
    }, {
      "title" : "Racist speech should be protected by the constitution",
      "author" : [ "D Cole." ],
      "venue" : "Hate crimes, pages 89–96.",
      "citeRegEx" : "Cole.,? 1996",
      "shortCiteRegEx" : "Cole.",
      "year" : 1996
    }, {
      "title" : "Empathy, ways of knowing, and interdependence as mediators of gender differences in attitudes toward hate speech and freedom of speech",
      "author" : [ "Gloria Cowan", "Désirée Khatchadourian." ],
      "venue" : "Psychology of Women Quarterly, 27(4):300–308.",
      "citeRegEx" : "Cowan and Khatchadourian.,? 2003",
      "shortCiteRegEx" : "Cowan and Khatchadourian.",
      "year" : 2003
    }, {
      "title" : "Hate speech and constitutional protection: Priming values of equality and freedom",
      "author" : [ "Gloria Cowan", "Miriam Resendez", "Elizabeth Marshall", "Ryan Quist." ],
      "venue" : "The Journal of social issues, 58(2):247– 263.",
      "citeRegEx" : "Cowan et al\\.,? 2002",
      "shortCiteRegEx" : "Cowan et al\\.",
      "year" : 2002
    }, {
      "title" : "Slurs",
      "author" : [ "Adam M Croom." ],
      "venue" : "Language Sciences, 33(3):343–358.",
      "citeRegEx" : "Croom.,? 2011",
      "shortCiteRegEx" : "Croom.",
      "year" : 2011
    }, {
      "title" : "Hate speech classifiers learn human-like social stereotypes",
      "author" : [ "Aida Mostafazadeh Davani", "Mohammad Atari", "Brendan Kennedy", "Morteza Dehghani" ],
      "venue" : null,
      "citeRegEx" : "Davani et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Davani et al\\.",
      "year" : 2021
    }, {
      "title" : "Racial bias in hate speech and abusive language detection datasets",
      "author" : [ "Thomas Davidson", "Debasmita Bhattacharya", "Ingmar Weber." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 25–35, Florence, Italy. Association for Com-",
      "citeRegEx" : "Davidson et al\\.,? 2019",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media.",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "Language choice matters: When profanity affects how people are judged",
      "author" : [ "Melanie DeFrank", "Patricia Kahlbaugh." ],
      "venue" : "Journal of Language and Social Psychology, 38(1):126–141.",
      "citeRegEx" : "DeFrank and Kahlbaugh.,? 2019",
      "shortCiteRegEx" : "DeFrank and Kahlbaugh.",
      "year" : 2019
    }, {
      "title" : "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
      "author" : [ "Emily Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Predicting the importance of freedom of speech and the perceived harm of hate speech",
      "author" : [ "Daniel M Downs", "Gloria Cowan." ],
      "venue" : "Journal of applied social psychology, 42(6):1353–1375.",
      "citeRegEx" : "Downs and Cowan.,? 2012",
      "shortCiteRegEx" : "Downs and Cowan.",
      "year" : 2012
    }, {
      "title" : "The impact of social threat on worldview and ideological attitudes",
      "author" : [ "John Duckitt", "Kirstin Fisher." ],
      "venue" : "Political Psychology, 24(1):199–222.",
      "citeRegEx" : "Duckitt and Fisher.,? 2003",
      "shortCiteRegEx" : "Duckitt and Fisher.",
      "year" : 2003
    }, {
      "title" : "Swearing methodologically : the (im)politeness of expletives in anonymous commentaries on youtube",
      "author" : [ "Marta Dynel." ],
      "venue" : "Journal of English Studies, 10(0):25–50.",
      "citeRegEx" : "Dynel.,? 2012",
      "shortCiteRegEx" : "Dynel.",
      "year" : 2012
    }, {
      "title" : "African american vernacular english: phonology",
      "author" : [ "Walter F Edwards." ],
      "venue" : "A Handbook of Varieties of English: Morphology and syntax.",
      "citeRegEx" : "Edwards.,? 2004",
      "shortCiteRegEx" : "Edwards.",
      "year" : 2004
    }, {
      "title" : "this is us”: Free speech embedded in whiteness, racism and coloniality in aotearoa, new zealand",
      "author" : [ "Christine Helen Elers", "Pooja Jayan." ],
      "venue" : "First Amendment Studies, 54(2):236–249.",
      "citeRegEx" : "Elers and Jayan.,? 2020",
      "shortCiteRegEx" : "Elers and Jayan.",
      "year" : 2020
    }, {
      "title" : "Controlling other people",
      "author" : [ "S T Fiske." ],
      "venue" : "the impact of power on stereotyping. The American psychologist, 48(6):621–628.",
      "citeRegEx" : "Fiske.,? 1993",
      "shortCiteRegEx" : "Fiske.",
      "year" : 1993
    }, {
      "title" : "Large scale crowdsourcing and characterization of twitter",
      "author" : [ "Antigoni Maria Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "Nicolas Kourtellis" ],
      "venue" : null,
      "citeRegEx" : "Founta et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta et al\\.",
      "year" : 2018
    }, {
      "title" : "Value sensitive design and information systems",
      "author" : [ "Batya Friedman", "Peter H Kahn", "Alan Borning." ],
      "venue" : "The handbook of information and computer ethics, pages 69–101.",
      "citeRegEx" : "Friedman et al\\.,? 2008",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2008
    }, {
      "title" : "Sounding black or white: Priming identity and biracial speech",
      "author" : [ "Sarah E Gaither", "Ariel M Cohen-Goldberg", "Calvin L Gidney", "Keith B Maddox." ],
      "venue" : "Frontiers in Psychology, 6:457.",
      "citeRegEx" : "Gaither et al\\.,? 2015",
      "shortCiteRegEx" : "Gaither et al\\.",
      "year" : 2015
    }, {
      "title" : "The reappropriation of stigmatizing labels: the reciprocal relationship between power and self-labeling",
      "author" : [ "Adam D Galinsky", "Cynthia S Wang", "Jennifer A Whitson", "Eric M Anicich", "Kurt Hugenberg", "Galen V Bodenhausen." ],
      "venue" : "Psychol. Sci.,",
      "citeRegEx" : "Galinsky et al\\.,? 2013",
      "shortCiteRegEx" : "Galinsky et al\\.",
      "year" : 2013
    }, {
      "title" : "A solution to the collective action problem in betweengroup conflict with within-group inequality",
      "author" : [ "Sergey Gavrilets", "Laura Fortunato." ],
      "venue" : "Nature communications, 5(1):1–11.",
      "citeRegEx" : "Gavrilets and Fortunato.,? 2014",
      "shortCiteRegEx" : "Gavrilets and Fortunato.",
      "year" : 2014
    }, {
      "title" : "Datasheets for datasets",
      "author" : [ "Timnit Gebru", "Jamie Morgenstern", "Briana Vecchione", "Jennifer Wortman Vaughan", "Hanna Wallach", "Hal Daumé III", "Kate Crawford." ],
      "venue" : "FAccT*.",
      "citeRegEx" : "Gebru et al\\.,? 2018",
      "shortCiteRegEx" : "Gebru et al\\.",
      "year" : 2018
    }, {
      "title" : "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets",
      "author" : [ "Mor Geva", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Geva et al\\.,? 2019",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2019
    }, {
      "title" : "Risk-free racism: Whiteness and so-called free speech",
      "author" : [ "David Gillborn." ],
      "venue" : "Wake Forest law review, 44:535.",
      "citeRegEx" : "Gillborn.,? 2009",
      "shortCiteRegEx" : "Gillborn.",
      "year" : 2009
    }, {
      "title" : "Expanding the debate about content",
      "author" : [ "Tarleton Gillespie", "Patricia Aufderheide", "Elinor Carmi", "Ysabel Gerrard", "Robert Gorwa", "Ariadna Matamoros-Fernandez", "Sarah T Roberts", "Aram Sinnreich", "Sarah Myers West" ],
      "venue" : "Scholarly",
      "citeRegEx" : "Gillespie et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gillespie et al\\.",
      "year" : 2020
    }, {
      "title" : "Postelection surrealism and nostalgic racism in the hands of donald trump",
      "author" : [ "Donna M Goldstein", "Kira Hall." ],
      "venue" : "HAU: Journal of Ethnographic Theory, 7(1):397–406.",
      "citeRegEx" : "Goldstein and Hall.,? 2017",
      "shortCiteRegEx" : "Goldstein and Hall.",
      "year" : 2017
    }, {
      "title" : "Data science as political action: Grounding data science in a politics of justice",
      "author" : [ "Ben Green." ],
      "venue" : "SSRN Electronic Journal.",
      "citeRegEx" : "Green.,? 2020",
      "shortCiteRegEx" : "Green.",
      "year" : 2020
    }, {
      "title" : "African American English: A Linguistic Introduction, 8.3.2002 edition edition",
      "author" : [ "Lisa Green" ],
      "venue" : null,
      "citeRegEx" : "Green.,? \\Q2002\\E",
      "shortCiteRegEx" : "Green.",
      "year" : 2002
    }, {
      "title" : "Fortifying toxic speech detectors against veiled toxicity",
      "author" : [ "Xiaochuang Han", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7732–7739, Online. Associa-",
      "citeRegEx" : "Han and Tsvetkov.,? 2020",
      "shortCiteRegEx" : "Han and Tsvetkov.",
      "year" : 2020
    }, {
      "title" : "Language, culture and the assessment of african american children",
      "author" : [ "A G Hilliard." ],
      "venue" : "Assessment for equity and inclusion: Embracing all our children, pages 229–240.",
      "citeRegEx" : "Hilliard.,? 1997",
      "shortCiteRegEx" : "Hilliard.",
      "year" : 1997
    }, {
      "title" : "who are these people?” evaluating the demographic characteristics and political preferences of mturk survey respondents",
      "author" : [ "Connor Huff", "Dustin Tingley." ],
      "venue" : "Research & Politics, 2(3):2053168015604648.",
      "citeRegEx" : "Huff and Tingley.,? 2015",
      "shortCiteRegEx" : "Huff and Tingley.",
      "year" : 2015
    }, {
      "title" : "Swearing, moral order, and online communication",
      "author" : [ "Timothy Jay." ],
      "venue" : "Journal of Language Aggression and Conflict, 6(1):107–126.",
      "citeRegEx" : "Jay.,? 2018",
      "shortCiteRegEx" : "Jay.",
      "year" : 2018
    }, {
      "title" : "The pragmatics of swearing",
      "author" : [ "Timothy Jay", "Kristin Janschewitz." ],
      "venue" : "Journal of Politeness Research, 4:267288.",
      "citeRegEx" : "Jay and Janschewitz.,? 2008",
      "shortCiteRegEx" : "Jay and Janschewitz.",
      "year" : 2008
    }, {
      "title" : "The politics of language purism",
      "author" : [ "Björn H Jernudd", "Michael J Shapiro." ],
      "venue" : "Mouton de Gruyter Berlin.",
      "citeRegEx" : "Jernudd and Shapiro.,? 1989",
      "shortCiteRegEx" : "Jernudd and Shapiro.",
      "year" : 1989
    }, {
      "title" : "Identifying and addressing design and policy challenges in online content moderation",
      "author" : [ "J A Jiang." ],
      "venue" : "Extended Abstracts of the 2020 CHI Conference on.",
      "citeRegEx" : "Jiang.,? 2020",
      "shortCiteRegEx" : "Jiang.",
      "year" : 2020
    }, {
      "title" : "Understanding international perceptions of the severity of harmful content online",
      "author" : [ "Jialun Aaron Jiang", "Morgan Klaus Scheuerman", "Casey Fiesler", "Jed R Brubaker." ],
      "venue" : "PloS one, 16(8):e0256762.",
      "citeRegEx" : "Jiang et al\\.,? 2021",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Social traditionalism and economic conservatism: Two conservative political ideologies in the united states",
      "author" : [ "Stephen D Johnson", "Joseph B Tamney." ],
      "venue" : "The Journal of social psychology, 141(2):233–243.",
      "citeRegEx" : "Johnson and Tamney.,? 2001",
      "shortCiteRegEx" : "Johnson and Tamney.",
      "year" : 2001
    }, {
      "title" : "Don’t ask if artificial intelligence is good or fair, ask how it shifts power",
      "author" : [ "Pratyusha Kalluri." ],
      "venue" : "Nature, 583(7815):169.",
      "citeRegEx" : "Kalluri.,? 2020",
      "shortCiteRegEx" : "Kalluri.",
      "year" : 2020
    }, {
      "title" : "A new front in the culture war? moral traditionalism and voting behavior in us house elections",
      "author" : [ "Jonathan Knuckey." ],
      "venue" : "American Politics Research, 33(5):645–671.",
      "citeRegEx" : "Knuckey.,? 2005",
      "shortCiteRegEx" : "Knuckey.",
      "year" : 2005
    }, {
      "title" : "Distinction without a difference? an assessment of mturk worker types",
      "author" : [ "Eric Loepp", "Jarrod T Kelly." ],
      "venue" : "Research & Politics, 7(1):2053168019901185.",
      "citeRegEx" : "Loepp and Kelly.,? 2020",
      "shortCiteRegEx" : "Loepp and Kelly.",
      "year" : 2020
    }, {
      "title" : "Reconsidering Reagan: Racism, Republicans, and the Road to Trump",
      "author" : [ "Daniel S Lucks." ],
      "venue" : "Beacon Press.",
      "citeRegEx" : "Lucks.,? 2020",
      "shortCiteRegEx" : "Lucks.",
      "year" : 2020
    }, {
      "title" : "Modern racism, ambivalence, and the modern racism scale",
      "author" : [ "John B McConahay." ],
      "venue" : "John F Dovidio, editor, Prejudice, discrimination, and racism, volume 337, pages 91–125. Academic Press, xiii, San Diego, CA, US.",
      "citeRegEx" : "McConahay.,? 1986",
      "shortCiteRegEx" : "McConahay.",
      "year" : 1986
    }, {
      "title" : "The radicalization risks of gpt-3 and advanced neural language models",
      "author" : [ "Kris McGuffie", "Alex Newhouse" ],
      "venue" : null,
      "citeRegEx" : "McGuffie and Newhouse.,? \\Q2020\\E",
      "shortCiteRegEx" : "McGuffie and Newhouse.",
      "year" : 2020
    }, {
      "title" : "Hate speech or “reasonable racism?” the other in stormfront",
      "author" : [ "Priscilla Marie Meddaugh", "Jack Kay." ],
      "venue" : "Journal of Mass Media Ethics, 24(4):251– 268.",
      "citeRegEx" : "Meddaugh and Kay.,? 2009",
      "shortCiteRegEx" : "Meddaugh and Kay.",
      "year" : 2009
    }, {
      "title" : "Conservatives and incivility",
      "author" : [ "Ashley Muddiman." ],
      "venue" : "Conservative Political Communication, pages 119–136. Routledge.",
      "citeRegEx" : "Muddiman.,? 2021",
      "shortCiteRegEx" : "Muddiman.",
      "year" : 2021
    }, {
      "title" : "Microaggressions and traumatic stress: Theory, research, and clinical treatment",
      "author" : [ "Kevin L Nadal." ],
      "venue" : "American Psychological Association.",
      "citeRegEx" : "Nadal.,? 2018",
      "shortCiteRegEx" : "Nadal.",
      "year" : 2018
    }, {
      "title" : "The marley hypothesis: Denial of racism reflects ignorance of history",
      "author" : [ "Jessica C Nelson", "Glenn Adams", "Phia S Salter." ],
      "venue" : "Psychological science, 24(2):213–218.",
      "citeRegEx" : "Nelson et al\\.,? 2013",
      "shortCiteRegEx" : "Nelson et al\\.",
      "year" : 2013
    }, {
      "title" : "Whites see racism as a Zero-Sum game that they are now losing",
      "author" : [ "Michael I Norton", "Samuel R Sommers." ],
      "venue" : "Perspectives on psychological science: a journal of the Association for Psychological Science, 6(3):215–218.",
      "citeRegEx" : "Norton and Sommers.,? 2011",
      "shortCiteRegEx" : "Norton and Sommers.",
      "year" : 2011
    }, {
      "title" : "Inherent disagreements in human textual inferences",
      "author" : [ "Ellie Pavlick", "Tom Kwiatkowski." ],
      "venue" : "TACL, 7:677–694.",
      "citeRegEx" : "Pavlick and Kwiatkowski.,? 2019",
      "shortCiteRegEx" : "Pavlick and Kwiatkowski.",
      "year" : 2019
    }, {
      "title" : "Modern racism attitudes among white students: The role of dominance and authoritarianism and the mediating effects of racial color-blindness",
      "author" : [ "V Paul Poteat", "Lisa B Spanierman." ],
      "venue" : "The Journal of Social Psychology, 152(6):758–774.",
      "citeRegEx" : "Poteat and Spanierman.,? 2012",
      "shortCiteRegEx" : "Poteat and Spanierman.",
      "year" : 2012
    }, {
      "title" : "On releasing Annotator-Level labels and information in datasets",
      "author" : [ "Vinodkumar Prabhakaran", "Aida Mostafazadeh Davani", "Mark Díaz." ],
      "venue" : "Proc. of LAWDMR workshop at EMNLP.",
      "citeRegEx" : "Prabhakaran et al\\.,? 2021",
      "shortCiteRegEx" : "Prabhakaran et al\\.",
      "year" : 2021
    }, {
      "title" : "The hierarchical structure of the interpersonal reactivity index",
      "author" : [ "Steven Pulos", "Jeff Elison", "Randy Lennon." ],
      "venue" : "Social behavior and personality, 32(4):355–360.",
      "citeRegEx" : "Pulos et al\\.,? 2004",
      "shortCiteRegEx" : "Pulos et al\\.",
      "year" : 2004
    }, {
      "title" : "The new utilities: Private power, social infrastructure, and the revival of the public utility concept",
      "author" : [ "K Sabeel Rahman." ],
      "venue" : "Cardozo law review, 39:1621.",
      "citeRegEx" : "Rahman.,? 2017",
      "shortCiteRegEx" : "Rahman.",
      "year" : 2017
    }, {
      "title" : "How game features give rise to altruism and collective action? implications for cultivating cooperation by gamification",
      "author" : [ "Marc Riar", "Benedikt Morschheuser", "Juho Hamari", "Rüdiger Zarnekow." ],
      "venue" : "Proceedings of the 53rd Hawaii International Confer-",
      "citeRegEx" : "Riar et al\\.,? 2020",
      "shortCiteRegEx" : "Riar et al\\.",
      "year" : 2020
    }, {
      "title" : "Looking Like a Language, Sounding Like a Race",
      "author" : [ "Jonathan Rosa." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Rosa.,? 2019",
      "shortCiteRegEx" : "Rosa.",
      "year" : 2019
    }, {
      "title" : "Unsettling race and language: Toward a raciolinguistic perspective",
      "author" : [ "Jonathan Rosa", "Nelson Flores." ],
      "venue" : "Language In Society, 46(5):621–647.",
      "citeRegEx" : "Rosa and Flores.,? 2017",
      "shortCiteRegEx" : "Rosa and Flores.",
      "year" : 2017
    }, {
      "title" : "Measuring the reliability of hate speech annotations: the case of the european refugee crisis",
      "author" : [ "Björn Ross", "Michael Rist", "Guillermo Carbonell", "Benjamin Cabrera", "Nils Kurowsky", "Michael Wojatzki." ],
      "venue" : "NLP 4 CMC Workshop.",
      "citeRegEx" : "Ross et al\\.,? 2017",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2017
    }, {
      "title" : "Hate speech is in the eye of the beholder: The influence of racial attitudes and freedom of speech beliefs on perceptions of racially motivated threats of violence",
      "author" : [ "Gina Roussos", "John F Dovidio." ],
      "venue" : "Social psychological and personality science,",
      "citeRegEx" : "Roussos and Dovidio.,? 2018",
      "shortCiteRegEx" : "Roussos and Dovidio.",
      "year" : 2018
    }, {
      "title" : "From user-centered to participatory design approaches, pages 1–7",
      "author" : [ "Elizabeth Sanders" ],
      "venue" : null,
      "citeRegEx" : "Sanders.,? \\Q2002\\E",
      "shortCiteRegEx" : "Sanders.",
      "year" : 2002
    }, {
      "title" : "The risk of racial bias in hate speech detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668–1678, Florence, Italy.",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A Smith", "Yejin Choi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "Rating offensive words in three television program contexts",
      "author" : [ "Barry S. Sapolsky", "Daniel M. Shafer", "Barbara K. Kaye." ],
      "venue" : "Mass Communication and Society, 14(1):45–70.",
      "citeRegEx" : "Sapolsky et al\\.,? 2010",
      "shortCiteRegEx" : "Sapolsky et al\\.",
      "year" : 2010
    }, {
      "title" : "The heightened importance of racism and sexism in the 2018 US midterm elections",
      "author" : [ "Brian F Schaffner." ],
      "venue" : "British journal of political science, pages 1–9.",
      "citeRegEx" : "Schaffner.,? 2020",
      "shortCiteRegEx" : "Schaffner.",
      "year" : 2020
    }, {
      "title" : "Analyzing the targets of hate in online social media",
      "author" : [ "Leandro Silva", "Mainack Mondal", "Denzil Correa", "Fabrício Benevenuto", "Ingmar Weber." ],
      "venue" : "Tenth international AAAI conference on web and social media.",
      "citeRegEx" : "Silva et al\\.,? 2016",
      "shortCiteRegEx" : "Silva et al\\.",
      "year" : 2016
    }, {
      "title" : "Averaging correlation coefficients: should fisher’s z transformation be used",
      "author" : [ "N Clayton Silver", "William P Dunlap" ],
      "venue" : "Journal of applied psychology,",
      "citeRegEx" : "Silver and Dunlap.,? \\Q1987\\E",
      "shortCiteRegEx" : "Silver and Dunlap.",
      "year" : 1987
    }, {
      "title" : "Exposure to hate speech increases prejudice through desensitization",
      "author" : [ "Wiktor Soral", "Michał Bilewicz", "Mikołaj Winiewski." ],
      "venue" : "Aggressive behavior, 44(2):136–146.",
      "citeRegEx" : "Soral et al\\.,? 2018",
      "shortCiteRegEx" : "Soral et al\\.",
      "year" : 2018
    }, {
      "title" : "African-american language use: Ideology and so-called obscenity",
      "author" : [ "Arthur K Spears" ],
      "venue" : "African-American English: Structure, history, and use, pages 226–250.",
      "citeRegEx" : "Spears,? 1998",
      "shortCiteRegEx" : "Spears",
      "year" : 1998
    }, {
      "title" : "The significance of hedonic values for environmentally relevant attitudes, preferences, and actions",
      "author" : [ "Linda Steg", "Goda Perlaviciute", "Ellen Van der Werff", "Judith Lurvink." ],
      "venue" : "Environment and behavior, 46(2):163–192.",
      "citeRegEx" : "Steg et al\\.,? 2014",
      "shortCiteRegEx" : "Steg et al\\.",
      "year" : 2014
    }, {
      "title" : "Political psycholinguistics: A comprehensive analysis of the language habits of liberal and conservative social media users",
      "author" : [ "Joanna Sterling", "John T Jost", "Richard Bonneau." ],
      "venue" : "Journal of personality and social psychology, 118(4):805–834.",
      "citeRegEx" : "Sterling et al\\.,? 2020",
      "shortCiteRegEx" : "Sterling et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning from the worst: Dynamically generated datasets to improve online hate detection",
      "author" : [ "Bertie Vidgen", "Tristan Thrush", "Zeerak Waseem", "Douwe Kiela." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Vidgen et al\\.,? 2021",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2021
    }, {
      "title" : "Equity, justice, and altruism",
      "author" : [ "Graham F Wagstaff." ],
      "venue" : "Current Psychology, 17(2):111–134.",
      "citeRegEx" : "Wagstaff.,? 1998",
      "shortCiteRegEx" : "Wagstaff.",
      "year" : 1998
    }, {
      "title" : "Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the first workshop on NLP and computational social science, pages 138– 142.",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88–93, San Diego, California. Association for Computa-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Disembodied machine learning: On the illusion of objectivity in nlp",
      "author" : [ "Zeerak Waseem", "Smarika Lulz", "Joachim Bingel", "Isabelle Augenstein." ],
      "venue" : "Anonymous preprint under review.",
      "citeRegEx" : "Waseem et al\\.,? 2021",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2021
    }, {
      "title" : "Freedom of racist speech: Ego and expressive threats",
      "author" : [ "Mark H White", "Christian S Crandall." ],
      "venue" : "Journal of personality and social psychology, 113(3):413–429.",
      "citeRegEx" : "White and Crandall.,? 2017",
      "shortCiteRegEx" : "White and Crandall.",
      "year" : 2017
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "Proceedings of the 26th international conference on world wide web, pages 1391–1399.",
      "citeRegEx" : "Wulczyn et al\\.,? 2017",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Black and banned: Who is free speech for",
      "author" : [ "Danyaal Yasin." ],
      "venue" : "Accessed: 2018-12-6.",
      "citeRegEx" : "Yasin.,? 2018",
      "shortCiteRegEx" : "Yasin.",
      "year" : 2018
    }, {
      "title" : "Predicting the type and target of offensive posts in social media",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "Sociology of race",
      "author" : [ "Zuleyka Zevallos." ],
      "venue" : ". Accessed: 2021-10-10.",
      "citeRegEx" : "Zevallos.,? 2017",
      "shortCiteRegEx" : "Zevallos.",
      "year" : 2017
    }, {
      "title" : "Challenges in automated debiasing for toxic language detection",
      "author" : [ "Xuhui Zhou", "Maarten Sap", "Swabha Swayamdipta", "Yejin Choi", "Noah Smith." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Perceiving the HARMOFHATESPEECH. Participants were instructed to rate their level of agreement with the item statements, using a 5",
      "author" : [ "Cowan" ],
      "venue" : null,
      "citeRegEx" : "Cowan,? \\Q2002\\E",
      "shortCiteRegEx" : "Cowan",
      "year" : 2002
    }, {
      "title" : "For each tweet in that corpus, we detected the presence of non-identity related profanity or swearwords using the list from Zhou et al. (2021), and extracted the likelihood that the tweet is in AAE using a lexical detector",
      "author" : [ "Founta" ],
      "venue" : null,
      "citeRegEx" : "Founta,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta",
      "year" : 2018
    }, {
      "title" : "As candidates, we selected 10 vulgar tweets that have low likelihood of being AAE, and 26 tweets that have high likelihood of being AAE but contain no vulgarity",
      "author" : [ "Blodgett" ],
      "venue" : "For anti-Black posts,",
      "citeRegEx" : "Blodgett,? \\Q2016\\E",
      "shortCiteRegEx" : "Blodgett",
      "year" : 2016
    }, {
      "title" : "2019), we do not consider posts that are annotated as anti-Black but detected as AAE",
      "author" : [ "Davidson" ],
      "venue" : null,
      "citeRegEx" : "Davidson,? \\Q2019\\E",
      "shortCiteRegEx" : "Davidson",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : ", contains hate speech, abuse, or is offensive) is inherently a subjective task that requires a nuanced understanding of the pragmatic implications of language (Fiske, 1993; Croom, 2011; Waseem et al., 2021).",
      "startOffset" : 160,
      "endOffset" : 207
    }, {
      "referenceID" : 20,
      "context" : ", contains hate speech, abuse, or is offensive) is inherently a subjective task that requires a nuanced understanding of the pragmatic implications of language (Fiske, 1993; Croom, 2011; Waseem et al., 2021).",
      "startOffset" : 160,
      "endOffset" : 207
    }, {
      "referenceID" : 89,
      "context" : ", contains hate speech, abuse, or is offensive) is inherently a subjective task that requires a nuanced understanding of the pragmatic implications of language (Fiske, 1993; Croom, 2011; Waseem et al., 2021).",
      "startOffset" : 160,
      "endOffset" : 207
    }, {
      "referenceID" : 25,
      "context" : "Without this nuance, both humans and machines are prone to biased judgments, such as over-relying on seemingly toxic keywords (e.g., expletives, swearwords; Dinan et al., 2019; Han and Tsvetkov, 2020) or backfiring against minorities (Yasin, 2018; Are, 2020, i.",
      "startOffset" : 126,
      "endOffset" : 200
    }, {
      "referenceID" : 44,
      "context" : "Without this nuance, both humans and machines are prone to biased judgments, such as over-relying on seemingly toxic keywords (e.g., expletives, swearwords; Dinan et al., 2019; Han and Tsvetkov, 2020) or backfiring against minorities (Yasin, 2018; Are, 2020, i.",
      "startOffset" : 126,
      "endOffset" : 200
    }, {
      "referenceID" : 75,
      "context" : "For example, racial biases have been uncovered in toxic language detection where text written in African American English (AAE) is falsely flagged as toxic (Sap et al., 2019; Davidson et al., 2019).",
      "startOffset" : 156,
      "endOffset" : 197
    }, {
      "referenceID" : 22,
      "context" : "For example, racial biases have been uncovered in toxic language detection where text written in African American English (AAE) is falsely flagged as toxic (Sap et al., 2019; Davidson et al., 2019).",
      "startOffset" : 156,
      "endOffset" : 197
    }, {
      "referenceID" : 87,
      "context" : "The crux of the issue is that not all text is equally toxic for everyone (Waseem, 2016; Al Kuwatly et al., 2020; Jiang et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 132
    }, {
      "referenceID" : 51,
      "context" : "The crux of the issue is that not all text is equally toxic for everyone (Waseem, 2016; Al Kuwatly et al., 2020; Jiang et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 132
    }, {
      "referenceID" : 88,
      "context" : "Yet, most previous research has treated this detection as a simple classification with one correct label, obtained by averaging judgments by a small set of human annotators per post (Waseem and Hovy, 2016; Wulczyn et al., 2017; Davidson et al., 2017; Founta et al., 2018; Zampieri et al., 2019).",
      "startOffset" : 182,
      "endOffset" : 294
    }, {
      "referenceID" : 91,
      "context" : "Yet, most previous research has treated this detection as a simple classification with one correct label, obtained by averaging judgments by a small set of human annotators per post (Waseem and Hovy, 2016; Wulczyn et al., 2017; Davidson et al., 2017; Founta et al., 2018; Zampieri et al., 2019).",
      "startOffset" : 182,
      "endOffset" : 294
    }, {
      "referenceID" : 23,
      "context" : "Yet, most previous research has treated this detection as a simple classification with one correct label, obtained by averaging judgments by a small set of human annotators per post (Waseem and Hovy, 2016; Wulczyn et al., 2017; Davidson et al., 2017; Founta et al., 2018; Zampieri et al., 2019).",
      "startOffset" : 182,
      "endOffset" : 294
    }, {
      "referenceID" : 32,
      "context" : "Yet, most previous research has treated this detection as a simple classification with one correct label, obtained by averaging judgments by a small set of human annotators per post (Waseem and Hovy, 2016; Wulczyn et al., 2017; Davidson et al., 2017; Founta et al., 2018; Zampieri et al., 2019).",
      "startOffset" : 182,
      "endOffset" : 294
    }, {
      "referenceID" : 93,
      "context" : "Yet, most previous research has treated this detection as a simple classification with one correct label, obtained by averaging judgments by a small set of human annotators per post (Waseem and Hovy, 2016; Wulczyn et al., 2017; Davidson et al., 2017; Founta et al., 2018; Zampieri et al., 2019).",
      "startOffset" : 182,
      "endOffset" : 294
    }, {
      "referenceID" : 64,
      "context" : "Such approaches ignore the variance in annotations (Pavlick and Kwiatkowski, 2019; Geva et al., 2019; Akhtar et al., 2021) based on who the annotators are, and what their beliefs are.",
      "startOffset" : 51,
      "endOffset" : 122
    }, {
      "referenceID" : 38,
      "context" : "Such approaches ignore the variance in annotations (Pavlick and Kwiatkowski, 2019; Geva et al., 2019; Akhtar et al., 2021) based on who the annotators are, and what their beliefs are.",
      "startOffset" : 51,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "Such approaches ignore the variance in annotations (Pavlick and Kwiatkowski, 2019; Geva et al., 2019; Akhtar et al., 2021) based on who the annotators are, and what their beliefs are.",
      "startOffset" : 51,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : ", for variation in detecting vulgar content as toxic) or representational harms (e.g., AAE being over-detected as toxic or anti-Black content being underdetected as toxic; Barocas et al., 2017; Blodgett et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 216
    }, {
      "referenceID" : 7,
      "context" : ", for variation in detecting vulgar content as toxic) or representational harms (e.g., AAE being over-detected as toxic or anti-Black content being underdetected as toxic; Barocas et al., 2017; Blodgett et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 216
    }, {
      "referenceID" : 19,
      "context" : "Prior work has extensively shown links between someone’s gender, political leaning, and race affects how likely they are to perceive or notice harmful speech or racism (Cowan et al., 2002; Norton and Sommers, 2011; Carter and Murphy, 2015; Prabhakaran et al., 2021).",
      "startOffset" : 168,
      "endOffset" : 265
    }, {
      "referenceID" : 63,
      "context" : "Prior work has extensively shown links between someone’s gender, political leaning, and race affects how likely they are to perceive or notice harmful speech or racism (Cowan et al., 2002; Norton and Sommers, 2011; Carter and Murphy, 2015; Prabhakaran et al., 2021).",
      "startOffset" : 168,
      "endOffset" : 265
    }, {
      "referenceID" : 14,
      "context" : "Prior work has extensively shown links between someone’s gender, political leaning, and race affects how likely they are to perceive or notice harmful speech or racism (Cowan et al., 2002; Norton and Sommers, 2011; Carter and Murphy, 2015; Prabhakaran et al., 2021).",
      "startOffset" : 168,
      "endOffset" : 265
    }, {
      "referenceID" : 66,
      "context" : "Prior work has extensively shown links between someone’s gender, political leaning, and race affects how likely they are to perceive or notice harmful speech or racism (Cowan et al., 2002; Norton and Sommers, 2011; Carter and Murphy, 2015; Prabhakaran et al., 2021).",
      "startOffset" : 168,
      "endOffset" : 265
    }, {
      "referenceID" : 66,
      "context" : "While some annotator toxicity ratings may highly correlate with demographic factors at face value (Prabhakaran et al., 2021; Jiang et al., 2021), we aim to go beyond demographics to investigate annotator beliefs that explain these correlations.",
      "startOffset" : 98,
      "endOffset" : 144
    }, {
      "referenceID" : 51,
      "context" : "While some annotator toxicity ratings may highly correlate with demographic factors at face value (Prabhakaran et al., 2021; Jiang et al., 2021), we aim to go beyond demographics to investigate annotator beliefs that explain these correlations.",
      "startOffset" : 98,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : "Recently, this belief has become associated with majority and conservative identities (Cole, 1996; Meddaugh and Kay, 2009; Gillborn, 2009; White and Crandall, 2017; Elers and Jayan, 2020).",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 59,
      "context" : "Recently, this belief has become associated with majority and conservative identities (Cole, 1996; Meddaugh and Kay, 2009; Gillborn, 2009; White and Crandall, 2017; Elers and Jayan, 2020).",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 39,
      "context" : "Recently, this belief has become associated with majority and conservative identities (Cole, 1996; Meddaugh and Kay, 2009; Gillborn, 2009; White and Crandall, 2017; Elers and Jayan, 2020).",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 90,
      "context" : "Recently, this belief has become associated with majority and conservative identities (Cole, 1996; Meddaugh and Kay, 2009; Gillborn, 2009; White and Crandall, 2017; Elers and Jayan, 2020).",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 30,
      "context" : "Recently, this belief has become associated with majority and conservative identities (Cole, 1996; Meddaugh and Kay, 2009; Gillborn, 2009; White and Crandall, 2017; Elers and Jayan, 2020).",
      "startOffset" : 86,
      "endOffset" : 187
    }, {
      "referenceID" : 81,
      "context" : "Perceiving the HARMOFHATESPEECH: the belief that hate speech or offensive language can be harmful for the targets of that speech (Soral et al., 2018; Nadal, 2018).",
      "startOffset" : 129,
      "endOffset" : 162
    }, {
      "referenceID" : 61,
      "context" : "Perceiving the HARMOFHATESPEECH: the belief that hate speech or offensive language can be harmful for the targets of that speech (Soral et al., 2018; Nadal, 2018).",
      "startOffset" : 129,
      "endOffset" : 162
    }, {
      "referenceID" : 65,
      "context" : "Endorsement of RACISTBELIEFS: the beliefs which deny the existence of racial inequality, or capture resentment towards racial minorities (Poteat and Spanierman, 2012).",
      "startOffset" : 137,
      "endOffset" : 166
    }, {
      "referenceID" : 57,
      "context" : "We measure RACISTBELIEFS using items from the validated Modern Racism Scale (McConahay, 1986); see Appendix §A.",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 52,
      "context" : "In the US, these beliefs are associated with generally conservative ideologies (Johnson and Tamney, 2001; Knuckey, 2005).",
      "startOffset" : 79,
      "endOffset" : 120
    }, {
      "referenceID" : 54,
      "context" : "In the US, these beliefs are associated with generally conservative ideologies (Johnson and Tamney, 2001; Knuckey, 2005).",
      "startOffset" : 79,
      "endOffset" : 120
    }, {
      "referenceID" : 49,
      "context" : "Language Purism (LINGPURISM): the belief that there is a “correct” way of using English (Jernudd and Shapiro, 1989).",
      "startOffset" : 88,
      "endOffset" : 115
    }, {
      "referenceID" : 77,
      "context" : "Typically, this belief also involves negative reactions to non-canonical ways of using language (Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019).",
      "startOffset" : 96,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : "Typically, this belief also involves negative reactions to non-canonical ways of using language (Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019).",
      "startOffset" : 96,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "the ability and willingness of recognizing and labeling hate speech (Cowan and Khatchadourian, 2003).",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 67,
      "context" : "We measure EMPATHY using an abbreviated Interpersonal Reactivity Index (Pulos et al., 2004); see Appendix §A.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 86,
      "context" : "ALTRUISM: one’s attitude of selfless concern about others’ well-being, which can move people to act when harm or injustice happens (Wagstaff, 1998; Gavrilets and Fortunato, 2014; Riar et al., 2020), including harms through hate speech (Cowan et al.",
      "startOffset" : 131,
      "endOffset" : 197
    }, {
      "referenceID" : 36,
      "context" : "ALTRUISM: one’s attitude of selfless concern about others’ well-being, which can move people to act when harm or injustice happens (Wagstaff, 1998; Gavrilets and Fortunato, 2014; Riar et al., 2020), including harms through hate speech (Cowan et al.",
      "startOffset" : 131,
      "endOffset" : 197
    }, {
      "referenceID" : 69,
      "context" : "ALTRUISM: one’s attitude of selfless concern about others’ well-being, which can move people to act when harm or injustice happens (Wagstaff, 1998; Gavrilets and Fortunato, 2014; Riar et al., 2020), including harms through hate speech (Cowan et al.",
      "startOffset" : 131,
      "endOffset" : 197
    }, {
      "referenceID" : 19,
      "context" : ", 2020), including harms through hate speech (Cowan et al., 2002).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "Specifically, we consider three dimensions or categories of text, based on recent work on text characteristics that tend to be over- or under-detected as toxic (Dinan et al., 2019; Sap et al., 2019; Han and Tsvetkov, 2020; Zhou et al., 2021): anti-Black language, presence of African American Engligh (AAE) dialect markers, and vulgar language (e.",
      "startOffset" : 160,
      "endOffset" : 241
    }, {
      "referenceID" : 75,
      "context" : "Specifically, we consider three dimensions or categories of text, based on recent work on text characteristics that tend to be over- or under-detected as toxic (Dinan et al., 2019; Sap et al., 2019; Han and Tsvetkov, 2020; Zhou et al., 2021): anti-Black language, presence of African American Engligh (AAE) dialect markers, and vulgar language (e.",
      "startOffset" : 160,
      "endOffset" : 241
    }, {
      "referenceID" : 44,
      "context" : "Specifically, we consider three dimensions or categories of text, based on recent work on text characteristics that tend to be over- or under-detected as toxic (Dinan et al., 2019; Sap et al., 2019; Han and Tsvetkov, 2020; Zhou et al., 2021): anti-Black language, presence of African American Engligh (AAE) dialect markers, and vulgar language (e.",
      "startOffset" : 160,
      "endOffset" : 241
    }, {
      "referenceID" : 95,
      "context" : "Specifically, we consider three dimensions or categories of text, based on recent work on text characteristics that tend to be over- or under-detected as toxic (Dinan et al., 2019; Sap et al., 2019; Han and Tsvetkov, 2020; Zhou et al., 2021): anti-Black language, presence of African American Engligh (AAE) dialect markers, and vulgar language (e.",
      "startOffset" : 160,
      "endOffset" : 241
    }, {
      "referenceID" : 11,
      "context" : "Anti-Black language denotes racially prejudiced or racist content—subtle (Breitfeller et al., 2019) or overt—which is often a desired target for toxic language detection research (Waseem, 2016; Vidgen et al.",
      "startOffset" : 73,
      "endOffset" : 99
    }, {
      "referenceID" : 87,
      "context" : ", 2019) or overt—which is often a desired target for toxic language detection research (Waseem, 2016; Vidgen et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 122
    }, {
      "referenceID" : 85,
      "context" : ", 2019) or overt—which is often a desired target for toxic language detection research (Waseem, 2016; Vidgen et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 122
    }, {
      "referenceID" : 27,
      "context" : "Based on prior work on linking conservative ideologies, endorsement of unrestricted speech, and racial prejudice with reduced likelihood to accept the term “hate speech” (Duckitt and Fisher, 2003; White and Crandall, 2017; Roussos and Dovidio, 2018; Elers and Jayan, 2020), we hypothesize that conservative annotators and those who score highly on the RACISTBELIEFS or FREEOFFSPEECH scales will rate anti-Black tweets as less toxic, and vice-versa.",
      "startOffset" : 170,
      "endOffset" : 272
    }, {
      "referenceID" : 90,
      "context" : "Based on prior work on linking conservative ideologies, endorsement of unrestricted speech, and racial prejudice with reduced likelihood to accept the term “hate speech” (Duckitt and Fisher, 2003; White and Crandall, 2017; Roussos and Dovidio, 2018; Elers and Jayan, 2020), we hypothesize that conservative annotators and those who score highly on the RACISTBELIEFS or FREEOFFSPEECH scales will rate anti-Black tweets as less toxic, and vice-versa.",
      "startOffset" : 170,
      "endOffset" : 272
    }, {
      "referenceID" : 73,
      "context" : "Based on prior work on linking conservative ideologies, endorsement of unrestricted speech, and racial prejudice with reduced likelihood to accept the term “hate speech” (Duckitt and Fisher, 2003; White and Crandall, 2017; Roussos and Dovidio, 2018; Elers and Jayan, 2020), we hypothesize that conservative annotators and those who score highly on the RACISTBELIEFS or FREEOFFSPEECH scales will rate anti-Black tweets as less toxic, and vice-versa.",
      "startOffset" : 170,
      "endOffset" : 272
    }, {
      "referenceID" : 30,
      "context" : "Based on prior work on linking conservative ideologies, endorsement of unrestricted speech, and racial prejudice with reduced likelihood to accept the term “hate speech” (Duckitt and Fisher, 2003; White and Crandall, 2017; Roussos and Dovidio, 2018; Elers and Jayan, 2020), we hypothesize that conservative annotators and those who score highly on the RACISTBELIEFS or FREEOFFSPEECH scales will rate anti-Black tweets as less toxic, and vice-versa.",
      "startOffset" : 170,
      "endOffset" : 272
    }, {
      "referenceID" : 18,
      "context" : "Overall, our results from both studies corroborate previous findings that studied the attitudes that different gender and racial identities have towards hate speech, specifically that conservatives, white people, and men tend to value free speech more, and that liberals, women, and non-white people perceive the harm of hate speech more (Cowan and Khatchadourian, 2003; Downs and Cowan, 2012).",
      "startOffset" : 338,
      "endOffset" : 393
    }, {
      "referenceID" : 26,
      "context" : "Overall, our results from both studies corroborate previous findings that studied the attitudes that different gender and racial identities have towards hate speech, specifically that conservatives, white people, and men tend to value free speech more, and that liberals, women, and non-white people perceive the harm of hate speech more (Cowan and Khatchadourian, 2003; Downs and Cowan, 2012).",
      "startOffset" : 338,
      "endOffset" : 393
    }, {
      "referenceID" : 41,
      "context" : "Our results also support the finding that those who hold generally conservative ideologies tend to be more accepting towards anti-Black or racially prejudiced content (Goldstein and Hall, 2017; Lucks, 2020; Schaffner, 2020).",
      "startOffset" : 167,
      "endOffset" : 223
    }, {
      "referenceID" : 56,
      "context" : "Our results also support the finding that those who hold generally conservative ideologies tend to be more accepting towards anti-Black or racially prejudiced content (Goldstein and Hall, 2017; Lucks, 2020; Schaffner, 2020).",
      "startOffset" : 167,
      "endOffset" : 223
    }, {
      "referenceID" : 78,
      "context" : "Our results also support the finding that those who hold generally conservative ideologies tend to be more accepting towards anti-Black or racially prejudiced content (Goldstein and Hall, 2017; Lucks, 2020; Schaffner, 2020).",
      "startOffset" : 167,
      "endOffset" : 223
    }, {
      "referenceID" : 87,
      "context" : "the attitudes of annotators towards free speech, racism, and their beliefs on the harms of hate speech, for an accurate estimation of anti-Black language as toxic, offensive, or racist (e.g., by actively taking into consideration annotator ideologies; Waseem, 2016; Vidgen et al., 2021).",
      "startOffset" : 185,
      "endOffset" : 286
    }, {
      "referenceID" : 85,
      "context" : "the attitudes of annotators towards free speech, racism, and their beliefs on the harms of hate speech, for an accurate estimation of anti-Black language as toxic, offensive, or racist (e.g., by actively taking into consideration annotator ideologies; Waseem, 2016; Vidgen et al., 2021).",
      "startOffset" : 185,
      "endOffset" : 286
    }, {
      "referenceID" : 79,
      "context" : "This can be especially important given that hateful content very often targets marginalized groups and racial minorities (Silva et al., 2016; Sap et al., 2020), and can catalyze violence against them (O’Keeffe et al.",
      "startOffset" : 121,
      "endOffset" : 159
    }, {
      "referenceID" : 76,
      "context" : "This can be especially important given that hateful content very often targets marginalized groups and racial minorities (Silva et al., 2016; Sap et al., 2020), and can catalyze violence against them (O’Keeffe et al.",
      "startOffset" : 121,
      "endOffset" : 159
    }, {
      "referenceID" : 16,
      "context" : ", 2020), and can catalyze violence against them (O’Keeffe et al., 2011; Cleland, 2014).",
      "startOffset" : 48,
      "endOffset" : 86
    }, {
      "referenceID" : 43,
      "context" : "English, common among, but not limited to, African-American or Black speakers (Green, 2002; Edwards, 2004).",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : "English, common among, but not limited to, African-American or Black speakers (Green, 2002; Edwards, 2004).",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 45,
      "context" : "This category has been shown to be considered “worse” English by non-AAE speakers (Hilliard, 1997; Blake and Cutler, 2003; Champion et al., 2012; Beneke and Cheatham, 2015; Rosa and Flores, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al.",
      "startOffset" : 82,
      "endOffset" : 195
    }, {
      "referenceID" : 6,
      "context" : "This category has been shown to be considered “worse” English by non-AAE speakers (Hilliard, 1997; Blake and Cutler, 2003; Champion et al., 2012; Beneke and Cheatham, 2015; Rosa and Flores, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al.",
      "startOffset" : 82,
      "endOffset" : 195
    }, {
      "referenceID" : 15,
      "context" : "This category has been shown to be considered “worse” English by non-AAE speakers (Hilliard, 1997; Blake and Cutler, 2003; Champion et al., 2012; Beneke and Cheatham, 2015; Rosa and Flores, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al.",
      "startOffset" : 82,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "This category has been shown to be considered “worse” English by non-AAE speakers (Hilliard, 1997; Blake and Cutler, 2003; Champion et al., 2012; Beneke and Cheatham, 2015; Rosa and Flores, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al.",
      "startOffset" : 82,
      "endOffset" : 195
    }, {
      "referenceID" : 71,
      "context" : "This category has been shown to be considered “worse” English by non-AAE speakers (Hilliard, 1997; Blake and Cutler, 2003; Champion et al., 2012; Beneke and Cheatham, 2015; Rosa and Flores, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al.",
      "startOffset" : 82,
      "endOffset" : 195
    }, {
      "referenceID" : 75,
      "context" : ", 2012; Beneke and Cheatham, 2015; Rosa and Flores, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al., 1998; Sap et al., 2019), particularly due to dialectspecific lexical markers (e.",
      "startOffset" : 125,
      "endOffset" : 164
    }, {
      "referenceID" : 34,
      "context" : "Based on prior work that correlate racial prejudice with negative attitudes towards AAE (Gaither et al., 2015; Rosa, 2019), we hypothesize that annotators who are white and who score high in RACISTBELIEFS will rate AAE posts as more toxic.",
      "startOffset" : 88,
      "endOffset" : 122
    }, {
      "referenceID" : 70,
      "context" : "Based on prior work that correlate racial prejudice with negative attitudes towards AAE (Gaither et al., 2015; Rosa, 2019), we hypothesize that annotators who are white and who score high in RACISTBELIEFS will rate AAE posts as more toxic.",
      "startOffset" : 88,
      "endOffset" : 122
    }, {
      "referenceID" : 77,
      "context" : "Additionally, since AAE can be considered non-canonical English (Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019), we hypothesize that annotators who are more conservative and who score higher in TRADITIONALISM and LINGPURISM will rate AAE posts with higher toxicity.",
      "startOffset" : 64,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "Additionally, since AAE can be considered non-canonical English (Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019), we hypothesize that annotators who are more conservative and who score higher in TRADITIONALISM and LINGPURISM will rate AAE posts with higher toxicity.",
      "startOffset" : 64,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "As an additional investigation, we measure whether attitudes or identities affects toxicity ratings of AAE posts that contain the word “n*gga,” a (reclaimed) slur that has very different pragmatic interpretations depending on speaker and listener identity (Croom, 2011).",
      "startOffset" : 256,
      "endOffset" : 269
    }, {
      "referenceID" : 70,
      "context" : "Our findings suggest that annotators perceive that AAE posts are associated with the Black racial identity (Rosa, 2019), which could cause those who score highly on the RACISTBELIEFS scale to annotate them as racist, potentially as a form of colorblind racism (e.",
      "startOffset" : 107,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "Our findings suggest that annotators perceive that AAE posts are associated with the Black racial identity (Rosa, 2019), which could cause those who score highly on the RACISTBELIEFS scale to annotate them as racist, potentially as a form of colorblind racism (e.g., where simply mentioning race is considered racist; Bonilla-Silva, 2006).",
      "startOffset" : 260,
      "endOffset" : 338
    }, {
      "referenceID" : 20,
      "context" : ", 1998), even though some of these might be reclaimed slurs (e.g., “n*gga”; Croom, 2011; Galinsky et al., 2013).",
      "startOffset" : 60,
      "endOffset" : 111
    }, {
      "referenceID" : 35,
      "context" : ", 1998), even though some of these might be reclaimed slurs (e.g., “n*gga”; Croom, 2011; Galinsky et al., 2013).",
      "startOffset" : 60,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : "These findings shed some light on the racial biases found in hate speech detection (Davidson et al., 2019; Sap et al., 2019), partially explaining why AAE is perceived as toxic.",
      "startOffset" : 83,
      "endOffset" : 124
    }, {
      "referenceID" : 75,
      "context" : "These findings shed some light on the racial biases found in hate speech detection (Davidson et al., 2019; Sap et al., 2019), partially explaining why AAE is perceived as toxic.",
      "startOffset" : 83,
      "endOffset" : 124
    }, {
      "referenceID" : 75,
      "context" : "For example, annotators could explicitly include speakers of AAE, or those who understand that AAE or its lexical markers are not inherently toxic, or are primed to do so (Sap et al., 2019).",
      "startOffset" : 171,
      "endOffset" : 189
    }, {
      "referenceID" : 70,
      "context" : "Avoiding an incorrect estimation of AAE as toxic is crucial to avoid upholding racio-linguistic hierarchies and thus representational harms against AAE speakers (Rosa, 2019; Blodgett et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 196
    }, {
      "referenceID" : 7,
      "context" : "Avoiding an incorrect estimation of AAE as toxic is crucial to avoid upholding racio-linguistic hierarchies and thus representational harms against AAE speakers (Rosa, 2019; Blodgett et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : "Both types of vulgarity can be mistaken for toxic despite also having non-hateful usages (e.g., to indicate emotion or social belonging; Croom, 2011; Dynel, 2012; Galinsky et al., 2013).",
      "startOffset" : 89,
      "endOffset" : 185
    }, {
      "referenceID" : 28,
      "context" : "Both types of vulgarity can be mistaken for toxic despite also having non-hateful usages (e.g., to indicate emotion or social belonging; Croom, 2011; Dynel, 2012; Galinsky et al., 2013).",
      "startOffset" : 89,
      "endOffset" : 185
    }, {
      "referenceID" : 35,
      "context" : "Both types of vulgarity can be mistaken for toxic despite also having non-hateful usages (e.g., to indicate emotion or social belonging; Croom, 2011; Dynel, 2012; Galinsky et al., 2013).",
      "startOffset" : 89,
      "endOffset" : 185
    }, {
      "referenceID" : 48,
      "context" : "Given that vulgarity can be considered noncanonical or impolite language (Jay and Janschewitz, 2008; Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019), we hypothesize that annotators who score high on LINGPURISM, TRADITIONALISM, and who are more conservative will rate vulgar posts as more offensive.",
      "startOffset" : 73,
      "endOffset" : 152
    }, {
      "referenceID" : 77,
      "context" : "Given that vulgarity can be considered noncanonical or impolite language (Jay and Janschewitz, 2008; Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019), we hypothesize that annotators who score high on LINGPURISM, TRADITIONALISM, and who are more conservative will rate vulgar posts as more offensive.",
      "startOffset" : 73,
      "endOffset" : 152
    }, {
      "referenceID" : 24,
      "context" : "Given that vulgarity can be considered noncanonical or impolite language (Jay and Janschewitz, 2008; Sapolsky et al., 2010; DeFrank and Kahlbaugh, 2019), we hypothesize that annotators who score high on LINGPURISM, TRADITIONALISM, and who are more conservative will rate vulgar posts as more offensive.",
      "startOffset" : 73,
      "endOffset" : 152
    }, {
      "referenceID" : 77,
      "context" : "Our findings corroborate prior work showing how adherence to societal traditional values is often opposed to the acceptability of vulgar language (Sapolsky et al., 2010).",
      "startOffset" : 146,
      "endOffset" : 169
    }, {
      "referenceID" : 47,
      "context" : "Traditional values and conservative beliefs have been connected to finding vulgar language as a direct challenging the moral order (Jay, 2018; Sterling et al., 2020; Muddiman, 2021).",
      "startOffset" : 131,
      "endOffset" : 181
    }, {
      "referenceID" : 84,
      "context" : "Traditional values and conservative beliefs have been connected to finding vulgar language as a direct challenging the moral order (Jay, 2018; Sterling et al., 2020; Muddiman, 2021).",
      "startOffset" : 131,
      "endOffset" : 181
    }, {
      "referenceID" : 60,
      "context" : "Traditional values and conservative beliefs have been connected to finding vulgar language as a direct challenging the moral order (Jay, 2018; Sterling et al., 2020; Muddiman, 2021).",
      "startOffset" : 131,
      "endOffset" : 181
    }, {
      "referenceID" : 28,
      "context" : "Moreover, annotators across different levels of traditionalism could be considered when collecting ratings of vulgarity, especially since perceptions might vary with generational and cultural norms (Dynel, 2012).",
      "startOffset" : 198,
      "endOffset" : 211
    }, {
      "referenceID" : 13,
      "context" : "This corroborate prior findings that show that toxicity detection models inherently encode a specific positionality (Cambo, 2021) and replicate human biases (Davani et al.",
      "startOffset" : 116,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "This corroborate prior findings that show that toxicity detection models inherently encode a specific positionality (Cambo, 2021) and replicate human biases (Davani et al., 2021).",
      "startOffset" : 157,
      "endOffset" : 178
    }, {
      "referenceID" : 76,
      "context" : "While several works have recruited or subselected annotators to hold particular ideologies (e.g., Waseem, 2016; Sap et al., 2020; Vidgen et al., 2021), annotator attitudes are only vaguely described.",
      "startOffset" : 91,
      "endOffset" : 150
    }, {
      "referenceID" : 85,
      "context" : "While several works have recruited or subselected annotators to hold particular ideologies (e.g., Waseem, 2016; Sap et al., 2020; Vidgen et al., 2021), annotator attitudes are only vaguely described.",
      "startOffset" : 91,
      "endOffset" : 150
    }, {
      "referenceID" : 37,
      "context" : "We advocate for collecting attitude scores based on relevant social science research, perhaps in lightweight format as done in our breadth-of-posts study, and reporting those scores along with the dataset (e.g., in datasheets; Gebru et al., 2018).",
      "startOffset" : 205,
      "endOffset" : 246
    }, {
      "referenceID" : 72,
      "context" : "As shown in our results and previous studies (e.g., Waseem, 2016; Ross et al., 2017; Waseem et al., 2021), determining what is toxic is subjective.",
      "startOffset" : 45,
      "endOffset" : 105
    }, {
      "referenceID" : 89,
      "context" : "As shown in our results and previous studies (e.g., Waseem, 2016; Ross et al., 2017; Waseem et al., 2021), determining what is toxic is subjective.",
      "startOffset" : 45,
      "endOffset" : 105
    }, {
      "referenceID" : 74,
      "context" : "However, given this subjectivity, the open question remains: whose perspective should be considered when using toxicity detection models? To try answering this question, we urge researchers and practitioners to consider all stakeholders and end users on which toxicity detection systems might be deployed (Sanders, 2002; Friedman et al., 2008).",
      "startOffset" : 305,
      "endOffset" : 343
    }, {
      "referenceID" : 33,
      "context" : "However, given this subjectivity, the open question remains: whose perspective should be considered when using toxicity detection models? To try answering this question, we urge researchers and practitioners to consider all stakeholders and end users on which toxicity detection systems might be deployed (Sanders, 2002; Friedman et al., 2008).",
      "startOffset" : 305,
      "endOffset" : 343
    }, {
      "referenceID" : 42,
      "context" : "Additionally, we urge people to embrace that each design decision has socio-political implications (Green, 2020; Cambo, 2021), and encourage them to develop technologies to shift power to the targets of oppression (Blodgett et al.",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "Additionally, we urge people to embrace that each design decision has socio-political implications (Green, 2020; Cambo, 2021), and encourage them to develop technologies to shift power to the targets of oppression (Blodgett et al.",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "Additionally, we urge people to embrace that each design decision has socio-political implications (Green, 2020; Cambo, 2021), and encourage them to develop technologies to shift power to the targets of oppression (Blodgett et al., 2020; Kalluri, 2020; Birhane, 2021).",
      "startOffset" : 214,
      "endOffset" : 267
    }, {
      "referenceID" : 53,
      "context" : "Additionally, we urge people to embrace that each design decision has socio-political implications (Green, 2020; Cambo, 2021), and encourage them to develop technologies to shift power to the targets of oppression (Blodgett et al., 2020; Kalluri, 2020; Birhane, 2021).",
      "startOffset" : 214,
      "endOffset" : 267
    }, {
      "referenceID" : 5,
      "context" : "Additionally, we urge people to embrace that each design decision has socio-political implications (Green, 2020; Cambo, 2021), and encourage them to develop technologies to shift power to the targets of oppression (Blodgett et al., 2020; Kalluri, 2020; Birhane, 2021).",
      "startOffset" : 214,
      "endOffset" : 267
    }, {
      "referenceID" : 68,
      "context" : "Finally, given the increasingly essential role of online platforms in people’s daily lives (Rahman, 2017), we echo calls for policy regulating these toxicity detection algorithms (Jiang, 2020; McGuffie and Newhouse, 2020).",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 50,
      "context" : "Finally, given the increasingly essential role of online platforms in people’s daily lives (Rahman, 2017), we echo calls for policy regulating these toxicity detection algorithms (Jiang, 2020; McGuffie and Newhouse, 2020).",
      "startOffset" : 179,
      "endOffset" : 221
    }, {
      "referenceID" : 58,
      "context" : "Finally, given the increasingly essential role of online platforms in people’s daily lives (Rahman, 2017), we echo calls for policy regulating these toxicity detection algorithms (Jiang, 2020; McGuffie and Newhouse, 2020).",
      "startOffset" : 179,
      "endOffset" : 221
    }, {
      "referenceID" : 40,
      "context" : "If the goal is to assist human content moderators, making our systems explain biased implications of statements could be more helpful than opaque toxicity scores (Gillespie et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 186
    }, {
      "referenceID" : 76,
      "context" : "Therefore, we advocate for moving away from classification frameworks, and towards more nuanced, holistic, and explainable frameworks for inferring the desired concepts of toxicity and social biases (e.g., the hierarchical Social Bias Frames formalism; Sap et al., 2020).",
      "startOffset" : 199,
      "endOffset" : 270
    }, {
      "referenceID" : 83,
      "context" : "These items are taken from the altruism part of the scale by (Steg et al., 2014).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 87,
      "context" : "Following crowdsourcing setups in prior work (Waseem, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Sap et al., 2019), we asked three fine-grained questions to annotators for each post in both our studies: • “How toxic/hateful/disrespectful or offensive does this post seem to you?” • “How much could this post be seen as toxic/hateful/disrespectful or offensive to anyone?” • “In your opinion, how racist is this post?” Given the high correlations between the two offensiveness variables (Pearson r ≥ .",
      "startOffset" : 45,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "Following crowdsourcing setups in prior work (Waseem, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Sap et al., 2019), we asked three fine-grained questions to annotators for each post in both our studies: • “How toxic/hateful/disrespectful or offensive does this post seem to you?” • “How much could this post be seen as toxic/hateful/disrespectful or offensive to anyone?” • “In your opinion, how racist is this post?” Given the high correlations between the two offensiveness variables (Pearson r ≥ .",
      "startOffset" : 45,
      "endOffset" : 143
    }, {
      "referenceID" : 91,
      "context" : "Following crowdsourcing setups in prior work (Waseem, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Sap et al., 2019), we asked three fine-grained questions to annotators for each post in both our studies: • “How toxic/hateful/disrespectful or offensive does this post seem to you?” • “How much could this post be seen as toxic/hateful/disrespectful or offensive to anyone?” • “In your opinion, how racist is this post?” Given the high correlations between the two offensiveness variables (Pearson r ≥ .",
      "startOffset" : 45,
      "endOffset" : 143
    }, {
      "referenceID" : 32,
      "context" : "Following crowdsourcing setups in prior work (Waseem, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Sap et al., 2019), we asked three fine-grained questions to annotators for each post in both our studies: • “How toxic/hateful/disrespectful or offensive does this post seem to you?” • “How much could this post be seen as toxic/hateful/disrespectful or offensive to anyone?” • “In your opinion, how racist is this post?” Given the high correlations between the two offensiveness variables (Pearson r ≥ .",
      "startOffset" : 45,
      "endOffset" : 143
    }, {
      "referenceID" : 75,
      "context" : "Following crowdsourcing setups in prior work (Waseem, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Sap et al., 2019), we asked three fine-grained questions to annotators for each post in both our studies: • “How toxic/hateful/disrespectful or offensive does this post seem to you?” • “How much could this post be seen as toxic/hateful/disrespectful or offensive to anyone?” • “In your opinion, how racist is this post?” Given the high correlations between the two offensiveness variables (Pearson r ≥ .",
      "startOffset" : 45,
      "endOffset" : 143
    }, {
      "referenceID" : 46,
      "context" : "challenging given that MTurk workers are usually tend to be predominantly white and skew liberal (Huff and Tingley, 2015; Burnham et al., 2018; Loepp and Kelly, 2020).",
      "startOffset" : 97,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "challenging given that MTurk workers are usually tend to be predominantly white and skew liberal (Huff and Tingley, 2015; Burnham et al., 2018; Loepp and Kelly, 2020).",
      "startOffset" : 97,
      "endOffset" : 166
    }, {
      "referenceID" : 55,
      "context" : "challenging given that MTurk workers are usually tend to be predominantly white and skew liberal (Huff and Tingley, 2015; Burnham et al., 2018; Loepp and Kelly, 2020).",
      "startOffset" : 97,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : "(2018), using the lexical detector of AAE by (Blodgett et al., 2016) and the vulgar wordlist by Zhou et al.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 75,
      "context" : "(2021) corpus (Sap et al., 2019; Davidson et al., 2019), we do not consider posts that are annotated as anti-Black but detected as AAE.",
      "startOffset" : 14,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "(2021) corpus (Sap et al., 2019; Davidson et al., 2019), we do not consider posts that are annotated as anti-Black but detected as AAE.",
      "startOffset" : 14,
      "endOffset" : 55
    }, {
      "referenceID" : 80,
      "context" : "Finally, we compare the high and low correlations using Fisher’s r-to-z transformation (Silver and Dunlap, 1987).",
      "startOffset" : 87,
      "endOffset" : 112
    } ],
    "year" : 0,
    "abstractText" : "Warning: this paper discusses and contains content that is offensive or upsetting. The perceived toxicity of language can vary based on someone’s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system’s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",
    "creator" : null
  }
}
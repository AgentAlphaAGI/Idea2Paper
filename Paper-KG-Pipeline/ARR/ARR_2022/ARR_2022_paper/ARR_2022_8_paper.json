{
  "name" : "ARR_2022_8_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Thanks to recent developments in self-supervised speech representation learning (van den Oord et al., 2017, 2019; Chorowski et al., 2019; Baevski et al., 2020), there is new hope for the development of speech processing systems without the need for full textual transcriptions. Supervised speech processing systems for tasks such as automatic speech recognition (ASR) rely on a large amount of textual transcriptions, but self-supervised systems can be applied to under-resourced languages in which such annotation is either scarce or unavailable. A key task of the self-supervised system is to learn a discrete representation. While it is possible to discretize the speech solely on the basis of its acoustic properties, a more desirable discrete representation would serve as a bridge from the continuous acoustic signal toward higher-level linguistic structures such as syntax and semantics. Such a representation would make it possible to repurpose\nalgorithms developed for written languages so that they could be used for unwritten languages in tasks such as speech translation and spoken language understanding. Words are the obvious choice for a discrete, semantic-driven speech representation, but a practical speech understanding system needs at least thousands of words; learning them in an unsupervised manner may be challenging. Phonemes may be a more learnable representation. According to the standard linguistic definition, phonemes are closely linked to words: Definition 1. (Linguistic definition of phonemes (Swadesh, 1934)) Phonemes are the smallest units in speech such that given a correct native word, the replacement of one or more phonemes by other phonemes (capable of occurring in the same position) results in a native word other than that intended, or a native-like nonsense word.\nFor example, the sentences “he thinks” and “he sinks” differ by exactly one phoneme but have very different meaning. The optimal compactness of a phoneme inventory as specified in the definition leads to three advantages. First, learning phonemes requires lower sample complexity than learning words since the number of distinct phonemes is much smaller than the number of distinct words in a language. Second, the phonemes are much more abundant and more balanced in classes than words within a speech corpus, which makes sample complexity less of an issue when learning phonemes. Third, phonemes are more generalizable in the sense that knowing the phoneme inventory allows the learner to memorize previously unseen words as sequences of phonemes, and, having memorized them, to begin seeking clues to their meaning. Motivated by the semantic-driven definition of phonemes, we formulate the problem of learning a phoneme inventory as a self-supervised learning problem, where a small amount of semantic supervision is available. The required supervision\nspecifies which acoustic segments are instances of the same word, and which are instances of different words. Such supervision might be acquired in a naturalistic setting by asking native speakers to name objects in a set of standardized images, as is commonly done in primary education classrooms, or by asking for the translations of common words in a second language, a common baseline approach in dialectology and historical linguistics (Swadesh, 1952). Our contributions are threefold: (1) we propose a computationally tractable definition of phoneme that is almost equivalent to the linguistic definition. (2) We propose a finite-sample objective function for learning phoneme-level units and prove that under mild conditions, the empirical risk minimizer (ERM) of this objective will find the correct phoneme inventory with exponentially low error rate. (3) We propose a novel class of neural networks called information quantizers to optimize the proposed objective, which achieve state-of-theart results in the phoneme inventory discovery task on the TIMIT and low-resourced Mboshi benchmarks with much less training data than previous approaches."
    }, {
      "heading" : "2 Related works",
      "text" : "Due to the challenge of learning phonemes, early works on unsupervised speech representation learning (Park and Glass, 2005; Lee and Glass, 2012; Ondel et al., 2016) focus on learning speech segments sharing similar acoustic properties, or phones, without taking into account the meaning of the speech they are part of. There are two main approaches in this direction. One approach is to learn discrete phone-like units without any textual labels by modeling phone labels of the speech segments as latent variables. In particular, (Park and Glass, 2005; Jansen et al., 2010) first detect segments with recurring patterns in the speech corpus followed by graph clustering using the similarity graph formed by the segments. (Lee and Glass, 2012; Ondel et al., 2016; Kamper et al., 2016) develop probabilistic graphical models to jointly segment and cluster speech into phone-like segments. An extension to the latent variable approach is to introduce additional latent variables such as speaker identity (Ondel et al., 2019) or language identity (Yusuf et al., 2020) and develop mechanisms to disentangle these variables.\nWith the advance of deep learning, neural network models have also been proposed to learn\nunsupervised phone-level representation either by first learning a continuous representation (Chung et al., 2019; Feng et al., 2019; Nguyen et al., 2020) followed by off-line clustering, or by learning a discrete representation end-to-end with Gumbel softmax (Eloff et al., 2019b; Baevski et al., 2020) or vector-quantized variational autoencoder (VQVAE) (van den Oord et al., 2017; Chorowski et al., 2019; Baevski et al., 2019). However, codebooks learned by the neural approaches tend to be much larger than the number of phonemes (Baevski et al., 2020), leading to low scores in standard phoneme discovery metrics. The second approach utilizes weak supervision such as noisy phone labels predicted by a supervised, multilingual ASR system trained on other languages. Along this direction, (Żelasko et al., 2020; Feng et al., 2021a) systematically study the performance of zero-shot crosslingual ASR on 13 languages trained with international phonetic alphabet (IPA) tokens and found that the system tends to perform poorly on unseen languages. Instead, (Feng et al., 2021b) is able to discover phone-like units by clustering bottleneck features (BNF) from a factorized time-delay neural network (TDNN-f) trained with phone labels predicted by a crosslingual ASR (Feng et al., 2021a).\nSeveral works have since shifted focus toward the more challenging phoneme discovery problem by formulating it as a self-supervised learning problem where the semantics of the speech are known, such as from translation, phonemelevel language models or other sensory modalities such as vision. (Jansen, 2013) has studied the use of pairwise word identity labels for training phoneme discovery models based on Gaussian mixture models (GMM); (Harwath and Glass, 2019) analyzes the hidden layers of a two-branch neural network trained to retrieve spoken captions with semantically related images and finds strong correlation between segment representation and phoneme boundaries. (Harwath et al., 2020) adds hierarchical vector quantization (VQ) layers in the same retrieval network and is able to find a much smaller codebook than the unsupervised neural approach (Baevski et al., 2020), and achieve high correlation with the phoneme inventory. (Godard et al., 2018; Boito et al., 2019) has studied the possibility of learning semantic units using an attention-based speech-to-text translation system, though the units appear to correlate more with words. Works on unsupervised speech recognition (Chen et al., 2019)\nattempt to learn to recognize phonemes by leveraging the semantic information from a phoneme language model unpaired with the speech, typically by matching the empirical prior and posterior distributions of phonemes either using cross entropy (Yeh et al., 2019) or adversarial loss (Chen et al., 2019; Baevski et al., 2021)."
    }, {
      "heading" : "3 Semantic-driven Phoneme Discovery",
      "text" : ""
    }, {
      "heading" : "3.1 Notation",
      "text" : "Throughout the paper, we use P{·} to denote probability. We use capital letters to denote random variables and lower-case letters to represent samples of random variables. We use PX := P{X = x} to denote both probability mass and density functions of random variableX , depending on whether it is continuous or discrete. Further, denote PY |X(y|x) := P{Y = y|X = x} as the true conditional probability distribution of random variable Y = y given random variable X = x. The probability simplex in Rd is denoted as ∆d."
    }, {
      "heading" : "3.2 Statistical Definition of Phonemes",
      "text" : "The linguistic definition of phonemes can be rephrased as follows. Define X to be the set of all physical acoustic segments that can ever be produced as instances of the phonemes of a given language. Definition 1 can be phrased as follows: Two sequences of segments x = [x1, · · · , xT ] and x′ = [x1:t−1, x ′ t, xt+1:T ], differing only in that x′t 6= xt, are instances of different words, y′ 6= y, if and only if x′t and xt are instances of different phonemes. In order to design effective algorithms, we will work with a relaxation of this definition, which we call the statistical definition of phonemes.\nDefinition 2. (Statistical definition of phonemes) Let X be the set of all speech segments in a language, and let X be a random vector taking values in X and Y be a random variable representing the word of which X is one segment. The phoneme inventory of a language is the minimal partition Z = {Z1, · · · ,ZK} of X (i.e., X = ∪Kk=1Zk,Zj ∩ Zk = ∅, ∀1 ≤ j, k ≤ K), such that if a speech segment pair (x, x′) ∈ X2 satisfies (x, x′) ∈ Z2k for some k ∈ {1, · · · ,K}, then their conditional distributions satisfy\nPY |X=x = PY |X=x′ . (1)"
    }, {
      "heading" : "In other words, given only the knowledge that two",
      "text" : "acoustic sequences contain instances of the same phoneme, the resulting conditional distributions across possible word labels are the same.\nThe fundamental intuition of Definition 2 is that different phonemes have different distributions across the words of the language. Two instances of the same phoneme, x and x′, might have different likelihoods PX=x|Y and PX=x′|Y , e.g., because of allophony; but their posteriors PY |X=x and PY |X=x′ cannot be different without violating Definition 1. The relationship between Definition 1 and Definition 2 is given by the following proposition, whose proof is in Appendix A.3.\nProposition 1. Let Z = ∪Kk=1Zk be a partition of X. If, for all possible {PY |X=xs}s 6=t, for any spoken word x = [x1, · · · , xT ], and for any segment pairs (xt, x′t) ∈ Z2k, k ∈ {1, · · · ,K}, changing xt to x′t does not alter the identity of the word, i.e.,\narg max y\nPY |X1:T (y|x1:t−1, x ′ t, xt+1:T )\n= arg max y PY |X1:T (y|x), (2)\nbut for any segment pairs xt ∈ Zk, x′′t ∈ Zl for k 6= l, changing xt to x′t alters the identity of the word, i.e.,\narg max y\nPY |X1:T (y|x1:t−1, x ′′ t , xt+1:T )\n6= arg max y PY |X1:T (y|x), (3)\nthen Z is a phoneme inventory from Definition 2. Define the phoneme assignment function z : X → {1, · · · ,K} such that z(x) = k if x ∈ Zk. Suppose a segment X is randomly chosen from X with probability distribution PX and its phoneme\nlabel is another random variable Z := z(X), then by Definition 2, for any pair x, x′ ∈ X such that z(x) = z(x′), we have PY |X=x = PY |X=x′ = PY |Z=z(x). The phoneme inventory is thereby completely characterized by the phoneme label function z(·) as well as the set of distributions associated with each class PY |Z ."
    }, {
      "heading" : "3.3 Problem Formulation",
      "text" : "Let z(·) be the phoneme assignment function from Definition 2 and assume the size of the phoneme inventory is known to be K.\nGiven a training set D = {(x(i), y(i))}ni=1, where each x(i) is an acoustic segment extracted from a spoken word, and each y(i) ∈ Y is the corresponding word label, a semantic-driven phoneme discovery (SPD) system tries to find an assignment function that minimizes the token error rate (TER):\nPTER(ẑ) := min π∈Π\nP{z(X) 6= π(ẑ(X))}, (4)\nwhere Π is the set of all permutations of length K, which is used because the problem is unsupervised and z(·) is not available during training. An assignment function ẑ is said to achieve exact discovery if PTER(ẑ) = 0. It can be easily shown that TER is equivalent to standard evaluation metrics for phoneme discovery such as normalized mutual information (NMI) (Yusuf et al., 2020; Harwath et al., 2020; Feng et al., 2021b) and token F1 (Dunbar et al., 2017), as presented in Appendix A.2. Thus, to provide guarantees for NMI and token F1, it suffices to provide a guarantee for TER."
    }, {
      "heading" : "4 Information Quantizer",
      "text" : "We solve the SPD problem using a novel type of neural network called an information quantizer (IQ), depicted in Figure 2. An IQ (θ, q) ∈\nΘ×QK consists of four main components: A presegmentation network, a speech encoder eθ1(·), a word posterior cθ2(·) and a quantizer q : ∆|Y| → C = {Q1, · · · , QK}, where [θ1, θ2] = θ and C is the distribution codebook and Qk’s are called the code distributions of q."
    }, {
      "heading" : "4.1 Phoneme inventory discovery with IQ",
      "text" : "IQ performs phoneme discovery in three stages. The pre-segmentation stage takes a raw speech waveform as input and extracts phoneme-level segments x = [x1, · · · , xT ] in a self-supervised fashion (Kreuk et al., 2020). Afterwards, in the joint distribution learning stage, the speech encoder extracts phoneme-level representations eθ1(x) = [eθ1(x1), · · · , eθ1(xT )] before passing them into the word posterior network to estimate the distribution of word labels, Y , given the presence in the word of acoustic phonetic segment X = x:\nP θY |X=xt = cθ2(eθ1(xt)), 1 ≤ t ≤ T. (5)\nNote that it is crucial that no recurrent connection exists between segments since our goal is to learn the probability of a word label given the presence of one phoneme segment. Finally, in the quantization stage, the quantizer creates the phoneme inventory by assigning each segment xt an integer index via codeword assignment function ẑ(xt) such that ẑ(xt) = k if q(P θY |X=xt) = Qk."
    }, {
      "heading" : "4.2 Training",
      "text" : "The loss function that IQ minimizes has two goals: learn a good estimator for the conditional distribution PY |X and learn a good quantization function q(·). The first goal is achieved by minimizing the cross entropy loss:\nLCE(Pn, θ) := − 1\nn n∑ i=1 logP θY |X(y (i)|x(i)), (6)\nwhere Pn is the empirical joint distribution. The second goal is achieved by minimizing the KLdivergence between the estimated conditional distribution before and after quantization:\nLQ(P̃n, θ, q) := 1\nn n∑ i=1 DKL(P θ Y |X=x(i) ||q(P θ Y |X=x(i))),\n(7)\nwhere P̃n := 1n ∑n i=1 δx(i)P θ Y |X=x(i) is the smoothed version of the empirical distribution. The final loss function of IQ for SPD is then:\nLIQ(Pn, θ, q) := LCE(Pn, θ) + λLQ ( P̃n, θ, q ) ,\n(P1)\nwhere λ > 0 is some hyperparameter set to approximately 1 for most experiments. Further, we restrict q to be nearest-neighbor so that:\nq(P ) = arg min Qk:1≤k≤K\nDKL(P ||Qk). (8)\nThis restriction does not increase the loss (P1) and serves as a regularization during phoneme discovery, as shown in Appendix A.3."
    }, {
      "heading" : "4.3 Theoretical Guarantee",
      "text" : "We show that under mild assumption, IQ is able to achieve exact discovery of phoneme inventory. First, let us state the main assumptions of the paper.\nAssumption 1. (boundedness of the density ratio) There exist universal constants Cl < Cu such that ∀θ ∈ Θ, ∀q ∈ QK , ∀(x, y) ∈ X × Y, log PY |X(y|x)\nP θ Y |X(y|x)\n∈ [Cl, Cu], log PY |X(y|x) q(P θ\nY |X(y|x)) ∈\n[Cl, Cu].\nAssumption 2. (log-smoothness of the density ratio) There exists ρ > 0 such that ∀θ1, θ2 ∈ Θ, x, y ∈ X× Y, ∣∣∣∣log P θ1Y |X(y|x)P θ2\nY |X(y|x) ∣∣∣∣ ≤ ρ‖θ1 − θ2‖. Assumption 3. (realizability) There exists a nonempty subset Θ∗ ⊂ Θ such that P θY |X = PY |X , ∀θ ∈ Θ∗. Assumption 4. The true prior of the phoneme inventory is known to be PZ(z) = 1K , 1 ≤ z ≤ K.\nThe first two assumptions are similar to the ones in (Tsai et al., 2020). Assumption 3 assumes that the true probability measure is within the function class, which combined with Assumption 1 requires the true distribution to share the same support as the estimated one. However, such assumption can be\nrelaxed so that DKL(P θ ∗ Y |X ||PY |X) ≤ ν, ∀θ ∗ ∈ Θ∗ for some small enough ν > 0, which does not affect the essential idea behind our analysis and can be achieved by some rich class of universal approximators such as neural networks (Hornik et al., 1989). The last assumption ensures the inventory to be identifiable by assuming knowledge of the prior of the phoneme inventory.\nNext, we will state the theoretical guarantee before giving some intuitive explanation.\nTheorem 1. Given Assumption 1-4, let the information quantizer (θ̂, q̂) with assignment function ẑ be an empirical risk minimizer (ERM) of (P1):\nLIQ(Pn, θ̂, q̂) = min θ∈Θ,q∈QK LIQ(Pn, θ, q). (9)\nFor any δ ∈ (0, 1], with probability at least 1 − δ, the cluster assignment function ẑ of the ERM information quantizer q̂ achieves PTER(ẑ) = 0 if the sample size n satisfies:\nn ≥ O\n( log 1δ\nmin{ ∗2, log KK−1}\n) , (10)\nwhere\n∗ = min z1,z2:z1 6=z2 c(z1, z2)DJS(PY |Z=z1 ||PY |Z=z2) 2\nfor some constants c(z1, z2) > 0, 1 ≤ z1, z2 ≤ K independent of n, δ, O(x) is such that O(x) ≤ αx for some α > 0 and DJS(P ||Q) := 1 2DKL ( P ||P+Q2 ) + 12DKL ( Q||P+Q2 ) is the Jensen-Shannon divergence.\nThe bound in Theorem 1 captures two main factors determining the sample complexity of exact phoneme discovery: the first factor is how close the word distributions of phonemes are from each other as measured by their Jensen-Shannon (JS) divergence, and the second factor is how hard it is for the training data to cover all the phonemes. The theorem works essentially because (P1) can be viewed as an approximation of the mutual information between the codeword ẑ(X) and word type Y , I(ẑ(X);Y ). Suppose P θ̂Y |X ≈ PY |X and let H(·|·) denotes conditional entropy, we have:\nLIQ(Pn, θ̂, q̂) ≈ H(Y |X) +DKL(PY |X ||q̂(PY |X)) ∝ −I(X;Y ) +DKL(PY |X ||q̂(PY |X)) = −I(ẑ(X);Y ),\nwhich is minimized if q̂(PY |X) = PY |z(X). In fact, we prove that ẑ for such q̂ is equivalent to z(·) up to a permutation in Appendix A.3."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "Datasets We construct four training datasets consisting of spoken words only. The vocabulary set with |Y| = 224 is selected from head words of noun phrases from the Flickr30kEntities dataset (Hodosh et al., 2010) that appear at least 500 times. For the Flickr audio word dataset, spoken words in the vocabulary are extracted from Flickr audio dataset (Harwath and Glass, 2015). For the Librispeech and TIMIT word dataset with |Y| = 224, spoken words are extracted from Librispeech (Vassil et al., 2015) 460-hour train-clean subset, resulting in a dataset of about 6 hours and 0.1 hours; for Librispeech and TIMIT word dataset with |Y| = 524 and |Y| = 824, we supplement the dataset with the speech for the top 300 frequent words and top 600 frequent words respectively (excluding the visual words) in Librispeech, resulting in datasets of about 15 and 21 hours. For Mboshi dataset, we found only about 20 actual words occur more than 100 times, so instead we use all n-grams except uni- and bigrams or bigrams+trigrams that occur more than 100 times as “words”, resulting in\na vocabulary size of 161 and 377 respectively. Note that the amount of labeled data we need is much lower than previous works (Yusuf et al., 2020): around 30 hours, (Feng et al., 2021b): around 600 hours) and the vocabulary size used is much smaller than the total vocabulary size in the language. More details of the sets can be found in Appendix B. We also test our models on two standard phoneme discovery benchmarks, which contain whole-sentence utterances with many words unseen during training. The first dataset is TIMIT (Garofolo et al., 1993), an English corpus consisting of about 5 hours speech and Mboshi (Godard et al., 2017), which contains about 2.4 hours speech from a lowresource language. For both datasets, we follow the split in (Yusuf et al., 2020), (Feng et al., 2021b)\nBaselines For phoneme discovery from segmented words, we compare our model (IQ) with four baselines. The first two baselines use continuous representation: the CPC+k-means model performs k-means clustering on the segment-level CPC features and the k-means model performs k-means clustering after the model is trained on the word recognition task. The last two baselines use discrete representations: the Gumbel variational information bottleneck (Alemi et al., 2017) (Gumbel VIB) is a neural model with a Gumbel softmax (Jang et al., 2016) layer to approximate the codebook assignment function z(·), and we set β = 0.001 and decay the temperature of the Gumbel softmax from 1 to 0.1 linearly for the first 300000 steps, keeping it at 0.1 afterwards, which works best in our experiments; the deterministic\ninformation bottleneck (DIB), a generalization of (Strouse and Schwab, 2016) for continuous feature variable X , which assumes the same deterministic relation between speech X and codebook unit Z as ours, but optimizes the models in a pipeline fashion (first the speech encoder and then the quantizer) by performing clustering on the learned conditional distributions. All models share the same speech encoder as IQ. For the whole-sentence datasets, we compare our models three phoneme discovery systems, namely, the unsupervised H-SHMM trained with multilingual speech (Yusuf et al., 2020), the ResDAVEnet-VQ (Harwath et al., 2020) with visual supervision and the TDNN-f system by (Feng et al., 2021b) trained with multilingual speech. To study how well our model performs in extreme low-resource speech recognition compared to other neural speech representation learning models, we compare our models with wav2vec (Schneider et al., 2019), wav2vec 2.0 (Baevski et al., 2020), vq-wav2vec with Gumbel softmax and k-means as discretization strategies (Baevski et al., 2019), CPC (van den Oord et al., 2019) and VQ-CPC (van Niekerk et al., 2020), using the pretrained models released by the authors. Implementation details of the baselines and our models are in Appendix C.\nEvaluation metrics Standard metrics are used such as NMI and boundary F1 for the quality of codebook and segmentation respectively with the same implementation as in prior works (Yusuf et al., 2020; Feng et al., 2021b). In addition, token F1 (Dunbar et al., 2017) is also reported. To examine the benefit of using our discovered phoneme inventory for low-resource speech recognition, we also evaluate using equivalent phone error rate (equiv. PER: Ondel et al. 2019). This metric can be viewed as a proxy for phone error rate (PER) applicable beyond supervised speech recognizers."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Word-level Phoneme Discovery",
      "text" : "The results on visual word-only test sets of Flickr audio and Librispeech are shown in Table 1 and the convergence plot during training is shown in Figure 8. On both datasets, IQ outperforms both Gumbel VIB and DIB in terms of all metrics, especially on Flickr audio, which has more phonemes than Librispeech and a larger test set. Moreover, the performance of IQ is very robust to the codebook size, achieving good results even when the\ncodebook size is very different from the size of the true phoneme inventory, suggesting our theory may be able to work with a relaxed Assumption 4."
    }, {
      "heading" : "6.2 Sentence-level Phoneme Discovery",
      "text" : "The results on TIMIT and Mboshi are shown in Table 3 and Table 5a respectively. On TIMIT, our model is able to outperform the visually grounded baseline (Harwath et al., 2020) for all training vocabulary, and all three baselines for |Y| = 524 and |Y| = 824 with and without gold segmentation in terms of all three metrics. Further, we also empirically verify the sample complexity bound in Theorem 1 as IQ performs better in Token F1 and NMI as the training vocabulary size get larger, which generally increases the JS divergence. On Mboshi, IQ with CPC feature consistently outpeforms (Feng et al., 2021b) in token F1 and boundary F1, and IQ with CPC+BNF features consistently outperform (Feng et al., 2021b) in all three metrics under various level of word supervision. The performance of our model on Mboshi compared with other neural self-supervised models are shown in Table 5b. We found that IQ outperforms the best self-supervised\nmodel, CPC+k-means in equiv. PER by 34% and 20% absolute with and without gold segmentation respectively and 12% absolute in terms of boundary F1, suggesting that IQ is able to learn consistent phoneme-like sequence useful for zero-resource or extremely low-resource speech recognition.\nEffect of segmentation and codebook size The use of unsupervised phoneme segmentation deteriorates the NMI by about 18% and 28% absolute on TIMIT and Mboshi respectively for our models since the distributional property of phonemes does not apply exactly to non-phoneme segments. On the other hand, in Appendix F we show that the quality of codeword assignments by IQs is very robust against varying codebook size, after experimenting with codebook size from 30 to 70 on TIMIT and Mboshi.\nMultilingual and word supervision are complimentary In all vocabulary sizes, concatenating the multilingual BNF from (Feng et al., 2021b) to the CPC output representation from the segmental speech encoder in Figure 2 significantly improves token F1 and NMI to allow our best models to outperform baselines in all three metrics."
    }, {
      "heading" : "6.3 Analysis",
      "text" : "IQ codebook resembles true phonemes From Figure 4b, we observe that the codeword assignments by IQ correlates well with the actual\nphonemes, but tends to confuse the most between phonemes within the same manner class, such as nasals /n/ and /m/. This is also confirmed by the t-SNE plot in Figure 4a, where the embeddings of most manner classes are well-clustered, except for related manner classes such as affricate and fricative, or glide and vowel. Further, from the examples shown in Figure 6, we can see that IQ is not only better at grouping segments of the same phonemes but also at detecting segment boundaries than the baselines. Also, across different examples, IQ assign the same codes to phonemes such as /a/ (31) and /s/ (7) more consistently than other models do. Please check Appendix G for more speech examples.\nLimitation While our theory predicts that with gold segmentation, the TER of IQ is asymptotically zero, in practice TER is nonzero due to the violation of Assumption 4, i.e., the phonemes are not uniformly distributed for languages such as Mboshi. As a result, the model often discards information of the rare phonemes by merging them into a more frequent phoneme cluster. Evidently, from Figure 7, where we use ABX accuracy (Munson and Gardner, 1950) to score how reliable the IQ codebook can identify segments of the same phoneme, we observe a strong correlation is observed between ABX accuracy and the frequency of the phonemes."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Motivated by the linguistic definition of phonemes, we propose information quantizer (IQ), a new neural network model with theoretical guarantee and strong empirical performance for semantic-driven phoneme discovery."
    }, {
      "heading" : "A Proofs of Theoretical Results",
      "text" : "A.1 Statistical definition of phonemes\nProof of Proposition 1. Without loss of generality, suppose (x1, x′1) ∈ X2, suppose there exists y1 such that PY |X(y1|xt) > PY |X(y1|x′t), then there exists y2 such that PY |X(y2|xt) < PY |X(y2|x′t), which means there exists 0 ≤ α1, α2 ≤ 1, α1 + α2 ≤ 1, such that PY |X(y1|x′t) PY |X(y2|x′t) ≤ α2α1 < PY |X(y1|xt) PY |X(y2|xt) . Now, since Equation 2 holds for arbitrary PY |X=xs ∈ ∆|Y|, s 6= t, we can set PY |X(y1|x2) = α1, PY |X(y2|x2) = α2, PY |X(y1|xt) = PY |X(y2|xt) = 12 ,∀t > 2, in which case Equation 2 boils down to arg maxi∈{1,2} αiPY |X(yi|x1) = arg maxi∈{1,2} αiPY |X(yi|x′1). However, by the choice of αi’s, the left-hand side is y1 since α1PY |X(y1|x1) > α2PY |X(y2|x1) and the right-hand side is y2 since α2PY |X(y2|x1) > α1PY |X(y1|x′1), and therefore Equation 2 cannot hold. Therefore, Equation 2 is true only if PY |X(y|x1) = PY |X(y|x′1), ∀(x1, x′1) ∈ X2, y ∈ Y.\nA.2 Equivalence of TER and standard phoneme discovery metrics\nConsider the groundtruth assignment z(·) and a codebook assignment ẑ(·) with K̂ code words, the NMI of ẑ is defined as:\nNMI(ẑ) = 2I(z(X); ẑ(X))\nH(z(X)) +H(ẑ(X)) , (11)\nwhere H(·) denotes the entropy and I(·; ·) denotes the mutual information.\nwhich is also related to the token F1 used for acoustic unit discovery (Dunbar et al., 2017). Since SPD is an unsupervised learning problem and ground truth phoneme labels are not available, matching between codebook indices and phoneme units is needed. When computing token F1, we consider two different many-to-one mappings πrec : {1, · · · ,K} → {1, · · · , K̂} and\nπprec : {1, · · · , K̂} → {1, · · · ,K} to compute the token recall and precision respectively as:\nRec(ẑ) := max πrec P{ẑ(X) = πrec(z(X))} (12)\nPrec(ẑ) := max πprec P{z(X) = πprec(ẑ(X))}, (13)\nbefore computing the harmonic mean between the two to obtain token F1: F1(ẑ) := 2Prec(ẑ)Rec(ẑ)Prec(ẑ)+Rec(ẑ) . The following proposition relates TER with token F1 and NMI.\nProposition 2. For any assignment function ẑ : {1, · · · ,K} → {1, · · · ,K}, PTER(ẑ) = 0 if and only if F1(ẑ) = NMI(ẑ) = 1.\nProof. First of all, for such ẑ, we have 1 ≥ F1(ẑ) ≥ min{Prec(ẑ),Rec(ẑ)} ≥ 1 − Pe, TER(ẑ) = 1, where the third inequality comes from the fact that the set of permutations is a smaller set than the set of all many-to-one mappings π : {1, · · · ,K} → {1, · · · ,K}. Further, using the fact that z and ẑ are functions of each other when PTER(ẑ) = 0, it can be shown that NMI(ẑ) = 2I(z(X),ẑ(X))H(z(X))+H(ẑ(X)) = 2H(z(X))/2H(z(X)) = 1.\nA.3 Exact Discovery Guarantee\nFirst, we prove the claim made in Section 4.2 about nearest neighbor information quantizers. Recall the definition of general and nearest-neighbor information quantizers as follows.\nDefinition 3. (Information quantizer) A K-point information quantizer is a function q : ∆|Y| → C = {Q1, · · · , QK} ⊂ ∆|Y|, where C is called the codebook and Qk’s are called the code distributions. Further, define QK to be the class of such functions.\nDefinition 4. (Nearest-neighbor Information quantizer) A K-point information quantizer is called nearest-neighbor if, ∀P ∈ ∆|Y|, DKL(P ||q(P )) = min1≤k≤K DKL(P ||Qk). Further, define QNNK to be the class of such functions.\nThen we have the following lemma.\nLemma 1. There exists an information quantizer θ̂n ∈ Θ, q̂n ∈ QNNK such that\nLIQ(Pn, θ̂, q̂) = min θ∈Θ,q∈QK LIQ(Pn, θ, q). (14)\nTherefore, (θ̂, q̂) is an ERM of (P1).\nProof of Lemma 1. Notice that only the LQ term of Equation P1 depends on q, so it suffices to show that minq∈QNNK LQ(P̃n, q) ≤ minq∈QK LQ(P̃n, q). This is true since\nmin q∈QK LQ(P̃n, q) = min q∈QK EP̃n [DKL(P θ Y |X ||q(P θ Y |X))]\n≥ EP̃n [ min1≤k≤KDKL(P θ Y |X ||qk)]\n= min q∈QNNK\nEP̃n [DKL(P θ Y |X ||q(P θ Y |X))]\n= min q∈QNNK\nLQ(P̃n, q).\nNext, we show under the condition P θY |X = PY |X and n → ∞, (P1) recovers z(·) up to a permutation. Proposition 3. The pair (z∗, P ∗Y |Z) is a minimizer to the following optimization problem:\nmax ẑ:X→{1,··· ,K},PY |Z∈∆|Y| I(ẑ(X);Y ), (P0)\nif and only if z∗ is equal to the true assignment function z up to a permutation.\nProof. ⇒: First, z(·) is a feasible solution by definition. By data processing inequality, we have\nI(z′(X);Y ) ≤ I(X;Y ) = I(z(X);Y ).\nTherefore, z(·) is also the optimal solution. ⇐: Suppose there exists some optimal (ẑ, P̂Y |Z) with P̂Y |ẑ(x) 6= PY |z(x) for at least one x ∈ X . Since such discrepancies are independent with each other, it suffices to show that each such discrepancy leads to lower I(Z;Y ). Indeed, for (ẑ, P̂Y |Z) with P̂Y |Z=ẑ(x) 6= PY |Z=z(x) only at x,\nI(ẑ(X);Y )− I(z(X);Y )\n=PX(x) ∑ y PY |X(y|x) log P̂Y |Z=ẑ(x) PY |Z=z(x)\n=− PX(x)D(PY |Z=z(x)||P̂Y |Z=ẑ(x)) < 0,\nwhich contradicts the optimality of ẑ. Therefore, P̂Y |ẑ(x) = PY |z(x) for all optimal solution of (P0).\nTo prove Theorem 1, we also need the following lemma.\nLemma 2. Under Assumption 3, for any bounded parameter set Θ, there exists γ > 0 and some optimal parameter θ∗ ∈ Θ∗ such that DKL(P θ Y |X ||P θ∗ Y |X) ≥ γ‖θ − θ ∗‖, ∀θ ∈ Θ.\nProof. We prove the lemma by contradiction. First, we assume θ 6∈ Θ∗ since the inequality satisfies trivially for any θ ∈ Θ∗. By boundedness, there exists some R > 0 such that ‖θ‖ ≤ R. Suppose for any γ > 0, there exists some θ ∈ Θ such that DKL(P θ Y |X ||P θ∗ Y |X) ≤ γ‖θ − θ ∗‖ ≤ 2γR, then we have DKL(P θY |X ||P θ∗\nY |X) ≤ infγ>0 γR = 0. However, since DKL(P θY |X ||P θ∗\nY |X) ≥ 0, we have DKL(P θ Y |X ||P θ∗ Y |X) = 0, which implies θ ∈ Θ ∗ and leads to contradiction.\nNote it is crucial that the parameter set is bounded, which is the case for neural nets. Further, Assumption 3 is needed or the inequality can be easily violated when the optimal parameter set Θ∗ is empty.\nNext, we need the following lemma, which is based on (Tsai et al., 2020):\nLemma 3. Under Assumptions 1-3, and consider θ̂ to be part of the ERM of (P1) with conditional distribution P̂Y |X := P θ̂Y |X . Then for any > 0, the following inequality holds:\nP {\nsup x∈X\nDKL(PY |X=x||P̂Y |X=x) > }\n(15)\n≤2 ∣∣∣∣N (Θ, 4ρ) ∣∣∣∣ exp(− γ2n 22ρ2(Cu − Cl)2 ) , (16)\nwhere N (A, ) is the -net of set A.\nProof. For notational ease, we drop the dependence of LCE on P if the context is clear. Using Assumption 3, let PY |X = P θ ∗\nY |X . Define Dn(P ||Q) as the empirical KL divergence. Further, notice that for PY |X , LQ can always be made 0 and therefore, the ERM of P1 needs to satisfy LCE(θ̂) ≤ LCE(θ∗). As a result,\nDn(PY |X ||P̂Y |X)\n:=EPn\n[ log\nPY |X(Y |X) P̂Y |X(Y |X) ] =LCE(θ̂)− LCE(θ∗) ≤ 0.\nNote that Dn(P ||Q) is an unbiased estimator of the conditional KL divergence between distributions P and Q: EPXn,Y nEPn log PY |X(Y |X) QY |X(Y |X) =\nD(PY |X ||QY |X). Therefore,\nP { DKL(PY |X ||P̂Y |X) > } ≤P { DKL(PY |X ||P̂Y |X)−Dn(PY |X ||P̂Y |X) >\n} =P {∣∣∣Dn(PY |X ||P̂Y |X)−DKL(PY |X ||P̂Y |X)∣∣∣ > }\n≤P {\nsup θ∈Θ ∣∣∣Dn(PY |X ||P θY |X)−DKL(PY |X ||P θY |X)∣∣∣ > } . To bound the last probability, consider an 4ρ - net in the parameter space N (Θ, 4ρ) and Θ = ∪ |N (Θ, 4ρ )|\nk=1 Θk, where Θk is the 4ρ -ball surrounding θk ∈ N (Θ, 4ρ), and let ∆n(θ) := Dn(PY |X ||P θY |X) − DKL(PY |X ||P θ Y |X) we have ∀θ ∈ Θk,\nP {\nsup θ∈Θ |∆n(θ)| >\n} (17)\n≤ ∣∣∣N (Θ, 4ρ )∣∣∣∑ k=1 P { sup θ∈Θk |∆n(θ)| > }\n≤ ∣∣∣∣N (Θ, 4ρ) ∣∣∣∣ sup k P { sup θ∈Θk |∆n(θ)| > } .\n(18)\nFurther, by Assumption 2, we have\nsup θ∈Θk\n|∆n(θ)−∆n(θk)|\n≤ sup θ∈Θk ∣∣∣Dn(PY |X ||P θY |X)−Dn(PY |X ||P θkY |X)∣∣∣ + ∣∣∣DKL(PY |X ||P θY |X)−DKL(PY |X ||P θkY |X)∣∣∣\n=EPn ∣∣∣∣∣∣log P θkY |X(Y |X) P θY |X(Y |X) ∣∣∣∣∣∣+ EPXY ∣∣∣∣∣∣log P θkY |X(Y |X) P θY |X(Y |X) ∣∣∣∣∣∣ ≤2ρ‖θk − θ‖ ≤\n2 .\nAs a result,\nP { sup θ∈Θk |∆n(θ)| > }\n≤P { |∆n(θk)|+ sup\nθ∈Θk |∆n(θ)−∆n(θk)| > } ≤P { |∆n(θk)| >\n2 } ≤2 exp ( − n 2\n2(Cu − Cl)2\n) ,\nby Assumption 1 and Hoeffding’s inequality. Plugging this into (17), we arrive at\nP { DKL(PY |X ||P̂Y |X) > } (19)\n≤2 ∣∣∣∣N (Θ, 4ρ) ∣∣∣∣ exp(− n 22(Cu − Cl)2 ) . (20)\nTo prove uniform convergence, use Assumption 2 to conclude that:\nDKL(PY |X=x||P̂Y |X=x)\n= ∑ y PY |X(y|x) log P θ ∗ Y |X(y|x)\nP θ̂nY |X(y|x)\n≤ sup y ∣∣∣∣∣∣log P θ ∗ Y |X(y|x)\nP θ̂nY |X(y|x) ∣∣∣∣∣∣ ≤ ρ‖θ∗ − θ̂n‖, for some θ∗ ∈ Θ∗. Therefore, using the local convexity property of the KL divergence around minima in Lemma 2, we arrive at the desired result:\nP {\nsup x∈X\nDKL(PY |X=x||P̂Y |X=x) ≥ }\n≤P { ‖θ∗ − θ̂n‖ ≥\nρ\n} ≤ P { DKL(PY |X ||P̂Y |X) ≥ γ\nρ } ≤2 ∣∣∣∣N (Θ, 4ρ) ∣∣∣∣ exp(− γ2n 22ρ2(Cu − Cl)2 ) .\nNext, we prove the following lemma by performing a perturbation analysis on (P1) inspired by ((Qiu et al., 2019)).\nLemma 4. Consider some subset of speech segments D ⊂ X such that for any 1 ≤ z ≤ K, there exists x ∈ X such that z(x) = z. Further, suppose there exists > 0 such that ‖P̂Y |X=x − PY |X=x‖1 ≤ ,∀x ∈ D. Then, ∀x ∈ X, ‖q̂(P̂Y |X=x) − PY |X=x‖1 ≤ c1 1/2 for some constant c1 > 0.\nProof. We first prove the statement for the segments from the set D. By the definition of ERM,\nLQ(Pn, q̂)− LQ(Pn, q∗) (21)\n=EP̃n\n[ log\nPY |X(Y |X) q̂(P̂Y |X(Y |X))\n] ≤ 0. (*)\nFrom the condition in the lemma, we have P̂Y |X=x = PY |X=x + φx for some ∈ [0, 1] and φx ∈ R|Y|, φ>x 1 = 0, ‖φx‖1 ≤ 1,∀x ∈ D.\nFurther, suppose q(P̂Y |X) = PY |X +δψx for some δ ∈ [0, 1] and ψx ∈ R|Y|, ψ>x 1 = 0, ‖ψx‖1 ≤ 1, ∀x ∈ X. Using Assumption 1 and the inequality log(1 + x) ≤ x− x24 ,∀x ∈ (−1, 1], we have\n∑ y P̂Y |X(y|x) log PY |X(y|x) q̂(P̂Y |X(y|x))\n=− ∑ y PY |X(y|x) log ( 1 + δ ψx(y) PY |X(y|x) )\n− ∑ y φx(y) log PY |X(y|x) q̂(P̂Y |X(y|x))\n≥ ∑ y δ2ψ2x(y) 4PY |X(y|x) − Cu ≥ δ2‖ψx(y)‖2 4\n≥ δ 2\n4|Y| − Cu ,\nfor every x ∈ D. Therefore, to maintain (21), we need δ2 ≤ 4Cu|Y| for the training examples Xn and the inequality in the lemma holds for examples from D with coefficient c′1 := 2 √ Cu|Y|.\nTo show the same claim holds for any unseen segments x′ ∈ X\\D, we first use Lemma 1 to conclude that there always exists a nearest-neighbor information quantizer q̂ that is an ERM. Further, since every phoneme class occurs in D, we can always find x ∈ D such that z(x) = z(x′). Therefore, using the inequality log(1+x) ≥ x− x21+x ,∀x > −1, we have\n1 2 ‖P̂Y |X=x′ − q̂(P̂Y |X=x′)‖21\n≤D(P̂Y |X=x′ ||q̂(P̂Y |X=x′)) ≤ D(P̂Y |X=x′ ||q(P̂Y |X=x)) ≤D(PY |X=x′ ||q(P̂Y |X=x))+ |D(PY |X=x′ ||q(P̂Y |X=x′))−D(PY |X=x′ ||P̂Y |X=x′)| ≤D(PY |X=x′ ||q(P̂Y |X=x)) + (Cu − Cl)\n≤ ∑ y δ2ψx′(y) 2 P̂Y |X(y|xj) + (Cu − Cl)\n≤ e Cuδ2\nminy:PY |X(y|z(x′))>0 PY |Z(y|z(x′)) + (Cu − Cl)\n≤a1 ,\nwhere a1 := eCuc′21 /miny:PY |Z(y|z(x′))>0 PY |Z(y|z(x ′))+ Cu − Cl > c′21 . Notice that the minimum is taken over y’s with nonzero probabilities due to the boundedness conditions in Assumption 1, which asserts φx(y) = ψx(y) ≡ 0 for y’s with zero\nprobabilities. Finally, using triangular inequality:\n‖PY |X=x′ − q̂(P̂Y |X=x′)‖1 ≤‖P̂Y |X=x′ − q̂(P̂Y |X=x′)‖1 + ‖P̂Y |X=x′ − PY |X=x′‖1 ≤ √ 2a1 + ≤ c1 √ , where c1 := √\n2a1 + 1 is the coefficient in the lemma.\nNow we are ready to prove Theorem 1.\nProof of Theorem 1. Define the event C := {supx∈XD(PY |X=x||P̂Y |X=x) < }. Further, suppose Θ is within the ball of radius R in Rd. By Lemma 3, we have:\nP (C ) ≥ 1− exp(−c2n 2 + c3( )), (22)\nwhere c2 := γ 2\n2ρ2(Cu−Cl)2 , c3( ) := d logR(1 +\n8ρ ) + log 2 ≥ log 2|N (Θ, 4ρ)| (see e.g., (Vershynin, 2018), Section 4.2). For the subsequent discussion, suppose C occurs. To prove that ẑ achieves zero TER, it suffices to prove that ẑ(x) = ẑ(x′) ⇔ z(x) = z(x′),∀x, x′ ∈ X. To prove the “⇒” direction, suppose for some segment pairs (x1, x2) ∈ X2, ẑ(x1) = ẑ(x2) = z′ but z(x1) = z1 6= z(x2) = z2. Invoke Lemma 4 and write Qẑ(xj) = PY |X=xj + δψxj , δ = c1\n1/4, ψ>xj1 = 0, ‖ψxj‖1 ≤ 1, j ∈ {1, 2}. Use the inequality log(1 +x) ≥ x− x21+x ,∀x > −1 we have\nDKL(PY |X=xj ||Qẑ(xj)) = − ∑ y PY |X(y|xj) log ( 1 + δψxj (y) PY |X(y|xj) )\n≤ ∑ y eCuδ2ψxj (y) 2 PY |X(y|xj) ≤ a2(z1, z2)δ2,\nwhere a2(z1, z2) = maxj∈{1,2} e\nCu/miny:PY |Z(y|zj)>0 PY |Z(y|zj). As a result,\n2a2(z1, z2)δ 2 ≥\nDKL(PY |X=x1 ||Qz′) +DKL(PY |X=x2 ||Qz′) ≥ 2DJS(PY |X=x1 ||PY |X=x2), (23)\nwhich cannot be true if δ2 ≤ DJS(PY |Z=z1 ||PY |Z=z2 )a2(z1,z2) , or ≤ DJS(PY |Z=z1 ||PY |Z=z2 ) 2\nc1(z1,z2)2a2(z1,z2)2 .\nTo prove the other direction, we use “⇒” to conclude that every phoneme occurs in at least one distinct cluster from other classes, since every cluster\nin Ĉ contains only a unique phoneme class. Further, define E = { 1 n minz ∑n i=1 1Zi=z = 0 } . Using Sanov’s theorem (see e.g., (Cover and Thomas, 2006)), we have:\nP (E) ≤ (n+ 1)K exp ( −n min\nP∈PE DKL(P ||PZ)\n) ,\nwhere PE := {P ∈ ∆K : minz P (z) = 0}. Use Assumption 4 and optimize the bound, we obtain\nmin P∈PE\nDKL (P ||PZ)\n= min P∈PE DKL\n( P || 1\nK 1 ) = logK − max\nP∈PE H(P ) = log\nK\nK − 1 and\nP (E) ≤ exp ( −n log K\nK − 1 +K log(n+ 1)\n) .\nAs a result, phonemes of each class occur at least once in the training set with high probability. If this is the case and if there exists some x, x′ ∈ X such that z(x) = z(x′) but ẑ(x) 6= ẑ(x′), Ĉ contains at leastK+1 clusters, which contradicts Assumption 4. Therefore, define the event R := {ẑ(X) = ẑ(X ′)⇔ z(X) = z(X ′)}, the token error rate can be upper bounded as\nPTER(ẑ)\n≤P (C ∩ Ec)P {R|C ∩ Ec}+ P (Cc ∪ E) = exp (−nmin{e1(n, ∗), e2(n,K)}) ,\nwhere\n∗ := min z1 6=z2\nDJS(PY |Z=z1 ||PY |Z=z2)2\nc1(z1, z2)2a2(z1, z2)2\n=: min z1 6=z2\nc(z1, z2)DJS(PY |Z=z1 ||PY |Z=z2) 2\ne1( ∗) := c2 ∗2 − c3( ∗)\nn\ne2 := log K K − 1 − K log(n+ 1) n .\nTherefore, PTER(ẑ) ≤ δ amounts to\nc2n ∗2 − c3( ∗) ≥ log\n1\nδ\nn log K K − 1 −K log(n+ 1) ≥ log 1 δ .\nThe first inequality implies\nn ≥ log c3( ∗) + (1/δ)\nc2 ∗2 = O\n( log(1δ )\n∗2\n) .\nFor the second inequality, rearranging the terms we obtain:\nn ≥ K log KK−1\nlog n+ log 1δ\nlog KK−1 , (24)\nwhich by Lemma A.2 from (Shalev-Shwartz and Ben-David, 2014) holds if\nn ≥ 4K log 2K log K K−1 + 2 log 1δ\nlog KK−1 = O\n( log 1δ\nlog KK−1\n) .\n(25)\nCombining Equation 24 and Equation 25 proves the theorem."
    }, {
      "heading" : "B Collection Process and Statistics of the Spoken Word Datasets",
      "text" : "The dataset statistics of all the datasets used for our experiments are shown in Table 2. We collect all the spoken word datasets from existing datasets in the following steps:\n1. Decide the train-test split: For Flickr audio, we use the original training and validation set to extract spoken words for the training set and the test set to extract words for test set; for Librispeech, we use train-clean-100 and train-clean-360 for training set and dev-clean for test set; for TIMIT and Mboshi, we use the whole dataset without SA utterances to extract spoken words, to be consistent with prior works. For the latter, it will not lead to overfitting since our setting is unsupervised in a sense that the target label, phoneme, is not available during training.\n2. Decide the phoneme inventory: The phoneme inventory of the English corpora such as Flickr audio, Librispeech and TIMIT are the standard 61 phonemes from TIMIT merged into 39 classes for Librispeech and 44 classes for Flickr Audio, due to slightly different phoneme set required for the forced alignment systems used to extract phoneme and word boundaries. The phoneme inventory of Mboshi is provided in (Godard et al., 2017).\n3. Decide the vocabulary: For English corpora, we use a neural dependency parser (Gardner et al., 2017) to extract head words of noun phrases from the Flickr30kEntities and choose\nthose with frequency more than 500 times in the entire Flickr30k corpus. For Mboshi, we use the bigrams and trigrams as proxy for words.\n4. Word and phoneme boundary detection: For evaluation purposes, we need to extract word and phoneme boundaries for the utterances. While TIMIT and Mboshi has provided framelevel phoneme transcriptions, such labels are not available for Flickr Audio and Librispeech. Therefore, we use the Montreal forced aligner to extract Librispeech word and phoneme boundaries and another HMM-DNN system to extract Flickr i\n5. Extract spoken word utterances: To keep the dataset as balanced as possible, we set a cutoff on the maximal number of word utterances per class, which is set to be 200 for Flickr Audio and 1000 for Librispeech, TIMIT and Mboshi."
    }, {
      "heading" : "C Model Implementation",
      "text" : "For the pre-segmentation stage in Figure 2 of IQ, we use the self-supervised model proposed in (Kreuk et al., 2020) to predict the phoneme-level segmentation for English datasets, and the segmentation generated by one of our baselines (Feng et al., 2021a) for experiments on Mboshi language. The segmental speech encoder eθ1(·) is a CPC model pretrained on the whole 960h Librispeech (Nguyen et al., 2020) with 256-dimensional representation for each 10ms frame followed by averaging across each segments. The word posterior cθ2(·) for the joint distribution learning stage consists of four hidden layers and 512 ReLU units per layer with layer normalization and one softmax output layer. All our models are trained for 20 epochs using Adam optimizer (Kingma and Ba, 2014) with learning\nrate of 0.001 decayed by 0.97 every 2 epochs and a batch size of 8. We slightly modify (P1) analogous to the VQ-VAE (van den Oord et al., 2017) to make it more suitable for gradient-based optimization:\nLIQ-VAE(Pn, θ, q) := LCE(Pn, θ)+ λEPn [DKL(sg[P θY |X ]||q(P θ Y |X))+DKL(P θ Y |X ||sg[q(P θ Y |X)])]\nwhere sg[·] denotes the stop-gradient operation and λ = 0.5 for all experiments. Exponential moving average (EMA) codebook update is used with a decay rate of 0.999 to optimize the first KL term. Each code distribution is initialized using a symmetric Dirichlet distribution with a concentration parameter of 100.\nFor CPC, wav2vec and wav2vec 2.0, we extract discrete units using the same predicted and gold segmentations as our IQ model using k-means clustering with the same number of clusters (K = 31)."
    }, {
      "heading" : "D Convergence Plot for Word-Level Phoneme Discovery",
      "text" : "The convergence plot of Token F1 during training of IQ on Flickr Audio compared to the baselines\nis shown in Figure 8."
    }, {
      "heading" : "E Further Analysis of Representations Learned by IQ",
      "text" : "The visualizations of the estimated distributions P θY |X using t-SNE (van der Maaten and Hinton, 2008) on Mboshi are shown in Figure 9. We again observe that IQ is capable of clustering phonemes from the same manner class as shown in the t-SNE plots for TIMIT in the main text. We also show the most confusing phoneme pairs for both datasets in\n(9a) Manner-level t-SNE plots of phoneme clusters discovered by IQ with |Y| = 161 and gold segmentation on Mboshi\nFigure 9: t-SNE plot by IQ on TIMIT and Mboshi"
    }, {
      "heading" : "F Effect of Codebook Size for IQ",
      "text" : "The phoneme discovery results of IQ with different codebook sizes on Mboshi and TIMIT are shown\nPhoneme Pair Error Prob.\nae, aa 1.00 ch, ah 0.85 sh, s 0.82 ah, aa 0.82 aw, aa 0.77\nz, s 0.75 n, m 0.73 p, k 0.70 r, er 0.67 iy, ey 0.60\n(10a) Top-10 most confusing phoneme pairs by IQ with |Y| = 824 and predicted segmentation on TIMIT\nPhoneme Pair Error Prob.\na, Ng 1.00 bv, b 0.82 e, a 0.79 ţ, s 0.77 i, e 0.73 b, Ng 0.68 p, k 0.68 f, a 0.59 g, a 0.59 o, mw 0.56\n(10b) Top-10 most confusing phoneme pairs by IQ with |Y| = 161 with predicted segmentation on Mboshi\nin Table 3 and Table 4 respectively. As discussed in the paper, our IQ model achieving equally good NMI and boundary F1 and is thus robust to the codebook size on both datasets."
    }, {
      "heading" : "G More Speech Examples",
      "text" : "Lastly, we provide eight more spoken utterances annotated with phoneme discovery results."
    } ],
    "references" : [ {
      "title" : "Deep variational information bottleneck",
      "author" : [ "Alexander A. Alemi", "Ian Fischer", "Joshua V. Dillon", "Kevin Murphy." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Alemi et al\\.,? 2017",
      "shortCiteRegEx" : "Alemi et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised speech recognition",
      "author" : [ "Alexei Baevski", "Wei-Ning Hsu", "Alexis Conneau", "Michael Auli." ],
      "venue" : "ArKiv.",
      "citeRegEx" : "Baevski et al\\.,? 2021",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2021
    }, {
      "title" : "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "author" : [ "Alexei Baevski", "Steffen Schneider", "Michael Auli." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Baevski et al\\.,? 2019",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2019
    }, {
      "title" : "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "author" : [ "Alexei Baevski", "Henry Zhou", "Abdelrahman Mohamed", "Michael Auli" ],
      "venue" : "In Neural Information Processing System",
      "citeRegEx" : "Baevski et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2020
    }, {
      "title" : "Empirical evaluation of sequenceto-sequence models for word discovery in lowresource settings",
      "author" : [ "Marcely Zanon Boito", "Aline Villavicencio", "Laurent Besacier." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (IN-",
      "citeRegEx" : "Boito et al\\.,? 2019",
      "shortCiteRegEx" : "Boito et al\\.",
      "year" : 2019
    }, {
      "title" : "Completely unsupervised speech recognition by a generative adversarial network harmonized with iteratively refined hidden markov models",
      "author" : [ "K.-Y. Chen", "C.-P. Tsai", "D.-R. Liu", "H.-Y. Lee", "L. shan Lee." ],
      "venue" : "Proc. Annual Conference of Interna-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised speech representation learning using wavenet autoencoders",
      "author" : [ "Jan Chorowski", "Ron J. Weiss", "Samy Bengio", "Aäron van den Oord." ],
      "venue" : "IEEE Transactions on Audio, Speech and Language Processing.",
      "citeRegEx" : "Chorowski et al\\.,? 2019",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2019
    }, {
      "title" : "An unsupervised autoregressive model for speech representation learning",
      "author" : [ "Yu-An Chung", "Wei-Ning Hsu", "Hao Tang", "James R. Glass." ],
      "venue" : "In Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Chung et al\\.,? 2019",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2019
    }, {
      "title" : "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)",
      "author" : [ "Thomas M. Cover", "Joy A. Thomas." ],
      "venue" : "Wiley-Interscience, USA.",
      "citeRegEx" : "Cover and Thomas.,? 2006",
      "shortCiteRegEx" : "Cover and Thomas.",
      "year" : 2006
    }, {
      "title" : "The zero resource speech challenge 2017",
      "author" : [ "Ewan Dunbar", "Xuan-Nga Cao", "Juan Benjumea", "Julien Karadayi", "Mathieu Bernard", "Laurent Besacier", "Xavier Anguera", "Emmanuel Dupoux." ],
      "venue" : "CoRR, abs/1712.04313.",
      "citeRegEx" : "Dunbar et al\\.,? 2017",
      "shortCiteRegEx" : "Dunbar et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised acoustic unit discovery for speech synthesis",
      "author" : [ "Ryan Eloff", "André Nortje", "Benjamin van Niekerk", "Avashna Govender", "Leanne Nortje", "Arnu Pretorius", "Elan van Biljon", "Ewald van der Westhuizen", "Lisa van Staden", "Herman Kamper" ],
      "venue" : null,
      "citeRegEx" : "Eloff et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Eloff et al\\.",
      "year" : 2019
    }, {
      "title" : "Combining adversarial training and disentangled speech representation for robust zero-resource subword modeling",
      "author" : [ "Siyuan Feng", "Tan Lee", "Zhiyuan Peng." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (IN-",
      "citeRegEx" : "Feng et al\\.,? 2019",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2019
    }, {
      "title" : "How phonotactic affect multilingual and zero-shot asr performance",
      "author" : [ "Siyuan Feng", "Piotr Żelasko", "Laureano MoroVelázquez", "Ali Abavisani", "Mark Hasegawa-Johnson", "Odette Scharenborg", "Najim Dehak." ],
      "venue" : "IEEE International Conference on",
      "citeRegEx" : "Feng et al\\.,? 2021a",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised acoustic unit discovery by leveraging a language-independent subword discriminative feature representation. In INTERSPEECH",
      "author" : [ "Siyuan Feng", "Piotr Z̀elasko", "Laureano MoroVelázquez", "Odette Scharenborg" ],
      "venue" : null,
      "citeRegEx" : "Feng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "Allennlp: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke S. Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Gardner et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2017
    }, {
      "title" : "The DARPA TIMIT AcousticPhonetic Continuous Speech Corpus CDROM",
      "author" : [ "John S. Garofolo", "Lori F. Lamel", "William M. Fisher", "Jonathon G. Fiscus", "David S. Pallett", "Nancy L. Dahlgren." ],
      "venue" : "Linguistic Data Consortium.",
      "citeRegEx" : "Garofolo et al\\.,? 1993",
      "shortCiteRegEx" : "Garofolo et al\\.",
      "year" : 1993
    }, {
      "title" : "A very low resource language speech corpus for computational language documentation experiments",
      "author" : [ "Boito." ],
      "venue" : "CoRR, abs/1710.03501.",
      "citeRegEx" : "Boito.,? 2017",
      "shortCiteRegEx" : "Boito.",
      "year" : 2017
    }, {
      "title" : "Unsupervised word segmentation from speech with attention",
      "author" : [ "Pierre Godard", "Marcely Zanon Boito", "Lucas Ondel", "Alexandre Berard", "Aline Villavicencio", "Laurent Besacier." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Godard et al\\.,? 2018",
      "shortCiteRegEx" : "Godard et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep multimodal semantic embeddings for speech and images",
      "author" : [ "David Harwath", "James Glass." ],
      "venue" : "Automatic Speech Recognition and Understanding.",
      "citeRegEx" : "Harwath and Glass.,? 2015",
      "shortCiteRegEx" : "Harwath and Glass.",
      "year" : 2015
    }, {
      "title" : "Towards visually grounded subword speech unit discovery",
      "author" : [ "David Harwath", "James Glass." ],
      "venue" : "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Harwath and Glass.,? 2019",
      "shortCiteRegEx" : "Harwath and Glass.",
      "year" : 2019
    }, {
      "title" : "Learning hierarchical discrete linguistic units from visually-grounded speech",
      "author" : [ "David Harwath", "Wei-Ning Hsu", "James Glass." ],
      "venue" : "International Conference on Learning Representation.",
      "citeRegEx" : "Harwath et al\\.,? 2020",
      "shortCiteRegEx" : "Harwath et al\\.",
      "year" : 2020
    }, {
      "title" : "Framing image description as a ranking task: data, models and evaluation metrics",
      "author" : [ "M. Hodosh", "P. Young", "J. Hockenmaier." ],
      "venue" : "Journal of Artificial Intelligence Research. 9",
      "citeRegEx" : "Hodosh et al\\.,? 2010",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2010
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "Kurt Hornik", "Maxwell Stinchcombe", "Halbert White." ],
      "venue" : "Neural Networks, 2(5):359–366.",
      "citeRegEx" : "Hornik et al\\.,? 1989",
      "shortCiteRegEx" : "Hornik et al\\.",
      "year" : 1989
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Jang et al\\.,? 2016",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2016
    }, {
      "title" : "Weak top-down constraints for unsupervised acoustic model training",
      "author" : [ "Aren Jansen." ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Jansen.,? 2013",
      "shortCiteRegEx" : "Jansen.",
      "year" : 2013
    }, {
      "title" : "Toward spoken term discovery at scale with zero resources",
      "author" : [ "Aren Jansen", "Kenneth Church", "Hynek Hermansky." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Jansen et al\\.,? 2010",
      "shortCiteRegEx" : "Jansen et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings",
      "author" : [ "Herman Kamper", "Aren Jansen", "Sharon Goldwater." ],
      "venue" : "IEEE Transaction on Audio, Speech and Language Processing, 24:669–679.",
      "citeRegEx" : "Kamper et al\\.,? 2016",
      "shortCiteRegEx" : "Kamper et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Selfsupervised contrastive learning for unsupervised phoneme segmentation",
      "author" : [ "Felix Kreuk", "Joseph Keshet", "Yossi Adi." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Kreuk et al\\.,? 2020",
      "shortCiteRegEx" : "Kreuk et al\\.",
      "year" : 2020
    }, {
      "title" : "A nonparametric Bayesian approach to acoustic model discovery",
      "author" : [ "Chiaying Lee", "James Glass." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, pages 40–49, Stroudsburg, PA, USA. As-",
      "citeRegEx" : "Lee and Glass.,? 2012",
      "shortCiteRegEx" : "Lee and Glass.",
      "year" : 2012
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9(86):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Standardizing auditory tests",
      "author" : [ "W.A. Munson", "Mark B. Gardner." ],
      "venue" : "The Journal of the Acoustical Society of America, 22.",
      "citeRegEx" : "Munson and Gardner.,? 1950",
      "shortCiteRegEx" : "Munson and Gardner.",
      "year" : 1950
    }, {
      "title" : "The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken lan",
      "author" : [ "Tu Anh Nguyen", "Maureen de Seyssel", "Patricia Rozé", "Morgane Rivière", "Evgeny Kharitonov", "Alexei Baevski", "Ewan Dunbar", "Emmanuel Dupoux" ],
      "venue" : null,
      "citeRegEx" : "Nguyen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge",
      "author" : [ "Benjamin van Niekerk", "Leanne Nortje", "Herman Kamper." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Niekerk et al\\.,? 2020",
      "shortCiteRegEx" : "Niekerk et al\\.",
      "year" : 2020
    }, {
      "title" : "Variational inference for acoustic unit discovery",
      "author" : [ "L. Ondel", "L. Burget", "J. Cernocký." ],
      "venue" : "Spoken Language Technology for Underresourced Languages.",
      "citeRegEx" : "Ondel et al\\.,? 2016",
      "shortCiteRegEx" : "Ondel et al\\.",
      "year" : 2016
    }, {
      "title" : "Bayesian subspace hidden Markov model for acoustic unit discovery",
      "author" : [ "L. Ondel", "H.K. Vydana", "L. Burget", "J. Cernocký." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Ondel et al\\.,? 2019",
      "shortCiteRegEx" : "Ondel et al\\.",
      "year" : 2019
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "ArXiv.",
      "citeRegEx" : "Oord et al\\.,? 2019",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural discrete representation learning",
      "author" : [ "Aaron van den Oord", "Oriol Vinyals", "koray kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oord et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards unsupervised pattern discovery in speech",
      "author" : [ "Alex Park", "James Glass." ],
      "venue" : "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).",
      "citeRegEx" : "Park and Glass.,? 2005",
      "shortCiteRegEx" : "Park and Glass.",
      "year" : 2005
    }, {
      "title" : "Probabilistic clustering using maximal matrix norm couplings",
      "author" : [ "David Qiu", "Anuran Makur", "Lizhong Zheng." ],
      "venue" : "2018 56th Annual Allerton Conference on Communication, Control, and Computing.",
      "citeRegEx" : "Qiu et al\\.,? 2019",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2019
    }, {
      "title" : "wav2vec: Unsupervised pre-training for speech recognition",
      "author" : [ "Steffen Schneider", "Alexei Baevski", "Ronan Collobert", "Michael Auli." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Schneider et al\\.,? 2019",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding Machine Learning",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Shalev.Shwartz and Ben.David.,? 2014",
      "shortCiteRegEx" : "Shalev.Shwartz and Ben.David.",
      "year" : 2014
    }, {
      "title" : "The deterministic information bottleneck",
      "author" : [ "DJ Strouse", "David Schwab." ],
      "venue" : "Association for Uncertainty in Artificial Intelligence.",
      "citeRegEx" : "Strouse and Schwab.,? 2016",
      "shortCiteRegEx" : "Strouse and Schwab.",
      "year" : 2016
    }, {
      "title" : "The phonemic principle",
      "author" : [ "Morris Swadesh." ],
      "venue" : "Language, 10(2):117–129.",
      "citeRegEx" : "Swadesh.,? 1934",
      "shortCiteRegEx" : "Swadesh.",
      "year" : 1934
    }, {
      "title" : "Lexico-statistic dating of prehistoric ethnic contacts: With special reference to North American Indians and Eskimos",
      "author" : [ "Morris Swadesh." ],
      "venue" : "Proc. American Philosophical Society, 96(4):452–463.",
      "citeRegEx" : "Swadesh.,? 1952",
      "shortCiteRegEx" : "Swadesh.",
      "year" : 1952
    }, {
      "title" : "Neural methods for point-wise dependency estimation",
      "author" : [ "Yao-Hung Hubert Tsai", "Han Zhao", "Makoto Yamada", "Louis-Philippe Morency", "Ruslan Salakhutdinov." ],
      "venue" : "Neural Information Processing System.",
      "citeRegEx" : "Tsai et al\\.,? 2020",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2020
    }, {
      "title" : "Librispeech: an ASR corpus based on public domain audio books",
      "author" : [ "Panayotov Vassil", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "ICASSP, page pp. 5206–5210.",
      "citeRegEx" : "Vassil et al\\.,? 2015",
      "shortCiteRegEx" : "Vassil et al\\.",
      "year" : 2015
    }, {
      "title" : "High-Dimensional Probability–An Introduction with Applications in Data Science",
      "author" : [ "Roman Vershynin." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Vershynin.,? 2018",
      "shortCiteRegEx" : "Vershynin.",
      "year" : 2018
    }, {
      "title" : "Unsupervised speech recognition via segmental empirical output distribution matching",
      "author" : [ "Chih-Kuan Yeh", "Jianshu Chen", "Chengzhu Yu", "Dong Yu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Yeh et al\\.,? 2019",
      "shortCiteRegEx" : "Yeh et al\\.",
      "year" : 2019
    }, {
      "title" : "A hierarchical subspace model for language-attuned acoustic unit discovery",
      "author" : [ "B. Yusuf", "L. Ondel", "L. Burget", "J. Cernocký", "M. Saraclar." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Yusuf et al\\.,? 2020",
      "shortCiteRegEx" : "Yusuf et al\\.",
      "year" : 2020
    }, {
      "title" : "That sounds familiar: an analysis of phonetic representations transfer across languages",
      "author" : [ "Piotr Żelasko", "Laureano Moro-Velázquez", "Mark Hasegawa-Johnson", "Odette Scharenborg", "Najim Dehak." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Żelasko et al\\.,? 2020",
      "shortCiteRegEx" : "Żelasko et al\\.",
      "year" : 2020
    }, {
      "title" : "2021b). those with frequency more than 500 times in the entire Flickr30k corpus. For Mboshi, we use the bigrams and trigrams as proxy for words",
      "author" : [ "Feng" ],
      "venue" : null,
      "citeRegEx" : "Feng,? \\Q2021\\E",
      "shortCiteRegEx" : "Feng",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Thanks to recent developments in self-supervised speech representation learning (van den Oord et al., 2017, 2019; Chorowski et al., 2019; Baevski et al., 2020), there is new hope for the development of speech processing systems without the need for full textual transcriptions.",
      "startOffset" : 80,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "Thanks to recent developments in self-supervised speech representation learning (van den Oord et al., 2017, 2019; Chorowski et al., 2019; Baevski et al., 2020), there is new hope for the development of speech processing systems without the need for full textual transcriptions.",
      "startOffset" : 80,
      "endOffset" : 159
    }, {
      "referenceID" : 43,
      "context" : "(Linguistic definition of phonemes (Swadesh, 1934)) Phonemes are the smallest units in speech such that given a correct native word, the replacement of one or more phonemes by other phonemes (capable of occurring in the same position) results in a native word other than that intended, or a native-like nonsense word.",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 44,
      "context" : "Such supervision might be acquired in a naturalistic setting by asking native speakers to name objects in a set of standardized images, as is commonly done in primary education classrooms, or by asking for the translations of common words in a second language, a common baseline approach in dialectology and historical linguistics (Swadesh, 1952).",
      "startOffset" : 331,
      "endOffset" : 346
    }, {
      "referenceID" : 38,
      "context" : "Due to the challenge of learning phonemes, early works on unsupervised speech representation learning (Park and Glass, 2005; Lee and Glass, 2012; Ondel et al., 2016) focus on learning speech segments sharing similar acoustic properties, or phones, without taking into account the meaning of the speech they are part of.",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 29,
      "context" : "Due to the challenge of learning phonemes, early works on unsupervised speech representation learning (Park and Glass, 2005; Lee and Glass, 2012; Ondel et al., 2016) focus on learning speech segments sharing similar acoustic properties, or phones, without taking into account the meaning of the speech they are part of.",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 34,
      "context" : "Due to the challenge of learning phonemes, early works on unsupervised speech representation learning (Park and Glass, 2005; Lee and Glass, 2012; Ondel et al., 2016) focus on learning speech segments sharing similar acoustic properties, or phones, without taking into account the meaning of the speech they are part of.",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 38,
      "context" : "In particular, (Park and Glass, 2005; Jansen et al., 2010) first detect segments with recurring patterns in the speech corpus followed by graph clustering using the similarity graph formed by the segments.",
      "startOffset" : 15,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : "In particular, (Park and Glass, 2005; Jansen et al., 2010) first detect segments with recurring patterns in the speech corpus followed by graph clustering using the similarity graph formed by the segments.",
      "startOffset" : 15,
      "endOffset" : 58
    }, {
      "referenceID" : 29,
      "context" : "(Lee and Glass, 2012; Ondel et al., 2016; Kamper et al., 2016) develop probabilistic graphical models to jointly segment and cluster speech into phone-like segments.",
      "startOffset" : 0,
      "endOffset" : 62
    }, {
      "referenceID" : 34,
      "context" : "(Lee and Glass, 2012; Ondel et al., 2016; Kamper et al., 2016) develop probabilistic graphical models to jointly segment and cluster speech into phone-like segments.",
      "startOffset" : 0,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "(Lee and Glass, 2012; Ondel et al., 2016; Kamper et al., 2016) develop probabilistic graphical models to jointly segment and cluster speech into phone-like segments.",
      "startOffset" : 0,
      "endOffset" : 62
    }, {
      "referenceID" : 35,
      "context" : "An extension to the latent variable approach is to introduce additional latent variables such as speaker identity (Ondel et al., 2019) or language identity (Yusuf et al.",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 49,
      "context" : ", 2019) or language identity (Yusuf et al., 2020) and develop mechanisms to disentangle these variables.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "With the advance of deep learning, neural network models have also been proposed to learn unsupervised phone-level representation either by first learning a continuous representation (Chung et al., 2019; Feng et al., 2019; Nguyen et al., 2020) followed by off-line clustering, or by learning a discrete representation end-to-end with Gumbel softmax (Eloff et al.",
      "startOffset" : 183,
      "endOffset" : 243
    }, {
      "referenceID" : 11,
      "context" : "With the advance of deep learning, neural network models have also been proposed to learn unsupervised phone-level representation either by first learning a continuous representation (Chung et al., 2019; Feng et al., 2019; Nguyen et al., 2020) followed by off-line clustering, or by learning a discrete representation end-to-end with Gumbel softmax (Eloff et al.",
      "startOffset" : 183,
      "endOffset" : 243
    }, {
      "referenceID" : 32,
      "context" : "With the advance of deep learning, neural network models have also been proposed to learn unsupervised phone-level representation either by first learning a continuous representation (Chung et al., 2019; Feng et al., 2019; Nguyen et al., 2020) followed by off-line clustering, or by learning a discrete representation end-to-end with Gumbel softmax (Eloff et al.",
      "startOffset" : 183,
      "endOffset" : 243
    }, {
      "referenceID" : 3,
      "context" : ", 2020) followed by off-line clustering, or by learning a discrete representation end-to-end with Gumbel softmax (Eloff et al., 2019b; Baevski et al., 2020) or vector-quantized variational autoencoder (VQVAE) (van den Oord et al.",
      "startOffset" : 113,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : ", 2020) or vector-quantized variational autoencoder (VQVAE) (van den Oord et al., 2017; Chorowski et al., 2019; Baevski et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : ", 2020) or vector-quantized variational autoencoder (VQVAE) (van den Oord et al., 2017; Chorowski et al., 2019; Baevski et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "However, codebooks learned by the neural approaches tend to be much larger than the number of phonemes (Baevski et al., 2020), leading to low scores in standard phoneme discovery metrics.",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 50,
      "context" : "Along this direction, (Żelasko et al., 2020; Feng et al., 2021a) systematically study the performance of zero-shot crosslingual ASR on 13 languages trained with international phonetic alphabet (IPA) tokens and found that the system tends to perform poorly on unseen languages.",
      "startOffset" : 22,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Along this direction, (Żelasko et al., 2020; Feng et al., 2021a) systematically study the performance of zero-shot crosslingual ASR on 13 languages trained with international phonetic alphabet (IPA) tokens and found that the system tends to perform poorly on unseen languages.",
      "startOffset" : 22,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : ", 2021b) is able to discover phone-like units by clustering bottleneck features (BNF) from a factorized time-delay neural network (TDNN-f) trained with phone labels predicted by a crosslingual ASR (Feng et al., 2021a).",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 24,
      "context" : "(Jansen, 2013) has studied the use of pairwise word identity labels for training phoneme discovery models based on Gaussian mixture models (GMM); (Harwath and Glass, 2019) analyzes the hidden layers of a two-branch neural network trained to retrieve spoken captions with semantically related images and finds strong correlation between segment representation and phoneme boundaries.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "(Jansen, 2013) has studied the use of pairwise word identity labels for training phoneme discovery models based on Gaussian mixture models (GMM); (Harwath and Glass, 2019) analyzes the hidden layers of a two-branch neural network trained to retrieve spoken captions with semantically related images and finds strong correlation between segment representation and phoneme boundaries.",
      "startOffset" : 146,
      "endOffset" : 171
    }, {
      "referenceID" : 20,
      "context" : "(Harwath et al., 2020) adds hierarchical vector quantization (VQ) layers in the same retrieval network and is able to find a much smaller codebook than the unsupervised neural approach (Baevski et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : ", 2020) adds hierarchical vector quantization (VQ) layers in the same retrieval network and is able to find a much smaller codebook than the unsupervised neural approach (Baevski et al., 2020), and achieve high correlation with the phoneme inventory.",
      "startOffset" : 170,
      "endOffset" : 192
    }, {
      "referenceID" : 17,
      "context" : "(Godard et al., 2018; Boito et al., 2019) has studied the possibility of learning semantic units using an attention-based speech-to-text translation system, though the units appear to correlate more with words.",
      "startOffset" : 0,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "(Godard et al., 2018; Boito et al., 2019) has studied the possibility of learning semantic units using an attention-based speech-to-text translation system, though the units appear to correlate more with words.",
      "startOffset" : 0,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "Works on unsupervised speech recognition (Chen et al., 2019)",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 48,
      "context" : "attempt to learn to recognize phonemes by leveraging the semantic information from a phoneme language model unpaired with the speech, typically by matching the empirical prior and posterior distributions of phonemes either using cross entropy (Yeh et al., 2019) or adversarial loss (Chen et al.",
      "startOffset" : 243,
      "endOffset" : 261
    }, {
      "referenceID" : 49,
      "context" : "It can be easily shown that TER is equivalent to standard evaluation metrics for phoneme discovery such as normalized mutual information (NMI) (Yusuf et al., 2020; Harwath et al., 2020; Feng et al., 2021b) and token F1 (Dunbar et al.",
      "startOffset" : 143,
      "endOffset" : 205
    }, {
      "referenceID" : 20,
      "context" : "It can be easily shown that TER is equivalent to standard evaluation metrics for phoneme discovery such as normalized mutual information (NMI) (Yusuf et al., 2020; Harwath et al., 2020; Feng et al., 2021b) and token F1 (Dunbar et al.",
      "startOffset" : 143,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : ", 2021b) and token F1 (Dunbar et al., 2017), as presented in Appendix A.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 28,
      "context" : "The pre-segmentation stage takes a raw speech waveform as input and extracts phoneme-level segments x = [x1, · · · , xT ] in a self-supervised fashion (Kreuk et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 171
    }, {
      "referenceID" : 45,
      "context" : "The first two assumptions are similar to the ones in (Tsai et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : "However, such assumption can be relaxed so that DKL(P θ ∗ Y |X ||PY |X) ≤ ν, ∀θ ∗ ∈ Θ∗ for some small enough ν > 0, which does not affect the essential idea behind our analysis and can be achieved by some rich class of universal approximators such as neural networks (Hornik et al., 1989).",
      "startOffset" : 267,
      "endOffset" : 288
    }, {
      "referenceID" : 21,
      "context" : "The vocabulary set with |Y| = 224 is selected from head words of noun phrases from the Flickr30kEntities dataset (Hodosh et al., 2010) that appear at least 500 times.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "For the Flickr audio word dataset, spoken words in the vocabulary are extracted from Flickr audio dataset (Harwath and Glass, 2015).",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 46,
      "context" : "For the Librispeech and TIMIT word dataset with |Y| = 224, spoken words are extracted from Librispeech (Vassil et al., 2015) 460-hour train-clean subset, resulting in a dataset of about 6 hours and 0.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 49,
      "context" : "Note that the amount of labeled data we need is much lower than previous works (Yusuf et al., 2020): around 30 hours, (Feng et al.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "The first dataset is TIMIT (Garofolo et al., 1993), an English corpus consisting of about 5 hours speech and Mboshi (Godard et al.",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 49,
      "context" : "For both datasets, we follow the split in (Yusuf et al., 2020), (Feng et al.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "The last two baselines use discrete representations: the Gumbel variational information bottleneck (Alemi et al., 2017) (Gumbel VIB) is a neural model with a Gumbel softmax (Jang et al.",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : ", 2017) (Gumbel VIB) is a neural model with a Gumbel softmax (Jang et al., 2016) layer to approximate the codebook assignment function z(·), and we set β = 0.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 42,
      "context" : "information bottleneck (DIB), a generalization of (Strouse and Schwab, 2016) for continuous feature variable X , which assumes the same deterministic relation between speech X and codebook unit Z as ours, but optimizes the models in a pipeline fashion (first the speech encoder and then the quantizer) by performing clustering on the learned conditional distributions.",
      "startOffset" : 50,
      "endOffset" : 76
    }, {
      "referenceID" : 49,
      "context" : "For the whole-sentence datasets, we compare our models three phoneme discovery systems, namely, the unsupervised H-SHMM trained with multilingual speech (Yusuf et al., 2020), the ResDAVEnet-VQ (Harwath et al.",
      "startOffset" : 153,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : ", 2020), the ResDAVEnet-VQ (Harwath et al., 2020) with visual supervision and the TDNN-f system by (Feng et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 40,
      "context" : "To study how well our model performs in extreme low-resource speech recognition compared to other neural speech representation learning models, we compare our models with wav2vec (Schneider et al., 2019), wav2vec 2.",
      "startOffset" : 179,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "0 (Baevski et al., 2020), vq-wav2vec with Gumbel softmax and k-means as discretization strategies (Baevski et al.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : ", 2020), vq-wav2vec with Gumbel softmax and k-means as discretization strategies (Baevski et al., 2019), CPC (van den Oord et al.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 49,
      "context" : "Evaluation metrics Standard metrics are used such as NMI and boundary F1 for the quality of codebook and segmentation respectively with the same implementation as in prior works (Yusuf et al., 2020; Feng et al., 2021b).",
      "startOffset" : 178,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "In addition, token F1 (Dunbar et al., 2017) is also reported.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "On TIMIT, our model is able to outperform the visually grounded baseline (Harwath et al., 2020) for all training vocabulary, and all three baselines for |Y| = 524 and |Y| = 824 with and without gold segmentation in terms of all three metrics.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 31,
      "context" : "Evidently, from Figure 7, where we use ABX accuracy (Munson and Gardner, 1950) to score how reliable the IQ codebook can identify segments of the same phoneme, we observe a strong correlation is observed between ABX accuracy and the frequency of the phonemes.",
      "startOffset" : 52,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "which is also related to the token F1 used for acoustic unit discovery (Dunbar et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 45,
      "context" : "Next, we need the following lemma, which is based on (Tsai et al., 2020):",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 39,
      "context" : "Next, we prove the following lemma by performing a perturbation analysis on (P1) inspired by ((Qiu et al., 2019)).",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "Decide the vocabulary: For English corpora, we use a neural dependency parser (Gardner et al., 2017) to extract head words of noun phrases from the Flickr30kEntities and choose",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 49,
      "context" : "Mboshi has the same number of training and test words since the whole datasets are used for both training and evaluation, consistent with prior works (Yusuf et al., 2020; Feng et al., 2021b).",
      "startOffset" : 150,
      "endOffset" : 190
    }, {
      "referenceID" : 28,
      "context" : "For the pre-segmentation stage in Figure 2 of IQ, we use the self-supervised model proposed in (Kreuk et al., 2020) to predict the phoneme-level segmentation for English datasets, and the segmentation generated by one of our baselines (Feng et al.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : ", 2020) to predict the phoneme-level segmentation for English datasets, and the segmentation generated by one of our baselines (Feng et al., 2021a) for experiments on Mboshi language.",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 32,
      "context" : "The segmental speech encoder eθ1(·) is a CPC model pretrained on the whole 960h Librispeech (Nguyen et al., 2020) with 256-dimensional representation for each 10ms frame followed by averaging across each segments.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "All our models are trained for 20 epochs using Adam optimizer (Kingma and Ba, 2014) with learning rate of 0.",
      "startOffset" : 62,
      "endOffset" : 83
    } ],
    "year" : 0,
    "abstractText" : "Phonemes are defined by their relationship to words: changing a phoneme changes the word. Learning a phoneme inventory with little supervision has been a longstanding challenge with important applications to under-resourced speech technology. In this paper, we bridge the gap between the linguistic and statistical definition of phonemes and propose a novel neural discrete representation learning model for self-supervised learning of phoneme inventory with raw speech and word labels. Under mild assumptions, we prove that the phoneme inventory learned by our approach converges to the true one with an exponentially low error rate. Moreover, in experiments on TIMIT and Mboshi benchmarks, our approach consistently learns a better phoneme-level representation and achieves a lower error rate in a zeroresource phoneme recognition task than previous state-of-the-art self-supervised representation learning algorithms.",
    "creator" : null
  }
}
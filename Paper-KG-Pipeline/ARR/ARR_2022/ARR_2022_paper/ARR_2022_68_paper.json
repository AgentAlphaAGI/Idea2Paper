{
  "name" : "ARR_2022_68_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Simulating Bandit Learning from User Feedback for Extractive Question Answering",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback."
    }, {
      "heading" : "1 Introduction",
      "text" : "Explicit feedback from users of NLP systems can be used to continually improve system performance. For example, a user posing a question to a question-answering (QA) system can mark if a predicted phrase is a valid answer given the context from which it was extracted. However, the dominant paradigm in NLP separates model training from deployment, leaving models static following learning and throughout interaction with users. This approach misses opportunities for learning during system usage, which beside several exceptions we discuss in Section 8 is understudied in NLP. In this paper, we study the potential of learning from explicit user feedback for extractive QA through simulation studies.\nExtractive QA is a popular testbed for language reasoning, with rich prior work on datasets (e.g., Rajpurkar et al., 2016), task design (Yang et al., 2018; Choi et al., 2018), and model architecture development (Seo et al., 2017; Yu et al., 2018). Learning from interaction with users remains relatively understudied, even though QA is well positioned to elicit user feedback. An extracted answer can be clearly visualized within its supporting context, and a language-proficient user can then easily validate\nif the answer is supported or not.1 This allows for simple binary feedback, and creates a contextual bandit learning scenario (Auer et al., 2002; Langford and Zhang, 2007). Figure 1 illustrates this learning signal and its potential.\nWe simulate user feedback using several widely used QA datasets, and use it as a bandit signal for learning. We study the empirical characteristics of the learning process, including its performance, sensitivity to initial system performance, and tradeoffs between online and offline learning. We also simulate zero-annotation domain adaptation, where we deploy a QA system trained from supervised data in one domain and adapt it solely from user feedback in a new domain.\n1Answers could also come from erroneous or deceitful contexts. This important problem is not studied by most work in extractive QA, including ours. We leave it for future work.\nThis learning scenario can mitigate fundamental problems in extractive QA. It reduces data collection costs, by delegating much of the learning to interaction with users. It can avoid data collection artifacts because the data comes from the actual system deployment, unlike data from an annotation effort that often involves design decisions immaterial to the system’s use case. For example, sharing question-annotator and answer-annotator roles (Rajpurkar et al., 2016), which is detrimental to emulate information seeking behavior (Choi et al., 2018). Finally, it gives systems the potential to evolve over time as the world changes (Lazaridou et al., 2021; Zhang and Choi, 2021).\nOur simulation experiments show that user feedback is an effective signal to continually improve QA systems across multiple benchmarks. For example, an initial system trained with a small amount of SQUAD (Rajpurkar et al., 2016) annotations (64 examples) improves from 18 to 81.6 F1 score, and adapting a SearchQA (Dunn et al., 2017) system to SQUAD through user feedback improves it from 45 to 84 F1 score. Our study shows the impact of initial system performance, trade-offs between online and offline learning, and the impact of source domain on adaptation. These results create the base for future work that goes beyond simulation to use feedback from human users to improve extractive QA systems. Our code will be made available upon publication."
    }, {
      "heading" : "2 Learning and Interaction Scenario",
      "text" : "We study a scenario where a QA model learns from explicit user feedback. We formulate learning as a contextual bandit problem. The input to the learner is a question-context pair, where the context paragraph contains the answer to the question.The output is a span in the context paragraph that is the answer to the question.\nGiven a question-context pair, the model predicts an answer span. The user then provides feedback about the model’s predicted answer, which is used to update the model parameters. We intentionally experiment with simple binary feedback and basic learning algorithms, to provide a baseline for what more advanced methods could achieve with as few assumptions as possible.\nScenario Formulation Let a question q̄ be a sequence of m tokens 〈q1, . . . , qm〉 and a context paragraph c̄ be a sequence of n tokens 〈c1, . . . , cn〉.\nAn extractive QA model2 π predicts a span ŷ = 〈ci, . . . , cj〉 where i, j ∈ [1, n] and i ≤ j in the context c̄ as an answer. When relevant, we denote πθ as a QA model parameterized by θ.\nWe formalize learning as a contextual bandit process: at each time step t, the model is given a question-context pair (q̄(t), c̄(t)), predicts an answer span ŷ, and receives a reward r(t) ∈ IR. The learner’s goal is to maximize the total reward∑T\nt=1 r (t). This formulation reflects a setup where, given a question-context pair, the QA system interacts with users, who validate the model-predicted answer in context, and provide feedback which is mapped to a numerical reward.\nLearning Algorithm We learn using policy gradient. Our learner is similar to REINFORCE (Sutton and Barto, 1998; Williams, 2004), but we use arg max to predict answers instead of Monte Carlo sampling from the model’s output distribution.3\nWe study online and offline learning, also referred to as on- and off-policy. In online learning (Algorithm 1), the model identity is maintained between prediction and update; the parameter values that are updated are the same that were used to generate the output receiving reward. This entails that a reward is only used once, to update the model after observing it. In offline learning (Algorithm 2), this relation between update and prediction does not hold. The learner observes reward, often across many examples, and may use it to update the model many times, even after the parameters drifted arbitrarily far from these that generated the prediction. In practice, we observe reward for the entire length of the simulation (T steps) and then update for E epochs. The reward is re-weighted to provide an unbiased estimation using inverse propensity score (IPS; Horvitz and Thompson, 1952). We clip the debiasing coefficient to avoid amplifying examples with large coefficients (line 10, Algorithm 2).\nIn general, offline learning is easier to implement because updating the model is not integrated with its deployment. Offline learning also uses a training loop that is similar to optimization practices in supervised learning. This allows to iterate over the data multiple times, albeit with the same feedback\n2In bandit literature, the term policy is more commonly used. We use the term model to align with the QA literature.\n3Early experiments showed that sampling is not as beneficial as arg max, potentially because of the relatively large output space of extractive QA. Yao et al. (2020) made a similar observation for semantic parsing. Table 4 in Appendix A provides experimental results with sampling.\nAlgorithm 1 Online learning. 1: for t = 1 · · · do 2: Receive a question q̄(t) and context c̄(k)\n3: Predict an answer ŷ(t) ← arg maxy πθ(y | q̄(t), c̄(t)) 4: Observe a reward r(t) 5: Update the model parameters θ using the gradient r(t)∇θ log πθ(ŷ(t) | q̄(t), c̄(t)) 6: end for\nAlgorithm 2 Offline learning. 1: for t = 1 · · ·T do 2: Receive a question q̄(t) and context c̄(t)\n3: Predict an answer ŷ(t) ← arg maxy πθ(y | q̄(t), c̄(t)) 4: p(t) ← πθ(ŷ(t) | q̄(t), c̄(t)) 5: Observe a reward r(t) 6: end for 7: for E epochs do 8: for t = 1 · · ·T do 9: Compute clipped importance-weighted reward ac-\ncording to the current model parameters: 10: r′ ← clip(πθ(ŷ\n(t)|q̄(t),c̄(t)) p(t) , 0, 1)r(t)\n11: Update the model parameters θ using the gradient r′∇θ log πθ(ŷ(t) | q̄(t), c̄(t)) 12: end for 13: end for\nsignal on each example. However, online learning often has lower regret as the model is updated after each interaction. It may also lead to higher overall performance, because as the model improves early on, it may observe more positive feedback overall, which is generally more informative. We empirically study these trade-offs in Sections 5 and 6.\nEvaluating Performance We evaluate model performance using token-level F1 on a held-out test set, as commonly done in the QA literature (Rajpurkar et al., 2016). We also estimate the learner regret, a common measure for evaluating bandit learning. Intuitively, regret is the deficit suffered by the learner relative to the optimal model (i.e., policy) up to a specific time step. Regret is calculated with respect to the optimal model π∗ ∈ arg maxπ∈Π E(q̄,c̄,y,r)∼D[r], where Π is the set of all models andD is the data distribution. The cumulative regret at time T is:\nRT := T∑ t=1 r∗(t) − T∑ t=1 r(t) , (1)\nwhere r(t) is the reward observed at time t and r∗(t) is the reward that the optimal model π∗ would observe. Minimising the cumulative regret is equivalent to maximising the total reward.4 Computing\n4Equivalently, the problem is often formulated as loss minimization (Bietti et al., 2018).\nregret requires access to the an oracle π∗. We use human annotation as an estimate (Section 3).5\nComparison to Supervised Learning In supervised learning, the data distribution is not dependent on the model, but on a fixed training set {(q̄(t), c̄(t), y(t)}Tt=1. In contrast, bandit learners are provided with reward data that depends on the model itself: {(q̄(t), c̄(t), ŷ(t), r(t))}Tt=1 where r is the reward for the model prediction ŷ(t) = arg maxy πθ(y | q̄(t), c̄(t)) at time step t. Such feedback can be freely gathered from users interacting with the model, while building supervised datasets requires costly annotation. This learning signal can also reflect changing task properties (e.g., world changes) to allow systems to adapt, and its origin in the deployed system use makes it more robust to biases introduced during annotation."
    }, {
      "heading" : "3 Simulation Setup",
      "text" : "We initialize our model with supervised data, and then simulate bandit feedback using supervised data annotations. Initialization is critical so the model does not return random answers, which are likely to all be bad because of the large output space. We use relatively little supervised data from the same domain for in-domain experiments (Sections 5 and 6) to focus on the data annotation reduction potential of user feedback. For domain adaptation, we assume access to a large amount of training data in the source domain, and no annotated data in the target domain (Section 7).\nReward We use supervised data annotations to simulate the reward. If the predicted answer span is an exact match index-wise to the annotated span, the learner observes a positive reward of 1.0, and negative reward -0.1 otherwise.6 This reward signal is more strict than QA evaluation metrics (tokenlevel F1 or exact match after normalization).7\nNoise Simulation We study robustness by simulating noisy feedback via reward perturbation: randomly flipping the binary reward with a fixed probability of 8% or 20% as the noise ratio.8\n5Our oracle is an estimate because of annotation noise and ambiguity in exact span selection.\n6We experimented with other reward values, but did not observe a significant difference in performance (Appendix A).\n7Normalization includes lowercasing, modifying spacing, removing articles and punctuation, etc. NQ is an exception, with an exact match measure that has similar strictness.\n8Even without our noise simulation, the simulated feedback inherits the noise from the annotation, either from crowdsourcing or distant supervision."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Data We use six QA datasets that provide substantial amount of annotated training data taken from the MRQA training portion (Fisch et al., 2019): SQUAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and NaturalQuestions (NQ; Kwiatkowski et al., 2019). Table 7 in Appendix B provides dataset details. We compute performance measures and learning curves on development sets following prior work (Rajpurkar et al., 2016; Ram et al., 2021).\nModel We conduct experiments with a pretrained SpanBERT model (Joshi et al., 2020), which shows substantially better performance on QA tasks in contrast to comparable models. We fine-tune the pre-trained SpanBERT-base model during initial learning and our simulations.\nImplementation Details We use Hugging Face Transformers (Wolf et al., 2020). When training initial models with little in-domain supervised data (Section 5; Section 6), we use a learning rate of 3e-5 with a linear schedule, batch size 10, and 10 epochs. We obtain the sets of 64, 256, or 1024 examples from prior work (Ram et al., 2021).9 For models initially trained on complete datasets (Section 7), we use a learning rate 2e-5 with a linear schedule, batch size 40, and four epochs.\nIn simulation experiments, we use batch size 40. We turn off dropout, because all experiments simulate interaction with users. For single-pass online learning experiments (Section 5; Section 7), we use a constant learning rate of 1e-5. For offline learning experiments (Section 6), we train the model for three epochs on the collected feedback with a linear schedule learning rate of 3e-5.\nOnline experiments with SQUAD, HotpotQA, NQ, and NewsQA take 2–4h each on one NVIDIA GeForce RTX 2080 Ti; 2.5–6h for offline. For TriviaQA and SearchQA, each online simulation experiment on one NVIDIA TITAN RTX takes 4–9.5h; 9–20h for offline."
    }, {
      "heading" : "5 Online Learning",
      "text" : "We simulate a scenario where only a limited amount of supervised data is available, and the model mainly learns from explicit user feedback\n9We use the seed 46 sets publicly available at https: //github.com/oriram/splinter.\non predicted answers. We use 64, 256, or 1,024 in-domain annotated examples to train an initial model. This section focuses on online learning, where the learner updates the model parameters after each feedback collection (Algorithm 1).\nFigure 2 presents the performance of in-domain simulation with online learning. The performance pattern varies across different datasets. Bandit learning consistently improves performance on SQUAD, HotpotQA, and NQ across different amounts of supervised data used to train the initial model. The performance gain is larger with weaker initial models (i.e., trained on 64 supervised examples): 63.6 on SQUAD, 42.7 on HotpotQA, and 40.0 on NQ. Bandit learning is not always effective on NewsQA, TriviaQA, and SearchQA, especially with weaker initial models. The may be attributed to the quality of training set annotations, which determines the accuracy of reward in our setup. SearchQA and TriviaQA use distant supervision to match questions and relevant contexts from the web, likely decreasing reward quality in our setup. While NewsQA is crowdsourced, Trischler et al. (2017) report relatively low human performance (69.4 F1), possibly indicating data challenges that also decrease our reward quality.\nFeedback Noise Simulation Figure 3 shows learning curves with simulated noise via different amounts of feedback perturbation (0%, 8%, or 20%). When perturbation-free simulation is effective, models remain robust to noise: 8% noise results in small fluctuations of the learning curve, but the final performance degrades minimally. Starting with weaker initial models and learning with a higher noise ratio may cause learning to fail (e.g., simulation on SQUAD with 64 initial examples and 20% noise). When online perturbation-free simulation fails, online learning with noisy feedback fails too. Learning progression across datasets shows that initial models trained with 1,024 examples can achieve peak performance with one third or even one quarter of feedback provided.\nSensitivity Analysis Training Transformerbased models has been shown to have stability issues, especially when training with limited amount of data (Zhang et al., 2021). Our non-standard training procedure (i.e., one epoch with a fixed learning rate) may further increase instability. We study the stability of the learning process using the challenging settings of using initial models trained on 64 in-domain supervised\nexamples on HotpotQA and TriviaQA: the former shows significant performance gain while the latter shows the opposite. We experiment with five initial models trained on different sets of 64 supervised examples, each used to initiate a separate simulation experiment. Four out of five experiments on HotpotQA show performance gains similar to what we observed so far, except one experiment that starts with very low initialization performance. In contrast, nearly all experiments on TriviaQA collapse (mean F1 of 7.3). We also conduct sensitivity analysis with stronger initial models trained with 1,024 examples, and observe that the final performance is stable across runs on both HotpotQA and TriviaQA (standard deviations are 0.5 and 2.6). Table 5 in Appendix B provides detailed performance numbers."
    }, {
      "heading" : "6 Offline Learning",
      "text" : "We simulate offline bandit learning (Algorithm 2), where feedback is collected all at once with the initial model. The learning scenario follows the previous section: only a limited amount of supervised data is available (64, 256, or 1,024 in-domain examples) to train initial models.\nTable 1 shows the performance of offline simulation experiments compared to online simulations. We observe mixed results. On SQUAD, HotpotQA, NQ, and NewsQA, offline learning outperforms online learning when using stronger initial models (i.e., models trained on 256 and 1,024 examples). This illustrates the benefit of the more standard training loop, especially with our Transformerbased model that is best optimized with a linear learning rate schedule and multiple epochs, both incompatible with the online setup. On TriviaQA and SearchQA, offline simulation is ineffective regardless of the performance of initial models. This result echoes the learning challenges in the online counterparts on these two datasets.\nOnline vs. Offline Regret Table 2 compares online and offline regret. Regret numbers are averaged over the number of feedback examples.10 Online learning generally displays lower regret for similar initial models on SQUAD, HotpotQA, and NQ. This is expected because later interactions in the simulation can benefit from early feedback in online learning. In contrast, in our offline scenario, we only update after seeing all examples, so regret\n10Table 8 in Appendix B lists the percentage of positive feedback in online and offline in-domain simulation.\nnumbers depend on the initial model only. Regret results on NewsQA, TriviaQA, and SearchQA are counterintuitive, generally showing that online learning has similar or higher regret. The cases showing significantly higher online regret (64+sim on NewsQA and SearchQA) can be explained by the learning failing, which impacts online regret, but not our offline regret. The others are more complex, and we hypothesize that they may be because of combination of (a) inherent noise in the data;11 and (b) in cases where online learning is effective, the gap between the strictly-defined reward that is used to compute regret and the relaxed F1 evaluation metric. Further analysis is required for a more conclusive conclusion."
    }, {
      "heading" : "7 Domain Adaptation",
      "text" : "Learning from user feedback creates a compelling avenue to deploy systems that target domains not addressed by existing datasets. The scenario we simulate in this section starts with training a QA model on a complete existing annotated dataset, and deploying it to interact with users and learn from their feedback in a new domain. We do not assume access to any annotated training data in the target domain. We report experiments with online learning. Offline adaptation experiments are discussed in Appendix B.3.\nFigure 4 shows online domain adaptation performance. On 22/30 configurations, online adaptation introduces significant performance gains (>2 F1 score). For example, adapting from TriviaQA and SearchQA to the other four domains improves performance by 27–72.8 F1. On HotpotQA, the model initially trained on TriviaQA shows an impressive adaptation, improving from 0.2 F1 to 73 F1.12\nOur simulations show reduced effectiveness when the target domain is either TriviaQA or SearchQA, likely because the simulated feedback is based on noisy distantly supervised data. For SearchQA, the low performance of initial models from other domains may also contribute to the adaptation failure. As expected, this indicates the effectiveness of the process depends on the relation between the source and target domains. SearchQA seems farthest from the other domains, mirroring observations from prior work (Su et al., 2019).\n11TriviaQA and SearchQA use distant supervision for data collection. While NewsQA is crowdsourced, Trischler et al. (2017) report relatively low human performance.\n12We replicate this result with different model initializations to confirm it is not random.\nFigure 5 shows learning curves for our simulation experiments. Generally, we observe the choice of source and target domains influences adaptation rates. Models quickly adapt to SQUAD, HotpotQA, and NQ, reaching near final performance with a quarter of the total feedback provided. On NewsQA, models initially trained on TriviaQA and SearchQA adapt slower than those initially trained on other three datasets. On TriviaQA, we observe little change in performance throughout simulation. On SearchQA, only the model initially trained on TriviaQA shows a performance gain. Both SearchQA and TriviaQA include context para-\ngraphs from the web, potentially making domain adaptation from one to the other easier.\nLastly, we compare bandit learning with initial models trained on a small amount of in-domain data (Section 5) and initial models trained on a large amount of out-of-domain data. Table 3 compares online learning with initial models trained on 1,024 in-domain supervised examples and online domain adaptation with a SQUAD-initialized model. SQUAD initialization provides a robust starting point for all datasets except SearchQA. On four out of five datasets, the final performance is better with SQUAD-initialized model. This is po-\ntentially because the model is exposed to different signals from two datasets and overall sees more data, either as supervised examples or through feedback. However, on SearchQA, learning with SQUAD-initialized model performs much worse than learning with the initial model trained on 1,024 in-domain examples, potentially because of the gap in initial model performance (23.5 vs. 65 F1)."
    }, {
      "heading" : "8 Related Work",
      "text" : "Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Sokolov et al., 2017; Kreutzer et al., 2018a,b; Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021). Human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b; Mendoncca et al., 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al., 2020), and dialogue (Jaques et al., 2020). Nguyen et al. (2017) simulates bandit feedback to improve an MT system fully trained on a large annotated dataset, including analyzing robustness to feedback perturbations. Our work shows that simulated bandit feedback is an effective learning signal for extractive question answering tasks. Our work differs in focus on reducing annotation costs by relying on few annotated examples only to train the initial model, or by eliminating the need for in-domain annotation completely by relying on data in other domains to train initial models.\nAlternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020; Khashabi et al., 2020a). Kratzwald et al. (2020) resembles our setting in that it seeks binary feed-\nback to replace span annotation, but their goal is to create supervised data more economically. Domain adaptation for QA has been studied in prior work (Fisch et al., 2019; Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training (Lee et al., 2019), contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021), and exploiting small lottery subnetworks (Zhu et al., 2021)."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We present a simulation study of learning from user feedback for extractive QA. We formulate the problem as contextual bandit learning. We conduct experiments to show the effectiveness of such feedback, the robustness to feedback noise, the impact of initial model performance, the trade-offs between online and offline learning, and the potential for domain adaptation. Our study design emphasizes the potential for reducing annotation costs by annotating few examples or by utilizing existing datasets for new domains.\nWe intentionally adopt a basic setup, including a simple binary reward and vanilla learning algorithms, to illustrate what can be achieved with a relatively simple variant of the contextual bandit learning scenario. Our results already indicate the strong potential of learning from feedback, which more advanced methods are likely to further improve. For example, the balance between online and offline learning can be further explored using proximal policy optimization (PPO; Schulman et al., 2017) or replay memory (Mnih et al., 2015). With welldesigned interface, human users may be able to provide more sophisticated feedback (Lamm et al., 2021), which will provide a stronger signal compared to our binary reward.\nOur aim in this study is to lay the foundation for future work, by formalizing the setup and showing its potential. This is a critical step in enabling future research, especially going beyond simulation to study using real human feedback for QA systems. Another important direction for future work is studying user feedback for QA systems that do both context retrieval and answer generation (Lewis et al., 2020), where assigning the feedback to the appropriate stage in the process poses a challenge. Beyond extractive QA, we hope our work will inspire research of user feedback as a signal to improve other types of NLP systems."
    }, {
      "heading" : "A Additional Discussion",
      "text" : "Reward Function Intuitively, partial credit reward may improve learning over binary rewards. We experiment with using F1 score of the predicted answer span as a more refined feedback.13 In practice, using F1 as feedback does not introduce stronger learning signals, potentially because the distribution over F1 scores is bimodal on extreme values: around 85 % F1 scores are either 0 or 1 for predicted spans from a SQUAD-trained model on 8% NQ training data. We observe similar trends on all six datasets across all setups. Experiments with BLEU score (Papineni et al., 2002) as feedback show similar conclusion and distribution to F1 score.\nPerturbation In practice, noise in feedback is likely to be more systematic than the statistical simplification which defines noise as the random percentage of wrong feedback. For example, prior work (Nguyen et al., 2017) on bandit neural machine translation (NMT) proposes that noisy human feedback is granular, high-variance, and skewed, which can be approximated by mathematical functions and shows to significantly impact the bandit NMT learning. We experiment with the three perturbation functions from Nguyen et al. (2017) on F1 reward. Our experiments show that the effect of adding these perturbation functions is negligible. We hypothesize that the reward distribution for NMT is likely to be closer to a normal distribution, rather than a bimodal one like QA."
    }, {
      "heading" : "B Additional Experiments",
      "text" : "B.1 Method of Sampling While arg max can bias towards exploitation, sampling can encourage more exploration. We experiment with prediction via arg max and sampling\n13We set the reward as -0.1 if receiving a 0 F1 score. In general, updating with negative rewards consistently shows a slightly higher performance across different setups for both binary and F1 reward.\nfrom the output distribution over spans. Table 4 shows that arg max performs better than random sampling on three datasets. This set of experiments is conducted with batch size 80.\nB.2 Sensitivity Analysis Table 5 shows the sensitivity analysis results for online in-domain simulation on HotpotQA and TriviaQA. We experiment with five initial models trained on different sets of 64 or 1,024 supervised examples, each used to initiate a separate simulation experiment. For weaker initial models trained on 64 supervised examples, four out of five experiments on HotpotQA show performance gains similar to our main results, except one experiment that starts with a very low initialization performance. Nearly all experiments on TriviaQA collapse (mean F1 of 7.3). Our sensitivity analysis with stronger initial models trained on 1,024 examples shows that the final performance is stable across runs on both HotpotQA and TriviaQA (standard deviations are 0.5 and 2.6).\nB.3 Offline Adaptation We perform domain adaptation with offline learning, and compare its performance with online adaptation. Table 6 shows the performance gain of offline adaptation simulation compared to the online setup. In most settings, online learning proves to be more effective, possibly because it observes feedback from partially adapted model predictions. In a few settings (4/30), we observe better adaptation with offline settings (+1.1 to +4.6). Overall, we observe that online learning is more effective on domain adaptation, while offline adaption performs slightly better when both domains are related (e.g., same source domain)."
    } ],
    "references" : [ {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire." ],
      "venue" : "SIAM Journal on Computing, 32(1):48–77.",
      "citeRegEx" : "Auer et al\\.,? 2002",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "A contextual bandit bake-off",
      "author" : [ "Alberto Bietti", "Alekh Agarwal", "John Langford." ],
      "venue" : "JMLR.",
      "citeRegEx" : "Bietti et al\\.,? 2018",
      "shortCiteRegEx" : "Bietti et al\\.",
      "year" : 2018
    }, {
      "title" : "Quac: Question answering in context",
      "author" : [ "Eunsol Choi", "He He", "Mohit Iyyer", "Mark Yatskar", "Wen tau Yih", "Yejin Choi", "Percy Liang", "Luke Zettlemoyer." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Benefits of intermediate annotations in reading comprehension",
      "author" : [ "Dheeru Dua", "Sameer Singh", "Matt Gardner." ],
      "venue" : "ACL.",
      "citeRegEx" : "Dua et al\\.,? 2020",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2020
    }, {
      "title" : "Searchqa: A new q&a dataset augmented with context from a search engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V. Ugur Güney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "Feedback attribution for counterfactual bandit learning in multidomain spoken language understanding",
      "author" : [ "Tobias Falke", "Patrick Lehnen." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Falke and Lehnen.,? 2021",
      "shortCiteRegEx" : "Falke and Lehnen.",
      "year" : 2021
    }, {
      "title" : "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
      "author" : [ "Adam Fisch", "Alon Talmor", "Robin Jia", "Minjoon Seo", "Eunsol Choi", "Danqi Chen." ],
      "venue" : "MRQA@EMNLP.",
      "citeRegEx" : "Fisch et al\\.,? 2019",
      "shortCiteRegEx" : "Fisch et al\\.",
      "year" : 2019
    }, {
      "title" : "Using question answering rewards to improve abstractive summarization",
      "author" : [ "Chulaka Gunasekara", "Guy Feigenblat", "Benjamin Sznajder", "Ranit Aharonov", "Sachindra Joshi." ],
      "venue" : "Findings@EMNLP.",
      "citeRegEx" : "Gunasekara et al\\.,? 2021",
      "shortCiteRegEx" : "Gunasekara et al\\.",
      "year" : 2021
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "Daniel G. Horvitz", "D.J. Thompson." ],
      "venue" : "Journal of the American Statistical Association.",
      "citeRegEx" : "Horvitz and Thompson.,? 1952",
      "shortCiteRegEx" : "Horvitz and Thompson.",
      "year" : 1952
    }, {
      "title" : "Human-centric dialog training via offline reinforcement learning",
      "author" : [ "Natasha Jaques", "Judy Hanwen Shen", "Asma Ghandeharioun", "Craig Ferguson", "Agata Lapedriza", "Noah Jones", "Shixiang Gu", "Rosalind Picard." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Jaques et al\\.,? 2020",
      "shortCiteRegEx" : "Jaques et al\\.",
      "year" : 2020
    }, {
      "title" : "Spanbert: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "TACL.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer." ],
      "venue" : "ACL.",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "More bang for your buck: Natural perturbation for robust question answering",
      "author" : [ "Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Khashabi et al\\.,? 2020a",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "Unifiedqa: Crossing format boundaries with a single qa system",
      "author" : [ "Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter E. Clark", "Hannaneh Hajishirzi." ],
      "venue" : "Findings@EMNLP.",
      "citeRegEx" : "Khashabi et al\\.,? 2020b",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning a cost-effective annotation policy for question answering",
      "author" : [ "Bernhard Kratzwald", "Stefan Feuerriegel", "Huan Sun." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kratzwald et al\\.,? 2020",
      "shortCiteRegEx" : "Kratzwald et al\\.",
      "year" : 2020
    }, {
      "title" : "2018a. Can neural machine translation be improved with user feedback? NAACL",
      "author" : [ "Julia Kreutzer", "Shahram Khadivi", "Evgeny Matusov", "Stefan Riezler" ],
      "venue" : null,
      "citeRegEx" : "Kreutzer et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kreutzer et al\\.",
      "year" : 2018
    }, {
      "title" : "Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning",
      "author" : [ "Julia Kreutzer", "Joshua Uyheng", "Stefan Riezler." ],
      "venue" : "ACL.",
      "citeRegEx" : "Kreutzer et al\\.,? 2018b",
      "shortCiteRegEx" : "Kreutzer et al\\.",
      "year" : 2018
    }, {
      "title" : "Back-training excels selftraining at unsupervised domain adaptation of question generation and passage retrieval",
      "author" : [ "Devang Kulshreshtha", "Robert Belfer", "Iulian Serban", "Siva Reddy." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kulshreshtha et al\\.,? 2021",
      "shortCiteRegEx" : "Kulshreshtha et al\\.",
      "year" : 2021
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc V. Le", "Slav Petrov." ],
      "venue" : "TACL.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Qed: A framework and dataset for explanations in question answering",
      "author" : [ "Matthew Lamm", "Jennimaria Palomaki", "Chris Alberti", "Daniel Andor", "Eunsol Choi", "Livio Baldini Soares", "Michael Collins." ],
      "venue" : "TACL.",
      "citeRegEx" : "Lamm et al\\.,? 2021",
      "shortCiteRegEx" : "Lamm et al\\.",
      "year" : 2021
    }, {
      "title" : "The epochgreedy algorithm for contextual multi-armed bandits",
      "author" : [ "John Langford", "Tong Zhang." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Langford and Zhang.,? 2007",
      "shortCiteRegEx" : "Langford and Zhang.",
      "year" : 2007
    }, {
      "title" : "Improving a neural semantic parser by counterfactual learning from human bandit feedback",
      "author" : [ "Carolin Lawrence", "Stefan Riezler." ],
      "venue" : "ACL.",
      "citeRegEx" : "Lawrence and Riezler.,? 2018",
      "shortCiteRegEx" : "Lawrence and Riezler.",
      "year" : 2018
    }, {
      "title" : "Domain-agnostic question-answering with adversarial training",
      "author" : [ "Seanie Lee", "Donggyu Kim", "Jangwon Park." ],
      "venue" : "MRQA@EMNLP.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieval-augmented generation",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Kuttler", "Mike Lewis", "Wen tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "L’ucia Santos INESC-ID Lisboa, Instituto Superior T’ecnico, AI Unbabel",
      "author" : [ "Vania Mendoncca", "Ricardo Rei", "Luísa Coheur", "A. Sardinha", "Ana" ],
      "venue" : "Centro de Lingu’istica da Universidade de Lisboa, and Faculdade de Ciencias de Universidade de Lisboa",
      "citeRegEx" : "Mendoncca et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Mendoncca et al\\.",
      "year" : 2021
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski." ],
      "venue" : "Nature,",
      "citeRegEx" : "Mnih et al\\.,? 2015",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning for bandit neural machine translation with simulated human feedback",
      "author" : [ "Khanh Nguyen", "Hal Daumé", "Jordan L. BoydGraber." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Nguyen et al\\.,? 2017",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Few-shot question answering by pretraining span selection",
      "author" : [ "Ori Ram", "Yuval Kirstain", "Jonathan Berant", "Amir Globerson", "Omer Levy." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ram et al\\.,? 2021",
      "shortCiteRegEx" : "Ram et al\\.",
      "year" : 2021
    }, {
      "title" : "Proximal policy optimization algorithms",
      "author" : [ "John Schulman", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Schulman et al\\.,? 2017",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2017
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Seo et al\\.,? 2017",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning structured predictors from bandit feedback for interactive nlp",
      "author" : [ "Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sokolov et al\\.,? 2016",
      "shortCiteRegEx" : "Sokolov et al\\.",
      "year" : 2016
    }, {
      "title" : "A shared task on bandit learning for machine translation",
      "author" : [ "Artem Sokolov", "Julia Kreutzer", "Kellen Sunderland", "Pavel Danchenko", "Witold Szymaniak", "Hagen Fürstenau", "Stefan Riezler." ],
      "venue" : "WMT.",
      "citeRegEx" : "Sokolov et al\\.,? 2017",
      "shortCiteRegEx" : "Sokolov et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to summarize from human feedback",
      "author" : [ "Nisan Stiennon", "Long Ouyang", "Jeff Wu", "Daniel M. Ziegler", "Ryan J. Lowe", "Chelsea Voss", "Alec Radford", "Dario Amodei", "Paul Christiano." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Stiennon et al\\.,? 2020",
      "shortCiteRegEx" : "Stiennon et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalizing question answering system with pre-trained language model fine-tuning",
      "author" : [ "Dan Su", "Yan Xu", "Genta Indra Winata", "Peng Xu", "HyeonJin Kim", "Zihan Liu", "Pascale Fung." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Sutton and Barto.,? 1998",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Newsqa: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "Rep4NLP@ACL.",
      "citeRegEx" : "Trischler et al\\.,? 2017",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams." ],
      "venue" : "Machine Learning.",
      "citeRegEx" : "Williams.,? 2004",
      "shortCiteRegEx" : "Williams.",
      "year" : 2004
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "An imitation game for learning semantic parsers from user interaction",
      "author" : [ "Ziyu Yao", "Yiqi Tang", "Wen-tau Yih", "Huan Sun", "Yu Su." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yao et al\\.,? 2020",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2020
    }, {
      "title" : "Qanet: Combining local convolution with global self-attention for reading comprehension",
      "author" : [ "Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Contrastive domain adaptation for question answering using limited text corpora",
      "author" : [ "Zhenrui Yue", "Bernhard Kratzwald", "Stefan Feuerriegel." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yue et al\\.,? 2021",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2021
    }, {
      "title" : "SituatedQA: Incorporating extra-linguistic contexts into QA",
      "author" : [ "Michael J.Q. Zhang", "Eunsol Choi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhang and Choi.,? 2021",
      "shortCiteRegEx" : "Zhang and Choi.",
      "year" : 2021
    }, {
      "title" : "Revisiting fewsample bert fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q. Weinberger", "Yoav. Artzi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Less is more: Domain adaptation with lottery ticket for reading comprehension",
      "author" : [ "Haichao Zhu", "Zekun Wang", "Heng Zhang", "Ming Liu", "Sendong Zhao", "Bing Qin." ],
      "venue" : "Findings@EMNLP.",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    }, {
      "title" : "Our experiments show that the effect of adding these perturbation functions is negligible. We hypothesize that the reward distribution for NMT is likely to be closer to a normal distribution",
      "author" : [ "Nguyen" ],
      "venue" : null,
      "citeRegEx" : "Nguyen,? \\Q2017\\E",
      "shortCiteRegEx" : "Nguyen",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 40,
      "context" : ", 2016), task design (Yang et al., 2018; Choi et al., 2018), and model architecture development (Seo et al.",
      "startOffset" : 21,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : ", 2016), task design (Yang et al., 2018; Choi et al., 2018), and model architecture development (Seo et al.",
      "startOffset" : 21,
      "endOffset" : 59
    }, {
      "referenceID" : 31,
      "context" : ", 2018), and model architecture development (Seo et al., 2017; Yu et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 79
    }, {
      "referenceID" : 42,
      "context" : ", 2018), and model architecture development (Seo et al., 2017; Yu et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "1 This allows for simple binary feedback, and creates a contextual bandit learning scenario (Auer et al., 2002; Langford and Zhang, 2007).",
      "startOffset" : 92,
      "endOffset" : 137
    }, {
      "referenceID" : 20,
      "context" : "1 This allows for simple binary feedback, and creates a contextual bandit learning scenario (Auer et al., 2002; Langford and Zhang, 2007).",
      "startOffset" : 92,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : "For example, sharing question-annotator and answer-annotator roles (Rajpurkar et al., 2016), which is detrimental to emulate information seeking behavior (Choi et al.",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : ", 2016), which is detrimental to emulate information seeking behavior (Choi et al., 2018).",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 44,
      "context" : "Finally, it gives systems the potential to evolve over time as the world changes (Lazaridou et al., 2021; Zhang and Choi, 2021).",
      "startOffset" : 81,
      "endOffset" : 127
    }, {
      "referenceID" : 28,
      "context" : "For example, an initial system trained with a small amount of SQUAD (Rajpurkar et al., 2016) annotations (64 examples) improves from 18 to 81.",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "6 F1 score, and adapting a SearchQA (Dunn et al., 2017) system to SQUAD through user feedback improves it from 45 to 84 F1 score.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 36,
      "context" : "Our learner is similar to REINFORCE (Sutton and Barto, 1998; Williams, 2004), but we use arg max to predict answers instead of Monte Carlo sampling from the model’s output distribution.",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 38,
      "context" : "Our learner is similar to REINFORCE (Sutton and Barto, 1998; Williams, 2004), but we use arg max to predict answers instead of Monte Carlo sampling from the model’s output distribution.",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "The reward is re-weighted to provide an unbiased estimation using inverse propensity score (IPS; Horvitz and Thompson, 1952).",
      "startOffset" : 91,
      "endOffset" : 124
    }, {
      "referenceID" : 28,
      "context" : "Evaluating Performance We evaluate model performance using token-level F1 on a held-out test set, as commonly done in the QA literature (Rajpurkar et al., 2016).",
      "startOffset" : 136,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "4 Computing (4)Equivalently, the problem is often formulated as loss minimization (Bietti et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "Data We use six QA datasets that provide substantial amount of annotated training data taken from the MRQA training portion (Fisch et al., 2019): SQUAD (Rajpurkar et al.",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : ", 2019): SQUAD (Rajpurkar et al., 2016), NewsQA (Trischler et al.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 37,
      "context" : ", 2016), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : ", 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : ", 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 40,
      "context" : ", 2017), HotpotQA (Yang et al., 2018), and NaturalQuestions (NQ; Kwiatkowski et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : "We compute performance measures and learning curves on development sets following prior work (Rajpurkar et al., 2016; Ram et al., 2021).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "We compute performance measures and learning curves on development sets following prior work (Rajpurkar et al., 2016; Ram et al., 2021).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "Model We conduct experiments with a pretrained SpanBERT model (Joshi et al., 2020), which shows substantially better performance on QA tasks in contrast to comparable models.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 39,
      "context" : "Implementation Details We use Hugging Face Transformers (Wolf et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 29,
      "context" : "We obtain the sets of 64, 256, or 1024 examples from prior work (Ram et al., 2021).",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 45,
      "context" : "Sensitivity Analysis Training Transformerbased models has been shown to have stability issues, especially when training with limited amount of data (Zhang et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 168
    }, {
      "referenceID" : 35,
      "context" : "SearchQA seems farthest from the other domains, mirroring observations from prior work (Su et al., 2019).",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 33,
      "context" : "Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Sokolov et al., 2017; Kreutzer et al., 2018a,b; Mendoncca et al., 2021), structured prediction (Sokolov et al.",
      "startOffset" : 99,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : "Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Sokolov et al., 2017; Kreutzer et al., 2018a,b; Mendoncca et al., 2021), structured prediction (Sokolov et al.",
      "startOffset" : 99,
      "endOffset" : 176
    }, {
      "referenceID" : 32,
      "context" : ", 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : ", 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al.",
      "startOffset" : 26,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : ", 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al.",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : ", 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021).",
      "startOffset" : 119,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "Human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b; Mendoncca et al., 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al.",
      "startOffset" : 68,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "Human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b; Mendoncca et al., 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al.",
      "startOffset" : 68,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : ", 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al.",
      "startOffset" : 26,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : ", 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al., 2020), and dialogue (Jaques et al.",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Alternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020; Khashabi et al., 2020a).",
      "startOffset" : 128,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "Alternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020; Khashabi et al., 2020a).",
      "startOffset" : 128,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Domain adaptation for QA has been studied in prior work (Fisch et al., 2019; Khashabi et al., 2020b), including using data augmentation (Yue et al.",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "Domain adaptation for QA has been studied in prior work (Fisch et al., 2019; Khashabi et al., 2020b), including using data augmentation (Yue et al.",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 43,
      "context" : ", 2020b), including using data augmentation (Yue et al., 2021), adversarial training (Lee et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : ", 2021), adversarial training (Lee et al., 2019), contrastive method (Yue et al.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 43,
      "context" : ", 2019), contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : ", 2021), back-training (Kulshreshtha et al., 2021), and exploiting small lottery subnetworks (Zhu et al.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 46,
      "context" : ", 2021), and exploiting small lottery subnetworks (Zhu et al., 2021).",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "For example, the balance between online and offline learning can be further explored using proximal policy optimization (PPO; Schulman et al., 2017) or replay memory (Mnih et al.",
      "startOffset" : 120,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "With welldesigned interface, human users may be able to provide more sophisticated feedback (Lamm et al., 2021), which will provide a stronger signal compared to our binary reward.",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : "Another important direction for future work is studying user feedback for QA systems that do both context retrieval and answer generation (Lewis et al., 2020), where assigning the feedback to the appropriate stage in the process poses a challenge.",
      "startOffset" : 138,
      "endOffset" : 158
    } ],
    "year" : 0,
    "abstractText" : "We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback.",
    "creator" : null
  }
}
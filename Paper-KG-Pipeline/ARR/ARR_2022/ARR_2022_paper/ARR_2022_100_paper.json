{
  "name" : "ARR_2022_100_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models (PTLMs) such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2020) have shown impressive results in various conventional natural language understanding (NLU) tasks by capturing syntactic and semantic knowledge from the pretraining tasks of masked language modeling and masked span infilling tasks on massive text corpora.\nThough yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020; Petroni et al., 2021; Schick and Schütze, 2020). Specifically, one type of knowledge that models often struggle with is the visual knowledge of common objects such as attributes (e.g. appearance, measurable quantity) and affordances. This is because this kind of knowledge is rarely explicitly described in the training\n1Code and data have been uploaded and will be published."
    }, {
      "heading" : "1. Orange is a shape of circle.",
      "text" : "text due to reporting bias. For example, as shown in Figure 1, people tend to report what interests them rather than general facts such as a shape or color of oranges they already know.\nTowards better knowledge-enhanced PTLMs, recent works incorporate external knowledge bases (e.g., knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al., 2019; Wang et al., 2020). However, these approaches still suffer from a lack of visual knowledge that is important to understand the real world.\nIn this paper, we conduct systematic experiments to understand whether such visual knowledge can be transferred into LMs, and if so, how to perform effective knowledge transfer. Specifically, we look into a series of analysis question as follows: (1) Can intermediate pre-training (Pruksachatkun et al., 2020) on image-caption pairs help transfer the knowledge? (2) What types of knowledge sources are more helpful? To answer questions, we explore various intermediate pre-training tasks (Pruksachatkun et al., 2020) on two different sources: text-only (text knowledge transfer from visual domains) and image-caption pairs (crossmodal knowledge transfer).\nFor the text knowledge transfer, we utilize text corpus from visual domain, e.g., image captions. We leverage two training objectives for the lan-\nguage model: (1) masked language modeling follows the domain adaptive pre-training scheme (Gururangan et al., 2020), assuming the corpus contains enriched visual knowledge or physical commonsense knowledge; (2) text contrastive learning augments the sentence representation with dropout to create positive samples while considering all others in the batch as negative samples for the contrastive learning (Gao et al., 2021), assuming training better sentence representations leads to better understanding of the corpus.\nFor the cross-modal knowledge transfer, we explore multiple methods to transfer visual-related knowledge to LMs: (1) masked language modeling with visual clues incorporates visual clues to capture dependencies between visual and linguistic contents (Su et al., 2019); (2) voken classification contextually aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs (Tan and Bansal, 2020); (3) cross-modal contrastive learning aims to improve text representations by maximizing the agreement between correct image-text pairs versus random (inbatch) and adversarial negative pairs by contrastive learning between image and text modalities; and (4) cross-modal knowledge distillation transfers the knowledge from the teacher model, which is trained by cross-modal contrastive learning on image and text modalities, to the student language model using knowledge distillation.\nWe perform comprehensive comparisons on\nfive downstream tasks that may require visual or physical commonsense knowledge, including PIQA (Bisk et al., 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al., 2018), OBQA (Mihaylov et al., 2018), and RiddleSense (Lin et al., 2021). Results suggest that: (1) Simple intermediate pre-training on captions can help improving performance on commonsense reasoning that needs physical or visual knowledge. (2) Cross-modal knowledge transfer approaches consistently improve the performance in a large margin when only few train examples are available. (3) Cross-modal contrastive learning shows that it is best for packaging visual knowledge into LMs."
    }, {
      "heading" : "2 Analysis Setup",
      "text" : "In this work, we study how to transfer the visual knowledge into language models. For this study, we introduce our analysis setup: problem formulation, analysis questions, and knowledge corpora."
    }, {
      "heading" : "2.1 Problem Formulation",
      "text" : "We focus on a pre-trained text encoder fL and an image encoder fV if images are available. fL and fV are initialized with pre-trained model and we continue to pre-train the models on different sources and tasks, which we call intermediate pretraining. After the intermediate pre-training, we fine-tune fL on downstream NLU tasks. Existing NLU benchmarks have been trained against standard supervised learning paradigms that typi-\ncally require a large number of question answering examples which need a large annotation efforts. However, in scenarios where the number of labeled examples is small, the model tends to overfit the training examples and shows poor generalization performance on test set. Here, we evaluate the intermediate pre-training objective’s generalization ability on test set in both fully supervised and lowresource settings."
    }, {
      "heading" : "2.2 Analysis Questions",
      "text" : "In this paper, we provide a comprehensive study for transferring the visual knowledge into LMs. Visual knowledge transfer can be done in two approaches, depending on the source to be trained: (1) Text knowledge transfer using the text corpus in the visual domain, e.g., image captions and (2) cross-modal knowledge transfer which passes visual knowledge about common objects to LMs by training over paired image and captions. By evaluating the model on 5 downstream datasets that require physical and visual commonsense knowledge, we explore following three research questions."
    }, {
      "heading" : "Q1: Can intermediate pre-training on external",
      "text" : "knowledge sources help transfer visual knowledge to augment text encoders? We investigate diverse intermediate pre-training methods with external knowledge sources including caption data to inject visual information from images and captions into LMs. We first analyze the performance of text and cross-modal knowledge transfer methods with a image-caption dataset, and we additionally study text knowledge transfer methods with other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al., 2015a).\nQ2: What types of knowledge sources are more helpful for visual knowledge transfer? As mentioned above, we have two categories to exploit visual information: (1) text knowledge transfer and (2) cross-modal knowledge transfer. Here, we explore which type of knowledge transfer is more useful to transfer the visual knowledge into LMs."
    }, {
      "heading" : "Q3: What intermediate pre-training objectives",
      "text" : "are effective for cross-modal knowledge transfer? We present three pre-training objectives for cross-modal knowledge transfer: (1) voken classification, (2) contrastive learning, and (3) knowledge distillation. Here, we want to present which strategy is best suited for cross-modal knowledge transfer. Furthermore, we study how to enhance\ncross-modal contrastive learning with adversarial negative samplings."
    }, {
      "heading" : "2.3 Pre-training Data",
      "text" : "To transfer the visual knowledge, we collect 250K image-caption pairs from MS COCO (Lin et al., 2014; Chen et al., 2015). MS COCO is a large scale dataset that contains images reflecting the composition of actual everyday scenes and corresponding captions which describe contextual reasoning between objects in the scene. We only use captions for text knowledge transfer while we use both images and captions for cross-modal knowledge transfer. As an ablation study, we explore other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al., 2015a)."
    }, {
      "heading" : "2.4 Downstream Tasks and Datasets",
      "text" : "For downstream benchmarks, we find tasks that can benefit from visual knowledge: multiple choice question answering tasks including PIQA (Bisk et al., 2020) which requires physical commonsense reasoning, CSQA (Talmor et al., 2018) for general understanding of commonsense reasoning, OBQA (Mihaylov et al., 2018) that needs elemenatry-level science knowledge, and RiddleSense (RS) (Lin et al., 2021) for complex understanding of figurative language, and binary classification task including Visual Paraphrasing (VP) (Lin and Parikh, 2015) that needs scene understanding. We use in-house test sets made from training sets for PIQA and CSQA since test set is not provided to public. We list the data statics in Table 1. Moreover, We additionally test on GLUE (Wang et al., 2018) to evaluate the general text understanding."
    }, {
      "heading" : "2.5 Evaluation Protocol",
      "text" : "We evaluate the models in both fully supervised and low-resource settings. For both settings, we consider accuracy for 5 different classification tasks\nand get average performance over tasks to check the final performance. In fully supervised setting, we evaluate models with 3 different random seeds and report the average accuracy. In a low-resource setting, we consider the size of train data to 64 or 128. For each experiment, we run over 5 different sub-samples and show the average accuracy."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we introduce the following two approaches to integrate visual knowledge into LMs: (1) text knowledge transfer; and (2) cross-modal knowledge transfer. Throughout this section, we assume the data is a collection of image-caption pairs { (xvi , x l i) }m i=1\nand image encoder fV and text encoder fL are given."
    }, {
      "heading" : "3.1 Text Knowledge Transfer",
      "text" : "For text knowledge transfer, we investigate following pre-training objectives: (1) masked language modeling; and (2) text contrastive learning.\nMasked Language Modeling (MLM) Following BERT (Devlin et al., 2018), we select 15% of input tokens and replace them with [MASK]. Of the selected tokens, 80% are replaced, 10% are not changed and 10% are replaced by random vocabulary token. Here, we employ dynamic masking, which performs random masking and replacement during training to prevent the same masking for the same examples (Liu et al., 2019). MLM objective is the cross-entropy loss for masked token predictions :\nℓMLM(x l i) = − log p(xli|xmasked), (1)\nwhere xi is the i-th token and xmasked is a mask.\nText Contrastive Learning (TCL) Contrastive learning aims to learn representations by pulling positive pairs closer and pushing negative pairs apart. Here, we employ the contrastive framework with cross-entropy objective and in-batch negatives (Chen et al., 2020a; Gao et al., 2021). Given a text encoder fL, and a caption xli, we first get text representations using the encoders hli = fL(x l i). Following Gao et al. (2021), we augment identical positive sample hl +\ni by different dropout representations. The contrastive loss is defined as follows:\nℓli = − log esim(h l i,h\nl+\ni )/τ∑N j=1 e sim(hli,h l+ i )/τ , (2)\nwhere N is a batch size and sim(·) represents cosine similarity, i.e., sim(u, v) = u · v/∥u∥∥v∥. τ represents a temperature parameter."
    }, {
      "heading" : "3.2 Cross-modal Knowledge Transfer",
      "text" : "Language models might learn additional information from visual sources such as images and captions. So we include a variety of vision-based approaches and investigate the approaches whether they can benefit from visual sources. We introduce vision-based approaches as follows.\nVoken Classification Vokenization (Tan and Bansal, 2020) employs token-level text-to-image retrieval to transfer visual knowledge. It aligns language tokens to their related images (called “vokens”) to transfer visual knowledge into LMs, and call “voken classification”. Given text x and a voken vi for the i-th token, the loss is defined as\nℓvokeni = − log(p(vi|x)). (3)\nSimilar to masked language modeling, it classifies each token to a corresponding voken. Vokenization trains language models with the voken classification task and MLM.\nMasked Language Modeling with Visual Clues VL-BERT (Su et al., 2019) adopts masked language modeling with visual clues in which models are given a caption with masked tokens and an image and predict the masked tokens using visual clues. VL-BERT is pre-trained on Conceptual Captions (Sharma et al., 2018) as an image-caption corpus, and BooksCorpus (Zhu et al., 2015b) and English Wikipedia as text-only corpora. It shows its effectiveness in many vision-language tasks. We investigate whether this model also succeed in NLP tasks and compare it with others.\nCross-modal Contrastive Learning (CMCL) To harness the visual knowledge from imagecaption datasets, we adopt contrastive loss on image and text vectors. Given an image encoder fV , a\ntext encoder fL, and an image-caption pair (xvi , x l i), we first get image and text representations using the encoders hvi = fV (x v i ), h l i = fL(x l i). Then the contrastive learning objective contains two loss functions: an image-to-text contrastive loss ℓ(v,l) and and a text-to-image contrastive loss ℓ(l,v). The image-to-text contrastive loss is defined as follows:\nℓ (v,l) i = − log\nesim(h v i ,h l i)/τ∑N\nj=1 e sim(hvi ,h l j)/τ\n, (4)\nwhere N is a batch size and sim(·) represents cosine similarity. This loss encourages a closer distance between representations of aligned imagecaption pairs than unaligned pairs given an image and multiple captions. Similarly, the text-to-image contrastive loss ℓ(l,v) is defined as follows:\nℓ (l,v) i = − log\nesim(h l i,h v i )/τ∑N\nj=1 e sim(hli,h v j )/τ\n. (5)\nThe final loss is defined as\nL = 1\nN N∑ i=1 (ℓ (v,l) i + ℓ (l,v) i ). (6)\nCLIP (Radford et al., 2021) and ConVIRT (Zhang et al., 2020) also adopt contrastive learning, but we freeze the image encoder in training and use the trained text encoder for downstream tasks.\nCMCL with Adversarial Negative Samples (ANS) As in-batch negatives in CMCL are not challenging enough for models to distinguish, we present adversarial negative sampling strategy to improve CMCL. Given an image-caption pair (xvi , x l i), we define a LM-perturbed sentence x l− i , which is a hard negative where n is replaced with a different word n′ from a probability distribution of PTLMs. We expect the l− is syntactically correct and plausible sentence even the word n is replaced to n′, while it does not semantically match to the corresponding image xvi . With such hard negative, we try to make more challenging task so that models can effectively learn from the task. For example, we choose a word ‘girl’ in the sentence ‘A girl puts an apple in her bag.’ in Figure 3. Then we mask the word with [MASK] token to do masked token predictions by PTLMs. Then we get topk predictions from language models and replace the masked tokens with one of the predicted ones. To avoid false negative sentences which may have the same semantics as the original sentence, we\nintroduce an additional filtering step: if the masked predictions are synonyms or hypernyms of the original tokens, we discard the predictions. We use WordNet (Miller, 1995) to find synonyms and hypernyms. The contrastive loss with hard negative is defined as follows:\n− log e sim(hvi ,h l i)/τ∑N\nj=1 e sim(hvi ,h l j)/τ + ∑M k=1 e sim(hvi ,h l− j )/τ ,\n(7) where M is the number of hard negative samples per positive pair. This formula is only for image-totext contrastive loss ℓ(v,l) and final loss is defined to same as equation (6).\nCMCL with Positive Sample Augmentation (PSA) In ANS, we filter perturbed sentences where the masked predictions are synonyms or hypernyms of the original tokens. Instead of excluding these perturbed sentences, another option is to include them as additional positive samples l+ to the paired images. We name this as positive sample augmentation (PSA). It also adopts LM-perturbed negative samples as in ANS.\nCross-modal Knowledge Distillation (CMKD) Cross-model knowledge distillation is to transfer knowledge between different modalities, e.g., image modality and text modality. In this category, CMKD is to transfer knowledge from a teacher model which is knowledgeable about visual information. VidLanKD (Tang et al., 2021) also utilizes a cross-modal knowledge distillation method to help with general language understanding. A teacher model is first trained using contrastive learning on a video-text dataset, and then it transfers its knowledge to a student language model using KD on a text corpus. Their contrastive learning loss (hinge loss) is defined as\nL = N∑ i [max(0, α−sim(hvi , hli)+sim(hv ′ i , h l i)) + max(0, α− sim(hvi , hli) + sim(hvi , hl ′ i ))], (8)\nwhere v′ and l′ are a random image and caption text, respectively. α is the margin between the similarities of a positive pair and a negative pair. Instead of video datasets, we use a MS COCO dataset to train a teacher model and use two versions of contrastive learning, equations (6) and (8).\nAs another version of CMKD, we consider distilling visual knowledge from a pre-trained vision-\nlanguage model, VL-BERT, which is knowledgeable about grounded language. We adopt masked language modeling on Wikitext103 (Merity et al., 2016), a subset of English Wikipedia, in the knowledge distillation step. For knowledge distillation, we adopt Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), which proves the effectiveness in VidLanKD (Tang et al., 2021)."
    }, {
      "heading" : "4 Experimental Settings",
      "text" : "For all the approaches, we use bert-base-uncased (Devlin et al., 2018) as text encoder fL and ResNeXt101 (Xie et al., 2017) as an image encoder fV . For text knowledge transfer, (1) MLM follows the exact setting of codebase in huggingface2 which uses dynamic masking strategy to conduct language modling task. (2) TCL conducts contrastive learning with fL. We choose the best checkpoint by the best spearman correlation on STSb (Cer et al., 2017). For cross-modal knowledge transfer, (1) CMKD explores VL-BERT, Vokenization, and VidLanKD approaches. Here, we use VL-BERT-large model to do CMKD. Vokenization uses a checkpoint from their official codebase3 and VidLanKD trains a teacher model by two versions of contrastive learning (equations (6) and (8)) on MS COCO dataset. We set α = 1 in VidLanKD (equation (8)). (2) CMCL conducts contrastive learning with fL and fV . Here, we set τ = 0.05 (equations (4) and (5)). (3) CMCL with ANS chooses three noun words or verb words to do masked prediction and\n2 https://github.com/huggingface/transformers/\ntree/master/examples/pytorch/language-modeling 3 https://github.com/airsplay/vokenization\nuse top-5 predictions from fL as replacement. We filter out synonyms and hypernyms of original words using WordNet (Miller, 1995). (4) CMCL with PSA includes the perturbed sentences with synonyms and hypernyms as additional positive samples. In CMCL, we adopt ResNeXt101 (Xie et al., 2017) as an image encoder fV and BERT as a text encoder fL. TCL and CMCL train with batch size 64, maximum sequence length 20, learning rate 1e-4 for 3 epochs. For fine-tuning on downstream tasks, we do grid search on learning rates {5e-5, 1e-4, 3e-4, 4e-4, 5e-4, 6e-4} and choose the best learning rate. We set maximum epochs to 30 in low-resource and 15 in fully supervised settings."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "We analyze the main results of intermediate pretraining. Tables 2 and 3 show the main results of low-resource learning and fully supervised learning with the MS COCO captioning dataset, respectively. We train the models with a few training examples,\n64 and 128, to understand the better initialization. We argue that if a model obtains better performance in the low-resource setup, then it is a faster learner and has better generalization on downstream tasks.\nCan text intermediate pre-training help improve text encoders? Text intermediate pre-training using MLM and TCL on a caption corpus improves the performance on downstream tasks in both lowresource and fully supervised settings. In particular, TCL shows significant improvement on OBQA and RiddleSense over BERT. These results suggest that text intermediate pre-training on visual-related datasets helps performance on commonsense reasoning tasks.\nCan cross-modal intermediate pre-training help transfer visual knowledge to augment text encoders? We observe that cross-modal intermediate pre-training is helpful in both fully supervised and low-resource settings (See Table 2 and 3). Specifically, CMKD with VidLanKD variant outperforms the baseline by 1.6% point on the PIQA dataset in fully supervised setting. CMCL also shows its effectiveness. However, we could find that it becomes more powerful when equipped with PSA and ANS. It suggests that data augmentation for positive and negative sampling is an important factor for CMCL. In low-resource setting, we find that cross-modal knowledge transfer helps better initialization and let models learn new tasks faster.\nWhat intermediate pre-training objectives are effective for cross-modal knowledge transfer? Among various cross-modal knowledge transfer methods, we study which method is the most effective for cross-modal knowledge transfer. Overall, CMCL with PSA and ANS shows the best performance among all cross-modal methods. Interestingly, VL-BERT also shows better performance\nthan BERT-base on all datasets in the low-resource setting. This suggesting that exploiting images in masked language modeling task help transfer the knowledge to language models.\nWhat types of knowledge sources are most helpful? Here, we investigate whether using an image source in addition to a text source can further improve the model. To answer this question, we analyze methods from different types of sources: text-only and text-image pair sources. We focus on the methods that use the contrastive learning objective: TCL and CMCL. Note that these two methods share the same objective but CMCL trains on cross modalities which are images and captions while TCL only trains on captions Overall, TCL performs slightly better than CMCL in low-resource and fully supervised settings. Interestingly, additional negative samples (ANS) and positive samples in TCL decreases the performance while they help CMCL to improve the performance. We conjecture that perturbed sentences in ANS might not be semantically negative to the original sentence so models learn from wrong labels."
    }, {
      "heading" : "5.1 Ablation Study How do models perform on general NLU tasks?",
      "text" : "Table 4 presents results on GLUE benchmark. In GLUE, text intermediate pre-training methods slightly underperform the original BERT-base. We conjecture that the intermediate pre-training on caption data might sacrifice knowledge of general language understanding.\nAnalysis on diverse text corpora Table 5 represents text approaches with different pre-training corpora: MS COCO captions (Lin et al., 2014; Chen et al., 2015), GenericsKB (Bhakthavatsalam et al., 2020), BooksCorpus (Zhu et al., 2015a), and WikiText103 (Merity et al., 2016). We notice that caption datasets are useful on OBQA and RiddleSense datasets while GenericsKB are the most helpful on PIQA datasets. Results are expected since GenericsKB contains a lot of everyday statements that contain various types of commonsense.\nDifferent training sizes. We test different training sizes on PIQA in Fig. 4. In the experiment, we observe that CMCL consistently outperforms BERT on all training sizes. Additional negative sample (ANS) improves the CMCL on different training sizes, and positive sample augmentation boosts the performance of CMCL further. This sug-\ngests including perturbed sentences as positive and negative samples are useful to cross-modal knowledge transfer."
    }, {
      "heading" : "6 Related Work",
      "text" : "Text Knowledge enhanced methods. Recently, huge efforts on integrating knowledge into PTLMs have been made. One typical form of knowledge is a knowledge graph. There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019; Peters et al., 2019; He et al., 2020). Some other works try to retrieve or generate the sub-graph from the graph to solve the problem (Lin et al., 2019; Wang et al., 2020). Another existing form of knowledge is extra large-scale corpus. Works that use such corpus present knowledge-related pre-training objectives such as concept order recovering (Zhou et al., 2021), entity category prediction (Yu et al., 2020) and source of knowledge prediction (Wang et al., 2021). They are mostly focused on injecting world knowledge presented in text, rather than physical and visual commonsense knowledge that can be found in images.\nCross-modal knowledge enhanced methods. There is a extensive line of works for a variety of vision-language tasks, such as VL-BERT (Su et al., 2019), VisualBert (Li et al., 2019), and Uniter (Chen et al., 2020b). These models aim to improve vision-language tasks, e.g., VQA (Goyal et al., 2017), and they are found to be not effective in improving language tasks (Tan and Bansal, 2020). Another line of works is to transfer visual knowledge to language models: Vokenization (Tan and Bansal, 2020) and VidLanKD (Tang et al., 2021). Vokenization employs token-level text-toimage retrieval to transfer visual knowledge to language models. For this, Vokenization introduces 30k vokens and matches each token into the limited voken space; it may have approximation errors. VidLanKD adopts contrastive learning to train a teacher model on video datasets and uses distillation approaches to distill visual knowledge from the teacher to a student model."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We study whether intermediate pre-training on visual knowledge can help transfer visual knowledge into LMs. We investigate text knowledge transfer and cross-modal knowledge transfer using images and captions. In our empirical analysis, we observe that intermediate pre-training on captions can help improving performance and cross-modal knowledge transfer approaches consistently improve performance. When the transfer methods are equipped with additional positive and negative samples, they show better performance. Future works include improving both commonsense reasoning and general language understanding."
    }, {
      "heading" : "A Dataset Properties",
      "text" : "PIQA is a multiple-choice question answering task, which chooses the most appropriate solution for physical commonsense questions, which may need illustration or description of physical interaction in the real world. VP is to tell if two descriptions are describing the same scene or two different scenes. While they seem like purely textual tasks, they require visual common sense to answer. CSQA is a multiple-choice question answering task that requires commonsense reasoning to answer. It is built from ConceptNet (Speer et al., 2017). OBQA is a multiple-choice question answering task, which is modeled after open book exams on elementarylevel core science questions. The task generally requires open book fact but also additional commonsense which can be learnt from scientific illustration. RiddleSense is a multiple-choice riddlestyle question answering which requires complex commonsense reasoning ability and understanding of figurative language which may benefit from visual knowledge."
    } ],
    "references" : [ {
      "title" : "Genericskb: A knowledge base of generic statements",
      "author" : [ "Sumithra Bhakthavatsalam", "Chloe Anastasiades", "Peter E. Clark." ],
      "venue" : "ArXiv, abs/2005.00660.",
      "citeRegEx" : "Bhakthavatsalam et al\\.,? 2020",
      "shortCiteRegEx" : "Bhakthavatsalam et al\\.",
      "year" : 2020
    }, {
      "title" : "Piqa: Reasoning about physical commonsense in natural language",
      "author" : [ "Yonatan Bisk", "Rowan Zellers", "Jianfeng Gao", "Yejin Choi" ],
      "venue" : "In Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Bisk et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bisk et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "arXiv preprint arXiv:1504.00325.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Uniter: Universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "European conference on computer vision, pages 104–120. Springer.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2104.08821.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT-MK: Integrating graph contextualized knowledge into pretrained language models",
      "author" : [ "Bin He", "Di Zhou", "Jinghui Xiao", "Xin Jiang", "Qun Liu", "Nicholas Jing Yuan", "Tong Xu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020,",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Like what you like: Knowledge distill via neuron selectivity transfer",
      "author" : [ "Zehao Huang", "Naiyan Wang." ],
      "venue" : "arXiv preprint arXiv:1707.01219.",
      "citeRegEx" : "Huang and Wang.,? 2017",
      "shortCiteRegEx" : "Huang and Wang.",
      "year" : 2017
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "KagNet: Knowledge-aware graph networks for commonsense reasoning",
      "author" : [ "Bill Yuchen Lin", "Xinyue Chen", "Jamin Chen", "Xiang Ren." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge",
      "author" : [ "Bill Yuchen Lin", "Ziyi Wu", "Yichi Yang", "Dong-Ho Lee", "Xiang Ren." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP",
      "citeRegEx" : "Lin et al\\.,? 2021",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2021
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European conference on computer vision, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Don’t just listen, use your imagination: Leveraging visual common sense for non-visual tasks",
      "author" : [ "Xiao Lin", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2984–2993.",
      "citeRegEx" : "Lin and Parikh.,? 2015",
      "shortCiteRegEx" : "Lin and Parikh.",
      "year" : 2015
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1609.07843.",
      "citeRegEx" : "Merity et al\\.,? 2016",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2016
    }, {
      "title" : "Can a suit of armor conduct electricity? a new dataset for open book question answering",
      "author" : [ "Todor Mihaylov", "Peter Clark", "Tushar Khot", "Ashish Sabharwal." ],
      "venue" : "arXiv preprint arXiv:1809.02789.",
      "citeRegEx" : "Mihaylov et al\\.,? 2018",
      "shortCiteRegEx" : "Mihaylov et al\\.",
      "year" : 2018
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Knowledge enhanced contextual word",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Robert Logan", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Peters et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "KILT: a benchmark for knowledge",
      "author" : [ "Fabio Petroni", "Aleksandra Piktus", "Angela Fan", "Patrick Lewis", "Majid Yazdani", "Nicola De Cao", "James Thorne", "Yacine Jernite", "Vladimir Karpukhin", "Jean Maillard", "Vassilis Plachouras", "Tim Rocktäschel", "Sebastian Riedel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2021
    }, {
      "title" : "Intermediate-task transfer learning with pretrained language models: When and why does it",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning transferable visual models from natural language supervision",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "The ThirtyFourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applica-",
      "citeRegEx" : "Schick and Schütze.,? 2020",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Vl-bert: Pre-training of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "arXiv preprint arXiv:1908.08530.",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "arXiv preprint arXiv:1811.00937.",
      "citeRegEx" : "Talmor et al\\.,? 2018",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2018
    }, {
      "title" : "Vokenization: improving language understanding with contextualized, visual-grounded supervision",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv:2010.06775.",
      "citeRegEx" : "Tan and Bansal.,? 2020",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2020
    }, {
      "title" : "Vidlankd: Improving language understanding via video-distilled knowledge transfer",
      "author" : [ "Zineng Tang", "Jaemin Cho", "Hao Tan", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv:2107.02681.",
      "citeRegEx" : "Tang et al\\.,? 2021",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Connecting the dots: A knowledgeable path generator for commonsense question answering",
      "author" : [ "Peifeng Wang", "Nanyun Peng", "Filip Ilievski", "Pedro Szekely", "Xiang Ren." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
      "author" : [ "Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Jianshu Ji", "Guihong Cao", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "Findings of the Association for Computational",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Aggregated residual transformations for deep neural networks",
      "author" : [ "Saining Xie", "Ross Girshick", "Piotr Dollár", "Zhuowen Tu", "Kaiming He." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500.",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "Jaket: Joint pre-training of knowledge graph and language understanding",
      "author" : [ "Donghan Yu", "Chenguang Zhu", "Yiming Yang", "Michael Zeng." ],
      "venue" : "arXiv preprint arXiv:2010.00796.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dict-bert: Enhancing language model pre-training with dictionary",
      "author" : [ "Wenhao Yu", "Chenguang Zhu", "Yuwei Fang", "Donghan Yu", "Shuohang Wang", "Yichong Xu", "Michael Zeng", "Meng Jiang." ],
      "venue" : "arXiv preprint arXiv:2110.06490.",
      "citeRegEx" : "Yu et al\\.,? 2021",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2021
    }, {
      "title" : "Contrastive learning of medical visual representations from paired images and text",
      "author" : [ "Yuhao Zhang", "Hang Jiang", "Yasuhide Miura", "Christopher D Manning", "Curtis P Langlotz." ],
      "venue" : "arXiv preprint arXiv:2010.00747.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "ERNIE: Enhanced language representation with informative entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Pre-training text-to-text transformers for concept-centric common sense",
      "author" : [ "Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "The IEEE International Con-",
      "citeRegEx" : "Zhu et al\\.,? 2015a",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE in-",
      "citeRegEx" : "Zhu et al\\.,? 2015b",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Pre-trained language models (PTLMs) such as BERT (Devlin et al., 2018), RoBERTa (Liu et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : ", 2018), RoBERTa (Liu et al., 2019), and T5 (Raffel et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : ", 2019), and T5 (Raffel et al., 2020) have shown impressive results in various conventional natural language understanding (NLU) tasks by capturing syntactic and semantic knowledge from the pretraining tasks of masked language modeling and masked span infilling tasks on massive text corpora.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "Though yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020; Petroni et al., 2021; Schick and Schütze, 2020).",
      "startOffset" : 200,
      "endOffset" : 273
    }, {
      "referenceID" : 22,
      "context" : "Though yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020; Petroni et al., 2021; Schick and Schütze, 2020).",
      "startOffset" : 200,
      "endOffset" : 273
    }, {
      "referenceID" : 26,
      "context" : "Though yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020; Petroni et al., 2021; Schick and Schütze, 2020).",
      "startOffset" : 200,
      "endOffset" : 273
    }, {
      "referenceID" : 40,
      "context" : ", knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al.",
      "startOffset" : 69,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : ", knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al.",
      "startOffset" : 69,
      "endOffset" : 146
    }, {
      "referenceID" : 35,
      "context" : ", knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al.",
      "startOffset" : 69,
      "endOffset" : 146
    }, {
      "referenceID" : 38,
      "context" : ", knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al.",
      "startOffset" : 69,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : ", 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al., 2019; Wang et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 118
    }, {
      "referenceID" : 34,
      "context" : ", 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al., 2019; Wang et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "Specifically, we look into a series of analysis question as follows: (1) Can intermediate pre-training (Pruksachatkun et al., 2020) on image-caption pairs help transfer the knowledge? (2) What types of knowledge sources are more helpful? To answer questions, we explore various intermediate pre-training tasks (Pruksachatkun et al.",
      "startOffset" : 103,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : ", 2020) on image-caption pairs help transfer the knowledge? (2) What types of knowledge sources are more helpful? To answer questions, we explore various intermediate pre-training tasks (Pruksachatkun et al., 2020) on two different sources: text-only (text knowledge transfer from visual domains) and image-caption pairs (crossmodal knowledge transfer).",
      "startOffset" : 186,
      "endOffset" : 214
    }, {
      "referenceID" : 6,
      "context" : "(a) masked language model (Devlin et al., 2018) on image captions.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "guage model: (1) masked language modeling follows the domain adaptive pre-training scheme (Gururangan et al., 2020), assuming the corpus contains enriched visual knowledge or physical commonsense knowledge; (2) text contrastive learning",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "augments the sentence representation with dropout to create positive samples while considering all others in the batch as negative samples for the contrastive learning (Gao et al., 2021), assuming training better sentence representations leads to better understanding of the corpus.",
      "startOffset" : 168,
      "endOffset" : 186
    }, {
      "referenceID" : 29,
      "context" : "For the cross-modal knowledge transfer, we explore multiple methods to transfer visual-related knowledge to LMs: (1) masked language modeling with visual clues incorporates visual clues to capture dependencies between visual and linguistic contents (Su et al., 2019); (2) voken classification contextually aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs (Tan and Bansal, 2020); (3) cross-modal contrastive learning aims to improve text representations by maximizing the agreement between correct image-text pairs versus random (inbatch) and adversarial negative pairs by contrastive learning between image and text modalities; and (4) cross-modal knowledge distillation transfers the knowledge from the teacher model, which is trained by cross-modal contrastive learning on image and text modalities, to the student language model using knowledge distillation.",
      "startOffset" : 249,
      "endOffset" : 266
    }, {
      "referenceID" : 31,
      "context" : ", 2019); (2) voken classification contextually aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs (Tan and Bansal, 2020); (3) cross-modal contrastive learning aims to improve text representations by maximizing the agreement between correct image-text pairs versus random (inbatch) and adversarial negative pairs by contrastive learning between image and text modalities; and (4) cross-modal knowledge distillation transfers the knowledge from the teacher model, which is trained by cross-modal contrastive learning on image and text modalities, to the student language model using knowledge distillation.",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "We perform comprehensive comparisons on five downstream tasks that may require visual or physical commonsense knowledge, including PIQA (Bisk et al., 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al.",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : ", 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 30,
      "context" : ", 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al., 2018), OBQA (Mihaylov et al.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "a image-caption dataset, and we additionally study text knowledge transfer methods with other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al.",
      "startOffset" : 126,
      "endOffset" : 156
    }, {
      "referenceID" : 18,
      "context" : ", 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "To transfer the visual knowledge, we collect 250K image-caption pairs from MS COCO (Lin et al., 2014; Chen et al., 2015).",
      "startOffset" : 83,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "To transfer the visual knowledge, we collect 250K image-caption pairs from MS COCO (Lin et al., 2014; Chen et al., 2015).",
      "startOffset" : 83,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : ", 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "For downstream benchmarks, we find tasks that can benefit from visual knowledge: multiple choice question answering tasks including PIQA (Bisk et al., 2020) which requires physical commonsense reasoning, CSQA (Talmor et al.",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : ", 2020) which requires physical commonsense reasoning, CSQA (Talmor et al., 2018) for general understanding of commonsense reasoning, OBQA (Mihaylov et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : ", 2018) for general understanding of commonsense reasoning, OBQA (Mihaylov et al., 2018) that needs elemenatry-level science knowledge, and RiddleSense (RS) (Lin et al.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : ", 2018) that needs elemenatry-level science knowledge, and RiddleSense (RS) (Lin et al., 2021) for complex understanding of figurative language, and binary classification task including Visual Paraphrasing (VP) (Lin and Parikh, 2015) that needs scene understanding.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : ", 2021) for complex understanding of figurative language, and binary classification task including Visual Paraphrasing (VP) (Lin and Parikh, 2015) that needs scene understanding.",
      "startOffset" : 124,
      "endOffset" : 146
    }, {
      "referenceID" : 33,
      "context" : "Moreover, We additionally test on GLUE (Wang et al., 2018) to evaluate the general text understanding.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Masked Language Modeling (MLM) Following BERT (Devlin et al., 2018), we select 15% of input tokens and replace them with [MASK].",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "Here, we employ dynamic masking, which performs random masking and replacement during training to prevent the same masking for the same examples (Liu et al., 2019).",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "Here, we employ the contrastive framework with cross-entropy objective and in-batch negatives (Chen et al., 2020a; Gao et al., 2021).",
      "startOffset" : 94,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "Here, we employ the contrastive framework with cross-entropy objective and in-batch negatives (Chen et al., 2020a; Gao et al., 2021).",
      "startOffset" : 94,
      "endOffset" : 132
    }, {
      "referenceID" : 31,
      "context" : "Voken Classification Vokenization (Tan and Bansal, 2020) employs token-level text-to-image",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "Masked Language Modeling with Visual Clues VL-BERT (Su et al., 2019) adopts masked language modeling with visual clues in which models are given a caption with masked tokens and an image and predict the masked tokens using visual clues.",
      "startOffset" : 51,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "VL-BERT is pre-trained on Conceptual Captions (Sharma et al., 2018) as an image-caption corpus, and BooksCorpus (Zhu et al.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 43,
      "context" : ", 2018) as an image-caption corpus, and BooksCorpus (Zhu et al., 2015b) and English Wikipedia as text-only corpora.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : ", 2021) and ConVIRT (Zhang et al., 2020) also adopt contrastive learning, but we freeze the image encoder in training and use the trained text encoder for downstream tasks.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "We use WordNet (Miller, 1995) to find synonyms and hypernyms.",
      "startOffset" : 15,
      "endOffset" : 29
    }, {
      "referenceID" : 32,
      "context" : "VidLanKD (Tang et al., 2021) also utilizes a cross-modal knowledge distillation method to help with general language understanding.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "We adopt masked language modeling on Wikitext103 (Merity et al., 2016), a subset of English Wikipedia, in the knowledge distillation step.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "tillation, we adopt Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), which proves the effectiveness in VidLanKD (Tang et al.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 32,
      "context" : "tillation, we adopt Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), which proves the effectiveness in VidLanKD (Tang et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "For all the approaches, we use bert-base-uncased (Devlin et al., 2018) as text encoder fL and ResNeXt101 (Xie et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 36,
      "context" : ", 2018) as text encoder fL and ResNeXt101 (Xie et al., 2017) as an image encoder fV .",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "We choose the best checkpoint by the best spearman correlation on STSb (Cer et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "We filter out synonyms and hypernyms of original words using WordNet (Miller, 1995).",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 36,
      "context" : "In CMCL, we adopt ResNeXt101 (Xie et al., 2017) as an image encoder fV and BERT as a text encoder fL.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "Analysis on diverse text corpora Table 5 represents text approaches with different pre-training corpora: MS COCO captions (Lin et al., 2014; Chen et al., 2015), GenericsKB (Bhakthavatsalam et al.",
      "startOffset" : 122,
      "endOffset" : 159
    }, {
      "referenceID" : 4,
      "context" : "Analysis on diverse text corpora Table 5 represents text approaches with different pre-training corpora: MS COCO captions (Lin et al., 2014; Chen et al., 2015), GenericsKB (Bhakthavatsalam et al.",
      "startOffset" : 122,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : ", 2015), GenericsKB (Bhakthavatsalam et al., 2020), BooksCorpus (Zhu et al.",
      "startOffset" : 20,
      "endOffset" : 50
    }, {
      "referenceID" : 42,
      "context" : ", 2020), BooksCorpus (Zhu et al., 2015a), and WikiText103 (Merity et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 40,
      "context" : "There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019; Peters et al., 2019; He et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 210
    }, {
      "referenceID" : 21,
      "context" : "There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019; Peters et al., 2019; He et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 210
    }, {
      "referenceID" : 10,
      "context" : "There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019; Peters et al., 2019; He et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 210
    }, {
      "referenceID" : 13,
      "context" : "Some other works try to retrieve or generate the sub-graph from the graph to solve the problem (Lin et al., 2019; Wang et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 132
    }, {
      "referenceID" : 34,
      "context" : "Some other works try to retrieve or generate the sub-graph from the graph to solve the problem (Lin et al., 2019; Wang et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 132
    }, {
      "referenceID" : 41,
      "context" : "Works that use such corpus present knowledge-related pre-training objectives such as concept order recovering (Zhou et al., 2021), entity category prediction (Yu et al.",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 37,
      "context" : ", 2021), entity category prediction (Yu et al., 2020) and source of knowledge prediction (Wang et al.",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : ", 2020) and source of knowledge prediction (Wang et al., 2021).",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : "There is a extensive line of works for a variety of vision-language tasks, such as VL-BERT (Su et al., 2019), VisualBert (Li et al.",
      "startOffset" : 91,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : ", 2019), VisualBert (Li et al., 2019), and Uniter (Chen et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : ", VQA (Goyal et al., 2017), and they are found to be not effective in improving language tasks (Tan and Bansal, 2020).",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 31,
      "context" : ", 2017), and they are found to be not effective in improving language tasks (Tan and Bansal, 2020).",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 31,
      "context" : "Another line of works is to transfer visual knowledge to language models: Vokenization (Tan and Bansal, 2020) and VidLanKD (Tang et al.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "Another line of works is to transfer visual knowledge to language models: Vokenization (Tan and Bansal, 2020) and VidLanKD (Tang et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 142
    } ],
    "year" : 0,
    "abstractText" : "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias. In this work, we study whether integrating visual knowledge into a language model can fill the gap. We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives. On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives. Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings.1",
    "creator" : null
  }
}
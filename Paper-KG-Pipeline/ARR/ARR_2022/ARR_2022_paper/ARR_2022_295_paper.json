{
  "name" : "ARR_2022_295_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Large-scale historical records in Korea were mostly produced during the Joseon dynasty (1392-1897), and the Institute for the Translation of Korean Classics (ITKC) keeps a comprehensive database of Korean classics at a scale of approximately 9 billion characters. This digital archive is a great resource for Korean historians, but the documents remain in the original Hanja language. Hanja is an extinct language, and as Table 1 illustrates with a simple sentence, Hanja is lexically and syntactically different from modern Korean, as well as simplified and\n1All codes, models, and dataset are available at https: //anonymous.4open.science/r/HUE-ARR\ntraditional Chinese. Understanding the documents in the digital archive is thus difficult and would benefit greatly from a Hanja language model which can also be used to accelerate the expert translation (Vale de Gato, 2015). There are two corpora we can use to train the language model, the Annals of Joseon Dynasty (AJD), first introduced to the NLP community in (Bak and Oh, 2015), and the Diaries of the Royal Secretariats (DRS) (Kang et al., 2021).\nIn this paper, we provide the HUE (Hanja Understanding Evaluation) dataset consisting of king prediction, topic classification, named entity recognition and summary retrieval, a suite of tasks to help build and evaluate the Hanja language model. In addition to AJD and DRS, we also work with the Daily Records of the Royal Court and Important Officials (DRRI). Unlike AJD and DRS that have been analyzed by historians and contain their annotations, DRRI lacks such systematic analysis, and we use it for zero-shot learning and introduce it to the NLP community.\nWe also provide pretrained language models (PLMs) for Hanja trained on AJD and DRS, finetuned for each task in HUE. Our pretrained models on the corpora from that era outperform the existing language models built for ancient Chinese,\nconfirming the need for specially-trained Hanja language models. We also run additional experiments based on the analyses of entity- and word-level changes on AJD by controlling input conditions by masking named entity and giving the time period as input. Finally, we demonstrate the effectiveness of our Hanja language model for analyzing unseen documents, running zero-shot experiments for king prediction and named entity recognition tasks on DRRI.\nOur main contributions are as follows:\n• We release the HUE dataset and Hanja PLMs to support historians to understand and analyze a large volume of historical documents written in Hanja. To the best of our knowledge, this is the first work proposing Hanja language models and releasing a NLP benchmark dataset for ancient Hanja documents.\n• We demonstrate that providing key information such as named entity and document age as input improves the performance of Hanja language model on the HUE tasks.\n• We run zero-shot experiments on several HUE tasks from DRRI which have not been discussed in the NLP community, and demonstrate the performance of our Hanja language models on unseen historical documents."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Hanja",
      "text" : "Hanja, the writing system based on ancient Chinese characters, was the main writing system in Korea before the 20th century, while Hangul, the unique Korean alphabet, has been the main writing system in Korea from the last century. Formal records from the Joseon dynasty (1392-1897) are written in Hanja, while spoken language and some written documents were in Hangul, developed in the 15th century. This co-existence of the written and colloquial languages has led Hanja to evolve to have the basic syntax of classical Chinese, mixing with the lexical, semantic, and syntactic characteristics of colloquial Korean.\nHanja is significantly different from both modern Korean and modern Chinese. Modern Korean uses a different alphabet and structure, and traditional Chinese shares some characters with Hanja, while the lexicon has evolved greatly to reflect the\ntemporal, geographical, and cultural differences between the Joseon dynasty and modern-day China. Simplified Chinese, the current written language in China has diverged more because of the simplification of the characters. These differences between Hanja and other related languages would lead to suboptimal performance when adopting the current Chinese language models to NLU tasks for the Korean historical Hanja documents."
    }, {
      "heading" : "2.2 Dataset",
      "text" : "We describe the three corpora of records written in Hanja during the Joseon dynasty, whose contents and additional information such as topic and named entities are provided by historians in IKTC 2. Table 2 shows the list of the Hanja corpora used.\nAnnals of the Joseon Dynasty (AJD) also called Veritable Records of the Joseon Dynasty, is a corpus of 27 sets of chronological records, and each set covers one ruler’s reign. AJD has been translated into Korean from 1968 to 1993 and includes relevant tags such as the named entities and dates of the documents 3. We use AJD for both training our Hanja language models and building the HUE dataset of NLP tasks.\nDiaries of the Royal Secretariat (DRS) is a corpus of detailed records of daily events and official schedules of the court from the first King Taejo to the last (27th) Sunjong. Many of the earlier records were lost, and we use the extant records starting from the 16th King Injo. DRS is known to hold the largest amount of authentic historic records and state secrets of the Joseon Dynasty 4. We use DRS to continue pretraining the language models.\nDaily Records of the Royal Court and Important Officials (DRRI) is a corpus of journals written from the 21st King Yeongjo to the last Emperor Sungjong and presumably initiated from the diaries of the crown prince who became the 22nd King Jeongjo after he was enthroned. DRRI has official daily records from both the central and the local governments, so encompasses all events in the country and reports to the king with summaries. DRRI is known to include details and events of the late Joseon Dynasty not recorded in the AJD or DRS 4, thus making it a good corpus for zero-shot\n2https://db.itkc.or.kr/ 3http://esillok.history.go.kr/ 4http://english.cha.go.kr/\nexperiments. We use DRRI for the supervised summary retrieval task and the zero-shot experiments for king prediction and named entity recognition."
    }, {
      "heading" : "3 HUE Dataset",
      "text" : "The HUE (Hanja Understanding Evaluation) dataset is built to assist history scholars to understand Korean historical records written in Hanja. HUE consists of NER, topic classification, king (i.e., time period) prediction, and summarization, which are tasks that can provide helpful information for studying the documents. We expect that the language models based on HUE will ultimately help historians to interpret unseen historical documents and public to grasp basic concept of those documents. We describe each task in detail below."
    }, {
      "heading" : "3.1 Task Description",
      "text" : "King Prediction (KP) is a classification task predicting the ruling king when the document was written. When given a Hanja document from AJD, a classifier outputs one of the 27 kings of the Joseon dynasty.\nTopic Classification (TC) is a multi-class and multi-label classification task to find the topics of the given document. For TC, we use Hanja document from AJD. We suggest two levels of topics, namely major and minor categories. The major categories consist of 4 broad topics: politics, economy, society, and culture. The minor categories go with 106 sub-topics such as diplomacy, agriculture, and science.\nNamed Entity Recognition (NER) is a sequence tagging task, identifying the two types of named entities, person and location, from the Hanja document from AJD. We divide train, validation, and test sets such that there are no overlapping entities across the sets.\nSummary Retrieval (SR) is a task to find the most relevant summary that matches the content among the summary candidates. For this task, we\nuse DRRI, in which each document is a pair of summary (gang) and detailed content (mok). Among 426k articles, 265k articles in DRRI dataset contain both gamg and mok. Also, we exclude those with gang longer than mok, in which gang is not the summary of mok. The final dataset contains 213K pairs of content and the corresponding summaries. We describe more details of the preprocessing in the Appendix."
    }, {
      "heading" : "4 Hanja Pretrained Model",
      "text" : "As far as we know, there have been no pretrained language models for the Hanja language. One can use related LMs, the pretrained models for ancient Chinese as well as multilingual BERT (Devlin et al., 2019) which includes traditional Chinese in its training corpus. AnchiBERT (Tian et al., 2021) is pretrained in ancient Chinese with the Chinese anthologies written around 1000BC to 200BC. There is some vocabulary overlap between the Hanja documents and traditional Chinese corpora, we can adopt multilingual BERT and AnchiBERT to learn the representations of the Hanja texts.\nWe propose the pretrained language models suitable for Hanja documents by continuing pretraining those two models on both AJD and DRS. Table 3 shows the ratio of unknown tokens in the test set of AJD by each model. It implies that existing AnchiBERT and multilingual BERT can also be exploited as language models for Hanja documents written in the Joseon dynasty, but the second phase of pretraining on the corpora of that era remarkably decreases the ratio of unknown tokens."
    }, {
      "heading" : "5 Experiment",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Settings",
      "text" : "We conduct experiments on HUE with our pretrained model. For the baseline model, we use BERT without pretraining and compare it to various BERT models described in Section 3.1. Specifically, we fine-tune each model to act as a re-ranker in the retrieval task for the summary retrieval. We first retrieve top-k relevant gangs (summaries) with\nBM25 (Robertson and Zaragoza, 2009) among all gangs in the dataset. Then, we fine-tune the model as a binary classifier to determine whether the summary matches the content of the mok with the cross-entropy loss (Nogueira and Cho, 2019). If the ground truth summary is not included in the top-k relevant summary candidates, we replace the last kth summary with the ground truth. We use k = 12 for training and k = 100 for inference. As the representative metrics, we present F1 scores for KP, TC, and NER tasks, and Mean Reciprocal Rank (MRR) for SR. The detailed results with other metrics are also available in Appendix."
    }, {
      "heading" : "5.2 Overall Results",
      "text" : "Table 4 shows the overall experimental results. The bold and the underlined texts in the table specify the best and the second best result, respectively. BERT without any pretraining shows the poorest results across all the tasks. AnchiBERT and mBERT, which are existing language models on the relevant domains, show better results, and the models continued pretraining on Hanja documents achieve the best performance among all tasks. This tendency indicates that all these tasks on understanding historical documents require pretraining language models on time-specific and domain-specific data.\nAnchiBERT pretrained on the Hanja corpora shows slightly better performance than mBERT pretrained on the same corpora. We assume this is because the original training corpora of AnchiBERT are much closer to the Hanja documents, even the era of those two corpora are completely different. The writing style of both Hanja documents in the Joseon dynasty and anthologies in ancient China had come from Classical Chinese and share similarities. On the other hand, the training corpora of mBERT is a contemporary texts whose characters contains Traditional Chinese, but the structure and the format might be considerably changed.\nKing Prediction (KP) Our models continued pretraining on Hanja corpora outperform other baselines on KP. Detailed analysis on KP result is illustrated in Section 6.1.\nTopic Classification (TC) Figure 1 gives ROC curves and AUC values of each model on each task. Our models show the similar trends to the overall results, outperforming other language models. For the evaluation results including F1 score, we find and set the best threshold to each label by Youden’s index (Youden, 1950).\nWhile F1 score goes down as the number of classes increases from 4 to 106, there is no significant difference on AUC value. This might result from consistently high recall achieving around 90% on both tasks. It indicates that the threshold is too low and models tend to predict plausible topics as many as they can, which might be solved by controlling the threshold. AnchiBERT pretrained on AJD and DRS, which shows the best performance, predicts 6.39 labels in average, while the average number of ground truth labels of the minor categories is 1.97. This is probably due to the meaning overlaps in minor categories. For instance, minor categories such as revenue, finance, general price level, and commerce are the sub-categories of economy in the major categories, whose use case cannot be strictly distinguished. It would be more appropriate in this case to provide all plausible topics roughly rather than suggesting the one only with high certainty.\nNamed Entity Recognition (NER) NER also indicates similar trends to the overall benchmark tasks, but with a small gap among models including BERT without pretraining. It implies that NER in Hanja documents is a comparably easy task. This might result from certain patterns in named entities\nin Hanja. Most of the person entities are 3 letters starting with the common characters (family name), and most of the location entities end with the common characters meaning locations or buildings. All models tend to predict person entities better than location entities.\nSummary Retrieval (SR) All fine-tuned BERTbased re-rankers outperform BM25 whose MRR is merely 29.87%, mostly retrieving the ground truth answer at the first trial. Likewise, our models shows the best results, while BERT without pretraining show the lowest MRR. It additionally implies that BERT-based re-rankers might be exploited for retrieving relevant documents from different chronicles in terms of written style or contents."
    }, {
      "heading" : "5.3 Effect of Entity and Document Age on Language Model",
      "text" : "We investigate whether providing additional information as input can improve the performances of language models. For KP, we mask all named entities in the input and fine-tuned the language model on masked data to examine the impact of entity information. For TC and NER, we concatenate document age information to the input text and run comparative experiments to verify the importance of time period on historical texts.\nEntity-Masked King Prediction Table 5 shows the difference on experimental results in KP when the named entities in the given input texts are masked. Compared to the default settings which does not mask named entities, all models show significant improvements. This is probably because models can truly focus on the content and changes in writing style without any disturbance of location entities consistently used for the whole era. Fine-tuned models with entity masked inputs also\nachieve nearly the same level of performance in the inference with plain inputs including named entities. It suggest to fine-tune models masking named entity in KP, considering the real scenario whose inference texts lack time period information.\nTopic Classifciation and Named Entity Recognition with the Age of Document It is for granted to regard that historical texts written over several eras reveal the time changes with respect to lexical choices and contents. Section 6.2 confirms the hypothesis above in terms of n-grams changes over time. Table 6 shows gaps between experimental results of TC and NER given which king reigned when the document was written. Providing document age definitely increases the performance of classifying topics and tagging named entities. There was a big gap on difference with non-pretrained BERT in TC, which is probably due to the poor performance of itself in the original setting. All models show similar trends on both tasks with improved performances compared to the original settings without document age as input. It is an obvious result considering that the first step for ancient manuscript is assuming the written era. It implies the significance of king prediction task in HUE, conveying that king prediction task might\nimprove the performance of other HUE tasks."
    }, {
      "heading" : "5.4 Zero-shot Experiment",
      "text" : "Countless number of Hanja documents still remain without any analysis and new documents continue to be unearthed. Therefore, we run zero-shot experiments to verify the effectiveness of our language models on extracting information from the historical documents irrelevant to the training corpora. We use DRRI dataset which is not included in both pretraining and fine-tuning data of our Hanja language models and execute KP and NER.\nTable 7 shows experimental results with KP and NER on DRRI. All models perform comparably well on the both tasks, regarding that random model will achieve approximately 3.70% performances with 27 classes in KP. Also, all models in KP commonly show high precision which might be due to the monotonous and redundant phrases in the veritable records. It shows similar trends compared to the Table 4, but the gap among models was notably emphasized in the zero-shot settings. This results imply that our KP models might be exploited for the time period prediction of unseen documents in anthology with a reliable level.\nOur models outperform others on NER achieving absolutely high performances, though entity maps between AJD and DRRI do not match strictly. Interestingly, all models tend to predict location entities better than person entities which is the opposite result compared to the original NER on AJD. It is probably due to the characteristics of each entity, where location entities are all commonly used in nationwide while person entity might differ by situation. Further analysis on person and location entities in the view of time changes is described in Section 6.2. We present that our models trained on the corpora of the Joseon dynasty provide reliable results on unseen records, implying that our model can be exploited for the low-resourced documents."
    }, {
      "heading" : "6 Further Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Do Historical Events Affect Language Models?",
      "text" : "To figure out the effect of historical events on models’ prediction, we analyzed the output of language models on KP. Figure 2 (a) shows a log-scale confusion matrix of AnchiBERT continued pretraining on AJD and DRS, and Figure 2 (b) indicates the mean absolute error between the predicted king order and the ground truth per each King. The x-axis in the Figure 2 (b) means the changes of time by\nthe king reign period. Each bar in Figure 2 (b) indicates the mean absolute error between the order of ground truth king and the one of predicted, and the line graph means the number of samples in the test set.\nThe results of the last two Emperors, Gojong and Sunjong, are remarkable in that the model rarely gets confused with those two labels to others and tends not to fail, showing notable difference on (a) and significantly low mean absolute error on (b). We believe that this is because our model learns the difference between those two records and the others in the historical view and get cues to distinguish them. The last two records are not treated as AJD in general, since those records were inspected and produced by the Office of GovernerGeneral during the Japanese colonial period with the view of Empire of Japan who ruled Korea 5.\nThe mean absolute error of the predicted order of each king achieved the difference around one, except for the first King Taejo and the second King Jeongjong, whose errors are almost doubled. We hypothesize that this is mainly because there are too small number of examples in those classes. A similar tendency where the more samples are, the less mean absolute error be has been observed in other classes. Also, the writing style of AJD had settled down from the third King Taejong, and those noisy records might confuse models to predict the exact dates."
    }, {
      "heading" : "6.2 Do Time Changes Affect Written Texts?",
      "text" : "It stands to reason that AJD written in five centuries reveal the features of the language changes. In this section, we investigate the hypothesis above with respect to the named entities and n-grams.\nWords Change over Time We analyze how frequently words change over time. For each king, we plot how many trigrams overlap by each king era\n5http://esillok.history.go.kr/\nin the order. Figure 3 shows overlapped trigrams in the 1st, 9th, 17th, and 25th king and the detailed results with all kings are described in Appendix. It is consistently observed that the closer the king era is, the more trigrams are overlapped. These changes result from not named entity but lexical choices, considering that person and location entities account for 6.38% and 2.05% in the characters of AJD, respectively. It verifies that words used in Joseon dynasty had changed over time gradually, and it enables the language models to capture those features.\nNamed Entity Changes over Time We investigate how the named entities had been used over time. In particular, we show frequency rates of top-10 frequently-used named entities by each king era and how they change over time in Figure 4. It implies a strong correlation between person entity and the passage of time, while there is no explicit correlation to location entity. Most person entities\ninclude officials of the time or the previous kings, relevant to the time. In contrast, most location entities include neighboring countries or place names in the Joseon, which are less dependent on the time. The examples of frequently appeared named entities are described in Appendix."
    }, {
      "heading" : "7 Related Work",
      "text" : "ML based NLP techniques have been recently applied to anthology to discover historical documents such as authorship attribution (Ouamour and Sayoud, 2012; Sayoud and Ouamour, 2017; Reisi and Mahboob Farimani, 2020; Hossain et al., 2020), NER (Won et al., 2018; Palladino et al., 2020), and manuscript age detection (Adam et al., 2018). Along with these works, several works provide language models suited for historical texts in ancient languages and evaluate those models on existing NLU tasks, which aims to support understanding those documents considering that the target languages are mostly extinct. Bamman and Burns (2020) propose Latin BERT for part-of-speech tagging in ancient Latin script. Tian et al. (2021) suggest AnchiBERT and evaluate their model on some NLP tasks including poem topic classification. However, there has been no research attempting to propose language models in Hanja, which is a dead language in Korea but absolutely necessary to explore Korean history. Most of the studies with Hanja only shed lights on translating historical Hanja documents and use AJD as their corpus (Park\net al., 2020; Jin et al., 2020; Kang et al., 2021)."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We present HUE (Hanja Understanding Evaluation) dataset and BERT-based pretrained language models for classical Hanja documents. HUE dataset includes diverse tasks that can support analyzing historical documents written in Hanja which is an extinct language in Korea: King Prediction (KP), Topic Classification (TC), Named Entity Recognition (NER), and Summary Retrieval (SR). Our models pretrained on Hanja corpora outperform other language models and we observe their performance on zero-shot settings with DRRI which is the dataset never been introduced in NLP community. The experimental results in king prediction imply that our models capture the historical events or facts disclosed in the texts. We also explore several methods to support Hanja language models such as masking named entities and giving document age as input sources, based on the analyses on textual features in AJD.\nHelp of adequate resources in Hanja documents might could fill some caveats in our work which lacks additional experiments and analyses on the records of different genre such as poetry, novel, and humanities resulting from the low resources that we can exploit. However, we expect that our dataset and accompanying language models might facilitate future works on historical documents written in Hanja by providing fundamental resources to leverage unknown Hanja corpora."
    }, {
      "heading" : "A Model",
      "text" : "Table 8 shows hyperparameter settings of our models. We used Intel(R) Xeon(R) Silver 4114 (40 CPUs) and GeForce RTX 2080 Ti 10GB (4 GPUs) for all experiments including training, fine-tuning, and inference."
    }, {
      "heading" : "B HUE Dataset",
      "text" : "B.1 Dataset Size\nB.2 Source Corpora\nB.2.1 Data Collection Process\nWe crawl AJD 6, DRS 7, and DRRI 8 from the comprehensive database for Korean classics which are publicly available published by IKTC. All source corpora are fully tagged with the written ages and named entities, while their entity maps differ to each other. AJD also provides topics which is tagged by the experts in the translation process.\nB.2.2 Dataset Preprocessing Table 10 shows good and bad example in DRRI to use as summary retrieval dataset. Bad examples mostly written from the 21st King Yeongjo to early in the 22nd King Jungjo describe daily lifes of the crown prince who is King Jeongjo. The bad example in Table 10 is depicting his study. These examples tend to present extremely short moks which cannot be treated as summary and content, while the offical records on administrative has much longer moks."
    }, {
      "heading" : "C Discussion",
      "text" : "C.1 Trigram Changes Over Time Figure 5 shows the changes of trigrams over all kings. It clearly delivers the changes of trigrams as time goes by. We can observe the same trend in either unigram or bigram.\nC.2 Top-5 Named Entities by Kings Table 11 shows top-5 person and location named entities in three King reigns. All person entities except King Sejong, the most frequent entity in Munjong, are officials in the reign periods, which is different by time changes. In contary, all location entities are the name of place, palace or site, showing some entities overlap among kings."
    }, {
      "heading" : "D Experimental Results",
      "text" : "For KP, we measure the Quadratic Weighted Kappa score (QWK score) as metrics that treat each king label hierarchically.\nSince TC is a multi-label classification task whose example might have multiple labels as the answer, we measure Hamming score along with accuracy. In this case, accuracy is the exact match score, and the hamming score is the accuracy of subset matched, | T ∩ P | / | T ∪ P |, where T is set of true labels and P is set of predicted labels (Godbole and Sarawagi, 2004). For the evaluation results in Table 13, we find and set the best threshold to each label by Youden’s index. All pretrained models outperform BERT without pretraining, and two LMs re-trained on hanja documents show the best performances.\n6http://sillok.history.go.kr/main/main. do\n7http://sjw.history.go.kr/main.do 8http://kyudb.snu.ac.kr/series/main.\ndo?item_cd=ILS"
    } ],
    "references" : [ {
      "title" : "Kertas: dataset for automatic dating of ancient arabic manuscripts",
      "author" : [ "Kalthoum Adam", "Asim Baig", "Somaya Al-Maadeed", "Ahmed Bouridane", "Sherine El-Menshawy." ],
      "venue" : "International journal on Document Analysis and Recognition (IJDAR).",
      "citeRegEx" : "Adam et al\\.,? 2018",
      "shortCiteRegEx" : "Adam et al\\.",
      "year" : 2018
    }, {
      "title" : "Five centuries of monarchy in Korea: Mining the text of the annals of the Joseon dynasty",
      "author" : [ "JinYeong Bak", "Alice Oh." ],
      "venue" : "Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",
      "citeRegEx" : "Bak and Oh.,? 2015",
      "shortCiteRegEx" : "Bak and Oh.",
      "year" : 2015
    }, {
      "title" : "Latin bert: A contextual language model for classical philology",
      "author" : [ "David Bamman", "Patrick J. Burns" ],
      "venue" : null,
      "citeRegEx" : "Bamman and Burns.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bamman and Burns.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Discriminative methods for multi-labeled classification",
      "author" : [ "Shantanu Godbole", "Sunita Sarawagi." ],
      "venue" : "Advances in Knowledge Discovery and Data Mining.",
      "citeRegEx" : "Godbole and Sarawagi.,? 2004",
      "shortCiteRegEx" : "Godbole and Sarawagi.",
      "year" : 2004
    }, {
      "title" : "A stylometric approach for author attribution system using neural network and machine learning classifiers",
      "author" : [ "Anika Samiha Hossain", "Nazia Akter", "Md. Saiful Islam." ],
      "venue" : "Proceedings of the International Conference on Computing Advancements.",
      "citeRegEx" : "Hossain et al\\.,? 2020",
      "shortCiteRegEx" : "Hossain et al\\.",
      "year" : 2020
    }, {
      "title" : "Korean historical documents analysis with improved dynamic word embedding",
      "author" : [ "KyoHoon Jin", "JeongA Wi", "KyeongPil Kang", "YoungBin Kim." ],
      "venue" : "Applied Sciences.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Restoring and mining the records of the Joseon dynasty via neural language modeling and machine translation",
      "author" : [ "Kyeongpil Kang", "Kyohoon Jin", "Soyoung Yang", "Soojin Jang", "Jaegul Choo", "Youngbin Kim." ],
      "venue" : "Proceedings of the NAACL: Human Language",
      "citeRegEx" : "Kang et al\\.,? 2021",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2021
    }, {
      "title" : "Passage re-ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Nogueira and Cho.,? 2019",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2019
    }, {
      "title" : "authorship attribution of ancient texts written by ten arabic travelers using a smo-svm classifier",
      "author" : [ "Siham Ouamour", "Halim Sayoud." ],
      "venue" : "International Conference on Communications and Information Technology (ICCIT).",
      "citeRegEx" : "Ouamour and Sayoud.,? 2012",
      "shortCiteRegEx" : "Ouamour and Sayoud.",
      "year" : 2012
    }, {
      "title" : "Ner on ancient greek with minimal annotation",
      "author" : [ "Chiara Palladino", "Farimah Karimi", "Brigitte Mathiak." ],
      "venue" : "https://dh2020. adho. org/.",
      "citeRegEx" : "Palladino et al\\.,? 2020",
      "shortCiteRegEx" : "Palladino et al\\.",
      "year" : 2020
    }, {
      "title" : "Ancient korean neural machine translation",
      "author" : [ "Chanjun Park", "Chanhee Lee", "Yeongwook Yang", "Heuiseok Lim." ],
      "venue" : "IEEE Access.",
      "citeRegEx" : "Park et al\\.,? 2020",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2020
    }, {
      "title" : "Authorship attribution in historical and literary texts by a deep learning classifier",
      "author" : [ "Ehsan Reisi", "Hassan Mahboob Farimani." ],
      "venue" : "journal of Applied Intelligent Systems and Information Sciences.",
      "citeRegEx" : "Reisi and Farimani.,? 2020",
      "shortCiteRegEx" : "Reisi and Farimani.",
      "year" : 2020
    }, {
      "title" : "The probabilistic relevance framework: Bm25 and beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "The Probabilistic Relevance Framework (PRF).",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "Score fusion based authorship attribution of ancient arabic texts",
      "author" : [ "Halim Sayoud", "Siham Ouamour." ],
      "venue" : "Florida Artificial Intelligence Research Society Conference.",
      "citeRegEx" : "Sayoud and Ouamour.,? 2017",
      "shortCiteRegEx" : "Sayoud and Ouamour.",
      "year" : 2017
    }, {
      "title" : "Anchibert: A pre-trained model for ancient chinese language understanding and generation",
      "author" : [ "Huishuang Tian", "Kexin Yang", "Dayiheng Liu", "Jiancheng Lv." ],
      "venue" : "2021 International Joint Conference on Neural Networks (IJCNN).",
      "citeRegEx" : "Tian et al\\.,? 2021",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2021
    }, {
      "title" : "The collaborative anthology in the literary translation course",
      "author" : [ "Margarida Vale de Gato." ],
      "venue" : "The Interpreter and Translator Trainer.",
      "citeRegEx" : "Gato.,? 2015",
      "shortCiteRegEx" : "Gato.",
      "year" : 2015
    }, {
      "title" : "Ensemble named entity recognition (ner): Evaluating ner tools in the identification of place names in historical corpora",
      "author" : [ "Miguel Won", "Patricia Murrieta-Flores", "Bruno Martins." ],
      "venue" : "Frontiers in Digital Humanities.",
      "citeRegEx" : "Won et al\\.,? 2018",
      "shortCiteRegEx" : "Won et al\\.",
      "year" : 2018
    }, {
      "title" : "Index for rating diagnostic tests",
      "author" : [ "William J Youden." ],
      "venue" : "Cancer, 3(1):32–35. 9",
      "citeRegEx" : "Youden.,? 1950",
      "shortCiteRegEx" : "Youden.",
      "year" : 1950
    }, {
      "title" : "|, where T is set of true labels and P is set of predicted labels (Godbole and Sarawagi, 2004). For the evaluation results in Table 13, we find and set the best threshold",
      "author" : [ "P T ∩ P | / | T" ],
      "venue" : null,
      "citeRegEx" : "∪,? \\Q2004\\E",
      "shortCiteRegEx" : "∪",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "There are two corpora we can use to train the language model, the Annals of Joseon Dynasty (AJD), first introduced to the NLP community in (Bak and Oh, 2015), and the Diaries of the Royal Secretariats (DRS) (Kang et al.",
      "startOffset" : 139,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "There are two corpora we can use to train the language model, the Annals of Joseon Dynasty (AJD), first introduced to the NLP community in (Bak and Oh, 2015), and the Diaries of the Royal Secretariats (DRS) (Kang et al., 2021).",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 3,
      "context" : "One can use related LMs, the pretrained models for ancient Chinese as well as multilingual BERT (Devlin et al., 2019) which includes traditional Chinese in its training corpus.",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "AnchiBERT (Tian et al., 2021) is pretrained in ancient Chinese with the Chinese anthologies written around 1000BC to 200BC.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "BM25 (Robertson and Zaragoza, 2009) among all gangs in the dataset.",
      "startOffset" : 5,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "Then, we fine-tune the model as a binary classifier to determine whether the summary matches the content of the mok with the cross-entropy loss (Nogueira and Cho, 2019).",
      "startOffset" : 144,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "For the evaluation results including F1 score, we find and set the best threshold to each label by Youden’s index (Youden, 1950).",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "ML based NLP techniques have been recently applied to anthology to discover historical documents such as authorship attribution (Ouamour and Sayoud, 2012; Sayoud and Ouamour, 2017; Reisi and Mahboob Farimani, 2020; Hossain et al., 2020), NER (Won et al.",
      "startOffset" : 128,
      "endOffset" : 236
    }, {
      "referenceID" : 14,
      "context" : "ML based NLP techniques have been recently applied to anthology to discover historical documents such as authorship attribution (Ouamour and Sayoud, 2012; Sayoud and Ouamour, 2017; Reisi and Mahboob Farimani, 2020; Hossain et al., 2020), NER (Won et al.",
      "startOffset" : 128,
      "endOffset" : 236
    }, {
      "referenceID" : 5,
      "context" : "ML based NLP techniques have been recently applied to anthology to discover historical documents such as authorship attribution (Ouamour and Sayoud, 2012; Sayoud and Ouamour, 2017; Reisi and Mahboob Farimani, 2020; Hossain et al., 2020), NER (Won et al.",
      "startOffset" : 128,
      "endOffset" : 236
    }, {
      "referenceID" : 17,
      "context" : ", 2020), NER (Won et al., 2018; Palladino et al., 2020), and manuscript age detection (Adam et al.",
      "startOffset" : 13,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : ", 2020), NER (Won et al., 2018; Palladino et al., 2020), and manuscript age detection (Adam et al.",
      "startOffset" : 13,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : ", 2020), and manuscript age detection (Adam et al., 2018).",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "Most of the studies with Hanja only shed lights on translating historical Hanja documents and use AJD as their corpus (Park et al., 2020; Jin et al., 2020; Kang et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "Most of the studies with Hanja only shed lights on translating historical Hanja documents and use AJD as their corpus (Park et al., 2020; Jin et al., 2020; Kang et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 174
    }, {
      "referenceID" : 7,
      "context" : "Most of the studies with Hanja only shed lights on translating historical Hanja documents and use AJD as their corpus (Park et al., 2020; Jin et al., 2020; Kang et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 174
    } ],
    "year" : 0,
    "abstractText" : "Historical records in Korea before the 20th century were primarily written in Hanja, an extinct language based on Chinese characters and not understood by modern Korean or Chinese speakers. Historians with expertise in this time period have been analyzing the documents, but that process is very difficult and time-consuming, and language models would significantly speed up the process. Toward building and evaluating language models for Hanja, we release the Hanja Understanding Evaluation dataset consisting of king prediction, topic classification, named entity recognition, and summary retrieval tasks. We also present BERT-based models continued pretraining on the two major corpora from the 14th to the 19th centuries: the Annals of the Joseon Dynasty and Diaries of the Royal Secretariats. 1 We compare the models with several baselines on all tasks and show there are significant improvements gained by training on the two corpora. Additionally, we run zeroshot experiments on the Daily Records of the Royal Court and Important Officials (DRRI). The DRRI dataset has not been studied much by the historians, and not at all by the NLP community.",
    "creator" : null
  }
}
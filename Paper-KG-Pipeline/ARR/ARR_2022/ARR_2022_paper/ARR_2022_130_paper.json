{
  "name" : "ARR_2022_130_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MultiSpanQA: A Dataset for Multi-Span Question Answering",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The task of reading comprehension, where models are required to process a text and answer questions about it, has seen rapid progress in recent years. As systems have increasingly matched humans on popular datasets (Rajpurkar et al., 2016, 2018), researchers have developed newer, more complex formulations of the task, such as very long contexts and answers (Kwiatkowski et al., 2019), multi-hop reasoning (Yang et al., 2018), and discrete operations over the content of paragraphs (Dua et al., 2019). One thing these datasets have in common is that the answer is constrained to be a single span that can be extracted or computed from the context.\nHowever, in practice, the answer to a question will often consist of multiple parts. As in the example in Figure 1, the answer set contains 10 countries,\nsome of which are discontiguous in the passage. Such cases are largely ignored in existing reading comprehension research, in part because there are no datasets of multi-span questions.\nIn this paper, we introduce MultiSpanQA, a new reading comprehension dataset consistinga of 6,465 multi-span examples. The raw questions and passages are extracted from Natural Questions (“NQ”: Kwiatkowski et al. (2019)), a large-scale open-domain QA dataset. Trained annotators were asked to identify question–passage pairs where the answer was multi-span, and annotate the spans. In addition to the basic version of the dataset consisting entirely of multi-span answers, we also prepare an expanded version with a selection of unanswerable questions, and questions with single- and multi-span answers, intended to reflect a more realistic QA setup.\nWe further classify answer semantics into 5 categories, and manually label the logical structure of the answer spans. We introduce metrics to evaluate multi-span QA systems across these different tasks.\nWe propose several baselines, and a new model which casts the task as a sequence tagging problem. The proposed model combines a sequence tagger with a span number predictor, span structure predictor, and span adjustment module. Experimental results show that the proposed model surpasses\nall baselines and achieves 62.58% exact-match F1 score and 77.46% overlapping F1 score.\nTo summarize, our contributions are: • A new reading comprehension dataset con-\ntaining 6.5k high-quality multi-span answers, along with analysis and metrics for multi-span QA. • A novel label set for capturing the semantics\nof multi-span answers, with annotations. • A new model for multi-span reading compre-\nhension which achieves state-of-the-art results on our dataset."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Question Answering Datasets",
      "text" : "Extractive QA Most existing extractive QA datasets such as SQuAD (Rajpurkar et al., 2016), SQuAD2.0 (Rajpurkar et al., 2018), SearchQA (Dunn et al., 2017), and QuAC (Choi et al., 2018) restrict the answer passage to a single span of text. SQuAD and SQuAD 2.0 limit the answer passage to a short paragraph from Wikipedia; the bestperforming systems have now exceeded human performance on these datasets. QuAC frames the task in a dialogue setting by introducing a teacher and student, where the student repeatedly asks the teacher questions about a topic and the teacher tries to find answers from the given passage. That is, it supports information seeking through multi-turn conversation. TriviaQA (Joshi et al., 2017) and HothopQA (Yang et al., 2018) extend the answer context from single passage to multiple passages, while HotpotQA further requires reasoning over multiple passages to answer the question. However, all of these datasets limit the answer to a single text span from the provided answer context.\nDROP (Dua et al., 2019) requires systems to resolve (possibly multiple) references in a question, and perform discrete operations (such as addition, sorting, or counting) over them. However, because these operations are mostly numeric, the spans are almost exclusively semantically homogeneous and related to numeric values. MASH-QA (Dua et al., 2019) extends the answer space to texts that span across a longer document, but this dataset is highly domain-specific, in the healthcare domain. Quoref (Dasigi et al., 2019) and Natural Questions (“NQ”: Kwiatkowski et al. (2019)) both contain multi-span answers. Quoref requires systems to resolve coreference among entities, to aid in span-selection. NQ is a large-scale dataset that provides questions with\nvery long answer contexts. The proportion of multispan answers is around 10% and 2% in Quoref and NQ, respectively. However in each case, multispan answers are captured as a single span, with no annotation of the internal structure of the component spans. WikiHowQA and WebQA (Cui et al., 2021) both focus on non-factoid (e.g., how, why) questions, with answers mostly being long spans or full sentences.\nGenerative QA Generative QA datasets usually require systems to answer questions in the form of several sentences, either selected from the provided answer context or generated based on it. WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) are two open-domain generative QA datasets, where answers in WikiQA are mostly sentences from the answer passage, while answers in MS Marco are free-form sentences generated by crowd workers. NarrativeQA (Kociský et al., 2018) is a dataset of movie and book summaries. SearchQA (Dunn et al., 2017), ELI5 (Fan et al., 2019), and CoQA (Reddy et al., 2019) are three multiple-document datasets. SearchQA is constructed from question–answer pairs crawled from Jeopardy!, and most questions can be answered with a short (99% less than 5 tokens) extractive span from a single document. ELI5 requires systems to generate paragraph-length answers by summarizing information from multiple documents. CoQA contains conversational questions, with freeform text as answers.\nCloze style Cloze datasets such as CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al., 2016), and BookTest (Bajgar et al., 2016) require systems to predict a missing word from a passage. However, researchers have shown that this task is artificial, and can be largely solved with simple methods and relatively little reasoning (Chen et al., 2016)."
    }, {
      "heading" : "2.2 Multi-span Models",
      "text" : "Dua et al. (2019) proposed to predict the number of output spans for each question, by applying a single-span predictor recursively, making training complex. Segal et al. (2020) first proposed to treat multi-span QA as a sequence tagging task, in the form of a multi-head architecture (Dua et al., 2019) to perform arithmetic operations between the predicted spans. Hu et al. (2019) applied the non-maximum suppression (NMS) algorithm (Rosenfeld and Thurston, 1971) to prune redundant\nbounding boxes from the top-k predicted spans of a single-span predictor. Pang et al. (2019) proposed HAS-QA, which supports multi-span prediction by computing answer probabilities at the question, paragraph and span levels. A common feature of these works is that the predicted spans are fed into an aggregation module, and the answers are usually a single span chosen from the prediction, or a number computed from them. Cui et al. (2021) proposed a model which can extract list-form answers across multiple spans. Their work mainly focuses on capturing the sequential and progressive relationships between long-span descriptions."
    }, {
      "heading" : "3 Dataset Construction and Composition",
      "text" : "In this section, we describe how we construct MultiSpanQA, and provide a statistical breakdown of its composition."
    }, {
      "heading" : "3.1 Data Collection and Preprocessing",
      "text" : "The question–passage pairs were selected from Natural Questions (NQ: Kwiatkowski et al. (2019)), a large-scale open-domain QA dataset made up of (question, passage, long answer, short answer) quadruples where: the questions are real queries issued to the Google search engine; the passage is a Wikipedia page which may or may not contain the information required to answer the question; the long answer is a paragraph from the page containing all information required to infer the answer; and the short answer is one or more text spans that answer the question. Both long and short answers can be NULL if no viable answer candidate exists on the page.\nTo create MultiSpanQA, we first extract NQ questions annotated with multiple short answers, and consider the long answer to be the answer passage. We then remove paragraphs that don’t contain any question part, to eliminate the informationretrieval component of NQ and focus more on the short answer extraction problem. To make the dataset easy to use, we strip HTML from the passages, so that they only contain plain text. As table structure cannot be captured in the plain text after removing HTML, we remove the passages that contain tables. Ultimately, around 6700 candidates remain where each candidate is a triple of (question, passage, set of answer spans).\nTo aid the annotation process, we classifies the samples into 5 categories according to the expected answer type of questions using a BERT-based clas-\nsifier trained on the TREC Question Classification dataset (Li and Roth, 2002). The classes are DESCRIPTION, LOCATION, HUMAN, NUMERIC, and OTHER ENTITY. Table 1 shows the breakdown and an example of each answer type class."
    }, {
      "heading" : "3.2 Issues in Existing Dataset",
      "text" : "NQ was originally annotated by around 50 annotators, with an average annotation time of 80 seconds per instance. However, we found a number of issues with the dataset: (1) grammatical errors in questions, due to them being actual queries submitted to the Google search engine by real users; (2) answer boundary inconsistencies or errors, such as the entity University of Michigan being annotated as an answer in one example but The University of Michigan being annotated in another; and (3) wrong or incomplete answer: some questions are not answered or are answered incompletely in the annotated answer span, for example, to answer the question Which countries does the River Danube flow through?, 10 countries should be included in the answer span while only 9 are annotated. These issues are relatively uncommon overall in the dataset, but occur disproportionately in multi-span answers."
    }, {
      "heading" : "3.3 High Quality Re-annotation",
      "text" : "We (re-)annotated all the data using the Brat annotation tool (Stenetorp et al., 2012).1 Three annotators were provided with a category-specific annotation guide (broken down across the 5 predicted answer types), and annotated the data on a per-category basis.2\nFor each annotation instance, we show the question, passage, and the original multiple answer spans to the annotator. The first-pass annotation was according to the following four categories:\n1http://brat.nlplab.org 2The annotation guide will be released along with the data.\n• Good example: the question is clear, and the answer spans are labelled consistent with the annotation guide, in which case accept the instance as is. • Bad question: the question is ungrammati-\ncal or not aligned with the passage content, in which case rewrite the question while preserving its original intended meaning where possible (otherwise reject). • Bad answer span(s): the answer span(s) are\nincorrect or incomplete, in which case remove the inappropriate spans and select the correct spans. • Bad question–answer pair: the question\ndoesn’t align with the passage content (e.g. there is no answer there) or there are not multiple answer spans in the passage (e.g. there is only a single answer span), in which case reject the instance.\nAlthough all examples in our dataset contain multiple answer spans, the semantic structure varies considerably. We hand-annotate this via a novel 5-way annotation scheme, as follows (see Table 2 for examples):\n• 1. CONJUNCTION: Each span is part of the answer, and the answer is complete only when all of the spans are combined • MULTI-PART-DISJUNCTION: Each span is\na complete (but independent) answer to the question, with one of the following structures:\n– 2. REDUNDANT: the multiple spans re-\nfer to the same concept or entity. For example, in the example in Table 2, each span is a full answer to the question, specified using different temporal reference points. – 3. NON-REDUNDANT: the different spans refer to different concepts or entities, each of which is independently correct in its respective context. For example, in the example in Table 2 each span is independently correct in the context of a particular national market.\n• 4. COMPLEX: The question is complex (made up of multiple sub-parts), and each span is an answer to a different sub-part, the internal logic of which is not enumeration. For example, in the example in Table 2, the two spans are independent answers to the two subquestions in the original question. • 5. SHARED STRUCTURE: Spans are enumer-\nated in the form of a syntactically-coordinated structure, sharing either a modifier or a head (i.e. the first word(s) of the first span or last word(s) of the last span). For example, in the example in Table 2, the three spans share the syntactic head bus service, and the full answer is equivalent to scheduled bus service + fixed-route regional bus service + commuter bus service."
    }, {
      "heading" : "3.4 Dataset Statistics",
      "text" : "The annotation was performed by three trained annotators with an average annotation time of 70 seconds per instance. To test the inter-annotator agreement (IAA), we randomly selected 100 instances for each pairing of the three annotators to anntotate. The same annotation (of all spans) of an instance is considered as an agreement, and any difference in one instance is considered as a disagreement. The average pairwise IAA is 0.86 for answer spans and 0.94 for answer structures (both based on macroaveraged exact match F1 score), with some disagreements between CONJUNCTION and MULTIPART-DISJUNCTION (NON-REDUNDANT). To better understand the composition of MultiSpanQA, we compare our annotations with those in NQ, and provide some basic statistics. Compared to the original annotations in NQ, the annotators rejected 3.1% of instances, re-wrote the question for 5.6% of instances, and modified the answer span annotations for 22% of instances.\nMultiSpanQA contains 6,465 instances with 5,173 for training, 646 for validation, and 646 for test. Table 3 provides the distribution of the number of answer spans in the dataset, from which we see the number of spans ranges from 2 to 21, but 80% of instances contain 2 or 3 spans, and only about 1% of instances contain more than 9 spans."
    }, {
      "heading" : "3.5 Dataset Expansion",
      "text" : "In its basic form, the MultiSpanQA dataset contains only multiple-span answers, and the correct answer can always be located in the passage (in the form of multiple answer spans). However, in a real-word QA scenario, single-span answer questions and unanswerable questions (i.e. the answer is not contained in the passage) would realistically exist. To create a more realistic and challenging variant of the dataset, we add a comparable number of single-span question–answer pairs and unanswerable instances to MultiSpanQA, by randomly sampling from NQ and applying the same preprocessing. The total size of the expanded dataset is 19,395 instances (three times the basic version, partitioned similarly to the basic version)."
    }, {
      "heading" : "4 Models",
      "text" : "Formally, given a question and passage pair <q,p>, the task of multi-span QA involves finding all answer spans s1, s2, ...sn, which are neither duplicated nor overlap with each other, as well as predict the answer structures."
    }, {
      "heading" : "4.1 Baselines",
      "text" : "Single-span Baseline Because most existing reading comprehension datasets only have singlespan answers, single-span architectures are widely used in reading comprehension research. Usually, a pre-trained model is used to encode the question and passage, and output a contextualised representation for all input tokens. Then two feed-forward networks are used to compute a score for each token which indicates whether the token is the start or end of the answer. Finally, a softmax layer followed by an argmax function is used to produce the start and end positions of the answer.\nTo make MultiSpanQA trainable for a singlespan architecture, we experimented with two preprocessing methods, and created two baselines accordingly:\n1. Mark the start of the answer as the start position of the first answer span and mark the end of the answer as the end position of the last answer span. In this way, the model can learn to find the shortest span that includes all answer spans. We select the best prediction for evaluation. 2. Suppose an instance has n answer spans, we replace the instance with n instances, one for each span with a single-span answer.\nIn this way, we can apply single-span answer models to our dataset.\nFor evaluation, to enable multi-span prediction, we output the 20 highest-scoring predictions, and tune a threshold t to select the answer spans with a confidence score larger than t that optimises performance on the training set. We remove overlapping predictions based on confidence scores, rejecting predictions with lower confidence scores. Note that for both baselines, we apply the pre-processing to the training data only.\nSequence Tagging Baseline Following Segal et al. (2020), we cast question answering as a sequence tagging task, predicting for each token whether it is part of an answer. In our experiments, we use the popular IOB tagging scheme to mark\nanswer spans in the passage where B denotes the first token of an answer span, I denotes subsequent tokens within a span, and O denotes tokens that are not part of an answer span."
    }, {
      "heading" : "4.2 Proposed Model",
      "text" : "By investigating the failures of sequence tagging baseline, we find there is an issue that the global information is hard to be captured during tagging. For example, the number of answer spans may be specified in the question, but cannot be imposed as a constraint on the tagger. To better use such global information, we propose a span number predictor, an answer structure predictor, and a span adjustment module (as in Figure 2), which can be combined with any on-the-fly sequence tagger (encoder).\nGiven a pair of question q and passage p, we first encode the question and context together using an sequence pair encoder as:\nH = Encoder(< q, p >) ∈ Rl×h (1)\nwhere H = [H[CLS];Hq;Hp] is the contextualised token representation of all input tokens with a pooled global token [CLS], h is the hidden-layer size, and l is the input length.\nAfter encoding, we fetch the hidden states of the context tokens and input them to a linear classifier to perform a preliminary answer span prediction,\nas:\nUp = FFN(Hp) ∈ Rlp×t (2) Op = CRF (Up) ∈ Rlp×t (3)\nwhere lp denotes the length of the passage, and t denotes the number of labels (t = 3 for the IOB tagging scheme).\nWe then fetch the hidden state of [CLS] token H[CLS] and input it to two feed-forward networks to predict the number of answer spans and the answer structure, respectively, as below:\nPnum = FFN(H[CLS]) (4)\nPstructure = FFN(H[CLS]) (5)\nWe use cross-entropy loss for answer span and structure prediction, and mean-square loss for span number regression. For training, we use the weighted sum of the three losses:\nL = Lspans + λ1Lnum + λ2Lstructure (6)\nFinally, a span adjustment module is used to explicitly combine the predicted span number with the span texts. We first assign a confidence score to each label of the preliminary classification using a softmax layer:\nαconf = softmax(Up) ∈ Rlp×t (7)\nThe confidence of a predicted answer span ai is defined as the maximum confidence of the tokens within ai. Suppose there are k spans that been tagged as answers and the predicted number of span is n, if n < k, we rank the predicted spans by confidence score, and keep the top-n answer spans as answers."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "For all baselines and our model, we use the HuggingFace implementation of BERTBase (Wolf et al., 2019; Devlin et al., 2019) as our encoder with max_sequence_length = 384 and doc_stride = 128 to deal with long passages. For training, we use the BertAdam optimizer with default hyperparameters and learning rate of 5e-5. All models are trained with a batch size of 8 for 3,000 steps. We use a two-layer feed-forward network with a ReLu activation function for all linear layers."
    }, {
      "heading" : "5.2 Evaluation Metrics",
      "text" : "We evaluate in terms of exact match and partial match performance, where an exact match occurs when a prediction fully matches one of the groundtruth answers, and the F1 score is computed by treating the predicted and ground-truth answer spans as a set of spans. By taking positional information into account, a partial match occurs when a prediction overlaps with a ground-truth span.\nWe use micro-averaged precision, recall, and F1 score for evaluation based on the standard formulation of Precision = TP/(TP + FP ), Recall = TP/(TP + FN), and F1 = 2 ∗ Precision ∗Recall /(Precision+Recall), where TP (True Positive) is the number of answer spans correctly predicted by the model, FP (False Positive) is the number of spans incorrectly predicted by the model, and FN (False Negative) is the number of answer spans not predicted by the model. In the case of an unanswerable question with the expanded dataset, we use a virtual span which indicates no answer. For answer structure prediction, we use accuracy to evaluate the model performance."
    }, {
      "heading" : "5.3 Results and Analysis",
      "text" : "Table 4 shows the dev set results on MultiSpanQA, where the left part is results on multi-span questions\nonly (the basic dataset), and the right part is the results on the expanded dataset (including singlespan answers and unanswerable questions).\nSingle-span model From the table, we see that single-span (v1) gets very low exact match scores but higher overlapping scores (compared to exact match), as it is trained to find a single long span that overlaps with all answer spans. By comparison, single-span (v2) improves the exact match scores on the basic dataset because it is trained on independent single-span answers. An interesting finding is that single-span (v2) does not drop in the overlapping scores, which seems to be because the tuned threshold t (see Section 4.1) works in this setting. The overall performance of the single-span baselines is relatively low, simply because the models can only predict a single-span answer, which is incompatible with the MultiSpanQA dataset.\nSequence tagging model Compared to the single-span baselines, the sequence tagging models perform much better. Without changing the encoder, there is an improvement of more than 30 absolute points on the exact match metrics, and about 3 for the overlapping F1 metric. Performance is boosted using joint training with span number prediction and answer semantics prediction. Our\nproposed model achieves the best F1 score in most settings.\nAnother interesting finding is that single-span models usually attain higher precision, while sequence tagging models attain higher recall. This demonstrates that single-span models are more accurate in the single-span answer they predict, while sequence tagging models predictably tend to make more predictions.\nComparing the two datasets Comparing results on the two datasets, we see that single-span baselines are boosted over the expanded dataset (where we add single-span answers and unanswerable questions), as single-span answers are more tractable for these simpler models. The relative improvements for sequence tagging models are more modest, but they still have a clear advantage over the single-span baselines.\nDifficulty analysis To explore the difficulty of the MultiSpanQA dataset, we report the dev set results categorised by answer type in Table 5 and categorised by the number of spans in Table 6. From the answer type perspective, the model performs best on HUMAN questions, followed by OTHER ENTITY and LOCATION (largely following the natural distribution of the respective classes in the dataset). There is quite a drop for the NUMERIC class, and a big drop again for the DESCRIPTION class, which was also the class our annotators found most difficulty with.\nFrom the perspective of the number of spans, the model performs best on questions with many (> 7) answers. We think this is because the answers are usually a list of spans with similar semantics, often structured as a simple coordination. The performance drops as the answer number decreases because the syntactic pattern in which answer spans occurs is less predictable.\nAnswer Semantics From the answer type perspective, LOCATION answers usually have easily predictable structure, while the structure of NUMERIC answers is the most difficult to predict. From the perspective of the number of spans, answers consisting of 4–7 spans are relatively easy to predict and there is no significant difference between answers contain few (2 or 3) spans or many (> 7) spans. Figure 3 shows the confusion matrix of the answer structure predictions. We can see that our model tends to predict CONJUNCTION and NON-REDUNDANT, and there are no REDUNDANT\nand SHARE predictions. The overall answer structure accuracy is 84.38%, which is slightly higher than the proportion of CONJUNCTION (the majority class) in the dataset. This suggests that directly applying a simple feedforward network to the pooled encoder output is ineffective for answer semantics prediction, and that this should be an area for future model refinement."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present MultiSpanQA, a reading comprehension dataset where answers consist of multiple discrete spans. As part of this, we proposed a method for classifying the semantic structure of answers, based on the semantic relation between answer spans. We also provide an expanded version of the dataset which includes unanswerable questions and single-answer questions, to make it both more challenging and more realistic. We additionally presented a number of models for multi-span QA extraction, and found that the best-performing model was sequence tagging-based, augmented by a span number prediction module and span adjustment module."
    } ],
    "references" : [ {
      "title" : "Embracing data abundance: Booktest dataset for reading comprehension",
      "author" : [ "Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst." ],
      "venue" : "CoRR, abs/1610.00956.",
      "citeRegEx" : "Bajgar et al\\.,? 2016",
      "shortCiteRegEx" : "Bajgar et al\\.",
      "year" : 2016
    }, {
      "title" : "A thorough examination of the CNN/daily mail reading comprehension task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Quac: Question answering in context",
      "author" : [ "Eunsol Choi", "He He", "Mohit Iyyer", "Mark Yatskar", "Wentau Yih", "Yejin Choi", "Percy Liang", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Listreader: Extracting list-form answers for opinion questions",
      "author" : [ "Peng Cui", "Dongyao Hu", "Le Hu." ],
      "venue" : "CoRR, abs/2110.11692.",
      "citeRegEx" : "Cui et al\\.,? 2021",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2021
    }, {
      "title" : "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning",
      "author" : [ "Pradeep Dasigi", "Nelson F. Liu", "Ana Marasovic", "Noah A. Smith", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Dasigi et al\\.,? 2019",
      "shortCiteRegEx" : "Dasigi et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North Ameri-",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "Searchqa: A new q&a dataset augmented with context from a search engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V. Ugur Güney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "CoRR, abs/1704.05179.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "ELI5: long form question answering",
      "author" : [ "Angela Fan", "Yacine Jernite", "Ethan Perez", "David Grangier", "Jason Weston", "Michael Auli." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, Florence, Italy, pages 3558–",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomás Kociský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Annual Conference on Neural Information Processing Systems, Montreal, Canada,",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "4th International Conference on Learning Representations, San Juan, Puerto Rico.",
      "citeRegEx" : "Hill et al\\.,? 2016",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "A multi-type multi-span network for reading comprehension that requires discrete reasoning",
      "author" : [ "Minghao Hu", "Yuxing Peng", "Zhen Huang", "Dongsheng Li." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Van-",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "The narrativeqa reading comprehension challenge",
      "author" : [ "Tomás Kociský", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gábor Melis", "Edward Grefenstette." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:317–328.",
      "citeRegEx" : "Kociský et al\\.,? 2018",
      "shortCiteRegEx" : "Kociský et al\\.",
      "year" : 2018
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:452–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Xin Li", "Dan Roth." ],
      "venue" : "19th International Conference on Computational Linguistics.",
      "citeRegEx" : "Li and Roth.,? 2002",
      "shortCiteRegEx" : "Li and Roth.",
      "year" : 2002
    }, {
      "title" : "MS MARCO: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "Proceedings of the Workshop on Cognitive Computation: Inte-",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "HAS-QA: hierarchical answer spans model for open-domain question answering",
      "author" : [ "Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Lixin Su", "Xueqi Cheng." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, Honolulu, USA, pages 6875–",
      "citeRegEx" : "Pang et al\\.,? 2019",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2019
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia, pages 784–789.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, USA, pages 2383–",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Coqa: A conversational question answering challenge",
      "author" : [ "Siva Reddy", "Danqi Chen", "Christopher D. Manning." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:249–266.",
      "citeRegEx" : "Reddy et al\\.,? 2019",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2019
    }, {
      "title" : "Edge and curve detection for visual scene analysis",
      "author" : [ "Azriel Rosenfeld", "Mark Thurston." ],
      "venue" : "IEEE Transactions on Computers, 20(5):562–569.",
      "citeRegEx" : "Rosenfeld and Thurston.,? 1971",
      "shortCiteRegEx" : "Rosenfeld and Thurston.",
      "year" : 1971
    }, {
      "title" : "A simple and effective model for answering multi-span questions",
      "author" : [ "Elad Segal", "Avia Efrat", "Mor Shoham", "Amir Globerson", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3074–",
      "citeRegEx" : "Segal et al\\.,? 2020",
      "shortCiteRegEx" : "Segal et al\\.",
      "year" : 2020
    }, {
      "title" : "brat: a web-based tool for NLP-assisted text annotation",
      "author" : [ "Pontus Stenetorp", "Sampo Pyysalo", "Goran Topić", "Tomoko Ohta", "Sophia Ananiadou", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of the Demonstrations at the 13th Conference of the European Chap-",
      "citeRegEx" : "Stenetorp et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Stenetorp et al\\.",
      "year" : 2012
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Wikiqa: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, pages 2013–2018.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : ", 2019), multi-hop reasoning (Yang et al., 2018), and discrete operations over the content of paragraphs (Dua et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : ", 2018), and discrete operations over the content of paragraphs (Dua et al., 2019).",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "Extractive QA Most existing extractive QA datasets such as SQuAD (Rajpurkar et al., 2016), SQuAD2.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : ", 2018), SearchQA (Dunn et al., 2017), and QuAC (Choi et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : ", 2017), and QuAC (Choi et al., 2018) restrict the answer passage to a single span of text.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "TriviaQA (Joshi et al., 2017) and HothopQA (Yang et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 26,
      "context" : ", 2017) and HothopQA (Yang et al., 2018) extend the answer context from single passage to multiple passages, while HotpotQA further requires reasoning over multiple passages to answer the question.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "DROP (Dua et al., 2019) requires systems to resolve (possibly multiple) references in a question, and perform discrete operations (such as addition, sorting, or counting) over them.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "MASH-QA (Dua et al., 2019) extends the answer space to texts that span across a longer document, but this dataset is highly domain-specific, in the healthcare domain.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Quoref (Dasigi et al., 2019) and Natural Questions (“NQ”: Kwiatkowski et al.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "WikiHowQA and WebQA (Cui et al., 2021) both focus on non-factoid (e.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : ", 2015) and MS Marco (Nguyen et al., 2016) are two open-domain generative QA datasets, where answers in WikiQA are mostly sentences from the answer passage, while answers in MS Marco are free-form sentences generated by crowd workers.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "NarrativeQA (Kociský et al., 2018) is a dataset of movie and book summaries.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : ", 2017), ELI5 (Fan et al., 2019), and CoQA (Reddy et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : ", 2019), and CoQA (Reddy et al., 2019) are three multiple-document datasets.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "Cloze style Cloze datasets such as CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : ", 2015), Children’s Book Test (CBT) (Hill et al., 2016), and BookTest (Bajgar et al.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : ", 2016), and BookTest (Bajgar et al., 2016) require systems to predict a missing word from a passage.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "However, researchers have shown that this task is artificial, and can be largely solved with simple methods and relatively little reasoning (Chen et al., 2016).",
      "startOffset" : 140,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "(2020) first proposed to treat multi-span QA as a sequence tagging task, in the form of a multi-head architecture (Dua et al., 2019) to perform arithmetic operations between the predicted spans.",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "(2019) applied the non-maximum suppression (NMS) algorithm (Rosenfeld and Thurston, 1971) to prune redundant",
      "startOffset" : 59,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "sifier trained on the TREC Question Classification dataset (Li and Roth, 2002).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "We (re-)annotated all the data using the Brat annotation tool (Stenetorp et al., 2012).",
      "startOffset" : 62,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "For all baselines and our model, we use the HuggingFace implementation of BERTBase (Wolf et al., 2019; Devlin et al., 2019) as our encoder with max_sequence_length = 384 and doc_stride = 128 to deal with long passages.",
      "startOffset" : 83,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "For all baselines and our model, we use the HuggingFace implementation of BERTBase (Wolf et al., 2019; Devlin et al., 2019) as our encoder with max_sequence_length = 384 and doc_stride = 128 to deal with long passages.",
      "startOffset" : 83,
      "endOffset" : 123
    } ],
    "year" : 0,
    "abstractText" : "Most existing reading comprehension datasets focus on single-span answers, which can be extracted as a single contiguous span from a given text passage. Multi-span questions, i.e., questions whose answer is a series of multiple discontiguous spans in the text, are common in real life but are less studied. In this paper, we present MultiSpanQA, a new dataset that focuses on questions with multi-span answers. Raw questions and contexts are extracted from the Natural Questions (Kwiatkowski et al., 2019) dataset. After multi-span re-annotation, MultiSpanQA consists of over a total of 6,000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multispan answers in the expanded version. We introduce new metrics for the purposes of multispan question answering evaluation, and establish several baselines using advanced models. Finally, we propose a new model which beats all baselines and achieves the state-of-the-art on our dataset. Dataset and code will be released on acceptance.",
    "creator" : null
  }
}
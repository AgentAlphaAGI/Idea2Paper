{
  "name" : "ARR_2022_66_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards Job-Transition-Tag Graph for a Better Job Title Representation Learning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The learning of job title representation has received much attention in the recruitment field because the learned representation is beneficial to various tasks, such as job recommendation (Dave et al., 2018; Liu et al., 2019b), job title benchmarking (Zhang et al., 2019), and job mobility prediction (Zhang et al., 2021). However, in practice, learning a good representation is challenging for the following reasons: (i) Noisy data: job title data is noisy due to personal subjective reasons (i.e., spelling errors) or objective reasons (i.e., resume parsers are not perfect). (ii) Messy data: job titles are messy because people have different ways of thinking, and naming conventions vary by company and industry. For example, there are many alternative job titles for the same position, e.g., “purchasing clerk” and “buyer”. Another problem is that due to the ambiguity of certain terms, they can refer to different positions in different contexts, e.g., “registered nurses sandwich rehab” and “sandwich maker”. For these reasons, standard semantic-based approaches that\naggregate (e.g., mean or sum) word representations to get job title semantic representation may lead to mismatches. Moreover, these methods ignore hidden relationships between job titles, e.g., titles in the same resume may be similar. (Dave et al., 2018; Zhang et al., 2019) learn representations from graphs. They create graphs from career trajectories, where nodes represent job titles, edges represent job transitions. Then they design different loss functions to embed the nodes into a lowdimensional space. However, the generated graphs are usually sparse due to the above reasons, limiting the performance of graph-based methods. Standardizing job titles before generating graphs can alleviate the sparsity issue to a certain extent, but at the cost of losing some information. To tackle these challenges, we propose to enrich graphs with structured contextual information and learn job title representations through network embedding methods. Specifically, inspired by domain-specific Named Entity tags (i.e., RESponsibility and FUNction) proposed in (Liu et al., 2019a), we treat the job title as a combination of responsibilities, functionalities, and other additional information. Words related to responsibility and functionality are defined as tags. We assume that job titles with the same tag describe similar job functions or responsibilities, so they are more likely to have similar representations. Along this line, we construct Job-Transition-Tag Graph, a heterogeneous graph containing two types of nodes, i.e., job titles and tags, which carries more information, thereby alleviating the sparsity problem."
    }, {
      "heading" : "2 Methodology",
      "text" : ""
    }, {
      "heading" : "2.1 Preliminaries",
      "text" : "A graph/network is represented as G = (V, E), with node set V and edge set E . Nodes and edges can optionally have a type, so a graph can be homogeneous or heterogeneous. In the recruitment field, the career trajectory of talents can be repre-\nsented by graphs. Formally, consider a job seeker set U and their working history set H = {Hu}u∈U , where the working history of each u is represented as a sequence of n work records ordered by time Hu = {J1, . . . , Jn}. The i-th record Ji is denoted by (ji, pi), indicating that u is engaged in a position (titled ji) during the pi period. The set of job titles ji that occurred in H is denoted as J . Based on H, Job-Transition Graph (Figure 1a) can be constructed, which is formally defined as:\nDefinition 1 (Job-Transition Graph) is defined as a directed homogeneous graph Gjj = (J , Ejj) generated from H, where J is a set of job titles, and the edge ejjxy ∈ Ejj represents the job transition from the former job jx to the next job jy."
    }, {
      "heading" : "2.1.1 Learning from Job-Transition Graph:",
      "text" : "An Overview\nGjj is often used for job title representation learning tasks. The current procedure is to first build a Gjj , and then learn job title representation from it. More specifically, (Dave et al., 2018) first builds Gjj and other two graphs. Then, the Bayesian personalized ranking and margin-based loss functions are used to learn job title representations from graphs. Job2Vec (Zhang et al., 2019) constructs a Gjj , where the node denotes job title affiliated with the specific company, and a multi-view representation learning method is proposed. (Zhang et al., 2021) adds company nodes in Gjj to build a heterogeneous graph. Then they use a graph neural network to represent the nodes. As mentioned above, the job title and job transition data are messy. Therefore, Gjj may be sparse (Zhang et al., 2019). To alleviate this issue, a simple method is to standardize job titles. For example, (Dave et al., 2018) normalizes titles by using Carotene (Javed et al., 2015), (Zhang et al., 2019) aggregates titles by filtering out low frequency words, and (Zhang et al., 2021) unifies titles according to IPOD (Liu et al., 2019a). However, the standardization of job titles may lose some specific information. Furthermore, these methods either ignore the semantic information contained in job titles (Dave et al., 2018; Zhang et al., 2021) or separate the semantic information from the graph topology (Zhang et al., 2019)."
    }, {
      "heading" : "2.1.2 Job Title Composition",
      "text" : "Generally, a job title consists of three parts (Liu et al., 2019a; Zhang et al., 2019): (i) Responsibility: describes the role and responsibility of a position from different levels (e.g., director, assistant,\nand engineer). (ii) Functionality: describes the business function of a position from various dimensions (e.g., sales, national and security). (iii) Additional Information: contains personal-specific information. We denote the words related to responsibility and functionality as tags, and they form a tag set T . These tags are the essence of the job title and provide important information about the position. However, few works directly include this information in the representation learning scheme. In this paper, we consider these tags when generating graphs. These tags can alleviate the graph sparsity problem of Gjj and provide additional information for the learning of job title representations."
    }, {
      "heading" : "2.2 Proposed Graphs",
      "text" : "In order to address the sparsity issue of Gjj , we consider adding more information when generating graphs, i.e., tags related to job responsibilities or functions, driven by the job title composition. Along this line, we define various types of graphs:\nDefinition 2 (Enhanced Job-Transition Graph) is based on Gjj with additional enhanced edges. It is defined as GjjE = (J , Ejj ∪ E jj E ), where E jj E is a set of enhanced edges. More specifically, if jx and jy share a word w, then we add a bi-directional edge between them, i.e., ejjxy and e jj yx.\nAs shown in Figure 1b, “purchasing manager” shares the tag “purchasing” with “purchasing clerk”, so we add an enhanced edge (i.e., the red dashed line) between them.\nDefinition 3 (Job-Tag Graph) is a heterogeneous graph Gjt = (J ∪ T , Ejt), with job titles and tags, two node types. Ejt is a set of bi-directional edges between a job title and a tag, representing the “has/in” relationship.\nAn example of the “has/in” relationship is given in Figure 1c (i.e., the green line), where the job title “automotive technician” has a tag “automotive”, and “automotive” is in “automotive technician”. In order to aggregate more information, we further combine Job-Transition Graph and Job-Tag Graph to build Job-Transition-Tag Graph:\nDefinition 4 (Job-Transition-Tag Graph) is a heterogeneous graph Gjtj = (J ∪ T , Ejj ∪ Ejt) with two node types and two edge types.\nInspired by the achievements of network embedding models in the node representation learning problem (Hamilton et al., 2017), we apply differ-\nent network embedding models to learn job title representation from the graphs defined above."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "CareerBuilder12 (CB12): an open dataset from a Kaggle competition. 1 It contains a collection of working experiences represented by sequences of job titles. For the node classification task, we use AutoCoder 2 to assign a SOC 2018 to each job title. The labeling details are given in Appendix A.1. Randstad: a private French resume dataset provided by Randstad company, where each resume is parsed into multiple sections, an example is given in Figure 3 of Appendix. Graphs are built from EmploymentHistory section.\nFor both datasets, we use the Top-200 tokens that appear most frequently in job titles and belong to IPOD (Liu et al., 2019a) as tags. The details of tag generation are given in Appendix A.3. We assign the one-hot encoding of the corresponding title for\n1https://www.kaggle.com/c/ job-recommendation\n2http://www.onetsocautocoder.com/plus/ onetmatch\neach title node as the node feature. The vocabulary set is obtained by filtering words with a frequency of 1 from the tokenized job titles. Statistics for datasets and graphs are summarized in Table 1. We can observe that Gjj (i.e., |V| and |Ejj |) are sparse."
    }, {
      "heading" : "3.2 Experimental Settings",
      "text" : "We evaluate the proposed job title representation learning scheme through (i) a node classification task (i.e., job title classification) and (ii) a link prediction task (i.e., next-job prediction). For job title classification, we randomly split the data into training/validation/test sets with a ratio of 60%/20%/20%. We keep the same split ratio on positive/negative edges for next-job prediction, where negative edges are randomly picked from unconnected node pairs (i.e., the same size as positive edges). The baselines used are listed below:\n• Homogeneous: Node2Vec (N2V) (Grover and Leskovec, 2016), GCN (Kipf and Welling, 2016) and GAT (Veličković et al., 2017). • Heterogeneous: Metapath2Vec (M2V) (Dong et al., 2017), RGCN (Schlichtkrull et al., 2018) and HAN (Wang et al., 2019b). • Semantic-based: Word2Vec (W2V) (Le and Mikolov, 2014) and BERT (Devlin et al., 2018).\nFor detailed parameter settings and baseline descriptions, see Appendix A.5 and A.4."
    }, {
      "heading" : "3.3 Results",
      "text" : ""
    }, {
      "heading" : "3.3.1 Job Title Classification",
      "text" : "Table 2 summarizes the best results of all methods on different graphs. We have the following obser-\nvations: (i) Among all graphs, all models generally have the lowest scores on Gjj because this graph is often sparse and can only provide limited information. (ii) All models perform better on GjjE (except Macro-F1 of GCN) than Gjj , which shows that the enhanced edges provide additional information. (iii) The heterogeneous models perform well on our proposed Gjtj , which indicates that the added tag nodes can effectively improve the quality of representation. We did not apply homogeneous methods to Gjtj , but results on GjjE prove that the information given by tags is useful. (iv) BERT has a better performance than W2V, but graph neural networks generally outperform semantic-based methods."
    }, {
      "heading" : "3.3.2 Next-Job Prediction",
      "text" : "We further evaluate the learning scheme using nextjob prediction, which can be viewed as a link prediction task to predict whether a position will be recommended as the next job. The results on CB12 are shown in Table 3. BERT performs best, followed by HAN on Gjtj . However, when we use the dot product used in HAN to obtain edge features for BERT, the AUC drops to 0.476, while W2V drops to 0.557. We will discuss such results in future work. Overall, the results of link prediction also demonstrate the effectiveness of our method."
    }, {
      "heading" : "3.3.3 Visualization",
      "text" : "For a more intuitive comparison, we visualize the learned representations in Figure 2, and each color corresponds to an occupation category 3. Overall,\n3Healthcare support (green), Healthcare practitioners and technical (blue), Architecture and engineering (purple), Office\nthe representations learned by HAN on all graphs are clustered into groups. However, when considering tags, representations are easier to be subdivided further in each category. For example, in Figure 2d, the orange occupation can be further divided into three sub-clusters, which proves that adding tag nodes can help capture more detailed information and make the learned representation more informative. This detailed information helps to classify positions further because we only classify job titles into the root category in this work."
    }, {
      "heading" : "4 Conclusion",
      "text" : "This paper first proposes to enrich Job-Transition Graph commonly used in job title representation learning tasks by adding tag nodes and then learn job title representations through network embedding methods. This graph can alleviate the sparsity problem, thereby improving the quality of learned representations, as demonstrated by node classification and link prediction tasks on two datasets. Future research lines will focus on learning from weighted graphs and improving the tag generation.\nand administrative support (orange) and Transportation and material handling (red)."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Job Title Label Assignment In the original working experience dataset provided by CareerBuilder12, job titles are not prelabeled. Therefore, for the job title classification task, we use an online third-party API O*Net-SOC AutoCoder 4 to assign a Standard Occupation Classification code (SOC) 2018 to each job title, as well as a match score (i.e., scores above 70 means that the correct code is accurately predicted at least 70% of the time). SOC 2018 is a four-level taxonomy structure, including MajorGroup (23), MinorGroup (98), BroadGroup (459) and DetailedOccupation (867). For example, O*Net-SOC AutoCoder assigns the code 11-2022 (Sales Managers) for the title “sales director”, which belongs to the level of DetailedOccupation. 11-2020 (Marketing and Sales Managers) is BroadGroup level, 11-2000 (Advertising, Marketing, Promotions, Public Relations, and Sales Managers) is MinorGroup level, and 11-0000 (Management Occupations) is MajorGroup level. In this work, we categorize job titles into MajorGroup. We have annotated a total of 30,000 job titles. The developer guarantees that the code assigned to the title plus description has an accuracy rate of 85%. However, only the job title is provided in our experiments, so the SOC 2018 code may be incorrectly assigned. For this reason, we filtered out job titles with scores below 70. Therefore, 22,590 job titles remain.\nA.2 Randstad Data Description Figure 3 shows an example of parsed resume in Randstad dataset. We build graphs from EmploymentHistory, which contains a JobTitle, and its corresponding occupation labels (i.e.,JobCode, JobGroup and JobClass). The hierarchical taxonomy structure used in the Randstad dataset has a threelevel hierarchy, where JobCodes are leaf classes, and each internal (JobGroup)/root class (JobClass) is the aggregation of all its descendant classes. There are 25 JobClasss, 295 JobGroups and 4,443 JobCodes, respectively. In this work, we categorize job titles into JobClass.\nA.3 Tag Generation For both datasets, we first tokenize titles into tokens and remove stopwords, numbers, and punctuation. The word frequency distribution of words\n4http://www.onetsocautocoder.com/plus/ onetmatch\nin two datasets are shown respectively in Figure 4, which are subject to the long-tail distribution, similar to the observation in (Zhang et al., 2019). Most words appear only once, i.e., 53.55% of words only appear one time in CB12 dataset, and this ratio is 56.55% in Randstad dataset. Figure 4 further shows the top ten and last ten frequent words in each dataset. Obviously, high-frequency words like “manager” and “sales” describe the responsibility or functionality of the job title, while low-frequency words are usually noise or personspecific words. Based on the domain-specific NE tags (i.e., RESponsibility, FUNction) proposed in IPOD (Liu et al., 2019a), we then select the Top200 tokens that appear most frequently and appear in the IPOD NE tag set as tags for each dataset.\nA.4 Baseline Description\nWe explore the following network embedding methods on our proposed graphs to learn job title representation. According to the type of graph, the network embedding methods are naturally divided into Homogeneous and Heterogeneous. Then, we further categorize each category into Unsupervised and Semi-Supervised according to whether node\nlabels are provided for learning. Homogeneous&Unsupervised\n• Node2Vec (N2V) (Grover and Leskovec, 2016): is an extension of DeepWalk with a biased random walk process for neighborhood exploration.\nHomogeneous&Semi-supervised:\n• GCN (Kipf and Welling, 2016): is a semisupervised GNN that generalizes the convolutional operation to homogeneous graphs. • GAT (Veličković et al., 2017): uses a selfattention strategy to learn the importance between a node and its neighbors.\nHeterogeneous&Unsupervised:\n• Metapath2Vec (M2V) (Dong et al., 2017): performs meta-path-guided walks and utilizes SkipGram to embed heterogeneous graphs.\nHeterogeneous & Semi-supervised:\n• RGCN (Schlichtkrull et al., 2018): is an extension of GCN on heterogeneous graphs, introducing relation-specific transformations based on the type of edges. • HAN (Wang et al., 2019b): proposes a hierarchical attention mechanism, i.e., node-level and semantic-level for heterogeneous graphs.\nIn addition to the comparison between network embedding methods, we will also compare the representation learned through graphs with the representation obtained by semantic-based methods.\nSemantic-based:\n• Word2Vec (W2V) (Le and Mikolov, 2014): the representation of a job title is obtained by averaging word vectors in it. We use word vectors trained on Google News 5 for CB12, and a pretrained French embedding model (Fauconnier, 2015) for Randstad. • BERT (Devlin et al., 2018): the job title representations are obtained by using bert-as-service package (Xiao, 2018). BERT-Base-Uncased is used for CB12, and BERT-Base-MultilingualCased (New) for Randstad.\nA.5 Parameter Settings Our implementation is based on the PyTorch version of the DGL package (Wang et al., 2019a). For job title classification, we randomly split the\n5https://code.google.com/archive/p/ word2vec/\ndata into training/validation/test sets with a ratio of 60%/20%/20%. We keep the same split ratio on positive/negative edges for next-job prediction, where negative edges are randomly picked from unconnected node pairs (i.e., the same size as positive edges). To ensure fairness, we keep the same data split for both methods. Each semi-supervised model was trained on the training set, and the parameters were optimized on the validation set. The final performance was evaluated on the test set. Models are optimized with the Adam (Kingma and Ba, 2014) with a learning rate of 1e-3, and we apply L2 regularization with value 5e-4. We use an early stop with a patience of 100, i.e., if the validation loss does not decrease in 100 consecutive epochs, we stop training. For models applying the attention mechanism, the dropout rate of attention is set to 0.2. For random-walk-based methods include N2V and M2V, we set the window size to 5, walk length to 10, walks per node to 50, the number of negative samples to 5. For M2V, we test all meta-paths and report the best performance. For a fair comparison, we set the dimension of node embedding to 128 for all the above methods, except for W2V. In both job title classification and next-job prediction, for unsupervised methods, node representations are learned from the entire dataset. The logistic regression classifier is then trained on both the training and validation sets. Edge features are represented by applying binary operators (Grover and Leskovec, 2016) on pairs of nodes. We use dot product for all semi-supervised methods, and for unsupervised methods, we choose the best operator based on the validation set. We repeat each prediction experiment ten times and report the average performance scores (i.e., Macro-F1 and Micro-F1 for job classification and AUC for next-job prediction)."
    } ],
    "references" : [ {
      "title" : "A combined representation learning approach for better job and skill recommendation",
      "author" : [ "Vachik S Dave", "Baichuan Zhang", "Mohammad Al Hasan", "Khalifeh AlJadda", "Mohammed Korayem." ],
      "venue" : "Proceedings of the 27th ACM International Conference on Informa-",
      "citeRegEx" : "Dave et al\\.,? 2018",
      "shortCiteRegEx" : "Dave et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "metapath2vec: Scalable representation learning for heterogeneous networks",
      "author" : [ "Yuxiao Dong", "Nitesh V Chawla", "Ananthram Swami." ],
      "venue" : "Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 135–",
      "citeRegEx" : "Dong et al\\.,? 2017",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2017
    }, {
      "title" : "node2vec: Scalable feature learning for networks",
      "author" : [ "Aditya Grover", "Jure Leskovec." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864.",
      "citeRegEx" : "Grover and Leskovec.,? 2016",
      "shortCiteRegEx" : "Grover and Leskovec.",
      "year" : 2016
    }, {
      "title" : "Representation learning on graphs: Methods and applications",
      "author" : [ "William L Hamilton", "Rex Ying", "Jure Leskovec." ],
      "venue" : "arXiv preprint arXiv:1709.05584.",
      "citeRegEx" : "Hamilton et al\\.,? 2017",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2017
    }, {
      "title" : "Carotene: A job title classification system for the online recruitment domain",
      "author" : [ "Faizan Javed", "Qinlong Luo", "Matt McNair", "Ferosh Jacob", "Meng Zhao", "Tae Seung Kang." ],
      "venue" : "2015 IEEE First International Conference on Big Data Computing Service and Ap-",
      "citeRegEx" : "Javed et al\\.,? 2015",
      "shortCiteRegEx" : "Javed et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc Le", "Tomas Mikolov." ],
      "venue" : "International conference on machine learning, pages 1188– 1196. PMLR.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Ipod: An industrial and professional occupations dataset and its applications to occupational data mining and analysis",
      "author" : [ "Junhua Liu", "Yung Chuen Ng", "Kristin L Wood", "Kwan Hui Lim." ],
      "venue" : "arXiv preprint arXiv:1910.10495.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Tripartite vector representations for better job recommendation",
      "author" : [ "Mengshu Liu", "Jingya Wang", "Kareem Abdelfatah", "Mohammed Korayem." ],
      "venue" : "arXiv preprint arXiv:1907.12379.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne Van Den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "European semantic web conference, pages 593–607. Springer.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Lio", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1710.10903.",
      "citeRegEx" : "Veličković et al\\.,? 2017",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep graph library: A graph-centric, highly-performant",
      "author" : [ "Minjie Wang", "Da Zheng", "Zihao Ye", "Quan Gan", "Mufei Li", "Xiang Song", "Jinjing Zhou", "Chao Ma", "Lingfan Yu", "Yu Gai", "Tianjun Xiao", "Tong He", "George Karypis", "Jinyang Li", "Zheng Zhang" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Heterogeneous graph attention network",
      "author" : [ "Xiao Wang", "Houye Ji", "Chuan Shi", "Bai Wang", "Yanfang Ye", "Peng Cui", "Philip S Yu." ],
      "venue" : "The World Wide Web Conference, pages 2022–2032.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "bert-as-service",
      "author" : [ "Han Xiao." ],
      "venue" : "https://github. com/hanxiao/bert-as-service.",
      "citeRegEx" : "Xiao.,? 2018",
      "shortCiteRegEx" : "Xiao.",
      "year" : 2018
    }, {
      "title" : "Job2vec: Job title benchmarking with collective multi-view representation learning",
      "author" : [ "Denghui Zhang", "Junming Liu", "Hengshu Zhu", "Yanchi Liu", "Lichen Wang", "Pengyang Wang", "Hui Xiong." ],
      "venue" : "Proceedings of the 28th ACM International Conference",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Attentive heterogeneous graph embedding for job mobility prediction",
      "author" : [ "Le Zhang", "Ding Zhou", "Hengshu Zhu", "Tong Xu", "Rui Zha", "Enhong Chen", "Hui Xiong." ],
      "venue" : "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The learning of job title representation has received much attention in the recruitment field because the learned representation is beneficial to various tasks, such as job recommendation (Dave et al., 2018; Liu et al., 2019b), job title benchmarking (Zhang et al.",
      "startOffset" : 188,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : "The learning of job title representation has received much attention in the recruitment field because the learned representation is beneficial to various tasks, such as job recommendation (Dave et al., 2018; Liu et al., 2019b), job title benchmarking (Zhang et al.",
      "startOffset" : 188,
      "endOffset" : 226
    }, {
      "referenceID" : 16,
      "context" : ", 2019b), job title benchmarking (Zhang et al., 2019), and job mobility prediction (Zhang et al.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : ", 2019), and job mobility prediction (Zhang et al., 2021).",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "(Dave et al., 2018; Zhang et al., 2019) learn representations from graphs.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "(Dave et al., 2018; Zhang et al., 2019) learn representations from graphs.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : ", RESponsibility and FUNction) proposed in (Liu et al., 2019a), we treat the job title as a combination of responsibilities, functionalities, and other additional information.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "More specifically, (Dave et al., 2018) first builds Gjj and other two graphs.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Job2Vec (Zhang et al., 2019) constructs a Gjj , where the node denotes job title affiliated with the specific company, and a multi-view representation learning method is proposed.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "(Zhang et al., 2021) adds company nodes in Gjj to build a heterogeneous graph.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "For example, (Dave et al., 2018) normalizes titles by using Carotene (Javed et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : ", 2018) normalizes titles by using Carotene (Javed et al., 2015), (Zhang et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : ", 2015), (Zhang et al., 2019) aggregates titles by filtering out low frequency words, and (Zhang et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : ", 2019) aggregates titles by filtering out low frequency words, and (Zhang et al., 2021) unifies titles according to IPOD (Liu et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : ", 2021) unifies titles according to IPOD (Liu et al., 2019a).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, these methods either ignore the semantic information contained in job titles (Dave et al., 2018; Zhang et al., 2021) or separate the semantic information from the graph topology (Zhang et al.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "Furthermore, these methods either ignore the semantic information contained in job titles (Dave et al., 2018; Zhang et al., 2021) or separate the semantic information from the graph topology (Zhang et al.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : ", 2021) or separate the semantic information from the graph topology (Zhang et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Generally, a job title consists of three parts (Liu et al., 2019a; Zhang et al., 2019): (i) Responsibility: describes the role and responsibility of a position from different levels (e.",
      "startOffset" : 47,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "Generally, a job title consists of three parts (Liu et al., 2019a; Zhang et al., 2019): (i) Responsibility: describes the role and responsibility of a position from different levels (e.",
      "startOffset" : 47,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "Inspired by the achievements of network embedding models in the node representation learning problem (Hamilton et al., 2017), we apply differ-",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "For both datasets, we use the Top-200 tokens that appear most frequently in job titles and belong to IPOD (Liu et al., 2019a) as tags.",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "• Homogeneous: Node2Vec (N2V) (Grover and Leskovec, 2016), GCN (Kipf and Welling, 2016) and GAT (Veličković et al.",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "• Homogeneous: Node2Vec (N2V) (Grover and Leskovec, 2016), GCN (Kipf and Welling, 2016) and GAT (Veličković et al.",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "• Homogeneous: Node2Vec (N2V) (Grover and Leskovec, 2016), GCN (Kipf and Welling, 2016) and GAT (Veličković et al., 2017).",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "• Heterogeneous: Metapath2Vec (M2V) (Dong et al., 2017), RGCN (Schlichtkrull et al.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : ", 2017), RGCN (Schlichtkrull et al., 2018) and HAN (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "• Semantic-based: Word2Vec (W2V) (Le and Mikolov, 2014) and BERT (Devlin et al.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "• Semantic-based: Word2Vec (W2V) (Le and Mikolov, 2014) and BERT (Devlin et al., 2018).",
      "startOffset" : 65,
      "endOffset" : 86
    } ],
    "year" : 0,
    "abstractText" : "Works on learning job title representation are mainly based on Job-Transition Graph, built from the working history of talents. However, since these records are usually messy, this graph is very sparse, which affects the quality of the learned representation and hinders further analysis. To address this specific issue, we propose to enrich the graph with additional nodes that improve the quality of job title representation. Specifically, we construct JobTransition-Tag Graph, a heterogeneous graph containing two types of nodes, i.e., job titles and tags (i.e., words related to job responsibilities or functions). Along this line, we reformulate job title representation learning as the task of learning node embedding on the JobTransition-Tag Graph. Experiments on two datasets show the interest of our approach.",
    "creator" : null
  }
}
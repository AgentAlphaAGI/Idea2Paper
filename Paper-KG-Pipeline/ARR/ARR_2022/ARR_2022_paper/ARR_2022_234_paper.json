{
  "name" : "ARR_2022_234_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine learning methods are naturally prone to errors as they typically have to deal with ambiguous and incomplete data during both training and inference. Unreliable predictions hinder the application of such methods in domains, where the price of mistakes is very high, such as clinical medicine. Even in more error-tolerant domains and tasks one would like to achieve a better tradeoff between an expressiveness of a model and its computational performance during inference.\nSince mistakes are inevitable, it is crucial to understand whether model predictions can be trusted or not and abstain from unreliable decisions. Uncertainty estimation (UE) of model predictions aims to solve this task. Ideally, uncertain instances should correspond to erroneous objects and help in misclassification detection. Besides misclassification detection, UE is a crucial component for active learning (Settles, 2012), adversarial attack detection (Lee et al., 2018), detection of out-of-distribution (OOD) instances (Van Amersfoort et al., 2020), etc.\nSome classical machine learning models (e.g. Gaussian processes (Rasmussen, 2003)) have built-in UE capabilities. Modern deep neural networks (DNNs) usually take advantage of a softmax layer, which output can be considered as a prediction probability. However, the softmax probabilities are usually unreliable and produce overconfident predictions (Guo et al., 2017). Some previously proposed techniques such as deep ensemble (Lakshminarayanan et al., 2017) are known for producing good UE scores but require a large additional memory footprint for storing several versions of weights and multiply an amount of computation for conducting several forward passes. Reliable UE of DNNs that does not introduce high computational overhead is an open research question (Van Amersfoort et al., 2020).\nIn this work, we investigate methods for UE of DNNs based on the Transformer architecture (Vaswani et al., 2017) in misclassification detection. We consider two of the most common NLP tasks: text classification and named entity recognition (NER). The latter has been overlooked in the literature on UE. To our knowledge, this work is the first to consider UE for NER.\nWe propose two novel computationally cheap methods for UE of Transformer predictions. The first method is the modification of Monte Carlo dropout with determinantal point process sampling of dropout masks (Shelmanov et al., 2021). We introduce an additional step for making masks more diverse, which helps to achieve substantial improvements on NER and approach the performance of computationallyintensive methods. The second method leverages Mahalanobis distance (Lee et al., 2018) but also adds a spectral normalization of the weight matrix in the classification layer (Liu et al., 2020). This method achieves the best results on most of the datasets and even outperforms computationallyintensive methods. We also investigate recently\nproposed regularisation techniques in combination with other UE methods. The contributions of this paper are the following:\n• We propose two novel computationally cheap modifications of UE methods for Transformer models. The method based on Mahalanobis distance with spectral normalization establishes new state of the art on most of the considered datasets.\n• This work is the first to investigate UE methods on the NER task.\n• We conduct an extensive empirical evaluation, in which we investigate recently proposed regularisation techniques in combination with other methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "It is well known that reliable uncertainty estimates can be obtained simply by constructing an ensemble of decorrelated neural networks (deep ensemble) (Lakshminarayanan et al., 2017). However, such a straightforward approach is coupled with substantial computational and memory overhead during training an ensemble, performing inference of all its components, and storing multiple versions of weights. This overhead is a serious obstacle to deploying ensemble-based uncertainty estimation methods in practice.\nUncertainty estimation is a built-in capability of Bayesian neural networks (Blundell et al., 2015). However, such models have similar issues as ensembles and also require special training procedures. Recently, it was shown by Gal and Ghahramani (2016) that dropout, a wellknown regularization technique, is mathematically equivalent to approximate variational inference in the deep Gaussian process. This method, known as Monte Carlo (MC) dropout, uses Bernoulli approximating the variational distribution on the network units and introduces no additional parameters for the approximate posterior. MC dropout does not introduce any overhead during training and does not require any additional memory. The main disadvantage of this method is that it often requires many forward-pass samplings, which makes it also computationally expensive.\nRecently, many works have investigated the approximate Bayesian inference for neural networks using deterministic approaches Lee et al. (2018); Liu et al. (2020); Van Amersfoort et al.\n(2020); Mukhoti et al. (2021), etc. These methods do not introduce notable overhead for inference, storing weights, and usually require compatible training time. However, most of the research in this area is accomplished for computer vision tasks.\nFor text classification, a series of works investigates UE methods for the OOD detection task (Liu et al., 2020; Podolskiy et al., 2021; Zeng et al., 2021; Hu and Khan, 2021). In this work, we focus on a more challenging task – misclassification detection. While OOD detection requires to model only the epistemic uncertainty inherent to model and caused by a lack of training data, misclassification detection also requires to model aleatoric uncertainty caused by noise and ambiguity in data (Mukhoti et al., 2021). We consider recently proposed methods in this area that are evaluated in text processing.\nThree recent works propose techniques for misclassification detection based on an additive regularisation of a training loss function. Zhang et al. (2019) suggest adding a penalty that reduces the Euclidean distance between training instances of the same class, and increases the distance between instances of different classes. He et al. (2020) suggest using two components in the loss function that reduce the difference between outputs from two versions of a model initialized with different weights. They also use mix-up (Thulasidasan et al., 2019) to generate additional training instance representations that help to capture aleatoric uncertainty, self-ensembling, and a distinctiveness score to measure the epistemic uncertainty. Xin et al. (2021) introduce a regularizer that penalizes overconfident instances with high loss. In another recent work, Shelmanov et al. (2021) propose to combine MC dropout with a Determinantal Point Process (DPP) to improve the diversity of predictions by considering the correlations between neurons and sampling the diverse neurons for activation in a dropout layer.\nIn this work, we conduct a systematic empirical investigation of UE methods on NLP tasks. We evaluate combinations of methods that were not tested before and propose two modifications, one of which achieves the best results on most of the considered datasets. The previous work focus on text classification tasks, while this work is the first to investigate UE also for NER."
    }, {
      "heading" : "3 Background and Methods",
      "text" : "In this section, we describe the baselines and propose novel uncertainty estimation techniques."
    }, {
      "heading" : "3.1 Softmax Response",
      "text" : "Softmax Response (SR) (Geifman and El-Yaniv, 2017) is a trivial baseline for UE that uses the probabilities generated via the output softmax layer of the neural network. SR is based on the maximum probability p(y|x) over classes y = c ∈ C. The smaller this probability is, the more uncertain model is:\nuSR(x) = 1−max c∈C p(y = c|x). (1)"
    }, {
      "heading" : "3.2 Monte Carlo Dropout",
      "text" : "Standard Monte Carlo Dropout (MC Dropout) Consider we have conducted T stochastic forward passes. In this work, we use the following ways to quantify uncertainty with the standard MC dropout:\n• Sampled maximum probability (SMP) is:\nuSMP = 1−max c∈C\n1\nT T∑ t=1 pct , (2)\nwhere pct is the probability of the class c for the t-th stochastic forward pass.\n• Probability variance (PV; Kampffmeyer et al. (2016); Gal et al. (2017)) is:\nuPV = 1\nC C∑ c=1\n( 1\nT − 1 T∑ t=1 (pct − pc)2 ) ,\n(3) where pc = 1T ∑ t p c t is the probability for a class c averaged across T stochastic forward passes.\n• Bayesian active learning by disagreement (BALD; Houlsby et al. (2011)) is:\nuB = − C∑ c=1 pc log pc + 1 T ∑ c,t pct log p c t . (4)\nTransformers contain multiple dropout layers (after the embedding layer, in each attention head, and before the last classification layer). It is shown in previous work that standard MC dropout outperforms the baseline SR only when all dropout layers are activated in a model (Shelmanov et al., 2021). Therefore, we follow this setting for experiments in this work. We note that due to activating all dropout layers, multiple stochastic predictions are required for the whole network, which introduces a large computational overhead.\nDiverse Determinantal Point Process Monte Carlo Dropout (DDPP MC dropout) (Ours) Determinantal point processes (DPPs; Kulesza and Taskar (2012)) are used for sampling a subset of diverse objects from a given set. Recently, Shelmanov et al. (2021) have combined MC dropout with a determinantal point process (DPP) for sampling neurons in a dropout layer and demonstrated that using stochasticity in the last dropout layer (in a classification head of Transformer) only is enough to improve upon SR in misclassification detection. This method is less computationally expensive than the standard MC dropout since it requires multiple stochastic predictions only for the top classification layer of the network with a small number of parameters.\nConsider the similarity matrix Ch between neurons of the h-th hidden layer (in particular, we use a correlation matrix between output values of neurons). Then one can construct the DPP-based dropout masks MDPPh using Ch as a likelihood kernel for the DPP: MDPPh ∼ DPP (Ch). That gives the following probability to select a set S of activations on the layer h:\nP [ MDPPh = S ] = det(CSh )\ndet(Ch + I) , (5)\nwhere CSh is the square submatrix of Ch obtained by keeping only rows and columns indexed by the sample S.\nIn this work, we improve this method by increasing the diversity of the sampled DPP masks. After multiple dropout masks are pre-generated via DPP in the inference step as in the original DPP MC dropout, we make an additional step, in which we select a diverse set of masks from this pre-generated pool using one of two strategies:\n• DDPP (+DPP): We sample a set of “diverse” masks that activate different sets of neurons. For this purpose, we apply DPP sampling again to the pool of pre-generated masks. As a similarity kernel in this step, we use an RBF-similarity or cosine-similarity matrix of masks.\n• DDPP (+OOD): We sample a set of masks that generate diverse predictions. For this purpose, we select the masks that yield the highest PV scores on the given OOD dataset.\nAfter a new set of T masks is selected, we use them as in standard MC dropout to obtain\nstochastic predictions. Increasing the diversity of masks in the proposed modification is motivated by the finding of Jain et al. (2020) that improving the diversity of elements in an ensemble leads to better uncertainty estimates.\nWe note that in masks generated with DPP, usually, less than 60% of neurons are activated, which makes predictions poorly calibrated. To mitigate this problem, for each constructed mask, we perform a temperature-scaling calibration (Guo et al., 2017) using a held-out dataset."
    }, {
      "heading" : "3.3 Deterministic Uncertainty Estimation",
      "text" : "Spectral-normalized Neural Gaussian Process (SNGP) Liu et al. (2020) suggest replacing the typical dense output layer of a network with a layer that implements a Gaussian process (GP) with an RBF kernel, whose posterior variance at a given instance is characterized by its L2 distance from the training data in the hidden space. The authors propose an approximation based on random Fourier feature expansion, which enables end-toend training and makes the inference feasible. In this work, we use this posterior variance as an uncertainty estimate.\nHowever, the original paper requires hidden representation to be distance-preserving in order to make it work. While distance between instances in the hidden space does not always have a meaningful correspondence to the distance in the input space, authors prove that to keep hidden representations distance-preserving, the transformation should satisfy the bi-Lipschitz condition. This requirement is satisfied if weight matrices for the nonlinear residual blocks have a spectral norm (i.e., the largest singular value) less than 1. Therefore, to enforce the aforementioned Lipschitz constraint, they apply a spectral normalization on weight matrices. For Transformers, they normalize matrices of the classification layers only.\nMahalanobis Distance (MD) Mahalanobis distance is a generalisation of the Euclidean distance, which takes into account the spreading of instances in the training set along various directions in a feature space. Lee et al. (2018) suggest estimating uncertainty by measuring the distance between a test instance and the closest class-conditional Gaussian distribution:\nuMD = min c∈C\n(hi − µc)TΣ−1(hi − µc), (6)\nwhere hi is a hidden representation of a i-th instance, µc is a centroid of a class c, and Σ is a covariance matrix for hidden representations of training instances.\nRecently, the Mahalanobis distance has been adopted for out-of-distribution detection with Transformer networks by Podolskiy et al. (2021).\nMahalanobis Distance with Spectralnormalized Network (MD SN) (Ours) Since the UE method based on the Mahalanobis distance utilizes the idea of a proximity of a tested instance hidden representation to the training distribution, we expect this method to benefit from distance-preserving representations. Therefore, we propose the modification of the method of Lee et al. (2018) and Podolskiy et al. (2021) that enforces the bi-Lipschitz constraints on transformation implemented by the network. We perform spectral normalization of the weight matrix of linear layers in the classification head of Transformer as it is suggested in SNGP (Liu et al., 2020). At each training step, a spectral norm ν is estimated using the power iteration method ν = ‖W‖2, and normalized weight matrix is obtained: W̃ = Wν . At the inference step, hidden representations are calculated using the normalized matrix h̃(x) = W̃x+b and are used for computing the Mahalanobis distance."
    }, {
      "heading" : "3.4 Training Loss Regularization",
      "text" : "Additive regularisation is another approach to improving UE of neural networks. Usually, the training loss combines the original task-specific loss Ltask (e.g. cross-entropy) and a regularisation component Lreg that facilitates producing better calibrated UEs:\nL = Ltask + λLreg, (7)\nwhere λ is a hyperparameter that controls the regularisation strength.\nThe positive side of such techniques is that, besides SR, they can be used to improve other methods like MC dropout and deterministic methods. The drawback is that regularisation affects the training procedure and can decrease the model quality.\nConfidence Error Regularizer (CER) Xin et al. (2021) propose a regularizer that adds a penalty for an instance that has a bigger loss than other\ninstances but also is more confident:\nLreg = k∑\ni,j=1\n∆i,j1[ei > ej ], (8)\n∆i,j = max{0,max c pci −maxc p c j}2, (9)\nwhere k is the number of instances in a batch and ei is an error of the i-th instance: ei is 1 if the prediction of the classifier matches the true label, and ei is 0 otherwise. The authors evaluate this type of regularization only in conjunction with SR.\nMetric Regularizer Zhang et al. (2019) propose a regularizer, which aims to shorten the intra-class distance and enlarge the inter-class distance: Lreg = C∑ c=1 { Lintra(c) + ε ∑ k 6=c Linter(c, k) } , (10)\nLintra(c) = 2 |Sc|2 − |Sc| ∑\ni,j∈Sc,i<j D(hi, hj),\n(11)381\nLinter(c, k) = 1 |Sc| · |Sk| ∑\ni∈Sc,j∈Sk\n[γ−D(hi, hj)]+,\n(12)3823\nD(ri, rj) = 1\nd ||hi − hj ||22, (13)384 where hi is a feature representation of an instance385 i, Sc is the set of instances from class c, |Sc| is386 the number of elements in Sc, ε and γ are positive387 hyperparameters, [x]+ = max(0, x).388\n4 Experimental Setup389\nIn the experiments, we train a model on a given390 dataset and perform inference on a test set to391 compute both predictions and UE scores u. We392 are interested in how the scores correlate with393 the mistakes ẽ of the model on the test set. For394 text classification, mistakes are computed in the395 following way:396\nẽi = { 1, yi 6= ŷi, 0, yi = ŷi, (14)397\nwhere yi is a true label, ŷi is a predicted label.398 For NER, we use two evaluation options: token-399 level and sequence-level. For the token-level400 evaluation, individual tokens are considered as401 separate instances as in text classification. For the402 sequence-level evaluation, mistakes are computed403 in the following way:404\nẽi = { 1, ∃j ∈ {1, . . . , n}, yij 6= ŷij , 0, ∀j ∈ {1, . . . , n}, yij = ŷij , (15)405\nwhere n is a sequence length, yij is a true label, ŷij 406 is a predicted label of a j-th token in a sequence. 407\nIn the sequence-level evaluation, UE of a 408 sequence is aggregated from UEs of individual 409 tokens by averaging. 410\n4.1 Metrics 411\nEl-Yaniv et al. (2010) suggest evaluating the 412 quality of UE using the area under the risk 413 coverage curve (RCC-AUC). The risk coverage 414 curve demonstrates the cumulative sum of loss due 415 to misclassification (cumulative risk) depending 416 on the uncertainty level used for rejection of 417 predictions. The lower area under this curve 418 indicates better quality of the UE method. 419\nXin et al. (2021) propose a reversed pair 420 proportion (RPP) metric. They note that instances 421 with higher confidence should have a lower loss l. 422 RPP measures how far the uncertainty estimator ũ 423 is to ideal, given the labeled dataset of size n: 424\nRPP = 1\nn2 n∑ i,j=1 1[ũ(xi) > ũ(xj), li > lj ]. (16) 425\nSimilar to Xin et al. (2021), for both metrics, l 426 is an indicator loss function. 427\nWe also present the results using the accuracy 428 rejection curve. This curve is drawn by varying 429 the rejection uncertainty level (horizontal axis) and 430 presenting the corresponding accuracy obtained 431 when all rejected instances are labeled with an 432 oracle (vertical axis). This emulates the work of 433 a human expert in conjunction with a machine 434 learning system. The higher the curve, the smaller 435 amount of labor is needed to achieve a certain level 436 of performance and the better is the UE method. A 437 similar evaluation approach in a table form is used 438 in (Zhang et al., 2019). A similar curve but without 439 oracle labeling is used in (Lakshminarayanan et al., 440 2017; Filos et al., 2019). 441\n4.2 Datasets 442\nFor experiments with text classification, we use 443 three datasets from the GLUE benchmark (Wang 444 et al., 2018) that were previously leveraged 445 by Shelmanov et al. (2021) and Xin et al. 446 (2021) for the same purpose: Microsoft Research 447 Paraphrase Corpus (MRPC) (Dolan and Brockett, 448 2005), Corpus of Linguistic Acceptability (CoLA) 449 (Warstadt et al., 2019), and Stanford Sentiment 450 Treebank (SST-2) (Socher et al., 2013). Similar to 451\n(Shelmanov et al., 2021), we randomly subsample452 SST-2 to 10% to emulate a low-resource setting.453 The experiments with NER were performed454 on the widely-used CoNLL-2003 task (Tjong455 Kim Sang and De Meulder, 2003). For this dataset,456 we also subsample the training part to 10%.457 As an out-of-domain dataset for DDPP MC458 dropout, we use the IMDB binary sentiment459 classification dataset (Maas et al., 2011). We460 randomly select 1,000 instances from its test part461 and use them to select DPP-generated masks.462 The dataset statistics are provided in Table 4 in463 Appendix A.464\n4.3 Model Choice and Hyperparameter465 Selection466\nFor experiments, we use the pre-trained ELECTRA467 model (Clark et al., 2020) with 110 million468 parameters. ELECTRA leverages a corrupted469 tokens detection objective, which helps it to achieve470 higher performance on the GLUE benchmark in471 comparison with previous models: BERT (Devlin472 et al., 2019) and RoBERTa (Liu et al., 2019).473 The optimal hyperparameter values for each474 triple <Dataset, Regularization Type, Spectral475 Normalization Usage> are presented in Table 5476 in Appendix A. For the optimal hyperparameter477 search, we split the original training data into478\ntraining and validation subsets in a ratio of 80 479 to 20 and apply Bayesian optimization with early 480 stopping. For text classification, we use accuracy 481 as an objective metric, while for sequence tagging, 482 we use span-based F1-score (Tjong Kim Sang and 483 De Meulder, 2003). Sets of pre-defined values 484 for each hyperparameter are given in the caption 485 of Table 5. After the hyperparameter search is 486 completed, we train the model on the original 487 training set using the optimal values. 488\nThe hyperparameters for UE methods are 489 presented in Table 7 in Appendix A. The values for 490 DDPP MC dropout are chosen using a grid search, 491 while validating on the held-out dataset (10% of 492 the training dataset). 493\n5 Results and Discussion 494\n5.1 Monte Carlo Dropout and Regularisation 495\nThe results of methods based on MC dropout and 496 loss regularisation are presented in Tables 1 and 497 8 in Appendix B. The standard computationally 498 intensive MC dropout achieves substantial 499 improvements over the SR baseline on all 500 datasets, including CoNLL-2003. All uncertainty 501 estimation scores perform almost similarly for text 502 classification. For named entity recognition on 503 CoNLL-2003, BALD achieves the best results, 504\nPV is slightly worse, and SMP performs poorly,505 similar to the SR baseline.506 The DDPP MC dropout method does not507 outperform the standard MC dropout. However,508 DDPP (+OOD) demonstrates small improvements509 over the SR baseline on MRPC, DDPP (+DPP)510 outperforms SR on MRPC and SST-2, and511 both of these methods show big improvements512 on CoNLL-2003 using BALD and PV scores.513 The main advantage of the proposed DDPP514 MC dropout method consists in its much faster515 inference compared to computationally intensive516 standard MC dropout. DDPP MC dropout has517 the same computational overhead as the original518 DPP MC dropout, which is only 0.5% of the519 overhead introduced by the standard MC dropout520 (Shelmanov et al., 2021).521 We conduct an ablation study of the two522 proposed modifications for the original DPP MC523 dropout. The experimental results of this study524 presented in Table 11 in Appendix C.1 demonstrate525 the benefits of using calibration, and results in526 Table 12 show improvements of DDPP MC dropout527 over the original DPP MC dropout.528 Metric regularisation achieves a slight advantage529 only on MRPC. CER gives improvements on all530 datasets except CoNLL-2003. It also slightly531 improves methods based on MC dropout. The532 standard MC dropout with CER regularisation533 achieves the best results among other methods534 in Table 1 on all datasets. CER and DDPP MC535 dropout also complement each other on CoNLL-536 2003, the results of their combination are slightly537 better than when they are applied individually.538\n5.2 Deterministic Methods539 The results for deterministic methods are presented540 in Tables 2 and 9 in Appendix B. SNGP gives541 substantial improvements over the SR baseline on542\nall datasets, except CoLA, where it performs on 543 par with SR in terms of RPP metric. MD yields 544 much larger improvements over the SR baseline 545 on all datasets and significantly outperforms SNGP. 546 MD with spectral normalization is able to improve 547 the misclassification detection performance even 548 further and achieves the best results across other 549 deterministic methods on CoLA, SST-2, and 550 CoNLL-2003. The biggest improvements are 551 achieved for the NER task, where it is able to 552 reduce RCC-AUC by more than 96% in the token- 553 level evaluation and by more than 65% in the 554 sequence-level evaluation. 555\nWe also conduct an ablation study (Tables 2 and 556 9), in which we use the spectral normalization 557 without MD. We see that SN on its own, as 558 expected, does not improve the UE performance 559 anywhere except MRPC. 560\nThe metric regularisation appears to be 561 malignant for the methods based on the 562 Mahalanobis distance. The results of applying 563 CER regularisation with deterministic methods 564 are mixed. It gives small improvements on text 565 classification datasets: for MD, CER helps on 566 MRPC, and for MD SN, it helps on CoLA and 567 SST-2. However, it deteriorates the performance 568 of MD SN for NER and only slightly improves 569 results of MD for the token-level evaluation. 570\n5.3 Best Results 571 Tables 3 and 10 in Appendix B compare the results 572 of the best methods in each group. We also 573 present the results of deep ensemble, which is 574 a strong yet computationally intensive baseline 575 (Ashukha et al., 2020). We can see that it is 576 possible to substantially improve the performance 577 of misclassification detection and achieve even 578 better results than MC dropout and deep ensemble 579 almost with no overhead in terms of memory 580\nconsumption and amount of computation. The581 proposed in this work MD SN outperforms all the582 other methods on all the datasets except MRPC,583 where it is slightly behind the original MD. Another584 method proposed in this work, DDPP MC dropout,585 is able to approach the quality of computationally586 intensive UE methods while introducing only a587 fraction of their overhead.588 Figure 1 also presents accuracy rejection curves589 for selected methods on MRPC.The figure shows590 that if we reject 20% of instances using UE591 obtained with MC dropout and ask human experts592 to label these uncertain objects, the accuracy score593 of such a human-machine hybrid system will594 increase from 87.6% to 95.7%, which is 1.5%595 better than the SR baseline. Such an additional596 gain over the SR baseline can be crucial for safe-597 critical applications. Deep ensemble, MD SN, and598 DDPP are close to each other and achieve 95.0%,599\n94.7%, and 94.9% accuracy. Rejecting 60% of 600 most uncertain instances gives 99.2% of accuracy 601 for the computationally-intensive deep ensemble, 602 while the proposed cheap MD SN method yields 603 even better results with 99.5% of accuracy, which 604 is almost 2% higher than the results of the SR 605 baseline. 606\n6 Conclusion 607\nOur extensive empirical investigation on text 608 classification and NER tasks, shows that 609 computationally cheap UE methods are able to 610 substantially improve misclassification detection 611 for Transformers, performing on par or even 612 better than computationally intensive MC dropout 613 and deep ensemble. The proposed in this work 614 method based on the Mahalanobis distance and 615 spectral normalization of a weight matrix (MD 616 SN) achieves the best results among other methods 617 on CoLA, SST-2, and CoNLL-2003. This method 618 does not require to modify a model architecture, 619 extra memory storage, and introduces only little 620 amount of additional computation on inference. 621\nWe also show that our modification of DPP 622 MC dropout that leverages diversity of generated 623 dropout masks, which is also a computationally 624 cheap method, is able to outperform the 625 softmax response baseline and approach the 626 computationally intensive methods on NER. 627 Finally, we find that regularisation can slightly 628 improve the results of methods based on MC 629 dropout and the Mahalanobis distance. 630\nReferences631 Arsenii Ashukha, Alexander Lyzhov, Dmitry632 Molchanov, and Dmitry P. Vetrov. 2020. Pitfalls of633 in-domain uncertainty estimation and ensembling in634 deep learning. In 8th International Conference on635 Learning Representations, ICLR 2020, Addis Ababa,636 Ethiopia, April 26-30, 2020. OpenReview.net.637\nCharles Blundell, Julien Cornebise, Koray638 Kavukcuoglu, and Daan Wierstra. 2015. Weight639 uncertainty in neural network. In International640 Conference on Machine Learning, pages 1613–641 1622. PMLR.642\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and643 Christopher D. Manning. 2020. ELECTRA: Pre-644 training text encoders as discriminators rather645 than generators. In International Conference on646 Learning Representations.647\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and648 Kristina Toutanova. 2019. BERT: Pre-training649 of Deep Bidirectional Transformers for Language650 Understanding. In Proceedings of the 2019651 Conference of the North American Chapter of the652 Association for Computational Linguistics: Human653 Language Technologies, Volume 1 (Long and Short654 Papers), pages 4171–4186, Stroudsburg, PA, USA.655 Association for Computational Linguistics.656\nWilliam B. Dolan and Chris Brockett. 2005.657 Automatically constructing a corpus of sentential658 paraphrases. In Proceedings of the Third659 International Workshop on Paraphrasing660 (IWP2005).661\nRan El-Yaniv et al. 2010. On the foundations of noise-662 free selective classification. Journal of Machine663 Learning Research, 11(5).664\nAngelos Filos, Sebastian Farquhar, Aidan N. Gomez,665 Tim G. J. Rudner, Zachary Kenton, Lewis Smith,666 Milad Alizadeh, Arnoud de Kroon, and Yarin Gal.667 2019. A systematic comparison of bayesian deep668 learning robustness in diabetic retinopathy tasks.669 CoRR, abs/1912.10481.670\nYarin Gal and Zoubin Ghahramani. 2016. Dropout671 as a bayesian approximation: Representing model672 uncertainty in deep learning. In Proceedings of673 The 33rd International Conference on Machine674 Learning, volume 48 of Proceedings of Machine675 Learning Research, pages 1050–1059, New York,676 New York, USA. PMLR.677\nYarin Gal, Riashat Islam, and Zoubin Ghahramani.678 2017. Deep bayesian active learning with679 image data. In Proceedings of the 34th680 International Conference on Machine Learning,681 ICML 2017, Sydney, NSW, Australia, 6-11 August682 2017, volume 70 of Proceedings of Machine683 Learning Research, pages 1183–1192. PMLR.684\nYonatan Geifman and Ran El-Yaniv. 2017. Selective685 classification for deep neural networks. Advances686\nin Neural Information Processing Systems, 30:4878– 687 4887. 688\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. 689 Weinberger. 2017. On calibration of modern 690 neural networks. In Proceedings of the 34th 691 International Conference on Machine Learning, 692 ICML 2017, Sydney, NSW, Australia, 6-11 August 693 2017, volume 70 of Proceedings of Machine 694 Learning Research, pages 1321–1330. PMLR. 695\nJianfeng He, Xuchao Zhang, Shuo Lei, Zhiqian Chen, 696 Fanglan Chen, Abdulaziz Alhamadani, Bei Xiao, 697 and ChangTien Lu. 2020. Towards more accurate 698 uncertainty estimation in text classification. In 699 Proceedings of the 2020 Conference on Empirical 700 Methods in Natural Language Processing (EMNLP), 701 pages 8362–8372. 702\nNeil Houlsby, Ferenc Huszar, Zoubin Ghahramani, 703 and Máté Lengyel. 2011. Bayesian active learning 704 for classification and preference learning. CoRR, 705 abs/1112.5745. 706\nYibo Hu and Latifur Khan. 2021. Uncertainty-aware 707 reliable text classification. In Proceedings of the 708 27th ACM SIGKDD Conference on Knowledge 709 Discovery & Data Mining, pages 628–636. 710\nSiddhartha Jain, Ge Liu, Jonas Mueller, and David 711 Gifford. 2020. Maximizing overall diversity for 712 improved uncertainty estimates in deep ensembles. 713 In Proceedings of the AAAI Conference on Artificial 714 Intelligence, volume 34, pages 4264–4271. 715\nMichael Kampffmeyer, Arnt-Borre Salberg, and 716 Robert Jenssen. 2016. Semantic segmentation 717 of small objects and modeling of uncertainty 718 in urban remote sensing images using deep 719 convolutional neural networks. In Proceedings of 720 the IEEE conference on computer vision and pattern 721 recognition workshops, pages 1–9. 722\nAlex Kulesza and Ben Taskar. 2012. Determinantal 723 point processes for machine learning. Foundations 724 and Trends® in Machine Learning, 5(2–3):123–286. 725\nBalaji Lakshminarayanan, Alexander Pritzel, and 726 Charles Blundell. 2017. Simple and scalable 727 predictive uncertainty estimation using deep 728 ensembles. In Proceedings of the 31st International 729 Conference on Neural Information Processing 730 Systems, NeurIPS’17, page 6405–6416, Red Hook, 731 NY, USA. Curran Associates Inc. 732\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 733 2018. A simple unified framework for detecting 734 out-of-distribution samples and adversarial attacks. 735 Advances in neural information processing systems, 736 31. 737\nJeremiah Z. Liu, Zi Lin, Shreyas Padhy, Dustin Tran, 738 Tania Bedrax-Weiss, and Balaji Lakshminarayanan. 739 2020. Simple and principled uncertainty estimation 740 with deterministic deep learning via distance 741 awareness. In Advances in Neural Information 742\nProcessing Systems 33: Annual Conference on743 Neural Information Processing Systems 2020,744 NeurIPS 2020, December 6-12, 2020, virtual.745\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,746 Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,747 Luke Zettlemoyer, and Veselin Stoyanov. 2019.748 RoBERTa: A robustly optimized bert pretraining749 approach.750\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,751 Dan Huang, Andrew Y. Ng, and Christopher752 Potts. 2011. Learning word vectors for sentiment753 analysis. In Proceedings of the 49th Annual754 Meeting of the Association for Computational755 Linguistics: Human Language Technologies, pages756 142–150, Portland, Oregon, USA. Association for757 Computational Linguistics.758\nJishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort,759 Philip HS Torr, and Yarin Gal. 2021. Deterministic760 neural networks with appropriate inductive biases761 capture epistemic and aleatoric uncertainty. arXiv762 preprint arXiv:2102.11582.763\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout,764 Ekaterina Artemova, and Irina Piontkovskaya. 2021.765 Revisiting mahalanobis distance for transformer-766 based out-of-domain detection. In Thirty-Fifth AAAI767 Conference on Artificial Intelligence, AAAI 2021,768 Thirty-Third Conference on Innovative Applications769 of Artificial Intelligence, IAAI 2021, The Eleventh770 Symposium on Educational Advances in Artificial771 Intelligence, EAAI 2021, Virtual Event, February 2-772 9, 2021, pages 13675–13682. AAAI Press.773\nCarl Edward Rasmussen. 2003. Gaussian processes in774 machine learning. In Summer school on machine775 learning, pages 63–71. Springer.776\nBurr Settles. 2012. Active learning. Synthesis Lectures777 on Artificial Intelligence and Machine Learning,778 6(1):1–114.779\nArtem Shelmanov, Evgenii Tsymbalov, Dmitri780 Puzyrev, Kirill Fedyanin, Alexander Panchenko,781 and Maxim Panov. 2021. How certain is your782 Transformer? In Proceedings of the 16th783 Conference of the European Chapter of the784 Association for Computational Linguistics: Main785 Volume, pages 1833–1840, Online. Association for786 Computational Linguistics.787\nRichard Socher, Alex Perelygin, Jean Wu, Jason788 Chuang, Christopher D. Manning, Andrew Ng, and789 Christopher Potts. 2013. Recursive deep models790 for semantic compositionality over a sentiment791 treebank. In Proceedings of the 2013 Conference on792 Empirical Methods in Natural Language Processing,793 pages 1631–1642, Seattle, Washington, USA.794 Association for Computational Linguistics.795\nSunil Thulasidasan, Gopinath Chennupati, Jeff A796 Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.797 2019. On mixup training: Improved calibration798\nand predictive uncertainty for deep neural networks. 799 Advances in Neural Information Processing Systems, 800 32. 801\nErik F. Tjong Kim Sang and Fien De Meulder. 802 2003. Introduction to the CoNLL-2003 shared task: 803 Language-independent named entity recognition. In 804 Proceedings of the Seventh Conference on Natural 805 Language Learning at HLT-NAACL 2003, pages 806 142–147. 807\nJoost Van Amersfoort, Lewis Smith, Yee Whye Teh, 808 and Yarin Gal. 2020. Uncertainty estimation using 809 a single deep deterministic neural network. In 810 Proceedings of the 37th International Conference 811 on Machine Learning, volume 119 of Proceedings 812 of Machine Learning Research, pages 9690–9700. 813 PMLR. 814\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob 815 Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz 816 Kaiser, and Illia Polosukhin. 2017. Attention is 817 all you need. In Advances in Neural Information 818 Processing Systems 30: Annual Conference on 819 Neural Information Processing Systems 2017, 4-9 820 December 2017, Long Beach, CA, USA, pages 5998– 821 6008. 822\nAlex Wang, Amanpreet Singh, Julian Michael, Felix 823 Hill, Omer Levy, and Samuel Bowman. 2018. 824 GLUE: A multi-task benchmark and analysis 825 platform for natural language understanding. 826 In Proceedings of the 2018 EMNLP Workshop 827 BlackboxNLP: Analyzing and Interpreting 828 Neural Networks for NLP, pages 353–355, 829 Brussels, Belgium. Association for Computational 830 Linguistics. 831\nAlex Warstadt, Amanpreet Singh, and Samuel R. 832 Bowman. 2019. Neural network acceptability 833 judgments. Transactions of the Association for 834 Computational Linguistics, 7:625–641. 835\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 836 2021. The art of abstention: Selective prediction and 837 error regularization for natural language processing. 838 In Proceedings of the 59th Annual Meeting of 839 the Association for Computational Linguistics 840 and the 11th International Joint Conference on 841 Natural Language Processing (Volume 1: Long 842 Papers), pages 1040–1051, Online. Association for 843 Computational Linguistics. 844\nZhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, 845 Yanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu. 846 2021. Modeling discriminative representations for 847 out-of-domain detection with supervised contrastive 848 learning. In Proceedings of the 59th Annual Meeting 849 of the Association for Computational Linguistics 850 and the 11th International Joint Conference on 851 Natural Language Processing (Volume 2: Short 852 Papers), pages 870–878, Online. Association for 853 Computational Linguistics. 854\nXuchao Zhang, Fanglan Chen, Chang-Tien Lu, and 855 Naren Ramakrishnan. 2019. Mitigating uncertainty 856\nin document classification. In Proceedings of857 the 2019 Conference of the North American858 Chapter of the Association for Computational859 Linguistics: Human Language Technologies,860 Volume 1 (Long and Short Papers), pages 3126–861 3136, Minneapolis, Minnesota. Association for862 Computational Linguistics.863\nA Dataset Statistics and Hyperparameter Values864\nB Experimental Results on CoLA865\nC Additional Ablation Studies866\nC.1 Calibration for DDP MC Dropout867 Table 11 compares results for misclassification868 detection tasks with MC-DPP method with869 different calibration datasets. We compare the870 results for two calibration strategies:871\n• train and calibration on the full train dataset872 (100%)873\n• split the training dataset into the training874 (90%) and calibration (10%) parts875\nWe can see that for both variants of DDPP,876 making calibration on a separate part of the dataset877 does not improve the results. Consequently, we878 calibrate these DDPP masks on the full train879 dataset.880\nC.2 Comparison of DPP MC dropout and881 DDPP MC dropout882\nTable 12 compares the results for both modification883 of DDPP MC dropout and the original method. We884 can see that at least one of our modifications always885 outperforms a the original method and the baseline.886"
    } ],
    "references" : [ {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Semantic segmentation",
      "author" : [ "Robert Jenssen" ],
      "venue" : null,
      "citeRegEx" : "Jenssen.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jenssen.",
      "year" : 2016
    }, {
      "title" : "Simple and scalable",
      "author" : [ "Charles Blundell" ],
      "venue" : null,
      "citeRegEx" : "Blundell.,? \\Q2017\\E",
      "shortCiteRegEx" : "Blundell.",
      "year" : 2017
    }, {
      "title" : "How certain is your",
      "author" : [ "Maxim Panov" ],
      "venue" : null,
      "citeRegEx" : "Panov.,? \\Q2021\\E",
      "shortCiteRegEx" : "Panov.",
      "year" : 2021
    }, {
      "title" : "Recursive deep models",
      "author" : [ "Christopher Potts" ],
      "venue" : null,
      "citeRegEx" : "Potts.,? \\Q2013\\E",
      "shortCiteRegEx" : "Potts.",
      "year" : 2013
    }, {
      "title" : "Neural network acceptability",
      "author" : [ "Bowman" ],
      "venue" : null,
      "citeRegEx" : "2019.,? \\Q2019\\E",
      "shortCiteRegEx" : "2019.",
      "year" : 2019
    }, {
      "title" : "Mitigating uncertainty",
      "author" : [ "Naren Ramakrishnan" ],
      "venue" : null,
      "citeRegEx" : "Ramakrishnan.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ramakrishnan.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Uncertainty estimation is a built-in capability of Bayesian neural networks (Blundell et al., 2015). However, such models have similar issues as ensembles and also require special training procedures. Recently, it was shown by Gal and Ghahramani (2016) that dropout, a wellknown regularization technique, is mathematically equivalent to approximate variational inference in the deep Gaussian process.",
      "startOffset" : 77,
      "endOffset" : 253
    }, {
      "referenceID" : 2,
      "context" : "Uncertainty estimation is a built-in capability of Bayesian neural networks (Blundell et al., 2015). However, such models have similar issues as ensembles and also require special training procedures. Recently, it was shown by Gal and Ghahramani (2016) that dropout, a wellknown regularization technique, is mathematically equivalent to approximate variational inference in the deep Gaussian process. This method, known as Monte Carlo (MC) dropout, uses Bernoulli approximating the variational distribution on the network units and introduces no additional parameters for the approximate posterior. MC dropout does not introduce any overhead during training and does not require any additional memory. The main disadvantage of this method is that it often requires many forward-pass samplings, which makes it also computationally expensive. Recently, many works have investigated the approximate Bayesian inference for neural networks using deterministic approaches Lee et al. (2018); Liu et al.",
      "startOffset" : 77,
      "endOffset" : 984
    }, {
      "referenceID" : 2,
      "context" : "Uncertainty estimation is a built-in capability of Bayesian neural networks (Blundell et al., 2015). However, such models have similar issues as ensembles and also require special training procedures. Recently, it was shown by Gal and Ghahramani (2016) that dropout, a wellknown regularization technique, is mathematically equivalent to approximate variational inference in the deep Gaussian process. This method, known as Monte Carlo (MC) dropout, uses Bernoulli approximating the variational distribution on the network units and introduces no additional parameters for the approximate posterior. MC dropout does not introduce any overhead during training and does not require any additional memory. The main disadvantage of this method is that it often requires many forward-pass samplings, which makes it also computationally expensive. Recently, many works have investigated the approximate Bayesian inference for neural networks using deterministic approaches Lee et al. (2018); Liu et al. (2020); Van Amersfoort et al.",
      "startOffset" : 77,
      "endOffset" : 1003
    }, {
      "referenceID" : 2,
      "context" : "Uncertainty estimation is a built-in capability of Bayesian neural networks (Blundell et al., 2015). However, such models have similar issues as ensembles and also require special training procedures. Recently, it was shown by Gal and Ghahramani (2016) that dropout, a wellknown regularization technique, is mathematically equivalent to approximate variational inference in the deep Gaussian process. This method, known as Monte Carlo (MC) dropout, uses Bernoulli approximating the variational distribution on the network units and introduces no additional parameters for the approximate posterior. MC dropout does not introduce any overhead during training and does not require any additional memory. The main disadvantage of this method is that it often requires many forward-pass samplings, which makes it also computationally expensive. Recently, many works have investigated the approximate Bayesian inference for neural networks using deterministic approaches Lee et al. (2018); Liu et al. (2020); Van Amersfoort et al. (2020); Mukhoti et al.",
      "startOffset" : 77,
      "endOffset" : 1033
    }, {
      "referenceID" : 2,
      "context" : "Uncertainty estimation is a built-in capability of Bayesian neural networks (Blundell et al., 2015). However, such models have similar issues as ensembles and also require special training procedures. Recently, it was shown by Gal and Ghahramani (2016) that dropout, a wellknown regularization technique, is mathematically equivalent to approximate variational inference in the deep Gaussian process. This method, known as Monte Carlo (MC) dropout, uses Bernoulli approximating the variational distribution on the network units and introduces no additional parameters for the approximate posterior. MC dropout does not introduce any overhead during training and does not require any additional memory. The main disadvantage of this method is that it often requires many forward-pass samplings, which makes it also computationally expensive. Recently, many works have investigated the approximate Bayesian inference for neural networks using deterministic approaches Lee et al. (2018); Liu et al. (2020); Van Amersfoort et al. (2020); Mukhoti et al. (2021), etc.",
      "startOffset" : 77,
      "endOffset" : 1056
    }, {
      "referenceID" : 5,
      "context" : "(2019) suggest adding a penalty that reduces the Euclidean distance between training instances of the same class, and increases the distance between instances of different classes. He et al. (2020) suggest using two components in the loss function that reduce the difference between outputs from two versions of a model initialized with different weights.",
      "startOffset" : 1,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "(2019) suggest adding a penalty that reduces the Euclidean distance between training instances of the same class, and increases the distance between instances of different classes. He et al. (2020) suggest using two components in the loss function that reduce the difference between outputs from two versions of a model initialized with different weights. They also use mix-up (Thulasidasan et al., 2019) to generate additional training instance representations that help to capture aleatoric uncertainty, self-ensembling, and a distinctiveness score to measure the epistemic uncertainty. Xin et al. (2021) introduce a regularizer that penalizes overconfident instances with high loss.",
      "startOffset" : 1,
      "endOffset" : 607
    }, {
      "referenceID" : 5,
      "context" : "(2019) suggest adding a penalty that reduces the Euclidean distance between training instances of the same class, and increases the distance between instances of different classes. He et al. (2020) suggest using two components in the loss function that reduce the difference between outputs from two versions of a model initialized with different weights. They also use mix-up (Thulasidasan et al., 2019) to generate additional training instance representations that help to capture aleatoric uncertainty, self-ensembling, and a distinctiveness score to measure the epistemic uncertainty. Xin et al. (2021) introduce a regularizer that penalizes overconfident instances with high loss. In another recent work, Shelmanov et al. (2021) propose to combine MC dropout with a Determinantal Point Process (DPP) to improve the diversity of predictions by considering the correlations between neurons and sampling the diverse neurons for activation in a dropout layer.",
      "startOffset" : 1,
      "endOffset" : 734
    } ],
    "year" : 0,
    "abstractText" : "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification / adversarial attack / out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which improves the state of the art and outperforms computationally intensive methods.",
    "creator" : null
  }
}
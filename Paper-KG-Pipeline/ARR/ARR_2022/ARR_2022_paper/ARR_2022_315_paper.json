{
  "name" : "ARR_2022_315_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Inductive Logic Reasoning for Multi-Hop Reading Comprehension",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reasoning has been extensively studied in the structured domain, e.g., knowledge base completion which infers missing facts given background entities and relations. However, when the background knowledge is expressed in natural languages, as shown in the multi-hop reading comprehension problem with triplet-form questions (Welbl et al., 2018), it becomes difficult to conduct complex reasoning. For example, consider the question “country(Moonhole, ?)”, given the following documents:\n“Moonhole is a private community on the island of Bequia. Moonhole was founded by Thomas and Gladys Johnston in the 1960s.”\n“Gladys Johnston was born in United States.”\n“Bequia is an island and is part of the country of Saint Vincent and the Grenadines”\nThe correct answer should be Saint Vincent and the Grenadines instead of United States although both entities have co-occurring contexts with Moonhole.\nDeep learning methods for multi-hop reading comprehension (RC) can be categorized as: 1) Memory-based models (Zhong et al., 2019; Wang et al., 2018; Zhuang and Wang, 2019) that learn to generate query-aware context representations. 2) Graph-based approaches (Song et al., 2018; De Cao et al., 2019) that use graph neural networks to propagate information based on pre-constructed entity (context) graphs. 3) Neural Module networks (Andreas et al., 2016) that decompose the question into a series of action modules (Jiang and Bansal, 2019; Min et al., 2019). However, DNNs only implicitly encode relevant contexts but fail to explicitly uncover the underlying relational compositions for complex inference. With the above example, DNNs may encode Bequia and Gladys Johnson into 1-hop features, given both entities co-occur with the query Moonhole. As a result, the model may predict United States by linking it with Gladys Johnson instead of the correct answer Saint Vincent and the Grenadines. However, a human would easily produce the correct answer given the knowledge “if A is in B and B is part of country C, then A is in country C” and by examining the relations between each entity pair co-occurred in the context.\nInductive logic programming (ILP) (Muggleton, 1991) aligns with human reasoning by inducing interpretable rules to entail positive but not negative examples. To answer the previous query, ILP could generate this rule: located_in(X,Z)∧ country(Z, Y )⇒country(X,Y ). Combining deep learning with ILP is a promising direction to benefit from both worlds (Evans and Grefenstette, 2018; Dong et al., 2019). Deep logic models have been proposed for structured knowledge base completion (Minervini et al., 2017, 2020; Yang and Song, 2020; Rocktäschel and Riedel, 2017; Yang et al., 2017). However, it becomes much more challenging when dealing with natural language inputs, as in the case of multi-hop reading comprehension. Weber et al. (2019) proposed to combine a\nsymbolic reasoner: prolog, with weak unifications based on distributed embeddings as a backwardchaining theorem prover to induce feasible rules for multi-hop reasoning. However, their work relies on the accuracies of pre-extracted NERs and is limited by the number of rule templates.\nTo address these limitations, we propose a novel end-to-end combination of deep learning and logic reasoning termed Deep Inductive Logic Reasoning (DILR). It consists of two components: 1) a hierarchical attentive reader that filters query-related and candidate-related information from given documents; 2) a multi-hop reasoner that conducts inductive logic reasoning by attentively selecting proper predicates to form candidate rules and refines them upon given examples. We introduce novel differentiable logic operators combined with attention mechanisms for smooth back-propagation. Compared to existing deep logic models, we build connections between raw text inputs and the symbolic domain by mapping high-level semantic representations to logic predicates and instantiating logic variables with neural representations to conduct relational reasoning. We also parameterize the entire process for end-to-end differentiable learning.\nThe contributions of this work include: 1) We introduce a novel smooth connection between deep representation learning with logic reasoning by associating distributed representations with discrete logic predicates and their probabilistic evaluations. 2) We propose deep-learning-based inductive logic programming via attentive memories and differentiable logic operators for the task of multi-hop reading comprehension considering the number of reasoning steps. 3) We provide comprehensive evaluations of our model on two benchmark datasets."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multi-Hop Reading Comprehension Recent works for multi-hop RC include memory-based methods which apply attentions to iteratively update query and context representations considering their interactions (Dhingra et al., 2018; Clark and Gardner, 2018; Wang et al., 2018; Zhong et al., 2019; Zhuang and Wang, 2019; Yichen Jiang and Bansal, 2019). To explicitly incorporate entity connections, De Cao et al. (2019), Ding et al. (2019), Qiu et al. (2019), Tang et al. (2020), Song et al. (2018) and Tu et al. (2019) build entity graphs and apply Graph Neural Networks for information propagation. Kundu et al. (2019) formalizes reasoning\nas a path-finding problem with neural encoding to rank candidate paths. Path modeling is also adopted in (Chen et al., 2019) using pointer networks. However, these approaches only focus on local information without the ability to generalize, and some of them rely on NER tools. Dhingra et al. (2020) converts texts into a virtual knowledge based for retrieval, but requires an entity database. Another category uses neural module networks (Jiang and Bansal, 2019; Min et al., 2019; Gupta et al., 2020; Chen et al., 2020) to decompose the question into a series of actions, each parameterized with a neural module, which also fail to explicitly uncover the underlying logic for reasoning. Deep Learning with Logic Reasoning Neurosymbolic learning aims to integrate deep learning’s ability on dealing with uncertainty and logic programming’s ability on reasoning. Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules. A more challenging direction is inductive logic programming that automatically learns rules through representation learning and differentiable backpropagation (Evans and Grefenstette, 2018; Dong et al., 2019; Wang et al., 2019; Yang and Song, 2020).\nNeuro-symbolic learning has been applied to knowledge-base completion through logic embeddings (Guo et al., 2016), tensor operations (Cohen, 2016; Rocktäschel and Riedel, 2017), adversarial learning (Minervini et al., 2017), variational learning (Qu and Tang, 2019) or attentions (Yang and Song, 2020). Differentiable theorem proving has also been proposed with weak unifications and backward chaining (Rocktäschel and Riedel, 2017; Campero et al., 2018; Minervini et al., 2020). However, unlike multi-hop RC, knowledge-base completion only takes structured inputs without the need to address language ambiguity. The most related work to ours is NLProlog (Weber et al., 2019), a neural theorem prover for multi-hop RC by converting language utterances to distributed embeddings. However, NLProlog relies on a NER tool to extract entities and its expressiveness is limited by the number of rule templates."
    }, {
      "heading" : "3 Background",
      "text" : "We focus on multi-hop reading comprehension tasks containing explicit query types which align\nwith the standard ILP setting. Formally, for each RC problem, we are given a set of documents D = {D1, ..., Dn}, a structured query in the form of a relational triplet (s, q, ?) where s denotes the subject of the relation q, and a list of candidate answers A = {a1, ..., am}. The task is to select an answer a ∈ A such that q(s, a) is satisfied, i.e., a is the object of relation q given the subject s. For example, country(Moonhole, ?) is a query asking for the country where Moonhole is located. This task could be converted into an ILP problem with the formal definition as follows.\nDefinition 3.1 (Inductive Logic Programming). Given a logic theory B representing the background knowledge (facts), a set of positive examples E+ and a set of negative examples E−, an ILP system aims to derive a hypothesis H which entails all the positive and none of the negative examples: B ∧H |= γ for γ ∈ E+. B ∧H ̸|= γ for γ ∈ E−.\nThe hypothesis H is a logic program consisting of definite clauses b1 ∧ ... ∧ bk ⇒ h where b1, ..., bk and h are logic atoms. The LHS of “⇒” is the clause body and h is the head. An atom is composed of a predicate and its arguments, e.g., h = located_in(X,Y ) with predicate “located_in” and arguments X , Y . A ground atom is obtained by instantiating variables in the arguments with constants, e.g., X = “US”. We use µ(·) to denote the value of an atom or a clause. For smooth optimization, we assign µ(·) ∈ [0, 1] which indicates the probability of the atom or clause being true.\nFor multi-hop reading comprehension, we treat the query relation q(X,Y ) as the head atom of the clauses to be induced. The correct answer a+i from each problem forms the set of positive examples E+={q(si, a+i )} N+ i=1, and the incorrect answer a−j forms the set of negative examples E−={q(sj , a−j )} N− j=1. Here we use lower cases: si, a+i , a − j to represent constants and upper cases: X , Y to represent variables. The predicates in the logic domain correspond to pairwise relations between two entities1. To differentiate the number of inference steps, we define a l-hop reasoning clause as F0(X0, X1)∧ ...∧Fl(Xl, Xl+1)⇒r(X0, Xl+1) with l denoting the number of extra arguments as bridging entities in the rule body except those in the head atom. Here each subclause Ft(Xt, Xt+1) can be one or a conjunction (∧) of 2-ary atoms\n1We restrict each atom as a 2-ary atom that takes exactly 2 arguments, analogical to relations in the knowledge base.\ntaking only Xt and Xt+1 as arguments, e.g., Ft(Xt, Xt+1) = r1(Xt, Xt+1) ∧ r2(Xt, Xt+1)."
    }, {
      "heading" : "4 Methodology",
      "text" : "Overall, DILR simulates multi-hop reasoning processes considering different number of inference steps. It is an end-to-end framework consisting of two components: a Hierarchical Attentive Reader and a Multi-hop Reasoner. The attentive reader learns to select relevant information from the given documents to produce query-aware, candidateaware and bridging entity representations. These representations are passed to the multi-hop reasoner to instantiate logic atoms in order to generate and evaluate clauses that are relevant to the query relation. The multi-hop reasoner conducts rule induction via attentive memories that softly select atoms to form new clauses and novel differentiable logic operators that produce probabilistic values for generated clauses. The final loss can be backpropagated smoothly to update the attentive reader for more accurate selections. Next, we illustrate each component with more details."
    }, {
      "heading" : "4.1 Hierarchical Attentive Reader",
      "text" : "To avoid inevitable errors brought by the NER tools for named entity extraction, we propose to learn to extract relevant information using an attentive reader. Since multiple documents (contexts) are involved for each question, we design a 2-level hierarchical attention network to progressively filter token-level and context-level information. Specifically, the token-level attentions aim to select lhop (l = 0, ..., L) relevant entities in each context. Then the context-level attentions produce the final representations by softly attending to each context considering different number of reasoning hops."
    }, {
      "heading" : "4.1.1 Token-Level Attention",
      "text" : "Given a query subject s with ns tokens, a candidate a with na tokens, and a context c of length nc, we denote by S ∈ Rns×D, A ∈ Rna×D and C ∈ Rnc×D as their word features after a biGRU layer, respectively. For multi-hop reasoning, we use different attentions for finding or relocating target tokens in each context, inspired by (Gupta et al., 2020). Firstly, a subject-to-context attention is adopted to find similar tokens as the subject in each context: Bsij = w ⊤ s [Si;Cj ;Si ◦ Cj ] where ws is a learnable transformation vector and [; ] denotes concatenations. We obtain the normalized\nsimilarity score αsij between i-th token in the subject and j-th token in the context via a softmax operation on each row of Bs. Then a subject-aware (0-hop) context representation is produced as\nhs = nc∑ j=1 ᾱsjCj , with ᾱ s j = ns∑ i=1 αsijβ s i (1)\nwhere βsi weighs the contribution of each subject token via a self-attention: βs = softmax(w̄⊤s S+ bs). Similarly, we produce an attention score αaij for the j-th context token w.r.t. the i-th candidate token and a candidate-aware context representation ha. We denote by s = βsS, and a = βaA the query subject and candidate representations, respectively.\nFor (l+1)-hop reasoning (l ≥ 0), it is desired to relocate to intermediate (bridging) entities related to the l-hop entities. Hence, we adopt context-tocontext attentions Bl+1ij = w ⊤ l [Ci +h l;Cj ; (Ci + hl)◦Cj ] given the l-hop representation hl where h0 = hs. We use αl+1ij to denote a normalized attention score between i-th and j-th context tokens after applying a softmax operator over each row of Bl+1. With ᾱ0j = ᾱ s j , the (l + 1)-hop bridging context representation becomes\nhl+1 = nc∑ j=1 ᾱl+1j Cj , with ᾱ l+1 j = nc∑ i=1 αl+1ij ᾱ l i. (2)"
    }, {
      "heading" : "4.1.2 Context-Level Attention",
      "text" : "With multiple contexts (documents) available, we use a context-level attention to produce the final l-hop feature representations. When l = 0, the model softly attends to each context to produce context-attended subject representation as\nĥs = K∑ k=1 γ̄skhs,k, (3)\nwhere γ̄sk is the attention weight of context ck obtained by normalizing over a score vector γs with entries γsk = v ⊤ s [s;hs,k; s ◦ hs,k]. Here hs,k is the subject-aware context representation computed in (1) corresponding to the k-th context ck. The final subject representation is produced as h̄s = Ws[s; ĥs; s ◦ ĥs] incorporating both original features and attended information. Similar procedure applies to each candidate entity to produce h̄a. We treat h̄s and h̄a as 0-hop subject and candidate representations, respectively.\nWhen l > 0, the context-level attention aims to produce the probability of each context being\nr1(s, a)\nrM (s, a)\nr1(s, c 1 k )\nrM (s, c 1 k )\nr1(c 1 k , a)\nrM (c 1 k , a)\nr1(s, c 1 k )\nrM (s, c 1 k )\nr1(c 2 k , a)\nrM (c 2 k , a)\nr1(c 1\nk , c\n2 k )\nrM (c 1\nk , c\n2 k )\nr 0 1 (s, a)\nr 0 M0 (s, a)\nr 1 1 (s, a)\nr 1 M1 (s, a)\nr 2 1 (s, a)\nr 2 M2 (s, a)\nc 1 1 c 1 K c 1 1 c 1 K\nq(s, a)\n0-hop 1-hop 2-hop\nc 2 1 c 2 K\nFigure 1: An example of multi-hop ILP. The existential predicates r1, ..., rM are used to define invented predicates rl1, ..., r l Ml through attentions. The invented predicates will produce the final clauses to define q.\nchosen as a bridging entity using\nplk = σ(v ⊤ l [h̄s;h l k; h̄s ◦ hlk]), (4)\nwhere σ(·) is the sigmoid function, hlk is the l-hop intermediate entity representation for context ck computed using (2)."
    }, {
      "heading" : "4.2 Multi-Hop Reasoner",
      "text" : "The multi-hop reasoner aims to conduct complex reasoning by first generating probable logic clauses and then evaluating each clause by instantiating the variables. The clause generation process is parameterized by attentive memories which compute the probability of selecting each atom to form a relevant clause to entail the query relation. An illustration of the procedure is shown in Figure 1 and will be elaborated in the next section. Then the clause evaluation process will ground each atom with query subjects, candidate entities or bridging entities. The outputs from the attentive reader, i.e., h̄s, h̄a and {hlk}’s (l > 0), can be regarded as these constant representations to compute the atom scores for clause evaluation and updates."
    }, {
      "heading" : "4.2.1 Clause Generation",
      "text" : "A definite clause is composed of atoms defined over relational predicates. Since there are no explicit relations given in this task, we pre-define a fixed set of relations for each corpus, named as existential predicates: PE={r1, ..., rM}, e.g., “located_in”, “next_to”. For expressiveness, we further create a set of invented predicates PI=∪Ll=0P lI defined from the existential predicates, inspired by (Evans and Grefenstette, 2018). Specifically, P lI = {rl1, ..., rlMl} consists of invented predicates r l m defined using l-hop reasoning clauses F0(X0, X1) ∧ ... ∧ Fl(Xl, Xl+1)⇒ rlm(X0, Xl+1). For example, located_in(X0, X1) ∧ next_to(X1, X2) ⇒\noutside(X0, X2) defines a 1-hop invented predicate “outside”. Here L is the maximum hop number. The final clauses defining the query relation will be produced by learning to select relevant invented predicates, e.g., rl1i (X,Y )∧ ...∧ r ln j (X,Y ) ⇒ q(X,Y ) with 0 ≤ l1 ≤ ... ≤ ln ≤ L. The number of actual inference steps ln to answer q is flexibly decided by the model itself (will be discussed later).\nThe clause generation process is divided into two stages: 1) generate clauses defining invented predicates using only the existential predicates; 2) generate final clauses defining query relation using only the invented predicates. To allow for smooth optimization, we parameterize both stages by computing an attention weight for each predicate indicating its probability to appear in the clause body. Specifically, we assign each predicate a learnable embedding to indicate its semantics. Let U ∈ RD×M denote the embedding matrix for M existential predicates and Ul ∈ RD×Ml (l ∈ {0, 1, ..., L}) denote the embedding matrix for l-hop invented predicates. In the first stage, we use attentive memories to generate\nSlt = sparsemax((W l tU l t) ⊤(WlbU)), (5)\nUlt+1 = U l t + S l t · (WlvU), (6)\nwhere Ul0 = U l. Wlt and W l b are transformation matrices for invented predicates (queries) and existential predicates (keys), respectively. We use sparsemax which is a sparse version of softmax (Martins and Astudillo, 2016) to select only a small number of predicates. Intuitively, to learn to define a l-hop invented predicate rlm, (5) and (6) will sequentially produce Ft(Xt, Xt+1) at each step t ∈{0, ..., l} to form the clause body by attending over all the existential predicates with attention weight Slt. For example, when l = 1, (5) first attends to the existential predicate ri to generate F0(X0, X1) = ri(X0, X1) at step t = 0, and then attends to another predicate rj to generate F1(X1, X2) = rj(X1, X2) at step t = 1. The resulting clause ri(X0, X1) ∧ rj(X1, X2) ⇒ r1m(X0, X2) defines the invented predicate r 1 m.\nIn the second stage, we produce H final clauses taking invented predicates to define the target atom q(X,Y ). Given an embedding uq ∈ RD for the target relation q, we use a multi-head attention mechanism to compute a probability distribution si over all the invented predicates for each head i ∈ {1, ...,H} to produce the i-th final clause:\nsi=sparsemax{(Wiquq)⊤(Wih[U0;...;UL])} (7)\nwhere si is a sparse selective distribution over PI = {r01, ..., rLML}. For example, if si selects r 0 1 and r 1 2, the final clause becomes r01(X,Y ) ∧ r12(X,Y ) ⇒ q(X,Y ), which involves at most 1 inference step (r12). This completes the recursive rule generation step with multi-hop inference. To this end, we generate H clauses that can be used to define q(X,Y )."
    }, {
      "heading" : "4.2.2 Clause Evaluation",
      "text" : "Instantiation The clauses generated using the attentive memories will be tested and refined against the given positive and negative examples, known as learning from entailment that tries to maximize the truth probabilities of positive examples and minimize those of negative examples. The positive examples correspond to q(s, a) and the negative examples correspond to {q(s, aj)}’s, where s, a and aj refers to the query subject, correct answer and incorrect candidate, respectively. To obtain the truth probabilities of these atoms, we first instantiate the variables for each generated clause, e.g., X = s and Y = a (or Y = aj) in q(X,Y ). The bridging variables X1, ..., Xl are instantiated using the bridging contexts selected via the attentive reader as introduced in 4.1. To avoid inaccurate selection, for each Xl, we pick K contexts {cl1, ..., clK} with highest probabilities according to plk in (4). Neural Logic Operator Given a definite clause b1 ∧ ... ∧ bK ⇒ h consisting of grounded atoms (e.g., b1 = r1(s, a)), we could obtain the value for its head atom as µ(h) = µ(b1 ∧ ... ∧ bk). To compute the RHS involving logic operators (∧, ∨), T-norm (Klement et al., 2013) is usually adopted: T : [0, 1] × [0, 1] → [0, 1]. For example, minimum t-norm defines T∧(µ1, µ2) = min(µ1, µ2), T∨(µ1, µ2) = max(µ1, µ2). Product t-norm defines T∧(µ1, µ2) = µ1 ·µ2, T∨(µ1, µ2) = 1− (1− µ1)·(1−µ2). Here µ1, µ2 ∈ [0, 1] refer to the value for the body atoms. However, minimum t-norm is prone to learning plateau because the gradient only flows through one of the inputs. Product t-norm is less stable and is prone to exponential decay when the number of atoms in the clause grows.\nTo address these limitations, we propose a novel neural logic operator G defined as follows:\nG∨(µ1, ..., µK)=1−exp ( K∑ k=1 log(1− µk + ϵ) ) 1 K\nG∧(µ1, ..., µK)=exp ( K∑ k=1 log(µk + ϵ) ) 1 K , (8)\nwhere µ1, ..., µK ∈ [0, 1] refer to the probabilistic values of all the atoms in the conjunctive (∧) or disjunctive (∨) clause. ϵ is a small value to guarantee the validity for logarithm. The operator G has the following property that is ideal for logic semantics:\nProposition 1. When ∀µk → 1 with 1 ≤ k ≤ K, G∧(µ1, ..., µK) → 1, aligning with logic “AND”. When ∃µk → 1, G∨(µ1, ..., µK) → 1, aligning with logic “OR”.\nProposition 2. 0 ≤ G∧(µ1, ..., µK) − µmin ≤ (K1/(1−K) − KK/(1−K))( ∏ k ̸=min µk)\n1/(K−1), where min refers to the index of the minimum value among {µ1, ..., µK}.\nIn other words, the difference between G∧ and µmin is bounded. When K = 2, the RHS of the inequality equals to 1/4 · µk ̸=min, which makes G∧ closer to µmin when µk ̸=min is smaller. The proof can be found in the Appendix. This formulation results in a more stable and smooth gradient flow compared to minimum t-norm. Moreover, It avoids exponential decay in the output when K > 1. It also facilitates neural learning when the exact clause is parameterized with attention scores. Evaluation With the neural logic operator defined above, the value for the head atom can be inferred once the value for each body atom is given. For grounded atoms over existential predicates, e.g., rm(s, a), we directly generate its value using a relational network F : Rd × Rd → RM that takes the features of two constant arguments as input to produce a probability distribution over all the existential predicates r1, ..., rM : F(h̄s, h̄a) = softmax(Wr tanh[h̄s; h̄a; h̄s − h̄a; h̄s ◦ h̄a]). Then µ(rm(s, a)) takes the m-th entry of F(h̄s, h̄a). Here h̄s and h̄a are the outputs from the attentive reader. Similarly, hlk can be regarded as the feature of clk generated from the attentive reader which is used to compute atom values with bridging entities, e.g., µ(rm(s, clk)).\nFor l-hop grounded atoms over invented predicates {rl1(s, a), ..., rlMl(s, a)}, we compute their values according to the value of the clause body that defines them, e.g., µ(F0(s, c1)∧ ...∧Fl(cl, a)) using neural logic operators:\nµl=max z∈Zl exp\n( l∑\nt=0\nSlt log(µ(zt,zt+1)+ϵ)\n) 1 (l+1)\n(9)\nHere µl = [µ(rl1(s, a)), ..., µ(r l Ml (s, a))]⊤ denotes the vector of the atom values formed by those l-hop invented predicates. We denote by\nZl = {(s, c1k, ..., clk, a)}1≤k≤K the set for all possible instantiations for l-hop reasoning and denote by zt the tth constant of z ∈ Zl. µ(zt,zt+1) = [µ(r1(zt, zt+1)), ..., µ(rM (zt, zt+1))]\n⊤ is a vector of values for grounded atoms over existential predicates. exp(·) thus gives a neural approximation of logic conjunctions as shown in (8) over {Ft(zt, zt+1)}0≤t≤l, each of which is a sparse selection of existential predicates using Slt. We use a max operator to generate the maximum score over all possible bridging entities to represent the final truth probability of each invented predicate. Intuitively, a relation between two entities should be satisfied as long as there is at least one instantiation that follows the rule. Also note that (9) has the effect that when Slt[i, j] ≈ 0, the corresponding predicate rj will have little effect on the value of its head rli, which is in contrast to existing T-norms.\nThe final value for q(s, a) is similarly computed:\nµ(q(s, a))=max 1≤i≤H\n{ exp ( si log([µ 0; ...;µL] + ϵ) )} .\nWe use the cross-entropy loss over µ(q(s, a)) as the final objective to train the entire model (except the word embeddings which are kept fixed) in an endto-end manner. Here we organize the dataset according to subject-candidate pairs: (sn, an). We associate the ground-truth label yn = 1 with (sn, an) if an is the correct answer, otherwise, yn = 0."
    }, {
      "heading" : "5 Experiment",
      "text" : "We conduct experiments on two multi-hop reading comprehension datasets, namely WikiHop and MedHop (Welbl et al., 2018). The WikiHop dataset contains 43,738 training and 5,129 development instances ranging over 277 query relations. MedHop is a medical dataset containing 1,620 training and 342 development instances with a unique query relation, i.e., “interact with”. For WikiHop, we experiment with both non-contextual (follow (Weber et al., 2019)) named as DILR and contextual word embeddings (BERT (Devlin et al., 2019)) named as DILR-BERT to demonstrate our model’s generalization ability. For MedHop, we use the same setting following (Weber et al., 2019). We define M = 10 relations as existential predicates and Ml = 5 invented predicates for each hop with l = 0, 1, 2. The number of final clauses to define the query relation is H = 5 and the number of candidate bridging contexts for each hop is set to K = 5. The dimension of predicate embeddings\nand biGRU layer is 100 and 200, respectively. For training, we adopt Adam optimization with learning rate initialized at 0.001. The batch size is set to 10. For all the experiments, we use the development dataset to evaluate the results because the test data is not publicly available."
    }, {
      "heading" : "5.1 Experimental Result",
      "text" : "Weber et al. (2019) only selects four different query relations from WikiHop, namely Publisher, Developer, Country and Record_label, to evaluate their model. For fair comparison, we first follow their setting to compare on these specific domains. Besides BIDAF (Seo et al., 2017) and FastQA (Weissenborn et al., 2017), we also consider another three representative deep learning baselines : EPAr (Yichen Jiang and Bansal, 2019), HDEG (Tu et al., 2019), DynSAN (Zhuang and Wang, 2019)2, and a differentiable reasoning model DrMD adapted from (Dhingra et al., 2020). HDEG is a graph-based DNN. EPAr and DynSAN are memory-based DNNs. DrMD is implemented following (Dhingra et al., 2020), except that we remove pre-defined entities and only consider mention interactions given our settings. BERT is a baseline model that concatenates query subject (or a candidate entity) with each context in the form of “[CLS] query subject [SEP] context [SEP]”. Then\n2These baselines demonstrate good performances according to the leaderboard with available code for implementation.\nwe feed the hidden representations corresponding to the query subject (or candidates) into an attention model to generate a single vector to be fed into a classifier. For all the baselines, we train the models on each query relation separately to test the reasoning capability, same as our setting.\nTable 1 lists the results for MedHop and four query relations from WikiHop according to (Weber et al., 2019). Clearly, DILR substantially outperforms all the baselines on MedHop, demonstrating the importance of the reasoning capabilities for interaction-intensive medical dataset. On the four query relations from WikiHop, we still obtain the best performances. Though NLProlog also conducts logic reasoning, it is limited by the model’s capacity and relies on the extraction accuracy of the NER tool. Furthermore, we also evaluate on all the other valid query relations3 in WikiHop for a more complete analysis. The results in terms of accuracy are shown in Table 2. For a more thorough analysis, we group the query relations in terms of the number of training instances. As shown in Table 2, there are 38 relations (D1) containing less than 1,000 training examples, 7 relations (D2) with training examples ranging from 1,000 to 4,000 and 2 relations (D3) having more than 4,000 training examples. We report the micro-average accuracy scores over all the domains within each data group and their combinations in Table 2. Our model achieves the best performances over all data groups, demonstrating the advantage of combining deep attentive learning with logic reasoning. The margin is larger for D1 and D3, demonstrating the consistency of our proposed model with varying data sizes. In fact, ILP could be beneficial when training data is not sufficient via learning of generalized rules. Even with well-trained contextualized word embeddings, DILR still brings consistent performance\n3We consider those query relations containing at least 20 development and 100 training instances as valid.\n5 10 15 20 25 30 # final clauses: H\n0.65\n0.70\n0.75\n0.80\n0.85\nac cu\nra cy\nMedHop Country\n5 10 15 20 25 30 # existential predicates: M\n0.65\n0.70\n0.75\n0.80\n0.85\nMedHop Country\ngains. The Detailed comparison on each query relation can be found in Appendix."
    }, {
      "heading" : "5.2 Analysis",
      "text" : "To provide detailed analysis, we conduct ablation experiments on 6 datasets as shown in Table 3. For fair demonstration, we pick one relation in D3 (Located), 2 relations in D2 (Occupation and Record) and 3 relations in D1 (Publisher, Producer, Country). The first four rows reflect the accuracies by varying the maximum allowed number of reasoning hops (L). Clearly, ≤ 0 Hop and ≤ 3 Hop produce lower accuracies due to either missing bridging entities or overfitting with excessive inference steps. The middle part of Table 3 reflects the effect of each element of DILR. Specifically, −PI removes the invented predicates: remove (5), (6), (9) and replace Ul with U in (7). −ILP removes the reasoner and uses a classifier on top of the attentive reader to produce the final predictions. −AR removes the attentive reader and uses NER tools to extract entities for reasoning. −Rel only computes binary relations that decide whether two constants are related or not. This demonstrates the effect of relational reasoning considering different relations. By comparison, it is evident that removing any component will suffer from non-trivial prediction loss, especially for ILP. To verify the effect of the neural logic operator (NLO), we compare it with two T-norm operators, namely “prod-T” for product T-norm and “min-T” for minimum T-norm. Clearly, NLO produces the best performances across all the experiments.\nTo provide a concrete view of how the attentive reader filters relevant information and how the generated clauses look like, we list three examples as shown in Table 4. The underlined texts have the maximum attention weights learned from the attentive reader. The bold texts indicate the query subject and the correct answer for each query. Clearly, the attentive reader is able to select bridging entities relevant to the answer. The third column lists some learned clauses from the reasoner. The first row of each example shows the clauses that define an invented predicate and the second row shows the final clause that entails the query relation4. We use abbreviated entities as the constants in each grounded atom (e.g., “CC” is short for “Chris Church”) due to space limitations. The two clauses for the first example could be read as: if Chris Church and Massachusetts has relation r7, and Massachusetts and United States has relation r2, then the country of Chris Church is United States.\nWe further demonstrate the robustness of DILR by varying model parameters, as shown in Figure 2. The top subplots reveal the accuracies on MedHop and Country datasets when changing the number of final clauses H (left) and the number of existential predicates M (right). The subplot in the bottom depicts the accuracies when varying the number of instantiations K of the bridging contexts for Genre dataset under both DILR and BERT-DILR models. We shall observe that the performances are relatively stable given that the total number of testing examples are less than 400 for each dataset."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose an end-to-end model DILR to solve the problem of multi-hop reading comprehension. DILR smoothly connects a hierarchical attentive reader with a multi-hop reasoner to conduct automatic information extraction and complex reasoning. We also introduce differentiable logic operators to induce valid clauses with smooth and stable gradient-based learning. Extensive experiments reveal consistent improvements brought by DILR.\n4We convert the attention scores to the exact clauses by keeping those predicates with scores higher than 0.4."
    }, {
      "heading" : "A Complete Comparison result",
      "text" : "A complete accuracy comparison on the WikiHop dataset (only considering the domains with more than 20 development examples) is listed in Table 5."
    } ],
    "references" : [ {
      "title" : "Neural module networks",
      "author" : [ "J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 39–48.",
      "citeRegEx" : "Andreas et al\\.,? 2016",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Logical rule induction and theory learning using neural theorem proving",
      "author" : [ "Andres Campero", "Aldo Pareja", "Tim Klinger", "Josh Tenenbaum", "Sebastian Riedel." ],
      "venue" : "CoRR, abs / 1809.02193.",
      "citeRegEx" : "Campero et al\\.,? 2018",
      "shortCiteRegEx" : "Campero et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-hop question answering via reasoning chains",
      "author" : [ "Jifan Chen", "Shih-Ting Lin", "Greg Durrett." ],
      "venue" : "CoRR, abs/1910.02610.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension",
      "author" : [ "Xinyun Chen", "Chen Liang", "Adams Wei Yu", "Denny Zhou", "Dawn Song", "Quoc V. Le." ],
      "venue" : "8th International Conference on Learning Represen-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple and effective multi-paragraph reading comprehension",
      "author" : [ "Christopher Clark", "Matt Gardner." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 845–855.",
      "citeRegEx" : "Clark and Gardner.,? 2018",
      "shortCiteRegEx" : "Clark and Gardner.",
      "year" : 2018
    }, {
      "title" : "Tensorlog: A differentiable deductive database",
      "author" : [ "William W. Cohen." ],
      "venue" : "CoRR, abs/1605.06523.",
      "citeRegEx" : "Cohen.,? 2016",
      "shortCiteRegEx" : "Cohen.",
      "year" : 2016
    }, {
      "title" : "Question answering by reasoning across documents with graph convolutional networks",
      "author" : [ "Nicola De Cao", "Wilker Aziz", "Ivan Titov." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural models for reasoning over multiple mentions using coreference",
      "author" : [ "Bhuwan Dhingra", "Qiao Jin", "Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Dhingra et al\\.,? 2018",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2018
    }, {
      "title" : "Differentiable reasoning over a virtual knowledge base",
      "author" : [ "Bhuwan Dhingra", "Manzil Zaheer", "Vidhisha Balachandran", "Graham Neubig", "Ruslan Salakhutdinov", "William W. Cohen." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Dhingra et al\\.,? 2020",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2020
    }, {
      "title" : "Cognitive graph for multi-hop reading comprehension at scale",
      "author" : [ "Ming Ding", "Chang Zhou", "Qibin Chen", "Hongxia Yang", "Jie Tang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2694–2703.",
      "citeRegEx" : "Ding et al\\.,? 2019",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural logic machines",
      "author" : [ "Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning explanatory rules from noisy data",
      "author" : [ "Richard Evans", "Edward Grefenstette." ],
      "venue" : "J. Artif. Intelligent Res., 61:1–64.",
      "citeRegEx" : "Evans and Grefenstette.,? 2018",
      "shortCiteRegEx" : "Evans and Grefenstette.",
      "year" : 2018
    }, {
      "title" : "Fast relational learning using bottom clause propositionalization with artificial neural networks",
      "author" : [ "Manoel V. França", "Gerson Zaverucha", "Artur S. D’avila Garcez" ],
      "venue" : null,
      "citeRegEx" : "França et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "França et al\\.",
      "year" : 2014
    }, {
      "title" : "Jointly embedding knowledge graphs and logical rules",
      "author" : [ "Shu Guo", "Quan Wang", "Lihong Wang", "Bin Wang", "Li Guo." ],
      "venue" : "EMNLP, pages 192–202.",
      "citeRegEx" : "Guo et al\\.,? 2016",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural module networks for reasoning over text",
      "author" : [ "Nitish Gupta", "Kevin Lin", "Dan Roth", "Sameer Singh", "Matt Gardner." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Gupta et al\\.,? 2020",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "Harnessing deep neural networks with logic rules",
      "author" : [ "Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing." ],
      "venue" : "ACL, pages 2410–2420.",
      "citeRegEx" : "Hu et al\\.,? 2016",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Self-assembling modular networks for interpretable multi-hop reasoning",
      "author" : [ "Yichen Jiang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Jiang and Bansal.,? 2019",
      "shortCiteRegEx" : "Jiang and Bansal.",
      "year" : 2019
    }, {
      "title" : "Triangular norms",
      "author" : [ "Erich Peter Klement", "Radko Mesiar", "Endre Pap." ],
      "venue" : "Springer Science and Business Media.",
      "citeRegEx" : "Klement et al\\.,? 2013",
      "shortCiteRegEx" : "Klement et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting explicit paths for multihop reading comprehension",
      "author" : [ "Souvik Kundu", "Tushar Khot", "Ashish Sabharwal", "Peter Clark." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2737–2747.",
      "citeRegEx" : "Kundu et al\\.,? 2019",
      "shortCiteRegEx" : "Kundu et al\\.",
      "year" : 2019
    }, {
      "title" : "Augmenting neural networks with first-order logic",
      "author" : [ "Tao Li", "Vivek Srikumar." ],
      "venue" : "ACL, pages 292– 302.",
      "citeRegEx" : "Li and Srikumar.,? 2019",
      "shortCiteRegEx" : "Li and Srikumar.",
      "year" : 2019
    }, {
      "title" : "Deepproblog: Neural probabilistic logic programming",
      "author" : [ "Robin Manhaeve", "Sebastijan Dumancic", "Angelika Kimmig", "Thomas Demeester", "Luc De Raedt." ],
      "venue" : "NeurIPS, pages 3749–3759.",
      "citeRegEx" : "Manhaeve et al\\.,? 2018",
      "shortCiteRegEx" : "Manhaeve et al\\.",
      "year" : 2018
    }, {
      "title" : "From softmax to sparsemax: A sparse model of attention and multi-label classification",
      "author" : [ "Andre Martins", "Ramon Astudillo." ],
      "venue" : "ICML, volume 48, pages 1614–1623.",
      "citeRegEx" : "Martins and Astudillo.,? 2016",
      "shortCiteRegEx" : "Martins and Astudillo.",
      "year" : 2016
    }, {
      "title" : "Multi-hop reading comprehension through question decomposition and rescoring",
      "author" : [ "Sewon Min", "Victor Zhong", "Luke Zettlemoyer", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Differentiable reasoning on large knowledge bases and natural language",
      "author" : [ "Pasquale Minervini", "Matko Bosnjak", "Tim Rocktäschel", "Sebastian Riedel", "Edward Grefenstette." ],
      "venue" : "AAAI, pages 5182–5190.",
      "citeRegEx" : "Minervini et al\\.,? 2020",
      "shortCiteRegEx" : "Minervini et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial sets for regularised neural link predictors",
      "author" : [ "Pasquale Minervini", "Thomas Demeester", "Tim Rocktäschel", "Sebastian Riedel." ],
      "venue" : "UAI.",
      "citeRegEx" : "Minervini et al\\.,? 2017",
      "shortCiteRegEx" : "Minervini et al\\.",
      "year" : 2017
    }, {
      "title" : "Inductive logic programming",
      "author" : [ "Stephen Muggleton." ],
      "venue" : "New Generation Computing, 8:295–318.",
      "citeRegEx" : "Muggleton.,? 1991",
      "shortCiteRegEx" : "Muggleton.",
      "year" : 1991
    }, {
      "title" : "Dynamically fused graph network for multi-hop reasoning",
      "author" : [ "Lin Qiu", "Yunxuan Xiao", "Yanru Qu", "Hao Zhou", "Lei Li", "Weinan Zhang", "Yong Yu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–",
      "citeRegEx" : "Qiu et al\\.,? 2019",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2019
    }, {
      "title" : "Probabilistic logic neural networks for reasoning",
      "author" : [ "Meng Qu", "Jian Tang." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Qu and Tang.,? 2019",
      "shortCiteRegEx" : "Qu and Tang.",
      "year" : 2019
    }, {
      "title" : "End-toend differentiable proving",
      "author" : [ "Tim Rocktäschel", "Sebastian Riedel." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 3788–3800.",
      "citeRegEx" : "Rocktäschel and Riedel.,? 2017",
      "shortCiteRegEx" : "Rocktäschel and Riedel.",
      "year" : 2017
    }, {
      "title" : "Query-reduction networks for question answering",
      "author" : [ "Minjoon Seo", "Sewon Min", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Seo et al\\.,? 2017",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2017
    }, {
      "title" : "Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks",
      "author" : [ "Linfeng Song", "Zhiguo Wang", "Mo Yu", "Yue Zhang", "Radu Florian", "Daniel Gildea." ],
      "venue" : "CoRR, abs/1809.02040.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-hop reading comprehension across documents with path-based graph convolutional network",
      "author" : [ "Zeyun Tang", "Yongliang Shen", "Xinyin Ma", "Wei Xu", "Jiale Yu", "Weiming Lu." ],
      "venue" : "Proceedings of the TwentyNinth International Joint Conference on Artificial",
      "citeRegEx" : "Tang et al\\.,? 2020",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "author" : [ "Ming Tu", "Guangtao Wang", "Jing Huang", "Yun Tang", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association",
      "citeRegEx" : "Tu et al\\.,? 2019",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2019
    }, {
      "title" : "Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
      "author" : [ "Po-Wei Wang", "Priya L. Donti", "Bryan Wilder", "J. Zico Kolter." ],
      "venue" : "ICML, pages 6545–6554.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Integrating deep learning with logic fusion for information extraction",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan." ],
      "venue" : "AAAI, pages 9225–9232.",
      "citeRegEx" : "Wang and Pan.,? 2020",
      "shortCiteRegEx" : "Wang and Pan.",
      "year" : 2020
    }, {
      "title" : "Multipassage machine reading comprehension with crosspassage answer verification",
      "author" : [ "Yizhong Wang", "Kai Liu", "Jing Liu", "Wei He", "Yajuan Lyu", "Hua Wu", "Sujian Li", "Haifeng Wang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "NLProlog: Reasoning with weak unification for question answering in natural language",
      "author" : [ "Leon Weber", "Pasquale Minervini", "Jannes Münchmeyer", "Ulf Leser", "Tim Rocktäschel." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Weber et al\\.,? 2019",
      "shortCiteRegEx" : "Weber et al\\.",
      "year" : 2019
    }, {
      "title" : "Fastqa: A simple and efficient neural architecture for question answering",
      "author" : [ "Dirk Weissenborn", "Georg Wiese", "Laura Seiffe." ],
      "venue" : "arXiv: Computation and Language.",
      "citeRegEx" : "Weissenborn et al\\.,? 2017",
      "shortCiteRegEx" : "Weissenborn et al\\.",
      "year" : 2017
    }, {
      "title" : "Constructing datasets for multi-hop reading comprehension across documents",
      "author" : [ "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:287– 302.",
      "citeRegEx" : "Welbl et al\\.,? 2018",
      "shortCiteRegEx" : "Welbl et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep Weighted MaxSAT for Aspect-based Opinion Extraction",
      "author" : [ "Meixi Wu", "Wenya Wang", "Sinno Jialin Pan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5618–5628.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "A semantic loss function for deep learning with symbolic knowledge",
      "author" : [ "Jingyi Xu", "Zilu Zhang", "Tal Friedman", "Yitao Liang", "Guy Van den Broeck." ],
      "venue" : "ICML, pages 5502–5511.",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Differentiable learning of logical rules for knowledge base reasoning",
      "author" : [ "Fan Yang", "Zhilin Yang", "William W Cohen." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 2319–2328.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Learn to explain efficiently via neural logic inductive learning",
      "author" : [ "Yuan Yang", "Le Song." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Yang and Song.,? 2020",
      "shortCiteRegEx" : "Yang and Song.",
      "year" : 2020
    }, {
      "title" : "Explore, propose, and assemble: An interpretable model for multi-hop reading comprehension",
      "author" : [ "Yen-chun Chen Yichen Jiang", "Nitish Joshi", "Mohit Bansal." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Nitish Shirish Keskar", "Richard Socher." ],
      "venue" : "7th International Conference on Learning Representations, ICLR.",
      "citeRegEx" : "Zhong et al\\.,? 2019",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    }, {
      "title" : "Token-level dynamic self-attention network for multi-passage reading comprehension",
      "author" : [ "Yimeng Zhuang", "Huadong Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2252–2262.",
      "citeRegEx" : "Zhuang and Wang.,? 2019",
      "shortCiteRegEx" : "Zhuang and Wang.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "However, when the background knowledge is expressed in natural languages, as shown in the multi-hop reading comprehension problem with triplet-form questions (Welbl et al., 2018), it becomes difficult to conduct complex reasoning.",
      "startOffset" : 158,
      "endOffset" : 178
    }, {
      "referenceID" : 45,
      "context" : "Deep learning methods for multi-hop reading comprehension (RC) can be categorized as: 1) Memory-based models (Zhong et al., 2019; Wang et al., 2018; Zhuang and Wang, 2019) that learn to generate query-aware context representations.",
      "startOffset" : 109,
      "endOffset" : 171
    }, {
      "referenceID" : 36,
      "context" : "Deep learning methods for multi-hop reading comprehension (RC) can be categorized as: 1) Memory-based models (Zhong et al., 2019; Wang et al., 2018; Zhuang and Wang, 2019) that learn to generate query-aware context representations.",
      "startOffset" : 109,
      "endOffset" : 171
    }, {
      "referenceID" : 46,
      "context" : "Deep learning methods for multi-hop reading comprehension (RC) can be categorized as: 1) Memory-based models (Zhong et al., 2019; Wang et al., 2018; Zhuang and Wang, 2019) that learn to generate query-aware context representations.",
      "startOffset" : 109,
      "endOffset" : 171
    }, {
      "referenceID" : 31,
      "context" : "2) Graph-based approaches (Song et al., 2018; De Cao et al., 2019) that use graph neural networks to propagate information based on pre-constructed entity (context) graphs.",
      "startOffset" : 26,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "3) Neural Module networks (Andreas et al., 2016) that decompose the question into a series of action modules (Jiang and Bansal, 2019; Min et al.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : ", 2016) that decompose the question into a series of action modules (Jiang and Bansal, 2019; Min et al., 2019).",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : ", 2016) that decompose the question into a series of action modules (Jiang and Bansal, 2019; Min et al., 2019).",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "Inductive logic programming (ILP) (Muggleton, 1991) aligns with human reasoning by inducing interpretable rules to entail positive but not negative examples.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "Combining deep learning with ILP is a promising direction to benefit from both worlds (Evans and Grefenstette, 2018; Dong et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "Combining deep learning with ILP is a promising direction to benefit from both worlds (Evans and Grefenstette, 2018; Dong et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 135
    }, {
      "referenceID" : 43,
      "context" : "Deep logic models have been proposed for structured knowledge base completion (Minervini et al., 2017, 2020; Yang and Song, 2020; Rocktäschel and Riedel, 2017; Yang et al., 2017).",
      "startOffset" : 78,
      "endOffset" : 178
    }, {
      "referenceID" : 29,
      "context" : "Deep logic models have been proposed for structured knowledge base completion (Minervini et al., 2017, 2020; Yang and Song, 2020; Rocktäschel and Riedel, 2017; Yang et al., 2017).",
      "startOffset" : 78,
      "endOffset" : 178
    }, {
      "referenceID" : 42,
      "context" : "Deep logic models have been proposed for structured knowledge base completion (Minervini et al., 2017, 2020; Yang and Song, 2020; Rocktäschel and Riedel, 2017; Yang et al., 2017).",
      "startOffset" : 78,
      "endOffset" : 178
    }, {
      "referenceID" : 8,
      "context" : "Multi-Hop Reading Comprehension Recent works for multi-hop RC include memory-based methods which apply attentions to iteratively update query and context representations considering their interactions (Dhingra et al., 2018; Clark and Gardner, 2018; Wang et al., 2018; Zhong et al., 2019; Zhuang and Wang, 2019; Yichen Jiang and Bansal, 2019).",
      "startOffset" : 201,
      "endOffset" : 341
    }, {
      "referenceID" : 4,
      "context" : "Multi-Hop Reading Comprehension Recent works for multi-hop RC include memory-based methods which apply attentions to iteratively update query and context representations considering their interactions (Dhingra et al., 2018; Clark and Gardner, 2018; Wang et al., 2018; Zhong et al., 2019; Zhuang and Wang, 2019; Yichen Jiang and Bansal, 2019).",
      "startOffset" : 201,
      "endOffset" : 341
    }, {
      "referenceID" : 36,
      "context" : "Multi-Hop Reading Comprehension Recent works for multi-hop RC include memory-based methods which apply attentions to iteratively update query and context representations considering their interactions (Dhingra et al., 2018; Clark and Gardner, 2018; Wang et al., 2018; Zhong et al., 2019; Zhuang and Wang, 2019; Yichen Jiang and Bansal, 2019).",
      "startOffset" : 201,
      "endOffset" : 341
    }, {
      "referenceID" : 45,
      "context" : "Multi-Hop Reading Comprehension Recent works for multi-hop RC include memory-based methods which apply attentions to iteratively update query and context representations considering their interactions (Dhingra et al., 2018; Clark and Gardner, 2018; Wang et al., 2018; Zhong et al., 2019; Zhuang and Wang, 2019; Yichen Jiang and Bansal, 2019).",
      "startOffset" : 201,
      "endOffset" : 341
    }, {
      "referenceID" : 46,
      "context" : "Multi-Hop Reading Comprehension Recent works for multi-hop RC include memory-based methods which apply attentions to iteratively update query and context representations considering their interactions (Dhingra et al., 2018; Clark and Gardner, 2018; Wang et al., 2018; Zhong et al., 2019; Zhuang and Wang, 2019; Yichen Jiang and Bansal, 2019).",
      "startOffset" : 201,
      "endOffset" : 341
    }, {
      "referenceID" : 2,
      "context" : "Path modeling is also adopted in (Chen et al., 2019) using pointer networks.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "Another category uses neural module networks (Jiang and Bansal, 2019; Min et al., 2019; Gupta et al., 2020; Chen et al., 2020) to decompose the question into a series of actions, each parameterized with a neural module, which also fail to explicitly uncover the underlying logic for reasoning.",
      "startOffset" : 45,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : "Another category uses neural module networks (Jiang and Bansal, 2019; Min et al., 2019; Gupta et al., 2020; Chen et al., 2020) to decompose the question into a series of actions, each parameterized with a neural module, which also fail to explicitly uncover the underlying logic for reasoning.",
      "startOffset" : 45,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "Another category uses neural module networks (Jiang and Bansal, 2019; Min et al., 2019; Gupta et al., 2020; Chen et al., 2020) to decompose the question into a series of actions, each parameterized with a neural module, which also fail to explicitly uncover the underlying logic for reasoning.",
      "startOffset" : 45,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Another category uses neural module networks (Jiang and Bansal, 2019; Min et al., 2019; Gupta et al., 2020; Chen et al., 2020) to decompose the question into a series of actions, each parameterized with a neural module, which also fail to explicitly uncover the underlying logic for reasoning.",
      "startOffset" : 45,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules.",
      "startOffset" : 93,
      "endOffset" : 231
    }, {
      "referenceID" : 16,
      "context" : "Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules.",
      "startOffset" : 93,
      "endOffset" : 231
    }, {
      "referenceID" : 21,
      "context" : "Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules.",
      "startOffset" : 93,
      "endOffset" : 231
    }, {
      "referenceID" : 41,
      "context" : "Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules.",
      "startOffset" : 93,
      "endOffset" : 231
    }, {
      "referenceID" : 20,
      "context" : "Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules.",
      "startOffset" : 93,
      "endOffset" : 231
    }, {
      "referenceID" : 35,
      "context" : "Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules.",
      "startOffset" : 93,
      "endOffset" : 231
    }, {
      "referenceID" : 40,
      "context" : "Deep neural networks have been used to parameterize discrete logic operators and logic atoms (França et al., 2014; Hu et al., 2016; Manhaeve et al., 2018; Xu et al., 2018; Li and Srikumar, 2019; Wang and Pan, 2020; Wu et al., 2020) given the logic rules.",
      "startOffset" : 93,
      "endOffset" : 231
    }, {
      "referenceID" : 12,
      "context" : "A more challenging direction is inductive logic programming that automatically learns rules through representation learning and differentiable backpropagation (Evans and Grefenstette, 2018; Dong et al., 2019; Wang et al., 2019; Yang and Song, 2020).",
      "startOffset" : 159,
      "endOffset" : 248
    }, {
      "referenceID" : 11,
      "context" : "A more challenging direction is inductive logic programming that automatically learns rules through representation learning and differentiable backpropagation (Evans and Grefenstette, 2018; Dong et al., 2019; Wang et al., 2019; Yang and Song, 2020).",
      "startOffset" : 159,
      "endOffset" : 248
    }, {
      "referenceID" : 34,
      "context" : "A more challenging direction is inductive logic programming that automatically learns rules through representation learning and differentiable backpropagation (Evans and Grefenstette, 2018; Dong et al., 2019; Wang et al., 2019; Yang and Song, 2020).",
      "startOffset" : 159,
      "endOffset" : 248
    }, {
      "referenceID" : 43,
      "context" : "A more challenging direction is inductive logic programming that automatically learns rules through representation learning and differentiable backpropagation (Evans and Grefenstette, 2018; Dong et al., 2019; Wang et al., 2019; Yang and Song, 2020).",
      "startOffset" : 159,
      "endOffset" : 248
    }, {
      "referenceID" : 14,
      "context" : "Neuro-symbolic learning has been applied to knowledge-base completion through logic embeddings (Guo et al., 2016), tensor operations (Cohen, 2016; Rocktäschel and Riedel, 2017), adversarial learning (Minervini et al.",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : ", 2016), tensor operations (Cohen, 2016; Rocktäschel and Riedel, 2017), adversarial learning (Minervini et al.",
      "startOffset" : 27,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : ", 2016), tensor operations (Cohen, 2016; Rocktäschel and Riedel, 2017), adversarial learning (Minervini et al.",
      "startOffset" : 27,
      "endOffset" : 70
    }, {
      "referenceID" : 25,
      "context" : ", 2016), tensor operations (Cohen, 2016; Rocktäschel and Riedel, 2017), adversarial learning (Minervini et al., 2017), variational learning (Qu and Tang, 2019) or attentions (Yang and Song, 2020).",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 28,
      "context" : ", 2017), variational learning (Qu and Tang, 2019) or attentions (Yang and Song, 2020).",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 43,
      "context" : ", 2017), variational learning (Qu and Tang, 2019) or attentions (Yang and Song, 2020).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 29,
      "context" : "Differentiable theorem proving has also been proposed with weak unifications and backward chaining (Rocktäschel and Riedel, 2017; Campero et al., 2018; Minervini et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "Differentiable theorem proving has also been proposed with weak unifications and backward chaining (Rocktäschel and Riedel, 2017; Campero et al., 2018; Minervini et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 175
    }, {
      "referenceID" : 24,
      "context" : "Differentiable theorem proving has also been proposed with weak unifications and backward chaining (Rocktäschel and Riedel, 2017; Campero et al., 2018; Minervini et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 175
    }, {
      "referenceID" : 37,
      "context" : "The most related work to ours is NLProlog (Weber et al., 2019), a neural theorem prover for multi-hop RC by converting language utterances to distributed embeddings.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "For multi-hop reasoning, we use different attentions for finding or relocating target tokens in each context, inspired by (Gupta et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "For expressiveness, we further create a set of invented predicates PI=∪l=0P l I defined from the existential predicates, inspired by (Evans and Grefenstette, 2018).",
      "startOffset" : 133,
      "endOffset" : 163
    }, {
      "referenceID" : 22,
      "context" : "We use sparsemax which is a sparse version of softmax (Martins and Astudillo, 2016) to select only a small number of predicates.",
      "startOffset" : 54,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "To compute the RHS involving logic operators (∧, ∨), T-norm (Klement et al., 2013) is usually adopted: T : [0, 1] × [0, 1] → [0, 1].",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 39,
      "context" : "We conduct experiments on two multi-hop reading comprehension datasets, namely WikiHop and MedHop (Welbl et al., 2018).",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 37,
      "context" : "For WikiHop, we experiment with both non-contextual (follow (Weber et al., 2019)) named as DILR and contextual word embeddings (BERT (Devlin et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : ", 2019)) named as DILR and contextual word embeddings (BERT (Devlin et al., 2019)) named as DILR-BERT to demonstrate our model’s generalization ability.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 37,
      "context" : "For MedHop, we use the same setting following (Weber et al., 2019).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : "Besides BIDAF (Seo et al., 2017) and FastQA (Weissenborn et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 38,
      "context" : ", 2017) and FastQA (Weissenborn et al., 2017), we also consider another three representative deep learning baselines : EPAr (Yichen Jiang and Bansal, 2019), HDEG (Tu et al.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 33,
      "context" : ", 2017), we also consider another three representative deep learning baselines : EPAr (Yichen Jiang and Bansal, 2019), HDEG (Tu et al., 2019), DynSAN (Zhuang and Wang, 2019)2, and a differentiable reasoning model DrMD adapted from (Dhingra et al.",
      "startOffset" : 124,
      "endOffset" : 141
    }, {
      "referenceID" : 46,
      "context" : ", 2019), DynSAN (Zhuang and Wang, 2019)2, and a differentiable reasoning model DrMD adapted from (Dhingra et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : ", 2019), DynSAN (Zhuang and Wang, 2019)2, and a differentiable reasoning model DrMD adapted from (Dhingra et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "DrMD is implemented following (Dhingra et al., 2020), except that we remove pre-defined entities and only consider mention interactions given our settings.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 37,
      "context" : "Table 1 lists the results for MedHop and four query relations from WikiHop according to (Weber et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 108
    } ],
    "year" : 0,
    "abstractText" : "Multi-hop reading comprehension requires the ability to reason across multiple documents. On the one hand, deep learning approaches only implicitly encode query-related information into distributed embeddings which fail to uncover the discrete relational reasoning process to infer the correct answer. On the other hand, logic-based approaches provide interpretable rules to infer the target answer, but mostly work on structured data where entities and relations are well-defined. In this paper, we propose a deep-learning based inductive logic reasoning method that firstly extracts query-related (candidate-related) information, and then conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation. The reasoning process is accomplished via attentive memories with novel differentiable logic operators. To demonstrate the effectiveness of our model, we evaluate it on two reading comprehension datasets, namely WikiHop and MedHop.",
    "creator" : null
  }
}
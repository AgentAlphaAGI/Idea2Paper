{
  "name" : "ARR_2022_304_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), Electra (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019; Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) benchmark datasets.\nHowever, their reliability is recently being challenged. Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020; Elazar et al.,\n2021). These issues raise concerns about PLMs’ stability and reliability, precluding them from applications in practice, especially in risk-sensitive areas.\nAnother critical problem of PLMs is their inaccurate behaviour on negation, which is a principal property in many language understanding tasks. For tasks where the LNP holds (p is true iff ¬p is false; see Aina et al. 2018), PLMs should make different answers for the original and negated inputs. However, several studies observed that PLMs violate this property. In masked knowledge retrieval tasks, PLMs frequently generate incorrect answers for negated input queries (Ettinger, 2020; Kassner and Schütze, 2020). In other studies, PLMs show a poor generalisation ability on negated natural language inference (NLI) datasets (Naik et al., 2018; Hossain et al., 2020).\nAlthough the aforementioned studies produced promising analysis results, they limited the scope of the LNP only to adding negation expressions (e.g., “no” and “not”). However, other perturbations that generate the opposite meaning also can be applied to the property. Therefore, a consideration of such perturbation methods is necessary to fully assess whether PLMs satisfy the LNP.\nAlso, remedies to alleviate the problem have not been studied much yet. Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2019) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM). However, this approach has several downsides. First, like previous works, Hosseini et al. (2021) only considered negation expressions. Second, the data augmentation method is contingent on many additional linguistic components, which causes the dependency of model’s performance on certain modules and precludes applying the method to other languages where such resources are unavailable. Finally, the model should be pre-trained from scratch with the unlikelihood objective, which consumes considerable time and resources.\nIn this paper, we expand the boundary of the LNP\nto lexical semantics, i.e., synonyms and antonyms, and ascertain that PLMs are prone to violate the LNP. Next, we propose a remedy, called intermediatetraining on meaning-matching (IM2), that hardly employs additional linguistic components. We hypothesise that a leading cause lies in the MLM training objective, which assumes the distributional hypothesis for learning the meaning of the text (Sinha et al., 2021a). Instead, we design a model that directly learns the correspondence between words and their semantic contents. Through experiments, we verify that our approach improves the model’s comprehension of the LNP, while showing a stable performance on multiple downstream tasks.\nOur main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy named IM2 that is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4)."
    }, {
      "heading" : "2 Probing Tasks for Investigating the Logical Negation Property",
      "text" : "We design three probing tasks to evaluate whether PLMs satisfy the LNP: masked knowledge retrieval on negated queries (MKR-NQ), masked word retrieval (MWR), and synonym/antonym recognition (SAR). Brief illustrations of each task are in Figure 1."
    }, {
      "heading" : "2.1 Masked Knowledge Retrieval on Negated Queries",
      "text" : "The MKR-NQ task examines whether PLMs generate incorrect answers for negated queries. Following the work of Kassner and Schütze (2020), we constructed the evaluation dataset by negating the LAMA dataset (Petroni et al., 2019), which contains masked free-text forms of ConceptNet (Speer et al., 2017) triplets and their corresponding answers (e.g., (bird, CapableOf, fly) → (“A bird can [MASK]”, fly)). The task aims to generate a correct word through MLM.\nAccording to the LNP, a model must not generate the original answer if the query is negated. To measure how likely PLMs generate wrong predictions for negated queries, we collected pairs of (negated_query, wrong_predictions). We selected\nseveral relations in the LAMA dataset that ensure mutual exclusiveness between the original and negated queries.1 For negating sentences, we selected LAMA data points that contain a single verb using the Spacy parts of speech (POS) tagger (Honnibal and Johnson, 2015). Next, we added negation expressions, such as “not” and “don’t”, or removed such expressions if they existed. Finally, we collected the wrong predictions from ConceptNet by using the head entity and relation. As a result, we collected 3,360 data points for this task. The list of the relations that we used and examples of the data are in Table 10 in Appendix B."
    }, {
      "heading" : "2.2 Masked Word Retrieval",
      "text" : "To expand the boundary of the LNP to lexical semantics, we design the MWR task, which generates an answer of a masked query, asking for the synonym/antonym of a target word through MLM (e.g., “happy is the synonym of [MASK]”).\nLet sw and aw denote masked queries that ask the synonym and antonym of the word w. Also, let As and Aa refer to the list of correct answers for sw and aw, respectively. Intuitively, Aa becomes the wrong predictions of sw, because sw and aw have the opposite meaning. Therefore, we can evaluate the violation of the LNP by investigating whether a PLM generates wrong predictions.\nTo extract commonly-used words for our experiment, we first extracted nouns, adjectives, and adverbs that appear more than five times in the SNLI dataset (Bowman et al., 2015). Among the extracted candidates, we filtered words that have synonyms or antonyms in ConceptNet. Finally, we generated masked queries by employing templates used by Camburu et al. (2020). As a result, we collected about 27K data points for MWR. The templates and examples of the data are in Table 11 in Appendix B."
    }, {
      "heading" : "2.3 Synonym/Antonym Recognition",
      "text" : "SAR is a classification that distinguishes whether two given words are synonyms or antonyms. It aims to evaluate whether the contextualised representations of PLMs reflect the lexical meaning of words. Therefore, we use a parametric probing model (Adi et al., 2017; Liu et al., 2019a; Belinkov and Glass, 2019; Sinha et al., 2021a) for the experiment. Specifically, the experiment is performed on the final layer of each PLMs, i.e., we only train the classifier while keeping the encoder frozen. We use ConceptNet to build the dataset. ConceptNet has much more synonym triplets\n1For example, the HasProperty relation is not suitable to use, because sentences like “Some adults are immature” and “Some adults are not immature” are not mutually exclusive.\ncompared to antonyms. As a result, we randomly sample the synonym triplets to maintain a balance. To that end, we collect 33K, 1K, and 2K data points for the train, dev, and test datasets, respectively."
    }, {
      "heading" : "2.4 Evaluation Metrics",
      "text" : "We use the top-k hit rate (HR@k) to evaluate the performance on the MKR-NQ and MWR tasks. Assume that P = {(p1, c1), (p2, c2), . . . , (pn, cn)} denotes the set of predictions for a data point x, where pt and ct refer to the predicted word and confidence score of the t-th prediction, respectively. Then, the top-k hit rate for a data point x is defined as follows:\nHR@k(x) =\n∑k i=1 1(pi ∈ Wx)\nk ,\nwhere Wx is the wrong prediction set of x. Intuitively, the metric measures the ratio of top-k predicted words that belong to the wrong prediction set.\nTo reflect the confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights. It is worth to mention that lower metrics mean a better model performance in both cases. The weighted metric can be defined as follows:\nWHR@k(x) = ∑k i=1 ci × 1(pi ∈ Wx)∑k\ni=1 ci .\nFor the SAR task, we employ accuracy as an evaluation metric, because each data point has its own label, and the label distribution is not skewed."
    }, {
      "heading" : "3 PLMs Lack Information of Negation and Lexical Semantics",
      "text" : "We select the following PLMs for the experiments: bidirectional encoder representations from transformers (BERT)-base/large (Devlin et al., 2019), RoBERTa-base/large (Liu et al., 2019b), and ALBERT-base/large (Lan et al., 2019). These PLMs are pre-trained with the MLM training objective. We\nadded the Electra-small/base/large models (Clark et al., 2020) for the SAR task, which are trained with the replaced token prediction (RTP) training objective. No additional training is required for the MKR-NQ and MWR tasks. For the SAR task, we fine-tune each PLM for 10 epochs and apply the early stopping technique. We use the AdamW optimiser (Loshchilov and Hutter, 2017) for training with a learning rate of 5e−6 and a batch size of 32."
    }, {
      "heading" : "3.1 Results for MKR-NQ",
      "text" : "The results for the MKR-NQ task are summarised in Table 1. In general, the results are consistent with previous works (Ettinger, 2020; Kassner and Schütze, 2020). We observe three important characteristics from the experimental results.\nFirst, large models produce a higher hit rate than their corresponding base-size models in all three PLMs, recording an average of about 1.5 times higher values. This implies that large-size models are more likely to generate wrong predictions for negated queries, even though they perform better than smallsize models in many benchmark tests. The results suggest that evaluating a model’s performance solely based on the accuracy metric is unwise.\nSecond, the hit rate decreases as k increases, which implies that the majority of PLMs’ top predictions (e.g., k=1 or k=2) are incorrect. Finally, the weighted hit rate is much higher than the vanilla hit rate, suggesting that PLMs generate wrong predictions with high confidence."
    }, {
      "heading" : "3.2 Results for MWR",
      "text" : "The results of the MWR task are summarised in Table 1. The three characteristics found in the MKR-NQ task are also observed in the MWR task. Also, we found the following additional patterns.\nPLMs lack knowledge of antonyms. In general, the hit rates are extremely high compared to the MKR-NQ task in all the PLMs. Analysing their predictions, we find that PLMs generate incorrect pre-\ndictions primarily in antonym-asking queries. Specifically, the average HR@1 of the antonym-asking queries is 41.9%, while that of the synonym-asking queries is only 1.4%. A leading cause is that PLMs simply replicate the word presented in the input query. Table 2 shows the ratio of instances where each PLM reproduces the same word in a question. While the values are quite high for both synonym-asking and antonym-asking queries, the problem is more severe in the latter case, because the generated predictions are definitely incorrect. Based on our results, we conclude that PLMs’ contextualised representations lack lexical semantic information. Our conclusion is in line with the findings of Liu et al. (2019a) showing that encoder-fixed PLMs are not suitable to deal with tasks that require fine-grained linguistic knowledge.\nIssues are more severe with nouns. We observe that the hit rates are higher when a word in a question is a noun. Specifically, the average HR@1 values of nouns, adjectives, and adverbs are 35.1%, 27.4%, and 11.8%, respectively. Interestingly, PLMs have a high error rate when dealing with nouns even though\nthey are trained with a large written English corpus, where nouns form the greatest portion (at least 37%) of all POS tags (Hudson, 1994; Liang and Liu, 2013)."
    }, {
      "heading" : "3.3 Results for SAR",
      "text" : "As part of the comparison, we fine-tune each PLM on the SAR task, i.e., train the entire set of parameters. The results are summarised in Table 3. We observe a huge gap between the performance of fine-tuned models and that of encoder-fixed models. In contrast to the fine-tuned models that produce a high accuracy, encoder-fixed models fall short of expectations, even recording almost a random guess performance in BERT models. Also, just as a common belief, large models’ performance is greatly improved when finetuned. However, the difference between the large and small encoder-fixed models is insignificant, except for the Electra models that exhibit only a marginal improvement. The two phenomenons suggest that PLMs’ outstanding performance is predicated on updating a great many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019; McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information."
    }, {
      "heading" : "4 Intermediate Training on Meaning",
      "text" : "Matching Task: IM2"
    }, {
      "heading" : "4.1 Issue of PLMs",
      "text" : "Through the previous experiments, we observe that PLMs contain little information about negation and especially lexical semantics. We hypothesise a leading cause lies in the training objective of PLMs: the language modelling (LM) objective, which is a backbone pre-training task of almost all PLMs.\nIn the LM objective, words are generated based on given contexts. The distributional hypothesis (Harris, 1954), which assumes that semantically related or similar words will appear in similar contexts (Mrkšić et al., 2016), is the underpinning assumption of the LM objective (Sinha et al., 2021a). Under this assumption, a model learns the meaning of texts based\non their correlation to others. This is a great benefit, because a model can learn the meaning of texts using only the text form, allowing unsupervised training. Based on this advantage, many unsupervised representations, such as Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), and current PLMs, have been developed.\nHowever, the problem is that the distributional hypothesis does not consistently hold in natural language. Words having different meanings can appear in similar or even the same contexts. For instance, consider the two words “boy” and “girl”. Despite their antonymy, we can readily imagine sentences in which the two words appear in the same context, e.g., “The little boy/girl cuddled the teddy bear closely.”. As a result, the meaning of the two words would become quite similar if they were trained based on the distributional hypothesis.2 Similarly, negated sentences have almost identical contexts with their original forms. As a result, models can not learn the true meaning of words and negation expressions, provided they leverage only the text forms."
    }, {
      "heading" : "4.2 Meaning-Matching Task",
      "text" : "In the light of meaning-text theory, there is a correspondence between linguistic expressions (text) and semantic contents (meaning) (Mel’čuk and Žolkovskij, 1970; Milićević, 2006). Instead of solely relying on the distributional hypothesis, we propose the new meaning-matching task, which can directly learn the correspondence. Specifically, meaningmatching is a classification that takes a word and a sentence as input and determines whether the sentence defines the word correctly. Through this task, a model can learn both meaning-text correspondences and correlations between a word and other words in a definition, which is rarely found in general corpora.\nFor training PLMs on our new task, we apply the intermediate-training technique (Phang et al., 2018; Wang et al., 2019a; Liu et al., 2019a; Pruksachatkun et al., 2020; Vu et al., 2020), which first fine-tunes PLMs on an intermediate task, and then fine-tunes the model again on target tasks. It has been shown that training on intermediate tasks that require high-level linguistic knowledge and inference ability could improve performance (Liu et al., 2019a; Pruksachatkun et al., 2020). Furthermore, it is more efficient in time and resources than pre-training models on large corpora (e.g., BERTNOT model (Hosseini et al., 2021)).\n2The cosine similarity between the Word2Vec vectors of “boy” and “girl” is 0.85 (and 0.74 for “high” and “low”.)\nDataset. We collect about 150K free-text definitions that depict the meaning of English words from WordNet (Miller, 1995) and the English Word, Meaning, and Usage Examples dataset.3 In cases when a word appears in both datasets, we concatenate the word’s definitions. Several examples of our data are presented in Table 12 in Appendix B. We use publicly available English datasets for convenience, but our approach is easily adaptable to other languages, since most of them have their own dictionaries.\nTraining details. It is necessary to generate false word-definition pairs to train PLMs on the meaningmatching task. To achieve this, we use a negative sampling technique. We investigate the proper k in the range of 3, 5, 10, and 20. For a hyperparameter search, the performance of the RoBERTa-base model on the SAR task is used as a criterion. Figure 2 illustrates the SAR performance of the RoBERTa-base model with different k values. Intuitively, a large k value will lead the model to better performance by investigating more word-meaning combinations. However, we observe that the model performs the best when k is 10, and the performance decreases if k is too large. We conjecture a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem. As a result, large k values can increase the possibility of recognising the meaning of such similar words as different.\nTo avoid the class-imbalance issue in a batch, we duplicate the correct word-definition pairs k times when we construct the training data. For training,\n3https://data.world/idrismunir/english-word-meaning-andusage-examples/\nthe AdamW optimiser is used with a learning rate of 5e−6. We use 5% of data points for validation and train the models for 15 epochs with a batch size of 32. The early stopping technique is used to prevent overfitting."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : "We conduct the same probing tasks after the intermediate training on the meaning-matching task."
    }, {
      "heading" : "5.1 SAR Results",
      "text" : "We first focus on the SAR task. For training, we use the same hyperparameters as described in Section 3. The results are summarised in Table 5.\nImproved lexical semantic information. We generally observe marginal or no significant improvements when fine-tuning the whole parameters, espe-\ncially for large-size PLMs. However, with fixed encoder, the performance is significantly improved for PLMs with more than 100M parameters, and the improvements are more significant for large PLMs. Our results show that the proposed approach assists PLMs to learn enhanced contextualised representations with more abundant lexical semantic information.\nCatastrophic forgetting. We find that small PLMs, such as Electra-small and ALBERT models, show no significant increase in performance or are negatively impacted. Because all PLMs achieve a comparable performance on the meaning-matching task, we hypothesise that a leading cause is catastrophic forgetting (Pruksachatkun et al., 2020; Wallat et al., 2020), where the model forgets previous knowledge learned through pre-training to accept new information from the intermediate task. To verify this, we measure the change of parameter values after IM2. Concretely, let Mi and Mmmi denote the parameter of i-th layer before and after IM2. We calculate the average Frobenius norm for each layer:\nFi = 1\n|Mi| |Mi −Mmmi |F .\nFigure 3 shows the boxplots of Fi for each PLMs. We observe that the parameters of the Electra-small model, which is negatively impacted, are changed\nconsiderably compared to other PLMs having parameters more than 100M. The results suggest that the size of PLMs is an important property to prevent the catastrophic forgetting issue."
    }, {
      "heading" : "5.2 MKR-NQ and MWR Results",
      "text" : "Next, we perform the MKR-NQ and MWR tasks after applying the IM2 method. Since our models are not trained with the MLM objective, we replace the encoder of original PLMs with that of the models after fine-tuning on the meaning-matching task and reuse the MLM classifier. For the experiments, we use BERT-large and RoBERTa-large, because they are pre-trained based on the MLM objective, and parameters are hardly changed after applying the IM2 method. The results are summarised in Table 4.\nWe observe substantial decreases in the hit rates of incorrect predictions in both PLMs. For the MWR task, we find that the issue of regenerating a word in a given query is greatly relieved after applying the IM2 method. Specifically, the percentage of such instances drops from 40.3% to 19.6% and from 33.8% to 25.2% for BERT-large and RoBERTa-large, respectively. Several examples of the predicted results are presented in Table 6. The results lend support to our claim that the IM2 approach is of benefit to learning lexical semantic information and the meaning of negated expressions."
    }, {
      "heading" : "5.3 Fine-Tuning on the GLUE Benchmark",
      "text" : "A critical drawback of intermediate training is that the target task performance could be negatively impacted if the intermediate task is not related to the target task (Liu et al., 2019a; Pruksachatkun et al., 2020). To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and Electra-large on 7 GLUE benchmark datasets (Wang et al., 2018). The results are presented in Table 7.\nWe find no significant difference in performance for tasks with large datasets, such as the MNLI, QNLI, QQP, and SST2. On the contrary, tasks with small datasets, like the MRPC and RTE, are slightly improved. The result is consistent with Pruksachatkun et al. (2020) and Vu et al. (2020), which showed that smaller tasks benefit much more from the intermediate training. Furthermore, unlike the previous studies that observed a negative transfer with the COLA dataset (Phang et al., 2018; Pruksachatkun et al., 2020), the performance is improved in our approach. The result suggests that the meaningmatching is a safe intermediate task that ensures positive transfer with target downstream tasks."
    }, {
      "heading" : "5.4 Experiments on the NegNLI Dataset",
      "text" : "Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020) where negation plays an important role for NLI tasks. As a baseline, we compare the reported performance of BERTNOT (Hosseini et al., 2021), which is a recently proposed remedy to improve PLMs’ ability to understand negation. Since Hosseini et al. (2021) used BERT-base as a backbone model, we also apply the IM2 method to BERT-base. The results are summarised in Table 8.\nFor both SNLI and MNLI, we observe that our approach outperforms BERTNOT in NegNLI datasets, while yielding a comparable performance in the original development datasets. It is interesting that our approach improves the understanding of negation in both MKR-NQ and NegNLI tasks. We conjecture that a leading cause is that the definitions of the meaning-matching dataset contain many negation expressions, which enables a model to learn their proposed meaning (see Table 12). The results suggest that our proposed approach is more efficient than BERTNOT, because the IM2 method leverages less time and resources for training."
    }, {
      "heading" : "6 Related Work",
      "text" : "PLMs are at the core of many success stories in natural language processing (NLP). However, it remains unclear to what extent PLMs understand the syntactic and semantic properties of the human language. A series of probing tasks have been conducted on PLMs and have found them lacking or falling short on some language properties. Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020; Elazar et al., 2021).\nIn addition to the above faulty behaviours, Ettinger (2020) and Kassner and Schütze (2020) show that PLMs fail to comprehend negation, which is an important property of language in many natural language understanding (NLU) tasks. Ettinger (2020) check the ability of PLMs to understand of the meaning of negation in given contexts. In their work, they check whether models are sensitive in their completions of sentences that either include negation or not. Under normal circumstances, the completions are expected to vary in truth depending on the presence\nor absence of negation in given sentences. Their results show that PLMs are insensitive to the impacts of negations when completing sentences. Kassner and Schütze (2020) construct the negated LAMA dataset by inserting negation elements (e.g., “not”) in the LAMA cloze questions (Petroni et al., 2019). They use negated and original question pairs to query PLMs and establish that models are equally prone to make the same predictions for both the original and negated questions. In a well-informed setting, it is expected that PLMs should make different predictions for the original and negated questions. This shows that PLMs struggle to comprehend negation.\nIn light of the highlighted faulty behaviours of PLMs, especially their struggle to comprehend negation, Hosseini et al. (2021) propose a remedy to alleviate the problem. In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2019) based on negated sentences from the training corpus. They use a syntactic augmentation method to generate negated sentences. In this method, the dependency parse of the sentences, POS tags and morphological information of each word are taken as input and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007). During training, they replace objects in negated sentences with [MASK] tokens and use unlikelihood training to make the masked-out tokens unlikely under the PLM distribution. To ensure that negated sentences are factually false, they use the corresponding positive sentences as context for the unlikelihood prediction task.\nPrevious studies, e.g., Kassner and Schütze (2020),\nhave mostly limited the scope of the logical negation property only to the negation expressions (e.g., “no” and “not”). However, the core spirit of the property is opposite-meaning, which is not only limited to negation. Welleck et al. (2019) consider negating sentences using dependency tree regular expression patterns. This widens the scope of negation, as it is not only limited to negation expressions “no” and “not”. However, their approach relies on other components, such as Semgrex, and dependency and POS parsers which could impact the quality of the data, hence impact the models’ performance. In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy called intermediate-training on meaning-matching (IM2) that hardly employs additional linguistic components."
    }, {
      "heading" : "7 Summary and Outlook",
      "text" : "In this work, we investigated PLMs’ LNP. Compared to previous works that only examine negation expressions, we expanded the boundary of LNP to lexical semantics. We confirmed that PLMs are likely to violate LNP through extensive experiments.\nWe hypothesise that the distributional hypothesis results in PLMs’ lack of understanding of the true meaning of texts. To alleviate the issue, we proposed a novel intermediate task: meaning-matching. Via experiments, we verified that meaning-matching is a stable intermediate task that substantially improves PLMs’ understanding of negation and lexical semantic information while guaranteeing a positive transfer with multiple downstream tasks. Also, our approach produces a better performance on the negated NLI datasets compared to the unlikelihood training-based method, which leverages much more time and resources. Our work suggests that it is time to move beyond the distributional hypothesis to develop logically consistent and stable language models."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Hyperparameters for the GLUE Benchmark\nFor the GLUE benchmark experiments, we train models 10 epochs for each dataset and apply the early stopping technique where the patience number is set to 3. The batch size per GPU and learning rates used for each dataset are described in Table 9. Datasets with large training set (e.g., MNLI, QNLI, and QQP) were not sensitive to the hyperparameters.\nB Example Appendix"
    } ],
    "references" : [ {
      "title" : "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
      "author" : [ "Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Adi et al\\.,? 2017",
      "shortCiteRegEx" : "Adi et al\\.",
      "year" : 2017
    }, {
      "title" : "A distributional study of negated adjectives and antonyms",
      "author" : [ "Laura Aina", "Raffaella Bernardi", "Raquel Fernández." ],
      "venue" : "CEUR Workshop Proceedings, volume 2253.",
      "citeRegEx" : "Aina et al\\.,? 2018",
      "shortCiteRegEx" : "Aina et al\\.",
      "year" : 2018
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, pages 632–642.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Language models are few-shot",
      "author" : [ "Daniel Ziegler", "Jeffrey Wu", "Clemens Winter", "Chris Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei" ],
      "venue" : null,
      "citeRegEx" : "Ziegler et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ziegler et al\\.",
      "year" : 2020
    }, {
      "title" : "Make up your mind! adversarial generation of inconsistent natural language explanations",
      "author" : [ "Oana-Maria Camburu", "Brendan Shillingford", "Pasquale Minervini", "Thomas Lukasiewicz", "Phil Blunsom." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Camburu et al\\.,? 2020",
      "shortCiteRegEx" : "Camburu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning alignments and leveraging natural logic",
      "author" : [ "Nathanael Chambers", "Daniel Cer", "Trond Grenager", "David Hall", "Chloe Kiddon", "Bill MacCartney", "Marie-Catherine de Marneffe", "Daniel Ramage", "Eric Yeh", "Christopher D. Manning." ],
      "venue" : "Proceedings of the ACL-",
      "citeRegEx" : "Chambers et al\\.,? 2007",
      "shortCiteRegEx" : "Chambers et al\\.",
      "year" : 2007
    }, {
      "title" : "ELECTRA: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring and improving consistency in pretrained language models",
      "author" : [ "Goldberg." ],
      "venue" : "arXiv preprint arXiv:2102.01017.",
      "citeRegEx" : "Goldberg.,? 2021",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2021
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Bert & family eat word salad: Experiments with text understanding",
      "author" : [ "Ashim Gupta", "Giorgi Kvernadze", "Vivek Srikumar." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12946–12954.",
      "citeRegEx" : "Gupta et al\\.,? 2021",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2021
    }, {
      "title" : "Distributional structure",
      "author" : [ "Zellig S. Harris." ],
      "venue" : "Word, 10(2-3):146–162.",
      "citeRegEx" : "Harris.,? 1954",
      "shortCiteRegEx" : "Harris.",
      "year" : 1954
    }, {
      "title" : "An improved non-monotonic transition system for dependency parsing",
      "author" : [ "Matthew Honnibal", "Mark Johnson." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373–1378.",
      "citeRegEx" : "Honnibal and Johnson.,? 2015",
      "shortCiteRegEx" : "Honnibal and Johnson.",
      "year" : 2015
    }, {
      "title" : "An analysis of natural language inference benchmarks through the lens of negation",
      "author" : [ "Md Mosharaf Hossain", "Venelin Kovatchev", "Pranoy Dutta", "Tiffany Kao", "Elizabeth Wei", "Eduardo Blanco." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Hossain et al\\.,? 2020",
      "shortCiteRegEx" : "Hossain et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding by understanding not: Modeling negation in language models",
      "author" : [ "Arian Hosseini", "Siva Reddy", "Dzmitry Bahdanau", "R. Devon Hjelm", "Alessandro Sordoni", "Aaron Courville." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of",
      "citeRegEx" : "Hosseini et al\\.,? 2021",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2021
    }, {
      "title" : "About 37% of word-tokens are nouns",
      "author" : [ "Richard Hudson." ],
      "venue" : "Language, 70(2):331–339.",
      "citeRegEx" : "Hudson.,? 1994",
      "shortCiteRegEx" : "Hudson.",
      "year" : 1994
    }, {
      "title" : "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
      "author" : [ "Nora Kassner", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811–7818.",
      "citeRegEx" : "Kassner and Schütze.,? 2020",
      "shortCiteRegEx" : "Kassner and Schütze.",
      "year" : 2020
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Noun distribution in natural languages",
      "author" : [ "Junying Liang", "Haitao Liu." ],
      "venue" : "Poznań Studies in Contemporary Linguistics, 49(4):509–529.",
      "citeRegEx" : "Liang and Liu.,? 2013",
      "shortCiteRegEx" : "Liang and Liu.",
      "year" : 2013
    }, {
      "title" : "Birds have four legs?! NumerSense: Probing numerical commonsense knowledge of pre-trained language models",
      "author" : [ "Bill Yuchen Lin", "Seyeon Lee", "Rahul Khanna", "Xiang Ren." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "ArXiv, abs/1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448.",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards a functioning ’meaning-text’ model of language",
      "author" : [ "A.K.I.A. Mel’čuk" ],
      "venue" : "Žolkovskij",
      "citeRegEx" : "Mel.čuk,? \\Q1970\\E",
      "shortCiteRegEx" : "Mel.čuk",
      "year" : 1970
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A short guide to the meaningtext linguistic theory",
      "author" : [ "Jasmina Milićević." ],
      "venue" : "Journal of Koralex, 8:187–233.",
      "citeRegEx" : "Milićević.,? 2006",
      "shortCiteRegEx" : "Milićević.",
      "year" : 2006
    }, {
      "title" : "WordNet: A lexical database for English",
      "author" : [ "George A. Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Counter-fitting word vectors to linguistic constraints",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Blaise Thomson", "Milica Gašić", "Lina M. Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young." ],
      "venue" : "Proceedings of the 2016 Conference of the",
      "citeRegEx" : "Mrkšić et al\\.,? 2016",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2016
    }, {
      "title" : "Stress test evaluation for natural language inference",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Norman Sadeh", "Carolyn Rose", "Graham Neubig." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340–2353.",
      "citeRegEx" : "Naik et al\\.,? 2018",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2018
    }, {
      "title" : "Probing neural network comprehension of natural language arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658–4664.",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "Investigating the limitations of transformers with simple arithmetic tasks",
      "author" : [ "Rodrigo Nogueira", "Zhiying Jiang", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:2102.13019.",
      "citeRegEx" : "Nogueira et al\\.,? 2021",
      "shortCiteRegEx" : "Nogueira et al\\.",
      "year" : 2021
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks? arXiv preprint arXiv:2012.15180",
      "author" : [ "Thang M Pham", "Trung Bui", "Long Mai", "Anh Nguyen" ],
      "venue" : null,
      "citeRegEx" : "Pham et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R. Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Intermediate-task transfer learning with pretrained language models: When and why does it work",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "On the systematicity of probing contextualized word representations: The case of hypernymy in BERT",
      "author" : [ "Abhilasha Ravichander", "Eduard Hovy", "Kaheer Suleman", "Adam Trischler", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the Ninth Joint Conference on Lexical",
      "citeRegEx" : "Ravichander et al\\.,? 2020",
      "shortCiteRegEx" : "Ravichander et al\\.",
      "year" : 2020
    }, {
      "title" : "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
      "author" : [ "Koustuv Sinha", "Robin Jia", "Dieuwke Hupkes", "Joelle Pineau", "Adina Williams", "Douwe Kiela." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in",
      "citeRegEx" : "Sinha et al\\.,? 2021a",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "UnNatural Language Inference",
      "author" : [ "Koustuv Sinha", "Prasanna Parthasarathi", "Joelle Pineau", "Adina Williams." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Sinha et al\\.,? 2021b",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of the 31st AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Do NLP models know numbers? probing numeracy in embeddings",
      "author" : [ "Eric Wallace", "Yizhong Wang", "Sujian Li", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "BERTnesia: Investigating the capture and forgetting of knowledge in BERT",
      "author" : [ "Jonas Wallat", "Jaspreet Singh", "Avishek Anand." ],
      "venue" : "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 174–183.",
      "citeRegEx" : "Wallat et al\\.,? 2020",
      "shortCiteRegEx" : "Wallat et al\\.",
      "year" : 2020
    }, {
      "title" : "2019a. Can you",
      "author" : [ "Alex Wang", "Jan Hula", "Patrick Xia", "Raghavendra Pappagari", "R. Thomas McCoy", "Roma Patel", "Najoung Kim", "Ian Tenney", "Yinghui Huang", "Katherin Yu", "Shuning Jin", "Berlin Chen", "Benjamin Van Durme", "Edouard Grave", "Ellie Pavlick", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1905.00537.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural text generation with unlikelihood training",
      "author" : [ "Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1908.04319. 11",
      "citeRegEx" : "Welleck et al\\.,? 2019",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), Electra (Clark et al.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : ", 2019), Electra (Clark et al., 2020), and GPT-2 and -3 (Radford et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 38,
      "context" : ", 2020), and GPT-2 and -3 (Radford et al., 2019; Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al.",
      "startOffset" : 26,
      "endOffset" : 68
    }, {
      "referenceID" : 47,
      "context" : ", 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al.",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 46,
      "context" : ", 2018) and SuperGLUE (Wang et al., 2019b) benchmark datasets.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 35,
      "context" : "Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al.",
      "startOffset" : 145,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al.",
      "startOffset" : 145,
      "endOffset" : 205
    }, {
      "referenceID" : 41,
      "context" : "Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al.",
      "startOffset" : 145,
      "endOffset" : 205
    }, {
      "referenceID" : 43,
      "context" : ", 2021b), incomprehension on number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al.",
      "startOffset" : 60,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : ", 2021b), incomprehension on number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al.",
      "startOffset" : 60,
      "endOffset" : 123
    }, {
      "referenceID" : 32,
      "context" : ", 2021b), incomprehension on number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al.",
      "startOffset" : 60,
      "endOffset" : 123
    }, {
      "referenceID" : 39,
      "context" : ", 2021), and lack of semantic content understanding (Ravichander et al., 2020; Elazar et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "In masked knowledge retrieval tasks, PLMs frequently generate incorrect answers for negated input queries (Ettinger, 2020; Kassner and Schütze, 2020).",
      "startOffset" : 106,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "In masked knowledge retrieval tasks, PLMs frequently generate incorrect answers for negated input queries (Ettinger, 2020; Kassner and Schütze, 2020).",
      "startOffset" : 106,
      "endOffset" : 149
    }, {
      "referenceID" : 30,
      "context" : "ability on negated natural language inference (NLI) datasets (Naik et al., 2018; Hossain et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "ability on negated natural language inference (NLI) datasets (Naik et al., 2018; Hossain et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 48,
      "context" : "(2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2019) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 40,
      "context" : "We hypothesise that a leading cause lies in the MLM training objective, which assumes the distributional hypothesis for learning the meaning of the text (Sinha et al., 2021a).",
      "startOffset" : 153,
      "endOffset" : 174
    }, {
      "referenceID" : 34,
      "context" : "Following the work of Kassner and Schütze (2020), we constructed the evaluation dataset by negating the LAMA dataset (Petroni et al., 2019), which contains masked free-text forms of ConceptNet (Speer et al.",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 42,
      "context" : ", 2019), which contains masked free-text forms of ConceptNet (Speer et al., 2017) triplets and their corresponding answers (e.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "1 For negating sentences, we selected LAMA data points that contain a single verb using the Spacy parts of speech (POS) tagger (Honnibal and Johnson, 2015).",
      "startOffset" : 127,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "To extract commonly-used words for our experiment, we first extracted nouns, adjectives, and adverbs that appear more than five times in the SNLI dataset (Bowman et al., 2015).",
      "startOffset" : 154,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "Therefore, we use a parametric probing model (Adi et al., 2017; Liu et al., 2019a; Belinkov and Glass, 2019; Sinha et al., 2021a) for the experiment.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "Therefore, we use a parametric probing model (Adi et al., 2017; Liu et al., 2019a; Belinkov and Glass, 2019; Sinha et al., 2021a) for the experiment.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "Therefore, we use a parametric probing model (Adi et al., 2017; Liu et al., 2019a; Belinkov and Glass, 2019; Sinha et al., 2021a) for the experiment.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 40,
      "context" : "Therefore, we use a parametric probing model (Adi et al., 2017; Liu et al., 2019a; Belinkov and Glass, 2019; Sinha et al., 2021a) for the experiment.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "We select the following PLMs for the experiments: bidirectional encoder representations from transformers (BERT)-base/large (Devlin et al., 2019), RoBERTa-base/large (Liu et al.",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 22,
      "context" : ", 2019), RoBERTa-base/large (Liu et al., 2019b), and ALBERT-base/large (Lan et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "We added the Electra-small/base/large models (Clark et al., 2020) for the SAR task, which are trained with the replaced token prediction (RTP) training objective.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "(Loshchilov and Hutter, 2017) for training with a learning rate of 5e−6 and a batch size of 32.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "Interestingly, PLMs have a high error rate when dealing with nouns even though they are trained with a large written English corpus, where nouns form the greatest portion (at least 37%) of all POS tags (Hudson, 1994; Liang and Liu, 2013).",
      "startOffset" : 202,
      "endOffset" : 237
    }, {
      "referenceID" : 19,
      "context" : "Interestingly, PLMs have a high error rate when dealing with nouns even though they are trained with a large written English corpus, where nouns form the greatest portion (at least 37%) of all POS tags (Hudson, 1994; Liang and Liu, 2013).",
      "startOffset" : 202,
      "endOffset" : 237
    }, {
      "referenceID" : 31,
      "context" : "dating a great many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019; McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.",
      "startOffset" : 90,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "dating a great many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019; McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.",
      "startOffset" : 90,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "The distributional hypothesis (Harris, 1954), which assumes that semantically related or similar words will appear in similar contexts (Mrkšić et al.",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 29,
      "context" : "The distributional hypothesis (Harris, 1954), which assumes that semantically related or similar words will appear in similar contexts (Mrkšić et al., 2016), is the underpinning assumption of the LM objective (Sinha et al.",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 40,
      "context" : ", 2016), is the underpinning assumption of the LM objective (Sinha et al., 2021a).",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 26,
      "context" : "Based on this advantage, many unsupervised representations, such as Word2Vec (Mikolov et al., 2013), Glove (Pennington et al.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 33,
      "context" : ", 2013), Glove (Pennington et al., 2014), and current PLMs, have been developed.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "In the light of meaning-text theory, there is a correspondence between linguistic expressions (text) and semantic contents (meaning) (Mel’čuk and Žolkovskij, 1970; Milićević, 2006).",
      "startOffset" : 133,
      "endOffset" : 180
    }, {
      "referenceID" : 36,
      "context" : "For training PLMs on our new task, we apply the intermediate-training technique (Phang et al., 2018; Wang et al., 2019a; Liu et al., 2019a; Pruksachatkun et al., 2020; Vu et al., 2020), which first fine-tunes PLMs on an intermediate task, and then fine-tunes the model again on target tasks.",
      "startOffset" : 80,
      "endOffset" : 184
    }, {
      "referenceID" : 21,
      "context" : "For training PLMs on our new task, we apply the intermediate-training technique (Phang et al., 2018; Wang et al., 2019a; Liu et al., 2019a; Pruksachatkun et al., 2020; Vu et al., 2020), which first fine-tunes PLMs on an intermediate task, and then fine-tunes the model again on target tasks.",
      "startOffset" : 80,
      "endOffset" : 184
    }, {
      "referenceID" : 37,
      "context" : "For training PLMs on our new task, we apply the intermediate-training technique (Phang et al., 2018; Wang et al., 2019a; Liu et al., 2019a; Pruksachatkun et al., 2020; Vu et al., 2020), which first fine-tunes PLMs on an intermediate task, and then fine-tunes the model again on target tasks.",
      "startOffset" : 80,
      "endOffset" : 184
    }, {
      "referenceID" : 21,
      "context" : "It has been shown that training on intermediate tasks that require high-level linguistic knowledge and inference ability could improve performance (Liu et al., 2019a; Pruksachatkun et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 194
    }, {
      "referenceID" : 37,
      "context" : "It has been shown that training on intermediate tasks that require high-level linguistic knowledge and inference ability could improve performance (Liu et al., 2019a; Pruksachatkun et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 194
    }, {
      "referenceID" : 28,
      "context" : "We collect about 150K free-text definitions that depict the meaning of English words from WordNet (Miller, 1995) and the English Word, Meaning, and Usage Examples dataset.",
      "startOffset" : 98,
      "endOffset" : 112
    }, {
      "referenceID" : 37,
      "context" : "Because all PLMs achieve a comparable performance on the meaning-matching task, we hypothesise that a leading cause is catastrophic forgetting (Pruksachatkun et al., 2020; Wallat et al., 2020), where the model forgets previous knowledge learned through pre-training to accept new information from the intermediate task.",
      "startOffset" : 143,
      "endOffset" : 192
    }, {
      "referenceID" : 44,
      "context" : "Because all PLMs achieve a comparable performance on the meaning-matching task, we hypothesise that a leading cause is catastrophic forgetting (Pruksachatkun et al., 2020; Wallat et al., 2020), where the model forgets previous knowledge learned through pre-training to accept new information from the intermediate task.",
      "startOffset" : 143,
      "endOffset" : 192
    }, {
      "referenceID" : 21,
      "context" : "A critical drawback of intermediate training is that the target task performance could be negatively impacted if the intermediate task is not related to the target task (Liu et al., 2019a; Pruksachatkun et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 216
    }, {
      "referenceID" : 37,
      "context" : "A critical drawback of intermediate training is that the target task performance could be negatively impacted if the intermediate task is not related to the target task (Liu et al., 2019a; Pruksachatkun et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 216
    }, {
      "referenceID" : 47,
      "context" : "To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and Electra-large on 7 GLUE benchmark datasets (Wang et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 148
    }, {
      "referenceID" : 36,
      "context" : "Furthermore, unlike the previous studies that observed a negative transfer with the COLA dataset (Phang et al., 2018; Pruksachatkun et al., 2020), the performance is improved in our approach.",
      "startOffset" : 97,
      "endOffset" : 145
    }, {
      "referenceID" : 37,
      "context" : "Furthermore, unlike the previous studies that observed a negative transfer with the COLA dataset (Phang et al., 2018; Pruksachatkun et al., 2020), the performance is improved in our approach.",
      "startOffset" : 97,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020) where negation plays an important role for NLI tasks.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "As a baseline, we compare the reported performance of BERTNOT (Hosseini et al., 2021), which is a recently proposed remedy to improve PLMs’ ability to understand negation.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 35,
      "context" : "Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al.",
      "startOffset" : 145,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al.",
      "startOffset" : 145,
      "endOffset" : 205
    }, {
      "referenceID" : 40,
      "context" : "Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2020; Gupta et al., 2021; Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al.",
      "startOffset" : 145,
      "endOffset" : 205
    }, {
      "referenceID" : 43,
      "context" : ", 2021a), struggle to comprehend number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al.",
      "startOffset" : 64,
      "endOffset" : 127
    }, {
      "referenceID" : 20,
      "context" : ", 2021a), struggle to comprehend number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al.",
      "startOffset" : 64,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : ", 2021a), struggle to comprehend number-related representations (Wallace et al., 2019; Lin et al., 2020; Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al.",
      "startOffset" : 64,
      "endOffset" : 127
    }, {
      "referenceID" : 39,
      "context" : ", 2021), and display a lack of semantic content understanding (Ravichander et al., 2020; Elazar et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 109
    }, {
      "referenceID" : 48,
      "context" : "In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2019) based on negated sentences from the training corpus.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "In this method, the dependency parse of the sentences, POS tags and morphological information of each word are taken as input and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).",
      "startOffset" : 239,
      "endOffset" : 262
    } ],
    "year" : 0,
    "abstractText" : "The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs (p is true iff ¬p is false), is an important property that a trustworthy language model must satisfy. However, much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property. In this paper, we perform experiments using probing tasks to assess PLMs’ LNP understanding. Unlike previous studies that only examined negation expressions, we expand the boundary of the investigation to lexical semantics. Through experiments, we observe that PLMs violate the LNP frequently. To alleviate the issue, we propose a novel intermediate training task, named meaningmatching, designed to directly learn a meaningtext correspondence, instead of relying on the distributional hypothesis. Through multiple experiments, we find that the task enables PLMs to learn lexical semantic information. Also, through finetuning experiments on 7 GLUE tasks, we confirm that it is a safe intermediate task that guarantees a similar or better performance of downstream tasks. Finally, we observe that our proposed approach outperforms our previous counterparts despite its time and resource efficiency.",
    "creator" : null
  }
}
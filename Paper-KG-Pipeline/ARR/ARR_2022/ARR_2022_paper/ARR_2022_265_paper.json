{
  "name" : "ARR_2022_265_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-centric Summarization",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Listening to and understanding fairy tales or storybooks are very crucial for children’s early intellectual and literacy development (Sim and Berthelsen, 2014). During the storybook reading process, prompting suitable questions with educational purposes can help children understand the content and inspire their interests (Zevenbergen and Whitehurst, 2003; Ganotice et al., 2017).\nHowever, it is challenging even for humans to ask educationally meaningful questions to engage children in storybook reading, which could be due to adults lacking the skills or time to integrate such interactive opportunities (Golinkoff et al., 2019). Recent research shows that AI-powered conversational agents can play the role of language partners to read fairy tales to children and ask them educational questions (Xu et al., 2021). This mo-\ntivates us to investigate techniques to generate high-cognitive-demand educational questions for children’s storybooks automatically. Automating the generation of such questions can have great value in supporting children’s language development through guided conversation.\nDuring the interactive storybook reading process, high-cognitive-demand questions require children to make inferences and predictions based on information not explicitly mentioned in the text. In contrast to low-cognitive-demand questions describing facts in stories (e.g., Who is Snow White’s mother?), high-cognitive-demand questions are often related to events and their relations (e.g., Why did the queen want to kill Snow White? or What happened after the huntsman raised his dagger in the forest?).\nMost previous work on question generation (QG) focuses on generating questions based on predefined answer spans (Krishna and Iyyer, 2019; Pyatkin et al., 2021; Cho et al., 2021). Such systems cannot be directly used to support interactive storybook reading for children as it is unclear whether such questions are appropriate to help children improve their story comprehension skills during the storybook reading activity. Recently, Yao et al. (2021) released a fairytale question answering dataset FairytaleQA containing around 10.5k question-answer pairs annotated by education experts. In addition, each question is assigned to a specific type, such as “action” or “causal relation”. This makes it possible to investigate techniques to generate educational questions to support children’s interactive storybook reading.\nIn this paper, we propose a novel framework combining question type prediction and eventcentric summarization to generate educational questions for storybooks. In the first stage, we learn to predict the question type distribution for a given input and add pseudo-label so that after prediction, we can know both the types of questions and\nhow many questions of each type. In the second stage, conditioned on question types and the order of the question under the current question type, we extract salient events that are most likely for educators to design questions on and then generate an event-centric summarization of the original input. Finally, in the third stage, we use the output of the second stage to generate questions. Each summarization is used to generate one question. Note that it is difficult to obtain gold annotations for eventcentric summarization. Instead, we rewrite annotated questions, and their corresponding hypothesized answers into question-answer statements (Demszky et al., 2018) as silver training samples. We hypothesize that high-cognitive-demand questions are around main plots in narratives and can guide our summarization model to focus on salient events. We evaluate our system on the FairytaleQA dataset and show the superiority of the proposed method on both automatic and human evaluation metrics compared to strong baselines.\nTo summarize, the main contributions of this paper are three-fold:\n• We propose to decompose the educational question generation problem of children storybooks into a question type distribution learning problem and an event-centric summarization problem, which makes the procedure of generating educational questions easier and more explainable.\n• For question type distribution learning, adding a pseudo-label, we can predict both question types and the number of questions for each type. Conditioned on such information, we propose an efficient method to generate educational questions through an event-centric summarization module.\n• On the FairytaleQA dataset, experiments with both automatic and human evaluation confirm the superior performance of our method, showing its potential on real-world applications."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Question Generation",
      "text" : "Question answering based on context has achieved remarkable results (Rajpurkar et al., 2016; Zhang et al., 2020b). The reverse problem of question answering, namely, question generation (Duan et al., 2017; Chan and Fan, 2019), usually relies on preselecting spans from an input text as answers and a single sentence as the context. However, to generate questions across a long paragraph in which the\nkey information may come from multiple different sentences in fairy tales (Yao et al., 2021), these existing models relying on one text segment usually do not work well.\nA few studies are focusing on generating questions that are based on multi-sentence or multi-document information fusion. NarrativeQA (Kočiský et al., 2018) is an effort that tries to integrate key information across multiple locations of a paragraph for question answering/generation. Similarly, MS MARCO (Nguyen et al., 2016) is a dataset that integrates multiple locations of answers for users’ queries in search engines. In Cho et al. (2021), a contrastive method is proposed that first trains a supervised model to generate questions based on a single document and then uses a reinforcement learning agent to align multiple questions from multiple documents. In Lyu et al. (2021), the authors use a rule-based method to generate questions with summaries and report to achieve good performance.\nThe methods mentioned above usually do not consider the educational dimension and may not work well on fairy tales. Considering our research focus of fairytales, it is vital to generate questions that have educational purposes. In FairytaleQA (Yao et al., 2021), experts usually write different types of questions for separate paragraphs. We hypothesize that context plays a significant role in deciding the type of questions that should be asked during the interactive storybook reading with children. Therefore it is necessary to investigate not only how to summarize salient events but also how to learn the question type distribution."
    }, {
      "heading" : "2.2 Text Summarization",
      "text" : "Summarization methods can be classified into extractive summarization and abstractive summarization. Extractive methods select sentences from the source documents to compose a summary; abstractive methods applies neural generative models to generate the summary token-by-token.\nExtractive summarization methods, such as TextRank (Mihalcea and Tarau, 2004), feature-based methods (Jagadeesh et al., 2005; Luhn, 1958; Nallapati et al., 2017), and topic-based methods (Ozsoy et al., 2010), do not work to generate highcognitive-demand questions on the fairytale scenario because such questions often are based on multiple sentences.\nAbstractive methods based on encoder-decoder\narchitectures usually encode an input document token-by-token sequentially (Rush et al., 2015) and cannot capture the fine-grained hierarchical relations in a document, such as actions, causal relationships. Graph neural network (GNN) models are recently used in summarization research (Wu et al., 2021; Wang et al., 2020; Xu et al., 2020; Li et al., 2021), thanks to their ability to model the complex relations in a document. For example, in Xu et al. (2020), researchers used a discourse-level dependency graph to encode a document and then decoded discourse-level embeddings to select sentences extractively. Similarly, in Wang et al. (2020), researchers have used a heterogeneous graph to encode both token-level and sentence-level relations in a document and then used it to extract sentences. Still, in the education domain, summarizing salient events of one paragraph that can be used to generate educational questions is an open problem. In this paper, we develop an event-centric summarization method based on BART (Lewis et al., 2020). To obtain the training data, we compose educational question-answer pairs through a rule-based method and use them as silver ground-truth samples."
    }, {
      "heading" : "3 Method",
      "text" : "The overview of our educational question generation system for storybooks is shown in Figure 1, which contains three modules: question type distribution learning module, event-centric summary generation module, and educational question generation module.\nGiven an input paragraph d, we first predict the type distribution of output questions p = (p1, p2, . . . , pT ), where pi denotes the probability of question type i, T is the total number of question types. We then transform the distribution into the number of questions under each question type l = (l1, l2, . . . , lT ). Afterwards, we first generate li summaries of type i with the input paragraph d, and then generate li questions of type iwith the corresponding summaries. More details are presented in the following sections."
    }, {
      "heading" : "3.1 Question Type Distribution Learning",
      "text" : "We fine-tuned a BERT model (Devlin et al., 2019), and adapt the output m dimensional class token hc ∈ Rm to learn the question type distribution. Specifically, the predicted distribution is obtained by pi = e\n(Whc+b)i∑T i=1 e (Whc+b)i , where W ∈ RT×m, b ∈\nRT are learnable parameters, (·)i denotes the oper-\nator of selecting the i-th element of a vector. Assuming there are N training samples, we minimize the K-L divergence loss LK−L =∑N j=1 1 N ∑T i=1 p (j) i log p (j) i\np̂ (j) i\n, where p(j)i denotes the\nprobability of question type i for the j-th sample, and p̂(j)i is our predicted value.\nTo improve the prediction performance, similar to Zhang et al. (2018), we also conduct a multilabel classification task, where we use the question type with the maximal probability as the class of the output. In particular, we add a cross entropy loss LCE = − ∑N j=1 1 N ∑T i=1 1(y (j) i ) log ŷ (j) i , where 1(y (j) i ) equals to 1 if i is the question type with the maximal probability for the sample j. In summary, we conduct a multi-task learning for question type distribution prediction, and the final training loss is a weighted sum of the K-L loss and the cross entropy loss: L = γLK−L+(1−γ)LCE , where γ is a weight factor.\nTo predict the number of questions for each question type during training, we add a pseudo label 1 to the original label l = (l1, l2, . . . , ln), i.e., l = (l1, l2, . . . , ln, 1). We can then normalize it to get the ground-truth probability distribution l = ( l1∑n\nk=1 lk+1 , . . . , ln∑n k=1 lk+1 , 1∑n k=1 lk+1\n). During testing, assuming we get the predicted distribution p = (p1, p2, . . . , pn, ppseudo), we can obtain the number of each type of questions by diving the probability of this pseudo label ppseudo as: ni = b pippseudo + 0.5c."
    }, {
      "heading" : "3.2 Event-centric Summary Generation",
      "text" : "In FairytaleQA, usually one paragraph has multiple questions with different question types, and information in one educational question may scatter across multiple parts of the input paragraph. As mentioned before, we assume that context play a big role to decide the type and the number of questions to be asked during the interactive storybook reading. And high-cognitive-demand questions are around salient events and the relations. With the output from the previous component, we can use the predicted question type distribution as a control signal, and select corresponding events for one particular question type.\nIn particular, we add two control signals before an input paragraph: question type signal <t> and question order signal <c>, where <t> ∈ T, <c> ∈ C, T denotes the set of all question types, C denotes the set of order, i.e., {<first>, <second>,\n<third>, ...}. We train a BART summarization model (Lewis et al., 2020) to conduct the eventcentric summary generation task. The input of the BART model is: <t> <c> d, and the output of the BART model is a summary that collects related events for an educational question type, where d denotes the input paragraph.\nObtaining the golden summaries is difficult. However, a QA dataset, like FairytaleQA, provides both questions and their corresponding answers. We can therefore re-write the annotated questions and answers together to obtain question-answer statements, which are used as silver summaries to train our summarization model."
    }, {
      "heading" : "3.3 Educational Question Generation",
      "text" : "With the summary generated in the second stage, generating an educational question is fairly straightforward. Because the summary has already contained all key events for the target educational question type, we can train a question generation model directly on top of it using the annotated questions. We fine-tune another BART model to generate questions, with the type and order control signals added before the input summary to control the generated results. Note that our question generation model does not reply on pre-selected answer spans."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "To demonstrate the effectiveness of our proposed method, we conducted a set of experiments on the FairytaleQA dataset."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "The FairytaleQA dataset (Yao et al., 2021) contains annotations of 278 books, including 232 training books, 23 test books, and 23 validation books. Each book has multiple paragraphs, and for each para-\ngraph of one book, there are several educational question-answer pairs annotated by education experts. In total, there are seven types of questions: •Character: questions that contain the character of the story as the subject and ask for additional information about that character; •Setting: questions that start with “Where/When”; •Feeling: questions that start with “How did/do/does X feel?”; •Action: questions that start with \"What did/do/does X do?\" or \"How did/do/does X\" or questions that contain a focal action and ask for additional information about that action; •Causal relationship: questions that start with “Why” or “What made/make”; •Outcome resolution: questions ask about logic relations between two events, such as “What happened...after...”; •Prediction: questions that start with “What will/would happen...”.\nFor question types character, setting, feeling, they are mainly related to noun chunks, and can be handled well by traditional span-based question generation methods (Yao et al., 2021). For the question type prediction, it usually asks for events that do not appear in storybooks, and requires the ability of imagination, which is not our focus in this paper. We only consider question types action, causal relationship, and outcome resolution in this paper, as these are often high-cognitive-demand questions and related to salient events of an input text. The statistics of the selected data is shown in Table 1 & 2."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "We compared our system with two baselines: 1) the method proposed in Yao et al. (2021) (denoted as QAG), which is the only method that consid-\ners generating educational questions; 2) using the FairytaleQA dataset, we also trained an end-to-end BART model as a baseline.\nQAG. For the question types action, causal relationship, and outcome resolution, generating questions with the QAG model (Yao et al., 2021) contains four steps. 1) generate a set of answers based on semantic roles of verbs; 2) generate questions based on these answers; 3) generate answers based on the questions generated in the second step; 4) rank generated question-answer pairs and choose the top questions. We trained the question generation model in the second step and the answer generation model in the third step using the question types action, causal relationship, and outcome resolution in FairytaleQA. We use the top 10/5/3/2/1 generated questions as baselines, denoted as QAG (top10), QAG (top5), QAG (top3), QAG (top2), and QAG (top1), respectively.\nE2E. Using FairytaleQA with question types action, causal relationship, and outcome resolution, we trained one BART-large model to generate questions based one paragraph end-to-end. During testing, we used a maximal length 100 tokens (roughly 7 questions according to Table 2) and selected the first 2 questions as the output for evaluation. We denote this method as E2E."
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "We adopt both automatic and human evaluation to measure the performance of our method."
    }, {
      "heading" : "4.3.1 Automatic Evaluation",
      "text" : "For automatic evaluation, similar to Yao et al. (2021), we use the Rouge-L score (Lin, 2004), and report the average precision, recall, and F1 values. Meanwhile, we also use BERTScore (Zhang et al., 2020a) to evaluate the semantic similarity of generated questions with the ground-truth questions, and report the average precision, recall, and F1 values. In contrast to Yao et al. (2021), we concatenate all generated questions into one sentence, and compare it with the concatenated ground-truth questions. This is because for each paragraph, we need to evaluate not only the generated quality of each question but also the number of generated questions for each question type."
    }, {
      "heading" : "4.3.2 Human Evaluation",
      "text" : "To evaluate the quality of our generated questions and their educational significance, we further conducted a human evaluation session. After regular group meetings, we concluded the following four dimensions: 1. Question type: whether the generated questions belong to any of the three event types. 2. Validity: whether the generated questions are valid questions according to the original paragraph. 3. Readability: whether the generated questions are coherent and grammatically correct. 4. Children appropriateness: how likely would you like to ask this question when you read the story to a five year’s old child?"
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "The silver ground-truth for the output of summarization and the input of question generation were obtained by transforming questions and their corresponding answers into declarative sentences. We used the rule-based method in Demszky et al. (2018) which inserts answers into the semantic parsed questions and eliminates question words. There are 8 sentences that cannot be parsed successfully. In this case, we wrote the silver statements manually. We also corrected 5 low-quality statements manually.\nThe weight factor for question type distribution learning is set as 0.7 empirically. For question type distribution learning, we used a BERT cased large model. For summary generation, we used a BART cased base model. For question generation, we used a BART cased large model. The batch sizes of all training are set as 1. For the generation process, we only used a greedy decoding method. For\nall methods, we removed duplicated questions and questions that has less than 3 tokens. All experiments were conducted on a Ubuntu server with Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz, 32G Memory, Nvidia GPU 2080Ti, Ubuntu 16.04."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Automatic Evaluation Results",
      "text" : "The results of automatic evaluation on both validation and test datasets are shown in Table 3. For Rough-L, compared to E2E and QAG, our method can achieve the best results except for the recall values. In particular, our method outperforms E2E by about 20 points, and outperforms the best QAG model (top2) by about 10 points on the precision scores. For F1, our method outperforms E2E by about 10 points, and outperforms the best QAG model (top2) by about 5 points. These results show that our method can match the ground-truth questions lexically better than other methods. However, the recall score of our method is not as good as E2E and QAG (top5 & 10). This is because for E2E and QAG (top5 & 10), they generally generate more questions than our method1. For BERTScore, our method achieves the best results on precision, recall, and F1, which shows that the questions generated by our method are closer to ground-truth questions semantically.\nApart from the overall performance, we also investigated the performance of each module of our method. Because the performance values on both the validation and test data are similar, to simplify our experiment, in the following sections, we only conducted experiments on the test data.\nQuestion Type Distribution Learning. On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth\n1On the test data, the mean of the generated questions by our method is 1.9 (std: 0.6), which is closer to the case of ground-truth (mean: 2.2, std: 1.5)\nis 0.0089, which shows that the performance of our question type distribution learning module is relatively satisfactory. We also use the groundtruth question type distribution as an input and calculate the final Rouge-L score with our system. The results are shown in Table 4. Compared to the ground-truth question type distribution, our system still has lower precision and F1 scores. Therefore, having a more accurate question type distribution prediction is beneficial for improving the overall performance.\nEvent-centric Summary Generation To investigate the quality of the generated summaries, we compare the generated results with the silver summary ground-truth. Similar to the evaluation method of generated questions, we concatenated the generated summaries and calculated the RougeL score with the concatenated ground-truth summaries. The results are 15.41 precision, 30.60 recall, and 18.85 F1, which shows that there is still a lot of room to improve the summarization module.\nUpper-bound Results with Silver Summary To see how the upper-bound performance is if we have perfect summaries, we input the silver summaries to our educational question generation model. The Rouge-L scores of generated questions are 92.71 precision, 85.65 recall, 87.67 F1, which shows the potential that once a good summary containing salient events is available, generating an educational question is relatively easy. The core challenge is to obtain good summaries, which we believe will be a valuable next step in future work."
    }, {
      "heading" : "5.2 Human Evaluation Results",
      "text" : "We conducted a human evaluation of our method against the best-performed baseline, i.e., QAG (top2). We first randomly sampled 10 books from the test set. For each book, we randomly sampled 5 paragraphs. We then conducted experiments to evaluate the generated results on question type and question quality, respectively.\nQuestion type. Three human participants annotated the types of all generated questions. The inter-coder reliability score (Krippendoff’s alpha (Krippendorff, 2011)) among three participants is 0.86, indicating a relatively high consistency. The annotated results are shown in Table 5. Overall, our method demonstrates a much smaller K-L distance (0.28) to the ground-truth distribution, compared to the baseline QAG (0.60). We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth. QAG has a biased question type distribution and generates more outcome resolution questions.\nQuestion quality. We invited another five human participants and conducted a human evaluation to further evaluate the quality of the generated questions from our model against the ground-truth and QAG, including validity, readability, and children appropriateness. Among the three dimensions, the children appropriateness is most closely related to the educational purpose; the former two dimensions mainly measure the factual correctness and fluency respectively.\nFor the total 10× 5 paragraphs, each participant is assigned 20 different paragraphs randomly, and each paragraph has annotation results from two participants. For each paragraph, participants need to read the paragraph and its corresponding questions and answers, and then rate the three dimensions on a five-point Likert-scale. The Krippendoff’s alpha scores along the four dimensions are between 0.60 and 0.80 (validity: 0.80, readability: 0.69, children\nappropriateness: 0.60), indicating an acceptable consistency (Gretz et al., 2020).\nWe conducted an independent-samples t-test to compare the performance of each model. Our model is significantly better than QAG on the main evaluation dimension of children appropriateness: the mean score of our model and QAG are 2.56 and 2.22, with corresponding standard derivation 1.31 and 1.20 respectively. This gives a significant score with p-value=0.009, showing that the questions generated by our model can indeed better fit the education scenario. For reference, the ground-truth has a mean score and standard derivation of 3.96 and 1.02, indicating a still large space to improve.\nOn validity and readability, our model is on par with QAG. This is not surprising because both models are based on large pre-trained BART models that are good at generating natural and fluent sentences. For validity, our model (avg: 3.19, std: 1.53) is a bit lower than QAG (avg: 3.27, std: 1.62); for readability, our model (avg: 4.19, std: 1.53) is a bit higher than QAG (avg: 4.12, std: 1.33). A further breakdown in Table 6 shows that QAG wins mainly on action questions, because it directly generates questions conditioned on verbs. For causal relationship and outcome resolution questions, our method generally outperforms QAG."
    }, {
      "heading" : "6 System Analysis",
      "text" : "To further investigate the effectiveness of our method, we conducted a set of ablation studies."
    }, {
      "heading" : "6.1 Question Type Distribution Learning",
      "text" : "To investigate the effects of our question type distribution learning, we conducted a comparison study. In particular, we removed the question type distribution learning module (denoted as w/o tdl), and directly trained the summarization and question generation models. In other words, during training, we concatenate all silver summaries as the output of the summarization model. During testing, we extract the first 2 sentences as the predicted summaries. The results are shown in Table 7. From the\ncomparison, we can see that without knowing question types, the Rouge-L scores drop significantly, which implies the importance of our question type distribution learning module."
    }, {
      "heading" : "6.2 Event-centric Summary Generation",
      "text" : "To investigate the effects of our event-centric summary generation module, we conducted a comparison with different summarization methods. The summarization methods include: 1) Lead3. We select the first three sentences of a section as the summary, and use them as input to the question generation model; 2) Last3. We select the last three sentences of a section as the summary, and use them as input to the question generation model. 3) Random3. We select the random three sentences of a section as the summary, and use them as input to the question generation model. 4) Total. We use each sentence of a section as the summary, and use them as input to the question generation model. 5) TextRank. TextRank is a typical extractive summarization method. We use TextRank to extract a summary, and for each sentence in the summary, we input it to the question generation model.\nFor other summarization methods, they cannot get the question type distribution like our method. For a fair comparison, we also remove the question type distribution learning module of our method, which is the same as the setting in section 6.1. The results are shown in Table 8, from which we can see that extracting sentences from the paragraph is not enough for covering salient events for educational question generation. Our event-centric summary generation method is an effective way for extracting educational events of fairy tales. Using all sentences (total) can have the highest recall score at the expense of accuracy, but the overall F1 score is still relatively low."
    }, {
      "heading" : "6.3 Multi-task Learning of Question Types",
      "text" : "Currently, we use control signals to constrain generating questions of different types, which can be viewed as a multi-task learning framework for multi-type question generation. To investigate whether sharing parameters is a good way for our\ntask, we trained individual summarization and question generation models using different question types. The results in Rouge-L are shown in Table 9. We can find that sharing parameters generally can achieve better performance because of the use of more training data. For only using one type of training data, owing to the error of question type distribution learning, the performance drops a lot, showing the importance of combining question type distribution learning and multi-task learning with different types of training data."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose a novel method for educational question generation for fairy tales, which can potentially be used in early childhood education. Our method contains three modules: question type distribution learning, event-centric summary generation, and educational question generation. Through question type distribution learning, we can decompose the challenges of educational question generation by extracting related events of one question type and generating educational questions with a short event-centric summary, which improves the performance significantly. On both automatic evaluation and human evaluation, we show the potential of our method. In the future, we plan to further investigate the event-centric summary generation module by considering discourse-level information to improve the summarization performance. We are also interested in deploying the system in real scenarios to benefit childcare-related domains."
    } ],
    "references" : [ {
      "title" : "A recurrent BERT-based model for question generation",
      "author" : [ "Ying-Hong Chan", "Yao-Chung Fan." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 154–162, Hong Kong, China. Association for Computational Lin-",
      "citeRegEx" : "Chan and Fan.,? 2019",
      "shortCiteRegEx" : "Chan and Fan.",
      "year" : 2019
    }, {
      "title" : "Contrastive multi-document question generation",
      "author" : [ "Woon Sang Cho", "Yizhe Zhang", "Sudha Rao", "Asli Celikyilmaz", "Chenyan Xiong", "Jianfeng Gao", "Mengdi Wang", "Bill Dolan." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association",
      "citeRegEx" : "Cho et al\\.,? 2021",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2021
    }, {
      "title" : "Transforming question answering datasets into natural language inference datasets",
      "author" : [ "Dorottya Demszky", "Kelvin Guu", "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "Demszky et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Demszky et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Question generation for question answering",
      "author" : [ "Nan Duan", "Duyu Tang", "Peng Chen", "Ming Zhou." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Duan et al\\.,? 2017",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2017
    }, {
      "title" : "Enhancing parent-child relationship through dialogic reading",
      "author" : [ "Fraide A. Ganotice", "Kevin Downing", "Teresa Ka Ming Mak", "Barbara Chan", "Wai Yip Lee." ],
      "venue" : "Educational Studies, 43:51 – 66.",
      "citeRegEx" : "Ganotice et al\\.,? 2017",
      "shortCiteRegEx" : "Ganotice et al\\.",
      "year" : 2017
    }, {
      "title" : "Language matters: Denying the existence of the 30-million-word gap has serious consequences",
      "author" : [ "Roberta Michnick Golinkoff", "Erika Hoff", "Meredith L. Rowe", "Catherine S. Tamis-LeMonda", "Kathy Hirsh-Pasek." ],
      "venue" : "Child development, 90 3:985–992.",
      "citeRegEx" : "Golinkoff et al\\.,? 2019",
      "shortCiteRegEx" : "Golinkoff et al\\.",
      "year" : 2019
    }, {
      "title" : "The workweek is the best time to start a family – a study of GPT-2 based claim generation",
      "author" : [ "Shai Gretz", "Yonatan Bilu", "Edo Cohen-Karlik", "Noam Slonim." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 528–544,",
      "citeRegEx" : "Gretz et al\\.,? 2020",
      "shortCiteRegEx" : "Gretz et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence extraction based single document summarization",
      "author" : [ "J Jagadeesh", "Prasad Pingali", "Vasudeva Varma." ],
      "venue" : "International Institute of Information Technology, Hyderabad, India, 5.",
      "citeRegEx" : "Jagadeesh et al\\.,? 2005",
      "shortCiteRegEx" : "Jagadeesh et al\\.",
      "year" : 2005
    }, {
      "title" : "The NarrativeQA reading comprehension challenge",
      "author" : [ "Tomáš Kočiský", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gábor Melis", "Edward Grefenstette." ],
      "venue" : "Transactions of the",
      "citeRegEx" : "Kočiský et al\\.,? 2018",
      "shortCiteRegEx" : "Kočiský et al\\.",
      "year" : 2018
    }, {
      "title" : "Computing krippendorff’s alpha-reliability",
      "author" : [ "Klaus Krippendorff" ],
      "venue" : null,
      "citeRegEx" : "Krippendorff.,? \\Q2011\\E",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2011
    }, {
      "title" : "Generating question-answer hierarchies",
      "author" : [ "Kalpesh Krishna", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2321–2334, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Krishna and Iyyer.,? 2019",
      "shortCiteRegEx" : "Krishna and Iyyer.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Timeline summarization based on event graph compression via time-aware optimal transport",
      "author" : [ "Manling Li", "Tengfei Ma", "Mo Yu", "Lingfei Wu", "Tian Gao", "Heng Ji", "Kathleen McKeown." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "The automatic creation of literature abstracts",
      "author" : [ "H.P. Luhn." ],
      "venue" : "IBM Journal of Research and Development, 2(2):159–165.",
      "citeRegEx" : "Luhn.,? 1958",
      "shortCiteRegEx" : "Luhn.",
      "year" : 1958
    }, {
      "title" : "Improving unsupervised question answering via summarizationinformed question generation",
      "author" : [ "Chenyang Lyu", "Lifeng Shang", "Yvette Graham", "Jennifer Foster", "Xin Jiang", "Qun Liu." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Lyu et al\\.,? 2021",
      "shortCiteRegEx" : "Lyu et al\\.",
      "year" : 2021
    }, {
      "title" : "TextRank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "author" : [ "Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, page",
      "citeRegEx" : "Nallapati et al\\.,? 2017",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2017
    }, {
      "title" : "Ms marco: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "CoCo@ NIPS. 9",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Text summarization of Turkish texts using latent semantic analysis",
      "author" : [ "Makbule Ozsoy", "Ilyas Cicekli", "Ferda Alpaslan." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 869–876, Beijing, China.",
      "citeRegEx" : "Ozsoy et al\\.,? 2010",
      "shortCiteRegEx" : "Ozsoy et al\\.",
      "year" : 2010
    }, {
      "title" : "Asking it all: Generating contextualized questions for any semantic role",
      "author" : [ "Valentina Pyatkin", "Paul Roit", "Julian Michael", "Yoav Goldberg", "Reut Tsarfaty", "Ido Dagan." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Pyatkin et al\\.,? 2021",
      "shortCiteRegEx" : "Pyatkin et al\\.",
      "year" : 2021
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Shared book reading by parents with young children: Evidencebased practice",
      "author" : [ "Susan Sim", "Donna Berthelsen." ],
      "venue" : "Australasian Journal of Early Childhood, 39(1):50–55.",
      "citeRegEx" : "Sim and Berthelsen.,? 2014",
      "shortCiteRegEx" : "Sim and Berthelsen.",
      "year" : 2014
    }, {
      "title" : "Heterogeneous graph neural networks for extractive document summarization",
      "author" : [ "Danqing Wang", "Pengfei Liu", "Yining Zheng", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph neural networks for natural language processing: A survey",
      "author" : [ "Lingfei Wu", "Yu Chen", "Kai Shen", "Xiaojie Guo", "Hanning Gao", "Shucheng Li", "Jian Pei", "Bo Long" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Discourse-aware neural extractive text summarization",
      "author" : [ "Jiacheng Xu", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5021–5031, Online. Association for Computa-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Same benefits, different communication patterns: Comparing children’s reading with a conversational agent vs",
      "author" : [ "Ying Xu", "Dakuo Wang", "Penelope Collins", "Hyelim Lee", "Mark Warschauer." ],
      "venue" : "a human partner. Computers & Education, 161:104059.",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "It is ai’s turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset",
      "author" : [ "Bingsheng Yao", "Dakuo Wang", "Tongshuang Wu", "Tran Hoang", "Branda Sun", "Toby Jia-Jun Li", "Mo Yu", "Ying Xu" ],
      "venue" : null,
      "citeRegEx" : "Yao et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2021
    }, {
      "title" : "Dialogic reading: A shared picture book reading intervention for preschoolers",
      "author" : [ "Andrea A. Zevenbergen", "Grover J. Whitehurst" ],
      "venue" : null,
      "citeRegEx" : "Zevenbergen and Whitehurst.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zevenbergen and Whitehurst.",
      "year" : 2003
    }, {
      "title" : "2020a. Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Text emotion distribution learning via multi-task convolutional neural network",
      "author" : [ "Yuxiang Zhang", "Jiamei Fu", "Dongyu She", "Ying Zhang", "Senzhang Wang", "Jufeng Yang." ],
      "venue" : "Proceedings of the TwentySeventh International Joint Conference on Artificial",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Machine reading comprehension: The role of contextualized language models and beyond",
      "author" : [ "Zhuosheng Zhang", "Hai Zhao", "Rui Wang." ],
      "venue" : "ArXiv, abs/2005.06249.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Listening to and understanding fairy tales or storybooks are very crucial for children’s early intellectual and literacy development (Sim and Berthelsen, 2014).",
      "startOffset" : 133,
      "endOffset" : 159
    }, {
      "referenceID" : 30,
      "context" : "During the storybook reading process, prompting suitable questions with educational purposes can help children understand the content and inspire their interests (Zevenbergen and Whitehurst, 2003; Ganotice et al., 2017).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 5,
      "context" : "During the storybook reading process, prompting suitable questions with educational purposes can help children understand the content and inspire their interests (Zevenbergen and Whitehurst, 2003; Ganotice et al., 2017).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 6,
      "context" : "However, it is challenging even for humans to ask educationally meaningful questions to engage children in storybook reading, which could be due to adults lacking the skills or time to integrate such interactive opportunities (Golinkoff et al., 2019).",
      "startOffset" : 226,
      "endOffset" : 250
    }, {
      "referenceID" : 28,
      "context" : "Recent research shows that AI-powered conversational agents can play the role of language partners to read fairy tales to children and ask them educational questions (Xu et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 183
    }, {
      "referenceID" : 11,
      "context" : "Most previous work on question generation (QG) focuses on generating questions based on predefined answer spans (Krishna and Iyyer, 2019; Pyatkin et al., 2021; Cho et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 177
    }, {
      "referenceID" : 21,
      "context" : "Most previous work on question generation (QG) focuses on generating questions based on predefined answer spans (Krishna and Iyyer, 2019; Pyatkin et al., 2021; Cho et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "Most previous work on question generation (QG) focuses on generating questions based on predefined answer spans (Krishna and Iyyer, 2019; Pyatkin et al., 2021; Cho et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : "Instead, we rewrite annotated questions, and their corresponding hypothesized answers into question-answer statements (Demszky et al., 2018) as silver training samples.",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : "Question answering based on context has achieved remarkable results (Rajpurkar et al., 2016; Zhang et al., 2020b).",
      "startOffset" : 68,
      "endOffset" : 113
    }, {
      "referenceID" : 33,
      "context" : "Question answering based on context has achieved remarkable results (Rajpurkar et al., 2016; Zhang et al., 2020b).",
      "startOffset" : 68,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "The reverse problem of question answering, namely, question generation (Duan et al., 2017; Chan and Fan, 2019), usually relies on preselecting spans from an input text as answers and a single sentence as the context.",
      "startOffset" : 71,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "The reverse problem of question answering, namely, question generation (Duan et al., 2017; Chan and Fan, 2019), usually relies on preselecting spans from an input text as answers and a single sentence as the context.",
      "startOffset" : 71,
      "endOffset" : 110
    }, {
      "referenceID" : 29,
      "context" : "However, to generate questions across a long paragraph in which the key information may come from multiple different sentences in fairy tales (Yao et al., 2021), these ex-",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 9,
      "context" : "NarrativeQA (Kočiský et al., 2018) is an effort that tries to integrate key information across multiple locations of a paragraph for question answering/generation.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "Similarly, MS MARCO (Nguyen et al., 2016) is a dataset that integrates multiple locations of answers for users’ queries in search engines.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 29,
      "context" : "In FairytaleQA (Yao et al., 2021), experts usually write different types of questions for separate paragraphs.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "Extractive summarization methods, such as TextRank (Mihalcea and Tarau, 2004), feature-based methods (Jagadeesh et al.",
      "startOffset" : 51,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "Extractive summarization methods, such as TextRank (Mihalcea and Tarau, 2004), feature-based methods (Jagadeesh et al., 2005; Luhn, 1958; Nallapati et al., 2017), and topic-based methods (Ozsoy et al.",
      "startOffset" : 101,
      "endOffset" : 161
    }, {
      "referenceID" : 15,
      "context" : "Extractive summarization methods, such as TextRank (Mihalcea and Tarau, 2004), feature-based methods (Jagadeesh et al., 2005; Luhn, 1958; Nallapati et al., 2017), and topic-based methods (Ozsoy et al.",
      "startOffset" : 101,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : "Extractive summarization methods, such as TextRank (Mihalcea and Tarau, 2004), feature-based methods (Jagadeesh et al., 2005; Luhn, 1958; Nallapati et al., 2017), and topic-based methods (Ozsoy et al.",
      "startOffset" : 101,
      "endOffset" : 161
    }, {
      "referenceID" : 20,
      "context" : ", 2017), and topic-based methods (Ozsoy et al., 2010), do not work to generate highcognitive-demand questions on the fairytale scenario because such questions often are based on multiple sentences.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "architectures usually encode an input document token-by-token sequentially (Rush et al., 2015) and",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "In this paper, we develop an event-centric summarization method based on BART (Lewis et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "We fine-tuned a BERT model (Devlin et al., 2019), and adapt the output m dimensional class token hc ∈ Rm to learn the question type distribution.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "model (Lewis et al., 2020) to conduct the eventcentric summary generation task.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : "The FairytaleQA dataset (Yao et al., 2021) contains annotations of 278 books, including 232 training books, 23 test books, and 23 validation books.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : "For question types character, setting, feeling, they are mainly related to noun chunks, and can be handled well by traditional span-based question generation methods (Yao et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 29,
      "context" : "For the question types action, causal relationship, and outcome resolution, generating questions with the QAG model (Yao et al., 2021) con-",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "(2021), we use the Rouge-L score (Lin, 2004), and report the average precision, recall, and F1 values.",
      "startOffset" : 33,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "The inter-coder reliability score (Krippendoff’s alpha (Krippendorff, 2011)) among three participants is 0.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "60), indicating an acceptable consistency (Gretz et al., 2020).",
      "startOffset" : 42,
      "endOffset" : 62
    } ],
    "year" : 0,
    "abstractText" : "Generating educational questions of fairytales or storybooks is vital for improving children’s literacy ability. However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness. In this paper, we propose a novel question generation method that first learns the question type distribution of an input story section, and then summarizes salient events which can be used to generate high-cognitive-demand questions. To train the event-centric summarizer, we finetune a pre-trained transformer-based sequenceto-sequence model using silver samples composed by educational question-answer pairs. On a newly proposed educational questionanswering dataset FairytaleQA, we show good performance of our method on both automatic and human evaluation metrics. Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation.",
    "creator" : null
  }
}
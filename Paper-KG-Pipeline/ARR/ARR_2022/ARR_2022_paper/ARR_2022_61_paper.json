{
  "name" : "ARR_2022_61_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1\nQuality Controlled Paraphrase Generation"
    }, {
      "heading" : "1 Introduction",
      "text" : "Paraphrase generation, namely rewriting a sentence using different words and/or syntax while preserving its meaning (Bhagat and Hovy, 2013), is an important technique in natural language processing, that has been widely used in various downstream tasks including question answering (Fader et al., 2014a; McCann et al., 2018), summarization (Rush et al., 2015), data augmentation (Yu et al., 2018) and adversarial learning (Iyyer et al., 2018). However, not all paraphrases are equally useful. For most real-world applications, paraphrases which are too similar to the original sentence are of limited value, while those with high linguistic diversity, i.e. with large syntactic/lexical differences between\nthe paraphrase and the original sentence, are more beneficial to the robustness and accuracy of automatic text evaluation and classification, and can avoid the blandness caused by repetitive patterns (Qian et al., 2019). The quality of paraphrases is often evaluated using three dimensions, where high quality paraphrases are those with high semantic similarity as well as high lexical and/or syntactic diversity (McCarthy et al., 2009).\nGenerating high quality paraphrases can be challenging (for both humans and automatic models) since it is increasingly difficult to preserve meaning with increasing linguistic diversity. Indeed, when examining the quality of paraphrases among paraphrase generation datasets, one can find a wide range of paraphrase qualities, where the area of high quality is often very sparse (see Figure 1). This in turn results in scarcity of supervised data for high-quality paraphrase generation.\nA recent approach aiming to produce high qual-\n2 ity paraphrases is controlled paraphrase generation, which exposes control mechanisms that can be manipulated to produce diversity. While the controlled generation approaches have yielded impressive results, they require providing the model with very specific information regarding the target sentence, such as its parse tree (Iyyer et al., 2018) or the list of keywords it needs to contain (Zeng et al., 2019). However, for most downstream applications, the important property of the paraphrase is its overall quality, rather than its specific syntactic or lexical form. The over-specificity of existing controlbased methods not only complicates their usage and limits their scalability, but also hinders their coverage. Thus, it would be desirable to develop a paraphrase generation model, which uses a simple mechanism for directly controlling paraphrase quality, while avoiding unnecessary complications associated with fine-grained controls. In this paper we propose a Quality-Guided Controlled Paraphrase Generation (QCPG) neural model, that given an input sentence and quality constraints, represented by a three dimensional vector of semantic similarity, and syntactic and lexical distances, produces a target sentence that conforms to the quality constraints. Our constraints are much simpler than previously suggested ones, such as parse trees or keyword lists, and leave the model the freedom to choose how to attain the desired quality levels. Enabling the direct control of the three quality dimensions, allows flexibility with respect to the specific requirements of the task at hand, and opens a range of generation possibilities: paraphrases of various flavors (e.g. syntactically vs. lexically diverse), quasi-paraphrases (with lower semantic similarity), and even non-paraphrases which may be useful for downstream tasks (e.g. hard negative examples of sentences that are linguistically similar but have different meanings (Guo et al., 2018; Reimers and Gurevych, 2020)). Our results show that the QCPG model indeed enables controlling paraphrase quality along the three quality dimensions. Furthermore, even though the training data is of mixed quality, and exhibits scarcity in the high quality area (see Figure 1), our model is able to learn high quality paraphrasing behavior, i.e. it increases the linguistic diversity of the generated paraphrases without decreasing the semantic similarity compared to the uncontrolled baseline."
    }, {
      "heading" : "2 Method",
      "text" : "In this section we provide a general description of our approach. We first explain how the different quality dimensions are measured. We then describe the controlled paraphrase generation model, QCPG, and finally we suggest a method that given the task requirements, detects the input control values which maximize the quality of the generated paraphrases. Figure 2 summarizes our proposed solution for generating controlled paraphrases, which is detailed in the rest of the section."
    }, {
      "heading" : "2.1 Quantifying Paraphrase Quality",
      "text" : "The most common dimensions for measuring paraphrase quality are the semantic, syntactic and lexical dimensions1 (McCarthy et al., 2009). Thus, given a sentence s and a paraphrase s′, we define the paraphrase quality as a three dimensional vector q(s, s′) = (qsem(s, s′), qsyn(s, s′), qlex(s, s′)), where qsem is a measure of semantic similarity, and qsyn and qlex are measures of syntactic and lexical variation, respectively. For the syntactic score, inspired by Iyyer et al. (2018) we choose qsyn(s, s′) to be the normalized tree edit distance (Zhang and Shasha, 1989) between the third level constituency parse-trees of s and s′, after removing the tokens - to increase the decoupling from the lexical distance metric. We define the lexical score qlex(s, s′) to be the normalized character-level minimal edit distance between the bag of words. This measure is independent of word order, and hence increases the decoupling from syntactic measures. Additionally, calculating the token distances on the character level enables to capture tokens that share the same stem/lemma. Character-level distance is also more robust to typos that may be found in noisy data. As for the semantic score, several strong metrics have been recently proposed for measuring semantic similarity between sentences. In order to select qsem(s, s′), we studied the agreement between the candidate metrics and human judgments, using only development data, and found Bleurt (Sellam et al., 2020) to have the highest correlation with human judgments (see Appendix A). Thus, we define qsem(s, s ′) to be the Bleurt score, normalized using the sigmoid function to ensure a uniform range of values, [0, 1], for all three quality dimensions. For 1Several previous works used also a fluency evaluation metric (Siddique et al., 2020). However, since our focus is on the supervised setting, we rely on the gold paraphrases as fluency guidance for the model.\n3\nease of presentation all metrics are presented on a 0− 100 scale."
    }, {
      "heading" : "2.2 The QCPG Model",
      "text" : "The main component of our solution is a quality controlled paraphrase generation model (QCPG), which is an encoder-decoder model trained on the task of controlled paraphrase generation. Given an input sentence s and a control vector c = (csem, csyn, clex), the goal of QCPG is to generate an output paraphrase QCPG(s, c) that conforms to c. We train QCPG using the training set pairs (s, t), by setting c to be q(s, t), and maximizing P (t|s, c = q(s, t)) over the training set via the autoregressive cross entropy loss."
    }, {
      "heading" : "2.3 Control Values Selection",
      "text" : "A major challenge in the research of controlled paraphrase generation, is selecting appropriate input control values that can be achieved by the model (Goyal and Durrett, 2020). Clearly, given a sentence, not all paraphrase qualities are achievable. Some sentences are more amenable to paraphrasing than others. For example, named entities and numbers are much harder to be replaced while keeping sentence meaning, and hence, the potential lexical diversity of paraphrases involving such terms is relatively limited. Forcing QCPG to conform to quality control values that are too high with respect to the input sentence, may lead to suboptimal quality of the resultant paraphrases. Thus, for a more effective use of QCPG, the control values should be determined with respect to the input sentence. Below we describe the second part of our solution, namely a method that given a sentence, predicts the input control values, c(s), that optimize the expected quality of the paraphrases generated by QCPG. For simplicity we assume that the quality distribution p(q|s) of all paraphrases of sentence s, is approximately normally distributed around a sentence dependent mean q0(s), and that the variance is approximately sentence-independent. We further assume that given an input sentence s, the difficulty to generate a paraphrase of a given quality, q, is dominated by p(q|s) rather than by the quality vector q itself. Following our assumptions, the level of difficulty can be expressed by the offset, o = (osem, osyn, olex) of q from q0(s). Thus, the input control, c(s), for QCPG, is the sum of q0(s) and an offset o. Our aim is to analyze the model results for varying levels of difficulty, namely under different offsets, o, from q0(s). The Quality Predictor (QP): Since q0(s) is unknown, we introduce QP, a regressor whose output, termed the reference of s, r(s) = (rsem(s), rsyn(s), rlex(s)), approximates q0(s). During training, QP aims to predict q(s, t) given s, where (s, t) are the input-output pairs of the training data. To summarize, we define sentence-aware quality control by decomposing the QCPG input control, c, into a sum of a sentence dependent reference point, r(s), and a sentence independent offset, o."
    }, {
      "heading" : "3 Data and Implementation Details",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "To test the ability of our model to learn high quality behavior from mixed quality data we use weakly annotated datasets. These datasets are large but noisy, and contain only a relatively small amount of high quality paraphrases. MSCOCO: This dataset consists of 123K images, where each image contains at most five\n4 human-labeled captions (Lin et al., 2014). Similar to previous works we consider different captions of the same image as paraphrases. WikiAnswers (WikiAns for short): The WikiAnswers corpus contains clusters of questions tagged by wiki-answers.com users as similar. There are 30, 370, 994 clusters with 25 question in each on average. In total, the corpus contains over 70 million question pairs (Fader et al., 2014b). ParaBank2.0: A dataset containing clusters of sentential paraphrases, produced from a bilingual corpus using negative constraints, inference sampling, and clustering (Hu et al., 2019). The dataset is composed of avarage of 5 paraphrases in every cluster and close to 100 million pairs in total. To get comparable results across all datasets, we randomly sub-sampled ParaBank2.0 and WikiAns to the same size as MSCOCO, and split them to train, dev and test sets, of sizes 900K, 14K and 14K respectively. We carefully made sure that there are no pairs from the same cluster in different splits of the data. The full data splits will be published with our code."
    }, {
      "heading" : "3.2 Implementation Details",
      "text" : "All models are trained with batch size of 32 on 2 NVIDIA A100 GPUs for 6 epochs. Full details as well as train and dev results can be found in Appendix B. QCPG: We use the pre-trained T5-base (Raffel et al., 2019) as the encoder-decoder model. The control input vector to QCPG is quantized at every dimension into 20 equally spaced values ranging from 0 to 100. Each value is assigned to a special saved-token. The three tokens corresponding to the quantized values of the control vector c, are concatenated to the head of the input sentence, and together used as input to the model. r(s) and o are also quantized in a similar way. QP: An Electra base model (Clark et al., 2020) finetuned with MSE loss to predict the typical quality values (see Section 2.3). Baseline Model (BL): A T5-base model finetuned on the training data. For all the models, we adopt the experimental setup used in (Devlin et al., 2019), i.e. we train the model with several learning rates and choose the one that achieves the highest dev set performance (see appendix B)."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Controlling the Quality Dimensions",
      "text" : "The aim of the following analysis is to study the level of control achieved by QCPG. To this end, we measure the model response to changes in the input offsets. We compute the expected difference in paraphrase quality, as a result of applying an input offset o compared to zero offset as a reference. More formally, we define the 3-dimensional responsiveness vector of QCPG at an offset o, R(o) as Q(o) −Q((0, 0, 0)), where Q(o) is the expected quality of the paraphrases generated by QCPG at an offset o. We estimate Q(o) by averaging q(QCPG(s, r(s) + o)) over the input sentences s of the dev set, and denote this estimate by Q̃(o) = (Q̃sem(o), Q̃syn(o), Q̃lex(o)), and the corresponding estimate of R(o) by R̃(o). Specifically, in the following analysis we are interested in studying the model response to each of the dimensions separately, i.e. how changing the input offset along a given quality dimension dim – the controlled dimension – while keeping the two other dimensions constant, affects the responsiveness in each of the three dimensions. A good control mechanism would imply that increasing the input offset in one dimension will result in a monotonically increasing responsiveness in that dimension, with relatively small responsiveness in the other two dimensions. Figure 3 shows, for each of the three datasets, the responsiveness in the three quality dimensions, when changing the input offset along each of the three dimensions, while fixing the input offsets in the other two dimensions at 0. Examining the actual values of quality in the paraphrases of the dev sets, reveals that the standard deviation is different in each dimension. Hence, for clarity of presentation, we present the input offset values and the responsiveness in units of standard deviation as measured in the respective dimension and dev set. For the range of offsets displayed in Figure 3, the responsiveness in the controlled dimension increases monotonically with the input offsets across all datasets and dimensions. As expected, the responsiveness in the uncontrolled dimensions does not zeros due to the inherent coupling between the dimensions. For example, many changes that increase syntactic diversity, also increase lexical diversity (e.g. a move from passive to active voice). Still, our control mechanism is able to increase the responsiveness in the controlled dimension with\n5 relative low responsiveness in the uncontrolled dimensions. Specifically, focusing on the relation between semantic similarity and expression diversity, the figure shows that there is a minor decrease in semantic similarity in response to an increase in lexical and syntactic diversity. In the next section, we will show that this does not prevent our model from generating paraphrases that are not only more lexically and syntactically diverse, but also more semantically similar to the source sentences, compared to the paraphrases generated by the uncontrolled baseline. Figure 3 focused on small to moderate input offsets, i.e. offsets up to 2 stds from the reference point. However, as we speculated before, with increasing offsets, i.e. the more the requested control value deviates from the typical value, it becomes increasingly difficult to generate a paraphrase that conforms to the requested control value. Figure 4 depicts the responsiveness in the syntactic and lexical dimensions for a larger range of offset values. For the semantic dimension, the typical values are too high to allow large positive offsets, which for most sentences result in exceeding the upper limit of the semantic score. Indeed, as can be seen in Figure 4, when moving to high offset values, the responsiveness in the syntactic and lexical dimensions starts to decrease. This behavior is in line with our aforementioned hypothesis, and reflects the detrimental effect of feeding QCPG with input control values that are too far from the typical paraphrase qualities of the input sentence. The nonmonotonic behavior of the responsiveness implies that the input offsets should be selected carefully in order to optimize the quality of resultant paraphrases. In Section 4.2 we suggest a method for identifying these optimal offsets."
    }, {
      "heading" : "4.2 Selecting Optimal Input Control Values",
      "text" : "In this section, we suggest a method that given task requirements, selects the input offsets that are expected to yield the desired quality of paraphrases. The idea is to compute the estimated expected quality, Q̃(o), for each input offset o, using the dev set as described in Section 4.1, and then search the 3D grid of input offsets to find the point for which Q̃(o) is best suited for the user’s requirements. We envision this analysis as a preliminary step in which the user chooses the input control parameters that best achieve his desired paraphrasing operation point, and then uses the chosen values at inference – which is why we use the dev set. We study the behavior of Q̃(o) as a function of the 3D grid of offset points in the relevant range, i.e every o where osem, osyn and olex in 0, 5, 10...50. Figure 5 depicts Q̃(o) for WikiAns, on a slice of the full offset grid. The results for the full grid on all datasets are shown in Figure 6. The righthand-side map depicts the estimated linguistic diversity (the average of Q̃syn(o) and Q̃lex(o)) and the left-hand-side depicts the semantic similarity, Q̃sem(o)). The maps are presented for osem = 50, and for different values of osyn and olex. As expected, the two measures are anti-correlated, where areas with increased semantic similarity are characterized by decreased linguistic diversity. The QCPG results are compared to two reference points, which are invariant to o and are marked on the colorbars with black squares: ’Dataset’ is the semantic-similarity/linguistic-diversity average value over the corresponding dev set paraphrases, and ’Baseline’ is the average semanticsimilarity/linguistic-diversity of the uncontrolled baseline over the corresponding dev set. Notice that the average diversity level achieved by the uncontrolled baseline is lower than that of the dev set mean, reflecting the difficulty of this model to generate diverse paraphrases. QCPG on the other hand, with suitable input offset values, is able to generate paraphrases which are on average higher than the baseline both in their linguistic diversity and in their semantic similarity, and in fact even higher in many cases than the values of the ground truth paraphrases in the dev-set. In general, the estimates of the expected quality achieved by QCPG at different input offsets, enable a user to generate paraphrases at different operation points, by manipulating the input offset control o to meet her desired quality values. Consider for example a typical use case, of aiming to maximize linguistic diversity under a constraint on semantic similarity. An example of such a case is an operation point, denoted by QCPG∗, which aims to exemplify the advantage of QCPG over the baseline, by maximizing linguistic diversity under the constraint that the semantic similarity is at least 5 points higher than the baseline. The input offset values to obtain this operation point depend on the dataset, and can be found using heatmaps such as in Figure 5. For WikiAns the input offset for the QCPG∗operation point values are (50, 35, 5) (entry marked by the black square).\n6\n7"
    }, {
      "heading" : "4.3 Quality Evaluation on the Test Set",
      "text" : "In the previous section we saw, using estimates based on the dev sets, that there are many operation points which generate paraphrases with higher quality than those achieved by the uncontrolled baseline. We now turn to evaluate one such operation point, namely QCPG∗, using the source sentences of the test sets which were not used in the selection of the input offset values.\nAutomatic Evaluation We use four quality measures to evaluate different aspects of generated paraphrases. The three quality measures used in the control of QCPG (Section 2.1) and i-BLEU (Sun and Zhou, 2012) as adapted in Li et al. (2019); Liu et al. (2019), which aims to measure the linguistic diversity in the generated paraphrases by penalizing copying from input sentences. As can be seen in Table 1, QCPG∗outperforms the baseline in all metrics across all datasets, as predicted using the dev-set heatmaps. A clear advantage is obtained even for i-BLEU, which was not part of the metrics used as input controls. Importantly, the quality of the paraphrases generated by our model is comparable to or at times better than the quality of the paraphrases in the ground truth of the datasets. This is an important step towards the goal of obtaining paraphrases in the very sparse area of high quality (recall the top right corner of Figure 1).\nHuman Evaluation While linguistic diversity can be automatically measured by reliable metrics such as i-BLEU, measuring semantic similarity is\nmore challenging. We therefore rely on automatic metrics for evaluating the lexical and syntactic diversity, but use human annotation for validating the semantic evaluation. To this end, we selected a sample of 50 source sentences from each test set, and generated one paraphrase using the uncontrolled baseline and one using QCPG∗. The workers were shown the source sentence, along with the two generated paraphrases (randomly ordered), and were asked which of the two better preserves the semantic meaning of the source sentence (ties are also allowed). In total, 150 triplets were evaluated by 5 judges. Table 2 demonstrates an advantage for QCPG∗in all datasets, with a large margin in MSCOCO and WikiAns. Thus, the human evaluation validates the results of the automatic semantic similarity measure. We also verified, that the results of this sample in terms of linguistic diversity are very similar to those shown in Table 1.\nFor examples of paraphrases generated by QCPG∗see Table 8 in the Appendix."
    }, {
      "heading" : "5 Related Work",
      "text" : "Many recent works on paraphrase generation have been focused on attempting to achieve high-quality paraphrases. These works can be divided into supervised and unsupervised approaches.\nSupervised Approaches To achieve diversity, some works focused on diverse decoding using heuristics such as Hamming distance or distinct n-grams to preserve diverse options during beam search (Vijayakumar et al., 2018). Other works\n8\ngenerate multiple outputs by perturbing latent representations (Gupta et al., 2018; Park et al., 2019). or by using distinct generators (Qian et al., 2019). These methods achieve some diversity, but do not control generation in an interpretable manner. The works that are most similar to ours strive to gain diversity using controlled-paraphrase generation, by exposing control mechanisms that are manipulated to produce either lexically (Zeng et al., 2019; Thompson and Post, 2020) or syntactically (Chen et al., 2019; Goyal and Durrett, 2020) diverse paraphrases. One approach is to use an exemplar sentence for guiding the syntax of the generated paraphrase (Chen et al., 2019; Bao et al., 2019). An alternative is to directly employ constituency tree as the syntax guidance (Iyyer et al., 2018; Li and Choi, 2020). Goyal and Durrett (2020) promote syntactic diversity by conditioning over possible syntactic rearrangements of the input. Zeng et al. (2019) use keywords as lexical guidance for the generation process. Here we introduce a simple model for jointly controlling the lexical, syntactic and semantic aspects of the generated paraphrases. Unsupervised Approaches Niu et al. (2020) rely on neural models to generate high quality paraphrases, using a decoding method that enforces diversity by preventing repetitive copying of the input tokens. Liu et al. (2020) optimize a quality oriented objective by casting paraphrase generation as an optimization problem, and searching the sentence space to find the optimal point. Garg et al.\n(2021) and Siddique et al. (2020) use reinforcement learning with quality-oriented reward combining textual entailment, semantic similarity, expression diversity and fluency. In this work, we employ similar metrics for guiding the generation of paraphrases within the supervised framework."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, we propose a novel controlled paraphrase generation model, that leverages measures of paraphrase quality for encouraging the generation of high quality paraphrases. We demonstrate the high level of control achieved by the model, and suggest a method for coping with the challenging problem of finding suitable control values. Aside from offering a simple and effective way for controlling models’ output quality, the quality control paradigm enables a holistic view of the data, the training process and the final model analysis. Namely: (I) Examination of the training data through the lens of data quality enables to characterize the data at hand, its strengths and limitations. (II) A quality-aware training process can be viewed as multi-task learning, where each quality level is a separate task with its own accurate supervision, as opposed to the standard quality-agnostic approach, where low quality data is in fact used as a poor supervision for a model which aims at generating higher quality output. (III) Analyzing the model behavior under different quality controls, allows finer understanding of the different model behaviors and the trade-offs between their output qualities. Better understanding the expected output quality of neural NLG models, for different input quality controls, can increase the trust in their output. Finally, our model analysis consistently shows that although the models generally follow the quality requirements, there is still room for improvement. A possible direction for future research is exploring methods, such as reinforcement learning, for further improving the ability of the model to satisfy the quality requirements.\n9"
    }, {
      "heading" : "A Selecting the semantic similarity measure",
      "text" : "Recently, several strong metrics have been proposed for measuring semantic similarity between sentences. In order to select the semantic similarity metric for QCPG, we performed a small experiment over the three dev sets, with the aim of measuring the agreement of the candidate metrics with human judgments. To this end, we leveraged two properties that characterize weakly labeled datasets, the underlying clusters of sentences, and the high variability of semantic similarity. Given a datset, we randomly selected 100 clusters, and picked three sentences at random from each cluster. For each triplet of sentences t = (t1, t2, t3) we asked 5 human annotators to choose which of the two sentences, t2, t3 better preserves the semantic meaning of t1. We then measured Spearman correlation between the human judgments and the judgments of each of the candidate metrics. Based on the results, we selected Bleurt due to its highest correlation with human judgments over the three datasets. We normalize Bleurt score using the sigmoid function to ensure a uniform range of values, [0, 1], for the three quality dimensions."
    }, {
      "heading" : "B Models Details and Training Results",
      "text" : "The learning rates for the QCPG and the Baseline models were selected in the following way. For a given dataset, we finetuned the models with 4 learning rates (1e-3, 1e-4, 5e-3, 5e-4) (The training results of the baseline presented in Table 3 and the results of QCPG presented in Table 6.). For the baseline we selected the one which yielded the best BLEU score on the corresponding dev set The best learning rate for every dataset was chosen based on the Dev set BLEU score. For the QCPG we chose the model that best conforms to the control input as measured by the MSE between the input control vector and the output quality vector (see Table 7). The QP model is an Electra-Base model finetuned with 4 different learning rates (1.5e-4, 1e-4, 3e-5, 5e-5). We choose the learning rate the yields the minimal MSE on the dev set (For full results see Table 5)\nB.1 Full Heatmaps The full heatmaps can be found in Figure 6.\n12\n13\n14"
    } ],
    "references" : [ {
      "title" : "Generating sentences from disentangled syntactic and semantic spaces",
      "author" : [ "Yu Bao", "Hao Zhou", "Shujian Huang", "Lei Li", "Lili Mou", "Olga Vechtomova", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "arXiv preprint arXiv:1907.05789.",
      "citeRegEx" : "Bao et al\\.,? 2019",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2019
    }, {
      "title" : "What is a paraphrase? Computational Linguistics, 39(3):463–472",
      "author" : [ "Rahul Bhagat", "Eduard Hovy" ],
      "venue" : null,
      "citeRegEx" : "Bhagat and Hovy.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bhagat and Hovy.",
      "year" : 2013
    }, {
      "title" : "Controllable paraphrase generation with a syntactic exemplar",
      "author" : [ "Mingda Chen", "Qingming Tang", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "arXiv preprint arXiv:1906.00565.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Electra: Pre-training text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V Le", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:2003.10555.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Open question answering over curated and extracted knowledge bases",
      "author" : [ "Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni." ],
      "venue" : "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, page",
      "citeRegEx" : "Fader et al\\.,? 2014a",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2014
    }, {
      "title" : "Open Question Answering Over Curated and Extracted Knowledge Bases",
      "author" : [ "Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni." ],
      "venue" : "KDD.",
      "citeRegEx" : "Fader et al\\.,? 2014b",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised contextual paraphrase generation using lexical control and reinforcement learning",
      "author" : [ "Sonal Garg", "Sumanth Prabhu", "Hemant Misra", "G. Srinivasaraghavan" ],
      "venue" : null,
      "citeRegEx" : "Garg et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural syntactic preordering for controlled paraphrase generation",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "arXiv preprint arXiv:2005.02013.",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "Effective parallel corpus mining using bilingual sentence embeddings",
      "author" : [ "Mandy Guo", "Qinlan Shen", "Yinfei Yang", "Heming Ge", "Daniel Cer", "Gustavo Hernandez Abrego", "Keith Stevens", "Noah Constant", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil" ],
      "venue" : null,
      "citeRegEx" : "Guo et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "A deep generative framework for paraphrase generation",
      "author" : [ "Ankush Gupta", "Arvind Agarwal", "Prawaan Singh", "Piyush Rai." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Gupta et al\\.,? 2018",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2018
    }, {
      "title" : "Largescale, diverse, paraphrastic bitexts via sampling and clustering",
      "author" : [ "J. Edward Hu", "Abhinav Singh", "Nils Holzenberger", "Matt Post", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering",
      "author" : [ "Changmao Li", "Jinho D. Choi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5709–5714, On-",
      "citeRegEx" : "Li and Choi.,? 2020",
      "shortCiteRegEx" : "Li and Choi.",
      "year" : 2020
    }, {
      "title" : "Decomposable neural paraphrase generation",
      "author" : [ "Zichao Li", "Xin Jiang", "Lifeng Shang", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:1906.09741.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "M. Maire", "Serge J. Belongie", "James Hays", "P. Perona", "D. Ramanan", "Piotr Dollár", "C.L. Zitnick." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised paraphrasing by simulated annealing",
      "author" : [ "Xianggen Liu", "Lili Mou", "Fandong Meng", "Hao Zhou", "Jie Zhou", "Sen Song." ],
      "venue" : "arXiv preprint arXiv:1909.03588.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised paraphrasing by simulated annealing",
      "author" : [ "Xianggen Liu", "Lili Mou", "Fandong Meng", "Hao Zhou", "Jie Zhou", "Sen Song." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312, Online. As-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "The natural language decathlon: Multitask learning as question answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "The components of paraphrase evaluations",
      "author" : [ "Philip M. McCarthy", "Rebekah H. Guess", "D. McNamara." ],
      "venue" : "Behavior Research Methods, 41:682–690.",
      "citeRegEx" : "McCarthy et al\\.,? 2009",
      "shortCiteRegEx" : "McCarthy et al\\.",
      "year" : 2009
    }, {
      "title" : "Unsupervised paraphrase generation via dynamic blocking",
      "author" : [ "Tong Niu", "Semih Yavuz", "Yingbo Zhou", "Huan Wang", "Nitish Shirish Keskar", "Caiming Xiong" ],
      "venue" : null,
      "citeRegEx" : "Niu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "Paraphrase diversification using counterfactual debiasing",
      "author" : [ "Sunghyun Park", "Seung-won Hwang", "Fuxiang Chen", "Jaegul Choo", "Jung-Woo Ha", "Sunghun Kim", "Jinyeong Yim." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring diverse expressions for paraphrase generation",
      "author" : [ "Lihua Qian", "Lin Qiu", "Weinan Zhang", "Xin Jiang", "Yong Yu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Qian et al\\.,? 2019",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Raffel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleurt: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur P Parikh." ],
      "venue" : "arXiv preprint arXiv:2004.04696.",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised paraphrasing via deep reinforcement learning",
      "author" : [ "AB Siddique", "Samet Oymak", "Vagelis Hristidis." ],
      "venue" : "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1800–1809.",
      "citeRegEx" : "Siddique et al\\.,? 2020",
      "shortCiteRegEx" : "Siddique et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint learning of a dual smt system for paraphrase generation",
      "author" : [ "Hong Sun", "Ming Zhou." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 38–42.",
      "citeRegEx" : "Sun and Zhou.,? 2012",
      "shortCiteRegEx" : "Sun and Zhou.",
      "year" : 2012
    }, {
      "title" : "Paraphrase generation as zero-shot multilingual translation: Disentangling semantic similarity from lexical and syntactic diversity",
      "author" : [ "Brian Thompson", "Matt Post." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 561–570, Online. As-",
      "citeRegEx" : "Thompson and Post.,? 2020",
      "shortCiteRegEx" : "Thompson and Post.",
      "year" : 2020
    }, {
      "title" : "Diverse beam search for improved description of complex scenes",
      "author" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasaath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial In-",
      "citeRegEx" : "Vijayakumar et al\\.,? 2018",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2018
    }, {
      "title" : "Qanet: Combining local convolution with global self-attention for reading comprehension",
      "author" : [ "Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "User-oriented paraphrase generation with keywords controlled network",
      "author" : [ "Daojian Zeng", "Haoran Zhang", "Lingyun Xiang", "Jin Wang", "Guoliang Ji." ],
      "venue" : "IEEE Access, 7:80542–80551.",
      "citeRegEx" : "Zeng et al\\.,? 2019",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple fast algorithms for the editing distance between trees and related problems",
      "author" : [ "Kaizhong Zhang", "Dennis Shasha." ],
      "venue" : "SIAM journal on computing, 18(6):1245–1262.",
      "citeRegEx" : "Zhang and Shasha.,? 1989",
      "shortCiteRegEx" : "Zhang and Shasha.",
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "ing its meaning (Bhagat and Hovy, 2013), is an important technique in natural language processing, that has been widely used in various downstream tasks including question answering (Fader et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "ing its meaning (Bhagat and Hovy, 2013), is an important technique in natural language processing, that has been widely used in various downstream tasks including question answering (Fader et al., 2014a; McCann et al., 2018), summarization (Rush et al.",
      "startOffset" : 182,
      "endOffset" : 224
    }, {
      "referenceID" : 18,
      "context" : "ing its meaning (Bhagat and Hovy, 2013), is an important technique in natural language processing, that has been widely used in various downstream tasks including question answering (Fader et al., 2014a; McCann et al., 2018), summarization (Rush et al.",
      "startOffset" : 182,
      "endOffset" : 224
    }, {
      "referenceID" : 25,
      "context" : ", 2018), summarization (Rush et al., 2015), data augmentation (Yu et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : ", 2015), data augmentation (Yu et al., 2018) and adversarial learning (Iyyer et al.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "The quality of paraphrases is often evaluated using three dimensions, where high quality paraphrases are those with high semantic similarity as well as high lexical and/or syntactic diversity (McCarthy et al., 2009).",
      "startOffset" : 192,
      "endOffset" : 215
    }, {
      "referenceID" : 12,
      "context" : "While the controlled generation approaches have yielded impressive results, they require providing the model with very specific information regarding the target sentence, such as its parse tree (Iyyer et al., 2018) or the list of keywords it needs to contain (Zeng et al.",
      "startOffset" : 194,
      "endOffset" : 214
    }, {
      "referenceID" : 32,
      "context" : ", 2018) or the list of keywords it needs to contain (Zeng et al., 2019).",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "tive examples of sentences that are linguistically similar but have different meanings (Guo et al., 2018; Reimers and Gurevych, 2020)).",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "tive examples of sentences that are linguistically similar but have different meanings (Guo et al., 2018; Reimers and Gurevych, 2020)).",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "phrase quality are the semantic, syntactic and lexical dimensions1 (McCarthy et al., 2009).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 26,
      "context" : "In order to select qsem(s, s′), we studied the agreement between the candidate metrics and human judgments, using only development data, and found Bleurt (Sellam et al., 2020) to have the highest correlation with human judgments (see Appendix A).",
      "startOffset" : 154,
      "endOffset" : 175
    }, {
      "referenceID" : 27,
      "context" : "Several previous works used also a fluency evaluation metric (Siddique et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "A major challenge in the research of controlled paraphrase generation, is selecting appropriate input control values that can be achieved by the model (Goyal and Durrett, 2020).",
      "startOffset" : 151,
      "endOffset" : 176
    }, {
      "referenceID" : 6,
      "context" : "In total, the corpus contains over 70 million question pairs (Fader et al., 2014b).",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "0: A dataset containing clusters of sentential paraphrases, produced from a bilingual corpus using negative constraints, inference sampling, and clustering (Hu et al., 2019).",
      "startOffset" : 156,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "QCPG: We use the pre-trained T5-base (Raffel et al., 2019) as the encoder-decoder model.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "QP: An Electra base model (Clark et al., 2020) finetuned with MSE loss to predict the typical quality values (see Section 2.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "For all the models, we adopt the experimental setup used in (Devlin et al., 2019), i.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : "1) and i-BLEU (Sun and Zhou, 2012) as adapted in Li et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 30,
      "context" : "Supervised Approaches To achieve diversity, some works focused on diverse decoding using heuristics such as Hamming distance or distinct n-grams to preserve diverse options during beam search (Vijayakumar et al., 2018).",
      "startOffset" : 192,
      "endOffset" : 218
    }, {
      "referenceID" : 32,
      "context" : "The works that are most similar to ours strive to gain diversity using controlled-paraphrase generation, by exposing control mechanisms that are manipulated to produce either lexically (Zeng et al., 2019; Thompson and Post, 2020) or syntactically",
      "startOffset" : 185,
      "endOffset" : 229
    }, {
      "referenceID" : 29,
      "context" : "The works that are most similar to ours strive to gain diversity using controlled-paraphrase generation, by exposing control mechanisms that are manipulated to produce either lexically (Zeng et al., 2019; Thompson and Post, 2020) or syntactically",
      "startOffset" : 185,
      "endOffset" : 229
    }, {
      "referenceID" : 2,
      "context" : "One approach is to use an exemplar sentence for guiding the syntax of the generated paraphrase (Chen et al., 2019; Bao et al., 2019).",
      "startOffset" : 95,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "One approach is to use an exemplar sentence for guiding the syntax of the generated paraphrase (Chen et al., 2019; Bao et al., 2019).",
      "startOffset" : 95,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "An alternative is to directly employ constituency tree as the syntax guidance (Iyyer et al., 2018; Li and Choi, 2020).",
      "startOffset" : 78,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "An alternative is to directly employ constituency tree as the syntax guidance (Iyyer et al., 2018; Li and Choi, 2020).",
      "startOffset" : 78,
      "endOffset" : 117
    } ],
    "year" : 0,
    "abstractText" : "Paraphrase generation has been widely used in various downstream tasks. Most tasks benefit mainly from high quality paraphrases, namely those that are semantically similar to, yet linguistically diverse from, the original sentence. Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity increases. Recent works achieve nice results by controlling specific aspects of the paraphrase, such as its syntactic tree. However, they do not allow to directly control the quality of the generated paraphrase, and suffer from low flexibility and scalability. Here we propose QCPG, a quality-guided controlled paraphrase generation model, that allows directly controlling the quality dimensions. Furthermore, we suggest a method that given a sentence, identifies points in the quality control space that are expected to yield optimal generated paraphrases. We show that our method is able to generate paraphrases which maintain the original meaning while achieving higher diversity than baseline. We will publish the models, the code, and the data.",
    "creator" : null
  }
}
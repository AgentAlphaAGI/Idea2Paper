{
  "name" : "ARR_2022_27_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ELLE: Efficient Lifelong Pre-training for Emerging Data",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models (PLM) have broken the glass ceiling for various natural language processing (NLP) tasks (Radford et al., 2018; Devlin et al., 2019; Han et al., 2021). However, most of the existing PLMs are typically trained with a static snapshot of the web information, ignoring that in real-world scenarios, streaming data from various sources may continuously grow, e.g., the gatherings of literary works (Zhu et al., 2015), news articles (Zellers et al., 2019) and science papers (Lo et al., 2020). In addition, the distribution of incoming data may also vary over time. This requires PLMs to continually integrate the information from all the sources to grasp the versatile structural and semantic knowledge comprehensively, so\nthat PLMs could utilize the proper knowledge to boost the performance in various downstream tasks.\nA simple yet effective way to integrate all the information is to pre-train PLMs on all the existing data exhaustively. However, such a process is computationally expensive (Schwartz et al., 2019), especially under the information explosion era when tremendous data is continually collected. This leaves us an important question: with limited computational resources, how can we efficiently adapt PLMs in a lifelong manner? We formulate it as the efficient lifelong pre-training problem. Similar to conventional lifelong learning, PLMs are expected to continually abosrb knowledge from emerging data, and in the meantime, mitigate the catastrophic forgetting (McCloskey and Cohen, 1989) on previously learned knowledge.\nIn addition, efficient lifelong pre-training poses two new challenges: (1) efficient knowledge growth. When the overall data scale accumulates to a certain magnitude, packing more knowledge into a fixed-sized PLM becomes increasingly hard, which significantly impacts the efficiency of PLM’s knowledge growth. This is because larger PLMs show superior sample efficiency and training efficiency over their smaller counterparts (Kaplan et al., 2020; Li et al., 2020) due to overparameterization (Arora et al., 2018). That is, larger PLMs learn knowledge in a more efficient way. Therefore, timely model expansions are essential for efficient knowledge growth; (2) proper knowledge stimulation. During pre-training, various knowledge from all domains is packed into PLMs hastily. However, a certain downstream task may largely require the knowledge from a specific domain. Thus it is essential for PLMs to disentangle different kinds of knowledge and properly stimulate the needed knowledge for each task.\nIn this paper, we propose ELLE, targeting at Efficient LifeLong pre-training for Emerging data. Specifically, (1) to facilitate the efficiency of knowl-\nedge growth, we propose the function preserved model expansion to flexibly expand an existing PLM’s width and depth. In this way, we increase PLM’s model size and thus improve its training efficiency. Before being adapted to a new domain, the expanded PLM performs a function recovering warmup to regain the functionality of the original PLM; (2) for proper knowledge stimulation, we pre-implant domain prompts during pre-training to prime the PLM which kind of knowledge it is learning. Therefore, versatile knowledge from multiple sources can be disentangled. During downstream fine-tuning, we could further utilize these implanted prompts and manipulate the PLM to stimulate the proper knowledge for a specific task.\nTo demonstrate the effectiveness of ELLE, we simulate the scenario where streaming data from 5 domains sequentially comes. We pre-train two typical PLMs (BERT and GPT) and expand their model sizes each time when the new data is available. We experiment when the number of parameters is sequentially grown from both 30M to 125M and 125M to 355M. The experimental results show the superiority of ELLE over multiple lifelong learning baselines in both pre-training efficiency and downstream task performances. In addition, we conduct sufficient experiments to verify the effectiveness of each component of ELLE. In general, we provide a promising research direction and hope this work could inspire more future attempts towards efficient lifelong pre-training."
    }, {
      "heading" : "2 Related Work",
      "text" : "Lifelong Learning for PLMs. Lifelong learning aims at incrementally acquiring new knowledge, and in the meantime, mitigating the catastrophic forgetting issue. Numerous efforts have been spent towards this goal, including (1) memory-based methods (Rebuffi et al., 2017; Rolnick et al., 2019), which perform experience replay with authentic data (de Masson d’Autume et al., 2019), automatically generated data (Sun et al., 2020), or previously computed gradients (Lopez-Paz and Ranzato, 2017) conserved in the memory, (2) consolidationbased methods (Kirkpatrick et al., 2017; Aljundi et al., 2018), which introduce additional regularization terms to consolidate the model parameters that are important to previous tasks, and (3) dynamic architecture methods (Rusu et al., 2016; Yoon et al., 2018), which fix trained network architectures in old tasks and dynamically grow branches for new\ntasks. Lifelong learning is also a hot topic for PLMs. Some target at domain adaptation through continual pre-training (Gururangan et al., 2020), parameter-efficient adapters (He et al., 2021) and sparse expert models (Gururangan et al., 2021). Others focus on the incremental acquisition of factual knowledge that changes over time (Dhingra et al., 2021; Jang et al., 2021). However, the existing works seldom consider our lifelong learning setting where streaming data from multiple sources is sequentially gathered. A concurrent work (Jin et al., 2021) conducts empirical studies on conventional continual learning algorithms for PLM adaptation. However, they do not focus on PLM’s training efficiency, which is different from our setting. More detailed comparisons are left in appendix F.\nEfficient Pre-training in NLP. Many attempts have been made towards improving the efficiency of pre-training, such as designing novel pretraining tasks (Clark et al., 2020), model architectures (Zhang and He, 2020), optimization algorithms (You et al., 2020) and parallel architectures (Shoeybi et al., 2019; Shazeer et al., 2018). Until recently, researchers propose to “back distill” the knowledge from existing PLMs to accelerate large PLMs’ pre-training (Qin et al., 2021). Another line of work proposes progressive training to dynamically expand an existing PLM’s size through parameter recycling (Gong et al., 2019; Gu et al., 2021; Chen et al., 2021). However, these methods typically focus on training PLMs on one static corpus, and thus cannot be directly applied to our lifelong pre-training setting."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "Background for PLM. A PLM M generally consists of an embedding layer and L Transformer (Vaswani et al., 2017) layers. Given an input x consisting of a series of tokens, i.e., x = {w1, . . . , w|x|}, M first converts the input into embeddings {h01, . . . ,h0|x|}, which are sequentially processed by each Transformer layer into contextualized hidden representations Hl = {hl1, . . . ,hl|x|}, where 1≤ l≤L.\nTask Definition. Assume a stream of corpus DN from N domains (e.g., news articles, web content and literary works) is sequentially gathered, i.e., DN = {D1, . . . ,DN}, where Di = {xji} |Di| j=1. The\nwhole training process can be partitioned into several stages. Initially, we have a PLM M1, which has been well trained on D1, and for the i-th stage (i > 1), we obtain a new collection of data Di. Assume in this stage, we only have limited computational resources Ri, our goal is to continually pretrain the existing PLM Mi−1 to learn new knowledge on Di, and obtain a new PLM Mi. Meanwhile, we expect the adapted PLM Mi should not forget the previously learned knowledge of Di−1.\nOverall Framework. As illustrated in Figure 1, starting from Mi−1, which is trained on previous data Di−1, we first expand Mi−1’s width and depth and construct an enlarged PLM MWDi−1 to improve its training efficiency. Then we perform function recovering warmup and train MWDi−1 to inherit the knowledge of Mi−1 to obtain MWD+i−1 . The above procedures are dubbed as function preserved model expansion (§ 3.2). After that, we continually pre-train MWD+i−1 to gain new knowledge on Di. To mitigate the catastrophic forgetting on the previously learned knowledge, we employ data-based memory replay on a subset of previously gathered data Dsubi−1 = {Dsub1 , . . . ,Dsubi−1} conserved in the memory, where Dsubk = {x1k, . . . , xBk } ∈ Dk (1 ≤ k ≤ i− 1) and B is the constrained memory size for each domain. To help PLMs disentangle the knowledge during pre-training and also stimulate the needed knowledge for each downstream task, we implant domain prompts into PLMs during the whole training process (§ 3.3)."
    }, {
      "heading" : "3.2 Function Preserved Model Expansion",
      "text" : "To accumulate knowledge more efficiently, each time when a new corpus Di comes, we expand both Mi−1’s width and depth to attain the superior\nsample efficiency and fast convergence brought by larger model capacity (Li et al., 2020).\nWidth Expansion. For width expansion, we borrow the function preserving initialization (FPI) from Chen et al. (2021). For a brief introduction, FPI expands the matrices of all modules of a Transformer layer to arbitrary larger sizes and constructs an enlarged PLM MWi−1. MWi−1 is initialized using the corresponding matrices of the original Mi−1 through parameter replication. For example, as visualized in Figure 1, the core principle of FPI is to divide the product of o×x1 into multiple partitions, e.g. o2 × x1 + o 2 × x1. Formally, FPI expands a matrix W ∈ Rh1×h2 of Mi−1 to an enlarged matrix W ′ ∈ R(h1+∆h1 )×h2 of MWi−1 as follows:\nm(i) = { i i ∈ [1, h1] U({1, . . . , h1}) i ∈ (h1, h1 +∆h1 ],\nCi = h1+∆h1∑ i′=1 I(m(i′) = m(i)),\nW ′(i,∗) = 1\nCi ·W(m(i),∗) + I(Ci > 1) · δi,\n(1)\nwhere U(·) denotes a uniform sampling function, m(·) denotes the mapping function between two matrices, I(·) is an indicator function, Ci counts how many partitions a specific neuron is splitted and δi ∈ Rh2 is a random gaussian noise. FPI ensures that both MWi−1 and Mi−1 have approximately the same functionality, i.e., both models have almost the same output given the same input. Besides function preservation, the initialized model could serve as a good starting point for further optimization. We refer readers to Chen et al. (2021) for more details about width expansion. Different from Chen et al. (2021), we additionally introduce random noises δi into the newly copied parameters\nof W ′ during initialization. These slight noises would break the symmetry after the replication and accelerate later pre-training.\nDepth Expansion. For depth expansion, previous works generally resort to stacking all the original PLM layers into 2× layers through parameter replication (Gong et al., 2019). Such initialization is demonstrated to improve training efficiency.\nHowever, the above layer stacking method restricts the number of layers of the enlarged PLM MDi−1 to be integer multiples of that of the original PLM Mi−1, which is not flexible for practical uses. To improve the expansion flexibility so that Mi−1 could be expanded with arbitrary number of layers, we propose a novel layer insertion method to construct a new PLM MDi−1 with L+L′ layers, where 1 ≤ L′ ≤ L. Specifically, we randomly select L′ layers from Mi−1, copy each layer’s parameters and insert the replication layer right before / after the original layer. We found empirically that inserting the copied layer into other positions would cause a performance drop, and the reason is that it will violate the processing order of the original layer sequence and break the PLM’s original functionality. At each expansion stage when new data comes, since different layers have different functionalities, we always choose those layers that have not been copied before to help PLMs develop in an all-around way, instead of just developing a certain kind of functionality. Since both width expansion and depth expansion are compatible with each other, we simultaneously expand both of them to construct an enlarged model MWDi−1, which inherits Mi−1’s knowledge contained in the parameters.\nFunction Recovering Warmup. Since the above model expansion cannot ensure exact function preservation and inevitably results in functionality loss and performance drops, we pre-train the initialized PLM MWDi−1 on the previous corpora Dsubi−1 conserved in the memory to recover the language abilities lost during model expansion, which is dubbed as function recovering warmup (FRW). After the warmup, we obtain MWD+i−1 , which successfully inherits the knowledge from Mi−1 and is also well-prepared for the next training stage."
    }, {
      "heading" : "3.3 Pre-trained Domain Prompt",
      "text" : "Instead of training a separate model for each domain, we expect a single compact PLM to integrate the knowledge from all the sources. When\nconfronted with a downstream task from a specific domain, the PLM needs to expose the proper knowledge learned during pre-training. To facilitate both knowledge acquisition during pre-training and knowledge exposure during fine-tuning, we resort to prompts as domain indicators and condition the PLM’s behavior on these prompts.\nSpecifically, during pre-training, to disentangle the knowledge from different sources, we implant a soft prompt token into the input to prime the PLM which kind of knowledge it is learning. The prompt of domain i is a tunable vector pi. We prepend pi before the original token embeddings H0 = {h01, . . . ,h0|x|} for an input x ∈ Di, resulting in the modified input H0∗ = {pi;h01, . . . ,h0|x|}, which is then processed by all the Transformer layers. Each pi is optimized together with other parameters of the PLM during pre-training. During fine-tuning, when applying the PLM on a similar domain of data seen before, we could leverage the trained domain prompt and prepend it before the input of downstream data. In this way, we manually manipulate the PLM to stimulate the most relevant knowledge learned during pre-training."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setting",
      "text" : "Data Streams. We simulate the scenario where streaming data from 5 domains is gathered sequentially, i.e., the concatenation of WIKIPEDIA and BOOKCORPUS (WB) (Zhu et al., 2015), NEWS ARTICLES (NS) (Zellers et al., 2019), AMAZON REVIEWS (REV) (He and McAuley, 2016), BIOMEDICAL PAPERS (BIO) (Lo et al., 2020) and COMPUTER SCIENCE PAPERS (CS) (Lo et al., 2020). For each corpus Di, we roughly sample 3, 400M tokens, and the quantity for each Di is comparable to the pre-training data of BERT (Devlin et al., 2019). In addition, considering that in practice, the expense of storage is far cheaper than the computational resources for pre-training, we maintain a relatively large memory compared with conventional lifelong learning settings by randomly sampling 200M tokens (Dsubi ) for each corpus Di.\nEvaluated Models. We mainly follow the model architectures of BERT and GPT (Radford et al., 2018). We use byte-level BPE vocabulary (Radford et al., 2018) to ensure there are few unknown tokens in each corpus. We experiment with the initial PLM M1 of 6 layers and hid-\nden size of 384 (around 30M parameters, denoted as BERTL6_D384 / GPTL6_D384), and linearly enlarge the PLM’s number of parameters for 4 times, to the final PLM M5 of 12 layers and hidden size of 768 (around 125M parameters, denoted as BERTL12_D768 / GPTL12_D768). We also experiment on a larger model size, i.e., growing the PLM from BERTL12_D768 (125M) to BERTL24_D1024 (355M). Details of each Mi’s architecture are listed in appendix C. We also discuss the effect of expanded model size at each stage in appendix B1.\nTraining Details. We train our model for 62, 500 steps for the first corpus. For the following domain i (i > 1), after the model expansion, we perform function recovering warmup for 5, 000 steps, then train the resulting PLM for 20, 000 steps on the new data together with memory replay. Following Chaudhry et al. (2019b), we jointly train PLMs on a mixture samples from both Di and D sub i−1 in each batch, and the sampling ratio of Di and D sub i−1 is set to 9 : 1 in every batch. Adam (Kingma and Ba, 2015) is chosen as the optimizer. All the experiments are conducted under the same environment of 8 V100 GPUs with a batch size of 2, 048. More training details of pre-training are left in ap-\n1Being the first work towards efficient lifelong pre-training, this paper experiments on an ideal setting that the corpus size of each domain is the same, and the number of parameters is grown linearly. We encourage future work to explore the effect of the size of streaming data and optimal expanded model size.\npendix C. We also experiment with fewer computational budgets and memory budgets in appendix I.\nEvaluation Metrics. We deem one algorithm to be more efficient if it could achieve the same performance with other methods utilizing fewer computations. For PLM, this is equivalent to achieving better performance using the same computations since pre-training with more computations almost always results in better performance (Clark et al., 2020). We evaluate the PLM’s performance during both pre-training and downstream fine-tuning.\nSpecifically, for pre-training, we propose two metrics to evaluate how PLMs perform on the learned domains following Chaudhry et al. (2019a): (1) average perplexity (AP) and (2) average increased perplexity (AP+). We record the train wall time (Li et al., 2020) during pre-training. For a model checkpoint at time step T when learning the j-th domain, we measure the checkpoint’s perplexity PPLT,i on the validation set of each domain i. Let PPLfi,i be the perplexity on the i-th domain when the PLM finishes training on the i-th domain, the above metrics are calculated as follows:\nAP = exp (1 j j∑ i=1 log PPLT,i ) ,\nAP+ = 1\nj − 1 j−1∑ i=1 (PPLT,i − PPLfi,i),\n(2)\nwhere AP measures the average performance on all the seen data {D1, . . . ,Dj}. Lower AP indicates\nthe PLM generally learns more knowledge from existing domains; AP+ measures the influence of current data Dj on previous data Dj−1. Lower AP+ means PLMs forget less knowledge learned before.\nTo evaluate PLMs’ performance in downstream tasks, for each domain, we select a representative task that is relatively stable, i.e., MNLI (Williams et al., 2018), HYPERPARTISAN (Kiesel et al., 2019), HELPFULLNESS (McAuley et al., 2015), CHEMPROT (Kringelum et al., 2016) and ACLARC (Jurgens et al., 2018) for WB, NS, REV, BIO and CS, respectively. Training details for finetuning are left in appendix D.\nBaselines. Keeping most of the experimental settings the same, we choose the following baselines for comparison: (1) Naive, which is a naive extension of Gururangan et al. (2020) to continually adapt PLMs for each domain and can be seen as the lower bound; (2) EWC (Schwarz et al., 2018), which adopts elastic weight consolidation to add L2 regularization on parameter changes; (3) MAS (Aljundi et al., 2018), which estimates parameter importance via the gradients of the model outputs; (4) ER (Chaudhry et al., 2019b), which alleviates forgetting by jointly training models on a mixture samples from new data Di and the memory Dsubi−1. ELLE is based on ER and additionally introduces the model expansion and pre-trained domain prompts. For ER, we set the sampling ratio of Di and Dsubi−1 to be 9 : 1 in every batch same as ELLE; (5) A-GEM (Chaudhry et al., 2019a), which constrains the new parameter gradients to make sure that optimization directions do not conflict with gradients on old domains; (6) Logit-KD, which\nprevents forgetting by distilling knowledge from the previous model Mi−1 using the old data in the memory; (7) PNN (Rusu et al., 2016), which fixes the old PLM Mi−1 to completely avoid knowledge forgetting and grows new branches for learning new knowledge. For a fair comparison, we control the total train wall time of ELLE and all the baselines to be the same at each training stage, so that each method consumes the same computational costs."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "Table 1 summarizes the pre-training performance each time when the PLM finishes training on a specific domain. Figure 2 depicts the trend of AP for BERT w.r.t. train wall time, other trend curves are illustrated in appendix E. We also report the final downstream performance for discriminative PLMs (BERT) on each domain after finishing the whole pre-training in Table 2. The intermediate downstream performance each time when the PLM finishes training on one domain is left in appendix D.\nSuperiority of ELLE. (1) From the results in Table 1, we observe that, compared with all the baselines, ELLE achieves the lowest AP and satisfying AP+ after finishing training on each domain. This demonstrates that, given limited computational resources, ELLE could acquire more knowledge and in the meantime, mitigate the knowledge forgetting problem. (2) We also observe from Figure 2 that the AP of ELLE descends the fastest, showing the superior training efficiency of ELLE over all baselines. (3) Besides, ELLE performs the best on all downstream tasks, indicating that the knowledge\nlearned during pre-training could be properly stimulated and leveraged for each downstream task. (4) The superiority of ELLE is consistently observed on the larger model size, i.e., BERTL24_D1024 and other model architectures, i.e., GPTL12_D768. This shows that ELLE is agnostic to both the model size and the specific PLM model architecture chosen. We expect future work to apply ELLE on other PLM architectures and extremely large PLMs.\nComparisons with Baselines. (1) First of all, consolidation-based methods (EWC and MAS) perform almost comparable with the naive baseline in either pre-training or downstream tasks. This means that parameter regularization may not be beneficial for PLMs’ knowledge acquisition. (2) Among memory-based methods, gradient-based reaply (A-GEM) exhibits poorer performance in pre-training, on the contrary, data-based replay (ER and Logit-KD) achieve lower AP and AP+ than the naive baseline, demonstrating that replaying real data points could more efficiently mitigate the knowledge forgetting problem. Meanwhile, all of the memory-based methods perform comparable or worse than the naive baseline in downstream performance. (3) Although PNN achieves significantly lower AP than other baselines, and is also immune to knowledge forgetting (AP+=0), it performs extremely poorly on downstream tasks. This indicates that although PNN acquires much knowledge during pre-training, such knowledge is not stimulated and leveraged during fine-tuning."
    }, {
      "heading" : "5 Analysis",
      "text" : "In this section, we conduct analyses to investigate the effect of ELLE’s components. We follow the setting in § 4 by choosing BERTL6_D384 as the initial model and continually growing it to BERTL12_D768. Specifically, we investigate the effect of (1) width expansion (WE), (2) depth expansion (DE), (3) function recovering warmup (FRW),\n(4) the random noises added into the newly constructed parameters during model expansion (δN ) and (5) the pre-trained domain prompts (PT). We test ELLE under different combinations of the above components and compare the results. The experimental results of pre-training and downstream tasks are summarized in Table 3 and Table 4, respectively. Detailed trend curves for AP and AP+ are illustrated in appendix E. We also show in appendix A that the expanded PLM by ELLE exhibits similar functionality to the original PLM.\nEffect of Width / Depth Expansion. First, we compare the differences of conducting only width expansion (WE+FRW), only depth expansion (DE+FRW) and expansion on both width and depth (WE+DE+FRW) before function preserving warmup. For a fair comparison, we keep the total number of Mi’s increased parameters for the above three strategies almost the same at each stage i. The specific model architectures are listed in appendix H. The results show that: (1) compared with the non-expanding baseline, all these three strategies achieve better pre-training and downstream performance, showing that with the growth of model size, the sample efficiency and training efficiency are extensively increased. Therefore, PLMs could gain more knowledge with limited computational resources and perform better in downstream tasks; (2) compared with expanding only width or depth, expanding both of them is more efficient and can also achieve better downstream performance on almost all domains, except the NS domain. This is also aligned with previous findings that PLM’s growth favors compound scaling (Gu et al., 2021). We also conclude from the trend curves in appendix E that only expanding depth will make the training process unstable.\nEffect of Function Recovering Warmup. We compare the performance of the model expansion\nw/ and w/o FRW, i.e., WE+DE and WE+DE+FRW. For a fair comparison, we keep the total train wall time for either strategy the same, in other words, for WE+DE, PLMs can be trained for more steps on the new domain due to the removal of FRW. However, the results show that WE+DE achieves worse AP and AP+, indicating that without FRW, PLM would learn new knowledge slower and also forget more previous knowledge. The trend curve in appendix E also shows that AP and AP+ decrease faster with FRW. This demonstrates the necessity of the warmup after model expansion, i.e., PLMs could better recover the knowledge lost during model expansion and also get prepared for learning new knowledge. Meanwhile, WE+DE+FRW performs slightly better than WE+DE in most of the downstream tasks, except the NS domain.\nEffect of Random Noises. Different from the original FPI (Chen et al., 2021), ELLE additionally adds random noises into the newly copied parameters after expanding the width of PLMs as mentioned in § 3.2. By comparing the model performance w/ and w/o this trick, i.e., WE+DE+FRW and WE+DE+FRW+δN , we can see that the added noises significantly speed up pre-training and also conduce to improving PLM’s overall downstream performance. This validates our hypothesis that random noises are useful for breaking the symmetry of the copied parameters, thus providing a better initialization that further optimization favors.\nEffect of Pre-trained Domain Prompts. To investigate the effect of pre-trained domain prompts, we first compare the performance w/ and w/o them, i.e., WE+DE+FRW+δN and WE+DE+FRW+δN+PT. From the results we can conclude that when aided with domain prompts, PLMs achieve lower AP and AP+ during pretraining, showing that domain prompts could accel-\nerate pre-training and alleviate catastrophic forgetting by disentangling the knowledge from different sources. Furthermore, domain prompts generally improve downstream performance by stimulating the proper knowledge needed for each task.\nTo rigorously investigate how domain prompts stimulate the knowledge during fine-tuning, for a PLM pre-implanted with prompts during pretraining, we test its downstream performance when (1) no prompt is prepended in the input (i.e., ELLEPTfine-tune) during fine-tuning and (2) a prompt from a random wrong domain is prepended in the input (i.e., ELLE + ¬PTfine-tune). The results in Table 5 show that both of the above strategies have lower downstream performance than prepending the right prompt (ELLE). We hypothesize the reasons are two-fold: (1) firstly, for ELLE- PTfine-tune, there exists a great gap between the formats of input during pre-training and fine-tuning, and such a gap would hinder the successful knowledge transfer; (2) secondly, for ELLE + ¬PTfine-tune, although the above gap disappears, the PLM is primed with a wrong domain prompt, and thus cannot properly stimulate the knowledge that is most relevant to the downstream task. Although manually deciding the most relevant domain prompt for a specific downstream task is relatively easy and fast, such a process can also be automated by training a domain discriminator, which is left as future work."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we present the efficient lifelong pretraining problem, which requires PLMs to continually integrate the information from emerging data efficiently. To achieve our goal, we propose ELLE and progressively expand PLMs to acquire knowledge efficiently and mitigate the knowledge forgetting. We also pre-implant domain prompts during pre-training and use them to stimulate the needed knowledge for downstream tasks. The experimental results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances."
    }, {
      "heading" : "A Attention Pattern Visualization of a Stream of PLMs",
      "text" : "Through the function preserved model expansion, PLMs inherit the knowledge of their “ancestors” contained in the parameters. Intuitively, the descendant PLM (the expanded larger PLM) should have similar functionalities to the ancestor PLM (the original PLM before model expansion). In this section, we investigate such functionality similarity through the lens of attention patterns of each attention head in the Transformer layer.\nSpecifically, we visualize the attention patterns of a stream of PLMs ({M1, . . . ,M5}) trained by ELLE when growing BERTL6_D384 to BERTL12_D768. We checkpoint each PLM Mi when it finishes training on the emerging data Di. We input the same data into these checkpoints to derive the attention patterns.\nThe results are illustrated in Figure 3, from which we observe that the attention patterns of a head in a descendant PLM are surprisingly similar to those of its “ancestors”, even if the descendant PLM is further trained on the new data and enlarged many times. This indicates that the expanded PLM by ELLE successfully inherits the knowledge from its “ancestor”, and thus exhibits similar functionality to some extent."
    }, {
      "heading" : "B Additional Analysis on Function Preserved Model Expansion",
      "text" : "In addition to the analyses of function preserved model expansion conducted in our main paper, in this section, we further analyze the effect of (1) the expanded model size at each training stage and (2) the choice of copied layer during depth expansion. We experiment on the combination of WE+DE+FRW as mentioned in § 5 and choose BERTL6_D384 as the initial PLM M1. Other settings are kept the same as § 5.\nEffect of Expanded Model Size. In our main experiments, we assume that the data size of each emerging corpus is the same and linearly enlarge the model size when conducting model expansion. In this section, we explore the effect of expanded model size given limited computational resources. We conduct experiments on a stream of data from 3 domains, i.e., WB, NS and REV domain. We start from the initial PLM BERTL6_D384 and continually adapt it to new corpora. Under the same training\nenvironment, we control the computational costs (train wall time) of each domain to be 7200 seconds. We compare the performances when the PLM expands 0, 2, 4, and 6 layers and heads for each domain, respectively. Note the PLMs expanded with a larger size would be trained with fewer steps to control the train wall time.\nThe results are shown in Table 6, from which we can conclude that the best performance is obtained when the model expands 2 layers and heads at each expansion stage, and expanding more or fewer parameters leads to a performance drop. The reasons are two-fold: (1) firstly, as mentioned before, expanding the model size improves the sample efficiency (Kaplan et al., 2020; Li et al., 2020), which is beneficial for PLMs’ knowledge acquisition; (2) secondly, when increasing the expanded model size, the benefits from inheriting the knowledge of a small PLM would become less and less evident. To sum up, expanding with an intermediate size strikes the best trade-off between the above two reasons, and there may exist an optimal expanded size when performing model expansion.\nIntuitively, the optimal expanded model size may be influenced by many factors, e.g., the computational budgets, the amount of emerging data, the PLM’s model architecture, etc. And systematically analyzing the effects of all these factors is beyond the scope of this paper, thus we expect future works to design algorithms to accurately estimate the optimal expanded size for model expansion.\nChoice of Copied Layer. As mentioned in § 3.2, each time when we conduct width expansion, we choose those layers that have not been copied before. To demonstrate the benefit of this trick, we compare three expansion strategies: (1) always replicating those layers that have not been copied before (WE+DE+FRW); (2) always replicating the first layer (WE+DEfirst+FRW) and (3) always replicating the last layer (WE+DElast+FRW).\nThe results in Figure 4 show that AP and AP+ descend the fastest when we always replicate those layers that have not been copied before (i.e., WE+DE+FRW). This demonstrates that, since different layers have different functionalities, choosing those layers that have not been expanded before would help PLMs develop in an all-around way, instead of just developing a certain kind of functionality. Furthermore, we find empirically that when pre-training PLMs continually on multiple domains, if we always choose those layers\nthat have not been expanded before at each depth expansion stage, then the final performance is not sensitive to choosing which layers to expand first."
    }, {
      "heading" : "C Pre-training Hyper-parameters",
      "text" : "In Table 7, we list the architectures and the hyperparameters for the PLMs we pre-trained with ELLE in this paper, including the total number of trainable parameters (nparams), the number of layers (nlayers), the number of units in each bottleneck layer (dmodel), the number of attention heads (nheads), the inner hidden size of FFN layer (dFFN), the learning rate (lr), the training steps of FRW (SF), the training steps of adaptation after FRW (STF) when learning the new corpus, the ratio of learning rate warmup (RW), and the total train wall time (TWT). We set the dropout rate for each model to 0.1, weight decay to 0.01 and use linear learning\nrate decay for BERT and inverse square root decay for GPT. We adopt Adam (Kingma and Ba, 2015) as the optimizer. The hyper-parameters for the optimizer is set to 1× 10−6, 0.9, 0.98 for ϵ, β1, β2, respectively. We reset the optimizer and the learning rate scheduler each time when the PLM finishes FRW or the training on new corpus. All experiments are conducted under the same computation environment with 8 NVIDIA 32GB V100 GPUs. All the pre-training implementations are based on fairseq2 (Ott et al., 2019) (MIT-license).\nD Implementation Details and Additional Experiments for Downstream Fine-tuning\nImplementation Details. Table 8 describes the hyper-parameters for fine-tuning PLMs on down-\n2https://github.com/pytorch/fairseq\nstream tasks of each domain. The implementations of MNLI are based on fairseq3 (Ott et al., 2019) (MIT-license). The implementations of HYPERPARTISAN, HELPFULNESS CHEMPROT, and ACL-ARC are based on (Gururangan et al., 2020)4.\nAdditional Experiments. Figure 5 visualizes the average F1 on all downstream tasks of seen domains {1, . . . , i} of PLMs trained with MAS, ER, Logit-KD, PNN and ELLE after finishing training on each domain i when we choose BERTL6_D384 as the initial PLM M1. The average F1 when fin-\n3https://github.com/pytorch/fairseq 4https://github.com/allenai/\ndont-stop-pretraining\nishing training on the i-th domain is calculated as follows:\nF1iavg = 1\ni i∑ j=1 F1j (3)\nwhere F1j is the F1 score on the downstream task of the j-th domain. In addition to the overall performance in Figure 5, we also list the detailed results for each task in Table 9, covering all PLMs trained by each lifelong learning method.\nThe results in both Figure 5 and Table 9 show that ELLE outperforms all the lifelong learning baselines after finishing training on each domain, demonstrating that ELLE could properly stimulate the learned knowledge during pre-training and\nboost the performance in downstream tasks.\nE Trend Curves for AP and AP+\nFor the experiments in § 4, the trend curves of average perplexity (AP) and average increased perplexity (AP+) w.r.t train wall time are shown in Figure 7 (growing from BERTL6_D384 to BERTL12_D768), Figure 8 (growing from BERTL12_D768 to BERTL24_D1024), and Figure 9 (growing from GPTL6_D384 to GPTL12_D768). Each figure illustrates the performance of different lifelong learning methods. The above results reflect that, compared with all the baselines, AP and AP+ of ELLE descend with the fastest speed, demonstrating that ELLE could acquire knowledge and mitigate the knowledge forgetting on previous domains more efficiently. Thus given limited computational resources, PLMs trained by ELLE could integrate more information from different domains.\nFor the analysis in § 5, we visualize the trend curves of AP and AP+ when choosing different combinations of strategies. Specifically, we investigate (1) the effect of width / depth expansion in Figure 10 (comparing WE+FRW, DE+FRW and WE+DE+FRW); (2) the effect of function recovering warmup in Figure 11 (comparing WE+DE and WE+DE+FRW); (3) the effect of random noises added into the newly initialized parameters during model expansion in Figure 11 (comparing WE+DE+FRW and WE+DE+FRW+δN ) and (4) the effect of pre-trained domain prompts in Figure 12 (comparing ELLE and ELLE-PT). All of the above results again demonstrate the effectiveness of ELLE’s each component.\nF Comparison between ELLE and Jin et al. (2021)\nSince for PLMs, pre-training with more computations almost always results in better perfor-\nmance (Clark et al., 2020; Li et al., 2020; Kaplan et al., 2020), a simple yet effective method to integrate the information from all domains is to continually pre-train existing PLMs on all the existing data exhaustively. In this regard, the most important consideration for lifelong pre-training should be the training efficiency. Therefore, when comparing different lifelong learning methods, it is important to equalize the computational costs consumed by each method. Conforming to this rule, we control the computational costs (estimated by train wall time (Li et al., 2020)) for all the methods in our experiments the same, and find that ELLE tends to be the most training efficient and could help PLMs acquire more knowledge.\nDifferent from our setting, a concurrent work (Jin et al., 2021) conducts sufficient empirical studies on conventional lifelong learning algorithms for incrementally adapting PLMs to emerging data, including (1) adapter-based methods, (2) memory replay approaches and (3) distillationbased methods. They find distillation-based methods tend to perform the best. When comparing these methods, they control the total training steps\nto be the same. However, reporting training steps does not account for the the computations consumed by (1) the newly introduced model parameters in adapters and (2) the teacher model’s forward during knowledge distillation. The above reasons would make the consumed FLOPs or train wall time of the evaluated methods different5. As mentioned before, in our experiments, by controlling the train wall time to be the same, we find distillation-based methods (Logit-KD) tend to perform worse than the memory replay algorithms (ER) in AP and downstream performances, which is different from Jin et al. (2021)’s conclusion.\nBesides, our work mainly focuses on the domainincremental data stream for PLM adaptation. Different from our work, Jin et al. (2021) also experiment on the PLM lifelong adaptation towards\n5We refer to Li et al. (2020) for the comparison among training steps, FLOPs and train wall time.\nchronologically-ordered tweet stream and discuss the data distribution shift. In general, we believe lifelong learning for PLMs is an interesting topic to explore and hope both Jin et al. (2021) and our work could inspire more future research attempts towards this field."
    }, {
      "heading" : "G Representational Similarity of a Stream of PLMs",
      "text" : "We investigate the representational similarity (Abnar et al., 2019) of a descendant PLM and its ancestors. Representational similarity measures how similar two PLMs represent the data. Specifically, we experiment on a stream of PLMs when growing BERTL6_D384 to BERTL12_D768. For a model Mj and its ancestor Mi (1 ≤ i ≤ j − 1), we randomly sample n [MASK] tokens from the raw corpus Dj , and get the probability distributions pik and p j k output by the LM head of Mi and Mj , respectively for each [MASK] token k, where 1 ≤ k ≤ n. We calculate the average representational similarity (ARS) between Mj and all its ancestors {M1, · · · ,Mj−1} as follows:\nARSj = −1\n(j − 1)× n j−1∑ i=1 n∑ k=1 KL(pik,p j k), (4)\nwhere KL denotes the Kullback-Leibler divergence between two probability distributions. Higher ARSj means the representations of Mj and its ancestors are more similar. To some extent, ARSj could reflect how much knowledge / functionality of the ancestors is preserved by Mj .\nWe compare ARS of PLMs trained by Naive, MAS, ER, Logit-KD and ELLE and illustrate the results in Figure 6, from which we observe that Logit-KD has the highest ARS. This is because the training objective of knowledge distillation in\nLogit-KD is highly correlated with ARS. In addition, ELLE takes second place. We also find that, with PLMs continually absorbing new knowledge, the ASR generally decreases."
    }, {
      "heading" : "H Model Architectures for the Analysis of Model Expansion",
      "text" : "In Table 12, we list the model architectures of all the investigated PLMs when conducting analysis of model expansion in § 5. Specifically, three strategies are investigated, including WE+FRW, DE+FRW and WE+DE+FRW. As mentioned in our main paper, for a fair comparison, we keep the total number of Mi’s increased parameters for the\nabove three strategies almost the same at each stage i."
    }, {
      "heading" : "I Performance of ELLE with Fewer Computational Budgets and Storage Budgets",
      "text" : "To investigate the performance of ELLE under limited (1) computational budgets and (2) storage budgets, in this section, we take an initial step to investigate the effect of (1) training resources (train wall time) and (2) memory size for ELLE. Following the experimental setting in § 4, we continually grow BERTL6_D384 to BERTL12_D768 on a stream of data from 5 domains. We test the performance of ELLE and a series of lifelong learning baselines (MAS, ER, Logit-KD and PNN), by (1) reducing the train wall time by half (for NS, REV, BIO and CS domain) and (2) randomly sample only 34M tokens (1% of the full corpus) as the memory Dsubi for each corpus i, compared with the memory size 200M in § 4.\nThe experimental results for the above two settings are listed in Table 10 (pre-training) and Table 11 (fine-tuning), respectively. We also illustrate the trend curves of AP and AP+ in Figure 13 and Figure 14. From the above results, we find that: (1) when given fewer computational budgets and storage budgets, ELLE still outperforms all the lifelong learning baselines in both pre-training and downstream performance, which demonstrates the superiority of ELLE; (2) for ELLE, when PLMs are trained with fewer computational budgets, we observe significant performance drops in both pre-training (higher AP and AP+) and\ndownstream tasks (lower average F1). This shows that pre-training with fewer computations would harm PLMs’ knowledge acquisition; (3) for ELLE, when there are fewer memory budgets, although we also observe slight performance drops in pretraining (higher AP and AP+), the performance in downstream tasks is generally not influenced, with the average F1 score keeping almost the same (77.8). This shows the data-efficiency of PLMs, i.e., PLMs could easily recall the learned knowledge by reviewing small-scale data conserved in the memory (as few as 1%). As mentioned before, considering that for pre-training, the expense of storage (e.g., hard disks) is far cheaper than the computational resources (e.g., GPUs), the storage space problem for memory seldom needs to be considered."
    } ],
    "references" : [ {
      "title" : "Blackbox meets blackbox: Representational similarity and stability analysis of neural language models and brains",
      "author" : [ "Samira Abnar", "Lisa Beinborn", "Rochelle Choenni", "Willem Zuidema." ],
      "venue" : "arXiv preprint arXiv:1906.01539.",
      "citeRegEx" : "Abnar et al\\.,? 2019",
      "shortCiteRegEx" : "Abnar et al\\.",
      "year" : 2019
    }, {
      "title" : "Memory aware synapses: Learning what (not) to forget",
      "author" : [ "Rahaf Aljundi", "Francesca Babiloni", "Mohamed Elhoseiny", "Marcus Rohrbach", "Tinne Tuytelaars." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "Aljundi et al\\.,? 2018",
      "shortCiteRegEx" : "Aljundi et al\\.",
      "year" : 2018
    }, {
      "title" : "On the optimization of deep networks: Implicit acceleration by overparameterization",
      "author" : [ "Sanjeev Arora", "Nadav Cohen", "Elad Hazan." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Swe-",
      "citeRegEx" : "Arora et al\\.,? 2018",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2018
    }, {
      "title" : "Efficient lifelong learning with A-GEM",
      "author" : [ "Arslan Chaudhry", "Marc’Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny" ],
      "venue" : "In 7th International Conference on Learning Representations,",
      "citeRegEx" : "Chaudhry et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chaudhry et al\\.",
      "year" : 2019
    }, {
      "title" : "On tiny episodic memories in continual learning",
      "author" : [ "Arslan Chaudhry", "Marcus Rohrbach", "Mohamed Elhoseiny", "Thalaiyasingam Ajanthan", "Puneet K Dokania", "Philip HS Torr", "Marc’Aurelio Ranzato" ],
      "venue" : "ArXiv preprint,",
      "citeRegEx" : "Chaudhry et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chaudhry et al\\.",
      "year" : 2019
    }, {
      "title" : "bert2bert: Towards reusable pretrained language models",
      "author" : [ "Cheng Chen", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Yujia Qin", "Fengyu Wang", "Zhi Wang", "Xiao Chen", "Zhiyuan Liu", "Qun Liu." ],
      "venue" : "ArXiv preprint, abs/2110.07143.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Episodic memory in lifelong language learning",
      "author" : [ "Cyprien de Masson d’Autume", "Sebastian Ruder", "Lingpeng Kong", "Dani Yogatama" ],
      "venue" : "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing",
      "citeRegEx" : "d.Autume et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "d.Autume et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Time-aware language models as temporal knowledge bases",
      "author" : [ "Bhuwan Dhingra", "Jeremy R Cole", "Julian Martin Eisenschlos", "Daniel Gillick", "Jacob Eisenstein", "William W Cohen." ],
      "venue" : "ArXiv preprint, abs/2106.15110.",
      "citeRegEx" : "Dhingra et al\\.,? 2021",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient training of BERT by progressively stacking",
      "author" : [ "Linyuan Gong", "Di He", "Zhuohan Li", "Tao Qin", "Liwei Wang", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-",
      "citeRegEx" : "Gong et al\\.,? 2019",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2019
    }, {
      "title" : "On the transformer growth for progressive BERT training",
      "author" : [ "Xiaotao Gu", "Liyuan Liu", "Hongkun Yu", "Jing Li", "Chen Chen", "Jiawei Han." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Gu et al\\.,? 2021",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2021
    }, {
      "title" : "Demix layers: Disentangling domains for modular language modeling",
      "author" : [ "Suchin Gururangan", "Mike Lewis", "Ari Holtzman", "Noah A Smith", "Luke Zettlemoyer." ],
      "venue" : "ArXiv preprint, abs/2108.05036.",
      "citeRegEx" : "Gururangan et al\\.,? 2021",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2021
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-trained models: Past, present and future",
      "author" : [ "Zhao", "Jun Zhu." ],
      "venue" : "ArXiv preprint, abs/2106.07139.",
      "citeRegEx" : "Zhao and Zhu.,? 2021",
      "shortCiteRegEx" : "Zhao and Zhu.",
      "year" : 2021
    }, {
      "title" : "On the effectiveness of adapter-based tuning for pretrained language model adaptation",
      "author" : [ "Ruidan He", "Linlin Liu", "Hai Ye", "Qingyu Tan", "Bosheng Ding", "Liying Cheng", "Jiawei Low", "Lidong Bing", "Luo Si." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Asso-",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "author" : [ "Ruining He", "Julian J. McAuley." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada,",
      "citeRegEx" : "He and McAuley.,? 2016",
      "shortCiteRegEx" : "He and McAuley.",
      "year" : 2016
    }, {
      "title" : "Towards continual knowledge learning of language models",
      "author" : [ "Choi", "Minjoon Seo." ],
      "venue" : "ArXiv preprint, abs/2110.03215.",
      "citeRegEx" : "Choi and Seo.,? 2021",
      "shortCiteRegEx" : "Choi and Seo.",
      "year" : 2021
    }, {
      "title" : "Lifelong pretraining: Continually adapting language models to emerging corpora",
      "author" : [ "Xisen Jin", "Dejiao Zhang", "Henghui Zhu", "Wei Xiao", "Shang-Wen Li", "Xiaokai Wei", "Andrew Arnold", "Xiang Ren." ],
      "venue" : "ArXiv preprint, abs/2110.08534.",
      "citeRegEx" : "Jin et al\\.,? 2021",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2021
    }, {
      "title" : "Measuring the evolution of a scientific field through citation frames",
      "author" : [ "David Jurgens", "Srijan Kumar", "Raine Hoover", "Dan McFarland", "Dan Jurafsky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:391–406.",
      "citeRegEx" : "Jurgens et al\\.,? 2018",
      "shortCiteRegEx" : "Jurgens et al\\.",
      "year" : 2018
    }, {
      "title" : "Scaling laws for neural language models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei." ],
      "venue" : "ArXiv preprint, abs/2001.08361.",
      "citeRegEx" : "Kaplan et al\\.,? 2020",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval2019 task 4: Hyperpartisan news detection",
      "author" : [ "Johannes Kiesel", "Maria Mestre", "Rishabh Shukla", "Emmanuel Vincent", "Payam Adineh", "David Corney", "Benno Stein", "Martin Potthast." ],
      "venue" : "Proceedings of the 13th International Workshop on",
      "citeRegEx" : "Kiesel et al\\.,? 2019",
      "shortCiteRegEx" : "Kiesel et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Overcoming catastrophic forgetting in neural networks",
      "author" : [ "James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 2017
    }, {
      "title" : "Chemprot-3.0: a global chemical biology diseases",
      "author" : [ "Jens Kringelum", "Sonny Kim Kjaerulff", "Søren Brunak", "Ole Lund", "Tudor I Oprea", "Olivier Taboureau" ],
      "venue" : null,
      "citeRegEx" : "Kringelum et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kringelum et al\\.",
      "year" : 2016
    }, {
      "title" : "Train big, then compress: Rethinking model size for efficient training and inference of transformers",
      "author" : [ "Zhuohan Li", "Eric Wallace", "Sheng Shen", "Kevin Lin", "Kurt Keutzer", "Dan Klein", "Joey Gonzalez." ],
      "venue" : "Proceedings of the 37th International Conference on",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "S2ORC: The semantic scholar open research corpus",
      "author" : [ "Kyle Lo", "Lucy Lu Wang", "Mark Neumann", "Rodney Kinney", "Daniel Weld." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online. Asso-",
      "citeRegEx" : "Lo et al\\.,? 2020",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 2020
    }, {
      "title" : "Gradient episodic memory for continual learning",
      "author" : [ "David Lopez-Paz", "Marc’Aurelio Ranzato" ],
      "venue" : "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Lopez.Paz and Ranzato.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lopez.Paz and Ranzato.",
      "year" : 2017
    }, {
      "title" : "Image-based recommendations on styles and substitutes",
      "author" : [ "Julian J. McAuley", "Christopher Targett", "Qinfeng Shi", "Anton van den Hengel." ],
      "venue" : "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information",
      "citeRegEx" : "McAuley et al\\.,? 2015",
      "shortCiteRegEx" : "McAuley et al\\.",
      "year" : 2015
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "Michael McCloskey", "Neal J Cohen." ],
      "venue" : "Psychology of learning and motivation, volume 24, pages 109–165. Elsevier.",
      "citeRegEx" : "McCloskey and Cohen.,? 1989",
      "shortCiteRegEx" : "McCloskey and Cohen.",
      "year" : 1989
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge inheritance for pre-trained language models",
      "author" : [ "Yujia Qin", "Yankai Lin", "Jing Yi", "Jiajie Zhang", "Xu Han", "Zhengyan Zhang", "Yusheng Su", "Zhiyuan Liu", "Peng Li", "Maosong Sun" ],
      "venue" : "ArXiv preprint,",
      "citeRegEx" : "Qin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "icarl: Incremental classifier and representation learning",
      "author" : [ "Sylvestre-Alvise Rebuffi", "Alexander Kolesnikov", "Georg Sperl", "Christoph H. Lampert." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
      "citeRegEx" : "Rebuffi et al\\.,? 2017",
      "shortCiteRegEx" : "Rebuffi et al\\.",
      "year" : 2017
    }, {
      "title" : "Experience replay for continual learning",
      "author" : [ "David Rolnick", "Arun Ahuja", "Jonathan Schwarz", "Timothy P. Lillicrap", "Gregory Wayne." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing",
      "citeRegEx" : "Rolnick et al\\.,? 2019",
      "shortCiteRegEx" : "Rolnick et al\\.",
      "year" : 2019
    }, {
      "title" : "Progressive neural networks",
      "author" : [ "Andrei A Rusu", "Neil C Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell." ],
      "venue" : "ArXiv preprint, abs/1606.04671.",
      "citeRegEx" : "Rusu et al\\.,? 2016",
      "shortCiteRegEx" : "Rusu et al\\.",
      "year" : 2016
    }, {
      "title" : "Green ai",
      "author" : [ "Roy Schwartz", "Jesse Dodge", "Noah A Smith", "Oren Etzioni." ],
      "venue" : "ArXiv preprint, abs/1907.10597. 10",
      "citeRegEx" : "Schwartz et al\\.,? 2019",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2019
    }, {
      "title" : "Progress & compress: A scalable framework for continual learning",
      "author" : [ "Jonathan Schwarz", "Wojciech Czarnecki", "Jelena Luketina", "Agnieszka Grabska-Barwinska", "Yee Whye Teh", "Razvan Pascanu", "Raia Hadsell." ],
      "venue" : "Proceedings of the 35th In-",
      "citeRegEx" : "Schwarz et al\\.,? 2018",
      "shortCiteRegEx" : "Schwarz et al\\.",
      "year" : 2018
    }, {
      "title" : "Mesh-tensorflow: Deep learning for supercomputers",
      "author" : [ "Noam Shazeer", "Youlong Cheng", "Niki Parmar", "Dustin Tran", "Ashish Vaswani", "Penporn Koanantakool", "Peter Hawkins", "HyoukJoong Lee", "Mingsheng Hong", "Cliff Young", "Ryan Sepassi", "Blake A. Hechtman" ],
      "venue" : null,
      "citeRegEx" : "Shazeer et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shazeer et al\\.",
      "year" : 2018
    }, {
      "title" : "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "author" : [ "Mohammad Shoeybi", "Mostofa Patwary", "Raul Puri", "Patrick LeGresley", "Jared Casper", "Bryan Catanzaro." ],
      "venue" : "ArXiv preprint, abs/1909.08053.",
      "citeRegEx" : "Shoeybi et al\\.,? 2019",
      "shortCiteRegEx" : "Shoeybi et al\\.",
      "year" : 2019
    }, {
      "title" : "LAMOL: language modeling for lifelong language learning",
      "author" : [ "Fan-Keng Sun", "Cheng-Hao Ho", "Hung-Yi Lee." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Lifelong learning with dynamically expandable networks",
      "author" : [ "Jaehong Yoon", "Eunho Yang", "Jeongtae Lee", "Sung Ju Hwang." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-",
      "citeRegEx" : "Yoon et al\\.,? 2018",
      "shortCiteRegEx" : "Yoon et al\\.",
      "year" : 2018
    }, {
      "title" : "Large batch optimization for deep learning: Training BERT in 76 minutes",
      "author" : [ "Yang You", "Jing Li", "Sashank J. Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh." ],
      "venue" : "8th International",
      "citeRegEx" : "You et al\\.,? 2020",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2020
    }, {
      "title" : "Defending against neural fake news",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Franziska Roesner", "Yejin Choi." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Accelerating training of transformer-based language models with progressive layer dropping",
      "author" : [ "Minjia Zhang", "Yuxiong He." ],
      "venue" : "ArXiv preprint, abs/2010.13369.",
      "citeRegEx" : "Zhang and He.,? 2020",
      "shortCiteRegEx" : "Zhang and He.",
      "year" : 2020
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "2015 IEEE Interna-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "2020) for the comparison among training steps, FLOPs and train wall time",
      "author" : [ "Li" ],
      "venue" : null,
      "citeRegEx" : "Li,? \\Q2020\\E",
      "shortCiteRegEx" : "Li",
      "year" : 2020
    }, {
      "title" : "2019) of a descendant PLM and its",
      "author" : [ "nar" ],
      "venue" : null,
      "citeRegEx" : "nar,? \\Q2019\\E",
      "shortCiteRegEx" : "nar",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Pre-trained language models (PLM) have broken the glass ceiling for various natural language processing (NLP) tasks (Radford et al., 2018; Devlin et al., 2019; Han et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 177
    }, {
      "referenceID" : 8,
      "context" : "Pre-trained language models (PLM) have broken the glass ceiling for various natural language processing (NLP) tasks (Radford et al., 2018; Devlin et al., 2019; Han et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 177
    }, {
      "referenceID" : 47,
      "context" : ", the gatherings of literary works (Zhu et al., 2015), news articles (Zellers et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 45,
      "context" : ", 2015), news articles (Zellers et al., 2019) and science papers (Lo et al.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 36,
      "context" : "However, such a process is computationally expensive (Schwartz et al., 2019), es-",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "Similar to conventional lifelong learning, PLMs are expected to continually abosrb knowledge from emerging data, and in the meantime, mitigate the catastrophic forgetting (McCloskey and Cohen, 1989) on previ-",
      "startOffset" : 171,
      "endOffset" : 198
    }, {
      "referenceID" : 20,
      "context" : "This is because larger PLMs show superior sample efficiency and training efficiency over their smaller counterparts (Kaplan et al., 2020; Li et al., 2020) due to overparameterization (Arora et al.",
      "startOffset" : 116,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "This is because larger PLMs show superior sample efficiency and training efficiency over their smaller counterparts (Kaplan et al., 2020; Li et al., 2020) due to overparameterization (Arora et al.",
      "startOffset" : 116,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : ", 2020) due to overparameterization (Arora et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 33,
      "context" : "Numerous efforts have been spent towards this goal, including (1) memory-based methods (Rebuffi et al., 2017; Rolnick et al., 2019), which perform experience replay with authentic data (de Masson d’Autume et al.",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 34,
      "context" : "Numerous efforts have been spent towards this goal, including (1) memory-based methods (Rebuffi et al., 2017; Rolnick et al., 2019), which perform experience replay with authentic data (de Masson d’Autume et al.",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 40,
      "context" : ", 2019), automatically generated data (Sun et al., 2020), or previously computed gradients (Lopez-Paz and Ranzato, 2017) conserved in the memory, (2) consolidationbased methods (Kirkpatrick et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 27,
      "context" : ", 2020), or previously computed gradients (Lopez-Paz and Ranzato, 2017) conserved in the memory, (2) consolidationbased methods (Kirkpatrick et al.",
      "startOffset" : 42,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : ", 2020), or previously computed gradients (Lopez-Paz and Ranzato, 2017) conserved in the memory, (2) consolidationbased methods (Kirkpatrick et al., 2017; Aljundi et al., 2018), which introduce additional regularization terms to consolidate the model parameters that are important to previous tasks, and (3) dynamic architecture methods (Rusu et al.",
      "startOffset" : 128,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : ", 2020), or previously computed gradients (Lopez-Paz and Ranzato, 2017) conserved in the memory, (2) consolidationbased methods (Kirkpatrick et al., 2017; Aljundi et al., 2018), which introduce additional regularization terms to consolidate the model parameters that are important to previous tasks, and (3) dynamic architecture methods (Rusu et al.",
      "startOffset" : 128,
      "endOffset" : 176
    }, {
      "referenceID" : 35,
      "context" : ", 2018), which introduce additional regularization terms to consolidate the model parameters that are important to previous tasks, and (3) dynamic architecture methods (Rusu et al., 2016; Yoon et al., 2018), which fix trained network architectures in old tasks and dynamically grow branches for new tasks.",
      "startOffset" : 168,
      "endOffset" : 206
    }, {
      "referenceID" : 43,
      "context" : ", 2018), which introduce additional regularization terms to consolidate the model parameters that are important to previous tasks, and (3) dynamic architecture methods (Rusu et al., 2016; Yoon et al., 2018), which fix trained network architectures in old tasks and dynamically grow branches for new tasks.",
      "startOffset" : 168,
      "endOffset" : 206
    }, {
      "referenceID" : 13,
      "context" : "continual pre-training (Gururangan et al., 2020), parameter-efficient adapters (He et al.",
      "startOffset" : 23,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : ", 2020), parameter-efficient adapters (He et al., 2021) and sparse expert models (Gururangan et al.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "A concurrent work (Jin et al., 2021) conducts empirical studies on conventional",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Many attempts have been made towards improving the efficiency of pre-training, such as designing novel pretraining tasks (Clark et al., 2020), model architectures (Zhang and He, 2020), optimization al-",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 46,
      "context" : ", 2020), model architectures (Zhang and He, 2020), optimization al-",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 44,
      "context" : "gorithms (You et al., 2020) and parallel architectures (Shoeybi et al.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 39,
      "context" : ", 2020) and parallel architectures (Shoeybi et al., 2019; Shazeer et al., 2018).",
      "startOffset" : 35,
      "endOffset" : 79
    }, {
      "referenceID" : 38,
      "context" : ", 2020) and parallel architectures (Shoeybi et al., 2019; Shazeer et al., 2018).",
      "startOffset" : 35,
      "endOffset" : 79
    }, {
      "referenceID" : 31,
      "context" : "Until recently, researchers propose to “back distill” the knowledge from existing PLMs to accelerate large PLMs’ pre-training (Qin et al., 2021).",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "Another line of work proposes progressive training to dynamically expand an existing PLM’s size through parameter recycling (Gong et al., 2019; Gu et al., 2021; Chen et al., 2021).",
      "startOffset" : 124,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "Another line of work proposes progressive training to dynamically expand an existing PLM’s size through parameter recycling (Gong et al., 2019; Gu et al., 2021; Chen et al., 2021).",
      "startOffset" : 124,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "Another line of work proposes progressive training to dynamically expand an existing PLM’s size through parameter recycling (Gong et al., 2019; Gu et al., 2021; Chen et al., 2021).",
      "startOffset" : 124,
      "endOffset" : 179
    }, {
      "referenceID" : 41,
      "context" : "A PLM M generally consists of an embedding layer and L Transformer (Vaswani et al., 2017) layers.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "To accumulate knowledge more efficiently, each time when a new corpus Di comes, we expand both Mi−1’s width and depth to attain the superior sample efficiency and fast convergence brought by larger model capacity (Li et al., 2020).",
      "startOffset" : 213,
      "endOffset" : 230
    }, {
      "referenceID" : 10,
      "context" : "For depth expansion, previous works generally resort to stacking all the original PLM layers into 2× layers through parameter replication (Gong et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 157
    }, {
      "referenceID" : 47,
      "context" : ", the concatenation of WIKIPEDIA and BOOKCORPUS (WB) (Zhu et al., 2015),",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 45,
      "context" : "NEWS ARTICLES (NS) (Zellers et al., 2019), AMAZON REVIEWS (REV) (He and McAuley, 2016), BIOMEDICAL PAPERS (BIO) (Lo et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : ", 2019), AMAZON REVIEWS (REV) (He and McAuley, 2016), BIOMEDICAL PAPERS (BIO) (Lo et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : ", 2019), AMAZON REVIEWS (REV) (He and McAuley, 2016), BIOMEDICAL PAPERS (BIO) (Lo et al., 2020) and COMPUTER SCIENCE PAPERS (CS) (Lo et al.",
      "startOffset" : 78,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : ", 2020) and COMPUTER SCIENCE PAPERS (CS) (Lo et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "For each corpus Di, we roughly sample 3, 400M tokens, and the quantity for each Di is comparable to the pre-training data of BERT (Devlin et al., 2019).",
      "startOffset" : 130,
      "endOffset" : 151
    }, {
      "referenceID" : 32,
      "context" : "We mainly follow the model architectures of BERT and GPT (Radford et al., 2018).",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 32,
      "context" : "We use byte-level BPE vocabulary (Radford et al., 2018) to ensure there are few unknown tokens in each corpus.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "better performance using the same computations since pre-training with more computations almost always results in better performance (Clark et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "We record the train wall time (Li et al., 2020) during pre-training.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 42,
      "context" : ", MNLI (Williams et al., 2018), HYPERPARTISAN (Kiesel et al.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : ", 2018), HYPERPARTISAN (Kiesel et al., 2019), HELPFULLNESS (McAuley et al.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : ", 2019), HELPFULLNESS (McAuley et al., 2015), CHEMPROT (Kringelum et al.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : ", 2015), CHEMPROT (Kringelum et al., 2016) and ACLARC (Jurgens et al.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : ", 2016) and ACLARC (Jurgens et al., 2018) for WB, NS, REV,",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 37,
      "context" : "ally adapt PLMs for each domain and can be seen as the lower bound; (2) EWC (Schwarz et al., 2018), which adopts elastic weight consolidation to add L2 regularization on parameter changes; (3) MAS (Aljundi et al.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : ", 2018), which adopts elastic weight consolidation to add L2 regularization on parameter changes; (3) MAS (Aljundi et al., 2018), which estimates parameter importance via the gradients of the model outputs; (4) ER (Chaudhry et al.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 35,
      "context" : "prevents forgetting by distilling knowledge from the previous model Mi−1 using the old data in the memory; (7) PNN (Rusu et al., 2016), which fixes the old PLM Mi−1 to completely avoid knowledge forgetting and grows new branches for learning new knowledge.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 11,
      "context" : "This is also aligned with previous findings that PLM’s growth favors compound scaling (Gu et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Different from the original FPI (Chen et al., 2021), ELLE addition-",
      "startOffset" : 32,
      "endOffset" : 51
    } ],
    "year" : 0,
    "abstractText" : "Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pretraining on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM’s width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pretraining and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. All the data, model parameters and codes used will be available upon publication.",
    "creator" : null
  }
}
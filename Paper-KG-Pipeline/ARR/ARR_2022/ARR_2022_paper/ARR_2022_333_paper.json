{
  "name" : "ARR_2022_333_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Thai Nested Named Entity Recognition Corpus",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) is a task of extracting named entities from given text. It identifies the span of each entity and categorizes the identified span into an entity category. NER is essential in many downstream tasks, e.g., entity linking, question answering, and knowledge graph. In addition, Yamada et al. (2020) show that the contextualized representations that include entity information can improve many downstream tasks. The conventional NER paradigm can only label one entity type for each entity span. For example, the entity “Chiang Mai University” will be considered as a single span ignoring the nested structure of the term “Chiang Mai,” which is the name of the town that the university is situated in. As a result, we may overlook critical information that\nmay have an impact on the language understanding in a downstream task. To mitigate this drawback, one may introduce a nested structure into the NER problem. Let us again consider the “Chiang Mai University” example. In addition to annotating the entire span as an organization, N-NER also identifies the sub-entity of “Chiang Mai” as a location. This feature can be useful in a downstream task that requires linking an entity to useful references, e.g., a university to its affiliated city. Considerable research attention has been dedicated to formulating a technique to solve the NNER problem (Straková et al., 2019a,b; Lin et al., 2019; Wang et al., 2020a; Luo and Zhao, 2020; Shibuya andHovy, 2020;Wang et al., 2020b). One can use an N-NER model to recursively decompose a complex entity into a tree structure of sub-\nentities and have them annotated accordingly. While N-NER has many potential benefits to downstream tasks that require deep language understanding, there is still a lack of datasets for low-resource languages to help develop reliable NNER models. In order to train N-NER models, we need a dataset with hierarchical information of each named entity. N-NER datasets are available in several languages. Especially, English, a high resource language, has a few N-NER datasets available for multiple domains (Doddington et al., 2004; Walker et al., 2006; Kim et al., 2003; Ringland et al., 2019) including news, social media, and molecular biology. The diversity of N-NER corpora is only available in English. N-NER datasets are not as widely available for other languages, let alone the diversity of corpora. In German, another high-resource language, there is only one N-NER dataset available (Benikova et al., 2014). For low-resource languages, such as Vietnamese, the two available datasets (Huyen and Luong, 2016; Nguyen et al., 2018) are still small compared to a large N-NER dataset in English (Ringland et al., 2019). In this paper, we address the scarcity of nonEnglish N-NER resources by introducing a Thai N-NER dataset. Despite over 58 million internet users1, the Thai language suffers from the lack of annotated resources to build NLP systems. We propose a Thai N-NER dataset comprising 264,798 entity mentions obtained from 4,894 documents. In addition to the nested entity structure, we also have more than one hundred classes providing great fidelity in entity categorization as shown in Figure 1. The number of entitymentions and variety of entity classes are comparable to a large N-NER dataset in English (Ringland et al., 2019). Our dataset contains text samples, in both formal and colloquial settings, from news articles and restaurant reviews. Additionally, our corpus allows for the multilingual evaluation of “language-agnostic” deep learningmodels, which is the current NLP research trend. To facilitate future N-NER research, we make the dataset, the annotation guideline, and the model weights publicly available. To summarize, our contributions are as follows: • We create the first Thai N-NER dataset annotated with extensive tagsets that cover a wide range of use cases.\n• We evaluate three recent state-of-the-art 1https://www.internetworldstats.com/stats3.htm\n(SOTA) N-NER models on our dataset and study the effect of long-tail classes. • We develop an N-NER benchmark comprising strong baselines for the Thai language that learn each annotation layer separately and achieve performance comparable with the three recent SOTA N-NER models."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we discuss various attempts on NNER corpora. As shown in Table 1, existing NNER corpora are mostly high-resource languages, i.e., English and German, while Vietnamese is the only Asian language that has an N-NER dataset. In terms of the number of classes, it is also worthnoting that three out of six corpora has less than ten and only NNE (Ringland et al., 2019) has more than 100 classes. The details of these corpora are given as follows.\nACE-2004 (Doddington et al., 2004) and ACE2005 (Walker et al., 2006) are early examples of N-NER datasets. ACE-2005 (Walker et al., 2006) dataset comprises 30,966 mentions from 12,548 sentences with 7 coarse-grained entity types. In addition to N-NER annotations, ACE-2005 also contains labels for other tasks such as recognition of relation and event extraction. GENIA (Kim et al., 2003) introduces an N-NER data for bioinformatics. This project provides a high-quality corpus annotated for biological entity names. The dataset composed of 2,000 abstracts, 92,681 mentions from 9,533 sentences with 32 entity types.\nNNE (Ringland et al., 2019) is a recent large finegrained N-NER dataset composed of 114 classes. Unlike previous N-NER corpora, the NNE dataset annotates entities with more details. For example, “6 September 2019”, a date named entity mention, in the NNE dataset, each element in this mention is annotated with finer detail, “6” is annotated with day tag, “September” with month tag, and “2019” with year tag. NoSta-D (Benikova et al., 2014) is the first and only German N-NER dataset. NoSta-D is composed of 41,005 mentions, 12 entity types, and 31,300 sentences from the German Wikipedia and online news. The previous German NER dataset, CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), shows that the performance of German is lower than English’s2. However, the German CoNLL-2003 dataset is known to be inconsistent because it was annotated by non-native speakers. Hence, NoSta-D aims to provide a high quality free public NER dataset by using native speakers as annotators. In contrast to previous N-NER corpora, NoSta-D has a less restrictive copyright license. VLSP–2018 (Nguyen et al., 2018) is a standard benchmark for Vietnamese N-NER. It was designed for the Vietnamese NER shared task to foster the development of high-quality open-source software. This dataset contains 35,817 mentions from 1,282 documents with 3 entity types. DAN+ (Plank et al., 2020) presents the first NNER dataset for Danish. This work investigates the possibility of transfer-learning between languages for the N-NER task. Moreover, DAN+ is a multi-domain dataset; they also study the challenges of domain-shift in their dataset. The dataset contains 6,425mentions, 130,095 tokens, 4 classes from 6,867 sentences, obtained from multiple domains such as news and social media (Reddit, Twitter, and Arto). NoSta-D, VLSP–2018, and DAN+ have a modest corpus size and a small number of entity types comparing to the NNE dataset. This shows that there is still a resource gap for non-English corpora. On the other hand, for Thai, there are only coarsegrained flatten-NER datasets which are publicly available."
    }, {
      "heading" : "3 Thai N-NER corpus",
      "text" : "In this section, we introduce Thai N-NER–the first Thai-Nested Named Entity Recognition dataset.\n2https://www.clips.uantwerpen.be/conll2003/ner/\nOur dataset is comparable to the NNE corpus (Ringland et al., 2019), which is the most elaborate English N-NER dataset in terms of the number of mentions, depth, and the number of classes. In particular, Thai N-NER comprises 264,798 mentions organized into 104 classes and has a maximum depth of 8 layers."
    }, {
      "heading" : "3.1 Data Collection Procedure",
      "text" : "To create the dataset, we gathered 4,894 documents from two different domains: news articles and restaurant reviews. In particular, we obtained 3,196 news articles from Prachathai3, a news website, and 1,698 restaurant reviews from Wongnai4, a crowd-sourced restaurant review platform. The Thai language poses a challenge to the annotation process. Previous work often conducts the annotation at the token level, which is quite convenient for more accurate annotation. However, the lack of clear word boundaries in the Thai writing system does not allow us to easily annotate at the word-level because the data must be word-segmented first, automatically or not. Automatic word segmentation often makes errors around out-of-vocabulary words, which are exactly what we need to annotate. Consequently, the annotation at the word level is not suitable for our purposes if the data are not manually segmented first, which incurs more cost of annotation. Annotating character-level data does not solve the problem either, because annotators are more prone to make an error. To ease and reduce annotation errors from entity span for the annotators, we provided our annotators with syllable-segmented data instead. Aroonmanakun (2002) shows that syllable level could resolve many word-level ambiguities in Thai. Plus, automatic syllable segmentation can be done at a near-perfect accuracy because the task is mostly solved by orthographic rules, assuming few typos exist in the data (Chormai et al., 2020). With syllable boundary indicators, we can avoid errors from word segmentation. In addition, syllablesegmented data reduces the number of indices drastically, which in turn reduces annotation errors."
    }, {
      "heading" : "3.2 Annotation Guideline",
      "text" : "Inspired by the guideline from (Ringland et al., 2019), we designed an N-NER annotation guide-\n3https://huggingface.co/datasets/prachathai67k 4https://github.com/wongnai/wongnai-corpus\nline for Thai5. To cover a wide range of use cases, our N-NER tagsets comprises coarsegrained and fine-grained categories. While finegrained categories create extra burden for the annotation and may result in more errors, the tradeoff is worth it because finer-grained categories lend themselves to be nested within a coarser category. For example, as shown in Figure 2, พ.ต.อ.ประเวศน์ มูลประมุข (phan.tamrùat.Pèk pràwê:t mu:npràmúk) ‘Police Colonel Prawet Munpramuk’ is tagged with PER–a coarse-grained class which encapsulates other fine-grained classes related to person name. Within a coarse-grained mention, we include nested fine-grained information to each nested named-entity element to give more detail. For example, we annotated พ.ต.อ. (phan.tamrùat.Pèk) ‘Police Colonel’ with title name, ประเวศน์ (pràwê:t) ‘Prawet’ with first name, and มูลประมุข (mu:npràmúk) ‘Munpramuk’ with last name.\nพ.ต.อ. ประเวศน์ มูลประมุข phan.tamrùat.ʔèk pràwêːt muːnpràmúk police colonel Prawet Munpramuk ‘Police Colonel Prawet Munpramuk’\nPerson Role First N. Last N.\nพ.ต.อ.ประเวศน์ มูลประมุข\n5we will provide the link to the guideline in the camera ready version. We have attached our guideline along with the dataset in this submission\nwe provided annotators with case studies for common annotating complications. One frequent complication that we found during the annotation process is ambiguous named entities that change their categories depending on the context. The same string annotated as one category in one context might be annotated as another in a different context. To illustrate this complication, we provide the following example:\n(1) ทหาร tháhǎ:n military ไทย thaj Thai โดน do:n is จับ tCàp arrested\n‘Thai military is arrested’\n(2) ทหาร tháhǎ:n military ไทย thaj Thai สั่ง sàŋ ordered ห้าม hâ:m prohibit ออก O:k leave จาก tCà:k from บ้าน bâ:n house\n‘Thai military prohibits going outside of the house’\nIn the example above, the word ทหารไทย (tháhǎ:n thaj) ‘Thai military’ is not always a named entity depending on the context. In example sentence (1) ทหารไทยโดนจับ (tháhǎ:n thaj do:n tCàp) ‘Thai military is arrested’, ‘Thai military’ is not a named entity because ‘Thai military’ refers to a Thai soldier. In contrast, the example sentence (2) ทหารไทยสั่งห้ามออกจากบ้าน (tháhǎ:n thaj sàŋ hâ:m Ò:k tCà:k bâ:n) ‘Thai military prohibits going outside of the house’, ‘Thai military’ is a named entity because it refers to the Thai military institution. A named entity mention that is composed of nested named entities can be regarded as a tree structure. Specifically, the first level of a mention is the outermost or the largest entity span of the mention. The nested entities within the mention in each level must not overlap and can-\nnot span outside of the mention. We provide an example of an issue that arises from overlapping annotations in Appendix A.3. Each coarse-grained entity type can appear in any level of the nested structure. However, finegrained entity type must be nested under its coarse-grained entity type. As shown in Table 2, ประธานคณะกรรมการ 40 ปี 14 ตุลาเพื่อประชาธิปไตยสมบูรณ์ (pràtha:n.kháná.kammáka:n sì:sìp pi: sìpsì: tùla: phŴa pràtCha:thíppàtaj sǒmbu:n) ‘The 40-year 14 Oct for complete democracy committee president’ is the first level of a named-entity mention which is annotated as a role type and the nested structure also contained other coarse-grained mentions such as date or duration. However, fine-grained entity mentions, such as day and month, can only be nested inside the date class."
    }, {
      "heading" : "3.3 Annotation Quality Control Procedure",
      "text" : "To make our dataset reliable, we required that annotators have a background in linguistics and are properly trained to annotate under our guidelines. We also did quality control and evaluation to verify the quality of our dataset."
    }, {
      "heading" : "3.3.1 Annotators",
      "text" : "The dataset was manually annotated by 47 linguistically trained annotators. The annotators met the linguistic background requirement and passed the N-NER guideline understanding test. We provided a communication channel to discuss annotation issues among the annotators and the project manager. We used Datasaur.ai 6 platform for the annotators to label the data according to our guideline, using syllable span highlighting to designate each span as a specific entity."
    }, {
      "heading" : "3.4 Annotation Verification Process",
      "text" : "Firstly, we manually checked the quality of annotated randomly data to find common mistakes. To find more annotation errors, we extracted only the first layer to train a simple flatten CRF model. Then we use the CRF model to filter its prediction errors for further error analysis. Combining the errors found by both humans and the model, we conducted an error analysis to find the pattern of mistakes from annotators. We found prominent annotation mistake patterns, for example, inconsistency tagging, incorrect tagging, and failure to follow the guideline. Then we compiled a list of annotation errors and sent it back to the annotators to reassess.\n6https://datasaur.ai\nAfter the first update, we used a rule-based program to filter overlapping annotations, which did not follow our guideline, then listed all the documents with overlapping annotations. Moreover, we employed a gazetteer to filter mislabeled entities. Later, we reported the list of overlapping documents and the list of mislabeled entities to the annotators to correct all the annotation errors. After the second update, to inspect our dataset quality, we trained an N-NERmodel from Shibuya and Hovy (2020) to see whether our data can be used to trained the model and to filter out more annotation errors. The test score is 75.44% F1 score. We then used the model’s prediction errors to filter out more annotation mistakes and reported them to the annotators for another correction session. Then, we split our dataset into 80% for a training set and 20% for a testing set, then re-annotated all the testing set with two annotators to validate. Finally, the third annotator corrected the annotation mismatches between the first two annotators. We used the Cohen’s Kappa agreement score to benchmark the reliability of our dataset. We computed the inter-annotator agreement using eight sampled documents composed of 2,922 tokens. We calculated the Cohen’s Kappa agreement score using two labeling schemes: CoNLL and Pyramidal, see Appendix A.4 for further descriptions. The agreement scores are given as follows:\n• CoNLL: 0.79; • Pyramidal: 0.85;\nThese high agreement scores imply that our dataset is of good quality."
    }, {
      "heading" : "3.5 Data Format",
      "text" : "To make our dataset convenient for research usage, we provide our dataset in CoNLL-format as shown in Table 2. We define the word boundaries in the dataset by using a maximal matching tokenizer from PyThaiNLP (Phatthiyaphaibun et al., 2016). In addition, we employ the BIOES tagging scheme to indicate the boundary of each named entity mention. Furthermore, we replace each empty space token with “_” in order to keep the integrity of the original text when we convert the CoNLL version back to the original text with no tokenization."
    }, {
      "heading" : "4 Data Statistics",
      "text" : "This section discusses the dataset statistics and analyzes the distribution of classes in the dataset. Ta-\nble 3 shows the dataset statistics of the Thai NNER. The Thai N-NER corpus contains 1,272,381 tokens from 4,894 documents. The dataset has 264,798 named entity mentions, 104 entity types, and 8 maximum depth.\nThe Thai N-NER dataset contains a nested structure for each named-entity mention. The first three layers contain 125,180, 120,909, and 16,500 mentions accounted for 99.2% and mentions all other levels contain 2,209 mentions combined accounted for only 0.8%. The 125,180 first-layer mentions can be divided into 67,168 nested mentions and 58,012 non-nested mentions. We split our dataset into training set, development set, and test set with proportion of 60%, 20%, and 20% respectively. The test set contains all the 104 classes appeared in the training set. We compare our dataset with other N-NER datasets in other langauges. Table 1 shows the statistics of N-NER datasets between NNE, GENIA, ACE-2005 (English), VLSP-2018 (Vietnamese), Dan+ (Danish), and our dataset (Thai). It should be noted that our dataset is comparable to the existing N-NER datasets in term of the number of tokens and the number of entity types. One of the challenges in this dataset is class imbalance. Due to the number of classes, the scarcity of data for rare classes contribute to the severity of class imbalance. We visualize the distribution of classes in training set in Figure 3. The graph shows the distribution of mentions per class in training set sorted by frequency. To analyse the severity of class imbalance, we divided the classes into three groups follow Pareto principle: head, body and tail with samples per classes are 80%, 15% and 5% respectively. More precisely, in body and tail parts, they contain only 20% of samples in training set,\nbut consist of 84 classes from 104 classes.\nTraining set statistics\nIn conclusion, we introduce a dataset for Thai N-NER that is comparable to the standard N-NER dataset in English. Additionally, we point out a challenging long-tail distribution problem in NNER that allows researchers to explore."
    }, {
      "heading" : "5 Experimental Settings and Results",
      "text" : "The objectives of the experimental studies are as follows: the first objective is to help researchers understand how existing techniques perform on our dataset and to help them choose the most appropriate baseline for future research. The second objective is concerned with the distribution of classes which follows the 80-20 Pareto principle. As shown in Figure 3, the top 20% most frequent classes account for 80% of the mentions. We also study how these techniques perform differently at the head, body, and tail parts of the distribution. The third objective is to compare how existing models perform on our Thai dataset with respect to results from existing studies conducted on English datasets."
    }, {
      "heading" : "5.1 Comparative N-NER Models",
      "text" : "Since there is no existing Thai N-NER model, we formulate comparative solutions based on three approaches. The first approach is to build a baseline N-NER method from a classical machine learning technique. The second approach is applying a Thai language model to perform a span classification task. The third approach is to adapt existing N-NER methods to Thai by replacing their encoders with a Thai language model. For ease of comparison, we apply the best existing Thai language model called WangchanBERTa (Lowphansirikul et al., 2021) to second and third approaches.\nClassical ML baseline: CRF model (Minh, 2018) We train multiple CRF models, each model is dedicated to each layer. Then, we merge the prediction results from all layers to form the final NNER result. For this model, we use the IOB tagging scheme because our dataset has a large number of classes; hence the IOBES scheme will take longer to train.\nDeep learning baseline: WangchanBERTa and XLM-RoBERTa. We finetune language model (LM) encoders on our corpus with two architectural variants, LM-separate and LM-shared as shown in Figure 4a and 4b, respectively. For both model, we simply use a fully-connected linear layer as a decoder. For separate-weight (sp) version, we assign one encoder-decoder model for each layer. For shared-weight (sh) version, we use multiple decoders, one for each layer, while sharing the same encoder. We provide more information about parameter settings in Appendix A.1.\nTo compare the performances between monolingual and multilingual BERT variants, we run experiments on both WangchanBERTa (Thai) and XLM-RoBERTa (multilingual).\nState-of-the-art Models: We select three recent SOTA N-NER models with open-source accesses and train them on our corpus. To get these models to work for Thai, we replace their encoders with the same Thai language model as the deep learning baselines (Lowphansirikul et al., 2021). For parameter configurations, we use GENIA’s parameter configurations to make it possible to do sanity check by reproducing previous results on GENIA\nSecond-best-learning (Shibuya and Hovy, 2020): This model learns to recursively decode the nested named entities from the outer to the\ninner nested entities. It is commonly used as a baseline in recent N-NER research. It has strong results for English N-NER.\nPyramid (Wang et al., 2020a): This model learns hierarchical representation from multiple nested levels by using pyramid and inverse pyramid mechanisms. This model currently has the highest score on the NNE dataset.\nLocate and Label (Shen et al., 2021): This model divides entity detection into two stages: (i) it locates the entity spans; (ii) it assigns a label to each entity span. It is the most recent state-of-theart model, it has top-performing scores on ACE2004 and GENIA corpora."
    }, {
      "heading" : "5.2 Evaluation Settings",
      "text" : "We follow the evaluation methodology from (Shibuya and Hovy, 2020), they consider a prediction as a true positive if both the predicted entity span and type are correct. In order to examine the long-tail issue as mentioned in Section 4, we evaluated the effect of long-tail distribution by dividing classes into three groups: head, body, and tail."
    }, {
      "heading" : "5.3 Thai N-NER Results",
      "text" : "Table 4 shows the results on different parts of the long-tailed distribution, as well as the overall results on our dataset. Among the three existing SOTA models, the Second-best-learning model has the highest overall performance. It obtains higher F1 scores on the head and body parts of the long-tail distribution, while the Pyramid model obtains the highest F1 score on the tail part. Interestingly, the deep learning baseline model, WangchanBERTa-sh, outperforms all the current SOTA models. The results show that both WangchanBERTa andXLM-R, while they perform poorer on the head part of the long-tailed distribution, they perform much better on less frequent classes. As shown in Table 4, the performances of WangchanBERTa models on the body and tail parts, and XLM-R models on the tail part are superior to the best SOTA model. By having better performances on body and tail parts, while maintaining a competitive performance on the head part, WangchanBERTa-sh outperforms all the SOTA models on our corpus. The performances of models based on the multilingual encoder (XLM-R) are superior to Pyramid and Locate and label models. However, compared to the monolingual encoder (WangchanBERTa), XLM-R models’ performances are only slightly\npoorer than the monolingual models. This suggests the possibility of cross-lingual N-NER tasks. (e.g. transferring cultural-specific named-entity knowledge from English to Thai). The long-tailed distribution of classes poses a challenge for the N-NER task. The performances across all models quickly deteriorate as we move from the head part of the long-tailed distribution, which represents common classes, to the tail part, which represents infrequent classes. Additionally, notice there are gaps between precision and recall for all models. These gaps imply that all models have a tendency to generate false negatives more than false positives. We can also see that the precision-recall gap has a tendency to increase as we move from the head to the tail part of the distribution. This result suggests that to improve the overall performance, we should pay attention to the recall. In addition, comparing to the results on English N-NER corpora, there is a performance gap for the Thai language. For example, the F1 score of the Pyramid model on the NNE corpus is 94.68, while its performance on our corpus is only 78.50. For the full comparison, see Appendix A.5."
    }, {
      "heading" : "6 Error Analysis",
      "text" : "To understand the limitation of current N-NER solutions, we investigate reoccurring mistake patterns from theWangchanBERTa-spmodels used in the experimental studies. We categorize the common prediction mistakes into four groups as follows: (1) Incorrect span prediction: out of 5,165 prediction errors, 2,977 errors are from span length mismatch as shown in Figure 5. (2) Ambiguous entity mentions: mentions with higher class distribution entropy have more error rates. (3) Ambiguity between fine-grained classes: there are 1,149 fewer errors when evaluated with coarse-grained ground truths. (4) Scarcity of training samples: the model only made 1,422 prediction attempts for\nmentions in tail classes. While 1,101 of the predictions are correct, there are 3,680 ground truths. The previous section also reveals this issue via the poor recall scores in the tail part of the long-tail distribution. We provide the description of each error pattern along with examples in Appendix A.7."
    }, {
      "heading" : "7 Summary",
      "text" : "We present the first Thai N-NER corpus with 104 classes. It has 1,272,381 words, and 264,798 mentions. The size of our corpus is comparable to one of the large N-NER corpora in English. Unlike other Thai NER corpora, in addition to nested structure information, our dataset is annotated with fine-grained entity types to provide more detail of the named entities. This corpus addresses the data scarcity issue for Thai NLP. In addition, it allows NLP researchers to benchmark their methods in a multilingual setting. Moreover, this dataset allows researchers to explore the effect of long-tail distribution. We hope that our dataset will encourage researchers to include Thai in their benchmark and reduce the disparity between Thai and high resource languages.\nEthical Consideration\nOur dataset consists of raw text data from two publicly available corpora: Prachatai-67k and Wongnai review. These corpora use public copyright licenses (LGPL and Creative Commons) that enable free distribution. The data has a minimal risk for privacy violation since all the data were published in a public space, such as a news site and a restaurant review site. All the news articles and restaurant reviews are meant to be shared publicly, not privately. Hence, the dataset does not contain any confidential information. Our preprocessing step, which includes cleaning data and tokenization, does not alter the original contents of the texts. On average, the annotators were compensated at least twice the local minimum wage. The annotators were paid by the number of entity-mentions annotated and the number of documents that they have read. We distributed the same amount of documents for each annotator for fair consideration. This dataset addresses the data scarcity issue for Thai, which can be considered as a lower-resource language. However, this dataset only includes the central Thai dialect, which most Thai understand. It is also the dialect for official usage and is often used as awritten language by Thai internet users. It reduces the language technology disparity gap between Thai and high-resource languages. In addition, it can facilitate researchers and the NLP community to investigate the N-NER task in a multilingual setting. We will open-source the dataset and distribute it publicly under the CC by SA 3.0 license. We will also publish the source code and all the models’ weights from our experiments to assist the NLP community in N-NER research and reduce unnecessary energy usage from training the models."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Parameter Settings For all the deep learning baselines, we use the following parameter configuration: We employ Adam optimizer with a learning rate of 1e-5. We utilize a learning rate decay scheduler that reduces the learning rate every 50 epochs by multiplying the decay factor of 0.1. The maximum training epoch is 500, and we early stop if there is no improvement for 16 epochs. For the Locate and Label model, we made further modifications to the model to use it for the Thai language. Unlike the original work, the sequence length limitation of WangchanBERTa is lower than BERT–large version (Devlin et al., 2019), we use only ten words from each neighboring sentence as the context words to keep the input sequence length within the limitation. In addition, apart from contextualized word embeddings, Locate and Labels also includes static word embeddings–GloVE. We replace the GloVE word embeddings with the static word embeddings layer of thai2fit (Polpanumas and Phatthiyaphaibun, 2021). thai2fit was trained on wisesightsentiment 7, prachathai-64k 8, and TH-wikipedia 9.\nA.2 Coarse-grained vs Fine-grained Scores Table 5 compares the WangchanBERTa-sh model’s performances between the coarse-grained and fine-grained ground truths. We converted finegrained labels to their respective coarse-grained labels to examine the negative effect from the ambiguity between fine-grained classes. Table 5 shows that there is a small gap between coarsegrained and fine-grained evaluations. It suggests that adding fine-grained information to the dataset does not introduce a major challenge for N-NER models. Nevertheless, errors from ambiguity between fine-grained classes still constitute a considerable amount of models’ prediction errors.\nA.3 Issue with Overlapped Annotations Similar to a morphological parse tree, a nested entity annotation structure does not allow overlapping between entities in the same depth level. For example, in Figure 6, รองโฆษกประจํา สํานักนายกรัฐมนตรี (rO:ŋ kho:sòk pràtCam sǎmnák.\n7https://github.com/PyThaiNLP/wisesight-sentiment 8https://github.com/PyThaiNLP/prachathai-67k 9https://dumps.wikimedia.org/thwiki\nรอง โฆษก ประจำ สำนัก นายกรัฐมนตรี rɔːŋ khoːsòk pràtɕam sǎmnák.na:jók.rátthàmontri: deputy spokesperson of office prime minister ‘Deputy Spokesperson of the Office of the Prime Minister’\nรองโฆษกประจำสำนักนายกรัฐมนตรี Role\nRole Government\nOverlapping\nL2\nL1\nFigure 6: The annotation scheme does not allow overlapping between entities in the same layer.\nna:jók.rátthàmontri:) ‘Deputy Spokesperson of the Office of the PrimeMinister’ is the first level of the nested named entity mention. In the second layer, we do not allow an annotator to tag รองโฆษกประจําสํานักนายก (rO:ŋ kho:sòk pràtCam sǎmnák.na:jók) ‘Deputy Spokesperson of the Office of the PM’ with a role tag and สํานัก นายกรัฐมนตรี (sǎmnák.na:jók.rátthàmontri:) ‘Office of the Prime Minister’ with a government tag, because it creates two chunks that share the word สํานักนายก (sǎmnák.na:jók) which is a abbreviated form of ‘Office of the PrimeMinister’ . This would violate the tree structure. In addition, annotating รองโฆษกประจําสํานักนายก (rO:ŋ kho:sòk pràtCam sǎmnák.na:jók) ‘Deputy Spokesperson of the Office of the PM’ as an instance of named entity suggests that ‘Deputy Spokesperson of the Office of the PM’ is a noun phrase and รัฐมนตรี (rátthàmontri:) ‘Minister’ is a nounmodifier, which is semantically incorrect. As far as compositional semantics is considered, the nested structure of named entities should not contain overlapping entities in the same level.\nA.4 Annotation Verification Process\nCoNLL: we format our dataset according to the CoNLL schema, then calculate the Cohen’s Kappa by comparing agreements of annotated entities layer by layer. The CoNLL schema takes the mention’s token length into account. For each disagreed mention, we count each disagreed token as one disagreement. Therefore, mentions with more token length may have more disagreement counts. In addition, if there is a mismatch within the same layer, we count it as a disagreement even though the annotations might agree if we were to compare them from different layers. Pyramidal: we format the labels in a pyramidal manner, where we generate all possible n-gram entity span candidates for each text sequence and assign them to layers according to their lengths in the same fashion as the Pyramid model (Wang et al., 2020a). Then we compare agreements of annotated candidates between the two annotated data. We calculated the score on both character level and token level, and found no difference. We report the score on the token level. Pyramidal scheme counts each disagreed mention as one disagreement despite its length. Since Thai has no word boundary, the pyramid scheme always provides a consistent score despite using it on a different word segmentation that varies the token length.\nA.5 The Performances of the Recent SOTA N-NER Models on English Datasets\nThis study compares the performances of the NNER models between Thai and English N-NER datasets. Table 4 shows the results on the Thai N-NER dataset, and Table 6 shows the results on English N-NER datasets. We can see that, when compared to the English results, all N-NERmodels performed poorer on the Thai dataset. For example, the F1-score of Pyramid on the NNE dataset (the most similar dataset compared to our work) is 94.68%, while the overall F1-score of Pyramid for Thai N-NER is only 78.50%. Although both datasets are similar in size, design, and diversity\nof entity classes, the performance gap is 16.18%. Experimental results verify that there is a performance gap between Thai and English N-NER. Furthermore, some model is based on the BERTlarge model, but Thai has only one BERT-based pretrained model which is based on RoBERTa (WangchanBERTa). This may have a direct affect on the performance gap. For example, the Locate and Label is based on the BERT-large model; replacing BERT-large with WangchanBERTa can effect the performance directly. Despite having the best performances across multiple English N-NER datasets, Locate and Label has the lowest score on the Thai N-NER dataset when compared to other SOTA models.\nA.6 Mention Distribution Table 7 shows the mention frequency of each finegrained entity type in our corpus before the traintest split. For each nested structure, we count all annotated mentions, not just the outermost mention. This table reveals classes with extremely low frequency which contribute to poor performances on the tail part of the long-tailed distribution.\nA.7 Error Analysis Incorrect span prediction: mismatches between the length of the predicted spans and the ground truths contribute to a large chunk of prediction errors. Figure 5 shows that out of 48,009 predictedmentions, 5,165 are incorrect. 2,977 out of 5,165 incorrect predicted mentions are due to the fact that the positions of the predicted spans are not correctly aligned with the positions of the ground truths. Often, we can find this error in the predictions for entity mentions that are very long. For example, consider the following text segment:\n(1) อาคาร Pa:ka:n. building รัฐประศาสนภักดี rátthàpràsà:tsànáphákdi: Ratthaprasatsanaphakdi ชัน้ tChán floor 6 hòk 6\nถนน thànǒn. road แจ้งวัฒนะ tCÊ:N.wáttháná chaengwatthana แขวง khwĚ:N. subdistrict\nทุ่งสองห้อง thûNsǑ:NhÔN thungsonghong เขต khè:t. district หลักสี่ làk.sì: laksi กรุงเทพมหานคร kruNthê:p.máhǎ:.ná.kO:n Bangkok 10210 nẀN.sǔ:n.sǑ:N.nẀN.sǔ:n 10210 ‘Ratthaprasasanabhakdi Building, 6th Floor, Chaeng Watthana Road, Thung Song Hong Subdistrict, Lak Si District, Bangkok 10210’\nThis large text segment is just one entity span for the address class. If a N-NER model yields a predicted span that does not cover the whole text segment, even by just one word, then we consider the prediction as incorrect.\nAmbiguous entity mentions: models may fail to disambiguate entity mentions that can belong to different classes depending on the context. For example, “English” can be tagged with different classes such as Language, National, or Location depending on the context. We use normalized class distribution entropy to quantify the effect of ambiguous entity mentions. We investigate entity mentions that can appear as different classes in the training set and calculate their entropy according to their class distribution in the training set. Then wemeasure the error rates of these mentions in the test set. We split entity mentions into three bins according to their entropy values: [0, 0.33), [0.33, 0.66),[0.66, 1.0]. We found that the average error rates of the three bins are as follows: 22.71%, 34.86%, and 59.20%, respectively. This confirms that ambiguity of entity mentions has a deleterious effect on the N-NER model.\nAmbiguity between fine-grained classes: there are fine-grained classes that have subtle differences in meaning between them and often appear in similar contexts. For example, the government tag refers to governmental organizations such as, government departments, while org:political refers to political organizations, such as political parties and advocacy groups. As mentioned in Appendix A.2, using coarsegrained ground truths to evaluate can reveal the detrimental effect of ambiguity between finegrained classes. There are 1,149 mentions that would be predicted correctly, if we were to use coarse-grained ground truths instead.\nScarcity of training samples: there are some\nclasses that models do not give any prediction because the number of training samples is too low, for example, food:ingredient, vehicle, org:religious, periodic, and station. As a result, all models have a tendency to generate false negatives more than false positives. This is the issue we mentioned in Section 5.3. Moreover, the best Thai N-NERmodel, WangchanBERTa-sh, tends to produce more predictions for the head classes, accounting for 80% of the mention distribution, than body and tail classes."
    } ],
    "references" : [ {
      "title" : "Collocation and thai word segmentation",
      "author" : [ "Wirote Aroonmanakun." ],
      "venue" : "Proc. SNLP and Oriental COCOSDA Workshop, 2002, pages 68–75.",
      "citeRegEx" : "Aroonmanakun.,? 2002",
      "shortCiteRegEx" : "Aroonmanakun.",
      "year" : 2002
    }, {
      "title" : "NoSta-D named entity annotation for German: Guidelines and dataset",
      "author" : [ "Darina Benikova", "Chris Biemann", "Marc Reznicek." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2524–2531, Reyk-",
      "citeRegEx" : "Benikova et al\\.,? 2014",
      "shortCiteRegEx" : "Benikova et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The automatic content extraction (ACE) program – tasks, data, and evaluation",
      "author" : [ "George Doddington", "Alexis Mitchell", "Mark Przybocki", "Lance Ramshaw", "Stephanie Strassel", "Ralph Weischedel." ],
      "venue" : "Proceedings of the Fourth International Conference",
      "citeRegEx" : "Doddington et al\\.,? 2004",
      "shortCiteRegEx" : "Doddington et al\\.",
      "year" : 2004
    }, {
      "title" : "VLSP 2016 shared task: Named entity recognition",
      "author" : [ "Nguyen Thi Minh Huyen", "Vu Xuan Luong." ],
      "venue" : "Proceedings of Vietnamese Speech and Language Processing (VLSP).",
      "citeRegEx" : "Huyen and Luong.,? 2016",
      "shortCiteRegEx" : "Huyen and Luong.",
      "year" : 2016
    }, {
      "title" : "Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl_1):i180–i182",
      "author" : [ "J-D Kim", "Tomoko Ohta", "Yuka Tateisi", "Jun’ichi Tsujii" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2003
    }, {
      "title" : "Sequence-to-nuggets: Nested entity mention detection via anchor-region networks",
      "author" : [ "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5182–5192, Florence,",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Wangchanberta: Pretraining transformer-based thai language models",
      "author" : [ "Lalita Lowphansirikul", "Charin Polpanumas", "Nawat Jantrakulchai", "Sarana Nutanong." ],
      "venue" : "arXiv e-prints, pages arXiv–2101.",
      "citeRegEx" : "Lowphansirikul et al\\.,? 2021",
      "shortCiteRegEx" : "Lowphansirikul et al\\.",
      "year" : 2021
    }, {
      "title" : "Bipartite flat-graph network for nested named entity recognition",
      "author" : [ "Ying Luo", "Hai Zhao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6408– 6418, Online. Association for Computational Lin-",
      "citeRegEx" : "Luo and Zhao.,? 2020",
      "shortCiteRegEx" : "Luo and Zhao.",
      "year" : 2020
    }, {
      "title" : "A feature-based model for nested named-entity recognition at vlsp-2018 ner evaluation campaign",
      "author" : [ "Pham Quang Nhat Minh." ],
      "venue" : "Journal of Computer Science and Cybernetics, 34.",
      "citeRegEx" : "Minh.,? 2018",
      "shortCiteRegEx" : "Minh.",
      "year" : 2018
    }, {
      "title" : "Vlsp shared task: Named entity recognition",
      "author" : [ "Huyen TM Nguyen", "Quyen T Ngo", "Luong X Vu", "Vu M Tran", "Hien TT Nguyen." ],
      "venue" : "Journal of Computer Science and Cybernetics, 34(4):283–294. 9",
      "citeRegEx" : "Nguyen et al\\.,? 2018",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2018
    }, {
      "title" : "DaN+: Danish nested named entities and lexical normalization",
      "author" : [ "Barbara Plank", "Kristian Nørgaard Jensen", "Rob van der Goot." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 6649–6662, Barcelona,",
      "citeRegEx" : "Plank et al\\.,? 2020",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2020
    }, {
      "title" : "NNE: A dataset for nested named entity recognition in English newswire",
      "author" : [ "Nicky Ringland", "Xiang Dai", "Ben Hachey", "Sarvnaz Karimi", "Cecile Paris", "James R. Curran." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Ringland et al\\.,? 2019",
      "shortCiteRegEx" : "Ringland et al\\.",
      "year" : 2019
    }, {
      "title" : "Locate and label: A two-stage identifier for nested named entity recognition",
      "author" : [ "Yongliang Shen", "Xinyin Ma", "Zeqi Tan", "Shuai Zhang", "Wen Wang", "Weiming Lu." ],
      "venue" : "InProceedings of the 59th AnnualMeeting of the Association for Computational Linguistics",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Nested named entity recognition via second-best sequence learning and decoding",
      "author" : [ "Takashi Shibuya", "Eduard Hovy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:605–620.",
      "citeRegEx" : "Shibuya and Hovy.,? 2020",
      "shortCiteRegEx" : "Shibuya and Hovy.",
      "year" : 2020
    }, {
      "title" : "Neural architectures for nested ner through linearization",
      "author" : [ "Jana Straková", "Milan Straka", "Jan Hajic." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5326–5331.",
      "citeRegEx" : "Straková et al\\.,? 2019a",
      "shortCiteRegEx" : "Straková et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural architectures for nested NER through linearization",
      "author" : [ "Jana Straková", "Milan Straka", "Jan Hajic." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5326–5331, Florence, Italy. Association for",
      "citeRegEx" : "Straková et al\\.,? 2019b",
      "shortCiteRegEx" : "Straková et al\\.",
      "year" : 2019
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "ACE 2005 Multilingual Training Corpus LDC2006T06",
      "author" : [ "Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda." ],
      "venue" : "https://catalog. ldc.upenn.edu/LDC2006T06. Linguistic Data Consortium.",
      "citeRegEx" : "Walker et al\\.,? 2006",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2006
    }, {
      "title" : "Pyramid: A layered model for nested named entity recognition",
      "author" : [ "Jue Wang", "Lidan Shou", "Ke Chen", "Gang Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5918–5928, Online. Association",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "HIT: Nested named entity recognition via head-tail pair and token interaction",
      "author" : [ "Yu Wang", "Yun Li", "Hanghang Tong", "Ziye Zhu." ],
      "venue" : "Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pages 6027–",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "LUKE: Deep contextualized entity representations with entityaware self-attention",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Hiroyuki Shindo", "Hideaki Takeda", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Yamada et al\\.,? 2020",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Considerable research attention has been dedicated to formulating a technique to solve the NNER problem (Straková et al., 2019a,b; Lin et al., 2019; Wang et al., 2020a; Luo and Zhao, 2020; Shibuya andHovy, 2020;Wang et al., 2020b).",
      "startOffset" : 104,
      "endOffset" : 230
    }, {
      "referenceID" : 19,
      "context" : "Considerable research attention has been dedicated to formulating a technique to solve the NNER problem (Straková et al., 2019a,b; Lin et al., 2019; Wang et al., 2020a; Luo and Zhao, 2020; Shibuya andHovy, 2020;Wang et al., 2020b).",
      "startOffset" : 104,
      "endOffset" : 230
    }, {
      "referenceID" : 8,
      "context" : "Considerable research attention has been dedicated to formulating a technique to solve the NNER problem (Straková et al., 2019a,b; Lin et al., 2019; Wang et al., 2020a; Luo and Zhao, 2020; Shibuya andHovy, 2020;Wang et al., 2020b).",
      "startOffset" : 104,
      "endOffset" : 230
    }, {
      "referenceID" : 20,
      "context" : "Considerable research attention has been dedicated to formulating a technique to solve the NNER problem (Straková et al., 2019a,b; Lin et al., 2019; Wang et al., 2020a; Luo and Zhao, 2020; Shibuya andHovy, 2020;Wang et al., 2020b).",
      "startOffset" : 104,
      "endOffset" : 230
    }, {
      "referenceID" : 3,
      "context" : "Especially, English, a high resource language, has a few N-NER datasets available for multiple domains (Doddington et al., 2004; Walker et al., 2006; Kim et al., 2003; Ringland et al., 2019) including news, social media, and molecular biology.",
      "startOffset" : 103,
      "endOffset" : 190
    }, {
      "referenceID" : 18,
      "context" : "Especially, English, a high resource language, has a few N-NER datasets available for multiple domains (Doddington et al., 2004; Walker et al., 2006; Kim et al., 2003; Ringland et al., 2019) including news, social media, and molecular biology.",
      "startOffset" : 103,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "Especially, English, a high resource language, has a few N-NER datasets available for multiple domains (Doddington et al., 2004; Walker et al., 2006; Kim et al., 2003; Ringland et al., 2019) including news, social media, and molecular biology.",
      "startOffset" : 103,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "Especially, English, a high resource language, has a few N-NER datasets available for multiple domains (Doddington et al., 2004; Walker et al., 2006; Kim et al., 2003; Ringland et al., 2019) including news, social media, and molecular biology.",
      "startOffset" : 103,
      "endOffset" : 190
    }, {
      "referenceID" : 1,
      "context" : "In German, another high-resource language, there is only one N-NER dataset available (Benikova et al., 2014).",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "2018) are still small compared to a large N-NER dataset in English (Ringland et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "The number of entitymentions and variety of entity classes are comparable to a large N-NER dataset in English (Ringland et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "In terms of the number of classes, it is also worthnoting that three out of six corpora has less than ten and only NNE (Ringland et al., 2019) has more than 100 classes.",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "ACE-2004 (Doddington et al., 2004) and ACE2005 (Walker et al.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : ", 2004) and ACE2005 (Walker et al., 2006) are early examples of N-NER datasets.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : "ACE-2005 (Walker et al., 2006) dataset comprises 30,966 mentions from 12,548 sentences with 7 coarse-grained entity types.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "GENIA (Kim et al., 2003) introduces an N-NER data for bioinformatics.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "NNE (Ringland et al., 2019) is a recent large finegrained N-NER dataset composed of 114 classes.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "NoSta-D (Benikova et al., 2014) is the first and only German N-NER dataset.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "VLSP–2018 (Nguyen et al., 2018) is a standard benchmark for Vietnamese N-NER.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "DAN+ (Plank et al., 2020) presents the first NNER dataset for Danish.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "be/conll2003/ner/ Our dataset is comparable to the NNE corpus (Ringland et al., 2019), which is the most elaborate English N-NER dataset in terms of the",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "Inspired by the guideline from (Ringland et al., 2019), we designed an N-NER annotation guide-",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "For ease of comparison, we apply the best existing Thai language model called WangchanBERTa (Lowphansirikul et al., 2021) to second and third approaches.",
      "startOffset" : 92,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "Classical ML baseline: CRF model (Minh, 2018) We train multiple CRF models, each model is dedicated to each layer.",
      "startOffset" : 33,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "To get these models to work for Thai, we replace their encoders with the same Thai language model as the deep learning baselines (Lowphansirikul et al., 2021).",
      "startOffset" : 129,
      "endOffset" : 158
    }, {
      "referenceID" : 14,
      "context" : "Second-best-learning (Shibuya and Hovy, 2020): This model learns to recursively decode the nested named entities from the outer to the inner nested entities.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "Pyramid (Wang et al., 2020a): This model learns hierarchical representation from multiple nested levels by using pyramid and inverse pyramid mechanisms.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "Locate and Label (Shen et al., 2021): This model divides entity detection into two stages: (i) it locates the entity spans; (ii) it assigns a label to each entity span.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "We follow the evaluation methodology from (Shibuya and Hovy, 2020), they consider a prediction as a true positive if both the predicted entity",
      "startOffset" : 42,
      "endOffset" : 66
    } ],
    "year" : 0,
    "abstractText" : "This paper presents the first Thai Nested Named Entity Recognition (N-NER) dataset. Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews. Our work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first non-English one with fine-grained classes. To understand the new challenges our proposed dataset brings to the field, we conduct an experimental study on (i) cutting edge N-NER models with the state-of-the-art accuracy in English and (ii) baseline methods based on wellknown language model architectures. From the experimental results, we obtained two key findings. First, all models produced poor F1 scores in the tail region of the class distribution. There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset. These findings suggest that further investigation is required to make a multilingual N-NER solution that works well across different languages.",
    "creator" : null
  }
}
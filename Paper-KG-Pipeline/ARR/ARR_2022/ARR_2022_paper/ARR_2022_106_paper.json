{
  "name" : "ARR_2022_106_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Conversational Agents against Imperceptible Toxicity Triggers",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Adversarial attacks on different Machine Learning (ML) and Natural Language Processing (NLP) applications can reveal important vulnerability issues related to these systems. Most existing research focuses on adversarial attacks that degrade performance of existing ML systems with regards to accuracy (Chakraborty et al., 2018; Zhang et al.,\n2020b). More recent work has considered attacks that target ethical concerns, such as triggering the models into outputting unfair predictions (Mehrabi et al., 2021; Solans et al., 2021), or in the context of NLP, generating biased (Sheng et al., 2020) and toxic (Wallace et al., 2019) text.\nIn this paper, we consider adversarial attacks on human-centric chatbots and dialog systems. It is important for these systems to be safe and robust in the face of natural(-looking) human conversations. Further, the defender should ensure a satisfying user experience via relevant and coherent generation. An instance of the attack and defense is demonstrated in Figure 1 in which the adversary tries to trigger the defender while the defender avoids the attack by not generating toxic utterances.\nThe existing work on adversarial attacks on language generation is relatively thin. Wallace et al. (2019) offer attacks based on universal adversarial triggers (UAT) that can result in toxic text generation with a relatively high success rate. However, those triggers are unnatural, incoherent sequences of words that can be easily detected via a language model loss. Furthermore, such attacks cannot be successful in voice-based dialog systems where the\ninput to the dialog model comes from speech recognition and should necessarily conform to human language norms. Xu et al. (2020) use human-andmodel-in-the-loop framework to generate naturallooking attacks to break chatbots, but this approach is costly and inherently not scalable.\nIn this paper, we propose imperceptible adversarial attacks on dialogue systems that leverage natural-looking and coherent utterances as triggers, which cannot be easily detected using anomaly detection techniques. As such, these attacks can also target voice-based assistants who see the world through the lens of speech recognition systems. Our proposed approach works by augmenting the UAT from Wallace et al. (2019) with additional selection criteria to generate imperceptible yet effective triggers. The method is fully automated and scalable, thus affording the exploration of a large number of attack vectors and system vulnerabilities efficiently. Through human and automatic evaluations we show the effectiveness of the proposed attack in provoking the defender into generating toxic responses while keeping the fluency and coherency of the conversation intact.\nWe then focus on a defense mechanism for the non-adversarial (defender) model to avoid generating toxic utterances. While simple defense methods such as (Xu et al., 2020) achieve near-perfect effectiveness against adversarial triggers, those methods work by essentially resetting the conversation topic which breaks the flow. Instead, we are interested in a defense mechanism that “detoxifies\" the response while preserving the natural conversation flow. Our proposed method relies on two levels of interpretable reasoning that helps the model to (1) identify the key adversarial tokens responsible for the attack and (2) avoid generating toxic response by masking those tokens during the generation process. We perform automatic and human evaluations to assess the effectiveness of our defense mechanism and demonstrate that it compares favorably with various state of the art baselines, both in terms of detecting the attacks and generating conversationally fluent responses. We finally demonstrate the generalizability of such a defense mechanism on generation tasks beyond conversational models.\nWe emphasize that while our problem formulation focuses on the adversarial scenario, the imperceptible and coherent-looking triggers used in our proposed attacks can also be invoked inadvertently by regular (non-adversarial) users. Thus, the de-\nfense mechanism proposed against such triggers will improve the overall robustness of conversational agents, not only against adversaries but also in interactions with regular users."
    }, {
      "heading" : "2 Attack Approaches",
      "text" : "In this section, we first discuss the universal adversarial trigger attack proposed by Wallace et al. (2019), which we use as our baseline. We then propose alterations to this baseline to make the universal triggers more natural-looking and suitable for conversational domain. Finally, we discuss our performed experiments and results.\n2.1 Methodology Universal Adversarial Trigger (UAT) (Wallace et al., 2019) The goal in universal adversarial trigger attack is to find a universal trigger sequence for a given trained model, which if attached to the start of any given input can cause the model to output the desired outcome (Wallace et al., 2019). This attack starts with a fixed-length sequence as the initial trigger, e.g., “the the the the the the” and tries to iteratively replace the tokens in the sequence to satisfy an objective. The iterations terminate when no improvement (replacement) can be made to further optimize the objective. The objective in this generative process is to search for triggers that can maximize the likelihood of toxic tokens being generated as follows:\nfUAT = ∑ y∈Y |y|∑ i=1 logP (yi|y1:i−1;t,θ).\nwhere Y is the set of toxic outputs, t denotes the trigger sequence, and θ is a trained language model. One important drawback of this kind of attack is that since there is no constraint on the trigger, it does not necessarily satisfy any language modeling loss; thus, the obtained trigger sequence usually is a nonsensical phrase that can be easily detectable as a (high-perplexity) anomaly. Universal Adversarial Trigger with Language Model Loss (UAT-LM) An intuitive solution to address the above shortcoming of UAT is to impose a language modeling objective on the trigger tokens. Thus, the objective for UAT-LM attack is\nfUAT-LM = fUAT + ∑ y∈Y |t|∑ j=1 logP (tj |t1:j−1, θ).\nNote that this optimization does not guarantee generation of sufficiently fluent triggers. Even if the\ngenerated triggers by themselves might be sensible, they will not generally retain the flow of the conversation in terms of coherency and relevancy. Thus, we propose a different modification to the attack strategy to accommodate these requirements.\nUnigram Trigger with Selection Criteria (UTSC) To consider the history of the conversation h and retain the fluency, coherency, and relevancy aspects of the conversation in generating the attack, we propose an alternative approach in which we generate a collection of unigram triggers (with sequence length one) from UAT. We then feed these triggers along with the history of the conversation h to our dialogue model and generate different attack utterances. Next, we pick the best suited attack utterance amongst all the generated attack utterances according to our selection criterion as demonstrated in Figure 2. Since we are relying on the dialogue model to generate the attack utterance given h, the generated utterance will be relevant to the conversation, Furthermore, since we are using only a unigram trigger from UAT, the fluency of the utterance is not going to be sacrificed noticeably.\nWe quantify the toxicity of each candidate attack utterance using either a single toxicity classifier or an ensemble of such classifiers; see Section 2.2 and Appendix A for more information. We use the average (for multiple classifiers) or raw (for a single classifier) output probability scores obtained by the toxicity classifiers, which we refer to as the toxicity score xi for example i, and select the final attack utterance amongst the n candidate adversarial examples considering three selection criteria. Previous work (Xu et al., 2020) has shown that toxic triggers are more likely to provoke toxic responses. Thus, in UTSC-1, we select the most toxic utterance among all generated attack utterances according to toxicity scores from toxicity classifiers as our final attack utterance (i.e., argmaxi∈[n]{xi}). We experiment with two additional criteria. For UTSC-2, we first apply a threshold T to toxicity scores of the candidate utterances and label the utterances above this threshold as toxic. Next, from the pool of all toxic utterances, we select the utterance with the lowest toxicity score (i.e., argmini∈[n]{xi|xi ≥ T}). If no utterances fall above the threshold, then the most toxic utterance is selected. Lastly, in UTSC3 we select the utterance with the lowest toxicity score, i.e., argmini∈[n]{xi}. Details are provided in Appendix A."
    }, {
      "heading" : "2.2 Experimental Setup",
      "text" : "General Setup We use DialoGPT (Zhang et al., 2020c) to generate 100 conversations around a specific topic. The topic is determined by the context sentence that starts the conversation between the adversary and the defender. Each conversation runs for 10 turns. To measure the effectiveness of the attack and defense mechanisms given the conversation history as well preservation of relevancy and coherency, the adversary generates the attack utterance on the third turn of each conversation.\nToxicity Detection Models To determine toxicity of the candidate attack utterances by the adversary, we utilize an ensemble of three different toxicity detection models: Toxic-bert1, Perspective API2, and Safety classifier (Xu et al., 2020). In short, Toxicbert is the least sensitive of the three, followed by Perspective API, and the Safety classifier (details in Appendix A). While using an ensemble of the three models results in the most effective attacks, to ensure that the adversary is not simply overfitting the toxicity detection model but rather forcing the defender to actually generate toxic language, we also study the transferability of these attacks. We allow the adversary to only use one of the toxicity detection models to design its attack. We then quantify toxicity using the other two toxicity detection methods, not accessed by the adversary.\nData The context sentences around which bots start their conversations come from two different datasets, Wizard of Wikipedia (Dinan et al., 2018) and ConvoKit’s Reddit Corpus.3 We intend to consider both controversial and neutral topics; thus, we consider two different datasets in which the Wizard\n1https://github.com/unitaryai/detoxify 2https://www.perspectiveapi.com 3https://convokit.cornell.edu\nof Wikipedia dataset contains context around neutral topics and the Reddit corpus contains context around more sensitive topics. We picked 50 random context sentences from the Wizard of Wikipedia and 50 from the Reddit datasets.\nAMT Experiments To compare and verify the quality of conversations generated during and after the attacks, we conduct human experiments in which we ask workers on Amazon’s Mechanical Turk (AMT) to rate the conversations in which UAT, UAT-LM, and UTSC-1 attacks had happened. We pick UTSC-1 attack as it is the strongest attack amongst our proposed UTSC attacks. We then asked AMT workers to rate the generated attack’s fluency, relevancy of the attack to previous utterances, and overall conversation coherency on a likert scale of 1 to 3 representing poor, moderate, and good qualities respectively. We also asked AMT workers to rate if the utterance after the attack is toxic or not to verify the effectiveness of the attack according to human judgment. During this study, AMT workers annotated 100 conversations from each of the three attacks and each conversation was annotated by 3 AMT workers giving us overall 900 annotated conversations 300 from each attack. More details about this study along with the survey can be found in Appendix A."
    }, {
      "heading" : "2.3 Results",
      "text" : "We first discuss the results from our automatic evaluations demonstrating the efficacy of each attack. We then discuss how well the attacks transfer to other toxicity detection classifiers. Finally, we present results from our human evaluation study. Unless otherwise mentioned, for the UTSC attacks, the adevrsary uses an equally weighted ensemble of all three toxicity detection classifiers to chose the final attack utterance.\nAttack Effectiveness Here we report the “attack effectiveness” by calculating the percentage of con-\nversations in which the defender was provoked by the adversary to generate a toxic response. We first demonstrate the results comparing the UAT baseline with UAT-LM and UTSC attacks. Results in Figure 3 demonstrate that two of our proposed attacks UAT-LM and UTSC-1 are performing the best according to the Perspective API and Toxicbert classifiers. UAT baseline performs the best according to Safety classifier. Overall results show that UTSC-1 and UAT-LM attacks are competitive attacks in terms of attack effectiveness. In addition, UTSC-1 and UAT-LM attacks have the advantage of being more fluent which makes the attack more imperceptible. UAT attack tends to generate meaningless phrases, e.g., “acist neighborhoodsJohnson carry morals Ukrain” which can easily be detected as an anomaly and make the conversation not flow naturally. In our experiments, we observe that the average perplexity score according the the GPT-2 language model for the attack phrases generated by UAT is absurdly high (∼107) compared to ∼104 for UAT-LM, and ∼160 for UTSC-1. The perplexity for UTSC-1 is comparable to the no attack case ∼39. This automatically confirms that our attacks are more fluent and natural, and thus more imperceptible. This observation is further confirmed by our human evaluations which we discuss later.\nImposing the language model constraint on UAT not only makes UAT-LM attack more fluent, but it also causes UAT-LM to generate more toxic triggers which results in more attack effectiveness. Our results confirm the previous results (Xu et al., 2020) in which authors show in a human adversary case that more toxic attacks perform better in forcing the model to generate toxic utterances. In our results, we also show that UTSC-3 performs the worst which is based on non-toxic utterances followed by the UTSC-2 attack which is based on the least toxic utterance attack constraint. However, UTSC-1 is the strongest as it relies on most toxic utterances followed by UAT-LM. Thus, results confirm that the toxicity of the attack plays a significant role in attack effectiveness.\nIn addition, we found that the adversary is able to force the defender into generating toxic utterances regardless of the context sentence and whether or not the conversation is around a sensitive topic (e.g., the Reddit corpus) or a more neutral one (e.g., the Wizard of Wikipedia). Details are in Appendix B.1.\nAttack Transferability Here, we discuss the transferability of our UTSC-1 attack toward different\ntoxicity detection classifiers. In Figure 4, we demonstrate that even if the attacker only uses one of the toxicity detection models (Toxic-bert), it still can force the defender to generate toxic responses according to Perspective API and Safety classifier and have comparable performance to when it uses all the toxicity classifiers. This confirms that the attack is forcing the defender to generate actual toxic language rather than fooling the toxicity classifier. The results for UTSC-1 using other toxicity detection models can be found in Appendix B.1.\nHuman Evaluation Results from our human evaluation studies are in Figure 5. Our UTSC-1 attack is rated to have the highest coherency. UTSC1 is rated to have more fluent attacks generated with mostly moderate to good scores and a higher average–shown by the black dotted lines–compared to the UAT and UAT-LM baselines. UTSC-1 also has better relevancy scores in terms of the attack being more relevant to the conversation. However, since UAT generates meaningless phrases, it is rated very poorly for all the mentioned qualities. With regards to toxicity scores, attacks are rated to have competitive and comparable performances at around 20% effectiveness close to automatic results from Perspective API classifier. Fleiss Kappa (Fleiss, 1971) annotator agreement results from this evaluation is reported in Table 1. Annotators have reasonable overall agreement for all the qualities."
    }, {
      "heading" : "3 Defense Approaches",
      "text" : "The defense against adversarial attacks has two components (a) detecting the attack and (b) mitigating its effect by ensuring that the defender does not generate a toxic response. The detection problem is rather straightforward, as the defense can simply run a toxicity classifier on the gener-\nated response. The mitigation is more challenging. Xu et al. (2020) suggested a mitigating approach which, when a toxic response is detected, simply resets the dialogue and generates a (non-toxic) utterance by randomly sampling from a predefined set of topics (see Section 3.2.1). As we mentioned before, we are interested in mitigation strategies that avoid generating toxic utterances but at the same time manage to keep the conversation flow intact. We now discuss our approach in more details."
    }, {
      "heading" : "3.1 Methodology",
      "text" : "Our defense is based on a two-stage mechanism in which the defender first runs a toxicity detection model on its generated utterance. If it finds that the generated utterance is toxic, it then proceeds with the second stage of the defense. The proposed defense mechanism in the second stage utilizes two layers of reasoning using two different interpretability techniques. The first layer aims to detect which tokens in the defender’s utterance is making the toxicity detection model to label the utterance as being toxic. We call these tokens the L1 tokens. The second layer aims to detect which tokens in the adversary’s attack utterance are responsible for generation of L1 tokens form defender’s utterance. We call these tokens identified in layer 2 as the L2 tokens. The defender then masks the L2 tokens from the adversary, which were responsible for triggering the defender model to generate toxic tokens, and generates a new utterance. We then apply a tox-\nicity classifier on this new utterance. If it is deemed safe, it is then going to replace the defender’s old toxic utterance, otherwise we iteratively apply the two-stage defense mechanism to mask more input tokens until the generated output is deemed safe. As we shall see, a single iteration of our defense is sufficient in most of the experiments.\nThe defense framework is demonstrated in Figure 6. For the first layer, we use transformers interpret4 which provides explanations and identifies the L1 token according to Toxic-bert model. For the second layer, we use LERG (Tuan et al., 2021) that provides local explanations for dialogue response generation and identifies the L2 token (given the L1 token in the response utterance it identifies the L2 token in the query utterance)."
    }, {
      "heading" : "3.2 Experimental Setup",
      "text" : "We use the aforementioned attacks, and apply our defense against them. This follows the same experimental setup, with the addition of baseline defenses to compare our defense effectiveness against."
    }, {
      "heading" : "3.2.1 Baselines",
      "text" : "Two-stage Non Sequitur Baseline (Xu et al., 2020) This baseline is also a two-stage approach like ours in which the defender first uses a toxicity classifier to detect if the utterance is toxic or not. It then changes the topic of the conversation if the utterance was detected to be toxic, e.g., “Hey do you want to talk about something else? How about we talk about X?” where X is a randomly chosen topic from 1087 topics judged as safe from the Wizard of Wikipedia conversational topic list (Dinan et al., 2018). Xu et al. (2020) used this defense against ad-\n4https://github.com/cdpierse/transformers-interpret\nversarial attacks performed by human adversaries that force the model to generate toxic responses.\nNotice that although this defense is using a templated sentence to change the topic into a non-toxic topic and can be considered as the perfect solution to avoid generating toxic responses, it can provide the user with a non-plausible conversational experience given that the topic of the conversation changes each time the defender detects a toxic utterance. To this end, we expect this baseline to do almost perfectly in terms of avoiding toxic response generation given that the toxicity detection classifier is a good detector; however, in terms of conversational quality it will have worse relevancy and coherency scores compared to our method as shown in our human evaluations.\nTrigger Masking (TM) Baseline In this baseline, we consider masking the adversarial trigger tokens. Note that the defender does not generally know which tokens were the trigger-tokens used by the adversary, so this approach is not applicable in realistic settings. However, we believe that considering this type of oracle baseline can still give us interesting insights, so we include it in our experiments."
    }, {
      "heading" : "3.2.2 AMT Experiments",
      "text" : "We asked AMT workers to evaluate the defense quality according to relevancy and fluency, the coherency of the overall conversation, and the toxicity of the defense utterance. 27 conversations were rated from each of the three defenses (TM, Twostage Non Sequitur, and our proposed defense) 3 AMT workers rated each conversation which gave us 243 annotations 81 from each defense. More details can be found in Appendix A."
    }, {
      "heading" : "3.3 Results",
      "text" : "Defense Effectiveness We report “defense effectiveness” as the percent decrease in a defender generating a toxic response after adversary’s attack when the defense is applied compared to when it isn’t. From our results, we observe that both our proposed defense mechanism as well as the Non Sequitur baseline achieve 100% defense effectiveness according to Toxic-bert classifier. We also noticed that for our proposed method for all the attacks except UAT-LM, we were able to reach 100% defense effectiveness by only masking one token. For UAT-LM, almost 90% of cases were resolved by masking one token and the rest were resolved by the iterative approach that masked multiple tokens (up to 3). In addition, our defense is also outperforming the oracle Trigger Masking which shows that using model interpretability can give us more valuable insights than blindly masking out the triggers. In some cases tokens generated after the trigger can themselves be more toxic and decisive in forcing the defender into generating toxic utterances (more details in Appendix B.1 Table 4.). As expected, the Non Sequitur defense is always effective as it replaces the toxic utterance with a non-toxic utterance by changing the topic; however, this approach is not necessarily creating the best conversational experience as also verified by our human experiments in terms of maintaining relevancy and coherency of the conversation.\nDefense Transferability We analyze transferability of our defense mechanism with regards to three different aspects as follows:\n1. Transferability to other toxicity detection classifiers: Results in Figure 7 demonstrate that even if the defender is using the interpretability results provided by the Toxic-bert classifier, it can still be effective in reducing toxicity according to Perspective API and Safety classifier on all attacks.\n2. Transferability when UTSC attack uses different toxicity classifier than what the defender uses in its defense: We also noticed that even if the defender and the attacker do not use the same toxicity detectors the defense can be effective. To see the results of our defense on all the combination of toxicity detectors used by the attacker for its selection criteria refer to Appendix B.1.\n3. Transferability of the defense to human generated attacks: Lastly, to make sure that our defense also transfers to human generated attacks and not just automatic attacks, we tried to generate attacks against the DialoGPT model and converse with it as the adversary. We managed to trigger the system for 10% of the cases, in line with the automatic attacks. We also saw 70% reduction in toxic generation when we applied only one iteration of our defense mechanism on these attacks. Human Evaluation Results of our human evaluations are demonstrated in Figure 8. Our defense is rated to have the highest fluency and relevancy scores. While our defense is mostly rated to have moderate to good ratings for relevancy, the Non Sequitur defense has poor relevancy scores. This is because the Non Sequitur defense changes the topic every-time a toxic utterance is generated which lowers the quality of the conversational experience. Thus, even if the Non Sequitur defense can be really effective in reducing the toxicity as it replaces the toxic utterance with a non-toxic templated sentence, it can create poor conversational experience as also rated by human annotators. Human annotator agreements were also reasonable for these tasks (Table 2) according to Fleiss Kappa scores."
    }, {
      "heading" : "4 Beyond Conversational Agents",
      "text" : "We show the generalizability of our defense method against non-conversational generation tasks, by conducting experiments with RealToxicityPrompts dataset (Gehman et al., 2020). Previous work showed that the prompts in RealToxicityPrompts can force different generative models such as GPT2 (Radford et al., 2019) to generate toxic responses. Thus, we used our defense to test whether it can also be effective in reducing the number of toxic responses given these prompts in RealToxicityPrompts in the GPT-2 model. As evident from the previous discussions, the Non Sequitur baseline defense (Xu et al., 2020) that we considered in our paper, only works for the conversational domain; however, our method has the advantage of working on any conditional generation task. We used the\n100k prompts in RealToxicityPrompts and reported the number of toxic generations before and after applying our defense from the GPT-2 model.\nResults in Figure 9 demonstrate that one iteration of our defense reduces the number of generated toxic responses by 81%, 31%, and 23%, according to Toxic-bert, Perspective API, and Safety classifier, respectively. Although the defense is based on Toxic-bert, the results still transfer to Perspective API and Safety classifier. These results show the effectiveness of our defense in reducing toxic generations beyond conversational domain and a step toward reducing toxic generation. Notice that the setup of this experiment was not adversarial; however, prompts were causing the toxic generations."
    }, {
      "heading" : "5 Related Work",
      "text" : "Crafting adversarial examples and using them in training was previously shown to be an effective technique in improving NLP and ML models (Nie et al., 2020; Dinan et al., 2019; Kiela et al., 2021). Not only that, but adversarial attacks can reveal important vulnerabilities in our systems (Zhang et al., 2020a). Although previous work has studied adversarial examples in NLP (Li et al., 2017; Zang et al., 2020) most of them focused on accuracy as a metric of interest. Among the ones that studied\ntoxicity and other ethical considerations (Wallace et al., 2019; Sheng et al., 2020) they did not put the focus on either conversational agents or they did not consider attacks being imperceptible. Cheng et al. (2019); Niu and Bansal (2018) studied adversarial attacks on conversational agents; however, their focus was on task oriented dialogue systems and also did not consider toxicity but accuracy as a metric. Xu et al. (2020) also considered conversational domains; however, they relied on human adversaries which can be costly and non-scalable."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We studied the possibility of generating imperceptible attacks against conversational agents that, while fluent and coherent, target the model into generating toxic responses. Through various automatic and human experiments, we showed the effectiveness of our attacks both in terms of being adversarial as well as being able to maintain coherency, relevancy, and fluency of the generated conversation (what we referred as the attack being imperceptible). We then proposed a defense mechanism that was shown to be effective through various automatic and human evaluations as well as its transferability to human attacks, general generations tasks, and different toxicity classifiers. Future work can focus on improving our proposed attacks both in terms of imperceptibility and effectiveness as well as more advanced defense mechanisms.\nBroader Impact\nIn this work, we proposed possible attacks and defenses against conversational models that can help improve robustness of conversational agents. We also discussed the extension of our defense work on any general generation task that can be an important contribution towards mitigating toxic generations from our models. By proposing effective imperceptible automatic attacks, we also eliminate the need for human labor, reduce the cost, and make this process more scalable.\nPrevious work has shown the importance of adversarially crafted examples into improving NLP systems (Nie et al., 2020; Dinan et al., 2019; Kiela et al., 2021); thus, our automatically generated examples can be useful in not only improving robustness of these systems and highlighting their vulnerabilities, but also a step towards their improvement. Not to mention our defense mechanism that can directly mitigate the discussed issues.\nHowever, we also acknowledge the negative impacts that our work can have if used irresponsibly. We acknowledge that our attack can be used by unethical adversaries to force the models to generate toxic responses which is undesirable as also previously observed in chatbots (Wolf et al., 2017; Henderson et al., 2018; Dinan et al., 2021).\nSince our defense mechanism relies on model interpretability, some of the models may be blackbox or not-interpretable. In that case, we show that the defender still can use proxy models which are interpretable and as shown in the results of our experiments the defense can still be transferable. However, we acknowledge that in such cases the defense might not be as effective, which can be considered a limitation of our work.\nIn our studies, we also incorporated human annotators to annotate the quality of our generated conversations. We made sure to provide the annotators with appropriate and sufficient instructions to complete the work along with a reasonable and acceptable compensation for their labor. We also made the annotators aware of possible toxic or inappropriate language in our generations ahead of time.\nWe hope that our study can be used for the benefit of the society and development of robust conversational systems along with reduced toxic generations in our models. We will release our code and data in a public Github repository for the community to be able to use and reproduce our results."
    }, {
      "heading" : "A Details of Experimental Setup",
      "text" : "General Setup For the UAT and UAT-LM attacks, we used a 6-gram trigger to include in our baselines. For “attack effectiveness”, we report the percentage of conversations out of 100 conversations that we study in this paper in which the defender was fooled by the adversary after the performed attack into generating a toxic response. This is done over one round of analysis. For “defense effectiveness”, we report the percent decrease in a defender generating a toxic response after adversary’s attack when the defense is applied compared to when it isn’t on the same set of 100 conversations that we used throughout the paper for different analysis. Similar to attack effectiveness, this is done over one round of analysis.\nComputational Resources We used Nvidia GeForce RTX 2080 to perform all our experiments except the experiment using the GPT-2 model which was ran on CPU for memory constraints.\nModel details We used the small DialoGPT model from Hugging face5. The GPT-26 and Unitary Toxic-bert7 models were also adapted from Hugging face. Safety classifier was adapted from ParlAI8. We used all the default thresholds set by the developers for all these toxicity detection classifiers and a threshold value of 0.5 for the Perspective API to detect whether an utterance is toxic or not. We use the same threshold values both in our attacks for the selection criteria and in defenses to determine if a generated utterance is toxic or not. Comparing the performance of three classifiers along with analyzing qualitative results, we realized that Toxic-bert is the least sensitive amongst the three classifiers, followed by Perspective API that has the closest agreement to humans, and Safety classifier.\nMechanical Turk Mechanical turk experiments were performed on Amazon’s MTurk platform9. We tested the experiment carefully on the sandbox platform before releasing it live. The turkers were\n5https://huggingface.co/microsoft/DialoGPT-small 6https://huggingface.co/gpt2 7https://huggingface.co/unitary/toxic-bert 8https://parl.ai/projects/safety_recipes/ 9https://www.mturk.com\nchosen from the master workers pool with additional qualifications set (e.g., HIT approval rate above 85, number of approved HITs above 1000) to make sure workers are reliable workers. We left a comment section to make sure we hear the workers’ concerns about the task and the pay. We received couple of comments about the task being interesting with no complains on the pay. We made sure to give reasonable and on time compensation for the amount of work the workers put into and made sure to hear their comments about the pay. We paid 0.30 for each HIT to be completed. Detailed survey instruction forms of our attack and defense are included in Figures 13 and 14. Selection Criteria Details in UTSC Attack For selection criteria, we used the average toxicity scores from three different classifiers (Perspective API, Toxic-bert, and Safety classifier) unless otherwise stated in which we either used the score from one toxicity classifier or the average score from two classifiers. To determine whether an utterance is toxic or not, we used the default thresholds set by the developers for Toxic-bert and Safety classifiers and a threshold value of 0.5 for Perspective API. In addition to toxicity scores, we considered other selection criteria, such as length of the generated attack; however, we saw no significant signal in using the length. Thus, we focused on using toxicity scores in the main text which as shown in the results play a significant role in attack effectiveness. Notice that other selection criteria can be considered along with length and toxicity scores, such as perplexity score for fluency or other metrics; however, for this study, we considered these two cases. In our experiments the adversary generates 10 candidate attack utterances for each of its attacks and the final attack utterance is selected based on the selection criteria out of those 10 generated candidates. Additionally, we report some statistics about toxicity scores of the adversary on the attack utterance as well as defender’s toxicity score after the attack for UTSC-1, UTSC-2, and UTSC-3 attacks which can provide additional intuition on how toxic each attack is. These results are on the 100 conversations that are used in our experiments and are reported in Table 3."
    }, {
      "heading" : "B Additional Results",
      "text" : "B.1 Additional Quantitative Results\nData Sensitivity In Figure 15, we demonstrate what proportion of the attack effectiveness comes\nfrom which of the two Wizard of Wikipedia and Reddit datasets. As also mentioned in the main text, Reddit dataset contains context topics around more sensitive issues, while the Wizard of Wikipedia data is more neutral. We show in our results that the topic context does not play a major role in our attacks being effective and indeed our attack can work as well or even better for the Wizard of Wikipedia dataset that contains more neutral context topics.\nAttack Transferability In Figure 11, we demonstrate that no matter what toxicity detection classifier the attacker uses to chose its attack utterance, the attack can still transfer to other toxicity detection classifiers. For instance, if the attacker only uses Perspective API to perform its attack, results show that the attack is still successful according to Toxic-bert and Safety classifiers in addition to Perspective API. Results for different combinations is shown in Figure 11.\nDefense Transferability In Figures 10 and 12, we show two different types of defense transferability. In Figure 10, we show that the defender and the attacker do not need to use the same toxicity detection classifiers for the defense to be effective. We show that for instance, if the attacker is only using Perspective API to perform its attack and the defender is using Toxic-bert to perform the defense the defense is still effective for 100% of the times. We demonstrate different combinations of classifiers used by the attacker against a defender that uses Toxic-bert to perform the defenese. In all the cases, we show that the defense is effective 100% of the times for our defense mechanism.\nIn Figure 12, we show that the defense trans-\nfers to other toxicity detection classifiers as well not only Toxic-bert for all the different combinations of the attacker toxicity detection classifiers. Thus, results show that even if the defender is using Toxic-bert to perform the defense, according to both Perspective API and Safety classifiers the amount of toxicity is still decreased after the attack irrespective of what toxicity classifier the attacker is using. Of course, the defense is the most effective for Toxic-bert classifier; however, it is interesting that the attack also transfers to other classifiers.\nB.2 Additional Qualitative Results Finally, we show some qualitative results from our attacks and defenses in Figure 16. We show results from our automatic attack strategy as well as our defense mechanism on it (Figure 16 (a)) along with our human experimental results in which a human adversary tries to fool the system into generating toxic utterances (Figure 16 (b)) and lastly the GPT2 experiments using the RealToxicityPromts and how effective our proposed defense mechanism works on these sets of prompts and model (Figure 16 (c-f)).\nNotice that our human performed attacks did not consider any contexts since the human adversary was defining the context and starting the conversation with the context in mind all in one shot. This is slightly different than our automatically performed attack setup in which we always start the conversations given a context topic to force the bots to converse around the given topic and not just a random topic. The rest of the experimental setup, however, is similar to the automatic attack/defense setup."
    } ],
    "references" : [ {
      "title" : "Adversarial attacks and defences: A survey",
      "author" : [ "Anirban Chakraborty", "Manaar Alam", "Vishal Dey", "Anupam Chattopadhyay", "Debdeep Mukhopadhyay." ],
      "venue" : "arXiv preprint arXiv:1810.00069.",
      "citeRegEx" : "Chakraborty et al\\.,? 2018",
      "shortCiteRegEx" : "Chakraborty et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating and enhancing the robustness of dialogue systems: A case study on a negotiation agent",
      "author" : [ "Minhao Cheng", "Wei Wei", "Cho-Jui Hsieh." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Anticipating safety issues in e2e conversational ai: Framework and tooling",
      "author" : [ "Emily Dinan", "Gavin Abercrombie", "A Stevie Bergman", "Shannon Spruit", "Dirk Hovy", "Y-Lan Boureau", "Verena Rieser." ],
      "venue" : "arXiv preprint arXiv:2107.03451.",
      "citeRegEx" : "Dinan et al\\.,? 2021",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2021
    }, {
      "title" : "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
      "author" : [ "Emily Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dinan et al\\.,? 2018",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2018
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin, 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "author" : [ "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A Smith." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Gehman et al\\.,? 2020",
      "shortCiteRegEx" : "Gehman et al\\.",
      "year" : 2020
    }, {
      "title" : "Ethical challenges in data-driven dialogue systems",
      "author" : [ "Peter Henderson", "Koustuv Sinha", "Nicolas AngelardGontier", "Nan Rosemary Ke", "Genevieve Fried", "Ryan Lowe", "Joelle Pineau." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and",
      "citeRegEx" : "Henderson et al\\.,? 2018",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Dynabench: Rethinking benchmarking in NLP",
      "author" : [ "Bansal", "Christopher Potts", "Adina Williams." ],
      "venue" : "Proceedings of the 2021 Conference of the North 9",
      "citeRegEx" : "Bansal et al\\.,? 2021",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2021
    }, {
      "title" : "Robust training under linguistic adversity",
      "author" : [ "Yitong Li", "Trevor Cohn", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 21–27, Va-",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Exacerbating algorithmic bias through fairness attacks",
      "author" : [ "Ninareh Mehrabi", "Muhammad Naveed", "Fred Morstatter", "Aram Galstyan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 35(10):8930–8938.",
      "citeRegEx" : "Mehrabi et al\\.,? 2021",
      "shortCiteRegEx" : "Mehrabi et al\\.",
      "year" : 2021
    }, {
      "title" : "Adversarial nli: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial oversensitivity and over-stability strategies for dialogue models",
      "author" : [ "Tong Niu", "Mohit Bansal." ],
      "venue" : "Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 486–496, Brussels, Belgium. Association for Com-",
      "citeRegEx" : "Niu and Bansal.,? 2018",
      "shortCiteRegEx" : "Niu and Bansal.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards Controllable Biases in Language Generation",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Prem Natarajan", "Nanyun Peng." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239–3254, Online. Association for Computational",
      "citeRegEx" : "Sheng et al\\.,? 2020",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Poisoning attacks on algorithmic fairness",
      "author" : [ "D Solans", "B Biggio", "C Castillo." ],
      "venue" : "European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD 2020, volume 12457, pages 162–177. Springer Science and Business Me-",
      "citeRegEx" : "Solans et al\\.,? 2021",
      "shortCiteRegEx" : "Solans et al\\.",
      "year" : 2021
    }, {
      "title" : "Local explanation of dialogue response generation",
      "author" : [ "Yi-Lin Tuan", "Connor Pryor", "Wenhu Chen", "Lise Getoor", "William Yang Wang." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Tuan et al\\.,? 2021",
      "shortCiteRegEx" : "Tuan et al\\.",
      "year" : 2021
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing NLP",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Why we should have seen that coming: Comments on microsoft’s tay “experiment,” and wider implications",
      "author" : [ "M.J. Wolf", "K.W. Miller", "F.S. Grodzinsky." ],
      "venue" : "The ORBIT Journal, 1(2):1–12.",
      "citeRegEx" : "Wolf et al\\.,? 2017",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2017
    }, {
      "title" : "Recipes for safety in open-domain chatbots",
      "author" : [ "Jing Xu", "Da Ju", "Margaret Li", "Y-Lan Boureau", "Jason Weston", "Emily Dinan." ],
      "venue" : "arXiv preprint arXiv:2010.07079.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Word-level textual adversarial attacking as combinatorial optimization",
      "author" : [ "Yuan Zang", "Fanchao Qi", "Chenghao Yang", "Zhiyuan Liu", "Meng Zhang", "Qun Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Zang et al\\.,? 2020",
      "shortCiteRegEx" : "Zang et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing neural models with vulnerability via adversarial attack",
      "author" : [ "Rong Zhang", "Qifei Zhou", "Bo An", "Weiping Li", "Tong Mo", "Bo Wu." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 1133–1146, Barcelona,",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial attacks on deeplearning models in natural language processing: A survey",
      "author" : [ "Wei Emma Zhang", "Quan Z Sheng", "Ahoud Alhazmi", "Chenliang Li." ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST), 11(3):1–41.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "More recent work has considered attacks that target ethical concerns, such as triggering the models into outputting unfair predictions (Mehrabi et al., 2021; Solans et al., 2021), or in the context of NLP, generating biased (Sheng et al.",
      "startOffset" : 135,
      "endOffset" : 178
    }, {
      "referenceID" : 15,
      "context" : "More recent work has considered attacks that target ethical concerns, such as triggering the models into outputting unfair predictions (Mehrabi et al., 2021; Solans et al., 2021), or in the context of NLP, generating biased (Sheng et al.",
      "startOffset" : 135,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : ", 2021), or in the context of NLP, generating biased (Sheng et al., 2020) and toxic (Wallace et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "While simple defense methods such as (Xu et al., 2020) achieve near-perfect effec-",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Universal Adversarial Trigger (UAT) (Wallace et al., 2019) The goal in universal adversarial trigger attack is to find a universal trigger sequence for a given trained model, which if attached to the start of any given input can cause the model to output the desired outcome (Wallace et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : ", 2019) The goal in universal adversarial trigger attack is to find a universal trigger sequence for a given trained model, which if attached to the start of any given input can cause the model to output the desired outcome (Wallace et al., 2019).",
      "startOffset" : 224,
      "endOffset" : 246
    }, {
      "referenceID" : 19,
      "context" : "Previous work (Xu et al., 2020) has shown that toxic triggers are more likely to provoke toxic responses.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "General Setup We use DialoGPT (Zhang et al., 2020c) to generate 100 conversations around a specific topic.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "Toxicity Detection Models To determine toxicity of the candidate attack utterances by the adversary, we utilize an ensemble of three different toxicity detection models: Toxic-bert1, Perspective API2, and Safety classifier (Xu et al., 2020).",
      "startOffset" : 223,
      "endOffset" : 240
    }, {
      "referenceID" : 4,
      "context" : "Data The context sentences around which bots start their conversations come from two different datasets, Wizard of Wikipedia (Dinan et al., 2018) and ConvoKit’s Reddit Corpus.",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 19,
      "context" : "Our results confirm the previous results (Xu et al., 2020) in which authors show in a human adversary case that more toxic attacks perform better in forcing the model to generate toxic utterances.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Fleiss Kappa (Fleiss, 1971) annotator agreement results from this evaluation is reported in Table 1.",
      "startOffset" : 13,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "For the second layer, we use LERG (Tuan et al., 2021) that provides local explanations for dialogue response",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : "Two-stage Non Sequitur Baseline (Xu et al., 2020) This baseline is also a two-stage approach like ours in which the defender first uses a toxicity classifier to detect if the utterance is toxic or not.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "talk about X?” where X is a randomly chosen topic from 1087 topics judged as safe from the Wizard of Wikipedia conversational topic list (Dinan et al., 2018).",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "We show the generalizability of our defense method against non-conversational generation tasks, by conducting experiments with RealToxicityPrompts dataset (Gehman et al., 2020).",
      "startOffset" : 155,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : "Previous work showed that the prompts in RealToxicityPrompts can force different generative models such as GPT2 (Radford et al., 2019) to generate toxic responses.",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "As evident from the previous discussions, the Non Sequitur baseline defense (Xu et al., 2020) that we considered in our paper, only works for the conversational domain; however, our method has the advantage of working on any conditional generation task.",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "Crafting adversarial examples and using them in training was previously shown to be an effective technique in improving NLP and ML models (Nie et al., 2020; Dinan et al., 2019; Kiela et al., 2021).",
      "startOffset" : 138,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "Crafting adversarial examples and using them in training was previously shown to be an effective technique in improving NLP and ML models (Nie et al., 2020; Dinan et al., 2019; Kiela et al., 2021).",
      "startOffset" : 138,
      "endOffset" : 196
    }, {
      "referenceID" : 21,
      "context" : "Not only that, but adversarial attacks can reveal important vulnerabilities in our systems (Zhang et al., 2020a).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "Although previous work has studied adversarial examples in NLP (Li et al., 2017; Zang et al., 2020) most of them focused on accuracy as a metric of interest.",
      "startOffset" : 63,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "Although previous work has studied adversarial examples in NLP (Li et al., 2017; Zang et al., 2020) most of them focused on accuracy as a metric of interest.",
      "startOffset" : 63,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Figure 9: Number of generated toxic responses before and after the defense was applied to GPT-2 from the RealToxicityPrompts dataset (Gehman et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : "toxicity and other ethical considerations (Wallace et al., 2019; Sheng et al., 2020) they did not put the focus on either conversational agents or they did not consider attacks being imperceptible.",
      "startOffset" : 42,
      "endOffset" : 84
    }, {
      "referenceID" : 14,
      "context" : "toxicity and other ethical considerations (Wallace et al., 2019; Sheng et al., 2020) they did not put the focus on either conversational agents or they did not consider attacks being imperceptible.",
      "startOffset" : 42,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "Previous work has shown the importance of adversarially crafted examples into improving NLP systems (Nie et al., 2020; Dinan et al., 2019; Kiela et al., 2021); thus, our automatically generated examples can be useful in not only improving robustness of these systems and highlighting their vulnerabilities, but also a step towards their improvement.",
      "startOffset" : 100,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "Previous work has shown the importance of adversarially crafted examples into improving NLP systems (Nie et al., 2020; Dinan et al., 2019; Kiela et al., 2021); thus, our automatically generated examples can be useful in not only improving robustness of these systems and highlighting their vulnerabilities, but also a step towards their improvement.",
      "startOffset" : 100,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "erate toxic responses which is undesirable as also previously observed in chatbots (Wolf et al., 2017; Henderson et al., 2018; Dinan et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "erate toxic responses which is undesirable as also previously observed in chatbots (Wolf et al., 2017; Henderson et al., 2018; Dinan et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "erate toxic responses which is undesirable as also previously observed in chatbots (Wolf et al., 2017; Henderson et al., 2018; Dinan et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 146
    } ],
    "year" : 0,
    "abstractText" : "Warning: this paper contains content that may be offensive or upsetting. Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems. Despite the abundance of research in this area, relatively less attention has been given to adversarial attacks that force the system to generate toxic language and the defense against them. Existing work to generate such attacks is either based on human-generated attacks which is costly and not scalable or, in case of automatic attacks, the attack vector does not conform to human-like language, which can be detected using a language model loss. In this work, we propose attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, i.e., they can automatically trigger the system into generating toxic language. We then propose a defense mechanism against such attacks which not only mitigates the attack but also attempts to maintain the conversational flow. Through automatic and human evaluations, we show that our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy. Lastly, we establish the generalizability of such a defense mechanism on language generation models beyond conversational agents.",
    "creator" : null
  }
}
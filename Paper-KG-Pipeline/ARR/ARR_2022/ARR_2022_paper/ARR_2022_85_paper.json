{
  "name" : "ARR_2022_85_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Consistent Representation Learning for Continual Relation Extraction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones. Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. However, these memory-based methods tend to overfit the memory samples and perform poorly on imbalanced datasets. To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory. Specifically, supervised contrastive learning based on a memory bank is first used to train each new task so that the model can effectively learn the relation representation. Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task. The proposed method can better learn consistent representations to alleviate forgetting effectively. Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-theart baselines and yield strong robustness on the imbalanced dataset."
    }, {
      "heading" : "1 Introduction",
      "text" : "Relation extraction (RE) is an essential issue in information extraction (IE), which can apply to many downstream NLP tasks, such as information retrieval (Xiong et al., 2017) and question and answer (Tao et al., 2018). For example, given a sentence x with the annotated entities pairs e1 and e2, the RE aims to identify the relations between e1 and e2. However, traditional relation extraction models (Zhou et al., 2016; Soares et al., 2019a) always assume a fixed set of predefined relations and train on\na fixed dataset, which cannot handle the growing relation types in real life well.\nTo solve this situation, continual relation extraction (CRE) is introduced (Wang et al., 2019; Han et al., 2020; Wu et al., 2021; Cui et al., 2021). Compared with traditional relation extraction, CRE aims to help the model learn new relations while maintaining accurate classification of old ones. Wang et al. (2019) shows that continual relation learning needs to alleviate the catastrophic forgetting of old tasks when the model learns new tasks. Because neural networks need to retrain a fixed set of parameters with each training, the most efficient solution to the problem of catastrophic forgetting is to store all the historical data and retrain the model with all the data each time a new relational instance appears. This method can achieve the best effect in continual relation learning, but it is not adopted in real life due to the time and computing power costs.\nSome recent works have proposed a variety of methods to alleviate the catastrophic forgetting problem in continual learning, including regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018), dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017), and memory-based methods (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2018). Although these methods have been verified in simple image classification tasks, previous works have proved that memory-based methods are the most effective in natural language processing applications (Wang et al., 2019; d’Autume et al., 2019). In recent years, the memory-based continual relation extraction model has made significant progress in alleviating the problem of catastrophic forgetting (Han et al., 2020; Wu et al., 2021; Cui et al., 2021). Wang et al. (2019) proposes a mechanism for embedding sentence alignment in memory maintenance to ensure the stability of the embedding space. Han et al. (2020) introduces a multi-round joint training\nprocess for memory consolidation. But these two methods only explore the problem of catastrophic forgetting in the overall performance of the task sequence. Wu et al. (2021) proposes to integrate curriculum learning. Although it is possible to analyze the characteristics of each subtask and the performance of the corresponding model, it still fails to make full use of the saved sample information. Cui et al. (2021) introduce an attention network to refine the prototype to better recover the interruption of the embedded space. However, this method will produce a bias in the classification of the old task as the new task continues to learn the classifier, which will affect the performance of the old task. Although the above method can alleviate catastrophic forgetting to a certain extent, it does not consider the consistency of relation embedding space.\nBecause the performance of the model of CRE is sensitive to the quality of sample embedding, it needs to ensure that the learning of new tasks will not damage the embedding of old tasks. Inspired by supervised contrastive Learning (Khosla et al., 2020) to explicitly constrain data embeddings, a consistent representation learning method is proposed for continual relation extraction, which constrains the embedding of old tasks not to occur significantly change through supervised contrastive learning and knowledge distillation. Specifically, the example encoder first trains on the current task data through supervised contrastive learning based on memory bank, and then uses k-means to select representative samples to storage as memory after the training is completed. To relieve catastrophic forgetting, contrastive replay is used to train memorized samples. At the same time, to ensure that the embedding of historical relations does not undergo significant changes, knowledge distillation is used to make the embedding distribution of the new and old tasks consistent. In the testing phase, the nearest class mean (NCM) classifier is used to classify the test sample, which will not be affected by the deviation of the classifier.\nIn summary, our contributions in this paper are summarized as follows: First, a novel CRE method is proposed, which uses supervised contrastive learning and knowledge distillation to learn consistent relation representations for continual learning. Second, consistent representation learning can ensure the stability of the relational embedding space to alleviate catastrophic forgetting and make full\nuse of stored samples. Finally, extensive experiments results on FewRel and TACRED datasets show that the proposed method is better than the latest baseline and effectively mitigates catastrophic forgetting."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Continual Learning",
      "text" : "Existing continual learning models mainly focus on three areas: (1) Regularization-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017) impose constraints on updating neural weights important to previous tasks for relieving catastrophic forgetting. (2) Dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017) extends the model architecture dynamically to learn new tasks and prevent forgetting old tasks effectively. However, these methods are unsuitable for NLP applications because the model size increases dramatically with increasing tasks. (3) Memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018; Mai et al., 2021) saves some samples from old tasks and continuously learns them in new tasks to alleviate catastrophic forgetting. Dong et al. (2021) proposes a simple relational distillation incremental learning framework to balance retaining old knowledge and adapting to new knowledge. Yan et al. (2021) proposes a new two-stage learning method that uses dynamic expandable representation for more effective incremental conceptual modelling. Among these methods, memory-based methods are the most effective in NLP tasks (Wang et al., 2019; Sun et al., 2019; d’Autume et al., 2019). Inspired by the success of memory-based methods in the field of NLP, we use the framework of memory replay to learn new relations that are constantly emerging."
    }, {
      "heading" : "2.2 Contrastive Learning",
      "text" : "Contrastive learning (CL) aims to make the representations of similar samples map closer to each other in the embedded space, while that of dissimilar samples should be farther away (Jaiswal et al., 2021). In recent years, the rise of CL has made great progress in self-supervised representation learning. (Wu et al., 2018; He et al., 2020; Li et al., 2020; Chen and He, 2021). The common point of these works is that no labels are available, so positive and negative pairs were formed through data augmentations. Recently, supervised contrastive learning (Khosla et al., 2020) has received\nmuch attention, which uses label information to extend contrastive learning. Hendrycks and Dietterich (2019) compares the supervised contrastive loss with the cross-entropy loss on the ImageNet-C dataset, and verifies that the supervised contrastive loss is not sensitive to the hyperparameter settings of the optimizer or data enhancement. Chen et al. (2020) proposed a contrastive learning framework for visual representations that does not require a special architecture or memory bank. Khosla et al. (2020) extend the self-supervised batch contrastive approach to the fully-supervised setting, which use supervised contrastive loss learning better represetation. Liu and Abbeel (2020) proposed a hybrid discriminant-generative training method based on an energy model. In this paper, contrastive learning is applied to continual relation extraction to extract better relation representation."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "In continual relation extraction, given a series of K tasks {T1, T2, ..., TK}, where the k-th task has its own training set Dk and relation set Rk. Each task Tk is a traditional supervised classification task, including a series of examples and their corresponding labels {(xi, yi)}Ni=1, where xi is the input data, including the natural language text and entity pair, and yi ∈ Rk is the relation label. The goal of continual relation learning is to train the model, which keeps learning new tasks while avoiding catastrophic forgetting of previous learning tasks. In other words, after learning the k-th task, the model can identify the relation of a given entity pair into R̂k, where R̂k = ∪ki=1Ri is the relation set already observed till the k-th task.\nIn order to mitigate catastrophic forgetting in continual relational extraction, episodic memory modules have been used in previous work (Wang et al., 2019; Han et al., 2020; Cui et al., 2021), to store small samples in historical tasks. Inspired by (Cui et al., 2021), we store several representative samples for each relation. Therefore, the episodic memory module for the observed relations in T1 ∼ Tk is M̂k = ∪r∈R̂kMr, where Mr = {(xi, yi)} O i=1, r represents a certain relation, and O is sample number (memory size).\nAlgorithm 1 Training procedure for Tk Input:\nThe training set of Dk of the k-th task, encoder E, projection head Proj, memory bank Mb, repaly memory bank Mr, history memory Mk−1, current relation set Rk, history relation set R̂k−1 Output: encoder fk(·), history memory Mk, history relation set R̂k\n1: if Tk is not the first task then 2: get memory knowledge with E on Mk−1; 3: end if 4: Mb ← E(Dk) ; 5: for i← 1 to epoch1 do 6: for each xj ∈ Dk do 7: Sample from Mb; 8: Update E and Proj with ∇LCL; 9: Update Mb;\n10: end for 11: end for 12: Select informative examples from Dk to store\ninto M̂ 13: Mk ←Mk−1 ∪ M̂ ; 14: R̂k ← R̂k−1 ∪Rk; 15: if Tk is not the first task then 16: M̃b ← E(Mk) ; 17: for i← 1 to epoch2 do 18: for each xj ∈Mk do 19: Sample from M̃b; 20: Update E and Proj with ∇LCR and ∇LKL; 21: Update Mr; 22: end for 23: end for 24: Select informative examples from Dk to store into M̂ ; 25: Mk ←Mk−1 ∪ M̂ 26: end if 27: return E, Mk, R̂k;"
    }, {
      "heading" : "3.2 Framework",
      "text" : "The consistent representation learning (CRL) in the current task is described in Algorithm 1, which consists of three main steps: (1) Init training for new task (line 4 ∼ 11): The parameters of the encoder and projector head are trained on the training sample in Dk with supervised contrastive learning. (2) Sample selection (line 12 ∼ 13): For each relation r ∈ Rk, we retrieve all samples labeled\nr from Dk. Then, the k-means algorithm is used to cluster the samples. The relation representation of the sample closest to the center is selected and stored in memory for each cluster. (3) Consistent representation learning (15 ∼ 24): In order to keep the embedding of historical relations in space consistent after learning new tasks, we perform contrastive replay and knowledge distillation constraints on the samples in memory."
    }, {
      "heading" : "3.3 Encoder",
      "text" : "The key of CRE is to obtain a better relation representation. The pre-trained language model BERT (Devlin et al., 2019) shows a powerful ability in extracting contextual representation of text. Therefore, BERT is used to encode entity pairs and context information to get the relational representation.\nGiven a sentence x = [w1, . . . , w|x|] and a pair of entities (E1,E2), we follow Soares et al. (2019b) augment x with four reserved word pieces to mark the begin and end of each entity mentioned in the sentence. The new token sequence is fed into BERT instead of x. To get the final relation representation between the two entities, the output corresponding to the positions of E1 and E2 are concatenated, and then map it to a high-dimensional hidden representation h ∈ Rdh , as follows:\nh =W[h[E1];h[E2]] + b, (1)\nwhere W ∈ R2dh×dh and b ∈ Rdh are trainable parameters. The encoder in which the abovementioned encoded sentence is a relation representation is denoted as E.\nThen, we use a projection head Proj to obtain the low-dimensional embedding:\nz̃ =Proj(h), (2)\nwhere Proj(·) = MLP(·) is composed of two layers of neural networks. The normalized embedding z = z̃/||z̃|| is used for contrastive learning, and the hidden representation is used for classification."
    }, {
      "heading" : "3.4 Inital training for new task",
      "text" : "Before training for each new task Tk, we first use Encoder to extract the embedding z̃ of the relational representation of each sentence in Dk, and use them as the initialized memory bank Mb:\nMb ← {zi}Ni=1. (3)\nAt the beginning of training, relation representation extraction is performed on each batch B. Then the data embedding is explicitly constrained by clustering through supervised contrastive learning (Khosla et al., 2020):\nLCL = ∑ i∈I −1 |P (i)| ∑ p∈P (i) log exp (zi · zp/τ)∑ j∈SI exp (zi · zj/τ) , (4) where I = {1, 2, . . . , |B|} is the set of indices of B. SI represents the indices set of randomly sampled partial samples from Mb. P (i) = {p ∈ SI : yp = yi} is the indices set that is the same as the zi label in Mb, and |P (i)| is its cardinality. τ ∈ R+ is an adjustable temperature parameter controling the separation of classes, the · indicates the dot product.\nAfter backpropagating the gradient of loss on each batch, we update the representation in the memory bank:\nMb[Ĩ]← {zi} |B| i=1. (5)\nwhere Ĩ is the corresponding index set of this batch of samples in Mb. After epoch1 training set training, the model can learn a better relation representation."
    }, {
      "heading" : "3.5 Selecting Typical Samples for Memory",
      "text" : "In order to make the model not forget the relevant knowledge of the old task when it learns the new task, some samples need to be stored in Mr. Inspired by (Han et al., 2020; Cui et al., 2021), we use k-means to cluster each relation, where the number of clusters is the number of samples that need to be stored for each class. Then, the relation representation closest to the center is selected and stored in memory for each cluster."
    }, {
      "heading" : "3.6 Consistent Representation Learning",
      "text" : "After learning a new task, the representation of the old relation in the space may change. In order to make the encoder not change the knowledge of the old task while learning the new task, we propose two replay strategies to learn consistent representation for alleviating this problem: contrastive replay and knowledge distillation. Figure 1 shows the main flow of consistent representation learning.\nContrastive Replay with Memory Bank After the new task learning is over, we use the new task to train the encoder to further train the encoder by replaying the samples stored in memory Mk. After the learning of the current task is over, we use the same method in Section 3.4 to replay the samples stored in memory Mk.\nThe difference here is that each batch uses all the samples in the entire memory bank for contrastive learning, as follows: LCR = ∑ i∈I −1 |P (i)| ∑ p∈P (i) log exp (zi · zp/τ)∑ j∈S̃I exp (zi · zj/τ) , (6) where S̃I represents the set of indices of all samples in M̃b. M̃b is the memory bank, which stores the normalized representation of all samples in Mk.\nBy replaying the samples in memory, the encoder can alleviate the forgetting of previously learned knowledge, and at the same time, consolidate the knowledge learned in the current task. However, contrastive replay allows the encoder to train on a small number of samples, which risks overfitting. On the other hand, it may change the distribution of relations in the previous task. Therefore, we propose knowledge distillation to make up for this shortcoming.\nKnowledge Distillation for Relieve Forgetting We hope that the model can retain the semantic knowledge between relations in historical tasks.\nTherefore, before the encoder is trained on a task, we use the similarity metric between the relations in memory as Memory Knowledge. Then use the knowledge distillation to relieve the model from forgetting this knowledge.\nSpecifically, the samples in the memory are encoded first, and then the prototype of each class is calculated:\npc = O∑ i=1 zci , (7)\nwhere O is the number of memory size, zci is the relation representation belonging to class c. Then, the cosine similarity between the classes is calculated to represent the knowledge learned in the memory:\naij = pTi pj ∥pi∥ ∥pj∥ , (8)\nwhere aij is the cosine similarity between prototype i and j.\nWhen performing memory replay, we use KL divergence to make the encoder retain the knowledge of the old task.\nLKL = ∑ iKL(Pi||Qi), (9)\nwhere Pi = {pij}|R̂k|j=1 is the metric distribution of the prototype before training, and pij = exp(aij/τ)∑ j exp(aij/τ)\n. Similarly, Qi = {qij}|R̂k|j=1 is the metric distribution of calculate the temporary prototype from the memory bank during training, and qij =\nexp(ãij/τ)∑ j exp(ãij/τ)\n. ã is the Embedding Knowledge of the memory Mk, which is the cosine similarity between temporary prototypes. The temporary prototype is dynamically calculated in each batch based on the memory bank M̃b."
    }, {
      "heading" : "3.7 NCM for Prediction",
      "text" : "To predict a label for a test sample x, the nearest class mean (NCM) compares the embedding of x with all the prototypes of memory and assigns the class label with the most similar prototype:\npc = 1\nnc ∑ i E (x̄i) · ⊮ {yi = c} ,\ny∗ =argmin c=1,...,k\n∥f(x)− pc∥ , (10)\nwhere x̄ ∈ Mk is stored sample, and y∗ is a predicted label. Since the NCM classifier compares the embedding of the test sample with prototypes, it does not require an additional FC layer. Therefore, new classes can be added without any architecture modification."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Our experiments are conducted on two benchmark datasets: in the experiment, the training-testvalidation that the split ratio is 3:1:1.\nFewRel (Han et al., 2018) It is a RE dataset that contains 80 relations, each with 700 instances. Following the experimental settings by Wang et al. (2019), the original train and valid set of FewRel are used for experimental, which contains 80 classes.\nTACRED (Zhang et al., 2017) It is a large-scale RE dataset containing 42 relations (including no relations) and 106,264 samples, built on news networks and online documents. Compared with FewRel, the samples in TACRED are imbalanced. Following Cui et al. (2021), the number of training samples for each relation is limited to 320 and the number of test samples of relation to 40."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "Average accuracy is a better measure of the effect of catastrophic forgetting because it emphasizes the model’s performance on earlier tasks (Han et al., 2020; Cui et al., 2021). This paper evaluates the\n1https://github.com/fd2014cl/RP-CRE\nmodel by using the average accuracy of K tasks at each step."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We evaluate CRL and several baselines on benchmarks for comparison:\n(1) EA-EMR (Wang et al., 2019) introduced a memory replay and embedding alignment mechanism to maintain memory and alleviate embedding distortion during training for new tasks.\n(2) EMAR (Han et al., 2020) constructs a memory activation and reconsolidation mechanism to alleviate the catastrophic forgetting problem in CRE.\n(3) CML (Wu et al., 2021) proposed a curriculum-meta learning method to alleviate the order sensitivity and catastrophic forgetting in CRE.\n(4) RP-CRE (Cui et al., 2021) achieves enhanced performance by utilizing relation prototypes to refine sample embeddings, thereby effectively avoiding catastrophic forgetting."
    }, {
      "heading" : "4.4 Training Details and Parameters Setting",
      "text" : "A completely random sampling strategy at the relation level is adopted. It simulates ten tasks by randomly dividing all relations of the dataset into 10 sets to simulate 10 tasks, as suggested in (Cui et al., 2021). For a fair comparison, we set the\nrandom seed of the experiment to be the same as the seed in (Cui et al., 2021), so that the task sequence is exactly the same. Note that our reproduced model RP-CRE † and CRL use strictly the same experimental environment. In order to facilitate the reproduction of our experimental results, the proposed method source code and detailed hyperparameters are provided on https: //github.com/submitacl22/CRL."
    }, {
      "heading" : "4.5 Results and Discussion",
      "text" : "Table 1 shows the results of the proposed methods and baselines ones compared on two datasets, where RP-CRE † is reproduced under the same conditions based on open source code. We also ablated knowledge distillation and contrastive replay for consistent representation learning. CRL (w/o KL) and CRL (w/o CR) respectively refer to removing knowledge distillation loss LKL and contrastive replay loss LCR when replaying memory. From the table, some conclusions can be drawn:\n(1) Our proposed CRL is significantly better than other baselines and achieves state-of-the-art performance. Compared with RP-CRE, our model also produces apparent advantages. It proves that CRL can learn better consistent relation representations and is more stable in the process of continual learning.\n(2) It is observed that all baselines perform worse on the TACRED dataset. The primary reason for this result is that TACRED is an imbalanced dataset. However, our model performs better than RP-CRE’s last task on TACRED (3.4% higher than RP-CRE), which is more significant than the improvement (0.5%) on the class-balanced dataset FewRel. It shows that our model is more robust to scenarios with class-imbalanced.\n(3) Comparing CRL and CRL (w/o KL), not\nadopting knowledge distillation during training can cause the model to drop 1% and 0.6% on FewRel and TACRED, respectively. The experimental results show that knowledge distillation can uniformly alleviate the model’s forgetting of previous knowledge to learn a better consistent representation.\n(4) Comparing CRL and CRL (w/o CR), removing L during memory replay caused the model to drop 2.4% and 4.8% on FewRel and TACRED, respectively. The reason for the significant drop is that only adopting LKL cannot make the model review the samples of the current task, which leads to overfitting in the historical relations during replay."
    }, {
      "heading" : "4.6 Effect of Memory Size",
      "text" : "The memory size is the number of memory samples needed for each relation. In this section, we will study the impact of memory size on the performance of our model and RP-CRE. We compare three memory sizes: 5, 10, and 20. The experimental results are shown in Figure 2.\nWe choose RP-CRE as the main competitor, where all configurations and task sequence remain unchanged. (1) As the size of the memory decreases, the performance of the model tends to decline, which shows that the size of the memory is a key factor that affects continuous learning and learning. But our model is more stable than RP-CRE (the performance gap in the final task), especially on the TACRED dataset. (2) On both FewRel and TACRED, CRL keeps the best performance under different memory sizes and produces obvious advantages in small memory. It indicates that utilizing consistent representation learning is a more effective way to utilize memory than the existing memory-based CRE method."
    }, {
      "heading" : "4.7 Effect of Consistent Representation Learning",
      "text" : "In order to explore the long-term effects of consistency representation learning in continual relation extraction, we tested our model and RP-CRE on TACRED to observe the changes in the embedding space of old tasks as new tasks continue to increase. The model performs feature extraction on all samples in the test set in task 1 at the end of tasks 1, 4, 7, and 10. Then t-SNE is used to represent the dimensionality reduction relation representation. All samples on the test set of task 1 are drawn, where different color points represent different groundtruth labels. The visualization results are shown in Figure 3.\nFrom Figure 3, we can see that although the relation embeddings of RP-CRE are clustered and separated in each class after prototype refinement, as new tasks are continuously learned, the data embedding of task 1 is obviously scattered. In contrast, our model retains a good separation between classes, while the data embedding within classes is compact and has a certain diversity. In addition, we can see that our model has relatively stable changes in the distribution of different classes in task 1, and retains the knowledge of historical tasks with training. This is mainly because our model learns through supervised comparison, and explicitly emphasizes that the samples in historical memory are compact within the class and far away from each other. And the knowledge of histori-\ncal memory is preserved through the distillation of memory knowledge. Because knowledge distillation preserves the distance distribution between classes, it can make up for the contrastive learning to over-optimize the distance between classes to prevent overfitting."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "This paper proposes a novel consistent representation learning method for the CRE task, mainly through contrastive learning and knowledge distillation when replaying memory. Specifically, we use supervised comparative learning based on a memory bank to train each new task so that the model can effectively learn the feature representation. In addition, in order to prevent the catastrophic forgetting of the old task, we compare and replay the memory samples, and at the same time, make the model retain the knowledge of the relation between the historical tasks through the knowledge distillation. Our method can better learn consistent representations to alleviate catastrophic forgetting effectively. Extensive experiments on two benchmark data sets show that our method significantly improves the performance of the most advanced technology and demonstrates powerful representation learning capabilities. In the future, we will continue to study cross-domain continual relation extraction to acquire ever-increasing knowledge."
    } ],
    "references" : [ {
      "title" : "Memory aware synapses: Learning what (not) to forget",
      "author" : [ "Rahaf Aljundi", "Francesca Babiloni", "Mohamed Elhoseiny", "Marcus Rohrbach", "Tinne Tuytelaars." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 139–154.",
      "citeRegEx" : "Aljundi et al\\.,? 2018",
      "shortCiteRegEx" : "Aljundi et al\\.",
      "year" : 2018
    }, {
      "title" : "Efficient lifelong learning with a-gem",
      "author" : [ "Arslan Chaudhry", "Marc’Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny" ],
      "venue" : "arXiv preprint arXiv:1812.00420",
      "citeRegEx" : "Chaudhry et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chaudhry et al\\.",
      "year" : 2018
    }, {
      "title" : "Net2net: Accelerating learning via knowledge transfer",
      "author" : [ "Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens." ],
      "venue" : "arXiv preprint arXiv:1511.05641.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "International conference on machine learning, pages 1597–1607. PMLR.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring simple siamese representation learning",
      "author" : [ "Xinlei Chen", "Kaiming He." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750–15758.",
      "citeRegEx" : "Chen and He.,? 2021",
      "shortCiteRegEx" : "Chen and He.",
      "year" : 2021
    }, {
      "title" : "Refining sample embeddings with relation prototypes to enhance continual relation extraction",
      "author" : [ "Li Cui", "Deqing Yang", "Jiaxin Yu", "Chengwei Hu", "Jiayang Cheng", "Jingjie Yi", "Yanghua Xiao." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for",
      "citeRegEx" : "Cui et al\\.,? 2021",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2021
    }, {
      "title" : "Episodic memory in lifelong language learning",
      "author" : [ "Cyprien de Masson d’Autume", "Sebastian Ruder", "Lingpeng Kong", "Dani Yogatama" ],
      "venue" : "arXiv preprint arXiv:1906.01076",
      "citeRegEx" : "d.Autume et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "d.Autume et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Few-shot class-incremental learning via relation knowledge distillation",
      "author" : [ "Songlin Dong", "Xiaopeng Hong", "Xiaoyu Tao", "Xinyuan Chang", "Xing Wei", "Yihong Gong." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1255–",
      "citeRegEx" : "Dong et al\\.,? 2021",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2021
    }, {
      "title" : "Pathnet: Evolution channels gradient descent in super neural networks",
      "author" : [ "Chrisantha Fernando", "Dylan Banarse", "Charles Blundell", "Yori Zwols", "David Ha", "Andrei A Rusu", "Alexander Pritzel", "Daan Wierstra." ],
      "venue" : "arXiv preprint arXiv:1701.08734.",
      "citeRegEx" : "Fernando et al\\.,? 2017",
      "shortCiteRegEx" : "Fernando et al\\.",
      "year" : 2017
    }, {
      "title" : "Continual relation learning via episodic memory activation and reconsolidation",
      "author" : [ "Xu Han", "Yi Dai", "Tianyu Gao", "Yankai Lin", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
      "author" : [ "Xu Han", "Hao Zhu", "Pengfei Yu", "Ziyun Wang", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Han et al\\.,? 2018",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2018
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Benchmarking neural network robustness to common corruptions and perturbations",
      "author" : [ "Dan Hendrycks", "Thomas Dietterich." ],
      "venue" : "arXiv preprint arXiv:1903.12261.",
      "citeRegEx" : "Hendrycks and Dietterich.,? 2019",
      "shortCiteRegEx" : "Hendrycks and Dietterich.",
      "year" : 2019
    }, {
      "title" : "A survey on contrastive selfsupervised learning",
      "author" : [ "Ashish Jaiswal", "Ashwin Ramesh Babu", "Mohammad Zaki Zadeh", "Debapriya Banerjee", "Fillia Makedon." ],
      "venue" : "Technologies, 9(1):2.",
      "citeRegEx" : "Jaiswal et al\\.,? 2021",
      "shortCiteRegEx" : "Jaiswal et al\\.",
      "year" : 2021
    }, {
      "title" : "Supervised contrastive learning",
      "author" : [ "Prannay Khosla", "Piotr Teterwak", "Chen Wang", "Aaron Sarna", "Yonglong Tian", "Phillip Isola", "Aaron Maschinot", "Ce Liu", "Dilip Krishnan." ],
      "venue" : "arXiv preprint arXiv:2004.11362.",
      "citeRegEx" : "Khosla et al\\.,? 2020",
      "shortCiteRegEx" : "Khosla et al\\.",
      "year" : 2020
    }, {
      "title" : "Overcoming catastrophic forgetting in neural networks",
      "author" : [ "James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 2017
    }, {
      "title" : "Prototypical contrastive learning of unsupervised representations",
      "author" : [ "Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven CH Hoi." ],
      "venue" : "arXiv preprint arXiv:2005.04966.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Hybrid discriminative-generative training via contrastive learning",
      "author" : [ "Hao Liu", "Pieter Abbeel." ],
      "venue" : "arXiv preprint arXiv:2007.09070.",
      "citeRegEx" : "Liu and Abbeel.,? 2020",
      "shortCiteRegEx" : "Liu and Abbeel.",
      "year" : 2020
    }, {
      "title" : "Rotate your networks: Better weight consolidation and less catastrophic forgetting",
      "author" : [ "Xialei Liu", "Marc Masana", "Luis Herranz", "Joost Van de Weijer", "Antonio M Lopez", "Andrew D Bagdanov." ],
      "venue" : "2018 24th International Conference on Pattern Recogni-",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Gradient episodic memory for continual learning",
      "author" : [ "David Lopez-Paz", "Marc’Aurelio Ranzato" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Lopez.Paz and Ranzato.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lopez.Paz and Ranzato.",
      "year" : 2017
    }, {
      "title" : "Supervised contrastive replay: Revisiting the nearest class mean classifier in online classincremental continual learning",
      "author" : [ "Zheda Mai", "Ruiwen Li", "Hyunwoo Kim", "Scott Sanner." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and",
      "citeRegEx" : "Mai et al\\.,? 2021",
      "shortCiteRegEx" : "Mai et al\\.",
      "year" : 2021
    }, {
      "title" : "Matching the blanks: Distributional similarity for relation learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Soares et al\\.,? 2019a",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Matching the blanks: Distributional similarity for relation learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,",
      "citeRegEx" : "Soares et al\\.,? 2019b",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Lamol: Language modeling for lifelong language learning",
      "author" : [ "Fan-Keng Sun", "Cheng-Hao Ho", "Hung-Yi Lee." ],
      "venue" : "arXiv preprint arXiv:1909.03329.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Get the point of my utterance! learning towards effective responses with multi-head attention mechanism",
      "author" : [ "Chongyang Tao", "Shen Gao", "Mingyue Shang", "Wei Wu", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Con-",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence embedding alignment for lifelong relation extraction",
      "author" : [ "Hong Wang", "Wenhan Xiong", "Mo Yu", "Xiaoxiao Guo", "Shiyu Chang", "William Yang Wang." ],
      "venue" : "arXiv preprint arXiv:1903.02588.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Curriculum-meta learning for order-robust continual relation extraction",
      "author" : [ "Tongtong Wu", "Xuekai Li", "Yuan-Fang Li", "Reza Haffari", "Guilin Qi", "Yujin Zhu", "Guoqiang Xu." ],
      "venue" : "CoRR, abs/2101.01926.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised feature learning via nonparametric instance discrimination",
      "author" : [ "Zhirong Wu", "Yuanjun Xiong", "Stella X Yu", "Dahua Lin." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733–3742.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Explicit semantic ranking for academic search via knowledge graph embedding",
      "author" : [ "Chenyan Xiong", "Russell Power", "Jamie Callan." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017, pages",
      "citeRegEx" : "Xiong et al\\.,? 2017",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "Der: Dynamically expandable representation for class incremental learning",
      "author" : [ "Shipeng Yan", "Jiangwei Xie", "Xuming He." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3014–3023.",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "Continual learning through synaptic intelligence",
      "author" : [ "Friedemann Zenke", "Ben Poole", "Surya Ganguli." ],
      "venue" : "International Conference on Machine Learning, pages 3987–3995. PMLR.",
      "citeRegEx" : "Zenke et al\\.,? 2017",
      "shortCiteRegEx" : "Zenke et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Relation extraction (RE) is an essential issue in information extraction (IE), which can apply to many downstream NLP tasks, such as information retrieval (Xiong et al., 2017) and question and answer (Tao et al.",
      "startOffset" : 155,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "However, traditional relation extraction models (Zhou et al., 2016; Soares et al., 2019a) always assume a fixed set of predefined relations and train on a fixed dataset, which cannot handle the growing relation types in real life well.",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 26,
      "context" : "To solve this situation, continual relation extraction (CRE) is introduced (Wang et al., 2019; Han et al., 2020; Wu et al., 2021; Cui et al., 2021).",
      "startOffset" : 75,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "To solve this situation, continual relation extraction (CRE) is introduced (Wang et al., 2019; Han et al., 2020; Wu et al., 2021; Cui et al., 2021).",
      "startOffset" : 75,
      "endOffset" : 147
    }, {
      "referenceID" : 27,
      "context" : "To solve this situation, continual relation extraction (CRE) is introduced (Wang et al., 2019; Han et al., 2020; Wu et al., 2021; Cui et al., 2021).",
      "startOffset" : 75,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "To solve this situation, continual relation extraction (CRE) is introduced (Wang et al., 2019; Han et al., 2020; Wu et al., 2021; Cui et al., 2021).",
      "startOffset" : 75,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "Some recent works have proposed a variety of methods to alleviate the catastrophic forgetting problem in continual learning, including regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018), dynamic architecture methods (Chen et al.",
      "startOffset" : 158,
      "endOffset" : 222
    }, {
      "referenceID" : 31,
      "context" : "Some recent works have proposed a variety of methods to alleviate the catastrophic forgetting problem in continual learning, including regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018), dynamic architecture methods (Chen et al.",
      "startOffset" : 158,
      "endOffset" : 222
    }, {
      "referenceID" : 19,
      "context" : "Some recent works have proposed a variety of methods to alleviate the catastrophic forgetting problem in continual learning, including regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018), dynamic architecture methods (Chen et al.",
      "startOffset" : 158,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : ", 2018), dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017), and memory-based methods (Lopez-Paz and Ranzato, 2017; Chaudhry et al.",
      "startOffset" : 38,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : ", 2018), dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017), and memory-based methods (Lopez-Paz and Ranzato, 2017; Chaudhry et al.",
      "startOffset" : 38,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : ", 2017), and memory-based methods (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : ", 2017), and memory-based methods (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 26,
      "context" : "Although these methods have been verified in simple image classification tasks, previous works have proved that memory-based methods are the most effective in natural language processing applications (Wang et al., 2019; d’Autume et al., 2019).",
      "startOffset" : 200,
      "endOffset" : 242
    }, {
      "referenceID" : 6,
      "context" : "Although these methods have been verified in simple image classification tasks, previous works have proved that memory-based methods are the most effective in natural language processing applications (Wang et al., 2019; d’Autume et al., 2019).",
      "startOffset" : 200,
      "endOffset" : 242
    }, {
      "referenceID" : 10,
      "context" : "In recent years, the memory-based continual relation extraction model has made significant progress in alleviating the problem of catastrophic forgetting (Han et al., 2020; Wu et al., 2021; Cui et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 207
    }, {
      "referenceID" : 27,
      "context" : "In recent years, the memory-based continual relation extraction model has made significant progress in alleviating the problem of catastrophic forgetting (Han et al., 2020; Wu et al., 2021; Cui et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "In recent years, the memory-based continual relation extraction model has made significant progress in alleviating the problem of catastrophic forgetting (Han et al., 2020; Wu et al., 2021; Cui et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 207
    }, {
      "referenceID" : 15,
      "context" : "Inspired by supervised contrastive Learning (Khosla et al., 2020) to explicitly constrain data embeddings, a consistent representation learning method",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Existing continual learning models mainly focus on three areas: (1) Regularization-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017) impose constraints on updating neural weights important to previous tasks for relieving catastrophic forgetting.",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 31,
      "context" : "Existing continual learning models mainly focus on three areas: (1) Regularization-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017) impose constraints on updating neural weights important to previous tasks for relieving catastrophic forgetting.",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "(2) Dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017) extends the model architecture dynamically to learn new tasks and prevent forgetting old tasks effectively.",
      "startOffset" : 33,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "(2) Dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017) extends the model architecture dynamically to learn new tasks and prevent forgetting old tasks effectively.",
      "startOffset" : 33,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "(Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018; Mai et al., 2021) saves some samples from old tasks and continuously learns them in new tasks to alleviate catastrophic forgetting.",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "(Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018; Mai et al., 2021) saves some samples from old tasks and continuously learns them in new tasks to alleviate catastrophic forgetting.",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "(Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018; Mai et al., 2021) saves some samples from old tasks and continuously learns them in new tasks to alleviate catastrophic forgetting.",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "(Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018; Mai et al., 2021) saves some samples from old tasks and continuously learns them in new tasks to alleviate catastrophic forgetting.",
      "startOffset" : 0,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "Among these methods, memory-based methods are the most effective in NLP tasks (Wang et al., 2019; Sun et al., 2019; d’Autume et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 138
    }, {
      "referenceID" : 24,
      "context" : "Among these methods, memory-based methods are the most effective in NLP tasks (Wang et al., 2019; Sun et al., 2019; d’Autume et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "Among these methods, memory-based methods are the most effective in NLP tasks (Wang et al., 2019; Sun et al., 2019; d’Autume et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "Contrastive learning (CL) aims to make the representations of similar samples map closer to each other in the embedded space, while that of dissimilar samples should be farther away (Jaiswal et al., 2021).",
      "startOffset" : 182,
      "endOffset" : 204
    }, {
      "referenceID" : 15,
      "context" : "Recently, supervised contrastive learning (Khosla et al., 2020) has received",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "In order to mitigate catastrophic forgetting in continual relational extraction, episodic memory modules have been used in previous work (Wang et al., 2019; Han et al., 2020; Cui et al., 2021), to store small samples in historical tasks.",
      "startOffset" : 137,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : "In order to mitigate catastrophic forgetting in continual relational extraction, episodic memory modules have been used in previous work (Wang et al., 2019; Han et al., 2020; Cui et al., 2021), to store small samples in historical tasks.",
      "startOffset" : 137,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "In order to mitigate catastrophic forgetting in continual relational extraction, episodic memory modules have been used in previous work (Wang et al., 2019; Han et al., 2020; Cui et al., 2021), to store small samples in historical tasks.",
      "startOffset" : 137,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "Inspired by (Cui et al., 2021), we store several representative samples for each relation.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "The pre-trained language model BERT (Devlin et al., 2019) shows a powerful ability in extracting contextual representation of text.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "the data embedding is explicitly constrained by clustering through supervised contrastive learning (Khosla et al., 2020):",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Inspired by (Han et al., 2020; Cui et al., 2021), we use k-means to cluster each relation, where the number of clusters is the number of samples that need to be stored for each class.",
      "startOffset" : 12,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Inspired by (Han et al., 2020; Cui et al., 2021), we use k-means to cluster each relation, where the number of clusters is the number of samples that need to be stored for each class.",
      "startOffset" : 12,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "The method marked by † represents the results generated from open source code1and the other baseline results copied from the original paper (Cui et al., 2021)",
      "startOffset" : 140,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "FewRel (Han et al., 2018) It is a RE dataset that contains 80 relations, each with 700 instances.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "Average accuracy is a better measure of the effect of catastrophic forgetting because it emphasizes the model’s performance on earlier tasks (Han et al., 2020; Cui et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "Average accuracy is a better measure of the effect of catastrophic forgetting because it emphasizes the model’s performance on earlier tasks (Han et al., 2020; Cui et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 177
    }, {
      "referenceID" : 26,
      "context" : "(1) EA-EMR (Wang et al., 2019) introduced a memory replay and embedding alignment mechanism to maintain memory and alleviate embedding distortion during training for new tasks.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "(2) EMAR (Han et al., 2020) constructs a memory activation and reconsolidation mechanism to alleviate the catastrophic forgetting problem in CRE.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "(3) CML (Wu et al., 2021) proposed a curriculum-meta learning method to alleviate the order sensitivity and catastrophic forgetting in CRE.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "(4) RP-CRE (Cui et al., 2021) achieves enhanced performance by utilizing relation prototypes to refine sample embeddings, thereby effectively avoiding catastrophic forgetting.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "It simulates ten tasks by randomly dividing all relations of the dataset into 10 sets to simulate 10 tasks, as suggested in (Cui et al., 2021).",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "random seed of the experiment to be the same as the seed in (Cui et al., 2021), so that the task sequence is exactly the same.",
      "startOffset" : 60,
      "endOffset" : 78
    } ],
    "year" : 0,
    "abstractText" : "Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones. Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. However, these memory-based methods tend to overfit the memory samples and perform poorly on imbalanced datasets. To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory. Specifically, supervised contrastive learning based on a memory bank is first used to train each new task so that the model can effectively learn the relation representation. Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task. The proposed method can better learn consistent representations to alleviate forgetting effectively. Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-theart baselines and yield strong robustness on the imbalanced dataset.",
    "creator" : null
  }
}
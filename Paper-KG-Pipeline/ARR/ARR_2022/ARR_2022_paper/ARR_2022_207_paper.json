{
  "name" : "ARR_2022_207_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Mediate Disparities Towards Pragmatic Communication",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In human communication, speakers often adjust language production by taking into consideration listeners’ personality, background knowledge, perceptual or physical capabilities etc (Clark, 1996; Bara et al., 2021). Recent years have seen an increasing amount of work that explores pragmatic reasoning based on Rational Speech Act (RSA)(Andreas and Klein, 2016; Fried et al., 2018; Fried et al.; White et al.; Cohn-Gordon et al., 2018), multi-agent emergent communication framework (Lazaridou et al., 2020; Lazaridou and Baroni, 2020), and Theory of Mind in communication (Zhu\n(a) Target (b) Distractor.\nLiteral Speaker: e.g., There is an owl on the table. Rational Speaker: e.g., There is a pizza on the table. Listener’s Disparity: understands hypernym of food only. Pragmatic Rational Speaker: e.g., There is food on the table.\nFigure 1: Given two images, the speaker generates a description for the target and ask the listener to pick out the image described. Both players win if the listener picks the correct one. In this example, a Literal Speaker could generate any captions that suit the target, such as the one above, whereas a Rational Speaker will only describe the unique features of the target (e.g. pizza). If the listener only understands the hypernym of food (disparity), a Pragmatic Rational Speaker would use food instead of pizza to help listener understand.\net al., 2021). However, most previous works assume that the listeners and the speakers have the same background knowledge and capabilities, including vocabulary size, visual access, and relative locations. This assumption is a great simplification of real-world communication where speakers and listeners often have various types of disparities.\nTo address this limitation, this paper extends the Rational Speech Act (RSA) (Frank and Goodman, 2012) model towards rational agents learning to adapt behaviors based on their experience with the listener. The design choice of our model is inspired by the human cognitive system (Cowan, 2008; Wardlow, 2013) where a limited capacity working memory is built on top of the long-term memory to adjust the output to be task and working environment specific. Each conversation is a modification on the long-term belief (Reed, 2012) with situation-specific factors. In our framework, we fix the long-term memory for the Literal Speaker, and introduce a light-weighted working memory (Miyake and Shah, 1999) for the Pragmatic Ratio-\nnal Speaker to accommodate two goals: 1) a task goal which retrieves relevant information from the long-term memory to accomplish the specific task, and 2) a disparity goal which learns and adjusts language production to accommodate the listener’s disparity through interactions and reinforcement learning. We separate each component as they are independent of each other in utility, and that they can be easily switched and adapted for new tasks and new environment.\nDifferent from previous works which only demonstrate how learned models task performance (e.g. (Shridhar et al., 2021; Zhu et al., 2021; Corona et al., 2019)), one of our goals is to also provide transparency on what models have indeed learned towards the end goal. It’s well established that endto-end neural models can often take advantage of spurious data bias to gain end task performance. Models that only report end task measure without “showing their internal works” would not be sufficient to tell the whole story about models’ abilities. To serve this goal, we situated this investigation in the context of a referential game, as shown in Figure 1. We carefully curated a dataset to simulate two types of disparity: vocabulary limitation and impaired visual access. Our empirical results demonstrate that our model is able to significantly improve the collaborative game performance by shifting communication towards the language that the listeners with disparities are able to understand. In addition, our results show that separating working memory from long-term memory leads to faster learning and better performance than the previous model that conducted joint end-to-end learning.\nOur contributions are the following. 1) Following human cognition, we demonstrate the benefits of separating working memory from the long-term memory, compared to end-to-end joint training. 2) We proposed a new dataset to simulate multiple distinct categories of disparities, and demonstrate the generalizability of our model. 3) To the best of our knowledge, we are among the first to demonstrate model’s strong language shift capability to actually accommodate listener’s disparities instead of a simple end task performance measure. The dataset and code will be made available to facilitate future work that addresses pragmatics and theory of mind in language interpretation and generation."
    }, {
      "heading" : "2 Related Work",
      "text" : "Rational Speech Act (RSA)\nOur work is inspired by (Andreas and Klein, 2016) and (Lazaridou et al., 2020; Lazaridou and Baroni, 2020), where the same referential game setup was used to propose a rational speaker that learns to reason the collaborative game and to produce natural sounding image captions based on the Rational Speech Act model.\nThe Rational Speech Act (RSA) model (Frank and Goodman, 2012) is a is a probabilistic model for the speakers and listeners to pragmatically reason about each other’s intention. In the context of a referential game (Monroe and Potts, 2015), for example, given an image m, it starts with a literal speaker S0 to generate caption c: PS0(c|m).\nA rational listener L1 reasons about the literal speaker’s (S0) strategy and picks the best image that matches the description. A rational speaker S1 then takes the rational listener’s (L1) strategy into account and produces a caption c that maximize the collaborative game goal.\nPL1(m|c) ∝ PS0(c|m) · P (m)\nPS1(c|m) ∝ PL1(m|c) · P (c)\nThe previous works use the RSA model to achieve the task goal, and assume that the speaker and listener have the exact same capabilities and knowledge background, which is unrealistic in the real world. In our work, we created listeners with disparity d and extend this model for the speaker to accommodate both the task and disparities goals.\nEmergent Communication\nOur work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019). These works involve reinforcement-based learning to reach a common goal in a collaborative game. Our work is inspired by these for the Pragmatic Rational Speaker to realistically learn the listener’s disparities through communication interactions, without any prior knowledge on the listener’s background nor an oracle access to probe the listener’s brain.\nWorking Memory\nWorking memory (also short-term memory) is used in neuropsychology and cognitive science (Cowan,\n2008; Miyake and Shah, 1999) to refer to the memory that controls attention, plans and carries out behavior. It is a combination of multiple components, including the contribution of long-term memory (Reed, 2012; Sawangjit et al.) and situationspecific task processing (Funahashi, 2017).\nThe classical artificial intelligence work such as ACT (Heise and Westermann, 1989) and SOAR (Laird et al., 1987) also incorporated the concept of working memory to model human short-term memory. The similar concept has been used in recent work such as (Hermann et al., 2017; Hill et al., 2019). Out work is a novel application of the working memory concept to pragmatically adjustment communication for speaker-listener disparities, and take advantage of the internal simulation architecture to achieve the task-specific goal.\nTheory of Mind\nIt has been studied (Leung et al., 2021; Stephens et al., 2010; Wardlow, 2013) in psychology and neuroscience that human speakers adjust the way how they speak for successful communications after learning the listener’s disparity. A few recent works (Zhu et al., 2021; Corona et al., 2019; Hawkins et al., 2021) attempt to address similar questions in artificial intelligence and statistical modeling. (Zhu et al., 2021; Corona et al., 2019)’s work used metalearning to study the disparity within the same family (e.g. different lexical levels of multi-linguistic learning), whereas our model is able to generalize to different distinct types of disparities. Previous works were also trained in an end-to-end joint fashion, learning both the task goal and the disparity goal together. Our work demonstrates the benefits of working memory and separate training. Most importantly, few of the previous work were able to showcase model’s language capabilities and only evaluate them by the end task performance (e.g. accuracy), whereas our work emphasizes on evaluating how the models actually learn to shift the language towards the communication goal on top of the task accuracy."
    }, {
      "heading" : "3 Dataset",
      "text" : "There are many levels of disparities during verbal communication (Stephens et al., 2010), including phonetic, lexical, grammatical, semantic representations, etc. In our work, we focus on lexical and semantic, by challenging the speaker model to switch words, or pick alternative objects\nto describe for the conversation. We assembled two datasets to create two types of disparities: 1) background knowledge, and 2) visual limitations. These datasets are used to train the listeners with disparities as described in Section 4.2. Detailed dataset components can be found in the Appendix.\nThe knowledge background disparity is simulated through the hypernym dataset, where the listener only understands the hypernym for all the objects (e.g. “food” instead of “pizza”), whereas the speaker understands both. This dataset challenges the speaker model at the lexical level to learn what words the listener doesn’t understand, and shift towards the words that they do.\nThe visual disparity is simulated through the limited visual dataset, where the listener has impaired visual or some objects were physically blocked from the eyesight. This dataset challenges the speaker model at the semantic level to shift attention and pick the objects that are visible to the listener to describe. For control and analysis purposes, we remove all the animal-related objects and words from listener’s visual and knowledge access.\nThe dataset used in our experiments is Abstract Scenes (Gilberto Mateos Ortiz et al., 2015) with 10020 images. We split the data into training, validation and testing sets by a ratio of 8:1:1. Each image includes 3 ground truth captions, and a median of 6 to 7 objects. For control, we assembled ∼35k pairs of images that differ by ≤ 4 objects as the Hard set, ∼25k pairs that differ by > 4 objects as the Easy set, and together as the Combined set."
    }, {
      "heading" : "4 Method",
      "text" : "Given a pair of images m0,m1, the target image indicator t ∈ {0, 1}, and the listener’s disparity d, the goal for the Pragmatic Rational Speaker is to retrieve a list of candidate captions C from the long-term memory, output the best caption c in the working memory, that accommodates both 1) task goal: describes the unique features of the target image apart from the distractor, and 2) disparity goal: learns and accommodates the listener’s disparity.\nFollowing the RSA model(Figure 2), we start by introducing the Literal Speaker S0 that generates candidate captions c for a given imagem (Equation 1), which serves as the long-term memory for the Rational Speaker. We then present the Rational Listener L1 that picks out an image as the target given speaker’s description (Equation 2). The task goal is achieved by the vanilla Rational Speaker S1,\nwhich simulates the listener’s mind internally in its working memory (Equation 3). The Pragmatic Rational Speaker Sd1 adds a light-weight disparity adjustment layer (Equation 5) to learn and accommodate listener’s disparity through interaction, and achieves both goals. Each component can be easily switched and adapted to new tasks or environment.\nS0 : P (c|mt) (1)\nL1 : P (t|m0,m1, c) ∝ PS0(c|mt) · P (mt) (2)\nS1 :P (c|m0,m1, t) ∝ PL1(t|m0,m1, c) · P (c|m0,m1)\n(3)\nLd1 :P (t|m0,m1, c, d) ∝ PS1(c|m0,m1, t, d) · P (t|m0,m1, d)\n(4)\nSd1 :P (c|m0,m1, t, d) ∝ PLd1 (t|m0,m1, c, d) · P (c|m0,m1, d) (5)"
    }, {
      "heading" : "4.1 Literal Speaker S0",
      "text" : "The Literal Speaker S0 (Figure 2) is an object detection based image captioning module that generates caption candidates for the target image.\no1, . . . , ok, b1, . . . , bk = ObjDet(mt)\ne1, . . . , ek = WordEmb(o1, . . . , ok)\nc1, . . . , cn = Transformer(e1, . . . , b1, . . . )\n(6)\nFor a given target image mt, since it’s important to ground words to the scenes in order to control the disparities in vocabularies, we applied the object detector YOLO3 (Redmon and\nFarhadi, 2018) to extract a list of k detected objects O = {o1, o2, . . . , ok}, and their corresponding bounding boxes B = {b1, b2, . . . , bk}. Each image chooses at most max_obj = 9 detected objects, and the names of each were embedded with a pre-trained BERT (Devlin et al., 2019) word embedding E = {e1, e2, . . . , ek}. These embeddings are then concatenated with their bounding box locations, and sent to the Transformer Decoder to generate beam_size = 30 candidate captions C = {c1, c2, . . . , cn} for each target image.\n4.2 Rational Listener (L1)\nWithout disparity concerns, the Rational Listener picks out the image that they believe is the target.\ng0 = FT_Transformer(m0, c)\ng1 = FT_Transformer(m1, c)\nt = argmaxi∈{0,1}CosSim(gi, c)\n(7)\nRecall that S0 used a Transformer module to connect the image and its corresponding captions. We reuse the same fixed pre-trained training-mode Transformer module (named FT_Transformer) to decide which image does the caption ground better in. Adopting the idea of teacher-forcing language training, the output (gi) of FT_Transformer with an input pair (mi, c) should closely resemble the original input c if the input image mi is indeed the one used to generate the caption c. By calculating the cosine similarity of each (gi, c) pair, the image that grounds better (higher CosSim) in the description would be chosen as the target.\nThis module allows the agents to quickly and\naccurately make the decisions without further training. In theory, if the speaker and the listener were to have the exact same brain (same model and weights), the performance of this task should approach 100%. The results of “No Disparity” speaker in Figure 3 confirmed the design choice.\n4.3 Rational Speaker (S1) Without disparity concerns, the Rational Speaker (S1) fulfills the task goal by simulating (Figure 2) the Rational Listener (L1)’s behavior, and rank the candidate captions generated by the Literal Speaker (S0) according to how well they can describe the target image apart from the distractors. This design is under the fair assumption that both speakers and listeners are aware of the collaborative game goal, but can be switched for other task purposes.\nFor i ∈ {0, · · · , n},where n = |C| : ti, pi = Simulate_L1(m0,m1, ci)\nc = cargmaxi[[ti==t∗]]·pi\n(8)\nGiven an image pair (m0,m1), and a list of candidate captionsC = {c1, · · · , cn} generated by S0, the Rational Speaker goes through each caption ci and simulate how well the listener (Simulate_L1) would pick out the correct target image. If a candidate caption ci helps the simulator pick out the correct target image (i.e. ti == t∗) with high confidence (pi), then it will be chosen as the final caption sent over the the actual listener. The simulated listener shares the same architecture as L1 and initializes the weights pre-trained from S0. By doing so, the Rational Speaker takes the listener’s intention into account and achieves the task goal.\n4.4 Listener with Disparities (Ld1) In real-world communications, however, it is hardly the case that different agents have the exact same knowledge background, experiences, physical capabilities, etc. The listener’s decision making process is influenced by various kinds of disparities d.\nTo study speaker’s ability of situated language adjustment, we created two representative types\nof listeners with different knowledge background and visual capabilities by training different caption grounding modules (FT_Transformer) with the datasets assembled in Section 3. These disparities would challenge the speaker model to adjust the language at both lexical and semantic levels.\n1. Ld11 : Hypernym. With limited vocabulary and knowledge in a certain domain, people tend to refer to objects in their hypernym form (e.g. “animal” instead of “cat”). In this experiment, we create listeners that would refer to all the detected objects by their hypernyms. This disparity would require the speaker to switch individual words that share similar meanings.\n2. Ld21 : Limited Visual. Due to the physical orientation or impaired vision capability, it is likely that some objects are blocked or hardly visible to one party but not the other. In this experiment, we remove all the animal objects from listener’s visual detected object list (O), and replace the relevant descriptions with the special token ’[UNK]’. This disparity would require the speaker to shift attention, and choose alternative objects to describe.\nWe investigate in listeners with narrower capabilities than the speakers under the argument that in the opposite case, the listener could use only a subset of the knowledge to achieve best performance without having the speakers to adjust the speech. Other disparities can be inferred through transfer learning or are left for further investigation with broader information access and datasets.\n4.5 Pragmatic Rational Speaker (Sd1 ) On top of the Rational Speaker (S1), the Pragmatic Rational Speaker incorporates a disparity adjustment layer to learn and accommodate the listener’s disparity through emergent communication.\nFor i ∈ {0, · · · , n},where n = |C| : qi = MLP(SentenceEmb(ci)) ai = [[ti == t ∗]] · pi · qi\nc = cargmaxiai\n(9)\nWe use a pretrained BERT model to embed each candidate caption ci, add a single MLP layer, and approximate the REINFORCE policy through Equation 9. The reward(rc) for each final caption c is +1 when the listener picks out the correct target image, and −1 otherwise. The loss is calculated for all the captions across each batch (Equation 10)\nL = − ∑ c log(ac) · rc (10)"
    }, {
      "heading" : "4.6 Communication with Words",
      "text" : "We also conducted the same sets of experiments using individual words (object names) instead of sentences in the referential game to demonstrate the effects of working memory disparity accommodation and internal task simulation, reducing the noise that came from the imperfection of the image description generator. The simplified pipeline uses the detected object name embedding for disparity adjustment, and the listener picks the target images by conducting simple word matching."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "We evaluate our models along three dimensions: End-task Performance, Efficiency, and Transparency. Implementation details, qualitative examples, and more experiment results can be found in Appendix.\n1. [Task Performance] that measures overall accuracy of the collaborative game. Task performance is often the sole evaluation metrics in previous work.\n2. [Efficiency] that measures time used for model training across tasks.\n3. [Transparency] that uncovers the underlying distribution shift of vocabulary use learned to accommodate different types of disparities."
    }, {
      "heading" : "5.1 Task Performance Comparison",
      "text" : "To assess the performance of the rational speaker in the collaborative game, Figure 3 presents the task accuracies with Literal Speaker (S0), Rational Speaker (S1), Pragmatic Rational Speaker (Sd1 ), and No Disparity, where it has the same structure as the Rational Speaker (S1) plus the same type of disparity as the corresponding listener. This serves as the upper bound of performance. The same experiments were conducted at the word level, too.\nFor each type of listener disparity, the performance is S0 << S1 < Sd1 < No Disparity. The vanilla Rational Speaker (S1) improved the overall performance from Literal Speaker by over 25% because it is achieving the task goal to describe the target image apart from the distractor. The Pragmatic Rational Speaker (Sd1 ) is able to learn and\nadjust for the corresponding type of listener disparity, and further improve the game performance by ∼10%. There is still, however a gap between Sd1 and the upper bound No Disparity, where the speaker and the listener have the exact same kind of knowledge limitation and capabilities, potentially due to the imperfection in caption generations.\nBreaking down between the hard, easy datasets in Table 2, the Pragmatic Rational Speaker (Sd1 ) in the easy dataset is able to improve a lot more against its corresponding Rational Speaker (S1) than the one trained in the hard dataset. The gap between Sd1 and No Disparity is also a lot smaller for the easy dataset trained model. This is likely because when a pair of images have a lot more objects that differ (i.e. easier), the model has more options to adjust upon, hence the higher performance.\nCompared to the sentence level model, the word level pragmatic speaker achieves even higher improvement against the corresponding Rational Speaker, especially for Ld11 . They achieve almost perfect accuracy with close to zero gap to the upper bound. This suggests the high potential of the disparity adjustment design, especially after reducing the caption generation and interpretation noise."
    }, {
      "heading" : "5.2 Learning Efficiency",
      "text" : "To study the training efficiency of the working memory, we compared our model to the “MultiTask leaning” model described in (Lazaridou et al., 2020)’s work, where the image captioning model and the REINFORCE learning are joint trained through a combined loss function:\nL = λfL functional + λsL structural\nFunctional refers to the REINFORCE learning to achieve both task and disparity goals (evaluated by Accuracy), and structural refers to the caption generation loss for natural-sounding language (evaluated by BLEU4). We used λf = λs = 1 as in previous work for our experiments.\nDetailed training and comparison strategies can be found in the Appendix. Table 3 shows that for each type of listener disparity, our model separating the working memory from the long-term memory is able to achieve higher accuracy and higher BLEU4 score than the joint training. Moreover, the Joint Trained model needs to retrain all the weights for each type disparity, whereas our model only needs to train the long-term memory once, and retrain the light weighted working memory for each type of disparity, which is much more efficient."
    }, {
      "heading" : "5.3 Transparency: Vocabulary Adjustment",
      "text" : "To gain insights in whether the Pragmatic Rational Speaker is actually adjusting the descriptions for listeners’ disparities or taking the advantage of statistical bias to achieve higher task performance, we plotted the word distribution shift across different types of disparities. More qualitative examples can be found in the Appendix. For each experiment, the word frequencies of all the chosen captions were calculated for the Rational Speakers, the Pragmatic Rational Speakers, and Joint Training. We collected the top choice of each speaker per image pair, repeated the experiments 3 times, and reported the mean and standard deviation in Figure 4.\nIn the Hypernym disparity (Figure 4a) experiment, where the listener only understands the hypernym of detected objects, the lower-case words on the left are the top detected object names, and the upper-case words on the right are hypernyms. On the left side, the word frequencies of the Pragmatic Rational Speaker significantly dropped from the Rational Speaker. On the right side, the model is maintaining similar level, or using some of the hypernyms more frequently (y-axis in log scale). Note that the Rational Speaker can generate both hypernym and hyponym descriptions without aware of disparities, and there are multiple valid captions available for all speakers to choose from (e.g.“The girl sits on the ground”). For the Joint Trained Speaker, we also observed a hyponym usage drop(left), but it’s unclear how they are accommodating the disparity without using hypernyms. This result shows that the model learned to avoid using hyponyms, and replace them with their hypernym forms to accommodate the disparity.\nFor the Limited Visual disparity (Figure 4b), since all the animal objects are missing for the listener, there is a sharp decline in Sd21 ’s use of animal related words during the communication. Instead, it is choosing other objects such as “hat”, and “ball” to describe the target image. The Pragmatic Rational Speaker is accommodating listener Ld21 ’s disparity by semantically shifting the attention and choosing alternative objects other than animals to communicate. The behavior of the Joint Trained Speaker, however, is harder to interpret."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this work, we present a novel framework based on the Rational Speech Act framework for pragmatic communication that can adjust the conversation content according to listener’s disparities by adding a light-weighted working memory on top of speaker’s long-term memory. The Pragmatic Rational Speaker significantly improved the collaborative game performance by shifting the conversation towards the language that the listeners are able to understand. The flexibility and training efficiency also makes it easy to be applied broadly. We believe this work can be greatly helpful for future situated language generation and adjustment.\nThere are, however, several limitations that requires future work and further investigation.\nOne of the limitations is the image captioning module, and therefore, the literal speaker and listener models. More specifically, caption generation doesn’t 100% resemble the image due to complicated object interactions, and the top choices may\nnot cover all the word replacements. Pragmatic inference models are built on top of great functioning base models, and the performance of a base model could affect the outcome of the pragmatic model.\nWe conducted our experiments in a relative simple and artificial environment with the purpose of easy control and demonstration. We emphasize on evaluating model’s actual language ability of adjusting for the disparities on top of task performance. The next step would be to apply the framework to more realistic images and interactive environment.\nOther than listener’s capabilities, there are a lot of reasons for a conversation to be adjusted, such as the physical environment, relative positions, speaker’s personalities, etc. Studying how a rational agent can accommodate these disparities would require additional multimodal datasets, and information processing methods.\nAt the moment, the Pragmatic Rational Speaker trains a new layer in working memory from scratch for each type of disparity. This could have backward influence on the long-term memory. In lifelong learning (Parisi et al., 2019) like human, the working memory can shape their long-term memory. At the very least, the model could store each learned disparity adjustments for future encounter. This modification is left for future work.\nLast but not least, instead of training for every single type of disparity to name, human learners have the ability of meta-learning and zero-shot transferring existing knowledge to a new category. Future work on pragmatic reasoning should be easily adaptable to different disparities and situations."
    }, {
      "heading" : "A Speaker and Listener Model Architecture Breakdown",
      "text" : "(a) Literal Speaker S0: for each input image, we run YOLO3 object detector to get a list of detected object names. Each name is embedded with pre-trained BERT embedding, and concatenated with their bounding box location. The embedded images goes through a Transformer Decoder to generate a list of candidate captions.\n(b) Rational Listener L1: for each pair of images and an input caption, the Rational Listener reuses a pre-trained Transformer Decoder as in S0 to figure out which image does the caption ground better in. Inspired the teacher-forcing caption training procedure, given an image and the input caption, if it generates a sentence that’s closer to the input caption than the other image, then this image is chosen as the target.\n(c) Rational Speaker S1: for a pair of images, and a list of candidate captions generated by S0, the Rational Speaker goes through each candidate caption via the internal simulated listener (same model as L1 with no disparity), to figure out whether the caption can help the listener pick the correct target image, and if so, how confident. It ranks all the captions by the correctness and confidence score.\n(d) Pragmatic Rational Speaker Sd1 : given a list of ranked (by S1) candidate captions, the Pragmatic Rational Speaker picks the most confident one and send it to the actual listener with disparities (Ld1), and receives a reward feedback. This feedback helps Sd1 to learn the disparity, and rerank all the captions to accommodate the difference and optimize for the task goal."
    }, {
      "heading" : "B Qualitative Examples",
      "text" : "Qualitative examples for each disparity adjustment can be found in Figure 6 and 7. The orange words are the vocabulary that the Pragmatic Rational Speaker is avoiding, and the blue words are the preferred alternative for the listeners. The strikethrough sentences are discarded because they can be used to describe both images.\nIn Figure 6, the Pragmatic Rational Speaker is avoiding the specific object names, such as “Jenny”, “football”, and “dog”, and choosing the hypernyms “Girl” and “toys”.\nIn Figure 7, the Pragmatic Rational Speaker is avoiding animal related words to accommodate Listener’s disparity, and chose the caption “Jenny is sitting on the ground” instead.\n(a) Target (b) Distractor.\nCandidate Captions: - Jenny is throwing a football - The dog is looking at Jenny - The dog is watching - Jenny is waving at boy - Boy is playing football - Girl is throwing the toys - Jenny is throwing the toys in the sky object - The dog is standing by a toy\nChosen Caption: Girl is throwing the toys\nChosen Caption: Jenny is sitting on the ground\nFigure 7: Ld21 : Limited Vision\nC Implementation Details\nWe pretrained the image captioning models using 2 layers of Transformer Decoder with 4 attention heads each, and 512 in internal dimension for 100 epochs each. The dropout rate was 0.5, learning rate started at 1e−4, on a scheduled decline rate of 0.8 for each 20 unimproved epochs.\nWe also pretrained the literal listeners and the literal speaker with different disparity datasets. All the weights are fixed before being integrated into the interactive learning phase. During disparity learning, each pair of speaker and listener were trained for 150 epochs, with batch size of 128, learning rate starting at 1e−3, and on decline at the rate of 0.8 per 20 unimproved epoches. Each experiment is repeated 3 times. The mean and standard deviation were reported in figures. Similarly in the word level training, the model was trained for 200 epochs, and with learning rate starting at 2, and on a scheduled decline rate of 0.8 for each 50 unimproved epochs.\nFor the efficiency comparison experiment, we used the combined test dataset for this experiment, trained each component until 50 unimproved epochs, and selected the top performances within the first 30 minutes of each. All models have reached stable performance by then. All experiments done on a single NVIDIA(R) GeForce(R) RTX 2070 SUPER(TM) 8GB GDDR6 and 10th Gen Intel(R) Core(TM) i9-10900K processor."
    }, {
      "heading" : "D Balancing Between Task and Disparity Goal",
      "text" : "ai = ([[ti == t ∗]] · pi)λl · (qi)λd (11)\nThe working memory of the Pragmatic Rational Speaker (Sd1 ) has two two goals: 1) Task Goal: an internal simulation of a listener to rank the candidate captions by their uniqueness in describing only the target image, and 2) Disparity Goal: a disparity adjustment layer to learn and accommodate the listener’s disparity through reinforcement interactions. Each goal component can be formalized in the above two terms (Recall Equation 9). We parameterized each term with λl and λd to study how different λl : λd weight ration could affect rational speaker’s ability to achieve both goals.\nFigure 8 shows that when the Pragmatic Rational Speaker puts a high emphasis on adjusting the listener’s disparity λd, it would “forget” to describe the unique characters of the target image and lower\nthe overall performance. On the other hand when the Rational Speaker emphasize too much on the task goal, it would “forget” to accommodate listener’s disparities, and lower the overall performance as well. In the end, we chose λl : λd = 1 : 1 for all experiments demonstrated above."
    } ],
    "references" : [ {
      "title" : "Reasoning about pragmatics with neural listeners and speakers",
      "author" : [ "Jacob Andreas", "Dan Klein." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182, Austin, Texas. Association for Compu-",
      "citeRegEx" : "Andreas and Klein.,? 2016",
      "shortCiteRegEx" : "Andreas and Klein.",
      "year" : 2016
    }, {
      "title" : "MindCraft: Theory of mind modeling for situated dialogue in collaborative tasks",
      "author" : [ "Cristian-Paul Bara", "Sky CH-Wang", "Joyce Chai." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1112–1125,",
      "citeRegEx" : "Bara et al\\.,? 2021",
      "shortCiteRegEx" : "Bara et al\\.",
      "year" : 2021
    }, {
      "title" : "Using Language",
      "author" : [ "Herbert H. Clark." ],
      "venue" : "’Using’ Linguistic Books. Cambridge University Press.",
      "citeRegEx" : "Clark.,? 1996",
      "shortCiteRegEx" : "Clark.",
      "year" : 1996
    }, {
      "title" : "Pragmatically informative image captioning with character-level inference",
      "author" : [ "Reuben Cohn-Gordon", "Noah Goodman", "Christopher Potts" ],
      "venue" : null,
      "citeRegEx" : "Cohn.Gordon et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Cohn.Gordon et al\\.",
      "year" : 2018
    }, {
      "title" : "Modeling conceptual understanding in image reference games",
      "author" : [ "Rodolfo Corona", "Stephan Alaniz", "Zeynep Akata." ],
      "venue" : "CoRR, abs/1910.04872.",
      "citeRegEx" : "Corona et al\\.,? 2019",
      "shortCiteRegEx" : "Corona et al\\.",
      "year" : 2019
    }, {
      "title" : "Chapter 20 what are the differences between long-term, short-term, and working memory? In Wayne S",
      "author" : [ "Nelson Cowan." ],
      "venue" : "Sossin, Jean-Claude Lacaille, Vincent F. Castellucci, and Sylvie Belleville, editors, Essence of Memory, volume 169 of Progress",
      "citeRegEx" : "Cowan.,? 2008",
      "shortCiteRegEx" : "Cowan.",
      "year" : 2008
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to communicate with deep multi-agent reinforcement learning",
      "author" : [ "Jakob Foerster", "Ioannis Alexandros Assael", "Nando de Freitas", "Shimon Whiteson." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.",
      "citeRegEx" : "Foerster et al\\.,? 2016",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting pragmatic reasoning in language games",
      "author" : [ "Michael C. Frank", "Noah D. Goodman." ],
      "venue" : "Science, 336(6084):998–998.",
      "citeRegEx" : "Frank and Goodman.,? 2012",
      "shortCiteRegEx" : "Frank and Goodman.",
      "year" : 2012
    }, {
      "title" : "Unified pragmatic models for generating and following instructions",
      "author" : [ "Daniel Fried", "Jacob Andreas", "Dan Klein." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Fried et al\\.,? 2018",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2018
    }, {
      "title" : "Working memory in the prefrontal cortex",
      "author" : [ "S. Funahashi." ],
      "venue" : "Brain Sci, 7(5).",
      "citeRegEx" : "Funahashi.,? 2017",
      "shortCiteRegEx" : "Funahashi.",
      "year" : 2017
    }, {
      "title" : "Learning to interpret and describe abstract scenes",
      "author" : [ "Luis Gilberto Mateos Ortiz", "Clemens Wolff", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Ortiz et al\\.,? 2015",
      "shortCiteRegEx" : "Ortiz et al\\.",
      "year" : 2015
    }, {
      "title" : "From partners to populations: A hierarchical bayesian account of coordination and convention",
      "author" : [ "Robert D. Hawkins", "Michael Franke", "Michael C. Frank", "Adele E. Goldberg", "Kenny Smith", "Thomas L. Griffiths", "Noah D. Goodman" ],
      "venue" : null,
      "citeRegEx" : "Hawkins et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hawkins et al\\.",
      "year" : 2021
    }, {
      "title" : "Anderson’s Theory of Cognitive Architecture (ACT*), pages 103–127",
      "author" : [ "Elke Heise", "Rainer Westermann." ],
      "venue" : "Springer Berlin Heidelberg, Berlin, Heidelberg.",
      "citeRegEx" : "Heise and Westermann.,? 1989",
      "shortCiteRegEx" : "Heise and Westermann.",
      "year" : 1989
    }, {
      "title" : "Understanding early word learning in situated artificial agents",
      "author" : [ "Felix Hill", "Stephen Clark", "Karl Moritz Hermann", "Phil Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Hill et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural language does not emerge ‘naturally’ in multi-agent dialog",
      "author" : [ "Satwik Kottur", "José Moura", "Stefan Lee", "Dhruv Batra." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2962–2967,",
      "citeRegEx" : "Kottur et al\\.,? 2017",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2017
    }, {
      "title" : "Soar: An architecture for general intelligence",
      "author" : [ "John Laird", "Allen Newell", "Paul Rosenbloom." ],
      "venue" : "Artificial Intelligence, 33:1–.",
      "citeRegEx" : "Laird et al\\.,? 1987",
      "shortCiteRegEx" : "Laird et al\\.",
      "year" : 1987
    }, {
      "title" : "Emergent multi-agent communication in the deep learning era",
      "author" : [ "Angeliki Lazaridou", "Marco Baroni" ],
      "venue" : null,
      "citeRegEx" : "Lazaridou and Baroni.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lazaridou and Baroni.",
      "year" : 2020
    }, {
      "title" : "Multi-agent communication meets natural language: Synergies between functional and structural language learning",
      "author" : [ "Angeliki Lazaridou", "Anna Potapenko", "Olivier Tieleman." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Lazaridou et al\\.,? 2020",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2020
    }, {
      "title" : "Parents fine-tune their speech to children’s vocabulary knowledge",
      "author" : [ "Ashley Leung", "Alexandra Tunkel", "Daniel Yurovsky." ],
      "venue" : "Psychological Science, 32(7):975–984. PMID: 34212788.",
      "citeRegEx" : "Leung et al\\.,? 2021",
      "shortCiteRegEx" : "Leung et al\\.",
      "year" : 2021
    }, {
      "title" : "Models of Working Memory: Mechanisms of Active Maintenance and Executive Control",
      "author" : [ "Akira Miyake", "Priti Shah." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Miyake and Shah.,? 1999",
      "shortCiteRegEx" : "Miyake and Shah.",
      "year" : 1999
    }, {
      "title" : "Learning in the rational speech acts model",
      "author" : [ "Will Monroe", "Christopher Potts." ],
      "venue" : "CoRR, abs/1510.06807.",
      "citeRegEx" : "Monroe and Potts.,? 2015",
      "shortCiteRegEx" : "Monroe and Potts.",
      "year" : 2015
    }, {
      "title" : "Continual lifelong learning with neural networks: A review",
      "author" : [ "German I. Parisi", "Ronald Kemker", "Jose L. Part", "Christopher Kanan", "Stefan Wermter." ],
      "venue" : "Neural Networks, 113:54–71.",
      "citeRegEx" : "Parisi et al\\.,? 2019",
      "shortCiteRegEx" : "Parisi et al\\.",
      "year" : 2019
    }, {
      "title" : "2018. Yolov3: An incremental improvement",
      "author" : [ "Joseph Redmon", "Ali Farhadi" ],
      "venue" : null,
      "citeRegEx" : "Redmon and Farhadi.,? \\Q2018\\E",
      "shortCiteRegEx" : "Redmon and Farhadi.",
      "year" : 2018
    }, {
      "title" : "Human Cognitive Architecture, pages 1452–1455",
      "author" : [ "Stephen K. Reed." ],
      "venue" : "Springer US, Boston, MA.",
      "citeRegEx" : "Reed.,? 2012",
      "shortCiteRegEx" : "Reed.",
      "year" : 2012
    }, {
      "title" : "Bootstrapping a neural conversational agent with dialogue self-play, crowdsourcing and on-line reinforcement learning",
      "author" : [ "Pararth Shah", "Dilek Hakkani-Tür", "Bing Liu", "Gokhan Tür." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Shah et al\\.,? 2018",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2018
    }, {
      "title" : "Alfworld: Aligning text and embodied environments for interactive learning",
      "author" : [ "Mohit Shridhar", "Xingdi Yuan", "Marc-Alexandre Côté", "Yonatan Bisk", "Adam Trischler", "Matthew Hausknecht" ],
      "venue" : null,
      "citeRegEx" : "Shridhar et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Shridhar et al\\.",
      "year" : 2021
    }, {
      "title" : "Speaker–listener neural coupling underlies successful communication",
      "author" : [ "Greg J. Stephens", "Lauren J. Silbert", "Uri Hasson." ],
      "venue" : "Proceedings of the National Academy of Sciences, 107(32):14425–14430.",
      "citeRegEx" : "Stephens et al\\.,? 2010",
      "shortCiteRegEx" : "Stephens et al\\.",
      "year" : 2010
    }, {
      "title" : "Individual differences in speakers’ perspective taking: the roles of executive control and working memory",
      "author" : [ "Liane Wardlow." ],
      "venue" : "Psychon Bull Rev, 20(4):766– 772.",
      "citeRegEx" : "Wardlow.,? 2013",
      "shortCiteRegEx" : "Wardlow.",
      "year" : 2013
    }, {
      "title" : "Know what you don’t know: Modeling a pragmatic speaker that refers to objects of unknown categories",
      "author" : [ "Sina Zarrieß", "David Schlangen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 654–659, Flo-",
      "citeRegEx" : "Zarrieß and Schlangen.,? 2019",
      "shortCiteRegEx" : "Zarrieß and Schlangen.",
      "year" : 2019
    }, {
      "title" : "Few-shot language coordination by modeling theory of mind",
      "author" : [ "Hao Zhu", "Graham Neubig", "Yonatan Bisk." ],
      "venue" : "International Conference on Machine Learning, pages 12901–12911. PMLR.",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In human communication, speakers often adjust language production by taking into consideration listeners’ personality, background knowledge, perceptual or physical capabilities etc (Clark, 1996; Bara et al., 2021).",
      "startOffset" : 181,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "In human communication, speakers often adjust language production by taking into consideration listeners’ personality, background knowledge, perceptual or physical capabilities etc (Clark, 1996; Bara et al., 2021).",
      "startOffset" : 181,
      "endOffset" : 213
    }, {
      "referenceID" : 0,
      "context" : "Recent years have seen an increasing amount of work that explores pragmatic reasoning based on Rational Speech Act (RSA)(Andreas and Klein, 2016; Fried et al., 2018; Fried et al.; White et al.; Cohn-Gordon et al., 2018), multi-agent emergent communication framework (Lazaridou et al.",
      "startOffset" : 120,
      "endOffset" : 219
    }, {
      "referenceID" : 9,
      "context" : "Recent years have seen an increasing amount of work that explores pragmatic reasoning based on Rational Speech Act (RSA)(Andreas and Klein, 2016; Fried et al., 2018; Fried et al.; White et al.; Cohn-Gordon et al., 2018), multi-agent emergent communication framework (Lazaridou et al.",
      "startOffset" : 120,
      "endOffset" : 219
    }, {
      "referenceID" : 3,
      "context" : "Recent years have seen an increasing amount of work that explores pragmatic reasoning based on Rational Speech Act (RSA)(Andreas and Klein, 2016; Fried et al., 2018; Fried et al.; White et al.; Cohn-Gordon et al., 2018), multi-agent emergent communication framework (Lazaridou et al.",
      "startOffset" : 120,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : ", 2018), multi-agent emergent communication framework (Lazaridou et al., 2020; Lazaridou and Baroni, 2020), and Theory of Mind in communication (Zhu (a) Target (b) Distractor .",
      "startOffset" : 54,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : ", 2018), multi-agent emergent communication framework (Lazaridou et al., 2020; Lazaridou and Baroni, 2020), and Theory of Mind in communication (Zhu (a) Target (b) Distractor .",
      "startOffset" : 54,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "To address this limitation, this paper extends the Rational Speech Act (RSA) (Frank and Goodman, 2012) model towards rational agents learning to adapt behaviors based on their experience with the listener.",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "The design choice of our model is inspired by the human cognitive system (Cowan, 2008; Wardlow, 2013) where a limited capacity working memory is built on top of the long-term memory to adjust the output to be task and working environment specific.",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 28,
      "context" : "The design choice of our model is inspired by the human cognitive system (Cowan, 2008; Wardlow, 2013) where a limited capacity working memory is built on top of the long-term memory to adjust the output to be task and working environment specific.",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "Each conversation is a modification on the long-term belief (Reed, 2012) with situation-specific factors.",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "In our framework, we fix the long-term memory for the Literal Speaker, and introduce a light-weighted working memory (Miyake and Shah, 1999) for the Pragmatic Ratio-",
      "startOffset" : 117,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : "(Shridhar et al., 2021; Zhu et al., 2021; Corona et al., 2019)), one of our goals is to also provide transparency on what models have indeed learned towards the end goal.",
      "startOffset" : 0,
      "endOffset" : 62
    }, {
      "referenceID" : 30,
      "context" : "(Shridhar et al., 2021; Zhu et al., 2021; Corona et al., 2019)), one of our goals is to also provide transparency on what models have indeed learned towards the end goal.",
      "startOffset" : 0,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "(Shridhar et al., 2021; Zhu et al., 2021; Corona et al., 2019)), one of our goals is to also provide transparency on what models have indeed learned towards the end goal.",
      "startOffset" : 0,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Our work is inspired by (Andreas and Klein, 2016) and (Lazaridou et al.",
      "startOffset" : 24,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "Our work is inspired by (Andreas and Klein, 2016) and (Lazaridou et al., 2020; Lazaridou and Baroni, 2020), where the same referential game setup was",
      "startOffset" : 54,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "Our work is inspired by (Andreas and Klein, 2016) and (Lazaridou et al., 2020; Lazaridou and Baroni, 2020), where the same referential game setup was",
      "startOffset" : 54,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "The Rational Speech Act (RSA) model (Frank and Goodman, 2012) is a is a probabilistic model for the speakers and listeners to pragmatically reason about each other’s intention.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "In the context of a referential game (Monroe and Potts, 2015), for example, given an image m, it starts with a literal speaker S0 to generate caption c: PS0(c|m).",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 17,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 7,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 25,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 15,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 29,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 30,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 4,
      "context" : "Our work also draws from recent work in emergent communication (Lazaridou et al., 2020; Lazaridou and Baroni, 2020; Foerster et al., 2016; Shah et al., 2018; Kottur et al., 2017; Zarrieß and Schlangen, 2019; Zhu et al., 2021; Corona et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 246
    }, {
      "referenceID" : 10,
      "context" : ") and situationspecific task processing (Funahashi, 2017).",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The classical artificial intelligence work such as ACT (Heise and Westermann, 1989) and SOAR (Laird et al.",
      "startOffset" : 55,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "The classical artificial intelligence work such as ACT (Heise and Westermann, 1989) and SOAR (Laird et al., 1987) also incorporated the concept of working memory to model human short-term memory.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "The similar concept has been used in recent work such as (Hermann et al., 2017; Hill et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : "It has been studied (Leung et al., 2021; Stephens et al., 2010; Wardlow, 2013) in psychology and neuroscience that human speakers adjust the way how they speak for successful communications after learning the listener’s disparity.",
      "startOffset" : 20,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "It has been studied (Leung et al., 2021; Stephens et al., 2010; Wardlow, 2013) in psychology and neuroscience that human speakers adjust the way how they speak for successful communications after learning the listener’s disparity.",
      "startOffset" : 20,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : "It has been studied (Leung et al., 2021; Stephens et al., 2010; Wardlow, 2013) in psychology and neuroscience that human speakers adjust the way how they speak for successful communications after learning the listener’s disparity.",
      "startOffset" : 20,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : "A few recent works (Zhu et al., 2021; Corona et al., 2019; Hawkins et al., 2021) attempt to address similar questions in artificial intelligence and statistical modeling.",
      "startOffset" : 19,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "A few recent works (Zhu et al., 2021; Corona et al., 2019; Hawkins et al., 2021) attempt to address similar questions in artificial intelligence and statistical modeling.",
      "startOffset" : 19,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "A few recent works (Zhu et al., 2021; Corona et al., 2019; Hawkins et al., 2021) attempt to address similar questions in artificial intelligence and statistical modeling.",
      "startOffset" : 19,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "(Zhu et al., 2021; Corona et al., 2019)’s work used metalearning to study the disparity within the same family (e.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "(Zhu et al., 2021; Corona et al., 2019)’s work used metalearning to study the disparity within the same family (e.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : "There are many levels of disparities during verbal communication (Stephens et al., 2010), including phonetic, lexical, grammatical, semantic representations, etc.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "For a given target image mt, since it’s important to ground words to the scenes in order to control the disparities in vocabularies, we applied the object detector YOLO3 (Redmon and Farhadi, 2018) to extract a list of k detected objects O = {o1, o2, .",
      "startOffset" : 170,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "Each image chooses at most max_obj = 9 detected objects, and the names of each were embedded with a pre-trained BERT (Devlin et al., 2019) word embedding E = {e1, e2, .",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "Task leaning” model described in (Lazaridou et al., 2020)’s work, where the image captioning model and the REINFORCE learning are joint trained through a combined loss function:",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "In lifelong learning (Parisi et al., 2019) like human, the working memory can shape their long-term memory.",
      "startOffset" : 21,
      "endOffset" : 42
    } ],
    "year" : 0,
    "abstractText" : "Human communication is a collaborative process. Speakers, on top of conveying their own intent, adjust the content and language expressions by taking the listeners into account, including their knowledge background, personality, perceptual or physical capabilities. Towards building AI agents that have similar abilities in language communication, we propose a novel rational reasoning framework (Pragmatic Rational Speaker) where the speaker attempts to learn and adjust the content it communicates according to the disparities between itself and the corresponding listener, by adding a light-weighted disparity adjustment layer into working memory on top of speaker’s long-term memory system. By fixing longterm memory, the Pragmatic Rational Speaker only needs to update its working memory to learn and adapt to different types of listeners. To validate our framework, we create a dataset which simulates different types of speakerlistener disparities in the context of referential games. Our empirical results demonstrate that the Pragmatic Rational Speaker is able to shift its output towards the language that listeners are able to understand, significantly improve the collaborative task outcome, and learn the disparity faster than joint training.",
    "creator" : null
  }
}
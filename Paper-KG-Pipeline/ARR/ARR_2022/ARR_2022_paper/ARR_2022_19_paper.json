{
  "name" : "ARR_2022_19_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing reranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of highquality open cloze tests along with sample system output and human annotations that can serve as a future benchmark."
    }, {
      "heading" : "1 Introduction",
      "text" : "Open cloze tests are a common type of learning exercise where a student must complete a sentence by filling in a gap without any options to choose from. Designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005; 2011).\nIn this paper, we propose a strategy to construct open cloze exercises using transformer models (Vaswani et al., 2017). Our transformer-based architecture employs two objectives to predict the words that should be gapped in a text passage. One objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not. The other objective is a language-modelbased objective whereby we attempt to minimise the language model error when predicting the right answer for each gap. Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is fine-tuned on the two described objectives in a multi-task scenario.\nOur output aims to mimic the style of open cloze\ntests in the First Certificate in English (FCE) exam1, which is targeted at learners of English at the B2 proficiency level of the Common European Framework of Reference (CEFR) for languages (Council of Europe, 2001). Unlike other tests, the FCE open cloze task aims to simultaneously test many aspects of grammar and vocabulary that students are expected to know at this level. Since the tests are created from a text passage, they must be skilfully designed in order to ensure an optimal distribution of gaps that adheres to guidelines. A shortened example is shown in Figure 1. Our system is evaluated under two settings: 1) automatic evaluation, where the generated gaps are compared to goldstandard gaps proposed by test experts, and 2) human evaluation, where the quality of the generated gaps is judged by test experts.\nThe main contributions of our work are as follows: 1) we are the first to employ transformer models for open cloze test generation, 2) unlike previous studies, we work at the paragraph level, which is a much more challenging task, 3) we propose a multi-task learning approach with two objectives: one is to classify tokens into gaps/non-gaps and the other to minimise the error of re-generating the gapped word, 4) we report state-of-the-art results, outperforming previous work and a strong baseline, 5) we propose additional components to control the structure of the final cloze tests as human experts do, 6) we perform both automatic and human evaluation and 7) make our test data, system output and human annotations available."
    }, {
      "heading" : "2 Related Work",
      "text" : "While research into automatic cloze test generation is vast (Mostow et al., 2017; Kurdi et al., 2020; Yang et al., 2021), work on open cloze tests for language learning is scarce. Pino et al. (2008) generate open cloze questions using sample sen-\n1Now known as B2 First: https://www.cambridge english.org/exams-and-tests/first/\ntences from a learners’ dictionary based on four linguistic criteria: (grammatical) complexity, welldefined context (collocations), grammaticality and length. A later version of their system adds hints for gapped words (Pino and Eskenazi, 2009). Exercise Maker (Malafeev, 2014) is a rule-based open source system that attempts to emulate exercises in Cambridge English examinations based on the most frequently tested words. Most of the gaps it proposes were found to be useful and the automated exercises were hard to differentiate from authentic tests.\nChinkina et al. (2017) generate open cloze exercises for phrasal verbs by extracting sentences from news articles and generating a pair of questions and answers where the identified particle verbs are gapped. Similarly, Soonklang et al. (2017) gap words in sentences according to their part of speech in order to practise articles, prepositions, etc. Finally, Marrese-Taylor et al. (2018) use LSTMs to build sequence labelling and classification models that decide where to insert a single gap in a single sentence. Automatic evaluation against goldstandard gaps showed the method was effective.\nOther work has focused on creating automated cloze tests by controlling aspects of the proposed gaps so that they correlate with a target proficiency level. Lee et al. (2019), for example, manipulate the difficulty of C-tests (open cloze tests with hints, Grotjahn et al. (2002)) by varying the position and word length of the gaps. A similar concept is presented by Settles et al. (2020) and McCarthy et al. (2021), although difficulty is predicted using a machine-learning model that correlates with CEFR levels. In these cases, tests are dynamically adapted to the examinee’s proficiency level during the test session. From a different perspective, Felice and Buttery (2019) show that controlling gap entropy can be useful for designing open cloze tests at different CEFR levels. The work we present in this paper, however, aims to model the more complex task of predicting a full set of gaps at the paragraph level that comply with design and testing principles and is, to the best of our knowledge, the first to employ and adapt transformer-based models for this task.\nSystem evaluation is also challenging, since there is usually more than one potential word in the text that could constitute a good gap. While previous work often made a choice between automatic (Marrese-Taylor et al., 2018) or human evaluation (Malafeev, 2014; Das and Majumder, 2017) for their experiments, we perform both: automatic evaluation to identify the best models during development and human evaluation to measure test quality in the final output."
    }, {
      "heading" : "3 Model",
      "text" : "We define open cloze generation as the task of predicting a set of tokens that should be gapped in the text. Unlike previous approaches that work at the sentence level, our models work at the paragraph level (i.e. take the full text as input), since we believe the interactions between gaps can only be optimally captured when the text is processed as a whole rather than sentence by sentence.\nGiven a text passage, we aim to predict the words that should be gapped in order to create a cloze test that would reliably assess student ability. The task is modelled as a supervised sequence tagging problem where each token is classified as being a good potential gap or not. We employ ELECTRA (Clark et al., 2020), one of the state-of-the-art pre-trained transformer-based language representation models (Wolf et al., 2020). ELECTRA is an extension of BERT (Devlin et al., 2019) with a different pretraining task which is a discriminator (rather than a generator) and aims to detect replaced tokens (rather than generating words for the masks). We fine-tune this model using two training objectives (see Figure 2): 1 A token classification objective which aims to\nminimise the error of classifying each token as a potential gap or not.\n2 A language modelling objective that aims to minimise the negative log-likelihood of regenerating the words that have been gapped.\nThe first objective is typical of any standard token classification model. In particular, we use ELECTRA’s discriminator head with softmax to tag each word in the input sequence as a ‘good’\ngap or not. All the gaps in our training data are replaced with the first intended target answer and labelled positive, while the remaining tokens are labelled negative ( A ).\nThe second objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context. This is to avoid generating gaps that are too ‘open’ and therefore ineffective, such as a gap that accepts any noun or adjective. Specifically, we mask the words in the positions that are predicted as gaps by the discriminator and use ELECTRA’s generative head to generate the expected words in the blanks ( B ).\nWhile the input layers are shared between the discriminator and the generator model, the two branches of the system leading to the two objectives are fine-tuned in parallel in a multi-task setting."
    }, {
      "heading" : "4 Extensions",
      "text" : "Our neural transformer-based sequence tagging model can be very effective at proposing potentially good gaps, but the task becomes more challenging when we expect the output to meet additional requirements such as no repetitions, no gap interdependence, a minimum distance between gaps and a varied selection of lexico-grammatical items. We address these issues using two complementary strategies: a manipulation of the loss function and a post-processing module."
    }, {
      "heading" : "4.1 Loss manipulation",
      "text" : "In order to spread gaps evenly throughout the text, we modify the token-level loss function of our tagging model by imposing a higher penalty on tokens that are in close proximity to a gap. Let g be the position of a gap in the sequence, then for each token in position i in the proximity of g, i.e. |g− i| < D, the loss function li′ for the token in position i is defined as:\nli ′ = li ∗\nW\n|g − i| (1)\nwhere W represents the penalty and D is the maximum distance scope for penalisation.2 Equation 1 thus gives more weight to tokens closer to gaps, which results in higher penalisation of their cost functions whenever they are misclassified."
    }, {
      "heading" : "4.2 Post-processing",
      "text" : "We also employ a post-processing strategy where we replace the gaps that are repeated in the text with better options. We optimise the choice of these alternative gaps by considering the distance between them and the resulting distribution of gaps with different part-of-speech (PoS) tags.\nOur post-processing step can be seen as a reranking function. The gap candidates that are originally ranked based on the model’s confidence scores change their ranking to match other desirable requirements of a well-structured cloze test. If the selected n-best gaps include repetitions, our post-processing algorithm randomly chooses one of them at a time and attempts to replace it with a better alternative. An alternative gap is deemed better if 1) its answer is not a repetition of another gapped word, 2) its distance to other selected gaps meets the minimum required distance or is higher than the pairwise distances of the originally selected gaps, and 3) it improves the PoS distribution of the gapped words. The PoS distribution of each new selection of gaps is compared to the average gapped PoS distribution of the cloze tests in the training data using Kullback-Leibler (KL) divergence. A combination of gaps that yields lower KL divergence is assumed to be a better selection.\nThese extensions to the base model bring our final cloze tests closer to those created by human experts by automatically controlling variables that would otherwise need to be adjusted manually. This makes our solution a fully-automated system\n2We empirically set the values of constants D and W to 3 and 3.0 respectively.\nthat can produce ready-to-use cloze tests from an input text passage."
    }, {
      "heading" : "5 Data",
      "text" : "To the best of our knowledge, there are no public datasets of full-text open cloze tests that could be used for our task. The CLOTH dataset (Xie et al., 2018), for example, contains gapped passages designed for language learners, but it is primarily focused on reasoning and reading comprehension and uses multiple choice questions where distractors play a major role, making it substantially different to the task we aim to model.\nFor this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by <anonymised> for research purposes. Each task consists of a text passage of no more than 300 tokens, a variable number of gaps (between 8 and 16) and a list of valid answers for each gap (between 1 and 7). During the design process, the tasks undergo extensive quality control and pretesting, so their gaps are guaranteed to be very effective at assessing student ability.\nFor training, we reconstruct the texts by replacing each gap with its first answer and we split the whole collection into train, dev and test. Details of our dataset are shown in Table 1.\nGiven the lack of publicly available data, we make our test set available with this paper3 so as to provide a common benchmark for the task and to encourage further research in this area. All the texts were tokenised and parsed using spaCy v2.34."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Setup",
      "text" : "We use the pre-trained ELECTRA base discriminator model5 with 12 attention heads and 12 hidden layers. Along with all the tokens in the sequences, we also input dependency parsing information to the system. More specifically, we concatenate\n3Available at <anonymised>. 4https://spacy.io/ 5https://github.com/huggingface/trans\nformers.\nthe ELECTRA representation of each token with the representation of its head in the dependency graph.6 On top of the encoding layers, we have two branches that are being learned simultaneously.\nThe first branch is a simple linear layer that aims to classify each token as a gap or non-gap. For the second branch, we add ELECTRA’s generation layer plus a linear layer which aims to predict the best word from the whole vocabulary. We are only interested in predicting the answer words for the gaps. Therefore, we change the input to the second branch by masking the words that are predicted as gaps by the first branch at each step of training. We employ cross-entropy loss on each branch and ignore the loss values for the tokens that are not masked in the second branch. The whole architecture is updated based on the sum of the two losses. Fine-tuning parameters are specified in Appendix B."
    }, {
      "heading" : "6.2 Baselines",
      "text" : "We compare our ELECTRA-based model to other systems, namely: Random baseline Generates a random set of gaps\nfor each task based on the average probability distribution of gapped PoS in the training data.\nExercise Maker Generates gaps using rules and a pre-compiled list of commonly gapped words from a variety of Cambridge English main suite exams (Malafeev, 2014). Set to FCE mode for our experiments.\nBERT Predicts potentially good gaps using BERT (Devlin et al., 2019) for token classification. We use the base pre-trained model with standard parameters and fine-tune the weights of the whole architecture.\nBoth random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions are not always met."
    }, {
      "heading" : "6.3 Evaluation",
      "text" : "We report precision (P), recall (R) and F1 scores based on a strict matching between the gaps predicted by our models and those in the gold standard. While this evaluation strategy might seem strict, it has the advantage of being fully automatic, thus avoiding the subjectivity and time required by human evaluation, so we adopt it during development.\n6If the token is the head, then its representation is repeated.\nIn addition to letting the models decide the optimal number of gaps, we also evaluate system performance when we fix the number of predicted gaps for each task to the number of gaps they have in the gold standard. The n-best predicted gaps are chosen based on their confidence scores. In this scenario, P, R and F1 become the same.\nWe also performed human evaluation, which was carried out by three test experts from <anonymised> who volunteered for the task. The experts were asked to label each proposed gap in each task of our test set (a total of 360 gaps) as either good or bad and provide a reason and optional comments for their choice. The list of labels available to annotators is shown in Table 2."
    }, {
      "heading" : "7 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "7.1 Automatic evaluation",
      "text" : "We carry out automatic evaluation by computing P, R and F1 on our development set. Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker and BERT. This is our base model, which does not include any loss manipulation or post-processing. In this setting, the number of predicted gaps was decided by each model based on the confidence scores (> 0.5 for the positive class).\nOverall, we observe that performance increases with more sophisticated models. Exercise Maker relies on previously seen gaps and so outperforms the random baseline by a large margin. However, it can only create gaps for the 139 words in its predefined FCE word list, missing gaps that are not on that list. Neural transformer-based models are the best, with improvements over Exercise Maker of at least 25 F1 on our development set. Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F1, a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity as shown in Figure 3.\nTable 4 shows the performance of our multiobjective ELECTRA model as we increase the nbest list of gaps according to their confidence score. The first row indicates the results of the system when it is forced to predict the exact same number of gaps per task as in the gold standard.7 This causes P and R to be the same. As we expect, the\n7The number of gaps can vary per passage (see Appendix A).\nresults show that the number of gaps in the gold data is actually the optimal number to achieve the best F1 score.\nAlthough our multi-objective model shows good performance based on automatic evaluation, a closer look at the output reveals that the structure of the cloze tests is far from ideal as they often contain repetitions and gaps that are too close to each other, aspects that are carefully controlled in the gold standard. Table 5 shows that system performance effectively improves as we add the extensions proposed in Section 4, indicating that global aspects of the task are not properly captured by our initial model and require further manipulation.\nIn order to make the structure of our output as similar as possible to our target tasks, we fix the number of predicted gaps for each task to the number of gaps they have in the gold standard. Note that P and R are the same in this setting so we only report F1. The effect of this decision is shown is Table 6. We can see that adding loss manipulation to our model decreases the number of adjacent gaps from 40 to 23, but increases the number of repeated gapped words from 18 to 33. The decline in the restricted F1 based on automatic evaluation is not favourable, but we make this sacrifice at the price of achieving a better-structured final test.\nAfter adding post-processing for repeated gaps, we observe that, although overall F1 performance drops slightly, the number of repeated gapped words decreases favourably from 33 to 9 (Table 6). It also creates a better spread of gaps, as shown by a lower KL-divergence between the average POS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59\nwithout it). Post-processing also removes two cases in the development set where the gaps do not meet the minimum 4-word distance.\nIt is worth recalling that these extensions are highly effective when we do not restrict the number of predicted gaps. Table 5 shows that significantly improve R, which results in higher overall F1.\nAs a result of these experiments, we stick with our post-processing approach for the rest of our experiments and use it to produce the output submitted for human annotation."
    }, {
      "heading" : "7.2 Human evaluation",
      "text" : "Following our intuition that test experts could find more value in our system than initially shown by our automatic evaluation, we asked a panel of three test experts to judge the quality of the gaps produced by our extended model on the test set. Inter-annotator agreement on gap classification (good/bad) was found to be moderate (percent agreement is 75.93%, Randolph’s free-marginal kappa is 0.52 (Randolph, 2005)).\nUnlike in automatic evaluation, we only report accuracy for our human experiment. System performance using automatic and human evaluation is compared in Table 7 (reported individually for each annotator). These results show that performance increases dramatically when the output is judged by human experts, confirming our suspicion that performance is underestimated by automatic evaluation and that there are many other words in the texts that could constitute equally useful gaps apart from those in the gold standard. With system accuracy ranging between 75%− 82% for human judgements, we can conclude that at least 7 out of 10 gaps proposed by our system are considered good by our experts.\nWe observed that differences between annotators’ judgements and the gold-standard can occur for many reasons, e.g.:\n• non-gaps in the gold standard are not necessarily bad gaps, • gold standard gaps are derived from pilot testing while annotators’ gaps are derived from their expertise, • previous judgements by the annotators can affect the judgement of new gaps (e.g. choosing the best of two close gaps), etc.\nAnnotator accuracy against the gold standard ranges between 50%− 60%.\nFollowing our classification in Table 2, we analysed the reasons why some gaps were not considered good by the annotators. Figure 4 shows the average frequency of the different reasons given by the annotators for rejecting a gap proposed by our system. Examples are included in Appendix C.\nThe most frequent reason is the violation of the minimum required distance between two gaps (42.43%). Although our loss-manipulation approach was successful in reducing these cases, we did not attempt to eradicate them completely since there are many factors at play when choosing more appropriate gaps than just distance. In many cases, gaps in close proximity test different words in the same phrase (e.g. take part in, in addition to, etc.) so we preferred to keep these cases and encourage annotators to comment on their preferences. Repetitions, on the contrary, are much better handled, accounting for only 0.87% of all bad gaps.\nThe second most frequent reason is ‘unacceptable outlier’ (32.47%), which normally accounts for cases where the difficulty of the gap is considered inappropriate for the target proficiency level (B2 in this case). This is an interesting phenomenon, since the fact that the text as a whole pertains to a given CEFR level does not guarantee that the gaps created will always be appropriate for\nthe level. The remaining reasons are substantially less frequent than the first two and mostly related to aspects that were not explicitly controlled in our models, except for the third topmost reason (‘Too many gaps of this type’) which we did control by comparing PoS distributions. These results show that our system is able to capture many aspects of the task that were not explicitly modelled.\nFinally, we compared system accuracy per task computed from annotators’ judgements vs. the gold standard. Average correlation across all annotators was found to be very weak (Pearson’s r = 0.0558, Spearman’s ρ = 0.1474). This suggests that automatic scores are not a good proxy for human perception, with experts being much more positive about our model’s output (as shown in Table 7)."
    }, {
      "heading" : "7.3 Predictions by Gapped Word Frequency",
      "text" : "We found that our model does not overfit to words that are most frequently gapped in the training data, with correlation between gapped word frequency and F1 scores in the test set being negligible (Pearson’s r = 0.0108, Spearman’s ρ = 0.0915).\nInterestingly, while our model was unable to predict gaps not previously seen in the training data (turned, amount, pushed and started), it did predict a (previously unseen) gap for the word fewer, which did not match the gold standard but was unanimously deemed good by our annotators."
    }, {
      "heading" : "7.4 Predictions by PoS",
      "text" : "We also classified predictions based on their PoS tags8 and report performance in Table 8. The most frequently gapped PoS tags in our datasets corre-\n8https://universaldependencies.org/u/ pos/\nspond to closed word classes (such as ADP, DET, SCONJ, AUX, etc.), which is expected given that our open cloze tests are mostly focused on testing grammar rather than vocabulary. The best predicted classes, however, are NUM, SCONJ, NOUN, ADJ and INTJ which on closer inspection turn out to be very restricted classes: NUM includes only the word one, INTJ only the word like, SCONJ only a few subordinating conjunctions while NOUN and ADJ, despite being open classes, are limited to words used in common constructions such as order (in order to) or same (the same).\nThe two worst performing classes are PART (the particles to and not) and AUX (auxiliary verbs) and, once again, we conjecture that these words are so common in the language and in non-gapped positions that the model is unable to get them right most of the time. The remaining PoS classes vary in performance but we found only very weak correlation between PoS gap frequency in the test set and F1 scores (Pearson’s r = 0.1932, Spearman’s ρ = 0.1350).\nWhen we look at human annotations on the test set, however, performance by PoS is consistently higher and more even across the board. If we require that gaps are rated ‘good’ by at least two annotators, accuracy values range between 75% and 100% for all PoS, with a mean of 85%.\nUnder these conditions, the best performing classes are NOUN (100%), INTJ (100%) and ADJ (95%), which agree with automatic evaluation. Out of these, only NOUN achieves perfect accuracy across all annotators. The worst performing classes are PRON (77%), NUM (77%) and VERB (77%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively). When we require agreement by all annotators, the worst overall class is CCONJ with 44%."
    }, {
      "heading" : "7.5 Qualitative Analysis",
      "text" : "Figure 5 shows the output of our model for a sample text passage, where darker red indicates higher confidence in inserting a gap. The final model’s predictions have a black frame (at, in, so, after, etc.) while the gold standard gaps are in yellow font (at, in, so, etc.). There are 8 matched gaps out of 11 in this example, yielding 72.73% accuracy.\nAs can be seen in the figure, our model is able to identify appropriate gap candidates, even if they do not match the gold standard. In fact, annotators considered all the unmatched gaps in this example (after, for and take) to be good and the second matched gap (in) to be inappropriate. It is also interesting to see how the model prioritises function words and content words that are highly restricted in context (such as take or part), skilfully avoiding general gaps that could accept multiple answers and would be less effective for testing purposes."
    }, {
      "heading" : "8 Conclusion and Future Work",
      "text" : "We described the first transformer-based approach to open cloze test generation. Our ELECTRAbased model is trained on two objectives: token classification (gap/non-gap) and language modelling (for predicting the expected answer). The model is further improved by manipulating the loss function and post-processing the results.\nSystem accuracy using automatic evaluation is 53.89% while human evaluation ranges between 75%− 82%, showing that at least 7 out of 10 gaps predicted are considered useful by experts. A detailed analysis of results reveals a few structural problems such as gaps in close proximity and inappropriate difficulty, which we plan to address in future work. Our test data and human annotations are released with this paper."
    }, {
      "heading" : "C Human labelling examples",
      "text" : ""
    }, {
      "heading" : "B Model parameters",
      "text" : ""
    }, {
      "heading" : "A Dataset composition",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Automatically generating questions to support the acquisition of particle verbs: Evaluating via crowdsourcing",
      "author" : [ "Maria Chinkina", "Simón Ruiz", "Detmar Meurers." ],
      "venue" : "CALL in a climate of change: adapting to turbulent global conditions – short pa-",
      "citeRegEx" : "Chinkina et al\\.,? 2017",
      "shortCiteRegEx" : "Chinkina et al\\.",
      "year" : 2017
    }, {
      "title" : "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Common European Framework of Reference for Languages: learning, teaching, assessment",
      "author" : [ "Council of Europe." ],
      "venue" : "Cambridge University Press, Cambridge.",
      "citeRegEx" : "Europe.,? 2001",
      "shortCiteRegEx" : "Europe.",
      "year" : 2001
    }, {
      "title" : "Factual open cloze question generation for assessment of learner’s knowledge",
      "author" : [ "Bidyut Das", "Mukta Majumder." ],
      "venue" : "International Journal of Educational Technology in Higher Education, 14:1–12.",
      "citeRegEx" : "Das and Majumder.,? 2017",
      "shortCiteRegEx" : "Das and Majumder.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Entropy as a proxy for gap complexity in open cloze tests",
      "author" : [ "Mariano Felice", "Paula Buttery." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 323–327, Varna, Bulgaria. INCOMA",
      "citeRegEx" : "Felice and Buttery.,? 2019",
      "shortCiteRegEx" : "Felice and Buttery.",
      "year" : 2019
    }, {
      "title" : "C-tests: an overview",
      "author" : [ "Rüdiger Grotjahn", "Christine Klein-Braley", "Ulrich Raatz." ],
      "venue" : "James A Coleman, Rüdiger Grotjahn, and Ulrich Raatz, editors, University language learning and the C-Test, pages 93–114. AKS-Verlag, Bochum, Germany.",
      "citeRegEx" : "Grotjahn et al\\.,? 2002",
      "shortCiteRegEx" : "Grotjahn et al\\.",
      "year" : 2002
    }, {
      "title" : "A systematic review of automatic question generation for educational purposes",
      "author" : [ "Ghader Kurdi", "Jared Leo", "Bijan Parsia", "Uli Sattler", "Salam Al-Emari." ],
      "venue" : "I. J. Artificial Intelligence in Education, 30(1):121– 204.",
      "citeRegEx" : "Kurdi et al\\.,? 2020",
      "shortCiteRegEx" : "Kurdi et al\\.",
      "year" : 2020
    }, {
      "title" : "Manipulating the difficulty of C-tests",
      "author" : [ "Ji-Ung Lee", "Erik Schwan", "Christian M. Meyer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 360–370, Florence, Italy. Association for Computational Lin-",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Language exercise generation: Emulating cambridge open cloze",
      "author" : [ "Alexey Malafeev." ],
      "venue" : "Int. J. Concept. Struct. Smart Appl., 2(2):20–35.",
      "citeRegEx" : "Malafeev.,? 2014",
      "shortCiteRegEx" : "Malafeev.",
      "year" : 2014
    }, {
      "title" : "Learning to automatically generate fill-in-the-blank quizzes",
      "author" : [ "Edison Marrese-Taylor", "Ai Nakajima", "Yutaka Matsuo", "Ono Yuichi." ],
      "venue" : "Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications,",
      "citeRegEx" : "Marrese.Taylor et al\\.,? 2018",
      "shortCiteRegEx" : "Marrese.Taylor et al\\.",
      "year" : 2018
    }, {
      "title" : "Jump-starting item parameters for adaptive language tests",
      "author" : [ "Arya D. McCarthy", "Kevin P. Yancey", "Geoff T. LaFlair", "Jesse Egbert", "Manqian Liao", "Burr Settles." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "McCarthy et al\\.,? 2021",
      "shortCiteRegEx" : "McCarthy et al\\.",
      "year" : 2021
    }, {
      "title" : "Developing, evaluating, and refining an automatic generator of diagnostic multiple choice cloze questions to assess children’s comprehension while reading",
      "author" : [ "Jack Mostow", "Yi-Ting Huang", "Hyeju Jang", "Anders Weinstein", "Joe Valeri", "Donna Gates." ],
      "venue" : "Nat-",
      "citeRegEx" : "Mostow et al\\.,? 2017",
      "shortCiteRegEx" : "Mostow et al\\.",
      "year" : 2017
    }, {
      "title" : "Measuring hint level in open cloze questions",
      "author" : [ "Juan Pino", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the Twenty-Second International Florida Artificial Intelligence Research Society Conference (FLAIRS), Sanibel Island, Florida, USA. AAAI Press.",
      "citeRegEx" : "Pino and Eskenazi.,? 2009",
      "shortCiteRegEx" : "Pino and Eskenazi.",
      "year" : 2009
    }, {
      "title" : "A selection strategy to improve cloze question quality",
      "author" : [ "Juan Pino", "Michael Heilman", "Maxine Eskenazi." ],
      "venue" : "Intelligent Tutoring Systems for Ill-Defined Domains: Assessment and Feedback in Ill-Defined Domains, page 22.",
      "citeRegEx" : "Pino et al\\.,? 2008",
      "shortCiteRegEx" : "Pino et al\\.",
      "year" : 2008
    }, {
      "title" : "Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss’ fixed-marginal multirater kappa",
      "author" : [ "Justus J Randolph." ],
      "venue" : "Online submission.",
      "citeRegEx" : "Randolph.,? 2005",
      "shortCiteRegEx" : "Randolph.",
      "year" : 2005
    }, {
      "title" : "Machine learning–driven language assessment",
      "author" : [ "Burr Settles", "Geoffrey T. LaFlair", "Masato Hagiwara." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:247–263.",
      "citeRegEx" : "Settles et al\\.,? 2020",
      "shortCiteRegEx" : "Settles et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic question generation system for english exercise for secondary students",
      "author" : [ "Tasanawan Soonklang", "Sunee Pongpinigpinyo", "Weenawadee Muangon", "Sirak Kaewjamnong." ],
      "venue" : "Proceedings of the 25th International Conference",
      "citeRegEx" : "Soonklang et al\\.,? 2017",
      "shortCiteRegEx" : "Soonklang et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Large-scale cloze test dataset created by teachers",
      "author" : [ "Qizhe Xie", "Guokun Lai", "Zihang Dai", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2344–2356, Brussels, Belgium. Associa-",
      "citeRegEx" : "Xie et al\\.,? 2018",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic generation of cloze items for repeated testing to improve reading comprehension",
      "author" : [ "Albert C.M. Yang", "Irene Y.L. Chen", "Brendan Flanagan", "Hiroaki Ogata." ],
      "venue" : "Educational Technology & Society, 24(3):147–158.",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "In this paper, we propose a strategy to construct open cloze exercises using transformer models (Vaswani et al., 2017).",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is fine-tuned on the two described objectives in a multi-task scenario.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "While research into automatic cloze test generation is vast (Mostow et al., 2017; Kurdi et al., 2020; Yang et al., 2021), work on open cloze tests for language learning is scarce.",
      "startOffset" : 60,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "While research into automatic cloze test generation is vast (Mostow et al., 2017; Kurdi et al., 2020; Yang et al., 2021), work on open cloze tests for language learning is scarce.",
      "startOffset" : 60,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "While research into automatic cloze test generation is vast (Mostow et al., 2017; Kurdi et al., 2020; Yang et al., 2021), work on open cloze tests for language learning is scarce.",
      "startOffset" : 60,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "A later version of their system adds hints for gapped words (Pino and Eskenazi, 2009).",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "Exercise Maker (Malafeev, 2014) is a rule-based open source system that attempts to emulate exercises in Cambridge English examinations based on the most frequently tested words.",
      "startOffset" : 15,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "While previous work often made a choice between automatic (Marrese-Taylor et al., 2018) or human evaluation (Malafeev, 2014; Das and Majumder, 2017) for their experiments, we perform both: automatic evaluation to identify the best models during development and human evaluation to measure test quality in the final output.",
      "startOffset" : 58,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : ", 2018) or human evaluation (Malafeev, 2014; Das and Majumder, 2017) for their experiments, we perform both: automatic evaluation to identify the best models during development and human evaluation to measure test quality in the final output.",
      "startOffset" : 28,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : ", 2018) or human evaluation (Malafeev, 2014; Das and Majumder, 2017) for their experiments, we perform both: automatic evaluation to identify the best models during development and human evaluation to measure test quality in the final output.",
      "startOffset" : 28,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "We employ ELECTRA (Clark et al., 2020), one of the state-of-the-art pre-trained transformer-based language representation models (Wolf et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "ELECTRA is an extension of BERT (Devlin et al., 2019) with a different pretraining task which is a discriminator (rather than a generator) and aims to detect replaced tokens (rather than generating words for the masks).",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "The CLOTH dataset (Xie et al., 2018), for example, contains gapped passages designed for language learners, but it is primarily focused on reasoning and reading comprehension and uses multiple choice questions where distractors play a major role, making it substantially different to the task we aim to model.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Exercise Maker Generates gaps using rules and a pre-compiled list of commonly gapped words from a variety of Cambridge English main suite exams (Malafeev, 2014).",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "BERT Predicts potentially good gaps using BERT (Devlin et al., 2019) for token classification.",
      "startOffset" : 47,
      "endOffset" : 68
    } ],
    "year" : 0,
    "abstractText" : "This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing reranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of highquality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",
    "creator" : null
  }
}
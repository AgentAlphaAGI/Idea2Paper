{
  "name" : "ARR_2022_302_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Efficiently Acquiring Annotations for Multilingual Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "While neural networks have become the de-facto method of tackling NLP tasks, they often require a lot of annotated data to do so. This task of data annotation is especially challenging while building systems aimed at serving numerous languages. Motivated by this, in this paper, we tackle the following problem:\nGiven the requirement of building systems for an NLP task in a multilingual setting with a fixed annotation budget, how can we efficiently acquire annotations to perform the task well across multiple languages?\nThe traditional approach to this problem has been building a separate model to serve each language. In this scenario, the annotation budget is split equally for all languages, and a model\nis trained for each one separately. Recently, another direction that has gained popularity has been leveraging multilingaul pre-trained language models (MPLMs) which inherently map multiple languages to a common embedding space (Devlin et al., 2019; Conneau et al., 2020). The popular method for leveraging these models has been leveraging their zero-shot transfer ability: training on an English-only corpus for the task, and then using the models zero-shot for the other languages.\nAnother orthogonal line of work aimed at building models under a constrained budget has been active learning (AL) (Shen et al., 2018; Ein-Dor et al., 2020). While this has shown to improve annotation efficiency, the predominant approach has been to train one model per language, using the (language specific) model for AL (Shen et al., 2018; Erdmann et al., 2019).\nIn this work, we show that a single MPLM trained on all languages simultaneously performs much better than training independent models for specific languages for a fixed total annotation budget. Further, while the benefits of using AL in conjunction with MPLMs has been studied for a monolingual setup (Ein-Dor et al., 2020), we show that AL also yields benefits in the multilingual setup. Concretely, we show that an AL acquisition on a single language helps improve zero-shot performance on all other languages, regardless of the language of the seed data. Furthermore, we show that AL also yields benefits for our proposed single model scenario. We demonstrate that our results are consistent on 3 different tasks across multiple languages: classification, sequence tagging and dependency parsing. Our approach removes the requirement of maintaining n different models, and uses 1{nth the parameters than when independent models are trained. Our analysis reveals that the model arbitrates between different languages based on its performance to form a multilingual curriculum. We release our code at URL."
    }, {
      "heading" : "2 Related Work",
      "text" : "Effective utilization of annotation budgets has been the area of focus for numerous active learning works, showing improvements for different tasks like POS tagging (Ringger et al., 2007), sentiment analysis (Karlos et al., 2012; Li et al., 2013; Brew et al., 2010; Ju and Li, 2012), syntactic parsing (Duong et al., 2018), and named entity recognition (Settles and Craven, 2008; Shen et al., 2018). The focus of most of these works, however, has been on learning for a single language (often English). Prior work on AL that uses a multilingual setup or cross-lingual information sharing and that goes beyond training a separate model for each language has thus been limited. The closest work where multiple languages influence each other’s acquisition is that of Qian et al. (2014); however, they still train a separate model for each language.\nFor transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019; Liu et al., 2020). Ein-Dor et al. (2020) studied the dataeffectiveness of these models when used in conjunction with AL, but, as with other AL work, with a single language focus. Finally, Lauscher et al. (2020) studied the effectiveness of the zero-shot setup, showing that adding a few examples to a model trained on English improves performance over zero-shot transfer. However, this assumes the availability of a full English task-specific corpus."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Task Specific Models:",
      "text" : "We use the multilingual-BERT-cased model (mBERT) as the base model for all the tasks. We use the standard training methodology for the tasks: for classification, we use a single layer over the [CLS] embedding, for sequence tagging, we use a single layer for each word to predict its tag, and for dependency parsing, we follow Kondratyuk and Straka (2019) and use mBERT embeddings with the graph-based bi-affine attention parser (Dozat and Manning, 2017); refer Appendix A for details."
    }, {
      "heading" : "3.2 Budget Allocation Settings",
      "text" : "To understand data acquisition in a multilingual setting, we consider multilingual datasets in the 3 tasks. For each task t, let L be the set of languages (n “ |L|). We then define st to be the seed\nsize, bt to be the total annotation budget and vt to be total number of annotated validation examples available to t. We compare our proposed Single Model Acquisition (SMA) setup to two baseline settings– Monolingual Acquisition (MonoA) and Multi Model Acquisition (MMA):\nMonoA: In this setting, the seed data as well as the validation data (st, vt) is acquired from a single language. Further, the entire annotation budget (bt) is assigned to the same language. We evaluate the test data performance on that language and on the other n ´ 1 languages in a zero-shot setting.\nMMA: For this setting, we train n individual models, one for each language. Each model starts with a seed of st{n, a validation set of vt{n, and is assigned an acquisition budget of bt{n. At test time, we evaluate the performance of the model on the language it was trained with.\nSMA: For this setting, we consider a single model for which both training and acquisition is done on all n languages simultaneously. The seed data and the validation set comprises of a random subset drawn from data corresponding to all languages. The whole of st, bt and vt are thus assigned to this single model. We compute the performance on the test data of each of the languages."
    }, {
      "heading" : "3.3 Active Learning Acquisition Strategies:",
      "text" : "The field of active AL tends not to reveal explicit winners—though there is a general consensus that AL does indeed outperform passive learning (Settles, 2009). Thus, we adopt the simplest confidence based strategies to demonstrate their efficacy for each task : Least Confidence (LC) for classification, Maximum Normalized Log Probability (MNLP) (Shen et al., 2018) for sequence tagging, and normalized log probability of decoded tree (NLPDT) (Li et al., 2016) for dependency parsing. Please see Appendix B for more details. To the best of our knowledge, this is the first work to explore an ALaugmented single model for multiple languages."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset Details",
      "text" : "Classification: We consider Sentiment Analysis, using the Amazon Reviews dataset (Prettenhofer and Stein, 2010). The dataset consists of reviews and their binary sentiments for 4 languages: English (en), French (fr), Japanese (ja), German (de).\nSequence Tagging: We choose Named Entity Recognition, and use the CoNLL02/03 datasets\n(Sang, 2002; Tjong Kim Sang and De Meulder, 2003) with 4 languages: English (en), Spanish (es), German (de) and Dutch (nl), and 4 named entities: Location, Person, Organization and Miscellaneous.\nDependency Parsing: We use a subset of treebanks with 5 languages (English (en), Spanish (es), German (de), Dutch (nl), Japanese (ja)) from the full Universal Dependencies v2.3 corpus (Nivre et al., 2018); a total of 11 treebanks."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "For each experiment, we run 4 training rounds: one training on initial seed data, followed by 3 acquisition rounds. We set st“bt“vt in all cases. For classification, we set st=300 sentences. For NER and Dependency Parsing, we use st“„10k and st“„17.5k tokens respectively (refer Appendix C). We report accuracy for classification, F1-Score for the NER, and unlabeled and labeled attachment scores (UAS and LAS) for dependency parsing.\nFor each task, we run the 3 settings (§3.2) across multiple languages. For each setting, we also train an AL model with a task-specific acquisition function (§3.3). In addition, we train both the SMA and MMA with all available data, i.e., we use all data to train one model for all languages and one model per language respectively. We report an average of 5 runs for each experiment. Refer Appendix D for hyperparameters and training details."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "Model Performance: Figure 1 shows the performance of NER on Spanish (refer Appendix H for the plots of all other languages and tasks). Although acquiring data independently per language (MMA) performs well, SMA outperforms MMA. Unsurprisingly, MonoA with es performs the best\nin the category, since it allocates its entire budget to acquiring es data; it thus forms an upper-bound of the model performance. However, SMA outperforms MonoA when its seed language and inference language differ. Finally, AL consistently provides gains over random acquisition.\nTo analyze the performance across all languages, we present the performance for each round of acquisition, aggregated across all languages for Classification (Figure 2) (refer Appendix H for Dependency Parsing and NER plots). Here, SMA consistently outperforms MMA for every round of acquisition because MMA suffers from a poorly utilized budget, potentially wasting annotation budget on languages where the task is easier. In contrast, SMA improves budget utilization while also benefiting from cross-lingual information. Finally, SMA, by virtue of performing well irrespective of language, consistently outperforms MonoA.\nFor a concise overview, we present the aggregate metrics across all rounds for each task in Table 1. We observe that SMA does much better compared to its counterparts; both with and without AL. We also observe these models to be extremely data efficient: with AL, a model with access to less than 5% of the data achieves a (relative) performance of around 88% accuracy (for classification), 95.5% F1-score (for NER) and 93.5% LAS (for dependency parsing) when compared to a model trained with all available data. Further, along with its superior performance, SMA also affords substantial parameter savings: requiring only a single model, compared to a number of models linear in n (thereby using 1\nnth parameters compared to MMA).\nMM Full vs SM Full: To analyze how effectively a single model performs on the languages in question despite using 1{nth the parameters, we\ntrain a single model on all data and compare it with n language-specific models, where each of the n models has the same number of parameters as the single model; this also serves as an upper-bound for our AL experiments. The rightmost columns of Table 1 show that having a single model does not adversely impact performance. A more detailed discussion is present in Appendix E.\nThe effectiveness of AL in MonoA: We consistently observe AL in the source language improving performance across all languages, irrespective of whether inference is being run for the source language or zero-shot on a different target language, both for NER and classification (Table 1). We hypothesize that the model selects semantically difficult or ambiguous examples that generalize across languages by virtue of mBERT’s shared embedding representation. To the best of our knowledge, this work is the first to demonstrate that AL can improve the data efficiency of both classification and NER in a zero-shot inference setup.\nIn the case of dependency parsing, we observe mixed results when the source and target languages differ. We hypothesize that this is because dependency parsing is a syntactic problem, making it more language specific, and zero-shot inference inherently harder. This is in contrast with both classification and NER, which are more semantic, making hard examples more generalizable across languages. Refer Appendix F for more details.\nWhat does SMA+AL acquire? One advantage of the SMA+AL setup is that the model can arbitrate between allocating its acquisition budget across different languages as training progresses. This is in contrast with training one model per language, where the models for languages with a high performance waste the overall budget by acquiring more than necessary, while models on languages where performance isn’t as good under-acquire.\nTo investigate this, for each language and each round, we plot the relative difference (%) between cumulative tokens acquired by the SMA+AL model for that language, and the tokens acquired in expectation if acquisition was done randomly (refer Appendix G for more details). For each language, we also plot the relative performance difference of the language at that round compared to the performance when 100% data is available.\nFigure 3 reveals the added benefit of SMA+AL for data acquisition for NER (refer Appendix G for other tasks): a single model can arbitrate between instances across languages automatically. The model initially acquires data from the high resource language (English). But as the training proceeds, the model favors acquiring data from languages it is uncertain about. This “multilingual curriculum” thus allows the model to be more effective in its use of the annotation budget. We find SMA+AL eventually achieves a similar relative difference from 100% data performance for all languages consistently across tasks as a consequence."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we consider the problem of efficiently building models that solve a task across multiple languages. We show that, contrary to traditional approaches, a single model arbitrating between multiple languages for data acquisition considerably improves performance in a constrained budget scenario, with AL providing additional benefits."
    }, {
      "heading" : "A Task Specific Details",
      "text" : "In this section, we elaborate on the task specific adaptations:\nClassification: As is common practice, we use a single linear layer over [CLS] embeddings generated by the BERT model to generate logits for the classification task, and the model is trained to minimize the cross-entropy loss.\nSequence Tagging: We apply a linear layer to the word embeddings1 generated by the BERT model to generate the tag logits, and the model is trained to minimize the negative log-likelihood of the observed tags.\nDependency Parsing: We use a graph-based biaffine attention parser introduced in (Dozat and Manning, 2017). Following (Kondratyuk and Straka, 2019), we use the output of the last BERT layer in place of the embeddings generated by the Bi-LSTM layers. These embeddings are then concatenated with the POS embeddings. A head feed-forward network and a child feed-forward network then generate embeddings for each head and dependant word of a dependency respectively. This is combined with a biaffine attention module to generate a probability distribution for each word to predict its head, as well as a bilinear layer to predict the label for each dependency relationship. Let τpiq “ tphpi,jq, dpi,jq, lpi,jq|hpi,jq ñ dpi,jq with label lpi,jqu be the ith gold dependency tree in the dataset. The model is then trained to maximize the log probability of the gold tree as :\nmax ÿ\ni\nÿ\nj\nlog ` Pphpi,jq|dpi,jqq ˘\n` log ` Pplpi,jq|hpi,jq ñ dpi,jqq ˘\n(1)\nDuring inference, the best dependency parse is generated by decoding with Chu-Liu/Edmonds algorithm (Chu, 1965; Edmonds, 1967).\nFor all the models mentioned above, all layers of mBERT are fine-tuned during training."
    }, {
      "heading" : "B Acquisition Strategies Details",
      "text" : "We mention the acquisition functions used below:\n1Following (Devlin et al., 2019) For words generating multiple wordpieces, we use the embedding of the first wordpiece.\nMaximum Normalized Log Probability (MNLP): This strategy chooses instances for which the log probability of the model prediction, normalized by sequence length, is the lowest. This AL strategy has been shown to be extremely effective for NER (Shen et al., 2018) and hence we adopt it in our setting.\nLeast Confidence (LC): This strategy chooses those instances for which the model confidence corresponding to the predicted class is the least. This acquisition strategy has been commonly applied in classification tasks, and although simple, has been consistently shown to often perform extremely well (Settles, 2009); consequently, we adopt it in our setting.\nNormalized Log Probability of the Decoded Tree (NLPDT): This strategy selects the instances with the minimum log probability of the decoded tree; i.e given d˚ to be the tree generated by the Chu-Liu/Edmonds algorithm, the log probability, as computed by Eqn. 1. Following (Li et al., 2016), we also normalize this score by N , where N indicates the number of tokens. We also tried normalizing by N2, as well as a globally normalized probability of d˚ (probability of the tree over all possible valid trees, with the partition function computed using the Matrix Tree Theorem (Koo et al., 2007; Smith and Smith, 2007)), but found both to perform worse."
    }, {
      "heading" : "C Dataset statistics",
      "text" : "We report the detailed dataset statistics in Table 2. Note that the seed was chosen to be roughly 5% of the size of the English training data, shown in the rightmost column of the table."
    }, {
      "heading" : "D Experimental Details",
      "text" : "Hyperparameters All experiments performed in this paper are averaged over 5 runs. For each experiment, we perform an LR search over (1e-5, 2e-5, 3e-5, 4e-5 and 5e-5), and choose the best LR according to the performance on the appropriate validation (sub)set, as recommended in (Devlin et al., 2019). In all experiments, we set the batch size to 32 and use an Adam (Kingma and Ba, 2015) optimizer. Each round of training is run with a patience of 25 epochs, for at most 75 epochs in total.\nData Preprocessing To avoid out-of-memory issues on the GPU, we pre-process the data so that\nthe examples in the train set of length larger than 175 and with larger than 256 word-pieces are filtered out for the NER. For classification, we simply truncate all instances at 256 word-pieces. We also de-duplicate the train set, to ensure that during all AL acquisition stages, no duplicates are selected at any point.\nCode All code used in this work was implemented using Python, PyTorch and AllenNLP (Gardner et al., 2018), using pre-trained models released by HuggingFace (Wolf et al., 2020)."
    }, {
      "heading" : "E SM Full vs MM Full Performance",
      "text" : "Given that the SMA setup uses 1{nth the number of parameters, an interesting question is whether fewer parameters leads to a loss in any expressive power for the single model, which might potentially lead to poorer performance (curse of multilinguality (Conneau et al., 2020)). To answer this question, we train a single model on all data and compare it with n language-specific models, where each of the n models has the same number of parameters as the single model.\nFrom the 100% (rightmost) columns of Table 1, we find that having a single model does not adversely impact performance and these trends hold irrespective of whether all the languages in the task are etymologically close (as in NER) or distant (ja for classification and dependency parsing). This might not be the case when there are a large number of languages, however; investigating how well this observation scales with the number of languages would be an interesting line of future work."
    }, {
      "heading" : "F Active Learning for the MonoA Setup",
      "text" : "An interesting observation from Table 1 is that AL in the source language helps improve performance across all languages, irrespective of whether the inference is being run for the source language in question or zero-shot on a different target language without any training. We observe this to be the case consistently for both the NER and the classification tasks (refer Figure 4 for classification\nand Appendix I for the other tasks), regardless of the source language. We hypothesize that this is because the model selects semantically difficult or ambiguous examples that generalize across languages by virtue of mBERT’s shared embedding representation, in contrast with random selection where easy examples the model can already tackle might be selected. We observe this even in the case of etymologically distant languages, such as when the model is trained in English and zero-shot inference is done in Japanese (or vice versa). Thus, the AL selection does not overfit on the specific language in question, instead choosing difficult but generalizable examples.\nWe observe mixed results for the MonoA setup for dependency parsing: AL improves substantially over Random when the target language and the source language are the same; however, when they differ, the results are mixed. We hypothesize that this discrepancy is a consequence of dependency parsing being a syntactic problem, making it more language specific, in turn making zero-shot an inherently harder problem. This is in contrast with both classification and NER, which are more semantic tasks. Consequently, hard examples for the latter tasks might be more generalizable across languages, resulting in their improved AL performance, when compared with the dependency parsing task."
    }, {
      "heading" : "G Acquisition Ablation Details and",
      "text" : "Curriculum\nIn this section, we describe the analysis of investigating the acquisitions of SMA+AL in more detail. Let α1 ¨ ¨ ¨αn be the language specific amount of data present in the entire dataset (i.e αi “ 0.3 implies that 30% of the entire dataset (training + unlabeled) is of language i), and let β1,1 ¨ ¨ ¨βm,n represent the amount of data acquired for every language at every round (i.e βi,j indicates the amount of data acquired by language j at round i). Then, for a task t, for each round i and language j, we plot p ři\nk“1 βk,jq´αj 9bt 9i αj 9bt 9i .\nFigures 5 and 6 show the acquisition (as described in Appendix G for both the classification and NER tasks. We observe a similar for both the tasks as that for dependency parsing."
    }, {
      "heading" : "H Detailed Results",
      "text" : "This section the additional plots as well as the detailed tables and results for all the experiments presented in the paper.\nH.1 UAS Scores per acquisition round for Dependency Parsing\nFigures 7 and 8 show the UAS and LAS for each round of acquisition for dependency parsing, aggregated across all languages.\nH.2 F-Score per acquisition round for NER Figure 9 shows the FScore for each round of acquisition for NER, aggregated across all languages.\nH.3 Experiments for NER Tables 3, 4, 5 and 6 show the performance of the different AL settings on English, Spanish, Dutch and German respectively. Each table shows the F-score across 4 acquisition rounds, both with and without MNLP (§3.2).\nH.4 Experiments for Classification Tables 7, 8, 9 and 10 show the performance of the different AL settings on English, French, Japanese and German respectively. Each table shows the accuracy across 4 acquisition rounds, both with and without LC (§3.2).\nH.5 Experiments for Dependency Parsing Table 11 compares the performance (LAS and UAS) of the single model trained on all data to the performance of one model trained per language. Table 12 gives the detailed breakdown of each AL setup for each of the dependency parsing datasets, aggregated across all the acquisition rounds.\nH.6 Plots for AL for NER\n52.00\n62.00\n72.00\n82.00\n92.00\nseed round1 round2 round3\nMMA MMA+MNLP MonoA[en] MonoA[en]+MNLP MonoA[es] MonoA[es]+MNLP MonoA[nl] MonoA[nl]+MNLP MonoA[de] MonoA[de]+MNLP SMA SMA+MNLP MM[100%] SM[100%]\nFigure 10: NER for English (en)\n60.00\n67.75\n75.50\n83.25\n91.00\nseed round1 round2 round3\nMMA MMA+MNLP MonoA[en] MonoA[en]+MNLP MonoA[es] MonoA[es]+MNLP MonoA[nl] MonoA[nl]+MNLP MonoA[de] MonoA[de]+MNLP SMA SMA+MNLP MM[100%] SM[100%]\nFigure 11: NER for Dutch (nl)\nH.7 Plots for AL for Classification\n56.00\n64.00\n72.00\n80.00\n88.00\nseed round1 round2 round3\nMMA MMA+LC MonoA[en] MonoA[en]+LC MonoA[fr] MonoA[fr]+LC MonoA[ja] MonoA[ja]+LC MonoA[de] MonoA[de]+LC SMA SMA+LC MM[100%] SM[100%]\nFigure 13: Classification for English (en)\nH.8 Plots for AL for Dependency Parsing\n65.00\n68.00\n71.00\n74.00\n77.00\nseed round1 round2 round3\nMMA MMA+NLPDT MonoA[en] MonoA[en]+NLPDT MonoA[es] MonoA[es]+NLPDT MonoA[de] MonoA[de]+NLPDT MonoA[nl] MonoA[nl]+NLPDT MonoA[ja] MonoA[ja]+NLPDT SMA SMA+NLPDT MM Full SM Full\nFigure 18: LAS for Japanese (ja)\n68.00\n74.25\n80.50\n86.75\n93.00\nseed round1 round2 round3\nMMA MMA+NLPDT MonoA[en] MonoA[en]+NLPDT MonoA[es] MonoA[es]+NLPDT MonoA[de] MonoA[de]+NLPDT MonoA[nl] MonoA[nl]+NLPDT MonoA[ja] MonoA[ja]+NLPDT SMA SMA+NLPDT MM Full SM Full\nFigure 19: LAS for Dutch (nl)\nI MonoAL: Dependency Parsing and NER\nThis section shows the plots for the performance of an mBERT model trained on de (the source language) in a MonoAL setting relative to the performance of an mBERT model trained on all de data available (100% data, without AL). The performance plots are shown for the dependency parsing (Figure 24) and NER (Figure 25) tasks."
    } ],
    "references" : [ {
      "title" : "Using crowdsourcing and active learning to track sentiment in online media",
      "author" : [ "Anthony Brew", "Derek Greene", "Pádraig Cunningham." ],
      "venue" : "ECAI.",
      "citeRegEx" : "Brew et al\\.,? 2010",
      "shortCiteRegEx" : "Brew et al\\.",
      "year" : 2010
    }, {
      "title" : "On the shortest arborescence of a directed graph",
      "author" : [ "Yoeng-Jin Chu." ],
      "venue" : "Scientia Sinica, 14.",
      "citeRegEx" : "Chu.,? 1965",
      "shortCiteRegEx" : "Chu.",
      "year" : 1965
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Active learning for deep semantic parsing",
      "author" : [ "Long Duong", "Hadi Afshar", "Dominique Estival", "Glen Pink", "Philip Cohen", "Mark Johnson." ],
      "venue" : "ACL.",
      "citeRegEx" : "Duong et al\\.,? 2018",
      "shortCiteRegEx" : "Duong et al\\.",
      "year" : 2018
    }, {
      "title" : "Optimum branchings",
      "author" : [ "Jack Edmonds." ],
      "venue" : "Journal of Research of the national Bureau of Standards B, 71(4).",
      "citeRegEx" : "Edmonds.,? 1967",
      "shortCiteRegEx" : "Edmonds.",
      "year" : 1967
    }, {
      "title" : "Active Learning for BERT: An Empirical Study",
      "author" : [ "Liat Ein-Dor", "Alon Halfon", "Ariel Gera", "Eyal Shnarch", "Lena Dankin", "Leshem Choshen", "Marina Danilevsky", "Ranit Aharonov", "Yoav Katz", "Noam Slonim." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Ein.Dor et al\\.,? 2020",
      "shortCiteRegEx" : "Ein.Dor et al\\.",
      "year" : 2020
    }, {
      "title" : "Practical, efficient, and customizable active",
      "author" : [ "Alexander Erdmann", "David Joseph Wrisley", "Benjamin Allen", "Christopher Brown", "Sophie Cohen-Bodénès", "Micha Elsner", "Yukun Feng", "Brian Joseph", "Béatrice Joyeux-Prunel", "Marie-Catherine de Marneffe" ],
      "venue" : null,
      "citeRegEx" : "Erdmann et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Erdmann et al\\.",
      "year" : 2019
    }, {
      "title" : "AllenNLP: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke Zettlemoyer." ],
      "venue" : "NLP-OSS.",
      "citeRegEx" : "Gardner et al\\.,? 2018",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2018
    }, {
      "title" : "Active learning on sentiment classification by selecting both words and documents",
      "author" : [ "Shengfeng Ju", "Shoushan Li." ],
      "venue" : "CLSW.",
      "citeRegEx" : "Ju and Li.,? 2012",
      "shortCiteRegEx" : "Ju and Li.",
      "year" : 2012
    }, {
      "title" : "An empirical study of active learning for text classification",
      "author" : [ "Stamatis Karlos", "Nikos Fazakis", "Sotiris Kotsiantis", "Kyriakos Sgarbas." ],
      "venue" : "ASSR, 6(2).",
      "citeRegEx" : "Karlos et al\\.,? 2012",
      "shortCiteRegEx" : "Karlos et al\\.",
      "year" : 2012
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "75 languages, 1 model: Parsing universal dependencies universally",
      "author" : [ "Dan Kondratyuk", "Milan Straka." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kondratyuk and Straka.,? 2019",
      "shortCiteRegEx" : "Kondratyuk and Straka.",
      "year" : 2019
    }, {
      "title" : "Structured prediction models via the matrix-tree theorem",
      "author" : [ "Terry Koo", "Amir Globerson", "Xavier Carreras", "Michael Collins." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Koo et al\\.,? 2007",
      "shortCiteRegEx" : "Koo et al\\.",
      "year" : 2007
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "Active learning for crossdomain sentiment classification",
      "author" : [ "Shoushan Li", "Yunxia Xue", "Zhongqing Wang", "Guodong Zhou." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Active learning for dependency parsing with partial annotation",
      "author" : [ "Zhenghua Li", "Min Zhang", "Yue Zhang", "Zhanyi Liu", "Wenliang Chen", "Hua Wu", "Haifeng Wang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual denoising pretraining for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "TAACL.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal dependencies 2.3. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL)",
      "author" : [ "Joakim Nivre", "Mitchell Abrams", "Željko Agić", "Ahrenberg" ],
      "venue" : "Faculty of Mathematics and Physics,",
      "citeRegEx" : "Nivre et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2018
    }, {
      "title" : "How multilingual is multilingual BERT? In ACL",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette" ],
      "venue" : null,
      "citeRegEx" : "Pires et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Crosslanguage text classification using structural correspondence learning",
      "author" : [ "Peter Prettenhofer", "Benno Stein." ],
      "venue" : "ACL.",
      "citeRegEx" : "Prettenhofer and Stein.,? 2010",
      "shortCiteRegEx" : "Prettenhofer and Stein.",
      "year" : 2010
    }, {
      "title" : "Bilingual active learning for relation classification via pseudo parallel corpora",
      "author" : [ "Longhua Qian", "Haotian Hui", "Ya’nan Hu", "Guodong Zhou", "Qiaoming Zhu" ],
      "venue" : null,
      "citeRegEx" : "Qian et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2014
    }, {
      "title" : "Active learning for part-of-speech tagging: Accelerating corpus annotation",
      "author" : [ "Eric Ringger", "Peter McClanahan", "Robbie Haertel", "George Busby", "Marc Carmen", "James Carroll", "Kevin Seppi", "Deryle Lonsdale." ],
      "venue" : "LAW.",
      "citeRegEx" : "Ringger et al\\.,? 2007",
      "shortCiteRegEx" : "Ringger et al\\.",
      "year" : 2007
    }, {
      "title" : "Ef: Introduction to the conll2002 shared task",
      "author" : [ "Tjong Kim Sang" ],
      "venue" : null,
      "citeRegEx" : "Sang.,? \\Q2002\\E",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Burr Settles." ],
      "venue" : "Technical report, University of Wisconsin-Madison Department of Computer Sciences.",
      "citeRegEx" : "Settles.,? 2009",
      "shortCiteRegEx" : "Settles.",
      "year" : 2009
    }, {
      "title" : "An analysis of active learning strategies for sequence labeling tasks",
      "author" : [ "Burr Settles", "Mark Craven." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Settles and Craven.,? 2008",
      "shortCiteRegEx" : "Settles and Craven.",
      "year" : 2008
    }, {
      "title" : "Deep active learning for named entity recognition",
      "author" : [ "Yanyao Shen", "Hyokun Yun", "Zachary C. Lipton", "Yakov Kronrod", "Animashree Anandkumar." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Probabilistic models of nonprojective dependency trees",
      "author" : [ "David A Smith", "Noah A Smith." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Smith and Smith.,? 2007",
      "shortCiteRegEx" : "Smith and Smith.",
      "year" : 2007
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "mt5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "arXiv preprint arXiv:2010.11934.",
      "citeRegEx" : "Xue et al\\.,? 2020",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2020
    }, {
      "title" : "Following (Kondratyuk and Straka, 2019), we use the output of the last BERT layer in place of the embeddings generated by the Bi-LSTM layers. These embeddings are then concatenated with the POS embeddings. A head",
      "author" : [ "Manning" ],
      "venue" : null,
      "citeRegEx" : "Manning and 2017..,? \\Q2019\\E",
      "shortCiteRegEx" : "Manning and 2017..",
      "year" : 2019
    }, {
      "title" : "2018), using pre-trained models released by HuggingFace (Wolf et al., 2020). E SM Full vs MM Full Performance Given that the SMA setup uses 1{nth the number",
      "author" : [ "Python", "PyTorch", "AllenNLP (Gardner" ],
      "venue" : null,
      "citeRegEx" : "Python et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Python et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Recently, another direction that has gained popularity has been leveraging multilingaul pre-trained language models (MPLMs) which inherently map multiple languages to a common embedding space (Devlin et al., 2019; Conneau et al., 2020).",
      "startOffset" : 192,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "Recently, another direction that has gained popularity has been leveraging multilingaul pre-trained language models (MPLMs) which inherently map multiple languages to a common embedding space (Devlin et al., 2019; Conneau et al., 2020).",
      "startOffset" : 192,
      "endOffset" : 235
    }, {
      "referenceID" : 27,
      "context" : "Another orthogonal line of work aimed at building models under a constrained budget has been active learning (AL) (Shen et al., 2018; Ein-Dor et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "Another orthogonal line of work aimed at building models under a constrained budget has been active learning (AL) (Shen et al., 2018; Ein-Dor et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 155
    }, {
      "referenceID" : 27,
      "context" : "While this has shown to improve annotation efficiency, the predominant approach has been to train one model per language, using the (language specific) model for AL (Shen et al., 2018; Erdmann et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "While this has shown to improve annotation efficiency, the predominant approach has been to train one model per language, using the (language specific) model for AL (Shen et al., 2018; Erdmann et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "Further, while the benefits of using AL in conjunction with MPLMs has been studied for a monolingual setup (Ein-Dor et al., 2020), we show that AL also yields benefits in the multilingual setup.",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "Effective utilization of annotation budgets has been the area of focus for numerous active learning works, showing improvements for different tasks like POS tagging (Ringger et al., 2007), sentiment analysis (Karlos et al.",
      "startOffset" : 165,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : ", 2007), sentiment analysis (Karlos et al., 2012; Li et al., 2013; Brew et al., 2010; Ju and Li, 2012), syntactic parsing (Duong et al.",
      "startOffset" : 28,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : ", 2007), sentiment analysis (Karlos et al., 2012; Li et al., 2013; Brew et al., 2010; Ju and Li, 2012), syntactic parsing (Duong et al.",
      "startOffset" : 28,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : ", 2007), sentiment analysis (Karlos et al., 2012; Li et al., 2013; Brew et al., 2010; Ju and Li, 2012), syntactic parsing (Duong et al.",
      "startOffset" : 28,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : ", 2007), sentiment analysis (Karlos et al., 2012; Li et al., 2013; Brew et al., 2010; Ju and Li, 2012), syntactic parsing (Duong et al.",
      "startOffset" : 28,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : ", 2010; Ju and Li, 2012), syntactic parsing (Duong et al., 2018), and named entity recognition (Settles and Craven, 2008; Shen et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 26,
      "context" : ", 2018), and named entity recognition (Settles and Craven, 2008; Shen et al., 2018).",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 27,
      "context" : ", 2018), and named entity recognition (Settles and Craven, 2008; Shen et al., 2018).",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al.",
      "startOffset" : 70,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al.",
      "startOffset" : 70,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al.",
      "startOffset" : 70,
      "endOffset" : 149
    }, {
      "referenceID" : 30,
      "context" : "For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al.",
      "startOffset" : 70,
      "endOffset" : 149
    }, {
      "referenceID" : 20,
      "context" : ", 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019; Liu et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : ", 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019; Liu et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "We use the standard training methodology for the tasks: for classification, we use a single layer over the [CLS] embedding, for sequence tagging, we use a single layer for each word to predict its tag, and for dependency parsing, we follow Kondratyuk and Straka (2019) and use mBERT embeddings with the graph-based bi-affine attention parser (Dozat and Manning, 2017); refer Appendix A for details.",
      "startOffset" : 342,
      "endOffset" : 367
    }, {
      "referenceID" : 25,
      "context" : "3 Active Learning Acquisition Strategies: The field of active AL tends not to reveal explicit winners—though there is a general consensus that AL does indeed outperform passive learning (Settles, 2009).",
      "startOffset" : 186,
      "endOffset" : 201
    }, {
      "referenceID" : 27,
      "context" : "Thus, we adopt the simplest confidence based strategies to demonstrate their efficacy for each task : Least Confidence (LC) for classification, Maximum Normalized Log Probability (MNLP) (Shen et al., 2018) for sequence tagging, and normalized log probability of decoded tree (NLPDT) (Li et al.",
      "startOffset" : 186,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : ", 2018) for sequence tagging, and normalized log probability of decoded tree (NLPDT) (Li et al., 2016) for dependency parsing.",
      "startOffset" : 85,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "1 Dataset Details Classification: We consider Sentiment Analysis, using the Amazon Reviews dataset (Prettenhofer and Stein, 2010).",
      "startOffset" : 99,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "(Sang, 2002; Tjong Kim Sang and De Meulder, 2003) with 4 languages: English (en), Spanish (es), German (de) and Dutch (nl), and 4 named entities: Location, Person, Organization and Miscellaneous.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "3 corpus (Nivre et al., 2018); a total of 11 treebanks.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "Dependency Parsing: We use a graph-based biaffine attention parser introduced in (Dozat and Manning, 2017).",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "Following (Kondratyuk and Straka, 2019), we use the output of the last BERT layer in place of the embeddings generated by the Bi-LSTM layers.",
      "startOffset" : 10,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "During inference, the best dependency parse is generated by decoding with Chu-Liu/Edmonds algorithm (Chu, 1965; Edmonds, 1967).",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "During inference, the best dependency parse is generated by decoding with Chu-Liu/Edmonds algorithm (Chu, 1965; Edmonds, 1967).",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Following (Devlin et al., 2019) For words generating multiple wordpieces, we use the embedding of the first wordpiece.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 27,
      "context" : "This AL strategy has been shown to be extremely effective for NER (Shen et al., 2018) and hence we adopt it in our setting.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "This acquisition strategy has been commonly applied in classification tasks, and although simple, has been consistently shown to often perform extremely well (Settles, 2009); consequently, we adopt it in our setting.",
      "startOffset" : 158,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "Following (Li et al., 2016), we also normalize this score by N , where N indicates the number of tokens.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "We also tried normalizing by N2, as well as a globally normalized probability of d ̊ (probability of the tree over all possible valid trees, with the partition function computed using the Matrix Tree Theorem (Koo et al., 2007; Smith and Smith, 2007)), but found both to perform worse.",
      "startOffset" : 208,
      "endOffset" : 249
    }, {
      "referenceID" : 28,
      "context" : "We also tried normalizing by N2, as well as a globally normalized probability of d ̊ (probability of the tree over all possible valid trees, with the partition function computed using the Matrix Tree Theorem (Koo et al., 2007; Smith and Smith, 2007)), but found both to perform worse.",
      "startOffset" : 208,
      "endOffset" : 249
    }, {
      "referenceID" : 3,
      "context" : "For each experiment, we perform an LR search over (1e-5, 2e-5, 3e-5, 4e-5 and 5e-5), and choose the best LR according to the performance on the appropriate validation (sub)set, as recommended in (Devlin et al., 2019).",
      "startOffset" : 195,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "In all experiments, we set the batch size to 32 and use an Adam (Kingma and Ba, 2015) optimizer.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "Code All code used in this work was implemented using Python, PyTorch and AllenNLP (Gardner et al., 2018), using pre-trained models released by HuggingFace (Wolf et al.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "E SM Full vs MM Full Performance Given that the SMA setup uses 1{nth the number of parameters, an interesting question is whether fewer parameters leads to a loss in any expressive power for the single model, which might potentially lead to poorer performance (curse of multilinguality (Conneau et al., 2020)).",
      "startOffset" : 286,
      "endOffset" : 308
    } ],
    "year" : 0,
    "abstractText" : "When tasked with supporting multiple languages for a given problem, two approaches have arisen: training a model for each language with the annotation budget divided equally among them, and training on a high-resource language followed by zero-shot transfer to the remaining languages. In this work, we show that the strategy of joint learning across multiple languages using a single model performs substantially better than the aforementioned alternatives. We also demonstrate that active learning provides additional, complementary benefits. We show that this simple approach enables the model to be data efficient by allowing it to arbitrate its annotation budget to query languages it is less certain on. We illustrate the effectiveness of our proposed method on a diverse set of tasks: a classification task with 4 languages, a sequence tagging task with 4 languages and a dependency parsing task with 5 languages. Our proposed method, whilst simple, substantially outperforms the other viable alternatives for building a model in a multilingual setting under constrained budgets.",
    "creator" : null
  }
}
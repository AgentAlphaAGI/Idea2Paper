{
  "name" : "ARR_2022_271_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Seq2Path: Generating Sentiment Tuples as Paths of a Tree",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A tree can represent “1-to-N” relations while a sequence can only represent “1-to-1” relations (e.g., an aspect term may correspond to multiple opinion terms). Each generated sentiment tuple can be viewed as a path of the tree. The paths do not have orders, so they can be generated independently.\nIn this paper, we propose Seq2Path to generate sentiment tuples by formulating it as a sequence-to-path problem. For training, we treat each path as an independent target, and we calculate the average loss over paths. For inference, we apply the beam search with constrained decoding. By introducing an additional “discriminative token”, paths of the tree can be automatically selected by pruning.\nWe conduct experiments on five tasks including AOPE, ASTE, TASD, UABSA, ACOS. We evaluate our method on four common benchmark datasets including Laptop14, Rest14, Rest15, Rest16. Our proposed method achieves state-of-the-art results in almost all cases."
    }, {
      "heading" : "1 Introduction",
      "text" : "ABSA tasks. Aspect-based sentiment analysis (ABSA) is a classic research topic and has received continuous attention. The ABSA tasks aim to extract sentiment elements including the aspect term (a), opinion term (o), aspect category (c), and sentiment polarity (s), respectively.\nThe recent trend is to design a unified framework to solve multiple ABSA tasks. We follow the tasks\ndefinitions in (Zhang et al., 2021b) and the output formats are shown in Table 1.\nAssumption. Throughout this paper, we always consider the ASTE as our default task to illustrate our ideas.\nSeq2Seq for ABSA. Recently, the Seq2Seq framework has been applied to ABSA tasks (Yan et al., 2021; Zhang et al., 2021a,b) by formulating them as a text-to-text problem\nInput text⇒ “(a1, o1, s1), (a2, o2, s2), ...” Despite the success of the Seq2Seq models, they have two main drawbacks:\n• Orders. The orders between the tuples does not naturally exist. • Independence. The generation of (a2, o2, s2) should not condition on (a1, o1, s1). As a result, the model trained on such “sequence targets” will be confused to make decisions between consecutive tuples. For example, why does (a1, o1, s1) have to be the first tuple? Should (a1, o1, s1) be followed by (a2, o2, s2) or “EOS”?\nSeq2Path for ABSA. We claim that the output can be better represented by a tree. As in Figure 1, a tree can represent “1-to-N” relations while a sequence can only represent “1-to-1” relations. The two sentiment tuples (“rolls”, “big”, “positive”) and (“rolls”, “not good”, “negative”) share the same aspect term “rolls”. It is a “1-to-N” relation because the aspect term “rolls” corresponds to two opinion\nterms. Now, we propose Seq2Path to generate sentiment tuples by formulating it as a sequence-topath problem\nInput text⇒  path-1: “a1, o1, s1” path-2: “a2, o2, s2” ...\nwhere each sentiment tuple (ai, oi, si) can be viewed as a path of a tree and can be independently generated. As long as the input text is given, one can determine any of the valid sentiment tuples independently. For example, one can determine (a2, o2, s2) is a valid sentiment tuple without knowing that (a1, o1, s1) is also a valid one.\nFor training, we treat every sentiment tuple as an independent target. We use the ordinary Seq2Seq model to learn each target and take the average loss. During inference, we apply the beam search to generate multiple paths along with their probabilities. The path with high probability is more likely to be correct, but not always the case. Thus, we introduce a discriminative token to automatically select correct paths from the beam search. We also augment the dataset to produce negative samples for the discriminative token.\nContributions. We list our main contributions and the organization of this paper as follows:\n• In Section 2, we propose Seq2Path, a parallel generative framework for ABSA. Seq2Path formulates the output as a tree and generates sentiment tuples as paths of a tree. • In Section 2.2 we discuss the details for training. We calculate the loss averaged over paths to learn the multi-targets. A discriminative token is introduced to automatically select valid paths. We also augment the dataset to produce the negative samples for the discriminative token. • In Section 2.3 we discuss the details for inference. We apply the beam search to generate paths independently with constrained decoding and pruning. • In Section 3, we discuss the learning of the conditional transition probability of the token generation process, which explains why Seq2Path is better than Seq2Seq models. • In Section 4, experimental results show that our model achieves state-of-the-art on four widely used datasets Laptop14, Rest14, Rest15, Rest16 on the AOPE, UABSA, ASTE, TASD, ACOS tasks. Our method outperforms the baseline models on F1 score in almost all cases."
    }, {
      "heading" : "2 Method",
      "text" : ""
    }, {
      "heading" : "2.1 Overview of Seq2Path",
      "text" : "The input is the text and the output is the set of all valid sentiment tuples with a binary discriminative token v ∈ {“true”, “false”} appended in the end:\nAOPE : Input text⇒ “a, o, v” ASTE : Input text⇒ “a, o, s, v” TASD : Input text⇒ “c, a, s, v”\nUABSA : Input text⇒ “a, s, v” ACOS : Input text⇒ “c, a, o, s, v”\nwhere a, o, c, s denotes the aspect, opinion, category, sentiment, respectively. The token generation process forms a tree, and we apply the beam search to produce the k paths with top sequence probabilities and output the valid ones by pruning. Since there are no negative samples for the discriminative token, we have to construct an augmented dataset for training."
    }, {
      "heading" : "2.2 Training",
      "text" : "Loss averaged over paths. For an input sentence x, we want to output a set of tuples\nY = {y1, ..., yk} (1)\nThe dataset D is a collection of (x, Y ) pairs. As shown in Figure 1, the set Y can be represented as a tree. Then each y corresponds to a path of the tree and k is the total number of paths.\nFor notation simplicity, we write yi → y from now on. For the prediction Ŷ from the input x, the loss can be defined as the average loss of the k paths\nL(Y, Ŷ |x) (2)\n= 1\nk ∑ y∈Y Lseq(y, ŷ|x) (3)\n= 1\nk ∑ y∈Y ∑ t l(yt, ŷt|x, y<t) (4)\nwhere Lseq denotes the ordinary Seq2Seq loss and l denotes the loss for each time step.\nData augmentation. Since there are no negative examples for the discriminative token, the data augmentation step is necessary. In order to automatically select the valid paths, a discriminative token v = “false” is appended at the end of each generated negative sample. We generate negative samples in the following two ways\n• D′1: To improve the model’s ability to match tuple elements, we randomly replace the tuple elements. For example, in Figure 1, we generate “rolls, wasn’t fresh, positive, false”, “sashimi, big, negative, false”, etc.\n• D′2: To improve the model’s ability to filter most of bad generations, we first train a “pseudo” model for small epochs then use the beam search to generate negative examples. For example, in Figure 1, we generate “sashimi, n’t fresh, negative, false”, etc.\nThen the augmented dataset is the union of the positive and negative samples\nDaug = D ⋃ D′ = D ⋃ (D′1 ⋃ D′2) (5)\nLoss mask for negative samples. When training on D′, we want the discriminative token v to be able to filter wrong paths. However, we do not want the model’s generation to follow the negative examples. We apply a tricky loss mask here. Suppose y = (y1, y2, ...yt, ...), the loss mask is defined as follows.\n• If y is a negative sample, i.e., the validation token of y is “false”, then the loss mask is\nm(yt) =  1, yt = “false” 1, yt = “EOS” 0, o.w.\n(6)\n• If y is a positive sample, i.e., the validation token of y is “true”, then the loss mask does not apply. In other words, we always have\nm(yt) = 1. (7)\nThe loss mask means the token is skipped in loss calculating, see an example in Table 2. All tokens except the discriminative token and the “EOS” token are masked.\nLet Lm(·) be the loss with the loss mask where only tokens with m(t) = 1 are involved in loss calculating\nLm(Y, Ŷ |x) = 1 k ∑ y∈Y ∑ m(t)=1 l(yt, ŷt|x, y<t) (8)\nand the loss for the augmented dataset is Loss(Daug) = ∑\n(x,Y )∈Daug\nLm(Y, Ŷ |x). (9)"
    }, {
      "heading" : "2.3 Inference",
      "text" : "Beam search. During the inference phase, we apply the beam search with constrained decoding. The top-k predictions are returned along with their sequence probabilities. Recall that the beam search algorithm (Srivastava et al., 2014) selects multiple alternatives for an input sequence at each step based on conditional probability. It is commonly used in Seq2Seq based models during inference. We apply the beam search algorithm to output top-k beams with decreasing probabilities. We observe that the probabilities from the beam search represent how likely the predictions are valid.\nThe constrained decoding is applied for the beam search. Instead of searching the whole vocabulary, we force the beam search to search within only the allowed candidate tokens (inspired by (De Cao et al., 2020)). The candidate tokens are either from the input text or some extra task-specific tokens. For example, the ASTE task has the extra tokens including the sentiment polarities “positive”, “negative”, “neutral” and the separator token.\nPruning. We apply pruning to filter invalid paths. First, we remove some “overlapping” predictions. For example,\n• If the beam search returns both “a, o, s, true” and “a, o, s, false”, we prefer the one with the higher sequence probability.\n• If the beam search returns both “a1, o, s, true” and “a2, o, s, true” where a1 and a2 are overlapping, then we also prefer the one with the higher sequence probability.\nPlease refer to the appendix for detailed conditions. Then, we output the valid paths with a discriminative token vi = “true” and filter the other invalid paths."
    }, {
      "heading" : "2.4 Algorithm",
      "text" : "We summarize our method in Algorithm 1.\nAlgorithm 1: Seq2Path. Input: A training dataset D, pretrained\nT5-base, numbers of epochs naug < n, beam size k.\nOutput: Valid sentiment tuples. 1 Train a “pseudo” T5-base model Maug on\nD for naug epochs; 2 Generate negative samples D′1 by Maug\nfrom beam search and D′2 from randomly replacing tuple elements. The augmented dataset Daug as in (5) where each target tuple is appended with a binary discriminative token;\n3 Train another T5-base model M on Daug for n epochs with loss masked on negative samples as in (9); 4 Inference with the model M and apply beam search with constrained decoding. Top k paths are returned in decreasing probabilities; 5 Apply the pruning to select the valid paths; 6 return."
    }, {
      "heading" : "3 Why Seq2Path?",
      "text" : "In this section, we give more detailed motivation for Seq2Path. We study the problem of learning the conditional transition probabilities for the token generation process\nP (yt = vi|x, y<t) (10)\nwhere x is the input sentence and\ny<t = (y1, y2, ..., yt−1) (11)\nrepresents the previous tokens and V = {v1, v2, ...} is the vocabulary.\nIntuitive case. Again, we take the example in Figure 1. Seq2Seq models formulate the output as sequence “(a1, o1, s1), (a2, o2, s2), ...”, then the target probability distribution at each time step t is a one-hot vector in R|V |.\nP (y4|x, y<4) = { 1, y4 = “big” 0, o.w.\n(12)\nHowever, true target probability distribution is actually a multi-hot vector in R|V |. For y<4 = (“BOS”, “rolls”, “,”),\nP (y4|x, y<4) =  0.5, y4 = “big” 0.5, y4 = “not” 0, o.w.\n(13)\nWhy average loss over paths? Recall, during training, we treat each sentiment tuple as an independent target and calculate the average of Seq2Seq losses. Here we justify it. Formally, suppose the target contains k paths with the previous tokens x, y<t, say\npath-1 : (x, y1, y2, ..., yt−1, vj1 , ...), path-2 : (x, y1, y2, ..., yt−1, vj2 , ...),\n...\npath-k : (x, y1, y2, ..., yt−1, vjk , ...).\nThe next token could be vj1 , ..., vjk , hence the target transition probability is a “multi-hot” vector p ∈ R|V |\np[i] =\n{ 1 k , i = j1, ..., jk,\n0, o.w. (14)\nOn the other hand, if we treat each target as an independent ordinary Seq2Seq output, then target probability for i-th path is a “one-hot” vector p′i ∈ R|V |\np′i[`] =\n{ 1, ` = ji,\n0, o.w. (15)\nThe next lemma explains the motivation on why Seq2Path is trained with loss averaged over paths. The proof is simple and can be found in the appendix.\nLemma 1 The average cross-entropy loss for the “one-hot” target (15) is equal to the cross-entropy loss for the multi-hot transition probability (14).\nThe purpose of this lemma is to show, the “multihot” conditional transition probability distribution for the token generation process can be learned properly with our loss. However, the previous Seq2Seq models learns a “one-hot” target probability distribution, which is biased."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Datasets We evaluate the proposed framework on four widely used benchmark datasets: Laptop14, Rest14, Rest15, and Rest16, originally provided by the SemEval shared challenges (Pontiki et al., 2014, 2015, 2016). We adopt the dataset provided by (Fan et al., 2019; Li et al., 2019a; Xu et al., 2020; Wan et al., 2020) for the AOPE, UABSA, ASTE, TASD, ACOS tasks respectively. For a fair comparison, we keep the same data splits as previous works.\nIn the following, we list the main baselines for each ABSA task. Several early baselines are skipped, especially those not encoded with BERT or T5. AOPE baselines:\n• SpanMlt (Zhao et al., 2020) is an end-to-end method to jointly extract the aspect and opinion, and the pair-wise relation by a unified multitask framework. • SDRN (Chen et al., 2020) proposes a synchronous dual-channel recursive network to simultaneously extract the opinion entities and relationships. • GAS-T5 (Zhang et al., 2021b) uses a unified generation method to solve various ABSA problems, and encodes natural language tags into the target output. ASTE baselines: • Jet-BERT (Xu et al., 2020) uses two sets of\nBIO tags to annotate aspect and opinion terms in the same sequence in an end-to-end manner. • Dual-MRC(Mao et al., 2021) solves all ABSA tasks through a unified joint training framework of two BERT-MRC. • GAS-T5 (Zhang et al., 2021b) was described previously. • ParaGen-T5 (Zhang et al., 2021a) treats quad prediction as a paraphrase generation. TASD baselines: • TAS (Wan et al., 2020) is the first to introduce\nthe target aspect sentiment detection task. • GAS-T5 (Zhang et al., 2021b) was described\npreviously.\nUABSA baselines: • BERT+GRU (Li et al., 2019b) add linear layer\nand GRU on the basis of BERT. • SPAN-BERT (Hu et al., 2019) adopts a new\ndata annotation method to extract the target and uses a heuristic algorithm to extract. • IMN-BERT (He et al., 2019) uses document corpus to perform the multi-task training with part of the shared parameters. • RACL (Chen and Qian, 2020) is a stacked multi-layer network based on BERT with a relation propagation mechanism. • Dual-MRC (Mao et al., 2021) was described previously. • GAS-T5 (Zhang et al., 2021b) was described previously. • ParaGen-T5 (Zhang et al., 2021a) was described previously.\nACOS baselines: • TAS (Cai et al., 2021) is the first to introduce\nthe aspect-category-opinion-sentiment quad prediction task.\nEvaluation metrics. We use the F1 score as the evaluation metrics, when all elements of the prediction result are correct, the prediction result is considered correct. As a fair comparison, all F1 scores reported in this paper are averaged over 5 runs with different random seeds.\nImplementation details. We use Google’s T5base model (Raffel et al., 2019) from Huggingface Transformer library1. The structure of the T5 encoder and decoder is similar to that of the Transformer (Vaswani et al., 2017). Although T5-base and BERT-base are both named as “base” models, the T5-base should be more powerful because it has more parameters and trained on a larger corpus. Since sentiment tuples are generated independently, the maximum output length can be very small (32) comparing to the maximum sequence length of T5 (128), which can reduce a lot of memory consumption.\nFor every ABSA subtask, we use a batch size 8 and a learning rate 1e−4 to train the model with a single Nvidia 1080Ti GPU. We first train the model with naug = 5 epochs for augmentation. Then the final model is trained for n = 20 epochs. The best model is determined based on the loss on the validation set. For inference, the number of beams depends on the task and dataset and can be\n1https://github.com/huggingface/ transformers\nk = 4, 6, 8, 10. Typically, k = 6 can be used for most cases. In addition, we use the separator = “|” in our experiments. It seems to be slightly better than separator = “,” that is probably due to the T5 decoding mechanism."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "The main results for the AOPE, UABSA, ASTE, TASD, ACOS tasks are reported in Table 3, 4, 5, 6, 7, respectively. Most baseline results are directly copied from (Zhang et al., 2021b). Our proposed method achieves the new state-of-the-art results in almost all F1 scores.\nOn the AOPE task, our proposed Seq2Path outperforms the previous best results by 4.74, 1.75, 3.91, 3.67 in percentage on Laptop14, Rest14, Rest15, Rest16 respectively. The main challenge for the AOPE task is to match the aspect a and the opinion o where there are many complex “1-to-N” relations. Our Seq2Path has a large performance gain because it can handle these complex relations very well.\nOn the ASTE task, our proposed Seq2Path outperforms the previous best results by 4.14, 3.36, 3.32, 1.97 in percentage on Laptop14, Rest14, Rest15, Rest16 respectively. The ASTE task is similar to the AOPE task, but ASTE is even harder as the sentiment s is also required. Again, our Seq2Path has a large performance gain.\nOn the TASD task, our proposed Seq2Path outperforms the previous best results by 2.14, 0.13 2 in percentage on Rest15, Rest16 respectively. The challenge for the TASD task is that the aspect terms a can be “NULL”, and an aspect a can have multiple categories c.\nOn the UABSA task, our proposed Seq2Path outperforms the previous best results by 1.36, 1.67, 2.23 in percentage on Laptop14, Rest15, Rest16 respectively. The result on Rest14 is slightly lower (almost equal) than GAS. The UABSA task is classic and relatively easier. One challenge is that the output can be a null set if there is no aspect in the input text. For such cases, it is most likely that all beams of Seq2Path will have a discriminative token v = “false”. Thus, our method is consistent with such a setting.\nThe ACOS task is newly released and the original paper is the only baseline available. Our proposed Seq2Path outperforms the previous best results by 7.17, 13.80 in percentage on Laptop14, Rest16 respectively. This improvement is huge and should be partially from the power of T5.\n2The “prompt” technique makes ParaGen a strong baseline. It may also be potentially helpful and we may also try it in the future."
    }, {
      "heading" : "4.3 Analysis",
      "text" : "Analysis on the beam size. The main results in Table 3, 4, 5, 6, 7 use various beam sizes of 4, 6, 8, 10. The beam size k is an important hyper-parameter which affects both data augmentation and inference. The choice of the optimal k depends on the task and the dataset. Roughly speaking, a smaller beam size will lead a worse recall while a larger beam size will lead a worse precision. Nevertheless, with our pruning process, our results are state-of-the-art regardless of the choice of k.\nAlthough the beam search for inference will require a larger GPU memory, the Seq2Path can use a much shorter max output sequence length. Then, the memory consumption will be reduced by a lot.\nAblation study on data augmentation. The purpose of data augmentation is to generate negative examples for the discriminative token to automatically select the paths. The ablation results for data augmentation are shown in Table 8.\nThe dataset D′1 has minor effects on the F1 scores. For most cases, adding D′1 can improve the F1 scores by up to 3%. It consists of negative\nexamples by randomly replacing tuple elements and improves the model’s ability to match tuple elements. The performance on Laptop14, Rest15 and Rest16 can benefit from D′1. However, D ′ 1 seems to lead a slight performance drop on Rest14. One possible reason is that Rest14 has the biggest sample size and D′1 may not be necessary.\nThe dataset D′2 is much more important than D ′ 1 on the F1 scores. It consists of negative examples from the beam search. Without D′2, the F1 scores are extremely low because it improves the model’s ability to filter most of bad generations.\nCase study. Figure 3 shows an example of the beam search generation for the ASTE task with the beam size k = 6. The probabilities are the sequence probabilities from the beam search. The top 3 paths with v =“true” are valid where they are marked as bold. The other 3 paths with v =“false” are filtered. The tuple “staff, chinese, positive” is a valid one because the probability for the token “true” is larger than “false” 0.7114 > 0.4425. The other paths such as “chinese stuff, nice, positive” are pruned, because it is not as good as others ones with higher probabilities."
    }, {
      "heading" : "5 Related Work",
      "text" : "There have been several studies addressing technical solutions for aspect-based sentiment analysis. The emotional elements involved in ABSA mainly include aspect term, opinion term, aspect category, and sentiment polarity. In order to extract these emotional elements, the main research direction of\nABSA is to extract aspect terms (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) and categorize the sentiment of a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020).\nThe early ABSA problems were mostly expressed as sequence labeling or multi-classification problems (Li et al., 2019a), which were predicted by designing task-specific classification networks and using the class index as labels for training (Huang and Carley, 2019; Wan et al., 2020). However, this approach requires the design of different classification models and ignores the label semantics. Recent works achieve good performance by converting the ABSA problems as a text generation problem (Yan et al., 2021; Zhang et al., 2021a,b).\nIn addition, the generative framework has been proven effective for many other natural language processing problems including dialogue state tracking (Feng et al., 2020), entity linking (De Cao et al., 2020), event extraction (Lu et al., 2021), information extraction (Sui et al., 2020), named entity recognition (Yan et al., 2021; Tan et al., 2021; Raffel et al., 2019; Athiwaratkun et al., 2020). In particular, (Paolini et al., 2021) solved various NLP tasks in a unified generative framework."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we propose Seq2Path, a novel parallel generative framework for ABSA. The previous Seq2Seq models formulates the output as a sequence, however, it has two main drawbacks of the order and the independence. Our proposed Seq2Path formulates the output as a tree and generates sentiment tuples as paths of a tree. Seq2Path can correctly learn the conditional transition probability for the token generation, by training with the loss averaged over paths. During inference, we apply beam search with constrained decoding and pruning. A discriminative token is also introduced to automatically select paths. Experiments show that our model achieves state-of-the-art on AOPE, ASTE, TASD, UABSA, ACOS tasks across Laptop14, Rest14, Rest15, Rest16 datasets in almost all cases. In the future, we plan to adapt our method to other structure prediction tasks in NLP, e.g., information extraction tasks."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Constrained Decoding Traditionally, a probability distribution is calculated over the whole vocabulary at each step of decoding. Indeed, the candidate token set can be reduced. For the input sentence x, the candidate tokens are the union of the original tokens in x and some extra task-specific candidate tokens\nT (x, task) = T (x) ⋃ T (task) (16)\nwhere T (x) stands for the token set for the input x and the task specific tokens T (task) are defined in Table 9.\nA.2 Pruning We define the condition when two predictions are “overlapping” for a specific task in Table 10. If two predictions are overlapping, then we prefer the one with a higher probability.\nA.3 Proof of Lemma 1 Proof: First, we consider the loss for the multihot vector. The cross-entropy loss for the target\nprobability distribution p ∈ R|V | and the predicted probability distribution ŷt ∈ R|V | is\nl(p, ŷt) = − n∑\ni=1\np[i] log ŷt[i] (17)\n= − k∑\ni=1\n1 k log ŷt[ji]. (18)\nThe equation holds because p ∈ R|V | is “multi-hot” and p[i] = 1 if i = ji for i = 1, 2, ..., k. Now, we consider the loss average over paths. For i-th path p′i ∈ R|V |,\nl(p′i, ŷt) = − n∑\n`=1\np′i[`] log ŷt[`] (19)\n= − log ŷt[ji]. (20)\nThe equation holds because p′i ∈ R|V | is “one-hot” and p′i[`] = 1 if ` = ji. Therefore, it follows that\n1\nk k∑ i=1 l(p′i, ŷt) = l(p, ŷt) (21)\nand the proof is done."
    } ],
    "references" : [ {
      "title" : "Augmented natural language for generative sequence labeling",
      "author" : [ "Ben Athiwaratkun", "Cicero Nogueira dos Santos", "Jason Krone", "Bing Xiang." ],
      "venue" : "arXiv preprint arXiv:2009.13272.",
      "citeRegEx" : "Athiwaratkun et al\\.,? 2020",
      "shortCiteRegEx" : "Athiwaratkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspectcategory-opinion-sentiment quadruple extraction with implicit aspects and opinions",
      "author" : [ "Hongjie Cai", "Rui Xia", "Jianfei Yu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
      "citeRegEx" : "Cai et al\\.,? 2021",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2021
    }, {
      "title" : "Recurrent attention network on memory for aspect sentiment analysis",
      "author" : [ "Peng Chen", "Zhongqian Sun", "Lidong Bing", "Wei Yang." ],
      "venue" : "Proceedings of the 2017 conference on empirical methods in natural language processing, pages 452–461.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Synchronous doublechannel recurrent network for aspect-opinion pair extraction",
      "author" : [ "Shaowei Chen", "Jie Liu", "Yu Wang", "Wenzheng Zhang", "Ziming Chi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Relation-aware collaborative learning for unified aspect-based sentiment analysis",
      "author" : [ "Zhuang Chen", "Tieyun Qian." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3685–3694.",
      "citeRegEx" : "Chen and Qian.,? 2020",
      "shortCiteRegEx" : "Chen and Qian.",
      "year" : 2020
    }, {
      "title" : "Autoregressive entity retrieval",
      "author" : [ "Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni." ],
      "venue" : "arXiv preprint arXiv:2010.00904.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Target-oriented opinion words extraction with target-fused neural sequence labeling",
      "author" : [ "Zhifang Fan", "Zhen Wu", "Xinyu Dai", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "A sequenceto-sequence approach to dialogue state tracking",
      "author" : [ "Yue Feng", "Yang Wang", "Hang Li." ],
      "venue" : "arXiv preprint arXiv:2011.09553.",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "arXiv preprint arXiv:1906.06906.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Open-domain targeted sentiment analysis via span-based extraction and classification",
      "author" : [ "Minghao Hu", "Yuxing Peng", "Zhen Huang", "Dongsheng Li", "Yiwei Lv." ],
      "venue" : "arXiv preprint arXiv:1906.03820.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Parameterized convolutional neural networks for aspect level sentiment classification",
      "author" : [ "Binxuan Huang", "Kathleen M Carley." ],
      "venue" : "arXiv preprint arXiv:1909.06276.",
      "citeRegEx" : "Huang and Carley.,? 2019",
      "shortCiteRegEx" : "Huang and Carley.",
      "year" : 2019
    }, {
      "title" : "A challenge dataset and effective models for aspect-based sentiment analysis",
      "author" : [ "Qingnan Jiang", "Lei Chen", "Ruifeng Xu", "Xiang Ao", "Min Yang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "A unified model for opinion target extraction and target sentiment prediction",
      "author" : [ "Xin Li", "Lidong Bing", "Piji Li", "Wai Lam." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6714–6721.",
      "citeRegEx" : "Li et al\\.,? 2019a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect term extraction with history attention and selective transformation",
      "author" : [ "Xin Li", "Lidong Bing", "Piji Li", "Wai Lam", "Zhimou Yang." ],
      "venue" : "arXiv preprint arXiv:1805.00760.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting bert for end-to-end aspect-based sentiment analysis",
      "author" : [ "Xin Li", "Lidong Bing", "Wenxuan Zhang", "Wai Lam." ],
      "venue" : "arXiv preprint arXiv:1910.00883.",
      "citeRegEx" : "Li et al\\.,? 2019b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Finegrained opinion mining with recurrent neural networks and word embeddings",
      "author" : [ "Pengfei Liu", "Shafiq Joty", "Helen Meng." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural language processing, pages 1433–1443.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Text2event: Controllable sequence-tostructure generation for end-to-end event extraction",
      "author" : [ "Yaojie Lu", "Hongyu Lin", "Jin Xu", "Xianpei Han", "Jialong Tang", "Annan Li", "Le Sun", "Meng Liao", "Shaoyi Chen." ],
      "venue" : "arXiv preprint arXiv:2106.09232.",
      "citeRegEx" : "Lu et al\\.,? 2021",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring sequence-tosequence learning in aspect term extraction",
      "author" : [ "Dehong Ma", "Sujian Li", "Fangzhao Wu", "Xing Xie", "Houfeng Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3538–",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "A joint training dual-mrc framework for aspect based sentiment analysis",
      "author" : [ "Yue Mao", "Yi Shen", "Chao Yu", "Longjun Cai." ],
      "venue" : "arXiv preprint arXiv:2101.00816.",
      "citeRegEx" : "Mao et al\\.,? 2021",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2021
    }, {
      "title" : "Structured prediction as translation between augmented natural languages",
      "author" : [ "Giovanni Paolini", "Ben Athiwaratkun", "Jason Krone", "Jie Ma", "Alessandro Achille", "Rishita Anubhai", "Cicero Nogueira dos Santos", "Bing Xiang", "Stefano Soatto." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Paolini et al\\.,? 2021",
      "shortCiteRegEx" : "Paolini et al\\.",
      "year" : 2021
    }, {
      "title" : "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
      "author" : [ "Haiyun Peng", "Lu Xu", "Lidong Bing", "Fei Huang", "Wei Lu", "Luo Si." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8600–8607.",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Semeval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "M. Pontiki", "D Galanis", "J. Pavlopoulos", "H. Papageorgiou", "S. Manandhar." ],
      "venue" : "Proceedings of International Workshop on Semantic Evaluation at. 9",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval-2016 task 5: Aspect based sentiment",
      "author" : [ "Maria Pontiki", "Dimitrios Galanis", "Haris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar", "Mohammad Al-Smadi", "Mahmoud Al-Ayyoub", "Yanyan Zhao", "Bing Qin", "Orphée De Clercq" ],
      "venue" : null,
      "citeRegEx" : "Pontiki et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2015 task 12: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitrios Galanis", "Harris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages",
      "citeRegEx" : "Pontiki et al\\.,? 2015",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The journal of machine learning research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint entity and relation extraction with set prediction networks",
      "author" : [ "Dianbo Sui", "Yubo Chen", "Kang Liu", "Jun Zhao", "Xiangrong Zeng", "Shengping Liu." ],
      "venue" : "arXiv preprint arXiv:2011.01675.",
      "citeRegEx" : "Sui et al\\.,? 2020",
      "shortCiteRegEx" : "Sui et al\\.",
      "year" : 2020
    }, {
      "title" : "A sequence-to-set network for nested named entity recognition",
      "author" : [ "Zeqi Tan", "Yongliang Shen", "Shuai Zhang", "Weiming Lu", "Yueting Zhuang." ],
      "venue" : "arXiv preprint arXiv:2105.08901.",
      "citeRegEx" : "Tan et al\\.,? 2021",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Target-aspect-sentiment joint detection for aspect-based sentiment analysis",
      "author" : [ "Hai Wan", "Yufei Yang", "Jianfeng Du", "Yanan Liu", "Kunxun Qi", "Jeff Z Pan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9122–9129.",
      "citeRegEx" : "Wan et al\\.,? 2020",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention-based lstm for aspectlevel sentiment classification",
      "author" : [ "Yequan Wang", "Minlie Huang", "Xiaoyan Zhu", "Li Zhao." ],
      "venue" : "Proceedings of the 2016 conference on empirical methods in natural language processing, pages 606–615.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Position-aware tagging for aspect sentiment triplet extraction",
      "author" : [ "Lu Xu", "Hao Li", "Wei Lu", "Lidong Bing." ],
      "venue" : "arXiv preprint arXiv:2010.02609.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "A unified generative framework for aspect-based sentiment analysis",
      "author" : [ "Hang Yan", "Junqi Dai", "Xipeng Qiu", "Zheng Zhang" ],
      "venue" : "arXiv preprint arXiv:2106.04300",
      "citeRegEx" : "Yan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised word and dependency path embeddings for aspect term extraction",
      "author" : [ "Yichun Yin", "Furu Wei", "Li Dong", "Kaimeng Xu", "Ming Zhang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1605.07843.",
      "citeRegEx" : "Yin et al\\.,? 2016",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis",
      "author" : [ "Mi Zhang", "Tieyun Qian." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3540–3549.",
      "citeRegEx" : "Zhang and Qian.,? 2020",
      "shortCiteRegEx" : "Zhang and Qian.",
      "year" : 2020
    }, {
      "title" : "Aspect sentiment quad prediction as paraphrase generation",
      "author" : [ "Wenxuan Zhang", "Yang Deng", "Xin Li", "Yifei Yuan", "Lidong Bing", "Wai Lam." ],
      "venue" : "arXiv preprint arXiv:2110.00796.",
      "citeRegEx" : "Zhang et al\\.,? 2021a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Towards generative aspect-based sentiment analysis",
      "author" : [ "Wenxuan Zhang", "Xin Li", "Yang Deng", "Lidong Bing", "Wai Lam." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
      "citeRegEx" : "Zhang et al\\.,? 2021b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Spanmlt: a span-based multi-task learning framework for pair-wise aspect and opinion terms extraction",
      "author" : [ "He Zhao", "Longtao Huang", "Rong Zhang", "Quan Lu", "Hui Xue." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "We follow the tasks definitions in (Zhang et al., 2021b) and the output formats are shown in Table 1.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 37,
      "context" : "Table 1: The ABSA tasks with their output formats: AOPE (Zhao et al., 2020; Chen et al., 2020), ASTE (Peng et al.",
      "startOffset" : 56,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "Table 1: The ABSA tasks with their output formats: AOPE (Zhao et al., 2020; Chen et al., 2020), ASTE (Peng et al.",
      "startOffset" : 56,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "Recall that the beam search algorithm (Srivastava et al., 2014) selects multiple alternatives for an input sequence at each step based on conditional probability.",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "We adopt the dataset provided by (Fan et al., 2019; Li et al., 2019a; Xu et al., 2020; Wan et al., 2020) for the AOPE, UABSA, ASTE, TASD, ACOS tasks respectively.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "We adopt the dataset provided by (Fan et al., 2019; Li et al., 2019a; Xu et al., 2020; Wan et al., 2020) for the AOPE, UABSA, ASTE, TASD, ACOS tasks respectively.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 31,
      "context" : "We adopt the dataset provided by (Fan et al., 2019; Li et al., 2019a; Xu et al., 2020; Wan et al., 2020) for the AOPE, UABSA, ASTE, TASD, ACOS tasks respectively.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "We adopt the dataset provided by (Fan et al., 2019; Li et al., 2019a; Xu et al., 2020; Wan et al., 2020) for the AOPE, UABSA, ASTE, TASD, ACOS tasks respectively.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 37,
      "context" : "AOPE baselines: • SpanMlt (Zhao et al., 2020) is an end-to-end method to jointly extract the aspect and opinion, and the pair-wise relation by a unified multitask framework.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "• SDRN (Chen et al., 2020) proposes a synchronous dual-channel recursive network to simultaneously extract the opinion entities and relationships.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 36,
      "context" : "• GAS-T5 (Zhang et al., 2021b) uses a unified generation method to solve various ABSA problems, and encodes natural language tags into the target output.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 31,
      "context" : "ASTE baselines: • Jet-BERT (Xu et al., 2020) uses two sets of BIO tags to annotate aspect and opinion terms in the same sequence in an end-to-end manner.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "• Dual-MRC(Mao et al., 2021) solves all ABSA tasks through a unified joint training framework of two BERT-MRC.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 36,
      "context" : "• GAS-T5 (Zhang et al., 2021b) was described previously.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 35,
      "context" : "• ParaGen-T5 (Zhang et al., 2021a) treats quad prediction as a paraphrase generation.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : "TASD baselines: • TAS (Wan et al., 2020) is the first to introduce the target aspect sentiment detection task.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 36,
      "context" : "• GAS-T5 (Zhang et al., 2021b) was described previously.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "UABSA baselines: • BERT+GRU (Li et al., 2019b) add linear layer and GRU on the basis of BERT.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "• SPAN-BERT (Hu et al., 2019) adopts a new data annotation method to extract the target and uses a heuristic algorithm to extract.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "• IMN-BERT (He et al., 2019) uses document corpus to perform the multi-task training with part of the shared parameters.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "• RACL (Chen and Qian, 2020) is a stacked multi-layer network based on BERT with a relation propagation mechanism.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "• Dual-MRC (Mao et al., 2021) was described previously.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 36,
      "context" : "• GAS-T5 (Zhang et al., 2021b) was described previously.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 35,
      "context" : "• ParaGen-T5 (Zhang et al., 2021a) was described previously.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "ACOS baselines: • TAS (Cai et al., 2021) is the first to introduce the aspect-category-opinion-sentiment quad prediction task.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "We use Google’s T5base model (Raffel et al., 2019) from Huggingface Transformer library1.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "The structure of the T5 encoder and decoder is similar to that of the Transformer (Vaswani et al., 2017).",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 36,
      "context" : "Most baseline results are directly copied from (Zhang et al., 2021b).",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "In order to extract these emotional elements, the main research direction of ABSA is to extract aspect terms (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) and categorize the sentiment of a given aspect (Wang et al.",
      "startOffset" : 109,
      "endOffset" : 179
    }, {
      "referenceID" : 33,
      "context" : "In order to extract these emotional elements, the main research direction of ABSA is to extract aspect terms (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) and categorize the sentiment of a given aspect (Wang et al.",
      "startOffset" : 109,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : "In order to extract these emotional elements, the main research direction of ABSA is to extract aspect terms (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) and categorize the sentiment of a given aspect (Wang et al.",
      "startOffset" : 109,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "In order to extract these emotional elements, the main research direction of ABSA is to extract aspect terms (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) and categorize the sentiment of a given aspect (Wang et al.",
      "startOffset" : 109,
      "endOffset" : 179
    }, {
      "referenceID" : 30,
      "context" : ", 2019) and categorize the sentiment of a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : ", 2019) and categorize the sentiment of a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : ", 2019) and categorize the sentiment of a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 34,
      "context" : ", 2019) and categorize the sentiment of a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : ", 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 178
    }, {
      "referenceID" : 29,
      "context" : ", 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 178
    }, {
      "referenceID" : 20,
      "context" : ", 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 178
    }, {
      "referenceID" : 37,
      "context" : ", 2019; Zhang and Qian, 2020), and to jointly predict multiple elements simultaneously at the same time (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "The early ABSA problems were mostly expressed as sequence labeling or multi-classification problems (Li et al., 2019a), which were predicted by designing task-specific classification networks and using the class index as labels for training (Huang and Carley, 2019; Wan et al.",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : ", 2019a), which were predicted by designing task-specific classification networks and using the class index as labels for training (Huang and Carley, 2019; Wan et al., 2020).",
      "startOffset" : 131,
      "endOffset" : 173
    }, {
      "referenceID" : 29,
      "context" : ", 2019a), which were predicted by designing task-specific classification networks and using the class index as labels for training (Huang and Carley, 2019; Wan et al., 2020).",
      "startOffset" : 131,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "In addition, the generative framework has been proven effective for many other natural language processing problems including dialogue state tracking (Feng et al., 2020), entity linking (De Cao et al.",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 16,
      "context" : ", 2020), event extraction (Lu et al., 2021), information extraction (Sui et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : ", 2021), information extraction (Sui et al., 2020), named entity recognition (Yan et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 32,
      "context" : ", 2020), named entity recognition (Yan et al., 2021; Tan et al., 2021; Raffel et al., 2019; Athiwaratkun et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : ", 2020), named entity recognition (Yan et al., 2021; Tan et al., 2021; Raffel et al., 2019; Athiwaratkun et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : ", 2020), named entity recognition (Yan et al., 2021; Tan et al., 2021; Raffel et al., 2019; Athiwaratkun et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : ", 2020), named entity recognition (Yan et al., 2021; Tan et al., 2021; Raffel et al., 2019; Athiwaratkun et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : "In particular, (Paolini et al., 2021) solved various NLP tasks in a unified generative framework.",
      "startOffset" : 15,
      "endOffset" : 37
    } ],
    "year" : 0,
    "abstractText" : "Aspect-based sentiment analysis (ABSA) tasks aim to extract sentiment tuples from a sentence. Recent generative methods such as Seq2Seq models have achieved good performance by formulating the output as a sequence of text. However, the order between the sentiment tuples does not naturally exist and the generation of the current tuple should not condition on the previous ones. A tree can represent “1-to-N” relations while a sequence can only represent “1-to-1” relations (e.g., an aspect term may correspond to multiple opinion terms). Each generated sentiment tuple can be viewed as a path of the tree. The paths do not have orders, so they can be generated independently. In this paper, we propose Seq2Path to generate sentiment tuples by formulating it as a sequence-to-path problem. For training, we treat each path as an independent target, and we calculate the average loss over paths. For inference, we apply the beam search with constrained decoding. By introducing an additional “discriminative token”, paths of the tree can be automatically selected by pruning. We conduct experiments on five tasks including AOPE, ASTE, TASD, UABSA, ACOS. We evaluate our method on four common benchmark datasets including Laptop14, Rest14, Rest15, Rest16. Our proposed method achieves state-of-the-art results in almost all cases.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_299_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Synchronous Refinement for Language Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, the encoder-decoder framework has obtained impressive results over various language generation tasks, such as machine translation (Vaswani et al., 2017), storytelling (Fan et al., 2018), and text summarization (Song et al., 2019). Typically, decoder first represents those generated target words as a dependent-time target representation, and then uses an attention mechanism to summarize a dependent-time context from the source input for generating the next target word. Since this generated target word is conditioned on previously generated target words at each time step, the decoding process is often called auto-regressive decoding. Finally, decoder generates a target language sentence word-by-word in the auto-regressive decoding manner (Bahdanau et al., 2015; Vaswani et al., 2017).\nHowever, the auto-regressive decoder often encounters an inherent one-pass issue whereby each generated target word is one element of the final output of the language generation model regardless of whether it is correct or not. These generated wrong target words are further added to the target historical context to affect the generation of subsequent target words. To address this issue, many efforts have been initiated on revising potential errors in the previously generated target fragment, for example, automatic post-editing (Niehues et al., 2016; Zhou et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) and two-pass decoding (Xia et al., 2017; Geng et al., 2018; Nema et al., 2019; Ghazvininejad et al., 2019). Despite their success, most of these approaches asynchronously simulated the generation of the next target word and the revision of the generated target words or required a complex modification of the existing models. Particularly, when these methods are applied to real-time language generation scenarios (e.g., simultaneous machine translation), it is difficult to satisfy actual application requirements.\nIn this paper, we propose a novel refinement method to refine the previously generated target words and generate the next target word synchronously. To this end, during the decoding, we consider their target future context for a part of previously generated target words, to synchronously obtain the refinement probabilities of the previously generated words and the generation probability of the next target word at each time step. When the refinement probability is greater than the previous generation probability on the same position, we replace the original generated target word with the revised target word. These refined target words together with the currently generated target word further provide an accurate target historical context for the generation of subsequent target words. Additionally, the\nproposed approach is easily introduced into various language generation models without complex modification. We extensively evaluated it on four widely-used language generation tasks, including standard machine translation, simultaneous machine translation, storytelling, and text summarization, and the experimental results demonstrated the effectiveness of our approach."
    }, {
      "heading" : "2 Background",
      "text" : "In this paper, we use the advanced encoderdecoder framework, Transformer (Vaswani et al., 2017), to introduce the language generation models. To simplify the process, we simply format the main self-attention network (SAN) module and do not involve other modules (e.g., positional encoding, multiple stacked layers, and so on). Encoder represents the source input X={x1, · · · , xJ} as the source representation H= {h1, · · · , hJ} using SANs. Decoder then generates the target sentence word-by-word based on H with attention mechanism and the generated target fragment. Specifically, given a sequence of word vectors in the generated target fragment {E[y1], · · · , E[yi−1]} (E is the embedding matrix of the target language vocabulary), they are packed into keyvalue matrices Ki−1 and Vi−1 at the i-th time-step:\nKi−1 = Vi−1 = M(E[y1], · · · ,E[yi−1]), (1)\nwhere the function M(·) packs a sequence of word vectors into a matrix. Another SelfATTs module is then used to learn the target representation si:\nsi = SelfATTs(ci−1,Ki−1,Vi−1), (2)\nwhere ci−1∈Rdmodel is the previous context vector and dmodel is the dimension of language generation model. si is then fed into another SelfATTc to compute the dependent-time context vector ci:\nci = SelfATTc(si,Ke,Ve), (3)\nwhere Ke and Ve are key and value matrices, respectively, that are transformed from the source representation H according to Eq.(1). The probability distribution Pg(yi|y<i, X) is then computed using the MLP layer:\nPg(yi|y<i, X) ∝ MLP(ci). (4)\nyi with the maximum probability is selected as the output of decoder at the i-th time-step. To obtain\nthe language generation model θ, the training objective maximizes the conditional generation probability over the training dataset {[X,Y]}:\nJ (θ) = Pg(Y|X; θ). (5)"
    }, {
      "heading" : "3 Methodology",
      "text" : "Intuitively, when there is a target sentence with several incorrect target words, a native target language speaker often detects incorrect words according to the contextual information and tries to revise them. We infer that there are two key aspects in the artificial refinement process: i) How to identify those incorrect target words based on the context information; ii) How to revise the identified incorrect target words. To this end, we propose to simulate the above artificial refinement process, to refine the previously generated target words and generate the next target word synchronously."
    }, {
      "heading" : "3.1 Synchronous Refinement",
      "text" : "Formally, at the i-th time step, given a key-value matrix pair {Ki−1, Vi−1} (see Eq.(1)) of the generated target language fragment {E[y1], E[y2], · · · , E[yi−1]}, we first pack the context vectors {c1, c2, · · · , ci−1} to generate the previous target words into a matrix Ci−1 using Eq.(1):\nCi−1 = M(c1, c2, · · · , ci−1). (6) We then use Ci−1 instead of ci−1 in Eq. (2) to learn the target representation matrix Si:\nSi = SelfATTs(Ci−1,Ki−1,Vi−1), (7)\nwhere Si ∈ Ri×dmodel is a matrix which includes the updated target representations of previously generated target words {s′1, s′2, · · · , s′i−1} in addition to the current target representation si. Here, for each of the previously generated target words, SelfATTs considers a subset of the future target context to update its target representation. For example, s′3 of y3 encodes its future target words {y3, · · · , yi−1} in addition to its previous target words {y1, y2}. The target future context, which has been shown to be useful for generating the target language in machine translation (Zhang et al., 2018; Zheng et al., 2018; Zhou et al., 2019; Zheng et al., 2019), provides more evidence information for correcting one among all the generated target words in this paper.\nThen, Si is fed into Eq. (3) to learn a sequence of the context vectors Ci:\nCi = SelfATTc(Si,Ke,Ve), (8)\nwhere Ci ∈ Ri×dmodel is a matrix which includes the updated context vectors of previously generated target words {c′1, · · · , c′i−1} in addition to the current context vector ci. We then use Ci as the input to Eq.4 to obtain the combined probabilities:\nPr(y ′ 1, · · · , y′i−1, yi|y<i, X) ∝ MLP(Ci), (9)\nwhere Pr(·) includes i − 1 additional refinement probability distributions at each time step i. That is, {y′1, · · · , y′i−1} provides a potential error set for revising the generated target words. Furthermore, we select target words with max probabilities from refinement probability distributions as the target candidate words to be refined. When each refinement probability of {y′1, · · · , y′i−1} is greater than its counterpart in the generation probability {y1, · · · , yi−1} at previous time step, the previously generated target word yk (0<k<i) will be replaced with the refined target word y′k. Finally, the revised target fragment {ŷ1, · · · , ŷi−1, yi} are computed:\nP (ŷ1, · · · , ŷi−1, yi|y<i, X) = max[Pg(y1, · · · , yi−1, 0|y<i, X),\nPr(y ′ 1, · · · , y′i−1, yi|y<i, X)]. (10)\nNote that during the inference, the greedy search was used to refine previously generated target words while beam search was only used to generate the next target word. This makes the search complexity of decoding with refinement as consistent as that of the original decoding with beam search, thereby efficiently performing the synchronous refinement in the existing autoregressive decoding."
    }, {
      "heading" : "3.2 Local Constraint",
      "text" : "For the proposed synchronous refinement, most of the generated target words will be refined many times, that is, the number of refinement operations is proportional to the length of the final target sentence. However, the native target language speaker may only revise the previously generated target words a few times. Thus, there may be a potential risk of “over-refinement” in the synchronous refinement, that is, excessive refinement operations may lead to new errors. To reduce the risk of “over-refinement”, we further design a local constraint (see Figure 1(a)) that focuses on refining part of the previously generated target words closest to the target word to be omitted at each time step, inspired by\nthe local attention (Luong et al., 2015) and the fixed iteration prediction (Ghazvininejad et al., 2019). Specifically, in the i-th time step, we select N previously generated target words {E[yi−N ], E[yi−N+1], · · · , E[yi−1]} closest to the target word to be omitted, and pack the context vectors {ci−N , ci−N+1, · · · , ci−1} into C′i−1: C′i−1 = M(ci−N , ci−N+1, · · · , ci−1). (11) Then, we feed the local target representation matrix C′i−1 into Eq. (7) instead of C′i−1, and thereby efficiently focus on the revision of part of generated target words according to Eqs.8, 9, and 10 in turn.\ny1 y2 y3 y4 … y8<\\s>\ns'2 s'3\nSelfATTs\nSelfATTc c'3 MLP layer y'4 c'2\ny'3 Max[Pg(.), Pr(.)] !\uD835\uDC66!\ns4\nc4\ny5\ny5!\uD835\uDC66\"\n(a)\nvectors {ci N , ci N+1, · · · , ci 1} into C0i 1:253\nC0i 1 = M(ci N , ci N+1, · · · , ci 1). (11)254\nThen, we feed the local target representation255 matrix C0i 1 into Eq.7 instead of the original target256 representation matrix C0i 1, and thereby efficiently257 perform refinement of local gener te target words258 according to Eqs.8, 9, and 10 in turn.259\n3.2 Model Training260\nIn the proposed Local-refinement, when the261 number of refined target words is N , each262 generated target word in the fixed window will263 be refined at most N times This means that we264 need to open up N+1 decoding paths, which265 may be inefficient for the training of neural266 network models. To efficiently inject this local-\ni-th time-step refinement mask 1 1 1 1 1 0 0 0 0 2 1 1 1 0 0 0 0 0 3 1 1 1 1 1 0 0 0 4 1 1 1 1 1 1 1 0 5 1 1 1 1 1 1 1 0 6 1 1 1 1 1 1 1 1 7 1 1 1 1 1 1 1 1 8 1 1 1 1 1 1 1 1\nTable 1: Mask matrix for learning the target representation during the training, where black \"1\" and blue \"1\" unmask the past context and the future context (i.e., N=3) for the proposed synchronous refinement.\n267\nrefinement capability into the training of language268\ngeneration models, we introduce an additional269\nlocal-refinement mask to select the target future270 context words for the refinement mechanism.271 Compared with the existing lower triangle mask272 (similar to black indexes in Table 1), the local-273 refinement mask contains additional target future274 context words closest to the target word to be275 omitted. After a target word is generated, it will be276 refined in the subsequent N sequential steps. This277 means that the number of future target words is278 different. Therefore, the local-refinement mask is279 to randomly select future target words not greater280 than N to cover a variety of different future contexts281 as red parts in Table 1.282\nJ (✓) = Pg(Y|X; ✓) + Pr(Y|X; ✓). (12)283\nThen, we use the original lower triangular284 mask and the proposed refinement mask to learn285 the generation and refinement context vectors,286 respectively. This allows the language generation287\nmodel to simultaneously simulate the generation 288 of generated target words and the refinement of the 289 current target word during the training, respectively. 290 Thus, the training objective maximizes the 291 generation and the refinement probabilities over 292 the training dataset {[X, Y]}: 293\n4 Experiments 294\nWe evaluated the proposed method on the four 295 typical language generation tasks, including stan- 296 dard machine translation, simultaneous machine 297 translation, text summarization, and storytelling. 298\n4.1 Standard Machine Translation 299 We evaluated the proposed method on three 300 widely-used standard machine translation tasks: 301 WMT14 En)De includes 4.43 million bilingual 302 sentence pairs, and we used the newstest2013 303 and newstest2014 datasets as the dev set and test 304 set, respectively; WMT14 En)Fr includes 36 305 million bilingual sentence pairs, and we used the 306 newstest2013 and newstest2014 datasets as the dev 307 set and test set, respectively; and WMT17 Zh)En 308 includes 22 million bilingual sentence pairs, and we 309 used the newsdev2017 and newstest2017 datasets 310 as the dev set and the test set, respectively. The 311 byte pair encoding algorithm (Sennrich et al., 2016) 312 was adopted, and the vocabulary size was set 313 to 40K. We set the dimension of all input and 314 output layers to 512, the dimension of the inner 315 feedforward neural network layer to 1024, and the 316 total heads of all multi-head modules to 8 in both 317 the encoder and decoder layers. Each training 318 batch consisted of a set of sentence pairs that 319 contained approximately 4000⇥8 source tokens 320 and 4000⇥8 target tokens. To evaluate the test 321 sets, following the training of 200,000 batches, 322 we used a single model obtained by averaging 323 the last five checkpoints, which validated the 324 model with an interval of 2,000 batches on the 325 dev set. We trained all models on eight V100 326 GPUs and evaluated them on a single V100 GPU. 327 We chose the Transformer NMT model (Vaswani 328 et al., 2017) as our baseline. For other configures 329 of Transformer (e.g., Trans.base/big) models, we 330 followed the settings in (Vaswani et al., 2017). We 331 used the multi-bleu.perl script as the evaluation 332 metric for the three translation tasks. 333\n4.1.1 Translation Results 334 Table 2 showed BLEU scores of the baseline 335 Trans.base/big models, +Local-refinement, +De- 336\n(b)\nFigure 1: (a) The auto-regressive decoder with synchronous refinement where red and black arrows denote the refinement data flow and the generation data flow. (b) Refinement mask for learning the target representation during the training, and blue number denotes synchronous refinement mask of the future context for the local constraints (i.e., N=3)."
    }, {
      "heading" : "3.3 Model Training",
      "text" : "When the local constraint of the synch onous refinement is set to N , each generated target word will be refined at most N times. This may be inefficient for the training of language generation models. To efficiently inject this synchronous refinement capability into the training of language generation models, we introduce an additional refinement mask o select the target future cont xt words under the local constraint. Compared with the existing lower triangle mask, the local constraint contains additional target future context words closest to the target word to be omitted, as shown in Figure 1(b). After a target word is generated, it will be refined in the subsequent N sequential steps, which indicates that the number of future target words is different. Therefore, the refinement mask with local constraint is to randomly select future target words not greater than N to cover a variety of different future contexts as blue parts in Figure 1(b).\n3\nThen, we use the proposed refinement mask to learn the generation and refinement context vectors at each time step. This allows the language generation models to synchronously simulate the generation of generated target words and the refinement of the current target word during the training. Thus, the training objective maximizes the generation and the refinement probabilities over the training dataset {[X,Y]}:\nJ (θ) = Pg(Y|X; θ) + Pr(Y|X; θ). (12)"
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluated the proposed synchronous refinement (SynRefinement) on the four typical language generation tasks, including standard machine translation, simultaneous machine translation, text summarization, and storytelling."
    }, {
      "heading" : "4.1 Standard Machine Translation",
      "text" : "We evaluated the proposed SynRefinement on three widely-used standard machine translation tasks: WMT14 En⇒De includes 4.43 million bilingual sentence pairs, and we used the newstest2013 and newstest2014 datasets as the dev set and test set, respectively; WMT14 En⇒Fr includes 36 million bilingual sentence pairs, and we used the newstest2013 and newstest2014 datasets as the dev set and test set, respectively; and WMT17 Zh⇒En includes 22 million bilingual sentence pairs, and we used the newsdev2017 and newstest2017 datasets as the dev set and the test set, respectively. The byte pair encoding algorithm (Sennrich et al., 2016) was adopted, and the vocabulary size was set to 40K. We set the dimension of all input and\noutput layers to 512, the dimension of the inner feedforward neural network layer to 1024, and the total heads of all multi-head modules to 8 in both the encoder and decoder layers. Each training batch consisted of a set of sentence pairs that contained approximately 4000×8 source tokens and 4000×8 target tokens. To evaluate the test sets, following the training of 200,000 batches, we used a single model obtained by averaging the last five checkpoints, which validated the model with an interval of 2,000 batches on the dev set. We trained all models on eight V100 GPUs and evaluated them on a single V100 GPU. We chose the Transformer NMT model (Vaswani et al., 2017) as our baseline. For other configures of Transformer (e.g., Trans.base/big) models, we followed the settings in (Vaswani et al., 2017). We used the multi-bleu.perl script as the evaluation metric for the three translation tasks."
    }, {
      "heading" : "4.1.1 Translation Results",
      "text" : "Table 1 showed BLEU scores of the baseline Trans.base/big models, +SynRefinement, +Deliberation (Xia et al., 2017) and +Two-stream (Song et al., 2020) attention models for comparison. First, +SynRefinement performed better than the baseline Trans.base/big models for three language pairs. This indicates that the proposed refinement mechanism improved the performance of NMT. Second, +SynRefinement was superior to the comparison +Deliberation network model, which confirms our hypothesis that jointly simulating generation and refinement of target sentence was better than the isolated multi-pass decoding way. Additionally, +SynRefinement outperformed the\ncomparison +Two-stream attention model. Also, +SynRefinement did not increase any additional model parameters but +Two-stream attention increased about 19.7% model parameters compared to the baseline Trans.base model. Meanwhile, both training and decoding speeds of +SynRefinement were faster than those of +Deliberation and +Twostream models. This means that the proposed refinement can more efficiently relieve the “onepass” issue for the machine translation."
    }, {
      "heading" : "4.1.2 Hyperparameter N of SynRefinement",
      "text" : "In Section 3.3, we designed a local constraint to reduce the “over-refinement” risk. Thus, the hyperparameter N in Eq. (11) was used to control the range of refinement operations. Figure 2 shows BLEU scores for Trans.base and +SynRefinement models for various hyperparameter N (x-axis) on the WMT14 En-De, WMT17 Zh-En, and WMT14 En-Fr dev sets. +SynRefinement achieved the highest BLEU scores with N=5 for three dev sets. As a result, we set the hyperparameter N as five to conduct the main experiments shown in Table 1."
    }, {
      "heading" : "4.1.3 Ablation of Local Constraint and Refinement Mask",
      "text" : "We incrementally added Refinement Mask and Local Constraint into the training and decoding passes of Trans.base model to evaluate their effectiveness. Table 3 showed the ablation results of Trans.base, +Refinement Mask, +Local\nConstraint, and Both (+Refinement Mask+Local Constraint) models. First, when Refinement Mask and Local Constraint were introduced to the training and the decoding, respectively, BLEU scores were higher than those of the Trans.base model on three translation tasks. The proposed approach was beneficial to the performance improvement of NMT. Second, when both Refinement Mask and Local Constraint were introduced into the training and decoding of the Trans.base model simultaneously, performance improved further. This means that maintaining consistent refinement operations in training and decoding helped NMT generate faithful and fluent target translation."
    }, {
      "heading" : "4.1.4 Investigation of SynRefinement Operation",
      "text" : "The proposed SynRefinement aims to revise potential errors in the generated target words. To investigate the effectiveness of synchronous refinement operations, we counted the number of different refinement times (e.g., replacement and deletion operation) on the same position during the inference. For example, “#2” denotes the number of BPE tokens that have been replaced (or refined) twice during the decoding. Table 3 showed statistical results for translations (include 74,487, 57,497 and 92,207 BPE tokens, respectively) of three translation tasks generated by Trans.base+SynRefinement models in Table 1. We observed that among the translations generated on the three tasks, total refinement operations of the same position occurred in 4,087, 2,459, and 5,058 positions, respectively. This indicates that the proposed SynRefinement worked during the decoding. When refinement times gradually increased from #1 to #5 on the same position, the number of such BPE tokens was greatly reduced.\n上海市 松江区 交管部门 近期 使用 电子 警察 整治 驾驶员 [Shanghai] [Songjiang District] [Traffic Control Department] [recently] [used] [electronic] [police] [monitor] [driver] 开车 玩 手机 ，一个星期 查获 30 多 起 [driving] [plays] [mobile phone] [in a week] [detected] [30] [more than] [cases]\nSrc:\ntraffic control department of Songjiang District in Shanghai recently used electronic police to clean up drivers' mobile phone, more than 30 seizures a week Trans.base:\ntraffic control department of Songjiang District in Shanghai recently used electronic police to monitor that the driver plays mobile phone, and discovered more than 30 cases in a week +SynRefinement:"
    }, {
      "heading" : "4.1.5 Effect of Different Target Lengths",
      "text" : "In the proposed SynRefinement, the number of refined words increased as the length of the generated target translation increased. To investigate the effect of SynRefinement on translations with different lengths, we divided each test set into six groups according to the length of the target translations. For example, “20” indicates that the length of target translations was between twenty and thirty. Figure 4 shows BLEU scores of the Trans.base and +SynRefinement models for the WMT14 En-De, WMT17 Zh-En, and WMT14 En-Fr test sets. First, when the length of target\ntranslations was between zero and ten, BLEU scores of +SynRefinement were almost the same as those of Trans.base model. This reason may be that the refinement operation was performed from the fifth time step for three translation tasks. Second, when the length of the target translations was more than ten, BLEU scores of +SynRefinement were higher than those of Trans.base models for three translation tasks. Particularly, the extent of the improvement gradually increased as the length of the target translations increased. This means that +SynRefinement improved the quality of the target translations, especially long target translations."
    }, {
      "heading" : "4.1.6 Case Study",
      "text" : "Figure 3 showed Chinese-to-English translation cases generated by Trans.base and +SynRefinement models. Trans.base first generated an inappropriate translation “clean up” and missed two key verbs “plays” and “detected”, and thereby generated a incorrect translation “seizures” compared to the “Ref”. Thus, the meaning in the target fragment was extremely confusing after “clean up” in the translation generated by the Trans.base model. +SynRefinement considered the future context “that the driver mobile phone” together with the past context “traffic ... police to” to revise “clean up” as the correct “monitor”. “monitor” constituted part of the target historical context which allowed +SynRefinement model to generate two missed key verbs “plays” and “discovered”. +SynRefinement further generated the correct target word “cases” compared to the inappropriate “seizures”. As a result, the translation generated by +SynRefinement was closer to the reference than that generated by the Trans.base model."
    }, {
      "heading" : "4.2 Simultaneous Machine Translation",
      "text" : "Simultaneous machine translation models start generating a target sequence before they have encoded the source sequence. We used the WMT15 De-En (includes 4.5 million bilingual sentence pairs) as the training set, and we used the newstest2013 and newstest2015 datasets as the dev set and test set, respectively. We followed the setting from Ma et al. (2020)’s work to evaluate the proposed Refinement mechanism for the simultaneous machine translation. The experimental results in Table 4 demonstrated that +SynRefinement outperformed the baseline Trans.base model and the comparison Look-back\nAttention (Arivazhagan et al., 2019) and Monotonic Attention (Ma et al., 2020) models. This indicates that the proposed SynRefinement was also useful for simultaneous machine translation."
    }, {
      "heading" : "4.3 Text Summarization",
      "text" : "Text summarization is a typical language generation task that creates a short and fluent summary of the given long text document. Song et al. (2019) fine-tuned the MASS pre-trained model on the text summarization task and achieved impressive results. We chose this model as our baseline, keeping the pre-training consistent with it, and using the proposed Refinement mechanism for enhancing the fine-tuning phase. The Annotated Gigaword corpus is used as the benchmark which is derived from news articles and consists of pairs of the main sentences in the article (long), and the headline (short). We used the article and the headline as the source input sentence and reference, respectively. The data included approximately 3.8M training samples, 400K validation samples, and 2K test samples. During the evaluation, we reported ROUGE-1, ROUGE-2, and ROUGEL (Lin, 2004).\nTable 5 showed the results for the text summarization task. We observed that +SynRefinement performed better than three comparison methods (Song et al., 2019) in terms of ROUGE1, ROUGE-2, and ROUGE-L evaluation metrics. This indicates that Refinement improved the generation of text summarization, which confirms the universality of the proposed approach."
    }, {
      "heading" : "4.4 Storytelling",
      "text" : "Storytelling is at the frontier of current language generation technologies: stories must maintain a consistent theme throughout the document and require very long-distance dependency modeling. Additionally, stories require creativity and a highlevel plot with planning rather than word-byword generation (Wiseman et al., 2017). A hierarchical story generation model (Fan et al., 2018) was proposed to tackle the challenges which first generates a sentence called a prompt that describes the theme or topic for the upcoming story generation, and then conditions on the prompt when generating the story. For the prompt-to-story generation, Fan et al. (2018) collected a dataset from Reddit’s WRITINGPROMPTS forum in which each prompt had multiple story responses. With the dataset, they trained a story generation model that obtained further improvements using a novel form of model fusion that improved the relevance of the story to the prompt and added a new gated multi-scale self-attention mechanism to model the long-range context.\nWe used the perplexity (PPL) of the model to evaluate the proposed method on the test set. For example, the lower the perplexity of the model, the better the performance of the model. Table 6 showed the results of the proposed SynRefinement and comparison methods. The experimental results show that the best execution of all comparison methods was the Conv seq2seq+self-attention (Fusion). When the proposed Refinement was added to the best comparison method Conv seq2seq+self-attention (Fusion), the perplexity was further reduced. These results suggest that the proposed Refinement significantly improved the quality of story generation, which further confirms the universality of our method."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Automatic Post-Editing",
      "text" : "For the refinement of target output in the classical machine translation (MT), a direct method is automatic post-editing (APE) (Simard et al., 2007). APE is the process of automatic correction of raw MT output, so that a closer resemblance to human post-edited MT output is achieved. Béchara et al. (2011) proposed to automatically create a new joined MT output and source token pairs to improve the automatic post-editing results (Béchara et al., 2012; Pal et al., 2017). Also, the MT output is refined by humans or another model (Niehues et al., 2016; Junczys Dowmunt and Grundkiewicz, 2017), which indicates that the generating and refining are two separate processes in APE."
    }, {
      "heading" : "5.2 Two-Pass Auto-regressive Decoding",
      "text" : "As the encoder-decoder framework becomes the dominant language generation method, the generated potential errors caused by the “onepass” issue still is a challenge. Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020). For example, review network (Yang et al., 2016) was proposed to refine the source representation for the caption generation model. Compared with reviewing the source information, a deliberation network (Xia et al., 2017) proposed two levels of decoders which generate a draft of the target sentence and polish the draft of the target sentence for MT, respectively. Additionally, most relevant to our work is that Song et al. (2020) leveraged the scheduled sampling to simulate the prediction errors during training and designed an additional content-stream attention network to correct the generated error information, which requires complex two-stream attention (Yang et al., 2019) or dual attention (Novak et al., 2016). The refinement network (Nema et al., 2019) for the QA task used a dual attention network to refine the question generated by the first decoder, thereby making the answer correct in the second decoder."
    }, {
      "heading" : "5.3 Iterative Refinement for Non-autoregressive Decoding",
      "text" : "Non-autoregressive decoding (Gu et al., 2018) was introduced to generate all words at once, but\nits performance was far away from that of the auto-regressive decoding due to lack of sufficient dependency modeling among target words. Lee et al. (2018) designed an iterative inference strategy to minimize the generation latency. Then, Ghazvininejad et al. (2019) proposed to first predict all of the target words non-autoregressively, and then repeatedly masked out and regenerated the subset of words for iterative refining the target translation. Different from the refinement of discrete target words, the iterative inference (Lee et al., 2020) was proposed to iterative perform refinement in the continuous space for enhancing dependency between target words.\nDiscussion: Inspired by iterative refinement for non-autoregressive decoding, we proposed a novel synchronous refinement for language generation. The proposed approach differs from previous studies in two ways. First, our method allows the language generation models to refine the previously generated target words and to generate the current target word synchronously instead of APE with separate refinement and asynchronous two-pass (or multi-pass) decoding. Second, the proposed SynRefinement can be introduced to the language generation models efficiently without complex modification of the existing language generation models. Additionally, the proposed SynRefinement can help the real-time language generation scenarios (e.g., simultaneous machine translation) to satisfy the practical application requirements."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper explored part of the target future context to revise fixed potential errors in the generated target fragment caused by the “one-pass” issue of the auto-regressive decoder. We proposed a novel SynRefinement approach to language generation, where the refinement of generated target words is synchronized with the generation of the next target word at each time step. Meanwhile, the proposed SynRefinement can be easily introduced into the encoder-decoder language generation models without complex modifications. We evaluated the effectiveness of the proposed approach on four classical language generation tasks, that is, standard MT, simultaneous MT, text summarization, and storytelling. In the future, we will try to explore how to intelligently identify and correct generation errors."
    } ],
    "references" : [ {
      "title" : "Monotonic infinite lookback attention for simultaneous machine translation",
      "author" : [ "Naveen Arivazhagan", "Colin Cherry", "Wolfgang Macherey", "Chung-Cheng Chiu", "Semih Yavuz", "Ruoming Pang", "Wei Li", "Colin Raffel." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Arivazhagan et al\\.,? 2019",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations, San Diego, CA.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "An evaluation of statistical post-editing systems applied to RBMT and SMT systems",
      "author" : [ "Hanna Béchara", "Raphaël Rubino", "Yifan He", "Yanjun Ma", "Josef van Genabith." ],
      "venue" : "Proceedings of COLING 2012, pages 215–230, Mumbai, India. The COLING 2012",
      "citeRegEx" : "Béchara et al\\.,? 2012",
      "shortCiteRegEx" : "Béchara et al\\.",
      "year" : 2012
    }, {
      "title" : "Statistical post-editing for a statistical mt system",
      "author" : [ "Hanna Béchara", "Ma Yanjun", "Genabith Josef van." ],
      "venue" : "the 13th Machine Translation Summit, page 308–315, Xiamen, China.",
      "citeRegEx" : "Béchara et al\\.,? 2011",
      "shortCiteRegEx" : "Béchara et al\\.",
      "year" : 2011
    }, {
      "title" : "Clause restructuring for statistical machine translation",
      "author" : [ "Michael Collins", "Philipp Koehn", "Ivona Kucerova." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 531–540, Ann Arbor, Michigan.",
      "citeRegEx" : "Collins et al\\.,? 2005",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2005
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Adaptive multi-pass decoder for neural machine translation",
      "author" : [ "Xinwei Geng", "Xiaocheng Feng", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 523–532,",
      "citeRegEx" : "Geng et al\\.,? 2018",
      "shortCiteRegEx" : "Geng et al\\.",
      "year" : 2018
    }, {
      "title" : "Mask-predict: Parallel decoding of conditional masked language models",
      "author" : [ "Marjan Ghazvininejad", "Omer Levy", "Yinhan Liu", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2019
    }, {
      "title" : "Nonautoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor O.K. Li", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "An exploration of neural sequence-tosequence architectures for automatic post-editing",
      "author" : [ "Marcin Junczys Dowmunt", "Roman Grundkiewicz." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Vol-",
      "citeRegEx" : "Dowmunt and Grundkiewicz.,? 2017",
      "shortCiteRegEx" : "Dowmunt and Grundkiewicz.",
      "year" : 2017
    }, {
      "title" : "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
      "author" : [ "Jason Lee", "Elman Mansimov", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Iterative refinement in the continuous space for non-autoregressive neural machine translation",
      "author" : [ "Jason Lee", "Raphael Shu", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421,",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Monotonic multihead attention",
      "author" : [ "Xutai Ma", "Juan Miguel Pino", "James Cross", "Liezl Puzon", "Jiatao Gu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Let’s ask again: Refine network for automatic question generation",
      "author" : [ "Preksha Nema", "Akash Kumar Mohankumar", "Mitesh M. Khapra", "Balaji Vasan Srinivasan", "Balaraman Ravindran." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical",
      "citeRegEx" : "Nema et al\\.,? 2019",
      "shortCiteRegEx" : "Nema et al\\.",
      "year" : 2019
    }, {
      "title" : "Pre-translation for neural machine translation",
      "author" : [ "Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1828–1836,",
      "citeRegEx" : "Niehues et al\\.,? 2016",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2016
    }, {
      "title" : "The COLING 2016 Organizing Committee",
      "author" : [ "Osaka", "Japan" ],
      "venue" : null,
      "citeRegEx" : "Osaka and Japan.,? \\Q2016\\E",
      "shortCiteRegEx" : "Osaka and Japan.",
      "year" : 2016
    }, {
      "title" : "Iterative refinement for machine translation",
      "author" : [ "Roman Novak", "Michael Auli", "David Grangier." ],
      "venue" : "CoRR, abs/1610.06602. 9",
      "citeRegEx" : "Novak et al\\.,? 2016",
      "shortCiteRegEx" : "Novak et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural automatic post-editing using prior alignment and reranking",
      "author" : [ "Santanu Pal", "Sudip Kumar Naskar", "Mihaela Vela", "Qun Liu", "Josef van Genabith." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for",
      "citeRegEx" : "Pal et al\\.,? 2017",
      "shortCiteRegEx" : "Pal et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Statistical phrase-based post-editing",
      "author" : [ "Michel Simard", "Cyril Goutte", "Pierre Isabelle." ],
      "venue" : "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main",
      "citeRegEx" : "Simard et al\\.,? 2007",
      "shortCiteRegEx" : "Simard et al\\.",
      "year" : 2007
    }, {
      "title" : "Neural machine translation with error correction",
      "author" : [ "Kaitao Song", "Xu Tan", "Jianfeng Lu." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3891–3897. International Joint Conferences",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "Tie-Yan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 5926–5936.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Challenges in data-to-document generation",
      "author" : [ "Sam Wiseman", "Stuart Shieber", "Alexander Rush." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Wiseman et al\\.,? 2017",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2017
    }, {
      "title" : "Deliberation networks: Sequence generation beyond one-pass decoding",
      "author" : [ "Yingce Xia", "Fei Tian", "Lijun Wu", "Jianxin Lin", "Tao Qin", "Nenghai Yu", "Tie-Yan Liu." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and",
      "citeRegEx" : "Xia et al\\.,? 2017",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2017
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Review networks for caption generation",
      "author" : [ "Zhilin Yang", "Ye Yuan", "Yuexin Wu", "William W. Cohen", "Ruslan R. Salakhutdinov." ],
      "venue" : "Proceedings of the 30th International Conference on Neural Information Processing Systems, page 2369–2377,",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Asynchronous bidirectional decoding for neural machine translation",
      "author" : [ "Xiangwen Zhang", "Jinsong Su", "Yue Qin", "Yang Liu", "Rongrong Ji", "Hongji Wang." ],
      "venue" : "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Dynamic past and future for neural machine translation",
      "author" : [ "Zaixiang Zheng", "Shujian Huang", "Zhaopeng Tu", "XinYu Dai", "Jiajun Chen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling past and future for neural machine translation",
      "author" : [ "Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lili Mou", "Xinyu Dai", "Jiajun Chen", "Zhaopeng Tu." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:145–157.",
      "citeRegEx" : "Zheng et al\\.,? 2018",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural system combination for machine translation",
      "author" : [ "Long Zhou", "Wenpeng Hu", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
      "citeRegEx" : "Zhou et al\\.,? 2017",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2017
    }, {
      "title" : "Synchronous bidirectional neural machine translation",
      "author" : [ "Long Zhou", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:91–105. 10",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Recently, the encoder-decoder framework has obtained impressive results over various language generation tasks, such as machine translation (Vaswani et al., 2017), storytelling (Fan et al.",
      "startOffset" : 140,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : ", 2017), storytelling (Fan et al., 2018), and text summarization (Song et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "Finally, decoder generates a target language sentence word-by-word in the auto-regressive decoding manner (Bahdanau et al., 2015; Vaswani et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 151
    }, {
      "referenceID" : 24,
      "context" : "Finally, decoder generates a target language sentence word-by-word in the auto-regressive decoding manner (Bahdanau et al., 2015; Vaswani et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "To address this issue, many efforts have been initiated on revising potential errors in the previously generated target fragment, for example, automatic post-editing (Niehues et al., 2016; Zhou et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) and two-pass decoding (Xia et al.",
      "startOffset" : 166,
      "endOffset" : 247
    }, {
      "referenceID" : 32,
      "context" : "To address this issue, many efforts have been initiated on revising potential errors in the previously generated target fragment, for example, automatic post-editing (Niehues et al., 2016; Zhou et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) and two-pass decoding (Xia et al.",
      "startOffset" : 166,
      "endOffset" : 247
    }, {
      "referenceID" : 26,
      "context" : ", 2017; Junczys Dowmunt and Grundkiewicz, 2017) and two-pass decoding (Xia et al., 2017; Geng et al., 2018; Nema et al., 2019; Ghazvininejad et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : ", 2017; Junczys Dowmunt and Grundkiewicz, 2017) and two-pass decoding (Xia et al., 2017; Geng et al., 2018; Nema et al., 2019; Ghazvininejad et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : ", 2017; Junczys Dowmunt and Grundkiewicz, 2017) and two-pass decoding (Xia et al., 2017; Geng et al., 2018; Nema et al., 2019; Ghazvininejad et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : ", 2017; Junczys Dowmunt and Grundkiewicz, 2017) and two-pass decoding (Xia et al., 2017; Geng et al., 2018; Nema et al., 2019; Ghazvininejad et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 154
    }, {
      "referenceID" : 24,
      "context" : "In this paper, we use the advanced encoderdecoder framework, Transformer (Vaswani et al., 2017), to introduce the language generation models.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 29,
      "context" : "The target future context, which has been shown to be useful for generating the target language in machine translation (Zhang et al., 2018; Zheng et al., 2018; Zhou et al., 2019; Zheng et al., 2019), provides more evidence information for correcting one among all the generated target words in this paper.",
      "startOffset" : 119,
      "endOffset" : 198
    }, {
      "referenceID" : 31,
      "context" : "The target future context, which has been shown to be useful for generating the target language in machine translation (Zhang et al., 2018; Zheng et al., 2018; Zhou et al., 2019; Zheng et al., 2019), provides more evidence information for correcting one among all the generated target words in this paper.",
      "startOffset" : 119,
      "endOffset" : 198
    }, {
      "referenceID" : 33,
      "context" : "The target future context, which has been shown to be useful for generating the target language in machine translation (Zhang et al., 2018; Zheng et al., 2018; Zhou et al., 2019; Zheng et al., 2019), provides more evidence information for correcting one among all the generated target words in this paper.",
      "startOffset" : 119,
      "endOffset" : 198
    }, {
      "referenceID" : 30,
      "context" : "The target future context, which has been shown to be useful for generating the target language in machine translation (Zhang et al., 2018; Zheng et al., 2018; Zhou et al., 2019; Zheng et al., 2019), provides more evidence information for correcting one among all the generated target words in this paper.",
      "startOffset" : 119,
      "endOffset" : 198
    }, {
      "referenceID" : 13,
      "context" : "To reduce the risk of “over-refinement”, we further design a local constraint (see Figure 1(a)) that focuses on refining part of the previously generated target words closest to the target word to be omitted at each time step, inspired by the local attention (Luong et al., 2015) and the fixed iteration prediction (Ghazvininejad et al.",
      "startOffset" : 259,
      "endOffset" : 279
    }, {
      "referenceID" : 7,
      "context" : ", 2015) and the fixed iteration prediction (Ghazvininejad et al., 2019).",
      "startOffset" : 43,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "The byte pair encoding algorithm (Sennrich et al., 2016) was adopted, and the vocabulary size was set to 40K.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 24,
      "context" : "We chose the Transformer NMT model (Vaswani et al., 2017) as our baseline.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "base/big) models, we followed the settings in (Vaswani et al., 2017).",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "base/big models, +SynRefinement, +Deliberation (Xia et al., 2017) and +Two-stream (Song et al.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : ", 2017) and +Two-stream (Song et al., 2020) attention models for comparison.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Attention (Arivazhagan et al., 2019) and Monotonic Attention (Ma et al.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : ", 2019) and Monotonic Attention (Ma et al., 2020) models.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "During the evaluation, we reported ROUGE-1, ROUGE-2, and ROUGEL (Lin, 2004).",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "We observed that +SynRefinement performed better than three comparison methods (Song et al., 2019) in terms of ROUGE1, ROUGE-2, and ROUGE-L evaluation metrics.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "Additionally, stories require creativity and a highlevel plot with planning rather than word-byword generation (Wiseman et al., 2017).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "A hierarchical story generation model (Fan et al., 2018) was proposed to tackle the challenges which first generates a sentence called a prompt that describes the theme or topic for the upcoming story generation, and then conditions on the prompt when generating the story.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "For the refinement of target output in the classical machine translation (MT), a direct method is automatic post-editing (APE) (Simard et al., 2007).",
      "startOffset" : 127,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "(2011) proposed to automatically create a new joined MT output and source token pairs to improve the automatic post-editing results (Béchara et al., 2012; Pal et al., 2017).",
      "startOffset" : 132,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "(2011) proposed to automatically create a new joined MT output and source token pairs to improve the automatic post-editing results (Béchara et al., 2012; Pal et al., 2017).",
      "startOffset" : 132,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "Also, the MT output is refined by humans or another model (Niehues et al., 2016; Junczys Dowmunt and Grundkiewicz, 2017), which indicates that the generating and refining are two separate processes in APE.",
      "startOffset" : 58,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 253
    }, {
      "referenceID" : 26,
      "context" : "Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 253
    }, {
      "referenceID" : 29,
      "context" : "Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 253
    }, {
      "referenceID" : 6,
      "context" : "Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 253
    }, {
      "referenceID" : 33,
      "context" : "Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 253
    }, {
      "referenceID" : 15,
      "context" : "Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 253
    }, {
      "referenceID" : 22,
      "context" : "Many studies proposed two-pass decoding to revise the fixed potential errors in the auto-regressive language generation (Yang et al., 2016; Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Zhou et al., 2019; Nema et al., 2019; Song et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 253
    }, {
      "referenceID" : 28,
      "context" : "For example, review network (Yang et al., 2016) was proposed to refine the source representation for the caption generation model.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "Compared with reviewing the source information, a deliberation network (Xia et al., 2017) proposed two levels of decoders which generate a draft of the target sentence and polish the draft of the target sentence for MT, respectively.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : "(2020) leveraged the scheduled sampling to simulate the prediction errors during training and designed an additional content-stream attention network to correct the generated error information, which requires complex two-stream attention (Yang et al., 2019) or dual attention (Novak et al.",
      "startOffset" : 238,
      "endOffset" : 257
    }, {
      "referenceID" : 15,
      "context" : "The refinement network (Nema et al., 2019) for the QA task used a dual attention network to refine the question generated by the first decoder, thereby making the answer correct in the second decoder.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "Non-autoregressive decoding (Gu et al., 2018) was introduced to generate all words at once, but its performance was far away from that of the auto-regressive decoding due to lack of sufficient dependency modeling among target words.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Different from the refinement of discrete target words, the iterative inference (Lee et al., 2020) was proposed to iterative perform refinement in the continuous space for enhancing dependency between target words.",
      "startOffset" : 80,
      "endOffset" : 98
    } ],
    "year" : 0,
    "abstractText" : "Language generation typically adopts an encoder-to-decoder framework, in which the decoder generates the target sentence word-byword in an auto-regressive manner. However, the auto-regressive decoder faces a deeprooted one-pass issue whereby each generated word is considered as one element of the final output regardless of whether it is correct or not. These generated wrong words further constitute the target historical context to affect the generation of subsequent target words. This paper proposes a novel synchronous refinement method to revise potential errors in the generated words by considering part of the target future context. Particularly, the proposed approach allows the auto-regressive decoder to refine the previously generated target words and generate the next target word synchronously. The experimental results on four widely-used language generation tasks, including standard machine translation, simultaneous machine translation, storytelling, and text summarization, demonstrated the effectiveness of the proposed approach.",
    "creator" : null
  }
}
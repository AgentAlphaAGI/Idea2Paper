{
  "name" : "ARR_2022_306_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "In-BoXBART: Get Instructions into Biomedical Multi-task Learning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "For long, task-specific models have played a central role in achieving state-of-the-art performance in both general and biomedical NLP (Wang et al., 2021a). During 2017-2019, pre-train and fine-tune paradigm (Liu et al., 2021) became the prevalent approach in NLP. Due to success of Language Models (LMs) in the biomedical domain such as BioBERT (Lee et al., 2020), ClinicalXLNET (Huang et al.,\n1Code and data is available at <anonymized link>\n2019), and others (Alrowili and Vijay-Shanker, 2021; Kraljevic et al., 2021; Phan et al., 2021), this paradigm is widely used for creating many task-specific models (Wang et al., 2021a; Banerjee et al., 2021). However, task-specific models have limitations to real-world applications because this approach is computationally expensive (i.e., require large computational resources) and timeconsuming (Strubell et al., 2019; Schwartz et al., 2020). Hence, there is a need for generalization where a single model can perform various tasks leading to a computationally efficient approach. Past attempts have been made in general-domain NLP to achieve generalization across tasks such as MQAN (McCann et al., 2018), UNICORN (Lourie et al., 2021), and UnifiedQA (Khashabi et al., 2020). However, approaches to achieve generalization across various biomedical NLP tasks have not been systematically studied. Hence, this paper studies the multi-tasking approach that can generalize over different biomedical NLP tasks. Figure 1 shows the overview of our proposed multi-tasking approach where the single model can perform various biomedical NLP tasks.\nRecently, prompt-based models have been widely used because of their ability to achieve generalization instead of task-specific models (Liu et al., 2021). Mishra et al. (2021b); Wei et al. (2021) and (Sanh et al., 2021) show the effectiveness of instructional prompts in generalizing on seen as well as unseen general-domain NLP tasks. In this paper, we adapted this instructional promptbased approach for the first time to achieve generalization across various biomedical NLP tasks. To this extent, this paper introduces a collection of 32 instruction tasks for Biomedical NLP across (X) various categories (BoX) and proposes a unified model that can generalize over 32 different biomedical NLP tasks. The proposed unified model (i.e., In-BoXBART) is trained on the instruction-based meta-dataset (i.e., BoX) and evaluated on each task individually from the BoX.\nTo evaluate the proposed approach, we compare our model (i.e., In-BoXBART) with two baselines: (1) single-task models (i.e., models trained on one task and evaluated on the same task), and (2) multitask model (i.e., a single model trained on a combination of all tasks) without instructions. Experimental results show that In-BoXBART outperforms single-task baseline by ∼3%, and multi-task baseline by ∼18%. We also analyze few-shot learning scenario using In-BoXBART since obtaining annotated data in the biomedical domain is costly and time-consuming. In the few-shot setting (i.e., 32 instances per task), In-BoXBART outperforms the single-task baseline by 23.33%. This indicates that Multi-Task Learning (MTL) and instruction-tuning have an advantage in the low resources settings. Although the performance of the In-BoxBART is promising, our analysis reveals that there is still room for improvement on some tasks, implying the scope for future research direction. Concisely, our contributions can be summarized in three folds:\n1. This paper introduces the first benchmark metadataset in biomedical domain, i.e., BoX: a collection of 32 instruction tasks for Biomedical NLP across (X) various categories. Each task is processed in a unified format and equipped with instructions that can be used to train sequenceto-sequence models. 2. Using this meta-dataset, we propose an instruction-tuned Bidirectional and AutoRegressive Transformer (BART) model, termed as In-BoXBART. The comparison of In-BoxBART and two baselines shows that\nIn-BoXBART outperforms single-task baseline by ∼ 3% and multi-task (without instruction) baseline by ∼ 18%. 3. In the few-shot setting, we show that InBoxBART significantly outperforms the singletask baseline by ∼ 23%. This indicates the potential application of instruction-tuning in the biomedical domain where annotated data is difficult to obtain."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multi-task Learning Owing to the problems associated with single-task learning in terms of their space and time requirements, several multi-task learning approaches have been proposed over the years. DecaNLP (McCann et al., 2018) built a multi-tasking model by converting format of each tasks to question answering format. Several other works have followed similar approach by converting tasks to reading comprehension format (Mishra et al., 2020) and textual entailment (Wang et al., 2021b) . The multitasking model T5 (Raffel et al., 2020) was built with the help of a unified framework that converts all text-based language problems into a text-to-text format. SCIFIVE (Phan et al., 2021) involved building a text to text model for the biomedical literature. T0 (Sanh et al., 2021) uses prompts along with instances to do multitask learning and they focus on achieving zero-shot task generalization.\nInstruction Learning The turking test (Efrat and Levy, 2020) was proposed to measure the efficacy of models to follow instructions. Natural Instructions (Mishra et al., 2021b) broke down each task to multiple sub-tasks that helped models in following instructions and subsequently generalize to unseen tasks (cross-task generalization). FLAN (Wei et al., 2021) model was built by leveraging instructiontuning on diverse range of tasks and achieving zeroshot generalization on target unseen tasks. Task reframing (Mishra et al., 2021a) proposed several guidelines to reframe task instructions to improve model response to follow instructions."
    }, {
      "heading" : "3 BoX",
      "text" : "We use existing, widely adopted 29 biomedical NLP datasets collected from various challenges, platforms and organizations to create BoX. We define the BoX as a benchmark dataset for biomedical MTL across 9 different categories. In the BoX,\nwe reframed all the datasets as text generation tasks (see examples in Appendix B) and created 32 instruction tasks. BoX consists of high-quality human-authored Biomedical Instructions (BIs) for all 32 tasks. Figure 2 shows the 9 different categories and corresponding generated tasks. Each category is defined as colored box and each box contains instruction tasks re-purposed from original datasets."
    }, {
      "heading" : "3.1 Tasks",
      "text" : "Table 1 shows the number of training samples we have used for each category. Further details of each instruction task statistics is shown in Appendix A. Each category and corresponding tasks from the BoX are defined as below:\nNamed Entity Recognition (NER) NER has been considered a necessary first step in processing literature for biomedical text mining where the\nmodel helps in identifying named entities such as protein, gene, chemical, disease, treatment. We use fifteen publicly available biomedical NER datasets (Crichton et al., 2017) to create instructions tasks.\nDe-Identification In this task, the model takes medical discharge records of a patient as input and identify Private Health Information (PHI) such as organizations, persons, locations, dates. We use n2c2 2006 de-identification challenge dataset (Uzuner et al., 2007) to perform this task.\nPart-Of-Speech (POS) Tagging The goal of this task is to identify various POS tags from the biomedical text. We use GENIA corpus (Tateisi et al., 2005) built from MEDLINE abstracts for the POS tagging task.\nQuestion-Answering (QA) QA models receive a question and a corresponding context as input and output the relevant answer from the given context. To execute this task, we used the BioASQ-8b dataset (Nentidis et al., 2020) for different question types, i.e., yes/no, factoid, and list type questions. We created three different tasks from this dataset. Also, we use PubMedQA dataset (Jin et al., 2019) for this task.\nRelation Extraction (RE) We used two datasets for this task: (1) CHEMPROT corpus from biocreative VI precision medicine track (Islamaj Doğan et al., 2019), and (2) Drug-Drug Interaction (DDI) corpus from SemEval 2013 DDI Extraction challenge (Herrero-Zazo et al., 2013).\nSystematic Review We have included data from the following five Systematic Reviews (SRs) that were conducted using the traditional (manual) process and published in relevant venues by Mayo Clinic physicians: (1) Hormone Replacement Therapy (HRT), (2) Cooking, (3) Accelerometer, (4) Acromegaly, and (5) COVID for this task. More details about these datasets creation and statistics are given in Appendix C.\nSentiment Analysis Analyzing the sentiment of people towards medical drugs is an essential task in the biomedical domain. To that effect, we use medical drug sentiment analysis dataset2 to identify one of three sentiments: (1) positive, (2) negative, and (3) neutral.\n2https://www.kaggle.com/arbazkhan971/ analyticvidhyadatasetsentiment\nDocument Classification We have used the Hallmarks of Cancer (HoC) dataset (Baker et al., 2016) for this task.\nRisk Factor Identification The goal of this task is to identify risk factors for Coronary Artery Disease (CAD) in diabetic patients over time. For this, we used n2c2 2014 shared task track 2 dataset (Kumar et al., 2015) with two different purposes: (1) identify if the risk factor is presented in the medical discharge summary and (2) time of risk factor present in the discharge records."
    }, {
      "heading" : "3.2 Biomedical Instructions",
      "text" : "Motivated by (Mishra et al., 2021b), we have used a similar approach to create Biomedical Instructions (BIs). BI consists of natural language instructions that describe a task and contain instances of that task. Figure 4 shows an example of BI that describe a “Named Entity Recognition (NER)” task accompanied with a few positive examples. Here, we have introduced a unified schema to present BI and described how we can construct BI for each task given in the BoX."
    }, {
      "heading" : "3.2.1 Unified Schema",
      "text" : "All BIs are mapped to the unified schema. Figure 3 illustrates the schematic representation of the schema. As shown in Figure 3, unified schema consists of a definition, prompt, and positive examples. This schema helps in understandably organizing each BI. Each of the elements of the schema is explained below:\nDefinition contains the core explanation about the task and detailed instruction to the model that what needs to be done in the given task.\nPrompt is the short explanation of the task that needs to be done.\nExamples contain the input/output pairs of the task instance along with the explanation of how the output is generated. Generally, we provide 2-3 examples for each task.\nInstances contain the input/output pairs of training samples from the task datasets."
    }, {
      "heading" : "3.2.2 Construction of BI",
      "text" : "We have created a BI for each dataset given in the BoX. To create BI, we manually fill in the fields of unified instruction schema (Figure 3). For each dataset, the BI is created by one author and were verified by other authors.\nQuality of BIs In the instruction verification process, we edit BIs if needed in terms of grammar, typos, ambiguity, etc. to improve quality. According to (Beltagy et al., 2020), concise instructions are more beneficial compare to repetition, hence, we also redact repetition from BIs. So, our BIs consists of high-quality, short, and meaningful task definition, and prompts.\nPositive examples and its explanation For each dataset, we have provided 2-3 positive examples and corresponding explanations to give an idea of how to perform the given task. As we know, the\nselection of examples has an impact on model performance (Lu et al., 2021). To that extent, we have been careful in selecting examples for text generation and classification tasks. For text generation, we have provided 2-3 examples with a detailed explanation about how the output is generated. For text classification tasks, we have included examples corresponding to each class with an explanation of why the particular class is assigned to a given input instance. All positive examples are drawn from training instances and have been removed from training in order to avoid repetition. All the explanations of examples pass through the verification process to maintain high quality.\nCollection of input/output instances Since each biomedical NLP dataset included in the BoX has there own annotated input/output instances, we converted them into text-to-text format (Lourie et al., 2021). Examples of instances converted for each task is given in Appendix B. After this, we appended all instances tuple (i.e., <input, output>) with instruction schema (as shown in Figure 3)."
    }, {
      "heading" : "4 Problem Setup and Models",
      "text" : ""
    }, {
      "heading" : "4.1 Problem setup",
      "text" : "Let us assume, we have input/output instances pair (Xt, Yt) for given task t. Along with that, each task is described in terms of its instruction BIt.\nSingle-task models Traditional supervised models learn mapping function (fM ) between input (x) and output (y), where (x, y) ∈ (Xttrain, Yttrain) and evaluated on the same task (Xttest, Yttest). We refer this setup as single-task learning.\nMulti-task models In this setup, we combined training data and corresponding biomedical instruction of all tasks together. The goal of multitask learning models to learn mapping function (fM ) between input (x), output (y) and biomedical instruction BIt, i.e., fM (BIt, x) = y, where (x, y) ∈ (Xt, Yt). This model is evaluated on taskspecific instances (x, y) ∈ (Xttest, Yttest) In contrast to single-task models, single model is used here to solve various tasks, hence, achieving generalization. We refer this as MTL."
    }, {
      "heading" : "4.2 Models",
      "text" : "We propose an instruction-based model to achieve multi-tasking and compare it with two baselines: (1) single-task models, and (2) multi-task models\nwithout instructions. We have fine-tuned the BART (base) model (Lewis et al., 2019) to build baselines as well as the proposed model."
    }, {
      "heading" : "4.2.1 Baselines",
      "text" : "Single-Task models As formulated in the singletask problem setup, we have trained the BART model on each task from the BoX and evaluated it on the same task.\nMulti-task without instruction To build this baseline, we have combined training data of each task from the BoX together without appending BIs and trained a single model on the combined data. We refer this model as Vanilla-BoXBART. This model is evaluated on each task of the BoX."
    }, {
      "heading" : "4.2.2 Proposed Model",
      "text" : "As formulated in the multi-task problem setup, we have combined training data and the corresponding BI of each task. To combine instruction with input instances, we map a BI and an input (x) into the textual format and obtain enc(BIt, x). After that, BART model is used to predict an output (y) using mapping function fM : enc(BIt, x) → y. To perform encoding, a standard NLP paradigm of mapping is used, i.e., mapping an input to text. Here, we map each element of BI (i.e., definition and positive examples as shown in the schema) to a textual format and append it before the input instances. After appending BI of each task to instances, we combined all training data of each task. Now, we fine-tuned the BART model with this combined instruction meta-dataset. We refer this instruction-tuned model as In-BoXBART."
    }, {
      "heading" : "5 Experiments and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "We have used BART (base) model to build all baselines and proposed model. All the experiments are performed using Quadro RTX 8000 GPU. All models are trained for 3 epochs. In particular, we have used huggingface implementation of the BART and its pre-defined functions for the training and evaluation with default parameters.\nInstance Selection As we know, BART (base) can accept the input of a maximum 1024 token length. Since there are few instances in some datasets that exceed this limit (after including instructions), we have discarded those instances while creating instruction tasks. We have also removed those same instances while training two\nbaselines to do a fair comparison. We have discarded long samples (>1024 token length) from validation and testing data as well.\nExample Selection As discussed in (Lu et al., 2021), the selection and order of the examples included in instructions matters for mainly classification tasks and affects the performance of the model. We empirically conclude that the proposed model benefits from ignoring examples from biomedical instructions for classification tasks during training and evaluation. Hence, we have discarded all examples from the BIs associated with the classification instruction tasks.\nInstance Sampling Some classification datasets used to create the BoX are imbalanced. To balance these datasets, we have applied the sampling techniques (Poolsawad et al., 2014) before using datasets to create BoX. In particular, we have analyzed three sampling techniques: (1) undersampling, (2) average-sampling, and (3) oversampling. In under-sampling, we have reduced instances for all the classes to the class with the lowest number of instances. In contrast, we have over-sampled instances via replication of random instances to the class with the highest number of instances to achieve over-sampling. In average sampling, we calculated mean of number of instances across all the classes and over-sampled or undersampled instances accordingly for each class.\nFew-shot setting Similar to the (Schick and Schütze, 2020), we have started with 32 randomly selected instances for each instruction task from the BoX to exhibit few-shot learning. After that, we have increased randomly selected instance instances per task to 100/1k/4k. If any task have already less number of instances than the threshold (i.e., 100/1k/4k), we keep all the instances from that task. While selecting the instances, we made sure that we select balanced data for the classification tasks. Moreover, the BoX contains an average 6k instances per task.\nEvaluation Metric We have used Rouge-L (Lin, 2004) as our evaluation metric since we have treated all the tasks as text generation problems."
    }, {
      "heading" : "5.2 Results and Findings",
      "text" : "Effect of Sampling As mentioned above, we have conducted three experiments to analyze the effect of sampling on In-BoXBART. We trained\nour model using training data obtained from (1) under-sampling, (2) average-sampling, and (3) over-sampling. We achieved on an average (across all instruction tasks) 69.62%, 70.23% and 73.49% Rouge-L for under-, average- and over-sampling, respectively. Here, we observed from the experimental results that over-sampling gives better performance compared to under- and averagesampling since there is a loss of training data samples for under- and average-sampling. Hence, we have reported results of over-sampling as the main result in Table 2.\nPerformance comparison Table 2 presents the results for single-task model, Vanilla-BoXBART and In-BoXBART. We can see from Table 2 that the single-task model, Vanilla-BoXBART, and InBoXBART achieve on an average (across all tasks) Rouge-L of 70.51%, 55.55%, and 73.49%, respectively. From the result, we can observe that VanillaBoXBART reduces the complexity compared to the single-task model (i.e., 110 million parameters vs. 32x110 million parameters), however, the on an average performance drops by 14.96% in terms of Rouge-L compared to single-task models. This indicates that multi-task learning in biomedical is difficult than general domain NLP since many previous works have shown that the multi-task model outperforms the single-task model (Lourie et al., 2021; McCann et al., 2018). On the other hand, In-BoXBART, which has the same complexity as Vanilla-BoXBART, significantly outperforms Vanilla-BoXBART by on average 17.94%, and also outperforms the single-task model by a 2.98% margin, precisely. This indicates the benefit of using instructions to achieve the MTL in the biomedical domain.\nEffect of instruction in few-shot learning We have compared the average Rouge-L of InBoXBART with a single-task baseline. Figure 5 shows the relative performance of In-BoXBART compared to single-task baseline. We have shown results for all few-shot learning experiments in Appendix D. From the results, we see that InBoXBART achieves on an average 60.64% RougeL and the single-task model achieves 37.31% for 32 instances per task. In-BoxBART significantly outperforms the single-task baseline by 23.33%. From Figure 5, we can see that In-BoXBART consistently perform better compared to baseline. As we know, obtaining a large annotated dataset in\nthe biomedical domain is difficult, time-consuming and costly. From few-shot learning, we can see that instructions are beneficial in achieving high performance compared to task-specific models."
    }, {
      "heading" : "5.3 Analysis",
      "text" : "For which tasks, instruction is helpful? From Table 2, we can see that In-BoXBART outperforms baselines for 5 categories, i.e., NER, deidentification, RE, SR and risk factor identifica-\ntion. From this, we can see that instructions are more helpful in these five categories. However, InBoXBART achieves performance lower or par with the single-task baseline for the tasks from QA, POS tagging, sentiment analysis and document classification which indicates room for improvement in this direction.\nWhich are harder tasks to solve using instructions? Although instructions help in achieving\nbetter performance for some tasks compared to the single-task model, the overall performance is still lower. For example, instruction improves performance for de-identification, but overall performance on this task is only 50.82% which can be improved. A similar pattern we can see for BioNLP12CG and CRAFT from NER, BioASQ8b (factoid, list) and PubmedQA from QA, and Medical Drug from the sentiment analysis category. In general, we can observe that tasks that include either multi-class scenario or answer generation from the context are most likely to be harder to solve using instructions. For example, CRAFT and BioNLP13CG have 6 entity types which are higher than any other tasks from NER, and we can see that the performance for these two tasks is lower compared to other tasks from NER.\nFor which tasks, instruction is the most beneficial in few shot setting? From the results shown in Appendix D, tasks from the NER, deidentification, QA, sentiment analysis and risk factor identification shows on average larger improvement compared to baselines for the few-shot settings (i.e., 32 and 100 instances per task). This indicates that instructions are beneficial for the tasks from the above categories."
    }, {
      "heading" : "6 Discussion",
      "text" : "Can we design better instructions? Since instruction teach the model how to solve a given task, domain specific information rich instructions can improve model performance. One potential way is to use the knowledge of domain experts. However, designing a good biomedical instruction can be one research direction.\nHow to handle long-context input? Training instances of many biomedical datasets consist Electronic Health Records (EHRs) or discharge summaries of patients. Because of this, these instances are long and exceed the maximum input length of LMs such as BERT, BART. In this scenario, encoding extra information in terms of prompts or instructions becomes difficult. A potential solution is use longformer (Beltagy et al., 2020) kind of LMs.\nHow to handle multi-class classification tasks? Multiple classes cause an issue while creating biomedical instructions that we can not present one example per class. If we do that, the encoding of BI and input will exceed the maximum length of LMs. A naive solution is to select examples of a few labels or remove the examples. However, this will cause a label bias issue or performance degradation. Potential future research direction can be designing a methodology to handle multi-class classification tasks.\nHow far we are from the SOTA? We have presented preliminary comparison of our results w.r.t. state-of-the-art (SOTA) single-task systems for 21 instruction tasks3 from the BoX as shown in Appendix E. Form the results, we can see that the performance of the proposed model remains far from the SOTA for some tasks, indicating significant room for further research in this domain."
    }, {
      "heading" : "7 Summary and Conclusions",
      "text" : "This research shows the impact of instructions in MTL for the first time in the biomedical domain. To this extent, we introduced the BoX, a first benchmark dataset consisting of 32 instruction tasks across various biomedical NLP domains. Using this meta-dataset, we proposed a unified model, i.e., In-BoXBART which outperforms single-task baseline and Vanilla-BoxBART by ∼ 3% and ∼ 18%, respectively. Our proposed approach also shows an effective performance for a few-shot setting which is more beneficial in the biomedical domain where obtaining large annotated datasets is difficult. We hope that the BoX benchmark, In-BoXBART, and experimental results encourage future research into more unified models for biomedical NLP.\n3Since we have re-purposed original datasets, some tasks will not have SOTA systems."
    }, {
      "heading" : "A Statistics of Instruction Tasks",
      "text" : "This section provides all the statistics of training, validation and inference data used for experiments in Table 3. All the number of instances provided in Table 3 are calculated after discarding the instances with more than 1024 token length as described in the section 5.1. We have divided the dataset into standard 70/10/20 splits for train/validation/test if there is no separate validation and testing set provided in the dataset.\nB Instruction Tasks and Examples\nTo build all the models (baselines, proposed model and few-shot learning), we adapt the unified format for all the tasks of BoX. We converted all the tasks into the text-to-text format, including the classification tasks. Table 4 shows an example of input and output from each category. Moreover, we have also re-purposed some biomedical datasets to create more than one task as described in the section 3.1."
    }, {
      "heading" : "C Systematic Review Datasets",
      "text" : "This section describes the brief data creation process for Systematic Reviews (SRs) that are used in this study. The relentless growth in clinical research and published articles have created a need for automation to expedite the process of SRs and to enable Living Systematic Reviews (LSRs). A crucial step in both SRs and LSRs is the title and abstract-based screening of the articles. A new dataset was developed from six SRs in the clinical domain by Mayo clinic physicians. In this study, we used data from the following five SRs that were conducted using the traditional (manual) process and published in relevant venues: (1) Hormone Replacement Therapy (HRT), (2) Cooking, (3) Accelerometer, (4) Acromegaly, and (5) COVID. The initial bibliographic search was designed and conducted by an experienced librarian with guidance from the principal investigators for the respective studies. The search was conducted in different bibliographic databases like PubMed, PubMed Central (PMC), Embase, EBM Reviews, and Ovid MEDLINE(R). Each article in the bibliographic search results was categorized by two physicians with domain expertise as “Include” or “Exclude”, by reading the title and abstract of the article. When there was a disagreement between two annotators, a positive class (i.e., “Include”) was preferred."
    }, {
      "heading" : "D Few-Shot Learning results",
      "text" : "This section presents the results of few-shot learning for all instruction tasks in Table 5."
    }, {
      "heading" : "E State-of-the-art results",
      "text" : "In Table 6, we present State-Of-The-Art (SOTA) results for 21 tasks. To compare the SOTA results with the proposed model, we calculate the corresponding metric used in particular research from our model predictions. For each task, we gather the best performance, and specifically, they are BioASQ-8b (Nentidis et al., 2020), Chemprot (Peng et al., 2019), DDI (Peng et al., 2019). In Chemprot and DDI, we compare results with the base LMs instead of large for a fair comparison. SOTA results for all 15 NER datasets are obtained from (Banerjee et al., 2021). Best performance for the HoC dataset is obtained from (Peng et al., 2019). Here, we have considered the result of the best system submitted to (Stubbs et al., 2015) as SOTA result."
    } ],
    "references" : [ {
      "title" : "Biomtransformers: Building large biomedical language models with bert, albert and electra",
      "author" : [ "Sultan Alrowili", "K Vijay-Shanker." ],
      "venue" : "Proceedings of the 20th Workshop on Biomedical Language Processing, pages 221–227.",
      "citeRegEx" : "Alrowili and Vijay.Shanker.,? 2021",
      "shortCiteRegEx" : "Alrowili and Vijay.Shanker.",
      "year" : 2021
    }, {
      "title" : "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
      "author" : [ "Simon Baker", "Ilona Silins", "Yufan Guo", "Imran Ali", "Johan Högberg", "Ulla Stenius", "Anna Korhonen." ],
      "venue" : "Bioinformatics, 32(3):432–440.",
      "citeRegEx" : "Baker et al\\.,? 2016",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 2016
    }, {
      "title" : "Biomedical named entity recognition via knowledge guidance and question answering",
      "author" : [ "Pratyay Banerjee", "Kuntal Kumar Pal", "Murthy Devarakonda", "Chitta Baral." ],
      "venue" : "ACM Transactions on Computing for Healthcare, 2(4):1–24.",
      "citeRegEx" : "Banerjee et al\\.,? 2021",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2021
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural network multi-task learning approach to biomedical named entity recognition",
      "author" : [ "Gamal Crichton", "Sampo Pyysalo", "Billy Chiu", "Anna Korhonen." ],
      "venue" : "BMC bioinformatics, 18(1):1–14.",
      "citeRegEx" : "Crichton et al\\.,? 2017",
      "shortCiteRegEx" : "Crichton et al\\.",
      "year" : 2017
    }, {
      "title" : "The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982",
      "author" : [ "Avia Efrat", "Omer Levy" ],
      "venue" : null,
      "citeRegEx" : "Efrat and Levy.,? \\Q2020\\E",
      "shortCiteRegEx" : "Efrat and Levy.",
      "year" : 2020
    }, {
      "title" : "The ddi corpus: An annotated corpus with pharmacological substances and drug–drug interactions",
      "author" : [ "María Herrero-Zazo", "Isabel Segura-Bedmar", "Paloma Martínez", "Thierry Declerck." ],
      "venue" : "Journal of biomedical informatics, 46(5):914–920.",
      "citeRegEx" : "Herrero.Zazo et al\\.,? 2013",
      "shortCiteRegEx" : "Herrero.Zazo et al\\.",
      "year" : 2013
    }, {
      "title" : "Clinical xlnet: modeling sequential clinical notes and predicting prolonged mechanical ventilation",
      "author" : [ "Kexin Huang", "Abhishek Singh", "Sitong Chen", "Edward T Moseley", "Chih-ying Deng", "Naomi George", "Charlotta Lindvall." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the biocreative vi precision medicine",
      "author" : [ "Rezarta Islamaj Doğan", "Sun Kim", "Andrew ChatrAryamontri", "Chih-Hsuan Wei", "Donald C Comeau", "Rui Antunes", "Sérgio Matos", "Qingyu Chen", "Aparna Elangovan", "Nagesh C Panyam" ],
      "venue" : null,
      "citeRegEx" : "Doğan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Doğan et al\\.",
      "year" : 2019
    }, {
      "title" : "Pubmedqa: A dataset for biomedical research question answering",
      "author" : [ "Qiao Jin", "Bhuwan Dhingra", "Zhengping Liu", "William W Cohen", "Xinghua Lu." ],
      "venue" : "arXiv preprint arXiv:1909.06146.",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unifiedqa: Crossing format boundaries with a single qa system",
      "author" : [ "Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Clark", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:2005.00700.",
      "citeRegEx" : "Khashabi et al\\.,? 2020",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "Medgpt: Medical concept prediction from clinical narratives",
      "author" : [ "Zeljko Kraljevic", "Anthony Shek", "Daniel Bean", "Rebecca Bendayan", "James Teo", "Richard Dobson." ],
      "venue" : "arXiv preprint arXiv:2107.03134.",
      "citeRegEx" : "Kraljevic et al\\.,? 2021",
      "shortCiteRegEx" : "Kraljevic et al\\.",
      "year" : 2021
    }, {
      "title" : "Creation of a new longitudinal corpus of clinical narratives",
      "author" : [ "Vishesh Kumar", "Amber Stubbs", "Stanley Shaw", "Özlem Uzuner." ],
      "venue" : "Journal of biomedical informatics, 58:S6–S10.",
      "citeRegEx" : "Kumar et al\\.,? 2015",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2015
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "author" : [ "Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig." ],
      "venue" : "arXiv preprint arXiv:2107.13586.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark",
      "author" : [ "Nicholas Lourie", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:2103.13009.",
      "citeRegEx" : "Lourie et al\\.,? 2021",
      "shortCiteRegEx" : "Lourie et al\\.",
      "year" : 2021
    }, {
      "title" : "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "author" : [ "Yao Lu", "Max Bartolo", "Alastair Moore", "Sebastian Riedel", "Pontus Stenetorp." ],
      "venue" : "arXiv preprint arXiv:2104.08786.",
      "citeRegEx" : "Lu et al\\.,? 2021",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2021
    }, {
      "title" : "The natural language decathlon: Multitask learning as question answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "Reframing instructional prompts to gptk’s language",
      "author" : [ "Swaroop Mishra", "Daniel Khashabi", "Chitta Baral", "Yejin Choi", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:2109.07830.",
      "citeRegEx" : "Mishra et al\\.,? 2021a",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2021
    }, {
      "title" : "Cross-task generalization via natural language crowdsourcing instructions",
      "author" : [ "Swaroop Mishra", "Daniel Khashabi", "Chitta Baral", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:2104.08773.",
      "citeRegEx" : "Mishra et al\\.,? 2021b",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2021
    }, {
      "title" : "Towards question format independent numerical reasoning: A set of prerequisite tasks",
      "author" : [ "Swaroop Mishra", "Arindam Mitra", "Neeraj Varshney", "Bhavdeep Singh Sachdeva", "Chitta Baral." ],
      "venue" : "ArXiv, abs/2005.08516.",
      "citeRegEx" : "Mishra et al\\.,? 2020",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2020
    }, {
      "title" : "Overview of bioasq 8a and 8b: Results of the eighth edition of the bioasq tasks a and b",
      "author" : [ "Anastasios Nentidis", "Anastasia Krithara", "Konstantinos Bougiatiotis", "Georgios Paliouras." ],
      "venue" : "CLEF (Working Notes).",
      "citeRegEx" : "Nentidis et al\\.,? 2020",
      "shortCiteRegEx" : "Nentidis et al\\.",
      "year" : 2020
    }, {
      "title" : "Transfer learning in biomedical natural language processing: an evaluation of bert and elmo on ten benchmarking datasets",
      "author" : [ "Yifan Peng", "Shankai Yan", "Zhiyong Lu." ],
      "venue" : "arXiv preprint arXiv:1906.05474.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Scifive: a text-to-text transformer model for biomedical literature",
      "author" : [ "Long N Phan", "James T Anibal", "Hieu Tran", "Shaurya Chanana", "Erol Bahadroglu", "Alec Peltekian", "Grégoire Altan-Bonnet." ],
      "venue" : "arXiv preprint arXiv:2106.03598.",
      "citeRegEx" : "Phan et al\\.,? 2021",
      "shortCiteRegEx" : "Phan et al\\.",
      "year" : 2021
    }, {
      "title" : "Balancing class for performance of classification with a clinical dataset",
      "author" : [ "N Poolsawad", "C Kambhampati", "JGF Cleland." ],
      "venue" : "proceedings of the World Congress on Engineering, volume 1, pages 1–6.",
      "citeRegEx" : "Poolsawad et al\\.,? 2014",
      "shortCiteRegEx" : "Poolsawad et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitask prompted training enables zero-shot task generalization",
      "author" : [ "Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen H Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Teven Le Scao", "Arun Raja" ],
      "venue" : null,
      "citeRegEx" : "Sanh et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2021
    }, {
      "title" : "It’s not just size that matters: Small language models are also few-shot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2009.07118.",
      "citeRegEx" : "Schick and Schütze.,? 2020",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "Green ai",
      "author" : [ "Roy Schwartz", "Jesse Dodge", "Noah A Smith", "Oren Etzioni." ],
      "venue" : "Communications of the ACM, 63(12):54–63.",
      "citeRegEx" : "Schwartz et al\\.,? 2020",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "Energy and policy considerations for deep learning in nlp",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "arXiv preprint arXiv:1906.02243.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Identifying risk factors for heart disease over time: Overview of 2014 i2b2/uthealth shared task track 2",
      "author" : [ "Amber Stubbs", "Christopher Kotfila", "Hua Xu", "Özlem Uzuner." ],
      "venue" : "Journal of biomedical informatics, 58:S67–S77.",
      "citeRegEx" : "Stubbs et al\\.,? 2015",
      "shortCiteRegEx" : "Stubbs et al\\.",
      "year" : 2015
    }, {
      "title" : "Syntax annotation for the genia corpus",
      "author" : [ "Yuka Tateisi", "Akane Yakushiji", "Tomoko Ohta", "Jun’ichi Tsujii" ],
      "venue" : "In Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts",
      "citeRegEx" : "Tateisi et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Tateisi et al\\.",
      "year" : 2005
    }, {
      "title" : "Evaluating the state-of-the-art in automatic deidentification",
      "author" : [ "Özlem Uzuner", "Yuan Luo", "Peter Szolovits." ],
      "venue" : "Journal of the American Medical Informatics Association, 14(5):550–563.",
      "citeRegEx" : "Uzuner et al\\.,? 2007",
      "shortCiteRegEx" : "Uzuner et al\\.",
      "year" : 2007
    }, {
      "title" : "2021a. Pre-trained language models in biomedical domain: A systematic survey",
      "author" : [ "Benyou Wang", "Qianqian Xie", "Jiahuan Pei", "Prayag Tiwari", "Zhao Li" ],
      "venue" : "arXiv preprint arXiv:2110.05006",
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Entailment as few-shot learner",
      "author" : [ "Sinong Wang", "Han Fang", "Madian Khabsa", "Hanzi Mao", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2104.14690.",
      "citeRegEx" : "Wang et al\\.,? 2021b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Finetuned language models are zero-shot learners",
      "author" : [ "Jason Wei", "Maarten Bosma", "Vincent Y Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M Dai", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:2109.01652.",
      "citeRegEx" : "Wei et al\\.,? 2021",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "During 2017-2019, pre-train and fine-tune paradigm (Liu et al., 2021) became the prevalent approach in NLP.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Due to success of Language Models (LMs) in the biomedical domain such as BioBERT (Lee et al., 2020), ClinicalXLNET (Huang et al.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "2019), and others (Alrowili and Vijay-Shanker, 2021; Kraljevic et al., 2021; Phan et al., 2021), this paradigm is widely used for creating many task-specific models (Wang et al.",
      "startOffset" : 18,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "2019), and others (Alrowili and Vijay-Shanker, 2021; Kraljevic et al., 2021; Phan et al., 2021), this paradigm is widely used for creating many task-specific models (Wang et al.",
      "startOffset" : 18,
      "endOffset" : 95
    }, {
      "referenceID" : 25,
      "context" : "2019), and others (Alrowili and Vijay-Shanker, 2021; Kraljevic et al., 2021; Phan et al., 2021), this paradigm is widely used for creating many task-specific models (Wang et al.",
      "startOffset" : 18,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : ", 2021), this paradigm is widely used for creating many task-specific models (Wang et al., 2021a; Banerjee et al., 2021).",
      "startOffset" : 77,
      "endOffset" : 120
    }, {
      "referenceID" : 31,
      "context" : ", require large computational resources) and timeconsuming (Strubell et al., 2019; Schwartz et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 105
    }, {
      "referenceID" : 30,
      "context" : ", require large computational resources) and timeconsuming (Strubell et al., 2019; Schwartz et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "Past attempts have been made in general-domain NLP to achieve generalization across tasks such as MQAN (McCann et al., 2018), UNICORN (Lourie et al.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : ", 2018), UNICORN (Lourie et al., 2021), and UnifiedQA (Khashabi et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Recently, prompt-based models have been widely used because of their ability to achieve generalization instead of task-specific models (Liu et al., 2021).",
      "startOffset" : 135,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "(2021) and (Sanh et al., 2021) show the effectiveness of instructional prompts in generalizing on seen as well as unseen general-domain NLP tasks.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "DecaNLP (McCann et al., 2018) built a multi-tasking model by converting format of each tasks to question answering format.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "The multitasking model T5 (Raffel et al., 2020) was built with the help of a unified framework that converts all text-based language problems into a text-to-text format.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "SCIFIVE (Phan et al., 2021) involved building a text to text model for the biomedical literature.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 28,
      "context" : "T0 (Sanh et al., 2021) uses prompts along with instances to do multitask learning and they focus on achieving zero-shot task generalization.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "Instruction Learning The turking test (Efrat and Levy, 2020) was proposed to measure the efficacy of models to follow instructions.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 21,
      "context" : "Natural Instructions (Mishra et al., 2021b) broke down each task to multiple sub-tasks that helped models in following instructions and subsequently generalize to unseen tasks (cross-task generalization).",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 37,
      "context" : "FLAN (Wei et al., 2021) model was built by leveraging instructiontuning on diverse range of tasks and achieving zeroshot generalization on target unseen tasks.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 20,
      "context" : "Task reframing (Mishra et al., 2021a) proposed several guidelines to reframe task instructions to improve model response to follow instructions.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "We use fifteen publicly available biomedical NER datasets (Crichton et al., 2017) to create instructions tasks.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 34,
      "context" : "We use n2c2 2006 de-identification challenge dataset (Uzuner et al., 2007) to perform this task.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : "We use GENIA corpus (Tateisi et al., 2005) built from MEDLINE abstracts for the POS tagging task.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "To execute this task, we used the BioASQ-8b dataset (Nentidis et al., 2020) for different question types, i.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "Also, we use PubMedQA dataset (Jin et al., 2019) for this task.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : ", 2019), and (2) Drug-Drug Interaction (DDI) corpus from SemEval 2013 DDI Extraction challenge (Herrero-Zazo et al., 2013).",
      "startOffset" : 95,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Document Classification We have used the Hallmarks of Cancer (HoC) dataset (Baker et al., 2016) for this task.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "For this, we used n2c2 2014 shared task track 2 dataset (Kumar et al., 2015) with two different purposes: (1) identify if the risk factor is presented in the medical discharge summary and (2) time of risk factor present in the discharge records.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : "Motivated by (Mishra et al., 2021b), we have used a similar approach to create Biomedical Instructions (BIs).",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "According to (Beltagy et al., 2020), concise instructions are more beneficial compare to repetition, hence, we also redact repetition from BIs.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "selection of examples has an impact on model performance (Lu et al., 2021).",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "Collection of input/output instances Since each biomedical NLP dataset included in the BoX has there own annotated input/output instances, we converted them into text-to-text format (Lourie et al., 2021).",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "We have fine-tuned the BART (base) model (Lewis et al., 2019) to build baselines as well as the proposed model.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "Example Selection As discussed in (Lu et al., 2021), the selection and order of the examples included in instructions matters for mainly classification tasks and affects the performance of the model.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : "To balance these datasets, we have applied the sampling techniques (Poolsawad et al., 2014) before using datasets to create BoX.",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 29,
      "context" : "Few-shot setting Similar to the (Schick and Schütze, 2020), we have started with 32 randomly",
      "startOffset" : 32,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "Evaluation Metric We have used Rouge-L (Lin, 2004) as our evaluation metric since we have treated all the tasks as text generation problems.",
      "startOffset" : 39,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "This indicates that multi-task learning in biomedical is difficult than general domain NLP since many previous works have shown that the multi-task model outperforms the single-task model (Lourie et al., 2021; McCann et al., 2018).",
      "startOffset" : 188,
      "endOffset" : 230
    }, {
      "referenceID" : 19,
      "context" : "This indicates that multi-task learning in biomedical is difficult than general domain NLP since many previous works have shown that the multi-task model outperforms the single-task model (Lourie et al., 2021; McCann et al., 2018).",
      "startOffset" : 188,
      "endOffset" : 230
    }, {
      "referenceID" : 3,
      "context" : "A potential solution is use longformer (Beltagy et al., 2020) kind of LMs.",
      "startOffset" : 39,
      "endOffset" : 61
    } ],
    "year" : 0,
    "abstractText" : "Single-task models have proven pivotal in solving specific tasks; however, they have limitations in real-world applications where multitasking is necessary and domain shifts are exhibited. Recently, instructional prompts have shown significant improvement towards multitask generalization; however, the effect of instructional prompts and Multi-Task Learning (MTL) has not been systematically studied in the biomedical domain. Motivated by this, this paper explores the impact of instructional prompts for biomedical MTL. We introduce the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X) various categories. Using this meta-dataset, we propose a unified model termed as In-BoXBART, that can jointly learn all tasks of the BoX without any task-specific modules. To the best of our knowledge, this is the first attempt to propose a unified model in the biomedical domain and use instructions to achieve generalization across several biomedical tasks. Experimental results indicate that the proposed model: 1) outperforms single-task baseline by ∼3% and multitask (without instruction) baseline by ∼18% on an average, and 2) shows ∼23% improvement compared to single-task baseline in few-shot learning (i.e., 32 instances per task) on an average. Our analysis indicates that there is significant room for improvement across tasks in the BoX, implying the scope for future research direction.1",
    "creator" : null
  }
}
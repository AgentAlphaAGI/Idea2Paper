{
  "name" : "ARR_2022_21_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improve Discourse Dependency Parsing with Contextualized Representations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Discourse dependency parsing (DDP) is the task of identifying the structure and relationship between Elementary Discourse Units (EDU) in a document. It is a fundamental task of natural language understanding and can benefit many downstream applications.\nAlthough existing works have achieved much progress using transition systems (Jia et al., 2018b,a; Hung et al., 2020) or graph-based models (Li et al., 2014a; Shi and Huang, 2018; Afantenos et al., 2015), this task still remains a challenge. Different from syntactic parsing, the basic components in a discourse are EDUs, sequences of words, which are not trivial to represent in a straightforward way like word embeddings. Predicting the dependency and relationship between EDUs sometimes necessitates the help of a global understanding of the context so that contextualized EDU representations in the discourse is needed. Furthermore, previous studies have shown the benefit of breaking discourse analysis into intra- and inter-sentential\nlevels, building sub-trees for each sentence first and then assembling sub-trees to form a complete discourse tree. In this Sentence-First (Sent-First) framework, it is even more crucial to produce appropriate contextualized representations for text units when analyzing in intra- or inter-sentential levels.\nFigure 1 shows a discourse dependency structure for a scientific abstract from SciDTB (Yang and Li, 2018). The lengths of EDUs vary a lot, from more than 10 words to 2 words only (EDU 12: tests show), making it especially hard to encode by themselves alone. Sometimes it is sufficient to consider the contextual information in a small range as in the case of EDU 13 and 14, other times we need to see a larger context as in the case of EDU 1 and 4, crossing several sentences. This again motivates us to consider encoding contextual representations of EDUs separately on intra- and inter-sentential levels to dynamically capture specific features needed for discourse analysis on different levels.\nAnother motivation from this example is the discovery that the distribution of discourse relations between EDUs seems to follow certain patterns shared across different articles. Writing patterns\nare document structures people commonly use to organize their arguments. For example, in scientific abstracts like the instance in Figure 1, people usually first talk about background information, then introduce the topic sentence, and conclude with elaborations or evaluations. Here, the example first states the background of widely used automatic metrics, introduces the topic sentence about their contribution of a significance test followed by evaluation and conclusion. Taking advantage of those writing patterns should enable us to better capture the interplay between individual EDUs with the context.\nIn this paper, we explore different contextualized representations for DDP in a Sent-First parsing framework, where a complete discourse tree is built up sentence by sentence. We seek to dynamically capture what is crucial for DDP at different text granularity levels. We further propose a novel discourse relation identification method that addresses the task in a sequence labeling paradigm to exploit common conventions people usually adopt to develop their arguments. We evaluate our models on both English and Chinese datasets, and experiments show our models achieve the state-of-the-art results by explicitly exploiting structural information in the context and capturing writing patterns that people use to organize discourses.\nIn summary, our contributions are mainly twofold: (1) We incorporate the Pre-training and Fine-tuning framework into our design of a SentFirst model and develop better contextualized EDU representations to dynamically capture different information needed for DDP at different text granularity levels. Experiments show that our model outperforms all existing models by a large margin. (2) We formulate discourse relation identification in a novel sequence labeling paradigm to take advantage of the inherent structural information in the discourse. Building upon a stacked BiLSTM architecture, our model brings a new state-of-theart performance on two benchmarks, showing the advantage of sequence labeling over the common practice of direct classification for discourse relation identification."
    }, {
      "heading" : "2 Related Works",
      "text" : "A key finding in previous studies in discourse analysis is that most sentences have an independent well-formed sub-tree in the full document-level discourse tree (Joty et al., 2012). Researchers have\ntaken advantage of this finding to build parsers that utilize different granularity levels of the document to achieve the state-of-the-art results (Kobayashi et al., 2020). This design has been empirically verified to be a generally advantageous framework, improving not only works using traditional feature engineering (Joty et al., 2013; Wang et al., 2017), but also deep learning models (Jia et al., 2018b; Kobayashi et al., 2020). We, therefore, introduce this design to our dependency parsing framework. Specifically, sub-trees for each sentence in a discourse are first built separately, then assembled to form a complete discourse tree.\nHowever, our model differs from prior works in that we make a clear distinction to derive better contextualized representations of EDUs from fine-tuning BERT separately for intra- and intersentential levels to dynamically capture different information needed for discourse analysis at different levels. We are also the first to design stacked sequence labeling models for discourse relation identification so that its hierarchical structure can explicitly capture both intra-sentential and intersentential writing patterns.\nIn the case of implicit relations between EDUs without clear connectives, it is crucial to introduce sequential information from the context to resolve ambiguity. Feng and Hirst (2014) rely on linearchain CRF with traditional feature engineering to make use of the sequential characteristics of the context for discourse constituent parsing. However, they greedily build up the discourse structure and relations from bottom up. At each timestep, they apply the CRF to obtain the locally optimized structure and relation. In this way, the model assigns relation gradually along with the construction of the parsing tree from bottom up, but only limited contextual information from the top level of the partially constructed tree can be used to predict relations. Besides, at each time-step, they sequentially assign relations to top nodes of the partial tree, without being aware that those nodes might represent different levels of discourse units (e.g. EDUs, sentences, or even paragraphs). In contrast, we explicitly train our sequence labeling models on both intra- and inter-sentential levels after a complete discourse tree is constructed so that we can infer from the whole context with a clear intention of capturing different writing patterns occurring at intra- and inter-sentential levels."
    }, {
      "heading" : "3 Task Definition",
      "text" : "We define the task of discourse dependency parsing as following: given a sequence of EDUs of length l, (e1, e2, ..., el) and a set of possible relations between EDUs Re, the goal is to predict another sequence of EDUs (h1, h2, ..., hl) such that ∀hi, hi ∈ (e1, e2, ..., el) is the head of ei and a sequence of relations (r1, r2, ..., rl) such that ∀ri, ri is the relation between tuple (ei, hi)."
    }, {
      "heading" : "4 Our Model",
      "text" : "We follow previous works (Wang et al., 2017) to cast the task of discourse dependency parsing as a composition of two separate yet related subtasks: dependency tree construction and relation identification. We design our model primarily in a twostep pipeline. We incorporate Sent-First design as our backbone (i.e. building sub-trees for each sentence and then assembling them into a complete discourse tree), and formulate discourse relation identification as a sequence labeling task on both intra- and inter-sentential levels to take advantage of the structure information in the discourse. Figure 1 shows the overview of our model."
    }, {
      "heading" : "4.1 Discourse Dependency Tree Constructor",
      "text" : "To take advantage of the property of well-formed sentence sub-trees inside a full discourse tree, we break the task of dependency parsing into two different levels, discovering intra-sentential sub-tree structures first and then aseembling them into a full discourse tree by identifying the inter-sentential structure of the discourse.\nArc-Eager Transition System Since discourse dependency trees are primarily annotated as projective trees (Yang and Li, 2018), we design our tree constructor as a transition system, which converts the structure prediction process into a sequence of predicted actions. At each timestep, we derive a state feature to represent the state, which is fed into an output layer to get the predicted action. Our model follows the standard Arc-Eager system, with the action set: O= {Shift, Left−Arc,Right− Arc,Reduce}.\nSpecifically, our discourse tree constructor maintains a stack S, a queue I, and a set of assigned arcs A during parsing. The stack S and the set of assigned arcs A are initialized to be empty, while the queue I contains all the EDUs in the input sequence. At each time step, an action in the action\nset O is performed with the following definition: Shift pushes the first EDU in queue I to the top of stack S; Left-Arc adds an arc from the first EDU in queue I to the top EDU in stack S (i.e. assigns the first EDU in I to be the head of the top EDU in S) and removes the top EDU in S; Right-Arc adds an arc from the top EDU in stack S to the first EDU in queue I (i.e. assigns the top EDU in S to be the head) and pushes the first EDU in I to stack S; Reduce removes the top EDU in S. Parsing terminates when I becomes empty and the only EDU left in S is selected to be the head of the input sequence. More details of Arc-Eager transition system can be referred from Nivre (2003).\nWe first construct a dependency sub-tree for each sentence, and then treat each sub-tree as a leaf node to form a complete discourse tree across sentences. In this way, we can break a long discourse into smaller sub-structures to reduce the search space. A mathematical bound for the reduction of search space of our Sent-First framework for DDP and discourse constituent parsing is also provided in Appendix.\nContextualized State Representation Ideally, we would like the feature representation to contain both the information of the EDUs directly involved in the action to be executed and rich clues from the context from both the tree-structure and the text, e.g. the parsing history and the interactions between individual EDUs in the context with an appropriate scope of text. In order to capture the structural clues from the context, we incorporate the parsing history in the form of identified dependencies in addition to traditional state representations to represent the current state. At each timestep, we select 6 EDUs from the current state as our feature template, including the first and the second EDU at the top of stack S, the first and the second EDU in queue I, and the head EDUs for the first and the second EDU at the top of stack S, respectively. A feature vector of all zeros is used if there is no EDU at a certain position.\nEDU Representations To better capture an EDU in our Sent-First framework, we use pre-trained BERT (Devlin et al., 2018) to obtain representations for each EDU according to different context. We argue that an EDU should have different representations when it is considered in different parsing levels, and thus requires level-specific contextual representations. For intra-sentential tree construc-\ntor, we feed the entire sentence to BERT and represent each EDU by averaging the last hidden states of all tokens in that EDU. The reason behind is that sentences are often self-contained sub-units of the discourse, and it is sufficient to consider interactions among EDUs within a sentence for intra-sentential analysis. On the other hand, for inter-sentential tree constructor, we concatenate all the root EDUs of different sentences in the discourse to form a pseudo sentence, feed it to BERT, and similarly, represent each root EDU by averaging the last hidden states of all tokens in each root EDU. In this way, we aim to encourage EDUs across different sentences to directly interact with each other, in order to reflect the global properties of a discourse. Figure 2 shows the architecture for our two-stage discourse dependency tree constructor."
    }, {
      "heading" : "4.2 Discourse Relation Identification",
      "text" : "After the tree constructor is trained, we train separate sequence labeling models for relation identification. Although discourse relation identification in discourse dependency parsing is traditionally treated as a classification task, where the common practice is to use feature engineering or neural language models to directly compare two EDUs involved isolated from the rest of the context (Li et al., 2014a; Shi and Huang, 2018; Cheng et al., 2021), sometimes relations between EDU pairs can be hard to be classified in isolation, as global information from the context like how EDUs are organized to support the claim in the discourse is sometimes required to infer the implicit discourse relations\nwithout explicit connectives. Therefore, we propose to identify discourse relation identification as a sequence labeling task.\nStructure-aware Representations For sequence labeling, we need proper representations for EDU pairs to reflect the structure of the dependency tree. Therefore, we first tile each EDU in the input sequence (e1, e2, ..., el) with their predicted heads to form a sequence of EDU pairs ((e1, h1), (e2, h2), ..., (el, hl)). Each EDU pair is reordered so that two arguments appear in the same order as they appear in the discourse. We derive a relation representation for each EDU pair with a BERT fine-tuned on the task of direct relation classification of EDU pairs with the [CLS] representation of the concatenation of two sentences.\nPosition Embeddings We further introduce position embeddings for each EDU pair (ei, hi), where we consider the position of ei in its corresponding sentence, and the position of its sentence in the discourse. Specifically, we use cosine and sine functions of different frequencies (Vaswani et al., 2017) to include position information as:\nPEj = sin(No/10000 j/d) + cos(ID/10000j/d)\nwhere PE is the position embeddings, No is the position of the sentence containing ei in the discourse, ID is the position of ei in the sentence, j is the dimension of the position embeddings, d is the dimension of the relation representation. The position embeddings have the same dimension as relation representations, so that they can be added\ndirectly to get the integrated representation for each EDU pair.\nStacked BiLSTM We propose a stacked BiLSTM neural network architecture to capture both intra-sentential and inter-sentential interplay of EDUs. After labeling the entire sequence of EDU pairs ((e1, h1), (e2, h2), ..., (el, hl)) with the first layer of BiLSTM, we select the root EDU for each sentence (namely the root EDU selected from our intra-sentential tree constructor for each setence) to form another inter-sentential sequence. Another separately trained BiLSTM is then applied to label those relations that span across sentences. Note that we will overwrite predictions of inter-sentential relations of the previous layer if there is a conflict of predictions."
    }, {
      "heading" : "4.3 Training",
      "text" : "Our models are trained with offline learning. We train the tree constructor first, while relation labeling models are trained separately after that. We attain the static oracle to train tree constructors and use the gold dependency structure to train our discourse relation labelling models. Intra- and intersentential tree constructors are trained separately. To label discourse relations, we fine-tune the BERT used to encode the EDU pair with an additional output layer for direct relation classification. Sequence labeling models for relation identification are trained on top of the fine-tuned BERT. We use cross entropy loss for training."
    }, {
      "heading" : "5 Experiments",
      "text" : "Our experiments are designed to investigate how we can better explore contextual representations to improve discourse dependency parsing.\nWe evaluate our models on two discourse treebanks of different language, i.e., Discourse Dependency Treebank for Scientific Abstracts (SciDTB) (Yang and Li, 2018) in English and Chinese Discourse Treebank (CDTB) (Li et al., 2014b). SciDTB contains 1,355 English scientific abstracts collected from ACL Anthology. Averagely, an abstract includes 5.3 sentences, 14.1 EDUs, where an EDU has 10.3 tokens in average. On the other hand, CDTB was originally annotated as connectivedriven constituent trees, and manually converted into a dependency style by Cheng et al. (2021). CDTB contains 2,332 news documents. The average length of a paragraph is 2.1 sentences, 4.5\nEDUs. And an EDU contains 23.3 tokens in average.\nWe evaluate model performance using Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) for dependency prediction and discourse relation identification. UAS is defined as the percentage of nodes with correctly predicted heads, while LAS is defined as the percentage of nodes with both correctly predicted heads and correctly predicted relations to their heads. We report LAS against both gold dependencies and model predicted dependencies. We adopt the finegranularity discourse relation annotations in the original datasets, 26 relations for SciDTB and 17 relations for CDTB.\nFor both datasets, we trained our dependency tree constructors with an Adam optimizer with learning rate 2e-5 for 3 epochs. Our relation labeling models are all trained with an Adam optimizer for 15-20 epochs. Learning rate is set to 2e-5, weight-decay is set to be 1e-4.1"
    }, {
      "heading" : "5.1 Baselines",
      "text" : "Structure Prediction We compare with the following competitive methods for structure prediction. (1) Graph adopts the Eisner’s algorithm to predict the most probable dependency tree structure (Li et al., 2014a; Yang and Li, 2018; Cheng et al., 2021). (2) Two-stage, which is the stateof-the-art model on CDTB and SciDTB, uses an SVM to construct a dependency tree (Yang and Li, 2018; Cheng et al., 2021). (3) Sent-First LSTM is our implmentation of the state-of-theart transition-based discourse constituent parser on RST (Kobayashi et al., 2020), where we use a vanilla transition system with pretrained BiLSTM as the EDU encoder within the Sent-First framework to construct dependency trees. (4) Complete Parser is modified from the best constituent discourse parser on CDTB (Hung et al., 2020), using a transition system with BERT as the EDU encoder to construct a dependency tree.\nWe also implement several model variants for comparison and ablation stmodel udy. (5) Complete Parser (contextualized) is our modified version of Complete Parser where, instead of encoding each EDU separately, we obtain the EDU representations by encoding the whole sentence with BERT and average the corresponding token representations for the EDU. (6) BERT + Sent-First (shared)\n1Our code is available at: [url redacted for blind review].\nincorporate different contextualized embeddings from BERT into the Sent-First framework for parsing at intra- and inter-sentential levels, with the same BERT layer shared across intra-sentential and inter-sentential parsing. (7) BERT + Sent-First fine-tunes separate BERT layers for intra-sentential and inter-sentential parsing independently.\nRelation Identification (1) Graph uses an averaged perceptron to classify relations by direct classification (Cheng et al., 2021; Yang and Li, 2018). (2) Two-stage exploits careful feature engineering and trains an SVM to classify the relations for pairs\nof EDUs (Cheng et al., 2021; Yang and Li, 2018). (3) Sent-First LSTM uses biLSTM to encode each EDU separately and a feed forward neural network for direct relation classification. (4) BERT is our implementation of the state-of-the-art model from Cheng et al. (2021) and Hung et al. (2020), which fine-tunes a BERT model with an additional output layer to directly classify both intra-sentential and inter-sentential relations. (5) BERT + BiL formulates dependency discourse relation identification as a sequence labeling task, training an additional layer of BiLSTM on top of the BERT layer finetuned on direct classification. (6) BERT SBiL trains another BiLSTM to label inter-sentential relations on top of the original model BERT + BiL."
    }, {
      "heading" : "5.2 Main Results",
      "text" : "Dependency Prediction Table 1 summarizes the performances of different models on both datasets in terms of UAS. For traditional feature engineering models, Two-stage has already achieved satisfactory performance, even beating several neural models like Sent-First LSTM and Complete Parser. This is probably because traditional feature engineering methods design delicate structural features in addition to representations of EDUs\nso that they can include contextual clues to facilitate parsing. Complete Parser leverages the benefit of better representations from pre-trained transformers to encode the information of individual EDUs, achieving a significant improvement over Sent-First LSTM model with LSTM as primary encoders. However, we show that our model BERT + Sent-First that exploits the potential of Sent-First framework with proper contextualized representations to capture the interactions between individual EDUs and the context surpasses all the existing baselines. The performance of our model can be further improved if we encode contextualized embeddings separately for intra-sentential and intersentential parsing to dynamically capture different information required to parsing at different text granularity levels.\nRelation Identification Although previous methods like Graph, Two-stage, and Sent-First LSTM achieve decent results on both datasets, their performances are not comparable to transformer methods developed in recent years. BERT (Cheng21) is our implementation of the state-of-the-art method for relation classification in discourse dependency parsing, which improves the baseline by a large margin. Although BERT is still a very strong baseline in many NLP tasks, direct classification with BERT neglects the contextual clues in the discourse that can be exploited to aid discourse relation identification, as have been discussed in section 1. We show that the results can be further improved by making use of the sequential structure of the discourse. We design multiple novel sequence labeling models on top of the fine-tuned BERT and all of them achieve a considerable improvement (more than 1%) over BERT in terms of accuracy both on the gold dependencies and the predicted dependencies from our Sent-First (separate), showing the benefit of en-\nhancing the interactions between individual EDUs with the context. It yields another large gain when we introduce another layer of inter-sentential level BiLSTM, showing again that it is crucial to capture the interactions between EDUs and their context in both intra- and inter-sentential levels."
    }, {
      "heading" : "5.3 Detailed Analysis",
      "text" : "Contextualized Representations for Tree Construction Intuitively, a model should take different views of context when analyzing intra- and inter-sentential structures. As we can see in Table 1, BERT + Sent-First (shared) improves Complete Parser (contextualized) by 1.2% and 2.4% on SciTDB and CDTB, respectively. The only difference is BERT + Sent-First makes explicit predictions on two different levels, while Complete Parser (contextualized) treats them equally. When we force BERT + Sent-First to use different BERTs for intraand inter-sententential analysis, we observe further improvement, around 3% on both datasets.\nIf we take a closer look at their performance in intra- and inter-sentential views in Table 3, we can see that BERT + Sent-First (shared) performs better than single BERT model, Complete Parser (contextualized), on both intra- and inter- levels of SciDTB and CDTB, though in some cases we only observe marginal improvement like inter-sentential level of SciDTB. However, when we enhance BERT + Sent-First with different encoders for intra- and inter-sentential analysis, we can observe significant improvement in all cases. That again shows the importance of anaylzing with different but more focused contextual representations for the two parsing levels.\nClassification or Sequence Labeling? Most previous works treat discourse relation identification as a straightforward classification task, where given two EDUs, a system should identify which relationship the EDU pair hold. As can be seen from Table 2, all sequence labeling models (our main model as well as the variants) achieve a consid-\nerable gain over direct classification models on both datasets, especially in terms of accuracy on gold dependencies. This result verifies our hypothesis about the structural patterns of discourse relations shared across different articles. It is noticed that BERT + SBiL performs the best because its hierarchical structure can better capture different structured representations occuring at intra- and inter-sentential levels.\nIn Table 4, we include the performances of different models on intra- and inter-sentential relations on SciDTB with gold dependency structure. We observe that although our BERT+BiL model improves accuracies on both levels compared to the traditional classification model, the more significant improvement is on the inter-sentential level (by 2.1%). We show that it can even be promoted by another 2.4% if we stack an additional BiLSTM layer on top to explicitly capture the interplay between EDUs on the inter-sentential level. That’s probably because writing patterns are more likely to appear in a global view so that discourse relations on the inter-sentential level tend to be more structurally organized than that on the intra-sentential level.\nTo test the effectiveness of our model for implicit discourse relation identification, We delete some freely omissible connectives identified by Ma et al. (2019) to automatically generate implicit discourse relations. This results in 564 implicit instances in the test discourses. We run our model on the modified test data without retraining and compare the accuracies on those generated implicit relations. Table 5 shows the accuracies for those 564 instances before and after the modification. After the modification, although accuracies of all three models drop significantly, our sequence labeling model\nBERT+BiL and BERT+SBiL outperform the traditional direct classification model BERT by 1.4% and 2.5% respectively, showing that our sequence labeling models can make use of clues from the context to help identify relations in the case of implicit relations.\nIn addition, we experiment with other empirical implementations of contextualized representations instead of averaging tokens like using [CLS] for aggregate representations of sentences for intersentential dependency parsing, but we did not observe a significant difference. Averaging token representations turns out to have better generalizability and more straightforward for implementation."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "For the example shown in Figure 1, the relation between EDU 9 and EDU 13 is hard to classify using traditional direct classification because both of them contain only partial information of the sentences but their relation spans across sentences. Therefore, traditional direct classification model gets confused on this EDU pair and predicts the relation to be \"elab-addition\", which is plausible if we only look at those two EDUs isolated from the context. However, given the gold dependency structure, our sequence labeling model fits the EDU pair into the context and infers from common writing patterns to successfully yield the right prediction \"evaluation\". This shows that our model can refer to the structural information in the context to help make better predictions of relation labels."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we incorporate contextualized representations to our Sent-First general design of the model to dynamically capture different information required for discourse analysis on intra- and intersentential levels. We raise the awareness of taking advantage of writing patterns in discourse parsing and contrive a paradigm shift from direct classification to sequence labeling for discourse relation identification. We come up with a stacked biLSTM architecture to exploit its hierarchical design to capture structural information occurring at both intra- and inter-sentential levels. Future work will involve making better use of the structural information instead of applying simple sequence labeling."
    }, {
      "heading" : "A Proof of Theorems",
      "text" : "Theorem 1: For a document D with m sentences (s1, s2, ..., sm) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |si| ≥ 2. Let T be the set of all projective dependency trees obtainable from D,\nand let T ′ be the set of all projective dependency trees obtainable from D in a Sent-First fashion. Then the following inequality holds:\n|T ′| ≤ 2 n+ 1 |T |\nProof of Theorem 1: By the definition of our Sent-First method, trees in T ′ satisfy the property that there is exactly one EDU in each sentence whose head or children lies outside the sentence. It is clear that T ′ ⊂ T . We consider a document D with m sentences (s1, s2, ..., sm) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |si| ≥ 2.\n∀σ′ ∈ T ′, σ′ is a valid projective dependency tree obtainable from D in a Sent-First fashion. We define a t-transformation to a sentence si, |si| > 1 with its local root of the sentence eia not being the root of the document in σ′ with the following rules:\n1. If eia has no child outside si, eib is its furthest (in terms of distance to eia) child or one of its furthest children inside si, then delete the edge between eia) and eib and set the head of eib to be the head of eia.\n2. Else if eia has at least one child before eia inside si, and eib is its furthest child before eia inside si. Delete the edge between eia and eib. If i > 1, set the head of eib to be the local root of sentence si−1, else i = 1, set the head of eib to be the local root of sentence si+1.\n3. Else, eia has at least one child after eia inside si, and eib is its furthest child after eia inside si. Delete the edge between eia) and eib. If i < m, set the head of eib to be the local root of sentence si+1, else i = m, set the head of eib to be the local root of sentence sm−1.\nSuppose σi is obtained by applying ttransformation to the sentence si, it is obvious to show that σi ∈ T/T ′. n−1 valid t-transformations can be applied to σ′. A reverse transformation t−1 can be applied to σi with the following rule: if a sentence has two local roots, change the head of one of the roots to the other root. In this way, at most two possibly valid trees ∈ T ′ can be obtained because we are not sure which one is the original local root of the sentence. Therefore, at most 2 different σ′ ∈ T ′ can be found to share the same tree structure after a t-transformation. See Figure\n5 for illustration. Therefore,\n|T/T ′| ≥ n− 1 2 |T ′|\n|T ′| ≤ 2 n+ 1 |T |\nTheorem 1 shows that the search space shrinks with the number of sentences. Therefore, Sent-First approach is especially effective at the reduction of search space so that the parser has a better chance to find the correct result, no matter what kind of parser is used specifically. Since the effectiveness has been proved, this approach can even be confidently generalized to other cases where similar sentencelike boundaries can be identified.\nBesides, an even stronger bound regarding the use of Sent-First method can also be proved for constituent parsing.\nTheorem 2: For a document D with m > 1 sentences (s1, s2, ..., sm) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |si| ≥ 2. Let T be the set of all binary constituency trees obtainable from D, and let T ′ be the set of all binary constituency trees obtainable from D in a Sent-First fashion. Then the following inequality holds:\n|T ′| ≤ (1 2 )n|T |\nProof of Theorem 2: By the definition of our Sent-First method, trees in T ′ satisfy the property that EDUs in a sentence forms a complete subtree. It is clear that T ′ ⊂ T . We define a tree transformation t, for a tree u1 with child u2 and u3, u3 being a complete discourse tree of a sentence with more than 2 EDUs. u3 must also have 2 children named u4 and u5 where u4 is adjacent to u2 in the sentence. After transformation t, a new tree u′1 is derived whose children are u5 and a subtree u6 with children u2 and u4. u1 ∈ T ′, while u′1 ∈ T/T ′. Illustration see Figure 6. Note that t is one-to-one so that different u1 will be transformed\nto different u′1 after t-transformation and u1 can be applied t-transformation twice if both children of u1 are complete DTs for a sentence (more possible trees u′1 can be transformed into if the order of transformation is also considered). Transformation t is a local transformation and does not affect sub-trees u2, u4, and u5.\n∀σ′ ∈ T ′, σ′ is a valid projective dependency tree obtainable from D in a Sent-First fashion. Since all sub-trees representing a sentence must merge into one complete discourse tree representing the whole document, there must be n independent t transformations applicable to some subtrees in σ′, so that at least 2n − 1 trees can be obtained after i ≥ 1 t transformations ∈ T/T ′. Since t-transformation is one-to-one, ∀σ1, σ2 ∈ T ′, σ1 ̸= σ2, σ′1 is a tree obtained after some ttransformations on σ1, σ′2 is a tree obtained after some t-transformations on σ2, σ′1 ̸= σ′2.\nTherefore,\n|T/T ′| ≥ (2n − 1)|T ′|\n|T ′| ≤ (1 2 )n|T |\nB Additional Detailed Results"
    } ],
    "references" : [ {
      "title" : "Discourse parsing for multiparty chat dialogues",
      "author" : [ "Stergos Afantenos", "Eric Kow", "Nicholas Asher", "Jérémy Perret." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 928–937, Lisbon, Portugal.",
      "citeRegEx" : "Afantenos et al\\.,? 2015",
      "shortCiteRegEx" : "Afantenos et al\\.",
      "year" : 2015
    }, {
      "title" : "Unifying discourse resources with dependency framework",
      "author" : [ "Yi Cheng", "Sujian Li", "Yueyuan Li." ],
      "venue" : "CoRR, abs/2101.00167.",
      "citeRegEx" : "Cheng et al\\.,? 2021",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "A lineartime bottom-up discourse parser with constraints and post-editing",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 511–521,",
      "citeRegEx" : "Feng and Hirst.,? 2014",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2014
    }, {
      "title" : "A complete shift-reduce Chinese discourse parser with robust dynamic oracle",
      "author" : [ "Shyh-Shiun Hung", "Hen-Hsen Huang", "Hsin-Hsi Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 133–138,",
      "citeRegEx" : "Hung et al\\.,? 2020",
      "shortCiteRegEx" : "Hung et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved discourse parsing with two-step neural transition-based model",
      "author" : [ "Yanyan Jia", "Yansong Feng", "Yuan Ye", "Chao Lv", "Chongde Shi", "Dongyan Zhao." ],
      "venue" : "ACM Trans. Asian Low-Resour. Lang. Inf. Process., 17(2).",
      "citeRegEx" : "Jia et al\\.,? 2018a",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2018
    }, {
      "title" : "Modeling discourse cohesion for discourse parsing via memory network",
      "author" : [ "Yanyan Jia", "Yuan Ye", "Yansong Feng", "Yuxuan Lai", "Rui Yan", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
      "citeRegEx" : "Jia et al\\.,? 2018b",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2018
    }, {
      "title" : "A novel discriminative framework for sentence-level discourse analysis",
      "author" : [ "Shafiq Joty", "Giuseppe Carenini", "Raymond Ng." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
      "citeRegEx" : "Joty et al\\.,? 2012",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining intra- and multisentential rhetorical parsing for document-level discourse analysis",
      "author" : [ "Shafiq Joty", "Giuseppe Carenini", "Raymond Ng", "Yashar Mehdad." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Joty et al\\.,? 2013",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2013
    }, {
      "title" : "Topdown rst parsing utilizing granularity levels in documents",
      "author" : [ "Naoki Kobayashi", "Tsutomu Hirao", "Hidetaka Kamigaito", "Manabu Okumura", "Masaaki Nagata." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8099–8106.",
      "citeRegEx" : "Kobayashi et al\\.,? 2020",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2020
    }, {
      "title" : "Text-level discourse dependency parsing",
      "author" : [ "Sujian Li", "Liang Wang", "Ziqiang Cao", "Wenjie Li." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25–35, Baltimore, Maryland.",
      "citeRegEx" : "Li et al\\.,? 2014a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Building Chinese discourse corpus with connective-driven dependency tree structure",
      "author" : [ "Yancui Li", "Wenhe Feng", "Jing Sun", "Fang Kong", "Guodong Zhou." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Li et al\\.,? 2014b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Implicit discourse relation identification for open-domain dialogues",
      "author" : [ "Mingyu Derek Ma", "Kevin Bowden", "Jiaqi Wu", "Wen Cui", "Marilyn Walker." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 666–",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "An efficient algorithm for projective dependency parsing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Proceedings of the Eighth International Conference on Parsing Technologies, pages 149–160, Nancy, France.",
      "citeRegEx" : "Nivre.,? 2003",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2003
    }, {
      "title" : "A deep sequential model for discourse parsing on multi-party dialogues",
      "author" : [ "Zhouxing Shi", "Minlie Huang." ],
      "venue" : "CoRR, abs/1812.00176.",
      "citeRegEx" : "Shi and Huang.,? 2018",
      "shortCiteRegEx" : "Shi and Huang.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "CoRR, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A two-stage parsing method for text-level discourse analysis",
      "author" : [ "Yizhong Wang", "Sujian Li", "Houfeng Wang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 184–188, Vancouver,",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "SciDTB: Discourse dependency TreeBank for scientific abstracts",
      "author" : [ "An Yang", "Sujian Li." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 444–449, Melbourne, Australia. As-",
      "citeRegEx" : "Yang and Li.,? 2018",
      "shortCiteRegEx" : "Yang and Li.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Although existing works have achieved much progress using transition systems (Jia et al., 2018b,a; Hung et al., 2020) or graph-based models (Li et al.",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : ", 2020) or graph-based models (Li et al., 2014a; Shi and Huang, 2018; Afantenos et al., 2015), this task still remains a challenge.",
      "startOffset" : 30,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : ", 2020) or graph-based models (Li et al., 2014a; Shi and Huang, 2018; Afantenos et al., 2015), this task still remains a challenge.",
      "startOffset" : 30,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : ", 2020) or graph-based models (Li et al., 2014a; Shi and Huang, 2018; Afantenos et al., 2015), this task still remains a challenge.",
      "startOffset" : 30,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "Figure 1 shows a discourse dependency structure for a scientific abstract from SciDTB (Yang and Li, 2018).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "A key finding in previous studies in discourse analysis is that most sentences have an independent well-formed sub-tree in the full document-level discourse tree (Joty et al., 2012).",
      "startOffset" : 162,
      "endOffset" : 181
    }, {
      "referenceID" : 9,
      "context" : "Researchers have taken advantage of this finding to build parsers that utilize different granularity levels of the document to achieve the state-of-the-art results (Kobayashi et al., 2020).",
      "startOffset" : 164,
      "endOffset" : 188
    }, {
      "referenceID" : 8,
      "context" : "This design has been empirically verified to be a generally advantageous framework, improving not only works using traditional feature engineering (Joty et al., 2013; Wang et al., 2017),",
      "startOffset" : 147,
      "endOffset" : 185
    }, {
      "referenceID" : 16,
      "context" : "This design has been empirically verified to be a generally advantageous framework, improving not only works using traditional feature engineering (Joty et al., 2013; Wang et al., 2017),",
      "startOffset" : 147,
      "endOffset" : 185
    }, {
      "referenceID" : 16,
      "context" : "We follow previous works (Wang et al., 2017) to cast the task of discourse dependency parsing as a composition of two separate yet related subtasks:",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "Arc-Eager Transition System Since discourse dependency trees are primarily annotated as projective trees (Yang and Li, 2018), we design our tree constructor as a transition system, which converts the structure prediction process into a sequence of predicted actions.",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "EDU Representations To better capture an EDU in our Sent-First framework, we use pre-trained BERT (Devlin et al., 2018) to obtain representations for each EDU according to different context.",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "Although discourse relation identification in discourse dependency parsing is traditionally treated as a classification task, where the common practice is to use feature engineering or neural language models to directly compare two EDUs involved isolated from the rest of the context (Li et al., 2014a; Shi and Huang, 2018; Cheng et al., 2021), sometimes relations between EDU pairs can be hard to be classified in isolation, as global information from the context like how EDUs are organized to support the claim in the discourse is sometimes required to infer the implicit discourse relations without explicit connectives.",
      "startOffset" : 284,
      "endOffset" : 343
    }, {
      "referenceID" : 14,
      "context" : "Although discourse relation identification in discourse dependency parsing is traditionally treated as a classification task, where the common practice is to use feature engineering or neural language models to directly compare two EDUs involved isolated from the rest of the context (Li et al., 2014a; Shi and Huang, 2018; Cheng et al., 2021), sometimes relations between EDU pairs can be hard to be classified in isolation, as global information from the context like how EDUs are organized to support the claim in the discourse is sometimes required to infer the implicit discourse relations without explicit connectives.",
      "startOffset" : 284,
      "endOffset" : 343
    }, {
      "referenceID" : 1,
      "context" : "Although discourse relation identification in discourse dependency parsing is traditionally treated as a classification task, where the common practice is to use feature engineering or neural language models to directly compare two EDUs involved isolated from the rest of the context (Li et al., 2014a; Shi and Huang, 2018; Cheng et al., 2021), sometimes relations between EDU pairs can be hard to be classified in isolation, as global information from the context like how EDUs are organized to support the claim in the discourse is sometimes required to infer the implicit discourse relations without explicit connectives.",
      "startOffset" : 284,
      "endOffset" : 343
    }, {
      "referenceID" : 15,
      "context" : "Specifically, we use cosine and sine functions of different frequencies (Vaswani et al., 2017) to include position information as:",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : ", Discourse Dependency Treebank for Scientific Abstracts (SciDTB) (Yang and Li, 2018) in English and Chinese Discourse Treebank (CDTB) (Li et al.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : ", Discourse Dependency Treebank for Scientific Abstracts (SciDTB) (Yang and Li, 2018) in English and Chinese Discourse Treebank (CDTB) (Li et al., 2014b).",
      "startOffset" : 135,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "(1) Graph adopts the Eisner’s algorithm to predict the most probable dependency tree structure (Li et al., 2014a; Yang and Li, 2018; Cheng et al., 2021).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "(1) Graph adopts the Eisner’s algorithm to predict the most probable dependency tree structure (Li et al., 2014a; Yang and Li, 2018; Cheng et al., 2021).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "(1) Graph adopts the Eisner’s algorithm to predict the most probable dependency tree structure (Li et al., 2014a; Yang and Li, 2018; Cheng et al., 2021).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "(2) Two-stage, which is the stateof-the-art model on CDTB and SciDTB, uses an SVM to construct a dependency tree (Yang and Li, 2018; Cheng et al., 2021).",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "(2) Two-stage, which is the stateof-the-art model on CDTB and SciDTB, uses an SVM to construct a dependency tree (Yang and Li, 2018; Cheng et al., 2021).",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "(3) Sent-First LSTM is our implmentation of the state-of-theart transition-based discourse constituent parser on RST (Kobayashi et al., 2020), where we use a vanilla transition system with pretrained BiLSTM as the EDU encoder within the Sent-First framework to construct dependency trees.",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "(4) Complete Parser is modified from the best constituent discourse parser on CDTB (Hung et al., 2020), using a transition system with BERT as the EDU encoder to construct a dependency tree.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "Relation Identification (1) Graph uses an averaged perceptron to classify relations by direct classification (Cheng et al., 2021; Yang and Li, 2018).",
      "startOffset" : 109,
      "endOffset" : 148
    }, {
      "referenceID" : 17,
      "context" : "Relation Identification (1) Graph uses an averaged perceptron to classify relations by direct classification (Cheng et al., 2021; Yang and Li, 2018).",
      "startOffset" : 109,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "(2) Two-stage exploits careful feature engineering and trains an SVM to classify the relations for pairs of EDUs (Cheng et al., 2021; Yang and Li, 2018).",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "(2) Two-stage exploits careful feature engineering and trains an SVM to classify the relations for pairs of EDUs (Cheng et al., 2021; Yang and Li, 2018).",
      "startOffset" : 113,
      "endOffset" : 152
    } ],
    "year" : 0,
    "abstractText" : "Recent works show that discourse analysis benefits from modeling intraand inter-sentential levels separately, where proper representations for text units of different granularities are desired to capture both the meaning of text units and their relation to the context. In this paper, we propose to take advantage of transformers to encode contextualized representations to dynamically capture the information required for discourse dependency analysis on intraand inter-sentential levels. Motivated by the observation of writing patterns shared across articles, we propose to design sequence labeling methods to take advantage of such structural information from the context, which substantially outperforms traditional direct classification methods. Experiments show that our model achieves state-of-the-art results on both English and Chinese datasets.",
    "creator" : null
  }
}
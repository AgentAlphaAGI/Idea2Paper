{
  "name" : "ARR_2022_186_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning models have achieved great performance on many NLP problems (Devlin et al., 2019; Bahdanau et al., 2016). However, many recent works have shown that these models often rely on spurious correlations which are not necessarily the causal artifacts. Thus, these models that perform well on the in-distribution test set will be very likely to exhibit a huge performance decline on real world data (Tu et al., 2020; Kaushik and Lipton, 2018; Poliak et al., 2018; Gururangan et al., 2018; Zhang et al., 2019; Glockner et al., 2018). Adversarial or challenging datasets have been constructed for the purpose of benchmarking the performance of stateof-the-art models on out-of-distribution examples (Kaushik et al., 2020; Zhang et al., 2019; Glockner et al., 2018). Some of the adversarial examples\nused in prior work include random word swapping (Jin et al., 2020) and appending a sentence to the end of text (Jia and Liang, 2017), but do not take into consideration the unique linguistic properties and variations associated with name entities. The adversarial examples that do involve high linguistic quality augmentations are often human annotated such as in (Kaushik et al., 2020).\nWhile previous works have constructed challenging datasets for many NLP tasks, NER as a key problem setting involving the classification of the semantic categories of entities (e.g., Organizations, Locations) (Nadeau and Sekine, 2007) is still in need of improved benchmarks of better generalization. Previous works (Bernier-Colborne and Langlais, 2020; Fu et al., 2020; Stanislawek et al., 2019) have shown that overlapping or ambiguous categories or label shift where a word or phrase can have different entity labels in different scenarios often led to errors of NER models. This can be especially problematic in specific domain applications where such challenging case is common. For example, when training an NER model for political text mining, it would be of great importance to differentiate the categories of Clinton (Person) and the Clinton Foundation (Organization). We make use of this as the inspiration for designing expertguided heuristic linguistic patterns for creating a high quality adversarial dataset for NER.\nLeveraging such expert-guided heuristics, we manually construct a challenging dataset, on which the performance of state-of-the-art NER systems drops severely. Such expert-guided heuristics can also be used as one part of the training data. However, obtaining large-scale expert-guided heuristics often requires significant amount of time, cost and expertise. Thus, we propose to perform limited data learning on top of our expert-guided heuristics, which also mirrors the real world as we often do not have access to large high quality labeled data. Specifically, we examine whether training addition-\nally with limited expert-informed augmented data can improve model’s generalization performance. We also utilized a widely used technique mixup (Zhang et al., 2018; Chen et al., 2020), which has been proven to be effective at improving generalization (Verma et al., 2019), to interpolate text in the hidden space, allowing for better use of the adversarial and out-of-domain training data provided.\nTo sum up, our contributions are: 1) We introduce a challenging dataset consisting of 1000 high quality examples manually constructed using a variety of different expert-guided heuristics for the facilitation of benchmarking out-of-distribution performance of NER models; 2) We show that stateof-the-art NER models like BERT show dramatic performance drop on our challenging dataset; 3) We further demonstrate that using limited expertguided heuristic augmented data along with mixup enhances the generalization ability of NER models consistently on the in-distribution, challenge set, and out-of-domain settings."
    }, {
      "heading" : "2 Related Work",
      "text" : "Adversarial GLUE (Wang et al., 2021) employed the use of expert-annotated templates for constructing adversarial data, However, it does not include NER as a task. A few previous works have explored the generation of adversarial examples specifically for the NER task. For instance, Gui et al. (2021) performed augmentations by concatenating sentences, swapping/inserting/deleting a random character in an entity, entity swapping with Out-ofVocabulary entities, and cross category swapping. Similarly, Zeng et al. (2020) also took random entity swap approach but only selected entities of the same label to preserve linguistic correctness. In our work, we purposely alter the entity and add surrounding context with the goal of changing the label to target the NER hard cases.\nIn terms of techniques to improve generalization, we take a closer look at mixup, proposed in Zhang et al. (2018). Mixup involves linear interpolation of data points to create a virtual vicinity distribution around the original data space, which improves the classifier’s generalization ability when trained on these interpolated data points. Several previous studies (Thulasidasan et al., 2020; Verma et al., 2019) showed how using mixup allows neural models to be less confident during distribution shifts than models not using mixup. It also leads to a smoother decision-making surface than Empirical\nRisk Minimization (ERM), thereby making models more robust to adversarial examples (Liang et al., 2018). In this work, we use mixup to further improve the generalization ability of NER models when training on our expert-informed augmented data."
    }, {
      "heading" : "3 Expert-Guided Adversary Generation",
      "text" : "Current NER models often deal with unambiguous cases where one entity often gets assigned to the same label. By inducing challenging cases using the Overlapping Categories (Fu et al., 2020) that alter the entity and its label, models can then be tested to see whether they are only learning spurious correlations between the token and the label.\nCriteria for Augmentation Eligibility Table 1 contains the transition rules for creating guidedadversarial examples. It shows how an entity eligible for the heuristic augmentation will be transformed depending on the original type of the entity and the transition rule it was chosen to undertake. Eligibility is determined by the requirements of the type that can be transitioned to (See Appendix A.2).\nExpert-Guided Adversarial Augmentation These expert-guided augmentations involve changes to the entities and their surrounding context while leaving the majority of the original sentence’s semantic meaning unchanged. For the expert-guided heuristic augmentations, we assembled word “phrase” sets for each heuristic pattern type. We only insert words that are already in the CoNLL 2003 training set. An example can be seen in Table 1: when the word “University” is inserted after “Brazil”, the entity transitions from a LOCATION to an ORGANIZATION while the semantics of the rest of the sentence are not changed. An example for the transition to a PERSON type can also be seen in Table 1. When a random word phrase is selected from the set of possible last names, “Zardari” is selected, inserting it changes “Colts” from an ORGANIZATION to a PERSON Type. “and her team” is inserted after the entity to maintain semantic meaning."
    }, {
      "heading" : "3.1 Mixup with Adversarial Examples",
      "text" : "One easy way to improve model’s performance on adversarial examples is to directly train on these examples. Beyond that, we can further improve generalization by using mixup (Zhang et al., 2018)\nto linearly interpolate the original training examples with the guided-adversarial examples in the hidden space. This kind of interpolation can lead to smoother decision boundaries, thus improving generalization (Verma et al., 2019). Details on our implementation of mixup is provided in Appendix B. A beta distribution’s skew is determined by its two shape hyperparameters α and β. The mixing parameter λ is sampled from a beta distribution: λ ∼ B(α, β). In order to better generate virtual samples that are closer to out-of-distribution samples, we use two different beta distributions from which to sample λ. One beta distribution for when the original examples are to be mixed with their heuristically augmented versions, and another for when those heuristically augmented examples are to be mixed with their original counterparts."
    }, {
      "heading" : "4 Challenging Set and Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Pre-processing",
      "text" : "For the in-distribution (ID) dataset we use the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) with the BIO labeling scheme following Chen et al. (2020). In order to make mixup possible in recent transformer based models like BERT, we assigned labels to special tokens [SEP], [CLS], and [PAD]. For Challenge Set (CS), two graduate students who have linguistic background and are familiar with NER tasks, manually constructed the dataset consisting of the original test set transformed by our expert-guided augmentations. They manually annotated 1000 high quality data points ensuring their semantic and syntactic quality. Before moving onto the annotation and cleaning of this CS, they did a test annotation of a sample size of 50 examples to calculate the annotator agreement and the resulting annotator agreement was 78%. During the annotation and cleaning of this test set, we made small corrections to ensure gram-\nmatical correctness. We exclude 25% of the word phrases that are used in the construction of the CS from the word phrase sets used in the constructing of the adversarial training examples. For out-ofdomain (OOD) dataset, we use examples from the OntoNotes test set (Ralph Weischedel and Xue., 2011), see Appendix D for details."
    }, {
      "heading" : "4.2 Baselines and Model Settings",
      "text" : "We train six types of models: (1) a BERT Base (Devlin et al., 2019) model on only the original training examples (BERT ); (2) a BERT Base model on the regular training examples and training examples that are augmented with the expert-guided adversarial heuristics (BERT+AT ); (3) a BERT+AT model augmented with the dropout probability at 0.5 (Hinton et al., 2012) (BERT + AT + Dropout); (4) a BERT Base model utilizing Token-Aware Virtual Adversarial Training (TAVAT) (Li and Qiu, 2020), a gradient-based adversarial training technique (BERT + TAVAT ); (5) a BERT Base model trained with the text-based adversarial attacks proposed in (Gui et al., 2021) utilizing four of their defined NER task specific transformations (Appendix E) (BERT + TextFlint); (6) a BERT Base model utilizing mixup to linearly interpolate the original training examples with the guided-adversarial examples (BERT + AT + Mixup).\nIn order to allow for all non-mixup models utilizing text-based adversarial training to be trained on the same number of datapoints as BERT+AT+Mixup: Let the number of eligible examples be n and the number of ineligible examples be m. If p = 100% of the eligible examples, the number of data points to be trained on equals 2 ∗ n + m. We tried different values of p: 10%, 30%, 50%, and 100%. 5-shot training is used to provide limited OOD data during training to then be evaluated on the OOD test set. The support set is created by sampling 5 examples containing each\nclass as in (Huang et al., 2020). The augmented version of each example in the support set are included in the training for the models that utilize text-based adversarial training."
    }, {
      "heading" : "4.3 Results and Analysis",
      "text" : "As shown in Table 2, BERT had a significant performance decline when tested on the CS, demonstrating the effect of the proposed challenging set. While training on TextFlint examples increases OOD performance, it causes a severe degradation to BERT’s performance on ID and CS, which could be due to the noise injected by random swapping of entities. However, BERT+AT can increase generalization ability on OOD with limited effect on ID performance, and also boost performance on CS by a large margin. Further, BERT+AT+Mixup outperforms other baselines in all but one % category on both ID and CS and on 3 % categories on OOD. This is because when the guided-adversarial examples are used in combination with mixup, the result is interpolated data which is of higher quality from which to train a model, resulting in increased generalization abilities.\nTwo supporting evidence can be looked to in order to conclude that BERT+AT+Mixup’s outof-distribution outperformance is attributed to the combination of mixup with guided-adversarial examples. First, BERT+AT ’s underperforms\nBERT+AT+Mixup on all % categories on CS and OOD, with the OOD performance gap significantly larger. The mixing of the ID data with their guided-adversarial versions and the mixing of the limited OOD data with their guided-adversarial OOD versions led to these results. Second, we observed that random mixing of original examples with other original examples results in the CS performance significantly lower than when utilizing guided-adversarial mixing (see Figure 1 in Appendix). This proves that BERT+AT+Mixup’s out-of-distribution performance is due to the mixing of the original and guided-adversarial examples and not mixup exclusively."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This work introduced a high quality challenging set1 for NER tasks created using expertguided heuristics to facilitate the testing of outof-distribution performance of NER systems. We demonstrated how SOTA NER systems drop performance dramatically when evaluated on our challenge set. Furthermore, we demonstrate that leveraging mixup on our expert-informed examples can lead to better performances over state-of-the-art baselines on in-distribution data, challenging set and out-of-domain settings.\n1The dataset will be made public later."
    }, {
      "heading" : "A Expert-Guided Adversary Generation",
      "text" : "A.1 Adversarial Properties\nWhen the expert-guided augmentation is applied to the example, the entity’s new label is now the ground truth label. If the model classifies based upon the spurious correlation between the remnants of the original entity and context with the original label within the newly augmented text, it will be provoking the wrong classification by the prediction of the old label. This demonstrates the augmented example’s adversarial properties.\nA.2 Possible Augmented Entities Allowed\nFor transitioning to Organization and Person, an example is considered eligible for the heuristic augmentation if an entity contains within it only a single token. The augmented entity for the defined transitions do not have to exist in the real world. For example, for the transition to a Person type from a Location, there are only a couple locations that people accept as mainstream names such as Austin (in Texas US), Charlotte (in North Carolina US), and Brooklyn (borough of New York US). So\nwe do not find importance for this work to involve the restricting of possible Person names to this subset. Also for the example of transitioning to Organization, we do not restrict the possible company names to be existing companies. The transition to the Location label is different in the sense that we restrict the set of possible augmented entities to only contain entities that are real world locations.\nA.3 Transition to Location Type\nIn order to ensure the augmented entity of type LOCATION is a real world location, identify what word phrase does a documented case of an ORGANIZATION entity contain that also incorporates a LOCATION entity inside of it. We then delete this word phrase and leave the LOCATION entity by itself. After the entity augmentation, we then insert a random word phrase from the word phrase set corresponding to this transition to maintain the semantic quality of the sentence after the augmentation. As can be seen in Table 1, “Re” is removed from the sentence leaving the token “Munich” by itself. “’s largest corporation” is inserted after the augmented entity to maintain semantic quality."
    }, {
      "heading" : "B Mixup Details",
      "text" : "B.1 Background and Implementation\nThe mixup data augmentation technique was proposed in (Zhang et al., 2018). It was demonstrated by training an image classifier on linear interpolations of randomly sampled image data. Given a pair of data points (x,y) and (x’,y’), where x denotes an image in the raw pixel space, and y denotes the label in a one-hot representation. The mixup technique then creates a new data point by the interpolation of the images and their labels as shown in below:\nx̂ = λx+ (1− λ)x (1)\nŷ = λy + (1− λ)y (2)\nλ is drawn from a Beta distribution. The mixup technique trains the neural network for image classification by the minimization of the loss on the virtual examples. The linear interpolation of the data points creates a virtual vicinity distribution around the original data space causing the improvement of the classifier’s generalization performance when being trained on the interpolated data points.\nOur implementation of mixup is based on (Chen et al., 2020). Let (x, y) be a training example that is eligible for heuristic augmentation and is\npaired with its heuristically modified version (x’, y’). Then the interpolations of these two examples are computed in the hidden space using a L-layer encoder.\nLet hl = {h1..hn} be the hidden representations at the l-th layer where it is the concatenation of the token representations. The hidden representations for each token in the original example at the m-th layer (m is randomly sampled from 8,9,10 ) are linearly interpolated with each token in the augmented example by a ratio λ. The mixing parameter λ is sampled from the Beta distribution. See Appendix B.2 for more details on λ.\nh̃l = λhm + (1− λ)h′m (3)\nThe representation resulting from the interpolation is then fed to the upper layers of the L-layer encoder. Their corresponding labels are mixed at the same ratio. The resulting hidden representations from the L-layer encoder are fed into a classifier and the loss over all positions is minimized to train the model.\nB.2 Tuning of Hyperparameters\nWe use two different beta distributions to sample the mixing parameter from, one for when the regular examples are to be mixed and one for when the heuristically augmented examples are to be mixed. For the two shape hyperparameters corresponding to each of the two beta distributions from which the mixing parameter is sampled, α and β, we first set at 200 and 5 respectively. We experimented with lessening the skew of the beta distribution decreasing α to 150 and while keeping β at 5. We then further experimented with increasing its skew by decreasing α to 130 and while at the same time increasing β to values of 7 and 9.\nWhen using 5-shot training, our implementation of mixup uses four different beta distributions from which to sample the mixing parameter: one beta distribution for when the in-distribution examples are to be mixed with their heuristically augmented versions, and another for when those heuristically augmented examples are to be mixed with their in-distribution counterparts. There is also a beta distribution for when the out-of-domain examples are to be mixed with their heuristically augmented versions and another for when those heuristically augmented examples are to be mixed with their outof-domain counterparts. The values of the shape parameters used in the tuning of these beta distri-\nbutions are similar to the ones used during training without 5-shot."
    }, {
      "heading" : "C Adversarial Training Set Generation Code",
      "text" : "The guided-adversarial training examples used in the experiments are generated using code with the expert-guided heuristic word “phrase” sets given as input. The code proposed in this work can also be easily repurposed for other NER datasets due to its linguistic universality with the expert-guided heuristic word “phrase” sets for other domains being given as input to the code."
    }, {
      "heading" : "D Out-of-Domain Dataset",
      "text" : "For the out-of-domain (OOD) test set, we use the first 50 examples from the OntoNotes test set (Ralph Weischedel and Xue., 2011) that have a percentage of entity tokens out of all tokens that is greater than 49%. OntoNotes has a more finegrained entity category than CoNLL 2003, so we mapped the OntoNotes labels to the CoNLL 2003 labels so the data would be compatible with our models."
    }, {
      "heading" : "E TextFlint NER Task Specific Transformations",
      "text" : "The four TextFlint NER task specific transformations used are ConcatSent, EntTypos, CrossCategory, and SwapLonger. ConcatSent involves the concatenation of two sentences into a longer one. EntTypos involves the swapping/deleting/adding of a random character to entities. CrossCategory involves the swapping of entities with ones that\ncan be labeled by different labels. SwapLonger involves the substituting of shorter entities for longer ones. Since only ConcatSent and EntTypos were available through the TextFlint framework during the time of this work, we reimplemented CrossCategory and SwapLonger for the experiments."
    }, {
      "heading" : "F No Word Phrases Held Out Experiments",
      "text" : "When 100% of the word phrases are used during adversarial training, the model may have learned the spurious correlation between the words used in the word phrases and the entity labels instead of learning the linguistic augmentation which resulted in a high performance on the Challenge Set. When only 25% of the word phrases were held out for the test set, the models experienced a significant drop in performance on the 30%, 50%, and 100% categories. This can be observed by comparing the Challenge Set results shown in Table 3 with Table 2. This demonstrates that even though BERT’s performance increases when trained on the expertguided augmented data, the Challenge Set is still not \"solved\" as the removal of 25% of the word phrases from training has caused this significant of a performance drop."
    }, {
      "heading" : "G Tuning of Token-Aware Virtual Adversarial Training’s(TAVAT) Hyperparameters",
      "text" : "The hyperparameters unique to TAVAT such as the adversarial training step, the constraint bound of the pertubation, the adversarial step size, and the initialization bound are tuned using the values in\n(Li and Qiu, 2020).\nH Information for Reproducibility Checklist:\nSpecification of all dependencies: boto3 1.16.60 botocore 1.19.60 certifi 2020.12.5 chardet 4.0.0 click 7.1.2 cycler 0.10.0 filelock 3.0.12 idna 2.10 importlib-metadata 3.10.0 jmespath 0.10.0 joblib 1.0.1 kiwisolver 1.3.1 matplotlib 3.3.4 mkl-fft 1.3.0 mkl-random 1.1.1 mkl-service 2.3.0 numpy 1.19.2 olefile 0.46 packaging 20.9 pandas 1.2.2 Pillow 8.2.0 pip 21.0.1 protobuf 3.14.0 pyparsing 2.4.7 python-dateutil 2.8.1 pytz 2021.1 regex 2020.11.13 requests 2.25.1 s3transfer 0.3.4\nsacremoses 0.0.43 sentencepiece 0.1.95 setuptools 52.0.0.post20210125 six 1.15.0 tensorboardX 2.1 tokenizers 0.7.0 torch 1.4.0 tornado 6.1 tqdm 4.56.0 transformers 2.9.0 typing-extensions 3.7.4.3 urllib3 1.26.3 virtualenv-clone 0.5.4 wheel 0.36.2 xlrd 1.2.0 XlsxWriter 1.3.7 zipp 3.4.1\nDescription of computing infrastructure used:\nGEFORCE RTX 2080 CUDA Version: 11.0\nThe average runtime for each model or algorithm (e.g., training, inference, etc.): Training 2 to 2 and 1/2 hours. Inference: 3 minutes or less\nNumber of parameters in each model: BERT: 110 million parameters\nMixup 10 Percent Augmented data, Beta distribution hyperparameters: Regular examples: alpha=130 beta=9 Augmented examples: alpha=200 beta=5\nMixup 30 Percent Augmented data, Beta distribution hyperparameters: Regular examples: alpha=150 beta=5 Augmented examples: alpha=200 beta=5\nMixup 50 Percent Augmented data, Beta distribution hyperparameters: Regular examples: alpha=130 beta=7 Augmented examples: alpha=200 beta=5\nMixup 100 Percent Augmented data, Beta\ndistribution hyperparameters: Regular examples: alpha=150 beta=5 Augmented examples: alpha=200 beta=5\n5-Shot Experiments:\nMixup 10 Percent Augmented data, Beta distribution hyperparameters: Regular examples: alpha=150 beta=5 Augmented examples: alpha=200 beta=5 | Regular OOD examples: alpha=200 beta=5 Augmented OOD examples: alpha=130 beta=7\nMixup 30 Percent Augmented data, Beta distribution hyperparameters: Regular examples: alpha=200 beta=5 Augmented examples: alpha=150 beta=5 | Regular OOD examples: alpha=200 beta=5 Augmented OOD examples: alpha=130 beta=7\nMixup 50 Percent Augmented data, Beta distribution hyperparameters: Regular examples: alpha=150 beta=5 Augmented examples: alpha=200 beta=5 | Regular OOD examples: alpha=200 beta=5 Augmented OOD examples: alpha=130 beta=7\nMixup 100 Percent Augmented data, Beta distribution hyperparameters: Regular examples: alpha=130 beta=5 Augmented examples: alpha=200 beta=5 | Regular OOD examples: alpha=200 beta=5 Augmented OOD examples: alpha=130 beta=7\nTAVAT Model adv init mag=0.2, adv lr=0.05, adv max norm=0.5, adv steps=2, adv train=1\nTAVAT Model, 5-Shot Training adv init mag=0.2, adv lr=0.05, adv max norm=0.5, adv steps=2, adv train=1\nExplanation of evaluation metrics used, with links to code: F1 score, code included in submission\nThe exact number of training and evaluation runs: 3 Training Runs for each shown model\nperformance. Evaluation is performed throughout training and at the end of each training.\nBounds for each hyperparameter: Beta distribution shape parameters: alpha>0, beta>0\nHyperparameter configurations for best-performing models:\nMixup and TAVAT hyperparameters shown above.\nAll dropout models, dropout=0.5\nAll models:\nmax sequence length 256, batch size 8, number of training epochs 10, adam epsilon=1e-08, learning rate=5e-05, weight decay=0.0\nThe method of choosing hyperparameter values (e.g., uniform sampling, manual tuning, etc.) and the criterion used to select among them (e.g., accuracy) (*):\nManual Tuning, F1 score\nNumber of hyperparameter search trials:\nRoughly 10 trials of different beta distribution hyperparameters during manual tuning for all mixup models. Once best hyperparameters were selected, roughly 4 trials during manual tuning for each percentage category mixup model.\nRelevant details such as languages, and number of examples and label distributions:\nCoNLL 2003 Language: English\nTraining set for CoNLL 2003: Number of examples: 14041\nDev set for CoNLL 2003: Number of examples: 3250\nTest set for CoNLL 2003: Number of examples: 3453\nThe Challenging Set’s examples are annotated with a 1 for high quality and a 0 or 2 for low\nquality. The code only reads in examples annotated with a 1 from the file."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "HardEval: Focusing on challenging tokens to assess robustness of NER",
      "author" : [ "Gabriel Bernier-Colborne", "Phillippe Langlais." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 1704–1711, Marseille, France. European Language",
      "citeRegEx" : "Bernier.Colborne and Langlais.,? 2020",
      "shortCiteRegEx" : "Bernier.Colborne and Langlais.",
      "year" : 2020
    }, {
      "title" : "Local additivity based data augmentation for semi-supervised ner",
      "author" : [ "Jiaao Chen", "Zhenghui Wang", "Ran Tian", "Zichao Yang", "Diyi Yang" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking generalization of neural models: A named entity recognition case study",
      "author" : [ "Jinlan Fu", "Pengfei Liu", "Qi Zhang", "Xuanjing Huang" ],
      "venue" : null,
      "citeRegEx" : "Fu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Breaking nli systems with sentences that require simple lexical inferences",
      "author" : [ "Max Glockner", "Vered Shwartz", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Glockner et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Glockner et al\\.",
      "year" : 2018
    }, {
      "title" : "Textflint: Unified multilingual robustness evaluation toolkit",
      "author" : [ "Yuan Hu", "Qiyuan Bian", "Zhihua Liu", "Bolin Zhu", "Shan Qin", "Xiaoyu Xing", "Jinlan Fu", "Yue Zhang", "Minlong Peng", "Xiaoqing Zheng", "Yaqian Zhou", "Zhongyu Wei", "Xipeng Qiu", "Xuanjing Huang" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2021
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Fewshot named entity recognition: A comprehensive study",
      "author" : [ "Jiaxin Huang", "Chunyuan Li", "Krishan Subudhi", "Damien Jose", "Shobana Balakrishnan", "Weizhu Chen", "Baolin Peng", "Jianfeng Gao", "Jiawei Han" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "Jia and Liang.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits" ],
      "venue" : null,
      "citeRegEx" : "Jin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning the difference that makes a difference with counterfactually-augmented data",
      "author" : [ "Divyansh Kaushik", "Eduard Hovy", "Zachary C. Lipton" ],
      "venue" : null,
      "citeRegEx" : "Kaushik et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2020
    }, {
      "title" : "How much reading does reading comprehension require? a critical investigation of popular benchmarks",
      "author" : [ "Divyansh Kaushik", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Kaushik and Lipton.,? 2018",
      "shortCiteRegEx" : "Kaushik and Lipton.",
      "year" : 2018
    }, {
      "title" : "Tavat: Token-aware virtual adversarial training for language understanding",
      "author" : [ "Linyang Li", "Xipeng Qiu" ],
      "venue" : null,
      "citeRegEx" : "Li and Qiu.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li and Qiu.",
      "year" : 2020
    }, {
      "title" : "Understanding mixup training methods",
      "author" : [ "Daojun Liang", "Feng Yang", "Tian Zhang", "Peter Yang." ],
      "venue" : "IEEE Access, 6:58774–58783.",
      "citeRegEx" : "Liang et al\\.,? 2018",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2018
    }, {
      "title" : "A survey of named entity recognition and classification",
      "author" : [ "David Nadeau", "Satoshi Sekine." ],
      "venue" : "Lingvisticæ Investigationes, 30(1):3–26.",
      "citeRegEx" : "Nadeau and Sekine.,? 2007",
      "shortCiteRegEx" : "Nadeau and Sekine.",
      "year" : 2007
    }, {
      "title" : "Hypothesis only baselines in natural language inference",
      "author" : [ "Adam Poliak", "Jason Naradowsky", "Aparajita Haldar", "Rachel Rudinger", "Benjamin Van Durme" ],
      "venue" : null,
      "citeRegEx" : "Poliak et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Poliak et al\\.",
      "year" : 2018
    }, {
      "title" : "Ontonotes: A large training corpus for enhanced processing",
      "author" : [ "Mitchell Marcus Martha Palmer Robert Belvin Sameer Pradhan Lance Ramshaw Ralph Weischedel", "Eduard Hovy", "Nianwen Xue." ],
      "venue" : "Handbook of Natural Language Processing and Ma-",
      "citeRegEx" : "Weischedel et al\\.,? 2011",
      "shortCiteRegEx" : "Weischedel et al\\.",
      "year" : 2011
    }, {
      "title" : "Named entity recognition - is there a glass ceiling",
      "author" : [ "Tomasz Stanislawek", "Anna Wróblewska", "Alicja Wójcicka", "Daniel Ziembicki", "Przemyslaw Biecek" ],
      "venue" : "In Proceedings of the 23rd Conference on Computational Natural Language Learning",
      "citeRegEx" : "Stanislawek et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Stanislawek et al\\.",
      "year" : 2019
    }, {
      "title" : "On mixup training: Improved calibration and predictive uncertainty for deep neural networks",
      "author" : [ "Sunil Thulasidasan", "Gopinath Chennupati", "Jeff Bilmes", "Tanmoy Bhattacharya", "Sarah Michalak" ],
      "venue" : null,
      "citeRegEx" : "Thulasidasan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Thulasidasan et al\\.",
      "year" : 2020
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "An empirical study on robustness to spurious correlations using pre-trained language models",
      "author" : [ "Lifu Tu", "Garima Lalwani", "Spandana Gella", "He He." ],
      "venue" : "CoRR, abs/2007.06778.",
      "citeRegEx" : "Tu et al\\.,? 2020",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2020
    }, {
      "title" : "Manifold mixup: Better representations by interpolating hidden states",
      "author" : [ "Vikas Verma", "Alex Lamb", "Christopher Beckham", "Amir Najafi", "Ioannis Mitliagkas", "Aaron Courville", "David Lopez-Paz", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Verma et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Verma et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial GLUE: A multi-task benchmark for robustness evaluation of language models",
      "author" : [ "Boxin Wang", "Chejian Xu", "Shuohang Wang", "Zhe Gan", "Yu Cheng", "Jianfeng Gao", "Ahmed Hassan Awadallah", "Bo Li." ],
      "venue" : "Thirty-fifth Conference on Neural In-",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Counterfactual generator: A weaklysupervised method for named entity recognition",
      "author" : [ "Xiangji Zeng", "Yunliang Li", "Yuchen Zhai", "Yin Zhang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Zeng et al\\.,? 2020",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2020
    }, {
      "title" : "mixup: Beyond empirical risk minimization",
      "author" : [ "Hongyi Zhang", "Moustapha Cisse", "Yann N. Dauphin", "David Lopez-Paz" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "PAWS: paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He." ],
      "venue" : "CoRR, abs/1904.01130.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Information for Reproducibility Checklist: Specification of all dependencies: boto3",
      "author" : [ "Li", "Qiu", "2020). H" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Deep learning models have achieved great performance on many NLP problems (Devlin et al., 2019; Bahdanau et al., 2016).",
      "startOffset" : 74,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "Deep learning models have achieved great performance on many NLP problems (Devlin et al., 2019; Bahdanau et al., 2016).",
      "startOffset" : 74,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "Thus, these models that perform well on the in-distribution test set will be very likely to exhibit a huge performance decline on real world data (Tu et al., 2020; Kaushik and Lipton, 2018; Poliak et al., 2018; Gururangan et al., 2018; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 278
    }, {
      "referenceID" : 13,
      "context" : "Thus, these models that perform well on the in-distribution test set will be very likely to exhibit a huge performance decline on real world data (Tu et al., 2020; Kaushik and Lipton, 2018; Poliak et al., 2018; Gururangan et al., 2018; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 278
    }, {
      "referenceID" : 17,
      "context" : "Thus, these models that perform well on the in-distribution test set will be very likely to exhibit a huge performance decline on real world data (Tu et al., 2020; Kaushik and Lipton, 2018; Poliak et al., 2018; Gururangan et al., 2018; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 278
    }, {
      "referenceID" : 7,
      "context" : "Thus, these models that perform well on the in-distribution test set will be very likely to exhibit a huge performance decline on real world data (Tu et al., 2020; Kaushik and Lipton, 2018; Poliak et al., 2018; Gururangan et al., 2018; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 278
    }, {
      "referenceID" : 27,
      "context" : "Thus, these models that perform well on the in-distribution test set will be very likely to exhibit a huge performance decline on real world data (Tu et al., 2020; Kaushik and Lipton, 2018; Poliak et al., 2018; Gururangan et al., 2018; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 278
    }, {
      "referenceID" : 5,
      "context" : "Thus, these models that perform well on the in-distribution test set will be very likely to exhibit a huge performance decline on real world data (Tu et al., 2020; Kaushik and Lipton, 2018; Poliak et al., 2018; Gururangan et al., 2018; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 278
    }, {
      "referenceID" : 12,
      "context" : "Adversarial or challenging datasets have been constructed for the purpose of benchmarking the performance of stateof-the-art models on out-of-distribution examples (Kaushik et al., 2020; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 164,
      "endOffset" : 229
    }, {
      "referenceID" : 27,
      "context" : "Adversarial or challenging datasets have been constructed for the purpose of benchmarking the performance of stateof-the-art models on out-of-distribution examples (Kaushik et al., 2020; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 164,
      "endOffset" : 229
    }, {
      "referenceID" : 5,
      "context" : "Adversarial or challenging datasets have been constructed for the purpose of benchmarking the performance of stateof-the-art models on out-of-distribution examples (Kaushik et al., 2020; Zhang et al., 2019; Glockner et al., 2018).",
      "startOffset" : 164,
      "endOffset" : 229
    }, {
      "referenceID" : 11,
      "context" : "Some of the adversarial examples used in prior work include random word swapping (Jin et al., 2020) and appending a sentence to the end of text (Jia and Liang, 2017), but do not take into consideration the unique linguistic properties and variations associated with name entities.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : ", 2020) and appending a sentence to the end of text (Jia and Liang, 2017), but do not take into consideration the unique linguistic properties and variations associated with name entities.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "adversarial examples that do involve high linguistic quality augmentations are often human annotated such as in (Kaushik et al., 2020).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : ", Organizations, Locations) (Nadeau and Sekine, 2007) is still in need of improved benchmarks of better gener-",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "Previous works (Bernier-Colborne and Langlais, 2020; Fu et al., 2020; Stanislawek et al., 2019) have shown that overlapping or ambiguous categories or label shift where a word or phrase can have different entity labels in different scenarios",
      "startOffset" : 15,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "Previous works (Bernier-Colborne and Langlais, 2020; Fu et al., 2020; Stanislawek et al., 2019) have shown that overlapping or ambiguous categories or label shift where a word or phrase can have different entity labels in different scenarios",
      "startOffset" : 15,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "Previous works (Bernier-Colborne and Langlais, 2020; Fu et al., 2020; Stanislawek et al., 2019) have shown that overlapping or ambiguous categories or label shift where a word or phrase can have different entity labels in different scenarios",
      "startOffset" : 15,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "We also utilized a widely used technique mixup (Zhang et al., 2018; Chen et al., 2020), which has been proven to be effective at improving generalization (Verma et al.",
      "startOffset" : 47,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "We also utilized a widely used technique mixup (Zhang et al., 2018; Chen et al., 2020), which has been proven to be effective at improving generalization (Verma et al.",
      "startOffset" : 47,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : ", 2020), which has been proven to be effective at improving generalization (Verma et al., 2019), to interpolate text in the hidden space, allowing for better use of the ad-",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 24,
      "context" : "Adversarial GLUE (Wang et al., 2021) employed the use of expert-annotated templates for constructing adversarial data, However, it does not include",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "Several previous studies (Thulasidasan et al., 2020; Verma et al., 2019) showed how using mixup allows neural models to be less confident during distribution shifts than models not using mixup.",
      "startOffset" : 25,
      "endOffset" : 72
    }, {
      "referenceID" : 23,
      "context" : "Several previous studies (Thulasidasan et al., 2020; Verma et al., 2019) showed how using mixup allows neural models to be less confident during distribution shifts than models not using mixup.",
      "startOffset" : 25,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "It also leads to a smoother decision-making surface than Empirical Risk Minimization (ERM), thereby making models more robust to adversarial examples (Liang et al., 2018).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 4,
      "context" : "By inducing challenging cases using the Overlapping Categories (Fu et al., 2020) that alter the entity and its label, models can then be tested to see whether they are only learning spurious correlations between the token and the label.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 26,
      "context" : "Beyond that, we can further improve generalization by using mixup (Zhang et al., 2018)",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "This kind of interpolation can lead to smoother decision boundaries, thus improving generalization (Verma et al., 2019).",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "We train six types of models: (1) a BERT Base (Devlin et al., 2019) model on only the original training examples (BERT ); (2) a BERT Base model on the regular training examples and training ex-",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "5 (Hinton et al., 2012) (BERT + AT + Dropout); (4) a BERT Base model utilizing Token-Aware Vir-",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "tual Adversarial Training (TAVAT) (Li and Qiu, 2020), a gradient-based adversarial training technique (BERT + TAVAT ); (5) a BERT Base model trained with the text-based adversarial attacks proposed in (Gui et al.",
      "startOffset" : 34,
      "endOffset" : 52
    } ],
    "year" : 0,
    "abstractText" : "Named Entity Recognition (NER) systems often demonstrate great performance on indistribution data, but perform poorly on examples drawn from a shifted distribution. One way to evaluate the generalization ability of NER models is to use adversarial examples. Adversarial challenging datasets used in prior work often demonstrate limited linguistic quality. To this end, we propose to leverage expertguided heuristics to manually construct a challenging set for NER. We then provide a comprehensive empirical study on the performance of SOTA models on our challenging test set and an out-of-domain set when trained on a small amount of supervision, as such expert guided annotation might be limited. Experiments show that state-of-the-art NER systems like BERT drop performance dramatically on our challenge set and other out-of-domain set, and leveraging training on limited amounts of examples augmented using our universal linguistic pattern and the regularization technique mixup significantly improved generalization performances.",
    "creator" : null
  }
}
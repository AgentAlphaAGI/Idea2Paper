{
  "name" : "ARR_2022_22_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hyperlink-induced Pre-training for Passage Retrieval of Open-domain Question Answering",
    "authors" : [ "Mitja Okorn" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Open-domain question answering (OpenQA) aims to answer factual open questions with a large external corpus of passages. Current approaches to OpenQA usually adopt a two-stage retriever-reader paradigm (Chen et al., 2017; Zhu et al., 2021) to fetch the final answer span. The performance of OpenQA systems is largely bounded by the retriever as it determines the evidential documents for the reader to examine. Traditional retrievers, such as TF-IDF and BM25 (Robertson and Zaragoza, 2009), are considered incapable of adapting to sce-\nWe will soon release our source code and pre-training corpus on github.\nnarios where deep semantic understanding is required. Recent works (Lee et al., 2019; Karpukhin et al., 2020; Qu et al., 2021) show that by finetuning pre-trained language models on sufficient downstream data, dense retrievers can significantly outperform traditional term-based retrievers.\nConsidering the data-hungry nature of the neural retrieval models, extensive efforts (Lee et al., 2019; Chang et al., 2020; Sachan et al., 2021) have been made to design self-supervised tasks to pre-train the retriever. However, these pre-training tasks construct relevance signals largely depending on easily achieving sentence-level or document-level contextual relationships. For example, the relationship between a sentence and its originated context (shown by the ICT query in Figure 1) may not be sufficient enough to facilitate question-passage matching for the tasks of OpenQA. We also find that these pretrained retrievers still fall far behind BM25 in our\npilot study on the zero-shot experiment. In order to address the shortcomings of the matching-oriented pre-training tasks as mentioned above, we propose a pre-training method with better surrogates of real natural question-passage (QP) pairs. We consider two conditions of relevance within Q-P pairs, which is similar to the process of distantly supervised retriever learning (Mintz et al., 2009; Chen et al., 2017).\n1) Evidence Existence The evidence, such as entities and their corresponding relations, should exist across the query and the targeted passage as they both discuss similar facts or events related to the answer. 2) Answer Containing The golden passage should contain the answer of the query, which means that a text span within the passage can provide the information-seeking target of the query.\nIn this paper, we propose HyperLink-induced Pre-training (HLP), a pre-training method to learn effective Q-P relevance induced by the hyperlink topology within naturally-occurring Web documents. Specifically, these Q-P pairs are automatically extracted from the online documents with relevance adequately designed via hyperlink-based topology to facilitate downstream retrieval for question answering. Figure 1 shows an example of comparison between the human-written query and different pseudo queries. By the guidance of hyperlinks, our HLP query hold the relevance of answer containing with the passage (query title occurs in the passage). Meanwhile, the HLP query can introduce far more effective relevance of evidence existence than other pseudo queries by deeply mining the hyperlink topology, e.g., the dual-link structure. In figure 1, both HLP query and the passage both contain information corresponding to the same fact of “Mitja Okorn directed the film of Letters to Santa”. This makes our pseudo query low-cost and a good surrogate for the manually written query.\nOur contributions are two-fold. First, we present a hyperlink-induced relevance construction methodology that can better facilitate downstream passage retrieval for question answering, and specifically, we propose a pre-training method: Hyperlink-induced Pre-training (HLP). Second, we conduct evaluations on six popular QA datasets, investigating the effectiveness of our approach under zero-shot, few-shot, multi-hop, and out-of-domain\n(OOD) scenarios. The experiments show HLP outperforms BM25 in most of the cases under the zero-shot scenario and other pre-training methods under all scenarios."
    }, {
      "heading" : "2 Related Work",
      "text" : "Dense Retriever Pre-training Previous works have attempted to conduct additional pre-training for dense retrievers on various weakly supervised data. Borisov et al. (2016) and Dehghani et al. (2017) pre-trained ranking models on click-logs and BM25-induced signals respectively for web search. Lee et al. (2019) proposed the inverse cloze task (ICT) to pre-train a dense retrieval model, which randomly selects sentences as pseudo queries, and matched them to the passages that they originate from. Besides, Chang et al. (2020) proposed the pre-training task of wiki link prediction (WLP) and body first selection (BFS) tasks. Similar to our work, the WLP task also leveraged the hyperlinks within Wikipedia to construct relevant text pairs. However, as shown in figure 1, the WLP pseudo query can only ensure the weak doc-wise contextual relationship with the passage. Guu et al. (2020) proposed the masked-salient-span pre-training task which optimizes a retrieval model by the distant supervision of language model objective. As a follow-up, Sachan et al. (2021) combined ICT with the masked-salient-span task and further improved the pre-training effectiveness. Data Augmentation via Question Generation Ma et al. (2021), Reddy et al. (2021) and Oğuz et al. (2021) all investigate training a dense retriever on questions synthesized by large question generative (QG) models. Targeting on the zero-shot setting, Ma et al. (2021) trained a question generator on general-domain question passage pairs from community platforms and publicly available academic datasets. Reddy et al. (2021) focused more on domain transfer and trained the QG model on QA datasets of Wikipedia articles. Oğuz et al. (2021) uses the synthetically generated questions from PAQ datasaset (Lewis et al., 2021) and the post-comment pairs from dataset of Reddit conversations for retrieval pre-training. Recently, Shinoda et al. (2021) reveals that the QG models tend to generate questions with high lexical overlap which amplify the bias of QA dataset. Different to these studies, our method focuses on a more general setting where the retriever is only trained with the naturally occurring web documents, and has no\naccess to any downstream datasets."
    }, {
      "heading" : "3 Hyperlink-induced Pre-training (HLP)",
      "text" : "In this section, we firstly discuss the background of OpenQA retrieval, then our methodology and training framework."
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "Passage Retrieval Given a question q, passage retrieval aims to provide a set of relevant passages p from a large corpus D. Our work adopts Wikipedia as source corpus and each passage is a disjoint segment within a document from D. OpenQA Q-P Relevance For OpenQA, a passage p is considered relevant to the query q if p conveys similar facts and contains the answer to q. These two conditions of relevance, namely evidence existence and answer containing, are properly introduced into the HLP Q-P pairs under the guidance of desired hyperlink structure. We will discuss more in this section.\nTo better formulate the relevance of pseudo Q-P pairs, we denote the sequence of passages within a document as A = [a1, a2, ..., anA ] where A ∈ D. The corresponding topical entity and the title of document A are denoted as eA and tA, respectively. We use mA to indicate a mention of entity eA, which is a hypertext span linking to document A. Note that the mention span mA is usually identical to the document title tA or a variant version of it. Further, we define F(p) as the entity-level factual information conveyed by the passage p, which is a set consists of the topical entity eP and the entities\nmentioned within passage p. Evidence Existence in HLP With appropriately designed hyperlink topologies, our HLP Q-P pairs guarantee the co-occurrence of entities which are presented as hypertext or topics in q and p. This is considered as evidence across the Q-P pairs:\nF(q) ∩ F(p) 6= ∅ (1)\nFurthermore, we conjecture that HLP is more likely to achieve fact-level relevance than entitylevel overlap. We conduct human evaluation in Section 6.3 and case studies in Appendix G to support this conjecture. Moreover, we demonstrate that any Q-P pair containing hyperlink-induced factual evidence, which is represented as a triple that induced by hyperlinks, is included in our proposed topologies, which are included in Appendix D. Answer Containing in HLP We consider the document title tQ as the information-seeking target of q. Accordingly, the relevance of answer containing can be formulated as\ntQ ⊆ p (2)\nThe rationale behind this is that both the natural question and the Wikipedia document are intended to describe related facts and events regarding a targeted object, whereas the object is an answer for a question but a topic for a Wikipedia document. This similarity leads us to take the document title as the information-seeking target of its context."
    }, {
      "heading" : "3.2 Hyperlink-induced Q-P Pairs",
      "text" : "Based on analysis of how queries match their evidential passages in the NQ (Kwiatkowski et al.,\n2019) dataset, we propose two kinds of hyperlink topology for relevance construction: Dual-link and Co-mention. We present our exploratory data analysis on NQ dataset in Appendix C. Here we discuss the desired hyperlink topologies and the corresponding relevance of the pseudo Q-P pairs. Dual-link (DL) Among all NQ training samples, 55% of questions mention the title of their corresponding golden passage. This observation motivates us to leverage the topology of dual-link (DL) for relevance construction. We consider a passage pair (ai, bj) follows the dual-link topology if they link to each other. An example of a DL pair (ai, bj) is shown in Figure 2, in which passage bj mentions the title of document A as mA, satisfying the condition of answer containing:\ntA ≈ mA and mA ⊆ bj (3)\nFurther, since the passages ai and bj both mention the topical entity of the other, the entities eA and eB appear in both passages as evidence:\n{eA, eB} ⊆ F(ai) ∩ F(bj) (4)\nCo-mention (CM) Among all NQ training samples, about 40% of questions fail to match the duallink condition but mention the same third-party entity as their corresponding golden passages. In light of this observation, we utilize another topology of Co-mention (CM). We consider that a passage pair (ck, dl) follows the Co-mention topology if they both link to a third-party document E and dl links to ck. Figure 2 illustrates a CM pair (cl, dk) where answer containing is ensured as the title of ck occurs in dl:\ntC ≈ mC and mC ⊆ dl (5)\nSince both cl and dk mention a third-party entity eE , and that eC is a topical entity in cl while a mentioned entity in dk, we have entity-level evidence across cl and dk as:\n{eC , eE} ⊆ F(ck) ∩ F(dl) (6)\nIn practice, we use sentence-level queries which contain the corresponding evidential hypertext, and we do not prepend the title to the passage in order to reduce the superficial entity-level overlap. To improve the quality of CM pairs, we filter out those with a co-mentioned entity which has a top 10% highest-ranked in-degree among the Wikipedia entity. To illustrate how we construct our pseudo Q-P pairs, we present pseudo code in Appendix E.\nFurthermore, we highlight that HLP has the following advantages: 1) it introduces more semantic variants and paraphrasing for better text matching. 2) The hypertext reflects potential interests or needs of users in relevant information, which is consistent to the downstream information-seeking propose."
    }, {
      "heading" : "3.3 Bi-encoder Training",
      "text" : "We adopt a BERT-based bi-encoder to encode queries and passages separately into d-dimension vectors. The output representation is derived from the last hidden state of the [CLS] token and the final matching score is measured by the inner product:\nhq = BERTQ(q)([CLS])\nhp = BERTP(p)([CLS])\nS(p, q) = hTq · hp\nLet B = {〈qi, p+i , p − i 〉}ni=1 be a mini-batch with n instances. Each instance contains a question qi paired with a positive passage p+i and a negative passage p−i . With in-batch negative sampling, each question qi considers all the passages in B except its own gold p+i as negatives, resulting in 2n − 1 negatives per question in total. We use the negative log likelihood of the positive passage as our loss for optimization:\nL(qi, p + i , p − i,1, ..., p − i,2n−1)\n=− log e S(qi,p\n+ i )\neS(qi,p + i ) + ∑2n−1 j=1 e S(qi,p − i,j)"
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "In this session, we discuss the pre-training corpus preparation, downstream datasets, the hyperparameter and the basic setup for our experiments."
    }, {
      "heading" : "4.1 Pre-training Corpus",
      "text" : "We adopt Wikipedia as our source corpusD for pretraining as it is the largest encyclopedia covering diverse topics with good content quality and linking structures. We choose the snapshot 03-01-2021 of an English Wikipedia dump, and process it with WikiExtractor2 to obtain clean context. After filtering out documents with blank text or a title less than three letters, following previous work (Karpukhin et al., 2020), we split the remaining documents into disjoint chunks of 100 words as passages, resulting in over 22 million passages in the end.\n2Available at https://github.com/attardi/wikiextractor"
    }, {
      "heading" : "4.2 Downstream Datasets",
      "text" : "We evaluate our method on several open-domain question answering benchmarks which are shown below. Natural Questions (NQ) (Kwiatkowski et al., 2019) is a popular QA dataset with real queries from Google Search and annotated answers from Wikipedia. TriviaQA (Joshi et al., 2017) contains questionanswer pairs scraped from trivia websites. WebQuestions (WQ) (Berant et al., 2013) consists of questions generated by Google Suggest API with entity-level answers from Freebase. HotpotQA (Fullwiki) (Yang et al., 2018) is a human-annotated multi-hop question answering dataset. BioASQ (Tsatsaronis et al., 2015) is a competition on biomedical semantic indexing and question answering. We evaluate its factoid questions from task 8B. MS MARCO (Passage Ranking) (Nguyen et al., 2016) consists of real-world user queries and a large collection of Web passages extracted by Bing search engine. Retrieval Corpus For downstream retrieval, we use the 21M Wikipedia passages provided by DPR (Karpukhin et al., 2020) for NQ, TriviaQA and WQ. For BioASQ, we take the abstracts of PubMed articles from task 8A with the same split to Reddy et al. (2021)’s work. For HotpotQA and MS MARCO, we use the official corpus."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "During the pre-training, we train the bi-encoder for 5 epochs with parameters shared, using a batch size of 400 and an Adam optimizer (Kingma and Ba, 2014) with a learning rate 2× 10−5, linear scheduling with 10% warm-up steps. Our HLP and all the reproduced baselines are trained on 20 million Q-P pairs with in-batch negative sampling, and the best checkpoints are selected based on the average rank of gold passages evaluated on the NQ dev set. The pre-training takes around 3 days using eight NVIDIA V100 32GB GPUs.\nFor the downstream, we use the same hyperparameters for all experiments. Specifically, we fine-tune the pre-trained models for 40 epochs with a batch size of 256 and the same optimizer and learning rate settings to the pre-training. We conduct evaluation on respective dev sets to select best checkpoints, and we use the last checkpoint if there\nis no dev set or test set (e.g. HotpotQA). More details can be found in the Appendix A."
    }, {
      "heading" : "4.4 Baselines",
      "text" : "Most existing baselines have been implemented under different experimental settings, which have a substantial effect on the retrieval performance. To ensure fairness, we reproduce several pre-training methods (ICT, WLP, BFS, and their combination) under the same experimental setting, such as batch size, base model, amount of pre-training data, and so on. The only difference between our method and the re-implemented baselines is the self-supervision signal derived from the respective pre-training samples. Our reproduced BM25 baseline is better than that reported in Karpukhin et al. (2020), and the re-implemented pre-training methods also perform better than those reported by the recent work3. In addition, we include the work REALM (Guu et al., 2020) as a baseline which has recently been reproduced by Sachan et al. (2021) using 240 GPUs and is named masked salient spans (MSS). We note that most related works gain improvements from varying downstream setting or synthetic pre-training with access to the downstream data of respective domain, which is out of the scope of our interests."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Main Results",
      "text" : "Table 1 shows the retrieval accuracy of different models on three popular QA datasets under zeroshot and full-set fine-tuning settings.\nUnder zero-shot setting, HLP consistently outperforms BM25 except for the top-5 retrieval accuracy of TriviaQA, while all other pre-training baselines are far behind. We attribute the minor improvement over BM25 on TriviaQA to a high overlap between questions and passages, which gives term-based retriever a clear advantage. We investigate the coverage of the question tokens that appear in the gold passage and find that the overlap is indeed higher in TriviaQA (62.8%) than NQ (60.7%) and WQ (57.5%).\nAfter fine-tuning, all models with intermediate pre-training give better results than the vanilla DPR while our HLP achieves the best in nearly all cases.\n3Our reproduced ICT and BFS surpass the reproduction from recent work (Oğuz et al., 2021) by 15 and 12 points, respectively, in terms of top-20 retrieval accuracy on NQ test set under zero-shot setting.\nAmong ICT, WLP and BFS, we observe that WLP is the most competitive with or without fine-tuning, and additional improvements can be achieved by combining three of them. This observation indicates that pre-training with diverse relevance leads to better generalization to downstream tasks, while document-wise relevance is more adaptable for the OpenQA retrieval. The advantage of documentwise relevance may come from the fact that texts in different documents are likely written by different parties, providing less superficial cues for text matching, which is beneficial for the downstream retrieval. Our HLP learns both coarse-grained document-wise relationships as well as the finegrained entity-level evidence, which results in a significant improvement."
    }, {
      "heading" : "5.2 Few-shot Learning",
      "text" : "To investigate the retrieval effectiveness in a more realistic scenario, we conduct experiments for few-shot learning. Specifically, we fine-tune the pre-trained models on large datasets (NQ, TriviaQA) with m (m = {16, 256, 1024}) samples and present the few-shot retrieval results in Table 2. With only a few hundred labeled data for fine-tuning, all the models with intermediate pre-training perform better than that without, and HLP outperforms the other methods by a larger margin when m is smaller. Among three reimplemented baselines, WLP gains the largest improvement with increasing number of samples, outperforming ICT and BFS when a thousand labelled samples are provided for fine-tuning."
    }, {
      "heading" : "5.3 Out-of-domain (OOD) Scenario",
      "text" : "While HLP is pre-trained on Wikipedia pages, we conduct additional experiments on BioASQ and MS MARCO datasets with non-Wikipedia corpus to further verify its out-of-domain (OOD) generalization. Following Gururangan et al. (2020), we measure the similarity between corpus by computing the vocabulary overlap of the top 10K frequent words (excluding stopwords). We observe a vocabulary overlap of 36.2% between BioASQ and Wikipedia while 61.4% between MS MARCO and Wikipedia, indicating that these two domains differ considerably from our pre-training corpus.\nThe results of zero-shot retrieval on BioASQ\nand MS MARCO datasets are presented in Table 4. For BioASQ, HLP is competitive with both BM25 and AugDPR(Reddy et al., 2021) while significantly outperforming ICT, WLP, and BFS. Note that AugDPR is a baseline that has access to NQ labeled data whereas our HLP is trained in an unsupervised way. For MS MARCO, HLP consistently outperforms other pre-training methods but falls behind BM25 under zero-shot setting. We conjecture the performance degradation on MS MARCO is attributed to two factors: 1) the Q-P lexical overlap of MS MARCO (65.7%) is higher than that in BioASQ (48.7%) as well as other datasets; 2) the information-seeking target of the MS MARCO query is the entire passage rather than a short answer span, which is biased towards our proposed answer containing. we also observe that pre-training exclusively with DL pairs achieves better results in MS MARCO, indicating the generality of relevance induced by DL topology."
    }, {
      "heading" : "5.4 Multi-hop Retrieval",
      "text" : "While HLP aims to acquires the ability in matching document-wise concepts and facts, it raises our interest in its capability for multi-hop scenarios. We evaluate our methods on HotpotQA in a single-hop manner. Specifically, for each query, we randomly selects one golden passage from the two as a posi-\ntive passage and one additional passage with high TF-IDF scores as a negative passage. Our models are further fine-tuned on the HotpotQA training set and evaluated on the bridge and the comparison type questions from the development set, respectively. The results of our study are shown in Table 5 which reveals that HLP consistently outperforms others methods, with up to a 11-point improvement on top-5 retrieval accuracy of bridge questions. Furthermore, WLP yields a 4-point advantages in average over ICT and BFS on bridge questions, showing that document-wise relevance contributes to better associative abilities. We include a case study in Appendix F."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Ablation Study",
      "text" : "To better understand how different key factors affect the results, we conduct ablation experiments with results shown in Table 3. Hyperlink-based Topologies Our proposed dual-link (DL) and co-mention (CM) Q-P pairs, provide evidence induced by different hyperlinkbased topologies. To examine their respective effectiveness, we pre-train retrievers on Q-P pairs derived from each topology and their combinations. We present zero-shot retrieval results in Table 3, which show that retrievers pre-trained on DL pairs has a distinct advantage over that on CM pairs, while combining both gives extra improvement. Negative Passage In practice, negative sampling\nis essential for learning a high-quality encoder. Besides in-batch negative, our reported HLP employs one additional negative for each query. We further explore the impact of the additional negatives during pre-training. In our ablation study, pre-training with additional negatives improves the results significantly, which may be attributed to using more in-batch pairs for text matching. More details on implementation and negative sampling strategies can be found in Appendix B."
    }, {
      "heading" : "6.2 Analysis on Q-P Overlap",
      "text" : "We carry out extensive analysis on the Q-P lexical overlap in the task of retrieval. Specifically, we tokenize q, p using the BERT tokenizer and measure the Q-P overlap as the proportion of the question tokens that appear in the corresponding passage. Based on the degree of Q-P overlap, we divided the NQ dev set into five categories for further analysis. Distribution of Q-P Overlap Figure 3 shows both the pre-training and the retrieved pairs of HLP have a more similar overlap distribution with the downstream NQ dataset than the other methods, which implies the consistency between the relevance provided by HLP and that in real informationseeking scenario.\nRetrieval Performance vs. Q-P Overlap Figure 4 shows the top-20 retrieval accuracy on the samples with varying degrees of Q-P overlap. Both figures show that the retrievers are more likely to return answer-containing passages when there is higher Q-P overlap, suggesting that all these models can exploit lexical overlap for passage retrieval. Under the zero-shot setting, HLP outperforms all the methods except BM25 when r is larger than 0.8, which reflects the strong reasoning ability of HLP and the overlap-dependent nature of the termbased retrievers. After fine-tuning, models with additional pre-training perform better than the vanilla DPR while HLP outperforms all other methods in\nmost of the cases. It is important to note that HLP is pre-trained on more high-overlap text pairs while it performs better than all the other methods when fewer overlaps are provided. We speculate that this is because the overlapping in HLP Q-P pairs mostly comes from the factual information, such as entity, which introduces fewer superficial cues, allowing for better adaptation to the downstream cases."
    }, {
      "heading" : "6.3 Human Evaluation on Q-P pairs",
      "text" : "We conduct human evaluation to investigate the proportion of Q-P pairs that convey the similar factlevel information. Specially, we randomly selected one hundred examples from our constructed Q-P pairs and asked annotators to identify whether the query and the corresponding passage convey similar facts. Each case is evaluated by three annotators and the result is determined by their votes. Our results are shown in Table 6, and we further present case studies in Appendix G."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper proposes Hyperlink-induced Pretraining (HLP), a pre-training method for OpenQA passage retrieval by leveraging the online textual relevance induced by hyperlink-based topology. Our experiments show that HLP gains significant improvements across multiple QA datasets under different scenarios, consistently outperforming other pre-training methods. Our method provides insights into OpenQA passage retrieval by analyzing the underlying bi-text relevance. Future work involves addressing tasks like MS MARCO where the granularity of the information-seeking target is at the passage level."
    }, {
      "heading" : "A Parameter Details",
      "text" : "For the pre-training, all models including our reproduced baselines are trained with 20 million Q-P pairs with in-batch negative sampling. Specifically, our reported HLP is trained on the combination of 10 million DL pairs and 10 million CM pairs. The HLP (DL) and HLP (CM) reported in Table 4 are trained on 10 million DL pairs and 10 million CM pairs, respectively. More parameters details are shown in the table below.\nHyperparameter Pre-training Fine-tuning Epoch 5 40\nBatch Size 400 256 GPU Resource 32GB GPU × 8 32GB GPU × 8 Learning Rate 2e-5 2e-5 Warmup Ratio 0.1 0.1\nLearning Rate Decay Linear Linear Shared Encoder True False Maximum Q Length 150 256 Maximum P Length 256 256"
    }, {
      "heading" : "B Negative Sampling",
      "text" : "While negative sampling plays an import role in contrast learning, we have explored different types of negatives to pair with queries: (1) Random negatives: passages randomly selected from the corpus (2) Overlap negatives: passages have entity overlap with queries but fail to match either DL or CM topology. Our experimental results in Table 7 show that the model perform better when it adopts random negatives. We conjecture that the overlap negatives may be too hard for the self-supervised pre-training. Thus, we pair one random negative to each query during pre-training."
    }, {
      "heading" : "C Data Analysis on NQ Samples",
      "text" : "In this part, we detailedly discuss how we conduct exploratory data analysis on NQ training set to determine the hyperlink-based topology. Driven by a strong interest in what kind of roles the overlapping spans play between the queries q and passages p, we conduct exploratory data analysis on the widelyused NQ dataset.\nSpecifically, we recognize all entities and mentions from the queries and the passages using TagMe (Ferragina and Scaiella, 2010) for further investigation. As a result, we observe about 55% queries q either explicitly mentions the titles of p or the successfully links to the document where p originated via TagMe, which motivates us to construct the dual-link topology where the pseudo queries q mention p via a hypertext. Moreover, we observe about 45% queries do not mention title of q but share the same mentions with p which encourages us to adopt the co-mention topology where the pseudo q and p both mention a third-party document through hypertext."
    }, {
      "heading" : "D Fact-level Evidence Reduction",
      "text" : "Intuitively, we assume the mentioned entity, let’s say eY mentioned in a Wikipedia document X , is used to describe the topical entity eX of this document. In other words, eY is likely to attend\nin a topically relevant fact or event, which can be represented as a triple <eX , rXY , eY > where rXY is a latent relation between eX and eY .\nGiven any passage pair (q, p) from Wikipedia, we call q and p have fact-level evidence if they both entail a fact that can be represented as a triple <eX , rXY , eY > where eX , eY are entities and rXY is their corresponding relations. Further, if both passages q and p contain representative hypertext or topic of eX and eY , we say this fact-level evidence can be induced by hyperlink-based topology, namely hyperlink-induced fact. In this session, we prove that any Q-P pair with hyperlink-induced fact while satisfying answer containing is within either DL or CM hyperlink-based topology.\nFollowing the example above, both q and p contain a factual triple <eX , rXY , eY >. Since mentioned entities are used to describe the topical entity, we have facts <eQ, rQX , eX>, <eQ, rQY , eY > at q-side while <eP , rPX , eX>, <eP , rPY , eY > at p-side. Further, p contains <eP , rPQ, eQ> because of the answer containing condition.\nCase1: The entity eP = eX or eP = eY . Then q provides facts <eQ, rQP , eP> where rQP is likely but not necessarily to be identical to rPQ in p. In this case, (q, p) fits in the Dual-link topology in our definition.\nCase2: The entity eP 6= eX and eP 6= eY . Then given the facts <eQ, rQX , eX> at q-side, and <eP , rPX , eX>, <eP , rPQ, eQ> at p-side, (q, p) fits in the Co-mention topology."
    }, {
      "heading" : "E Pseudo Code for HLP Pairs",
      "text" : "Algorithm 1: HLP Pairs Identification Notation:\nq, p←Wikipedia passages tQ ← Topical entity of passage q M(q)← The set of entities mentioned in q din(q)← in-degree of the Wikipedia entity tQ K ← in-degree threshold for CM pairs\nDef IsDL(q, p): if tP ∈M(q) & tQ ∈M(p) then\nreturn 1 else\nreturn 0 ; Def IsCM(q, p):\nforeach m ∈M(q) do if din(m) < K & m ∈M(p) & tQ ∈M(p) then\nreturn 1 else\nreturn 0"
    }, {
      "heading" : "F Case Studies on Multi-hop Retrieval",
      "text" : "We evaluate HLP on multi-hop scenario where knowledge from different documents need to be associated for retrieval. Besides significant improvements shown in Table 5, we conduct case study to investigate its capability on knowledgeintensive retrieval. In Table 8, a complex question is proposed, requiring the retriever firstly to retrieve the document “Apple Remote” and then “Front Row (software)” to fetch the final answer. Our HLP successfully retrieves both golds in the top-10 retrieved passages while the vanilla DPR fails. We find 6 items retrieved by HIS are related to the brand “Apple” while 4 are by DPR, showing stronger comprehension and associative ability from HLP."
    }, {
      "heading" : "G Case Studies on Q-P Paraphrase",
      "text" : "Besides human evaluation, we present case studies on HLP Q-P pairs, which is shown in Table 9 and Table 10. As we can see in the tables, a few lexical variants of entities and fact-level paraphrasing are presented across questions and passages, which can be interpreted as factual evidence for OpenQA passage matching. For example, entity-level variants such as “Robert and Richard Sherman” vs. “Sherman Brothers”, and fact-level paraphrases such as “Abby Kelley and Stephen Symonds Foster ... working for abolitionism” vs. “... radical abolitionists, Abby Kelley Foster and her husband Stephen S. Foster” can be found in our examples."
    } ],
    "references" : [ {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "A neural click model for web search",
      "author" : [ "Alexey Borisov", "Ilya Markov", "Maarten de Rijke", "Pavel Serdyukov." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016, pages 531–541.",
      "citeRegEx" : "Borisov et al\\.,? 2016",
      "shortCiteRegEx" : "Borisov et al\\.",
      "year" : 2016
    }, {
      "title" : "Pretraining tasks for embedding-based large-scale retrieval",
      "author" : [ "Wei-Cheng Chang", "Felix X Yu", "Yin-Wen Chang", "Yiming Yang", "Sanjiv Kumar." ],
      "venue" : "arXiv preprint arXiv:2002.03932.",
      "citeRegEx" : "Chang et al\\.,? 2020",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reading wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural ranking models with weak supervision",
      "author" : [ "Mostafa Dehghani", "Hamed Zamani", "Aliaksei Severyn", "Jaap Kamps", "W. Bruce Croft." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Dehghani et al\\.,? 2017",
      "shortCiteRegEx" : "Dehghani et al\\.",
      "year" : 2017
    }, {
      "title" : "Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)",
      "author" : [ "Paolo Ferragina", "Ugo Scaiella." ],
      "venue" : "Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1625–1628.",
      "citeRegEx" : "Ferragina and Scaiella.,? 2010",
      "shortCiteRegEx" : "Ferragina and Scaiella.",
      "year" : 2010
    }, {
      "title" : "Don’t stop pretraining: adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:2004.10964.",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrieval augmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Mingwei Chang." ],
      "venue" : "International Conference on Machine Learning, pages 3929–3938. PMLR.",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Tom Kwiatkowski", "Jennimaria Palomaki", "Olivia Redfield", "Michael Collins", "Ankur Parikh", "Chris Alberti", "Danielle Epstein", "Illia Polosukhin", "Jacob Devlin", "Kenton Lee" ],
      "venue" : null,
      "citeRegEx" : "Kwiatkowski et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2019
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "PAQ: 65 million probably-asked questions and what you can do with them",
      "author" : [ "Patrick S.H. Lewis", "Yuxiang Wu", "Linqing Liu", "Pasquale Minervini", "Heinrich Küttler", "Aleksandra Piktus", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "CoRR,",
      "citeRegEx" : "Lewis et al\\.,? 2021",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2021
    }, {
      "title" : "Zero-shot neural passage retrieval via domain-targeted synthetic question generation",
      "author" : [ "Ji Ma", "Ivan Korotkov", "Yinfei Yang", "Keith B. Hall", "Ryan T. McDonald." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Com-",
      "citeRegEx" : "Ma et al\\.,? 2021",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2021
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Ms marco: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "CoCo@ NIPS.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain-matched pretraining tasks for dense retrieval",
      "author" : [ "Barlas Oğuz", "Kushal Lakhotia", "Anchit Gupta", "Patrick Lewis", "Vladimir Karpukhin", "Aleksandra Piktus", "Xilun Chen", "Sebastian Riedel", "Wen-tau Yih", "Sonal Gupta" ],
      "venue" : null,
      "citeRegEx" : "Oğuz et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Oğuz et al\\.",
      "year" : 2021
    }, {
      "title" : "Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering",
      "author" : [ "Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Wayne Xin Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Qu et al\\.,? 2021",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2021
    }, {
      "title" : "Towards robust neural retrieval",
      "author" : [ "Revanth Gangi Reddy", "Vikas Yadav", "Md Arafat Sultan", "Martin Franz", "Vittorio Castelli", "Heng Ji", "Avirup Sil" ],
      "venue" : null,
      "citeRegEx" : "Reddy et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2021
    }, {
      "title" : "The probabilistic relevance framework: BM25 and beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "Now Publishers Inc.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "End-to-end training of neural retrievers for open-domain question answering",
      "author" : [ "Devendra Singh Sachan", "Mostofa Patwary", "Mohammad Shoeybi", "Neel Kant", "Wei Ping", "William L Hamilton", "Bryan Catanzaro." ],
      "venue" : "arXiv preprint arXiv:2101.00408.",
      "citeRegEx" : "Sachan et al\\.,? 2021",
      "shortCiteRegEx" : "Sachan et al\\.",
      "year" : 2021
    }, {
      "title" : "Can question generation debias question answering models? a case study on question-context lexical overlap",
      "author" : [ "Kazutoshi Shinoda", "Saku Sugawara", "Akiko Aizawa." ],
      "venue" : "arXiv preprint arXiv:2109.11256.",
      "citeRegEx" : "Shinoda et al\\.,? 2021",
      "shortCiteRegEx" : "Shinoda et al\\.",
      "year" : 2021
    }, {
      "title" : "An overview of the bioasq",
      "author" : [ "George Tsatsaronis", "Georgios Balikas", "Prodromos Malakasiotis", "Ioannis Partalas", "Matthias Zschunke", "Michael R Alvers", "Dirk Weissenborn", "Anastasia Krithara", "Sergios Petridis", "Dimitris Polychronopoulos" ],
      "venue" : null,
      "citeRegEx" : "Tsatsaronis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsatsaronis et al\\.",
      "year" : 2015
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empiri-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Retrieving and reading: A comprehensive survey on open-domain question answering",
      "author" : [ "Fengbin Zhu", "Wenqiang Lei", "Chao Wang", "Jianming Zheng", "Soujanya Poria", "Tat-Seng Chua." ],
      "venue" : "arXiv preprint arXiv:2101.00774.",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    }, {
      "title" : "The film stars Sivaji Ganesan and Vanisri , and is the Tamil remake of the 1971 Telugu film ",
      "author" : [ ],
      "venue" : "Indian Tamil -language romance film,",
      "citeRegEx" : "Maligai,? \\Q1972\\E",
      "shortCiteRegEx" : "Maligai",
      "year" : 1972
    }, {
      "title" : "School Board is a school district headquartered in Oberlin in Allen Parish in southwestern Louisiana",
      "author" : [ ],
      "venue" : "United States. From 1960 to",
      "citeRegEx" : "Parish,? \\Q1969\\E",
      "shortCiteRegEx" : "Parish",
      "year" : 1969
    }, {
      "title" : "Pert tied for 2nd–4th with David Howell and Daniel Gormally, finishing third on tiebreak, in the British Chess Championship and later that year, he finished runner-up in the inaugural British Knockout Championship, which was held alongside the London Chess Classic. In this latter event, Pert, who replaced Nigel Short after his late withdrawal, eliminated Jonathan Hawkins in the quarterfinals",
      "author" : [ "Title: Nicholas Pert" ],
      "venue" : null,
      "citeRegEx" : "Pert,? \\Q2015\\E",
      "shortCiteRegEx" : "Pert",
      "year" : 2015
    }, {
      "title" : "1931) was an Austrian chess master, born in Zagreb",
      "author" : [ ],
      "venue" : "Meran",
      "citeRegEx" : "Wolf.Kalmar,? \\Q1924\\E",
      "shortCiteRegEx" : "Wolf.Kalmar",
      "year" : 1924
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Current approaches to OpenQA usually adopt a two-stage retriever-reader paradigm (Chen et al., 2017; Zhu et al., 2021) to fetch the final answer span.",
      "startOffset" : 81,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "Current approaches to OpenQA usually adopt a two-stage retriever-reader paradigm (Chen et al., 2017; Zhu et al., 2021) to fetch the final answer span.",
      "startOffset" : 81,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "Traditional retrievers, such as TF-IDF and BM25 (Robertson and Zaragoza, 2009), are considered incapable of adapting to sce-",
      "startOffset" : 48,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "Recent works (Lee et al., 2019; Karpukhin et al., 2020; Qu et al., 2021) show that by finetuning pre-trained language models on sufficient downstream data, dense retrievers can significantly outperform traditional term-based retrievers.",
      "startOffset" : 13,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "Recent works (Lee et al., 2019; Karpukhin et al., 2020; Qu et al., 2021) show that by finetuning pre-trained language models on sufficient downstream data, dense retrievers can significantly outperform traditional term-based retrievers.",
      "startOffset" : 13,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "Recent works (Lee et al., 2019; Karpukhin et al., 2020; Qu et al., 2021) show that by finetuning pre-trained language models on sufficient downstream data, dense retrievers can significantly outperform traditional term-based retrievers.",
      "startOffset" : 13,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "Considering the data-hungry nature of the neural retrieval models, extensive efforts (Lee et al., 2019; Chang et al., 2020; Sachan et al., 2021) have been made to design self-supervised tasks to pre-train the retriever.",
      "startOffset" : 85,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "Considering the data-hungry nature of the neural retrieval models, extensive efforts (Lee et al., 2019; Chang et al., 2020; Sachan et al., 2021) have been made to design self-supervised tasks to pre-train the retriever.",
      "startOffset" : 85,
      "endOffset" : 144
    }, {
      "referenceID" : 21,
      "context" : "Considering the data-hungry nature of the neural retrieval models, extensive efforts (Lee et al., 2019; Chang et al., 2020; Sachan et al., 2021) have been made to design self-supervised tasks to pre-train the retriever.",
      "startOffset" : 85,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "We consider two conditions of relevance within Q-P pairs, which is similar to the process of distantly supervised retriever learning (Mintz et al., 2009; Chen et al., 2017).",
      "startOffset" : 133,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "We consider two conditions of relevance within Q-P pairs, which is similar to the process of distantly supervised retriever learning (Mintz et al., 2009; Chen et al., 2017).",
      "startOffset" : 133,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "(2021) uses the synthetically generated questions from PAQ datasaset (Lewis et al., 2021) and the post-comment pairs from dataset of Reddit conversations for retrieval pre-training.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "After filtering out documents with blank text or a title less than three letters, following previous work (Karpukhin et al., 2020), we split the remaining documents into disjoint chunks of 100 words as passages, resulting in over 22 million passages in the end.",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "Natural Questions (NQ) (Kwiatkowski et al., 2019) is a popular QA dataset with real queries from Google Search and annotated answers from Wikipedia.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "TriviaQA (Joshi et al., 2017) contains questionanswer pairs scraped from trivia websites.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "WebQuestions (WQ) (Berant et al., 2013) consists of questions generated by Google Suggest API with entity-level answers from Freebase.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "HotpotQA (Fullwiki) (Yang et al., 2018) is a human-annotated multi-hop question answering dataset.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : "BioASQ (Tsatsaronis et al., 2015) is a competition on biomedical semantic indexing and question answering.",
      "startOffset" : 7,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "During the pre-training, we train the bi-encoder for 5 epochs with parameters shared, using a batch size of 400 and an Adam optimizer (Kingma and Ba, 2014) with a learning rate 2× 10−5, linear scheduling with 10% warm-up steps.",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "In addition, we include the work REALM (Guu et al., 2020) as a baseline which has recently been reproduced by Sachan et al.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "Our reproduced ICT and BFS surpass the reproduction from recent work (Oğuz et al., 2021) by 15 and 12 points, respectively, in terms of top-20 retrieval accuracy on NQ test set under zero-shot setting.",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "For BioASQ, HLP is competitive with both BM25 and AugDPR(Reddy et al., 2021) while significantly outperforming ICT, WLP, and BFS.",
      "startOffset" : 56,
      "endOffset" : 76
    } ],
    "year" : 0,
    "abstractText" : "To alleviate the data scarcity problem in training question answering systems, recent works propose additional intermediate pre-training for dense passage retrieval (DPR). However, there still remains a large discrepancy between the provided upstream signals and the downstream question-passage relevance, which leads to less improvement. To bridge this gap, we propose the HyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever with the text relevance induced by hyperlink-based topology within Web documents. We demonstrate that the hyperlinkbased structures of dual-link and co-mention can provide effective relevance signals for large-scale pre-training that better facilitate downstream passage retrieval. We investigate the effectiveness of our approach across a wide range of open-domain QA datasets under zeroshot, few-shot, multi-hop, and out-of-domain scenarios. The experiments show our HLP outperforms the BM25 by up to 7 points as well as other pre-training methods by up to 30 points in terms of top-20 retrieval accuracy under the zero-shot scenario. Furthermore, HLP significantly outperforms other pre-training methods under the other scenarios.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_101_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Predicting the semantic similarity between two sentences has been extensively studied in the literature (Gomaa et al., 2013; Agirre et al., 2015; Majumder et al., 2016; Cer et al., 2017). Several recent studies successfully utilized a pretrained language model such as BERT (Devlin et al., 2019) by finetuning it to capture sentence similarity (Reimers and Gurevych, 2019). To be specific, they define a similarity score between sentence embeddings, which are obtained by aggregating contextualized token embeddings (e.g., avg pooling) or using a special token (e.g., [CLS]), then optimize the score for natural language inference (NLI) or semantic textual similarity (STS) tasks (Gao et al., 2021).\nAlong with the quality of sentence similarity, interpreting the predicted sentence similarity is also\nimportant for end-users to better understand the results (Agirre et al., 2016; Gilpin et al., 2018; Rogers et al., 2020). In general, finding out the cross-sentence alignment and the importance of each aligned part is useful for analyzing sentence similarity (Sultan et al., 2015). For example, there were several attempts to use explicit features (e.g., TF-IDF) for easily analyzing the interaction among the shared terms (Salton and Buckley, 1988) or to adopt sophisticated metrics (e.g., word mover’s distance) for explicitly describing it by the importance and similarity of word pairs across two sentences (Kusner et al., 2015). However, for recent approaches that leverage sentence embeddings from a pretrained model, it has not been studied how the cross-sentence interaction of each part contributes to the final sentence similarity.\nIn this work, we propose an analytical method based on optimal transport to analyze existing approaches that leverage a pretrained model. We consider a sentence similarity measure a solution to a transportation problem, which aims to transport a collection of contextualized tokens in a sentence\nto the ones in another sentence. As byproducts of the problem, we obtain a cost matrix and a transportation matrix, which encode the similarities of all token pairs across sentences and their contributions to the sentence similarity, respectively. Using this analytical method, we point out that the existing approaches suffer from the rank-1 constraint in the transportation matrix; this eventually keeps the model from effectively capturing the similarities of semantically-aligned token pairs into sentence similarity. For example, considering transportation in a contextualized embedding space (Figure 1), the distance between averaged token embeddings (orange arrows) cannot clearly represent the distance of semantically-aligned token pairs (blue arrows).\nTo resolve the above limitation and enhance the interpretability of a model, we present a novel distance measure and a contrastive learning framework that optimizes the distance between sentences. First, we apply optimal transport in a contextualized embedding space and leverage the optimal solution for a relaxed transportation problem as our distance measure. This sentence distance is composed of the distances of semantically-aligned token pairs; this makes the result easily interpretable. Furthermore, we present a contrastive learning framework that adopts the proposed distance to finetune the model with token-level supervision. It optimizes the model to learn the relevance of semantically-aligned token pairs from that of sentence pairs, which further enhances interpretability.\nWe extensively evaluate our approach and validate the effectiveness of its sentence similarity and interpretation. The comparison on 7 STS benchmarks supports the superiority of sentence similarity predicted by the model trained by our framework. In particular, the evaluation on 2 interpretable-STS datasets demonstrates that the proposed distance measure finds out semantically relevant token pairs that are more consistent with human judgement compared to other baseline methods. Our qualitative analysis shows that both the token alignment and their similarity scores from our model serve as useful resources for end-users to better understand the sentence similarity."
    }, {
      "heading" : "2 Related work",
      "text" : ""
    }, {
      "heading" : "2.1 Semantic textual similarity",
      "text" : "Most recent studies tried to leverage a pretrained language model with various model architectures and training objectives for STS tasks, achieving\nthe state-of-the-art performance. In terms of model architecture, Devlin et al. (2019) focus on exhaustive cross-correlation between sentences by taking a concatenated text of two sentences as an input, while Reimers and Gurevych (2019) improve scalability based on a Siamese network and Humeau et al. (2020) adopt a hybrid approach. Along with the progress of model architectures, many advanced objectives for STS tasks were proposed as well. Specifically, Reimers and Gurevych (2019) mainly use the classification objective for an NLI dataset, and Wu et al. (2020) adopt contrastive learning to utilize self-supervision from a large corpus. Yan et al. (2021); Gao et al. (2021) incorporate a parallel corpus such as NLI datasets into their contrastive learning framework.\nDespite their effectiveness, the interpretability of the above models for STS tasks was not fully explored (Belinkov and Glass, 2019). One related task is interpretable STS, which aims to predict chunk alignment between two sentences (Agirre et al., 2016). For this task, a variety of supervised approaches were proposed based on neural networks (Konopík et al., 2016; Lopez-Gazpio et al., 2016), linear programming (Tekumalla and Jat, 2016), and pretrained models (Maji et al., 2020). However, these methods cannot predict the similarity between sentences because they focus on finding chunk alignment only. To the best of our knowledge, no previous approaches based on a pretrained model have taken into account both sentence similarity and interpretation."
    }, {
      "heading" : "2.2 Optimal transport",
      "text" : "Optimal transport (Monge, 1781) has been successfully adopted in natural language processing due to its ability to find a plausible correspondence between two objects (Li et al., 2020; Xu et al., 2021). For example, Kusner et al. (2015) adopt optimal transport to measure the distance between two documents with pretrained word vectors. In addition, Swanson et al. (2020) discover the rationale in textmatching via optimal transport, thereby improving model interpretability.\nOne well-known limitation of optimal transport is that finding the optimal solution is computationally intensive, and thus approximation schemes for this problem have been extensively researched (Grauman and Darrell, 2004; Shirdhonkar and Jacobs, 2008). To get the solution efficiently, Cuturi (2013) provides a regularizer inspired by a\nprobabilistic theory and then uses Sinkhorn’s algorithm. Kusner et al. (2015) relax the problem to get the quadratic-time solution by removing one of the constraints, and Wu et al. (2018) introduce a kernel method to approximate the optimal transport."
    }, {
      "heading" : "3 Method",
      "text" : "We first analyze the similarity measure used by existing models from the perspective of a transportation problem. Considering the above analysis, we present a novel distance measure and a contrastive sentence learning framework to enhance the interpretability of a sentence similarity model."
    }, {
      "heading" : "3.1 Distance as a transportation problem",
      "text" : "We briefly explain the transportation problem and how to interpret the total transportation cost as a distance measure. A transportation problem consists of three components: states before and after transportation, and a cost matrix. In general, the two states are represented in high-dimensional simplex, i.e., d1 ∈ Σd1 and d2 ∈ Σd2 , where each dimension implies a specific location with a nonnegative quantity. The cost matrix M ∈ Rd1×d2 encodes the unit transportation cost from location i to j into Mi,j . In this situation, we search the transportation plan to transport from d1 to d2 with the minimum cost. Using the above notations, the optimization problem is written as follows:\nminimize T∈Rd1×d2≥0 ∑ i,j Ti,jMi,j (1)\nsubject to T>~1 = d2, T~1 = d1,\nwhere each entry of the transportation matrix Ti,j indicates how much quantity is transferred from location i to j. The optimal solution to this problem is called optimal transport, which is also known as earth mover’s distance (EMD):\ndEMDM := ∑ i,j T∗i,jMi,j . (2)\nIn Equation (2), the distance is computed by the sum of element-wise multiplications of the optimal transportation matrix T∗ and the cost matrix M. In this sense, EMD considers the optimality of distance when combining unit costs in M. That is, the priority of each unit cost when being fused to the distance is encoded in the transportation matrix, which serves as a useful resource for analyzing the distance."
    }, {
      "heading" : "3.1.1 Example: Average pooling",
      "text" : "We express cosine similarity with average pooling as a transportation problem and analyze its properties in terms of the transportation matrix. Note that this similarity measure is widely adopted in most of the previous studies (Reimers and Gurevych, 2019; Wu et al., 2020; Gao et al., 2021). Formally, for a sentence of length L, the sentence embedding is generated by applying average pooling to L contextualized token embeddings, i.e., s = 1L ∑L i=1 xi, where xi is the i-th token embedding obtained from a pretrained model. Using the sentence embeddings, the sentence similarity is defined by\nsAVG = cos(s1, s2) = s1>s2\n‖s1‖‖s2‖ .\nThis average pooling-based sentence similarity can be converted into the distance, dAVG = 1 − sAVG, described by the token embeddings as follows:\ndAVG = 1− L1∑ i=1 L2∑ j=1 1 L1L2 ‖x1i ‖‖x2j‖ ‖s1‖ ‖s2‖ x1i >x2j ‖x1i ‖‖x2j‖ .\nFrom the perspective of Equation (1), this distance is interpreted as a naive solution of a special transportation problem, where the cost matrix and the transportation matrix are\nMAVGi,j = ‖s1‖‖s2‖ ‖x1i ‖‖x2j‖ − cos(x1i ,x2j ),\nTAVGi,j = 1\nL1L2\n‖x1i ‖‖x2j‖ ‖s1‖ ‖s2‖ . (3)\nEach entry of the cost matrix includes negative cosine similarities between token embeddings, and the contribution of each token pair to the sentence distance (i.e., the transportation matrix) is determined by the norms of the token embeddings. In theory, the rank of the transportation matrix is constrained to be one, which prevents effective integration of the token distances into the sentence distance. In practice, it is impossible to involve only semantically-aligned token pairs across sentences because all possible token pairs are considered by the products of their norms. From this analysis, we point out that the average pooling-based similarity is not effective enough to capture the token correspondence between sentences."
    }, {
      "heading" : "3.2 Relaxed optimal transport distance for contextualized token embeddings",
      "text" : "To resolve the ineffectiveness of the existing measure, we introduce a novel distance measure based\non optimal transport. We first define a transportation problem that considers semantic relevance in a contextualized embedding space. Given the token embeddings of two sentences from a pretrained language model, we construct a cost matrix MCMD ∈ RL1×L2 that encodes token similarities using cosine distance, and define the state vectors for the two sentences as one vectors normalized by their sentence lengths d1 := 1L1 ~1 and d2 := 1L2 ~1. As discussed in Section 3.1, we consider the optimal solution to this problem as a distance measure named contextualized token mover’s distance (CMD):\nMCMDi,j := 1− cos ( x1i ,x 2 j ) ,\ndCMDM := ∑ i,j T∗i,jM CMD i,j .\nHowever, finding T∗ incurs huge computational complexity of O(L3 logL) where L = max(L1, L2) (Villani, 2008). For this reason, we relax the optimization problem by removing the first constraint, T>~1 = d′, similar to Kusner et al. (2015). The optimal solution for this relaxed transportation problem is found in O(L2), keeping the rank of the transportation matrix larger than one. In the end, the optimal transportation matrix and the corresponding distance named relaxed CMD (RCMD) are derived as follows:\nTRCMD1i,j =\n{ 1 L1 if j = argminj′M CMD i,j′\n0 otherwise,\ndRCMD1M := 1\nL1 ∑ i min j MCMDi,j . (4)\nSimilarly, the elimination of the second constraint, T~1 = d, results in TRCMD2 and dRCMD2M , where the solutions for the two relaxed problems use min operation on the cost matrix in a row-wise and a column-wise manner, respectively. Note that TRCMD1 represents the token-level binary alignment from the first sentence to the second sentence and accordingly the final distance is computed by averaging all the distances of the aligned token pairs. Also, it is obvious that TRCMD1 has a much higher rank than TAVG, which implies that it can express more complex token-level semantic relationship between two sentences.\nWe remark that our solution provides better interpretability of semantic textual similarity compared to the case of average pooling. For the sentence distance in Equation (3), TAVG assigns non-zero values to all token pairs that include irrelevant pairs;\nthis makes it difficult to interpret the result. On the contrary, TRCMD1 in Equation (4) is designed to explicitly involve the most relevant token pairs across sentences for the sentence distance, which allows us to interpret the result easily."
    }, {
      "heading" : "3.3 Contrastive sentence similarity learning with semantically-aligned token pairs",
      "text" : "We present a contrastive learning framework for RCMD (CLRCMD) that incorporates RCMD into the state-of-the-art contrastive learning framework. To this end, we convert RCMD to the corresponding similarity by sRCMD1M = 1− d RCMD1 M :\nsRCMD1M (s 1, s2) =\n1\nL1 L1∑ i=1 max j cos(x1i ,x 2 j ).\nsRCMD2M is computed in the same manner as well, and we average them to consider bidirectional semantic alignment between two sentences; this provides diverse gradient signals during optimization. The final similarity is described by\nsRCMDM (s 1, s2) :=\n1\n2\n( sRCMD1M (s 1, s2) + sRCMD2M (s 1, s2) ) .\nAdopting this similarity measure, the contrastive learning objective for the i-th sentence pair in a training batch is defined as follows:\n− log exp(sRCMDM (s i, si+)/τ)∑B j=1(exp(s RCMD M (s i, sj+)/τ) + exp(s RCMD M (s i, sj−)/τ)) ,\nwhere τ is the temperature parameter and B is the batch size. Following (Gao et al., 2021), CLRCMD uses the other sentences in the batch to generate negative pairs.\nWe argue that CLRCMD enhances both the sentence similarity and its interpretability in the following aspects. First, CLRCMD alleviates the catastrophic forgetting of pretrained semantics during the finetuning process. Its token-level supervision is produced by leveraging the textual semantics encoded in a pretrained checkpoint, because token pairs are semantically aligned according to their similarities in the contextualized embedding space. Namely, CLRCMD updates the parameters to improve the quality of sentence similarity while less breaking token-level semantics in the pretrained checkpoint. Furthermore, CLRCMD directly distills the relevance of a sentence pair into the relevance of semantically-aligned token pairs. In this sense, our contextualized embedding space effectively captures the token-level semantic relevance from training sentence pairs, which provides better interpretation for its sentence similarity."
    }, {
      "heading" : "4 Experiments",
      "text" : "To analyze our approach in various viewpoints, we design and conduct experiments that focus on the following three research questions:\n• RQ1 Does CLRCMD effectively measure sentence similarities using a pretrained language model?\n• RQ2 Does CLRCMD provide the interpretation of sentence similarity which is well aligned with human judgements?\n• RQ3 Does CLRCMD efficiently compute its sentence similarity for training and inference?"
    }, {
      "heading" : "4.1 Training details",
      "text" : "We finetune a pretrained model using CLRCMD in the following settings. Following previous work (Gao et al., 2021), we use NLI datasets with hard negatives: SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). We use a pretrained backbone attached with a single head, which is the same with (Gao et al., 2021). As the initial checkpoint of the pretrained models, we employ bert-base-uncased and roberta-base provided by huggingface (Devlin et al., 2019; Liu et al., 2019). Adam optimizer is used with the initial learning rate 5e− 5 and linear decay schedule. Fp16 training is enabled where the maximum batch size is 128 on a single V100 GPU, and the softmax temperature is set to τ = 0.05 (Gao et al., 2021). The training is proceeded with 4 different random seeds and the best model is chosen using the best\nSpearman correlation on STSb validation set which is evaluated every 250 steps during training."
    }, {
      "heading" : "4.2 Semantic textual similarity",
      "text" : "We evaluate the similarity model finetuned by CLRCMD for STS task to quantitatively measure the quality of sentence similarity (RQ1).\nMetric We measure Spearman correlation for each of seven STS benchmarks and calculate their average to compare the capability of representing sentences in general (Conneau and Kiela, 2018).\nBaselines We select the baselines that leverage a pretrained model, and they turn out to outperform other traditional baselines. We only list the baseline names for BERTbase below; the names for RoBERTabase are obtained by replacing BERTbase with RoBERTabase.\n• BERTbase-avg generates sentence embeddings by averaging the token embeddings from BERTbase without finetuning. It indicates zero-shot performance of a checkpoint.\n• SBERTbase (Reimers and Gurevych, 2019) is a pioneering work to finetune a pretrained model for sentence embeddings. It trains a Siamese network using NLI datasets.\n• SimCSEcls-BERTbase (Gao et al., 2021) adopts a contrastive learning framework (Chen et al., 2020) using SNLI and MNLI datasets. The contextualized embedding of [CLS] is used as a sentence embedding.\n• SimCSEavg-BERTbase (Gao et al., 2021) is the same with SimCSEcls-BERTbase except that it performs average pooling on token embeddings to obtain a sentence embedding.\nResult Table 1 reports Spearman correlation for each dataset and their average. For most of the datasets, CLRCMD shows higher correlation compared to the state-of-the-art baselines. In particular, for STS14, STS15, SICK-R datasets, CLRCMD-BERTbase achieves comparable performance to SimCSEcls-RoBERTabase whose backbone language model is pretrained with 10 times larger data compared to BERTbase. This implies that finetuning with token-level supervision from CLRCMD achieves the performance as good as using an expensively pretrained checkpoint."
    }, {
      "heading" : "4.3 Interpretable semantic textual similarity",
      "text" : "Next, we measure the performance of our approach on interpretable STS (iSTS) tasks in order to validate that CLRCMD embeds a sufficient level of interpretability even without any supervision (i.e., labeled training data) about semantically-aligned chunk pairs (RQ2).\nExperimental setup We utilize the “images” and “headlines” data sources included in SemEval2016 Task 2: iSTS (Agirre et al., 2016). We measure the agreement between human judgement (gold semantic alignment across sentences) and the contributions of all token pairs to sentence similarity (element-wise multiplication of (1 −M) and T). One challenge to use our similarity model for this task is to convert token pair contributions into chunk-level alignment. First, we summarize token pair contributions into chunk pair contributions by applying simple average pooling based on the chunk mapping represented by c(i) = {k|is_overlap(ci, tk)}, where ci is the i-th chunk and tk is the k-th token in a sentence.1 Then, to obtain the alignment based on the pairwise chunk contributions, we design a criterion for selecting confident chunk pairs (i, j) as follows:\nCi,j = 1\n|c1(i)||c2(j)| c1(i)∑ k c2(j)∑ l Tk,lMk,l,\na(i, j) = I[j = argmax j′ Ci,j′ ] · I[i = argmax i′ Ci′,j ].\n1We use gold standard chunking information to focus on alignment only, which is the second subtrack in iSTS.\nUsing the aligned chunk pairs obtained by each method, we compute the alignment F1 score as the evaluation metric, which indicates the agreement between human judgement and chunk contribution.2 We consider eight different configurations to investigate the effectiveness of the following components: 1) sentence similarity, 2) contrastive learning, and 3) pretrained checkpoints.\nResult Table 2 shows the clear tendency of iSTS performance with respect to each of the above components. First of all, the token pair contribution from RCMD is more consistent with human judgement than that from average pooling. RCMD improves alignment F1 scores even without finetuning (BERTbase-RCMD and RoBERTabaseRCMD), indicating that RCMD effectively discovers the token-level relevance encoded inside a pretrained checkpoint. In addition, the alignment F1 score increases when we finetune a model using CLRCMD. Notably, CLRCMD-BERTbase successfully improves the alignment F1 score whereas SimCSEavg-BERTbase does not. This result shows that finetuning a model using the similarity measure based on semantically-aligned token pairs (i.e., fine-grained supervision induced by RCMD) further enhances the interpretability of a model."
    }, {
      "heading" : "4.4 Qualitative analysis",
      "text" : "We qualitatively analyze the sentence similarity from the perspective of the transportation problem in order to demonstrate that a model trained by CLRCMD provides clear and accurate explanation (RQ2). To this end, we visualize the contribution of token pairs obtained from CLRCMD-BERTbase\n2We employ alignment F1 score implemented in the evaluation script provided by the task organizer.\nand that from SimCSEavg-BERTbase, and then clarify how their sentence similarity is computed differently from each other. Three sentence pairs are randomly selected from STS13 dataset. Figure 2 illustrates the token pair contribution heatmap for positive, neutral, and negative sentence pairs.\nCLRCMD vs. SimCSEavg Overall, CLRCMD aligns two sentences better than the baseline. To be specific, CLRCMD effectively highlights the contributions of semantically-relevant token pairs and excludes the other contributions (Figure 2 upper). On the contrary, SimCSEavg fails to represent meaningful token-level relevance for sentence similarity (Figure 2 lower). The rank-1 constraint of SimCSEavg prevents the model from getting any plausible alignment between two sentences, while it simply tunes the contributions of all possible token pairs at once. We emphasize that the superfluous correlation in the heatmap not only inhibits the capability to capture sentence similarity, but also makes it difficult for humans to understand how sentence similarity is computed.\nCase study on positive, neutral, and negative sentence pairs For the positive pair (Figure 2 left), CLRCMD clearly matches all semanticallyaligned token pairs including the linking words ({“,”, “and”}–{“and”}), synonyms ({“realize”,\n“comprehend”}–{“comprehend”}), and omitted contexts ({“the”}–{“the nature of”, “of”}). For the neutral pair (Figure 2 middle), the two sentences have the same lexical structure except for the date. In this case, CLRCMD assigns low contributions to the token pairs about day and month ({“25”, “august”}–{“19”, “july”}), while keeping the contributions high for all the other pairs of identical tokens. Therefore, end-users can clearly figure out which part is semantically different based on their contributions as well as alignment. In case of the negative pair (Figure 2 right), both the models are not able to find any plausible alignment; CLRCMD lowers contributions for most of the token pairs except the token pair with identical contents (“after riots”). That is, end-users also can interpret the negative pair based on the heatmap where semantic correspondence between two sentences does not clearly exist but few overlapped tokens highly contribute to the sentence similarity."
    }, {
      "heading" : "4.5 Resource evaluation",
      "text" : "We measure GPU memory usage and inference time of CLRCMD-BERTbase to demonstrate that CLRCMD can be executed on a single GPU and an inference of our model takes almost the same cost to that of the baseline (RQ3)."
    }, {
      "heading" : "4.5.1 GPU memory usage analysis",
      "text" : "Implementation of RCMD We implement two variants of RCMD, RCMDdense and RCMDsparse, to investigate the effect of exploiting the sparseness in RCMD. Both of them calculate sentence distance by the sum of element-wise multiplications of the cost matrix and the transportation matrix. For an input sentence pair, RCMDdense maintains the full pairwise token distances (MCMD), whereas RCMDsparse only keeps the token distances at which the transportation matrix has nonzero values ({MCMDi,j |TCMDi,j 6= 0}). Note that the number of nonzero entries in the transportation matrix of RCMD is at most 2L, which is an order of magnitude smaller than the number of all entries, L2.\nResult Table 3 reports the GPU memory usage during the finetuning process. For batch-wise contrastive learning, GPU memory requirement becomesO(B2) in terms of the batch sizeB, because all pairwise sentence similarities within a batch need to be computed. In this situation, RCMDdense using a dense matrix drastically increases GPU memory usage by O(B2L2), and as a result, the batch size cannot grow larger than 32. In contrast, RCMDsparse successfully enlarges the batch size up to 128 by exploiting sparseness in the transportation matrix of RCMD, which eventually reduces the space complexity to O(B2L)."
    }, {
      "heading" : "4.5.2 Inference time analysis",
      "text" : "Experimental setup We measure the time for predicting the similarities of 512 sentence pairs on a single V100 GPU while increasing the sequence length from 8 to 128, which is the most influential factor for inference time. We repeat this process 10 times and report the average values.\nResult Figure 3 shows the average elapsed time for inference. The model with RCMD takes almost the same inference time as the model with the simple average pooling-based similarity. We highlight that 98% of the sentences in STS13 dataset consist of at most 48 tokens and particularly, the time difference is negligible in case of predicting the\nsentence pairs whose sentences have less than 48 tokens. This result shows that significant increment of inference time does not occur within the range of the sequence length owing to parallel GPU computations, even though RCMD has the quadratic time complexity with respect to the sentence length."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we present CLRCMD, a learning framework for an interpretable sentence similarity model based on optimal transport. First, we view each sentence similarity measure as a transportation problem, pointing out the unexpressiveness of the existing pooling-based similarity. Integrating the concept of optimal transport into a pretrained language model, CLRCMD defines the distance measure by using the semantically-aligned token pairs between two sentences and furthermore, it finetunes a model with this distance based on contrastive learning for better interpretability. We empirically show that CLRCMD accurately predicts sentence similarity while providing interpretable token pair contributions consistent with human judgements. With the belief that the ability to interpret model behavior is critical for future AI models, we focus on enhancing this virtue targeted on STS task throughout this research."
    } ],
    "references" : [ {
      "title" : "SemEval-2016 task 2: Interpretable semantic textual similarity",
      "author" : [ "Eneko Agirre", "Aitor Gonzalez-Agirre", "Iñigo LopezGazpio", "Montse Maritxalar", "German Rigau", "Larraitz Uria." ],
      "venue" : "Proceedings of the 10th International Workshop on Seman-",
      "citeRegEx" : "Agirre et al\\.,? 2016",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2016
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "SentEval: An evaluation toolkit for universal sentence representations",
      "author" : [ "Alexis Conneau", "Douwe Kiela." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language",
      "citeRegEx" : "Conneau and Kiela.,? 2018",
      "shortCiteRegEx" : "Conneau and Kiela.",
      "year" : 2018
    }, {
      "title" : "Sinkhorn distances: Lightspeed computation of optimal transport",
      "author" : [ "Marco Cuturi." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.",
      "citeRegEx" : "Cuturi.,? 2013",
      "shortCiteRegEx" : "Cuturi.",
      "year" : 2013
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2104.08821.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Explaining explanations: An overview of interpretability of machine learning",
      "author" : [ "Leilani H Gilpin", "David Bau", "Ben Z Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal." ],
      "venue" : "2018 IEEE 5th International Conference on data science and advanced an-",
      "citeRegEx" : "Gilpin et al\\.,? 2018",
      "shortCiteRegEx" : "Gilpin et al\\.",
      "year" : 2018
    }, {
      "title" : "A survey of text similarity approaches. international journal of Computer Applications, 68(13):13–18",
      "author" : [ "Wael H Gomaa", "Aly A Fahmy" ],
      "venue" : null,
      "citeRegEx" : "Gomaa and Fahmy,? \\Q2013\\E",
      "shortCiteRegEx" : "Gomaa and Fahmy",
      "year" : 2013
    }, {
      "title" : "Fast contour matching using approximate earth mover’s distance",
      "author" : [ "Kristen Grauman", "Trevor Darrell." ],
      "venue" : "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., volume 1, pages I–",
      "citeRegEx" : "Grauman and Darrell.,? 2004",
      "shortCiteRegEx" : "Grauman and Darrell.",
      "year" : 2004
    }, {
      "title" : "Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring",
      "author" : [ "Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Humeau et al\\.,? 2020",
      "shortCiteRegEx" : "Humeau et al\\.",
      "year" : 2020
    }, {
      "title" : "UWB at SemEval-2016 task 2: Interpretable semantic textual similarity with distributional semantics for chunks",
      "author" : [ "Miloslav Konopík", "Ondřej Pražák", "David Steinberger", "Tomáš Brychcín." ],
      "venue" : "Proceedings of the 10th International Workshop on Seman-",
      "citeRegEx" : "Konopík et al\\.,? 2016",
      "shortCiteRegEx" : "Konopík et al\\.",
      "year" : 2016
    }, {
      "title" : "From word embeddings to document distances",
      "author" : [ "Matt Kusner", "Yu Sun", "Nicholas Kolkin", "Kilian Weinberger." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Kusner et al\\.,? 2015",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving text generation with student-forcing optimal trans",
      "author" : [ "Jianqiao Li", "Chunyuan Li", "Guoyin Wang", "Hao Fu", "Yuhchen Lin", "Liqun Chen", "Yizhe Zhang", "Chenyang Tao", "Ruiyi Zhang", "Wenlin Wang", "Dinghan Shen", "Qian Yang", "Lawrence Carin" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "iUBC at SemEval-2016 task 2: RNNs and LSTMs for interpretable STS",
      "author" : [ "Iñigo Lopez-Gazpio", "Eneko Agirre", "Montse Maritxalar." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 771–776, San Diego,",
      "citeRegEx" : "Lopez.Gazpio et al\\.,? 2016",
      "shortCiteRegEx" : "Lopez.Gazpio et al\\.",
      "year" : 2016
    }, {
      "title" : "Logic constrained pointer networks for interpretable textual similarity",
      "author" : [ "Subhadeep Maji", "Rohan Kumar", "Manish Bansal", "Kalyani Roy", "Pawan Goyal." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelli-",
      "citeRegEx" : "Maji et al\\.,? 2020",
      "shortCiteRegEx" : "Maji et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantic textual similarity methods, tools, and applications: A survey",
      "author" : [ "Goutam Majumder", "Partha Pakray", "Alexander Gelbukh", "David Pinto." ],
      "venue" : "Computación y Sistemas, 20(4):647–665.",
      "citeRegEx" : "Majumder et al\\.,? 2016",
      "shortCiteRegEx" : "Majumder et al\\.",
      "year" : 2016
    }, {
      "title" : "Mémoire sur la théorie des déblais et des remblais",
      "author" : [ "Gaspard Monge." ],
      "venue" : "Histoire de l’Académie Royale des Sciences de Paris.",
      "citeRegEx" : "Monge.,? 1781",
      "shortCiteRegEx" : "Monge.",
      "year" : 1781
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "Termweighting approaches in automatic text retrieval",
      "author" : [ "Gerard Salton", "Christopher Buckley." ],
      "venue" : "Information processing & management, 24(5):513– 523.",
      "citeRegEx" : "Salton and Buckley.,? 1988",
      "shortCiteRegEx" : "Salton and Buckley.",
      "year" : 1988
    }, {
      "title" : "Approximate earth mover’s distance in linear time",
      "author" : [ "Sameer Shirdhonkar", "David W Jacobs." ],
      "venue" : "2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8. IEEE.",
      "citeRegEx" : "Shirdhonkar and Jacobs.,? 2008",
      "shortCiteRegEx" : "Shirdhonkar and Jacobs.",
      "year" : 2008
    }, {
      "title" : "DLS@CU: Sentence similarity from word alignment and semantic vector composition",
      "author" : [ "Md Arafat Sultan", "Steven Bethard", "Tamara Sumner." ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 148–153,",
      "citeRegEx" : "Sultan et al\\.,? 2015",
      "shortCiteRegEx" : "Sultan et al\\.",
      "year" : 2015
    }, {
      "title" : "Rationalizing text matching: Learning sparse alignments via optimal transport",
      "author" : [ "Kyle Swanson", "Lili Yu", "Tao Lei." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5609–5626, Online. Association",
      "citeRegEx" : "Swanson et al\\.,? 2020",
      "shortCiteRegEx" : "Swanson et al\\.",
      "year" : 2020
    }, {
      "title" : "IISCNLP at SemEval-2016 task 2: Interpretable STS with ILP based multiple chunk aligner",
      "author" : [ "Lavanya Tekumalla", "Sharmistha Jat." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 790–795, San",
      "citeRegEx" : "Tekumalla and Jat.,? 2016",
      "shortCiteRegEx" : "Tekumalla and Jat.",
      "year" : 2016
    }, {
      "title" : "Optimal Transport: Old and New, volume 338",
      "author" : [ "Cédric Villani." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Villani.,? 2008",
      "shortCiteRegEx" : "Villani.",
      "year" : 2008
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Word mover’s embedding: From Word2Vec to document embedding",
      "author" : [ "Lingfei Wu", "Ian En-Hsu Yen", "Kun Xu", "Fangli Xu", "Avinash Balakrishnan", "Pin-Yu Chen", "Pradeep Ravikumar", "Michael J. Witbrock." ],
      "venue" : "Proceedings of the 2018 Conference",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Clear: Contrastive learning for sentence representation",
      "author" : [ "Zhuofeng Wu", "Sinong Wang", "Jiatao Gu", "Madian Khabsa", "Fei Sun", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2012.15466.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Vocabulary learning via optimal transport for neural machine translation",
      "author" : [ "Jingjing Xu", "Hao Zhou", "Chun Gan", "Zaixiang Zheng", "Lei Li." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "ConSERT: A contrastive framework for self-supervised sentence representation transfer",
      "author" : [ "Yuanmeng Yan", "Rumei Li", "Sirui Wang", "Fuzheng Zhang", "Wei Wu", "Weiran Xu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Predicting the semantic similarity between two sentences has been extensively studied in the literature (Gomaa et al., 2013; Agirre et al., 2015; Majumder et al., 2016; Cer et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 186
    }, {
      "referenceID" : 3,
      "context" : "Predicting the semantic similarity between two sentences has been extensively studied in the literature (Gomaa et al., 2013; Agirre et al., 2015; Majumder et al., 2016; Cer et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "Several recent studies successfully utilized a pretrained language model such as BERT (Devlin et al., 2019) by finetuning it to capture sentence similarity (Reimers and Gurevych, 2019).",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : ", 2019) by finetuning it to capture sentence similarity (Reimers and Gurevych, 2019).",
      "startOffset" : 56,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : ", [CLS]), then optimize the score for natural language inference (NLI) or semantic textual similarity (STS) tasks (Gao et al., 2021).",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "In general, finding out the cross-sentence alignment and the importance of each aligned part is useful for analyzing sentence similarity (Sultan et al., 2015).",
      "startOffset" : 137,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : ", TF-IDF) for easily analyzing the interaction among the shared terms (Salton and Buckley, 1988) or to adopt sophisticated metrics (e.",
      "startOffset" : 70,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : ", word mover’s distance) for explicitly describing it by the importance and similarity of word pairs across two sentences (Kusner et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "Despite their effectiveness, the interpretability of the above models for STS tasks was not fully explored (Belinkov and Glass, 2019).",
      "startOffset" : 107,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "For this task, a variety of supervised approaches were proposed based on neural networks (Konopík et al., 2016; Lopez-Gazpio et al., 2016), linear programming (Tekumalla and Jat, 2016), and pretrained models (Maji et al.",
      "startOffset" : 89,
      "endOffset" : 138
    }, {
      "referenceID" : 17,
      "context" : "For this task, a variety of supervised approaches were proposed based on neural networks (Konopík et al., 2016; Lopez-Gazpio et al., 2016), linear programming (Tekumalla and Jat, 2016), and pretrained models (Maji et al.",
      "startOffset" : 89,
      "endOffset" : 138
    }, {
      "referenceID" : 27,
      "context" : ", 2016), linear programming (Tekumalla and Jat, 2016), and pretrained models (Maji et al.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 18,
      "context" : ", 2016), linear programming (Tekumalla and Jat, 2016), and pretrained models (Maji et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 20,
      "context" : "Optimal transport (Monge, 1781) has been successfully adopted in natural language processing due to its ability to find a plausible correspondence between two objects (Li et al.",
      "startOffset" : 18,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "Optimal transport (Monge, 1781) has been successfully adopted in natural language processing due to its ability to find a plausible correspondence between two objects (Li et al., 2020; Xu et al., 2021).",
      "startOffset" : 167,
      "endOffset" : 201
    }, {
      "referenceID" : 32,
      "context" : "Optimal transport (Monge, 1781) has been successfully adopted in natural language processing due to its ability to find a plausible correspondence between two objects (Li et al., 2020; Xu et al., 2021).",
      "startOffset" : 167,
      "endOffset" : 201
    }, {
      "referenceID" : 11,
      "context" : "One well-known limitation of optimal transport is that finding the optimal solution is computationally intensive, and thus approximation schemes for this problem have been extensively researched (Grauman and Darrell, 2004; Shirdhonkar and Jacobs, 2008).",
      "startOffset" : 195,
      "endOffset" : 252
    }, {
      "referenceID" : 24,
      "context" : "One well-known limitation of optimal transport is that finding the optimal solution is computationally intensive, and thus approximation schemes for this problem have been extensively researched (Grauman and Darrell, 2004; Shirdhonkar and Jacobs, 2008).",
      "startOffset" : 195,
      "endOffset" : 252
    }, {
      "referenceID" : 21,
      "context" : "Note that this similarity measure is widely adopted in most of the previous studies (Reimers and Gurevych, 2019; Wu et al., 2020; Gao et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 31,
      "context" : "Note that this similarity measure is widely adopted in most of the previous studies (Reimers and Gurevych, 2019; Wu et al., 2020; Gao et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Note that this similarity measure is widely adopted in most of the previous studies (Reimers and Gurevych, 2019; Wu et al., 2020; Gao et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 28,
      "context" : "However, finding T∗ incurs huge computational complexity of O(L3 logL) where L = max(L1, L2) (Villani, 2008).",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Following (Gao et al., 2021), CLRCMD uses the other sentences in the batch to generate negative pairs.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "We measure Spearman correlation on all the examples (Gao et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "Following previous work (Gao et al., 2021), we use NLI datasets with hard negatives: SNLI (Bowman et al.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : ", 2021), we use NLI datasets with hard negatives: SNLI (Bowman et al., 2015) and MNLI (Williams et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "We use a pretrained backbone attached with a single head, which is the same with (Gao et al., 2021).",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "As the initial checkpoint of the pretrained models, we employ bert-base-uncased and roberta-base provided by huggingface (Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "As the initial checkpoint of the pretrained models, we employ bert-base-uncased and roberta-base provided by huggingface (Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "Metric We measure Spearman correlation for each of seven STS benchmarks and calculate their average to compare the capability of representing sentences in general (Conneau and Kiela, 2018).",
      "startOffset" : 163,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "• SBERTbase (Reimers and Gurevych, 2019) is a pioneering work to finetune a pretrained model for sentence embeddings.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "• SimCSEcls-BERTbase (Gao et al., 2021) adopts a contrastive learning framework (Chen et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : ", 2021) adopts a contrastive learning framework (Chen et al., 2020) using SNLI and MNLI datasets.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "• SimCSEavg-BERTbase (Gao et al., 2021) is the same with SimCSEcls-BERTbase except that it performs average pooling on token embeddings to obtain a sentence embedding.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Experimental setup We utilize the “images” and “headlines” data sources included in SemEval2016 Task 2: iSTS (Agirre et al., 2016).",
      "startOffset" : 109,
      "endOffset" : 130
    } ],
    "year" : 0,
    "abstractText" : "Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-theart performance on the semantic textual similarity (STS) task. However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output. In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem, and then present the optimal transport-based distance measure, named RCMD; it identifies and leverages semantically-aligned token pairs. In the end, we propose CLRCMD, a contrastive learning framework that optimizes RCMD of sentence pairs, which enhances the quality of sentence similarity and their interpretation. Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks, indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement.",
    "creator" : null
  }
}
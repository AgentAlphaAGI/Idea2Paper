{
  "name" : "ARR_2022_18_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Few-shot Controllable Style Transfer for Low-Resource Multilinugal Settings",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work using automatic and human evaluations, our model achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across seven languages. Moreover, our method is better able to control the amount of style transfer using an input scalar knob. We report promising qualitative results for several attribute transfer directions, including sentiment transfer, text simplification, gender neutralization and text anonymization, all without retraining the model. Finally we found model evaluation to be difficult due to the lack of evaluation datasets and metrics for many languages. To facilitate further research in formality transfer for Indic languages, we crowdsource annotations for 4000 sentence pairs in four languages, and use this dataset1 to design our automatic evaluation suite."
    }, {
      "heading" : "1 Introduction",
      "text" : "Style transfer is a natural language generation task in which input sentences need to be re-written into a target style, while preserving semantics. It has many applications such as writing assistance (Heidorn, 2000), controlling generation for attributes\n1Dataset will be open-sourced on paper acceptance.\nlike simplicity, formality or persuasion (Xu et al., 2015; Smith et al., 2020; Niu and Carpuat, 2020), data augmentation (Xie et al., 2019; Lee et al., 2021), and author obfuscation (Shetty et al., 2018).\nMost prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018; Subramanian et al., 2019). Models built are style-specific and cannot generalize to new styles during inference, which is needed for applications like real-time adaptation to a user’s style in a dialog or writing application. Moreover, access to a large unpaired corpus with style labels is a strong assumption. Most standard “unpaired” style transfer datasets have been carefully curated (Shen et al., 2017) or were originally parallel (Xu et al., 2012; Rao and Tetreault, 2018). This is especially relevant in settings outside English, where NLP tools and labelled datasets are largely\nunderdeveloped (Joshi et al., 2020). In this work, we take the first steps studying style transfer in seven languages2 with nearly 1.5 billion speakers. Since no training data exists for these languages, we analyzed the current state-of-the-art in few-shot multilingual style transfer, the Universal Rewriter (UR) from Garcia et al. (2021). Unfortunately, we found it often copied input sentences verbatim (Section 3.1) without transferring their style.\nWe propose a simple inference-time trick of style-controlled translation through English, which improves the UR output diversity (Section 4.1). To further boost performance we propose DIFFUR,3 an algorithm using the recent finding that paraphrasing leads to stylistic changes (Krishna et al., 2020). DIFFUR extracts edit vectors from paraphrase pairs, which are used to condition and train the model (Figure 2). On formality transfer and code-mixing addition, our best performing DIFFUR variant significantly outperforms UR across all languages (by 2-3x) using automatic & human evaluation. Besides better rewriting, our system is better able to control the style transfer magnitude (Figure 1). A scalar knob (λ) can be adjusted to make the output text reflect the target style (provided by exemplars) more or less. We also observe promising qualitative results in several attribute transfer directions (Section 6) including sentiment transfer, simplification, gender neutralization and text anonymization, all without retraining the model and using just 3-10 examples at inference.\nFinally, we found it hard to precisely evaluate models due to the lack of evaluation datasets and style classifiers (often used as metrics) for many languages. To facilitate further research in Indic formality transfer, we crowdsource formality annotations for 4000 sentence pairs in four Indic languages (Section 5.1), and use this dataset to design the automatic evaluation suite (Section 5). In summary, our contributions provide an end-toend recipe for developing and evaluating style transfer models and evaluation in a low-resource setting."
    }, {
      "heading" : "2 Related Work",
      "text" : "Few-shot methods are a recent development in English style transfer, with prior work using variational autoencoders (Xu et al., 2020), or prompting large pretrained language models at inference (Reif et al., 2021). Most related is the state-of-the-art\n2Indic (hi,bn,kn,gu,te), Spanish, Swahili. 3“Difference Universal Rewriter”, pronounced as differ.\nTextSETTR model from Riley et al. (2021), who use a neural style encoder to map exemplar sentences to a vector used to guide generation. To train this encoder, they use the idea that adjacent sentences in a document have a similar style. Recently, the Universal Rewriter (Garcia et al., 2021) extended TextSETTR to 101 languages, developing a joint model for translation, few-shot style transfer and stylized translation. This model is the only prior few-shot system we found outside English, and our main baseline. We discuss its shortcomings in Section 3.1, and propose fixes in Section 4. Multilingual style transfer is mostly unexplored in prior work: a 35 paper survey by Briakou et al. (2021b) found only one work in Chinese, Russian, Latvian, Estonian, French. They further introduced XFORMAL, the first formality transfer evaluation dataset in French, Brazilian Portugese and Italian.4 To the best of our knowledge, we are the first to study style transfer for the languages we consider. More related work from Hindi linguistics and on style transfer control is provided in Appendix B."
    }, {
      "heading" : "3 The Universal Rewriter (UR) model",
      "text" : "We will start by discussing the Universal Rewriter (UR) model from Garcia et al. (2021), upon which our proposed DIFFUR model is built. The UR model extracts a style vector s from an exemplar sentence e, which reflects the desired target style. This style vector is used to style transfer an input sentence x. Consider fenc, fdec to be encoder & decoder Transformers initialized with mT5 (Xue et al., 2021b), which are composed to form the model fur.\nfstyle(e) = s = fenc([CLS]⊕ e)[0] fur(x, s) = fdec(fenc(x) + s)\nwhere ⊕ is string concatenation, + vector addition. fur is trained using the following objectives,\nLearning Style Transfer by Exemplar-driven Denoising: To learn a style extractor, the Universal Rewriter uses the idea that two non-overlapping spans of text in the same document are likely to have the same style. Concretely, let x1 and x2 be two non-overlapping spans in mC4. Style extracted from one span is used to denoise the other,\nx̄2 = fur(noise(x2), fstyle(x1))\nLdenoise = LCE(x̄2, x2) 4We do not use this data since it does not cover Indian languages, and due to Yahoo! L6 corpus restrictions for industry researchers (confirmed via authors correspondence).\nwhere LCE is the standard next-word prediction cross entropy loss function and noise(·) refers to 20-60% random token dropping and token replacement. This objective is used on the mC4 dataset (Xue et al., 2021b) with 101 languages. To build a general-purpose rewriter which can do translation as well as style transfer, the model is additionally trained on two objectives: (1) supervised machine translation using the OPUS-100 parallel dataset (Zhang et al., 2020), and (2) a self-supervised objective to learn effective stylecontrolled translation; more details in Appendix C.\nDuring inference (Figure 1), consider an input sentence x and a transformation from style A to B (say informal to formal). Let SA, SB to be exemplar sentences in each of the styles (typically 3-10 sentences). The output y is computed as,\nsA, sB = 1\nN ∑ y∈SA, SB fstyle(y)\ny = fur(x, λ(sB − sA))\nwhere λ acts as a control knob to determine the magnitude of style transfer, and the vector subtraction helps remove confounding style information.5"
    }, {
      "heading" : "3.1 Shortcomings of the Universal Rewriter",
      "text" : "We experimented with the UR model on Hindi formality transfer, and noticed poor performance. We noticed that UR has a strong tendency to copy sentences verbatim — 45.5% outputs were copied exactly from the input (and hence not style transferred) for the best performing value of λ. The copying increase for smaller λ, making magnitude control harder. We identify the following issues: 1. Random token noise leads to unnatural inputs & transformations: The Universal Rewriter uses 20-60% uniformly random token dropping / replacement to noise inputs, which leads to ungrammatical inputs during training. We hypothesize models tend to learn grammatical error correction, which encourages verbatim copying during inference where fluent inputs are used and no error correction is needed. Moreover, token-level noise does not differentiate between content / function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020). Too much noise could distort semantics and encourage hallucination, whereas too little will encourage copying.\n5Garcia et al. (2021) also recommend adding the style vectors from the input sentence x, but we found this increased the amount of verbatim copying and led to poor performance.\n2. Style vectors may not capture the precise style transformation: The Universal Rewriter extracts the style vector from a single sentence during training, which is a mismatch from the inference where a difference between vectors is taken. Without taking vector differences at inference, we observe semantic preservation and overall performance of the UR model is much lower.6\n3. mC4 is noisy: On reading training data samples, we noticed noisy samples with severe language identification errors in the Hindi subset of mC4. This has also been observed recently in Caswell et al. (2021), who audit 100 sentences in each language, and report 50% sentences in Marathi and 20% sentences in Hindi have the wrong language."
    }, {
      "heading" : "4. No translation data for several languages:",
      "text" : "We notice worse performance for languages which did not get parallel translation data (for the translation objective in Section 3). In Table 1 we see UR gets a score7 of 30.4 for Hindi and Bengali, languages for which it got translation data. However, the scores are lower for Kannada, Telugu & Gujarati (25.5, 22.8, 23.7), for which no translation data was used. We hypothesize translation data encourages learning language-agnostic semantic representations needed for translation from the given language, which in-turn improves style transfer."
    }, {
      "heading" : "4 Our Models",
      "text" : ""
    }, {
      "heading" : "4.1 Style-Controlled Backtranslation (+ BT)",
      "text" : "While the Universal Rewriter model has a strong tendency to exactly copy input sentences while rewriting sentences in the same language (Section 3.1), we found it is an effective style-controlled translation system. This motivates a simple inference-time trick to improve model outputs and reduce copying — translate sentences to English (en) in a style-agnostic manner with a zero style vector 0, and translate back into the source language (lx) with stylistic control.\nsA, sB = 1\nN ∑ y∈SA, SB fstyle(y)\nxen = fur(en⊕ x,0) x̄ = fur(lx⊕ xen, λ(sB − sA))\n6This difference possibly helps remove confounding information (like semantic properties, other styles) and focus on the specific style transformation. Since two spans in the same document will share aspects like article topic / subject along with style, we expect these semantic properties will confound the style vector space obtained after the UR training.\n7Using the r-AGG style transfer metric from Section 5.5.\nwhere x is the input sentence, SA, SB are exemplars of the styles we want to transfer between, en, lx are language codes prepended to indicate the output language (Appendix C). Prior work has shown that backtranslation is effective for paraphrasing (Wieting and Gimpel, 2018; Iyyer et al., 2018) and style transfer (Prabhumoye et al., 2018)."
    }, {
      "heading" : "4.2 Using Paraphrase Vector Differences for Style Transfer (DIFFUR)",
      "text" : "While style-controlled backtranslation is an effective strategy, it needs two translation steps. This is 2x slower than UR, and semantic errors increase with successive translations. To learn effective style transfer systems needing only a single generation step we develop DIFFUR, a new few-shot style transfer training objective (overview in Figure 2). DIFFUR tackles the issues discussed in Section 3.1 using paraphrases and style vector differences.\nParaphrases as a “noise” function: Instead of using random token-level noise (issue #1 in Section 3.1), we paraphrase sentences to “noise” them during training. Paraphrasing modifies the lexical & syntactic properties of sentences, while preserving fluency and input semantics. Prior work (Krishna et al., 2020) has shown that paraphrasing leads to stylistic changes, and denoising can be considered a style re-insertion process.\nTo create paraphrases, we backtranslate sentences from the UR model8 with no style control (zero vectors used as style vectors). To increase\n8Specifically, an Indic variant of the UR model is used, described in Section 4.3. Note it is not necessary to use UR for backtranslation, any good translation model can be used.\ndiversity, we use random sampling in both translation steps, pooling generations obtained using temperature values [0.4, 0.6, 0.8, 1.0]. Finally, we discard paraphrase pairs from the training data where the semantic similarity score9 is outside the range [0.7, 0.98]. This removes backtransation errors (score < 0.7), and exact copies (score > 0.98).\nUsing style vector differences for control: To fix the training / inference mismatch for style extraction (issue #2 in Section 3.1), we propose using style vector differences between the output and input as the stylistic control. Concretely, let x be an input sentence and xpara its paraphrase.\nsdiff = fstyle(x)− fstyle(xpara) x̄ = fur(xpara, stop-grad(sdiff))\nL = LCE(x̄, x)\nwhere stop-grad(·) stops gradient flow through sdiff, preventing the model from learning to copy x exactly. To ensure fstyle extracts meaningful style representations, we fine-tune a trained UR model. Vector differences have many advantages,\n1. Subtracting style vectors between a sentence and its paraphrase removes confounding features (like semantics) present in the vectors.\n2. The vector difference focuses on the precise transformation that is needed to reconstruct the input from its paraphrase.\n3. The length of sdiff acts as a proxy for the amount of style transfer, which is controlled using λ during inference (Section 3).\n9Calculated using LaBSE, discussed in Section 5.3.\nDIFFUR is related to neural editor models (Guu et al., 2018; He et al., 2020), where language models are decomposed into a probabilistic space of edit vectors over prototype sentences. We justify the DIFFUR design with ablations in Appendix G.1."
    }, {
      "heading" : "4.3 Indic Models (UR-INDIC, DIFFUR-INDIC)",
      "text" : "To address the issue of no translation data (issue #4 in Section 3.1), we train Indic variants of our models. We replace the OPUS translation data used for training the Universal Rewriter (Section 3) with Samanantar (Ramesh et al., 2021), which is the largest publicly available parallel translation corpus for 11 Indic languages. We call these variants UR-INDIC and DIFFUR-INDIC. This process significantly up-samples the parallel data seen between English / Indic languages, and gives us better performance (Table 1) and lower copy rates, especially for languages with no OPUS translation data."
    }, {
      "heading" : "4.4 Multitask Learning (DIFFUR-MLT)",
      "text" : "One issue with our DIFFUR-INDIC setup is usage of a stop-grad(·), to avoid verbatim copying from the input. This prevents gradient flow into the style extractor fstyle, and as we see in Appendix H, a degradation of the style vector space. To prevent this from happening, we simply do multi-task learning between the original Universal Rewriter objective (Section 3) and our DIFFUR-INDIC objective, using an equal number of minibatches for each objective."
    }, {
      "heading" : "5 Evaluation",
      "text" : "Automatic evaluation of style transfer is challenging (Pang, 2019; Mir et al., 2019; Tikhonov et al., 2019), and the lack of resources (such as evaluation datasets, style classifiers) make evaluation trickier for Indic languages. To tackle this issue, we first collect a small dataset of formality and semantic similarity annotations in four Indic languages (Section 5.1). We use this dataset to guide the design of an evaluation suite (Section 5.2-5.6). Since automatic metrics in generation are imperfect (Celikyilmaz et al., 2020), we complement our results with human evaluation (Section 5.7)."
    }, {
      "heading" : "5.1 Indic Formality Transfer Dataset",
      "text" : "Since no public datasets exist for formality transfer in Indic languages, it is hard to measure the extent to which automatic metrics (such as style classifiers) are effective. To tackle this issue, we build a dataset of 1000 sentence pairs in each of four\nIndic languages (Hindi, Bengali, Kannada, Telugu) with formality and semantic similarity annotations. We first style transfer held-out Samanantar sentences using our UR-INDIC + BT model (Section 4.1, 4.3) to create sentence pairs with different formality. We then asked three crowdworkers to 1) label the more formal sentence in each pair; 2) rate semantic similarity on a 3-point scale.\nOur crowdsourcing is conducted on Task Mate,10 where we hired native speakers from India with at least a high school education and 90% approval rating on the platform. To ensure crowdworkers understood “formality”, we provided instructions following advice from professional Indian linguists, and asked two qualification questions in their native language. More details (agreement, compensation, instructions) are provided in Appendix E.4."
    }, {
      "heading" : "5.2 Transfer Accuracy (r-ACC, a-ACC)",
      "text" : "Our first metric checks whether the output sentence reflects the target style. This is measured by an external classifier’s predictions on system outputs. We use two variants of transfer accuracy: (1) Relative Accuracy (r-ACC): does the target style classifier score the output sentence higher than the input sentence? (2) Absolute Accuracy (a-ACC): does the classifier score the output higher than 0.5? Building multilingual classifiers: Unfortunately, no large style classification datasets exist for most languages, preventing us from building classifiers from scratch. We resort to zero-shot cross lingual transfer techniques (Conneau and Lample, 2019), where large multilingual pretrained models are first fine-tuned on English classification data, and then applied to other languages at inference. We experiment with three such techniques, and find MAD-X classifiers with language adapters (Pfeiffer et al., 2020b) have the highest accuracy of 81% on our Hindi data from Section 5.1. However, MAD-X classifiers were only available for Hindi, so we use the next best XLM RoBERTa-base (Conneau et al., 2020) for other languages, which has 75%-82% accuracy on annotated data; details in Appendix E.1."
    }, {
      "heading" : "5.3 Semantic Similarity (SIM)",
      "text" : "Our second evaluation criteria is semantic similarity between the input and output. Following recent recommendations (Marie et al., 2021; Krishna et al., 2020), we avoid n-gram overlap metrics like BLEU (Papineni et al., 2002). Instead, we use\n10https://taskmate.google.com\nLaBSE (Feng et al., 2020), a language-agnostic semantic similarity model based on multilingual BERT (Devlin et al., 2019). LaBSE supports 109 languages, and is the only similarity model we found supporting all the Indic languages in this work. We also observed LaBSE had greater correlation with our annotated data (Section 5.1) compared to alternatives; details in Appendix E.2.\nQualitatively, we found that sentence pairs with LaBSE scores lower than 0.6 were almost never paraphrases. To avoid rewarding partial credit for low LaBSE scores, we use a hard threshold11 (L = 0.75) to determine whether pairs are paraphrases,\nSIM(x, y′) = 1 if { LaBSE(x, y′) > L } else 0"
    }, {
      "heading" : "5.4 Other Metrics (LANG, COPY, 1-g)",
      "text" : "Additionally, we measure whether the input and output sentences are in the same language (LANG), the fraction of outputs copied verbatim from the input (COPY), and the 1-gram overlap between input / output (1-g). High LANG and low COPY / 1-g (more diversity) is better; details in Appendix E.6."
    }, {
      "heading" : "5.5 Aggregated Score (r-AGG, a-AGG)",
      "text" : "To get a sense of overall system performance, we combine individual metrics into one score. Similar to Krishna et al. (2020) we aggregate metrics as,\nAGG(x, y′) = ACC(x, y′) · SIM(x, y′) · LANG(y′)\nAGG(D) = 1 |D| ∑ x,y′∈D AGG(x, y′)\nWhere (x, y′) are input-output pairs, and D is the test corpus. In other words, we measure the fraction of outputs which simultaneously transfer style, have a semantic similarity of at least L (our threshold in Section 5.3), and have the same language as the input. Depending on the variant of ACC (relative / absolute), we can derive r-AGG / a-AGG."
    }, {
      "heading" : "5.6 Evaluating Control (CALIB)",
      "text" : "An ideal system should not only be able to style transfer sentences, but also control the magnitude of style transfer using the scalar input λ. To evaluate this, for every system we first determine a λmax value and let [0, λmax] be the range of control values. While in our setup λ is an unbounded scalar, we noticed high values of λ significantly perturb\n11Roughly 73% pairs annotated as paraphrases (from dataset in Section 5.1) had L > 0.75. We experiment with different values of L in Appendix E.3 and notice similar trends.\nsemantics (also noted in Garcia et al., 2021), with systems outputting style-specific n-grams unfaithful to the output. We choose λmax to be the largest λ from the list [0.5, 1.0, 1.5, 2.0, 2.5, 3.0] whose outputs have an average semantic similarity score (SIM, Section 5.3) of at least 0.7512 with the validation set inputs. For each system we take three evenly spaced λ values in its control range, denoted as Λ = [13λmax, 2 3λmax, λmax]. We then compute the style calibration to λ (CALIB), or how often does increasing λ lead to a style score increase? We measure this with a statistic similar to Kendall’s τ (Kendall, 1938), counting concordant pairs in Λ,\nCALIB(x) = 1\nn ∑ λb>λa {style(yλb) > style(yλa)}\nwhere x is input, CALIB(x) is the average over all possible n (= 3) pairs of λ values (λa, λb) in Λ."
    }, {
      "heading" : "5.7 Human Evaluation",
      "text" : "Automatic metrics are usually insufficient for style transfer evaluation — according to Briakou et al. (2021a), 69 / 97 surveyed style transfer papers used human evaluation. We adopt the crowd-sourcing setup from Section 5.1, which was used to build our formality evaluation datasets. We presented 200 generations from each model and the corresponding inputs in a random order, and asked three crowdworkers two questions about each pair of sentences: (1) which sentence is more formal/codemixed? (2) how similar are the two sentences in meaning? This lets us evaluate r-ACC, SIM, r-AGG, CALIB with respect to human annotations instead of classifier predictions; details in Appendix E.4."
    }, {
      "heading" : "6 Main Experiments",
      "text" : "We evaluate models on (1) formality transfer; (2) increasing the amount of code-mixing with English. Seven languages with varying scripts and morphological richness are used for evaluation (hi,es,sw,bn,kn,te,gu). Note that no paired/unpaired data with style labels is used during training: models determine the target style at inference using 3-10 exemplars sentences. For few-shot formality transfer, we use the English exemplars from Garcia et al. (2021). We follow their setup and use English exemplars to guide nonEnglish transfer zero-shot. For code-mixing addition, we use Hindi/English code-mixed exemplars\n12This threshold is identical to the value chosen for paraphrase similarity in Section 5.3. We experiment with more/less conservative thresholds in Appendix E.3.\nin Devanagari (shown in Appendix D); more details of our training & evaluation setup in Appendix A. Each proposed method improves over prior work, DIFFUR-MLT works best. We present our automatic evaluation results for formality transfer across languages in Table 1, Table 2. Overall we find that each of our proposed methods (DIFFUR,\n*-INDIC, +BT) help improve performance over the baseline UR model (71.1, 58.3, 54.2 vs 30.4 rAGG on Hindi). Combining these ideas with multitask learning (DIFFUR-MLT) gives us the best performance of across all languages (78.1 on Hindi). On Gujarati, the DIFFUR model fail to get good performance (0.4, 36.0 r-AGG) since they did not see Gujarati paraphrase data (Appendix A), but this performance is recovered using DIFFUR-MLT (75.0). In Table 3 we see human evaluations support our au-\nInput Generations Analysis\nInformal अपनी वाली जॉब मुझ ेमत बताओ. (don’t tell me about your job)\nFormal (\uD835\uDF06 = 0.5) अपनी वाली नौकरी मुझ ेमत बताओ। (\uD835\uDF06 = 1.0) अपनी नौकरी के बारे में मुझ ेबताने की जरूरत नहीं। (\uD835\uDF06 = 1.5) आपकी नयुि त के बारे में मुझ ेना बताएं। As sentences get more formal, the english word “job” (जॉब) is converted to Persian (नौकरी) / high Sanskrit ( नयुि त) and honorifics are used (आपकी, बताएं)\nFormal हसंा में दो लोगों की मौत हुई थी और लगभग 150 घायल हुए थे। (two people died in the violence and 150 were injured) Informal (\uD835\uDF06 = 1.0) हसंा में दो लोग मारे गए और 150 के करीब लोग घायल हो गए. (\uD835\uDF06 = 1.5) हसंा में 2 लोग मारे गए थे व 150 लोग घायल हुए थे (\uD835\uDF06 = 2.0) हसंा में 2 लोग मारे गए और 150 घायल\nAs sentences get more informal besides lexical changes, sentence shortening is common, while roughly conveying same meaning\ntomatic evaluation for formality transfer. In Table 4 we perform human evaluation on a subset of models for code-mixing addition and see similar trends, with DIFFUR-MLT significantly outperforming UR, UR-INDIC (41.5 AGG vs 3.6, 15.3 on Hindi). DIFFUR-MLT and DIFFUR-INDIC are best at controlling magnitude of style transfer: In Table 5, we compare the extent to which models can control the amount of style transfer using λ. We find that all our proposed methods outperform the UR model, which gets only 29.2 CALIB. +BT models are not as effective at control (43.4 CALIB), while DIFFUR-INDIC and DIFFUR-MLT perform best (69.6, 69.0 CALIB). This is graphically illustrated in Figure 3. DIFFUR-MLT performs consistently well across different λ values (left plot), and gives a high style change without much drop in content similarity to the input as λ is varied (right plot); more control experiments in Appendix F.\nIn Appendix I we provide a breakdown by individual metrics and plots showing variation with λ.\nIn Appendix G we show ablations studies justifying the DIFFUR design, decoding scheme, etc. We also analyze the style encoder fstyle in Appendix H, finding it is an effective style classifier.\nWe analyze several qualitative outputs from DIFFUR-MLT in Figure 4. Besides formality transfer and code-mixing addition, we transfer several other attributes: sentiment (Li et al., 2018), simplicity (Xu et al., 2015), anonymity (Anandan et al., 2012) and gender neutrality (Reddy and Knight, 2016); more outputs in Appendix J.\nCONCLUSION: We present a recipe for building & evaluating controllable few-shot style transfer systems needing only 3-10 style examples at inference, useful in low-resource settings. Our methods outperform prior work in formality transfer & codemixing for 7 languages, with promising qualitative results. Future work includes further improving systems for some attributes, and considering languages where little / no translation data is available."
    }, {
      "heading" : "Ethical Considerations",
      "text" : "Recent work has highlighted issues of stylistic bias in text generation systems, specifically machine translation systems (Hovy et al., 2020). We acknowledge these issues, and consider style transfer and style-controlled generation technology as an opportunity to work towards fixing them (for instance, gender neutralization as presented in Section 6). Note that it is important to tread down this path carefully — In Chapter 9, Blodgett (2021) argue that style is inseparable from social meaning (as originally noted by Eckert, 2008), and humans may perceive automatically generated text very differently compared to automatic style classifiers.\nOur models were trained on 32 Google Cloud TPUs. As discussed in Appendix A, the UR & UR-INDIC model take roughly 18 hours to train. The DIFFUR-* and DIFFUR-MLT models are much cheaper to train (2 hours) since we finetune the pretrained UR-* models. The Google 2020 environment report mentions,13 “TPUs are highly efficient chips which have been specifically designed for machine learning applications”. These accelerators run on Google Cloud, which is carbon neutral today, and is aiming to “run on carbon-free energy, 24/7, at all of Google’s data centers by 2030” (https://cloud.google. com/sustainability)."
    }, {
      "heading" : "Appendices for “Few-shot Controllable Style Transfer for Low-Resource Multilinugal Settings”",
      "text" : ""
    }, {
      "heading" : "A Model training & evaluation details",
      "text" : "We compare the following models:\n• UR: the Universal Rewriter (Garcia et al., 2021), which is our main baseline (Section 3);\n• DIFFUR: our model with paraphrase vector differences (Section 4.2);\n• UR-INDIC, DIFFUR-INDIC: Indic variants of UR and DIFFUR models (Section 4.3);\n• DIFFUR-MLT: Multitask training between URINDIC and DIFFUR-INDIC (Section 4.4);\n• + BT: models with style-controlled backtranslation at inference time (Section 4.1).\nTo train the UR-INDIC model, we use mC4 (Xue et al., 2021b) for the self-supervised objectives and Samanantar (Ramesh et al., 2021) for the supervised translation. For creating paraphrase data for training our DIFFUR models (Section 4.2), we again leverage Indic language side of Samanantar sentence pairs. Our models are implemented in JAX (Bradbury et al., 2018) using the T5X library.14 We re-use the UR checkpoint from Garcia et al. (2021). To train the UR-INDIC model, we follow the setup in Garcia et al. (2021) and initialize the model with mT5-XL (Xue et al., 2021b), which has 3.7B parameters. We fine-tune the model for 25K steps with a batch size of 512 inputs and a learning rate of 1e-3, using the objectives in Section 3. Training was done on 32 Google Cloud TPUs which took a total of 17.5 hours. To train the DIFFUR and DIFFUR-INDIC models, we further finetune UR and UR-INDIC for a total of 4K steps using the objective from Section 4.2, taking 2 hours. Evaluation Datasets: Our models are evaluated on (1) formality transfer; (2) the task of adding code-mixing in text. Since we do not have access to any formality evaluation dataset,15 we hold out 22K sentences from Samanantar in each Indic language for validation / testing. For Swahili / Spanish, we use mC4 / WMT2018 sentences. These sets\n14https://github.com/google-research/ google-research/tree/master/flax_models/ t5x\n15We do not use GYAFC (Rao and Tetreault, 2018) and XFORMAL (Briakou et al., 2021b) due to reasons in footnote 4. Our dataset from Section 5.1 has already been used for classifier selection, and has machine generated sentences.\nhave similar number of formal / informal sentences, as marked by our formality classifiers (Section 5.2), and are transferred to the opposite formality. We re-use the hi/bn formality transfer splits for codemixing addition, where a system must increase the amount of code-mixing (with English) in a sentence, as shown in our exemplars in Appendix D. Seven languages with varying scripts and morphological richness are used for evaluation (hi,es,sw,bn,kn,te,gu). The UR model only saw translation data for hi,es,bn, whereas UR-INDIC sees translation data for all Indic languages (Section 4.3). To test the generalization capability of the DIFFUR, no Gujarati paraphrase training data for is used."
    }, {
      "heading" : "B More Related Work",
      "text" : "Multilingual style transfer is mostly unexplored in prior work: a 35 paper survey by Briakou et al. (2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019; Tikhonov and Yamshchikov, 2018; Korotkova et al., 2019; Niu et al., 2018). Briakou et al. (2021b) further introduced XFORMAL, the first formality transfer evaluation dataset in French, Brazilian Portugese and Italian.16 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006; Agnihotri, 2013; Kumar, 2014) and codemixing (Bali et al., 2014). Due to its prevalence in India, English-Hindi code-mixing has seen work in language modeling (Pratapa et al., 2018; Samanta et al., 2019) and core NLP tasks (Khanuja et al., 2020). To the best of our knowledge, we are the first to study style transfer for Indic languages. A few prior works build models which can control the degree of style transfer using a scalar input (Wang et al., 2019; Samanta et al., 2021). However, these models are style-specific and require large unpaired style corpora during training. We adopt the inference-time control method used by Garcia et al. (2021) and notice much better controllability after our proposed fixes in Section 4.2."
    }, {
      "heading" : "C More details on the translation-specific Universal Rewriter objectives",
      "text" : "In this section we describe the details of the supervised translation objective and the style-controlled translation objective used in the Universal Rewriter\n16We do not use this data since it does not cover Indian languages, and due to Yahoo! L6 corpus restrictions for industry researchers (confirmed via authors correspondence).\nmodel. See Section 3 for details on the exemplarbased denoising objective. Learning translation via direct supervision: This objective is the standard supervised translation setup, using zero vectors for style. The output language code is prepended to the input. Consider a pair of parallel sentences (x, y) in languages with codes lx, ly (prepended to the input string),\nȳ = fur(ly⊕ x,0) Ltranslate = LCE(ȳ, y)\nThe Universal Rewriter is trained on Englishcentric translation data from the high-resource languages in OPUS-100 (Zhang et al., 2020).\nLearning style-controlled translation: This objective emulates \"style-controlled translation\" in a self-supervised manner, via backtranslation through English. Consider x1 and x2 to be two non-overlapping spans in mC4 in language lx,\nxen2 = fur(en⊕ x2,−fstyle(x1)) x̄2 = fur(lx⊕ xen2 , fstyle(x1)) LBT = LCE(x̄2, x2)"
    }, {
      "heading" : "D Choice of Exemplars",
      "text" : ""
    }, {
      "heading" : "Formal exemplars",
      "text" : "1. This was a remarkably thought-provoking read. 2. It is certainly amongst my favorites. 3. We humbly request your presence at our gala in the coming week. Informal exemplars 1. reading this rly makes u think 2. Its def one of my favs 3. come swing by our bbq next week if ya can make it"
    }, {
      "heading" : "Complex exemplars",
      "text" : "1. The static charges remain on an object until they either bleed off to ground or are quickly neutralized by a discharge. 2. It is particularly famous for the cultivation of kiwifruit. 3. Notably absent from the city are fortifications and military structures."
    }, {
      "heading" : "Simple exemplars",
      "text" : "1. Static charges last until they are grounded or discharged. 2. This area is known for growing kiwifruit. 3. Some things important missing from the city are protective buildings and military buildings."
    }, {
      "heading" : "Positive sentiment exemplars",
      "text" : "1. The most comfortable bed I’ve ever slept on, I highly recommend it. 2. I loved it. 3. The movie was fantastic."
    }, {
      "heading" : "Negative sentiment exemplars",
      "text" : "1. The most uncomfortable bed I’ve ever slept on, I would never recommend it. 2. I hated it. 3. The movie was awful."
    }, {
      "heading" : "E Evaluation Appendix",
      "text" : ""
    }, {
      "heading" : "E.1 Multilingual Classifier Selection",
      "text" : "Due to the absence of a style classification dataset in Indic languages, we built our multilingual classifier drawing inspiration from recent research in zero-shot cross-lingual transfer (Conneau et al., 2018; Conneau and Lample, 2019; Pfeiffer et al., 2020b). We experimented with three zero-shot transfer techniques while selecting our classifiers for evaluating multilingual style transfer.\nTRANSLATE TRAIN: The first technique uses the hypothesis that style is preserved across translation. We classify the style of English sentences in the Samanantar translation dataset (Ramesh et al., 2021) using a style classifier trained on English formality data from Krishna et al. (2020). We use the human translated Indic languages sentences as training data. This training data is used to fine-tune a large-scale multilingual language model.\nZERO-SHOT: The second technique fine-tunes large-scale multilingual language models on a English style transfer dataset, and applies it\nzero-shot on multilingual data during inference.\nMAD-X: Introduced by Pfeiffer et al. (2020b), this technique is similar to ZERO-SHOT but additionally uses language-specific parameters (“adapters”) during inference. These language-specific adapters have been originally trained using masked language modeling on the desired language data.\nDataset for evaluating classifiers: We conduct our experiments on Hindi formality classification, leveraging our evaluation datasets from Section 5.1. We removed pairs which did not have full agreement across the three annotators and those pairs which had the consensus rating of “Equal” formality. This filtering process leaves us with 316 pairs in Hindi (out of 1000). In our experiments, we check whether the classifiers give a higher score to the more formal sentence in the pair.\nModels: We leverage the multilingual classifiers open-sourced17 by Krishna et al. (2020). These models have been trained on the English GYAFC formality classification dataset (Rao and Tetreault, 2018), and have been shown to be effective on the XFORMAL dataset (Briakou et al., 2021b) for formality classification in Italian, French and Brazilian Portuguese.13 These classifiers were trained on preprocessed data which had trailing punctuation stripped and English sentences lower-cased, encouraging the models to focus on lexical and syntactic choices. As base multilingual language models, we use (1) mBERT-base from Devlin et al. (2019); (2) XLM-RoBERTabase from Conneau et al. (2020).\nResults: Our results on Hindi are presented in Table 6 and other languages in Table 7. Consistent with Pfeiffer et al. (2020b), we find MAD-X to be a superior zero-shot cross lingual transfer method compared to baselines. We also find XLM-R has better multilingual representations than mBERT. Unfortunately, AdapterHub (Pfeiffer et al., 2020a) has XLM-R language adapters available only for Hindi & Tamil (among Indic languages). For other languages we use the ZERO-SHOT technique on XLM-R, consistent with the recommendations13 provided by Krishna et al. (2020) based on their ex-\n17https://github.com/ martiansideofthemoon/ style-transfer-paraphrase/blob/master/ README-multilingual.md\nperiments on XFORMAL (Briakou et al., 2021b)."
    }, {
      "heading" : "E.2 Semantic Similarity Model Selection",
      "text" : "We considered three models for evaluating semantic similarity between the input and output:\n(1) LaBSE (Feng et al., 2020); (2) m-USE (Yang et al., 2020); (3) multilingual Sentence-BERT (Reimers and Gurevych, 2020), the knowledge-distilled variant paraphrase-xlm-r-multilingual-v1\nAmong these models, only LaBSE has support for all the Indic languages we were interested in. No Indic language is supported by m-USE, and multilingual Sentence-BERT has been trained on parallel data only for Hindi, Gujarati and Marathi among our Indic languages. However, in terms of Semantic Textual Similarity (STS) benchmarks (Cer et al., 2017) for English, Arabic & Spanish, m-USE and Sentence-BERT outperform LaBSE (Table 1 in Reimers and Gurevych, 2020).\nLaBSE correlates better than Sentence-BERT with our human-annotated formality dataset: We measured the Spearman’s rank correlation between the semantic similarity annotations on our\nhuman-annotated formality datasets (Section 5.1). We discarded 10% sentence pairs which had no agreement among three annotators and took the majority vote for the other sentence pairs. We assigned “Different Meaning” a score of 0, “Slight Difference in Meaning” a score of 1 and “Approximately Same Meaning” a score of 2 before measuring Spearman’s rank correlation. In Table 8 we see a stronger correlation of human annotations with LaBSE compared to Sentence-BERT, especially for languages like Bengali, Kannada for which Sentence-BERT did not see parallel data.\nE.3 Evaluation with Different LaBSE thresholds\nIn Section 6, we set our LaBSE threshold L to 0.75. In this section, we present our evaluations with a more and less conservative value of L.\nIn Table 17, we present results with L = 0.65, and in Table 18 we set L = 0.85. Compared to Table 1, trends are mostly similar, with DIFFUR models and INDIC variants outperforming counterparts. Note that the absolute values of SIM and AGG metrics differ, with absolute values going down with the stricter threshold of L = 0.85, and up with the relaxed threshold of L = 0.65.\nComparing chosen thresholds with human annotations: To verify these three thresholds are reasonable choices, we measure the LaBSE similarity of the sentence pairs annotated by humans, and compare the LaBSE scores to human semantic similarity annotations. We pool the “Approximately Same Meaning” and “Slight Difference in Meaning” categories as “same”, and consider only sentence pairs with a majority rating of “same”. In Table 9 we see that the chosen thresholds span the spectrum of LaBSE values for the human annotated semantically similar pairs."
    }, {
      "heading" : "E.4 More Crowdsourcing Details",
      "text" : "In Figure 16, we show screenshots of our crowdsourcing interface along with all the instructions shown to crowdworkers. The instructions were written after consulting professional Indian linguists. Each crowdworker was allowed to annotate a maximum of 50 different sentence pairs per language, paying them $0.05 per pair. For formality classification, we showed crowdworkers two sentences and asked them to choose which one is more formal. Crowdworkers were allowed to mark ties using an “Equal” option. For semantic similarity annotation, we showed crowdworkers the sentence pair and provided three options — “approximately same meaning”, “slight difference in meaning”, “different meaning”, to emulate a 3-point Likert scale. While performing our human evaluation (Section 5.7), we use a 0.5 SIM score for “slight difference in meaning” and a 1.0 SIM score for “approximately same meaning” annotations. For every system considered, we analyzed the same set of 200 input sentences for style transfer performance, and 100 of those sentences for evaluating controllability. We removed sentences which were exact copies of the input (after removing trailing punctuation) or were in the wrong language to save annotator time and cost. When outputs were exact copies of the input, we assigned SIM = 100, ACC = 0, AGG = 0.\nIn Table 10 and Table 11 we show the interannotator agreement statistics. We measure Fleiss Kappa (Fleiss, 1971), Randolph Kappa (Randolph, 2005; Warrens, 2010), the fraction of sentence pairs with total agreement between the three annotators and the fraction of sentence pairs with no agreement.18 In the table we can see all agreement statis-\n18The κ scores are measured using the library https: //github.com/statsmodels/statsmodels.\ntics are well away from a uniform random annotation baseline, indicating good agreement."
    }, {
      "heading" : "E.5 Fluency Evaluation",
      "text" : "Unlike some prior works, we avoid evaluation of output fluency due to the following reasons: (1) lack of fluency evaluation tools for Indic languages;19 (2) fluency evaluation often discriminates against styles which are out-of-distribution for the fluency classifier, as discussed in Appendix A.8 of Krishna et al. (2020); (3) several prior works (Pang, 2019; Mir et al., 2019; Krishna et al., 2020) have recommended against using perplexity of style language models for fluency evaluation since it is unbounded and favours unnatural sentences with common words; (4) large language models are known to produce fluent text as perceived by humans (Ippolito et al., 2020; Akoury et al., 2020), reducing the need for this evaluation.\n19A potential tool for fluency evaluation in future work is LAMBRE (Pratapa et al., 2021). However, the original paper does not evaluate performance on Indic languages and the grammars for Indic languages would need to collected / built.\nE.6 Details of other individual metrics\nLanguage Consistency (LANG): Since our semantic similarity metric LaBSE is languageagnostic, it tends to ignore accidental translations, which are common errors in large multilingual transformers (Xue et al., 2021a,b), especially the Universal Rewriter (Section 3.1). Hence, we check whether the output sentence is in the same language as the input, using langdetect.20\nOutput Diversity (COPY, 1-g): As discussed in Section 3.1, the Universal Rewriter has a strong tendency to copy the input verbatim. We build two metrics to measure output diversity compared to the input, which have been previously used for extractive question answering evaluation (Rajpurkar et al., 2016). The first metric COPY measures the fraction of outputs which were copied verbatim from the input. This is done after removing trailing punctuation, to penalize models generations which solely modify punctuation. A second metric 1-g measures the unigram overlap F1 score between the input and output. A diverse style transfer system should minimize both COPY and 1-g."
    }, {
      "heading" : "F More Controllability Evaluations",
      "text" : "We follow the setup in Section 5.6 to first compute a λmax per system. We then compute the following,\n1. Style Transfer Performance (r-AGG): An ideal system should have good overall performance (Section 5.5) across different values in the range Λ. 2. Average Style Score Increase (INCR): As our control value increases, we want the classifier’s target style score (compared to the input) to increase. Additionally, we want the style score increase of λmax to be as high as possible, indicating the system can span the range of classifier scores. 3. Style Calibration to λ (CALIB, C-IN): As defined in Section 5.6. We additionally also measure calibration by including the input sentence x in the CALIB(x) calculation, treating it as the output for λ = 0 (no style transfer). Here, calibration is averaged over a total of n = 6 (λ1, λ2) pairs. We call this metric C-IN. A detailed breakdown of performance by different metrics for every model is shown in Table 14.\n20This package is the Python port of Nakatani (2010)."
    }, {
      "heading" : "G Ablation Studies",
      "text" : ""
    }, {
      "heading" : "G.1 Ablation Study for DIFFUR design",
      "text" : "This section describes the ablation experiments conducted for the DIFFUR modeling choices in Section 4.2. We ablate a DIFFUR-INDIC model trained on Hindi paraphrase data only, and present results for Hindi formality transfer in Table 15.\n- no paraphrase: We replaced the paraphrase noise function with the random token dropping / replacing noise used in the denoising objective of UR model (Section 3), and continued to use vector differences. As seen in Table 15, this significantly increases the copy rate, which lowers the style transfer performance.\n- no paraphrase semantic filtering: We keep a setup identical to Section 4.2, but avoid the LaBSE filtering done (discarding pairs having a LaBSE score outside [0.7, 0.98]) to remove noisy paraphrases or exact copies. As seen in Table 15, this decreases the semantic similarity score of the generations, lowering the overall performance.\n- no vector differences: Instead of using vector differences for DIFFUR-INDIC, we simply set sdiff = fstyle(x), or the style of the target sentence. In Table 15, we see this significantly decreases SIM scores, and LANG scores for λ = 2.0. We hypothesize that this training encourages the model to rely more heavily on the style vectors, ignoring the paraphrase input. This could happen since the style vectors are solely constructed from the output sentence itself, and semantic information / confounding style is not subtracted out. In other words, the model is behaving more like an autoencoder (through the style vector) instead of a denoising autoencoder with stylistic supervision.\n- mC4 instead of Samanantar: Instead of creating pseudo-parallel data with Samanantar, we leverage the mC4 dataset itself which was used to train the UR model. We backtranslate spans of text from the Hindi split of mC4 on-the-fly using the UR translation capabilities, and use it as the “paraphrase noise function”. To ensure translation performance does not deteriorate during training, 50% minibatches are supervised translation between Hindi and English. In Table 15, we see decent overall performance, but the LANG score is 6% lower than DIFFUR-INDIC. Qualitatively we found that the\nmodel often translates a few Hindi words to English while making text informal. Due to sparsity of English tokens, it often escapes penalization from LANG. - mC4 + exemplar instead of target: This setting is similar to the previous one, but in addition to the mC4 dataset we utilize the vector difference between the style vector of the exemplar span (instead of target span), and the “paraphrase noised” input. Results in Table 15 show this method is not effective, and it’s important for the vector difference to model the precise transformation needed."
    }, {
      "heading" : "G.2 Choice of Decoding Scheme",
      "text" : "We experiment with five decoding schemes on the Hindi formality validation set — beam search with beam size 1, 4 and top-p sampling (Holtzman et al., 2020) with p = 0.6, 0.75, 0.9.\nIn Table 16, we present results at a constant style transfer magnitude (λ = 3.0). Consistent with Krishna et al. (2020), we find that top-p decoding usually gets higher style accuracy (r-ACC, a-ACC) and output diversity (1-g, COPY) scores, but lower semantic similarity (SIM) scores. Overall beam search triumphs since the loss in semantic similarity leads to a worse performing model. In Figure 9, we see a consistent trend across different magnitudes of style transfer (λ). In all our main experiments, we use beam search with beam size 4 to obtain our generations."
    }, {
      "heading" : "G.3 Number of Training Steps",
      "text" : "In Figure 10, we present the variation in style transfer performance with number of training steps for our best model, the DIFFUR-MLT model. We find that with more training steps performance generally improves, but improvements saturate after 8k steps. We also see the peak of the graphs (best style transfer performance) shift rightwards, indicating a preference for higher λ values."
    }, {
      "heading" : "H Analysis Experiments",
      "text" : "H.1 Style vectors from fstyle as style classifiers\nThe Universal Rewriter models succeed in learning an effective style space, useful for few-shot style transfer. But can this metric space also act as a style classifier? To explore this, we measure the cosine distance between the mean style vector of our\ninformal exemplars,21 and the style vectors derived by passing human-annotated formal/informal pairs (from our dataset of Section 5.1) through fstyle. We only consider pairs which had complete agreement among annotators. In Table 12 we see good agreement (68.2%-80.7%) between human annotations and the classifier derived from the metric space of the UR-INDIC model. Agreement is lower (67.0%- 74.3%) for the DIFFUR-INDIC model, likely due to the stop gradient used in Section 4.2. With DIFFUR-MLT, agreement jumps back up to 75%- 81.7% since gradients flow into the style extractor as well."
    }, {
      "heading" : "H.2 Style Vector Analysis with Formal",
      "text" : "Exemplars Vectors\nIn Appendix H.1, we saw that the metric vector space derived from the style encoder fstyle of various models is an effective style classifier, using the informal exemplar vectors. In Table 13, we present a corresponding analysis using formal exemplar vectors. Most accuracy scores are close to 50%, implying this setup is not a very effective style classifier."
    }, {
      "heading" : "I Full Breakdown of Results",
      "text" : "A full breakdown of results by individual metrics, along with plots showing variation with change in\n21See Appendix D for the exemplar sentences. We found the informal exemplars more effective than formal exemplars for style classification; Appendix H.2 has a comparison.\nλ, is provided for — Hindi (Table 19, Figure 11), Bengali (Table 20, Figure 12), Kannada (Table 21, Figure 13), Telugu (Table 22, Figure 14), Gujarati (Table 23, Figure 15).\nIn the baseline Hindi UR model, we notice high COPY rates (45.4%), resulting in lower ACC scores. COPY reduces in our proposed models (4.4% for DIFFUR-MLT), which boosts overall performance. We find the lowest COPY (and lowest 1-g) for models with +BT (1%), which is due to two steps of translation. However, this lowers semantic similarity (also seen in Table 3) lowering the overall score compared to DIFFUR-MLT (60.0 vs 78.1 r-AGG)."
    }, {
      "heading" : "J More Model Outputs",
      "text" : "Please refer to Figure 8. In the main body, Figure 4 has a few examples as well with detailed analysis."
    } ],
    "references" : [ {
      "title" : "Hindi: An essential grammar",
      "author" : [ "Rama Kant Agnihotri." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Agnihotri.,? 2013",
      "shortCiteRegEx" : "Agnihotri.",
      "year" : 2013
    }, {
      "title" : "STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation",
      "author" : [ "Nader Akoury", "Shufan Wang", "Josh Whiting", "Stephen Hood", "Nanyun Peng", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Akoury et al\\.,? 2020",
      "shortCiteRegEx" : "Akoury et al\\.",
      "year" : 2020
    }, {
      "title" : "t-plausibility: Generalizing words to desensitize text",
      "author" : [ "Balamurugan Anandan", "Chris Clifton", "Wei Jiang", "Mummoorthy Murugesan", "Pedro PastranaCamacho", "Luo Si." ],
      "venue" : "Transactions on Data Privacy, 5(3):505–534.",
      "citeRegEx" : "Anandan et al\\.,? 2012",
      "shortCiteRegEx" : "Anandan et al\\.",
      "year" : 2012
    }, {
      "title" : "i am borrowing ya mixing?\" an analysis of english-hindi code mixing in facebook",
      "author" : [ "Kalika Bali", "Jatin Sharma", "Monojit Choudhury", "Yogarshi Vyas." ],
      "venue" : "13https://www.gstatic.com/",
      "citeRegEx" : "Bali et al\\.,? 2014",
      "shortCiteRegEx" : "Bali et al\\.",
      "year" : 2014
    }, {
      "title" : "Sociolinguistically driven approaches for just natural language processing",
      "author" : [ "Su Lin Blodgett." ],
      "venue" : "UMass Amherst Doctoral Dissertations. 2092.",
      "citeRegEx" : "Blodgett.,? 2021",
      "shortCiteRegEx" : "Blodgett.",
      "year" : 2021
    }, {
      "title" : "JAX: composable transformations",
      "author" : [ "James Bradbury", "Roy Frostig", "Peter Hawkins", "Matthew James Johnson", "Chris Leary", "Dougal Maclaurin", "George Necula", "Adam Paszke", "Jake VanderPlas", "Skye Wanderman-Milne", "Qiao Zhang" ],
      "venue" : null,
      "citeRegEx" : "Bradbury et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Bradbury et al\\.",
      "year" : 2018
    }, {
      "title" : "A review of human evaluation for style transfer",
      "author" : [ "Eleftheria Briakou", "Sweta Agrawal", "Ke Zhang", "Joel Tetreault", "Marine Carpuat." ],
      "venue" : "arXiv preprint arXiv:2106.04747.",
      "citeRegEx" : "Briakou et al\\.,? 2021a",
      "shortCiteRegEx" : "Briakou et al\\.",
      "year" : 2021
    }, {
      "title" : "Olá, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer",
      "author" : [ "Eleftheria Briakou", "Di Lu", "Ke Zhang", "Joel Tetreault." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Briakou et al\\.,? 2021b",
      "shortCiteRegEx" : "Briakou et al\\.",
      "year" : 2021
    }, {
      "title" : "Quality at a glance: An audit of web-crawled multilingual",
      "author" : [ "Isaac Caswell", "Julia Kreutzer", "Lisa Wang", "Ahsan Wahab", "Daan van Esch", "Nasanbayar Ulzii-Orshikh", "Allahsera Tapo", "Nishant Subramani", "Artem Sokolov", "Claytone Sikasote" ],
      "venue" : null,
      "citeRegEx" : "Caswell et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Caswell et al\\.",
      "year" : 2021
    }, {
      "title" : "Evaluation of text generation: A survey",
      "author" : [ "Asli Celikyilmaz", "Elizabeth Clark", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:2006.14799.",
      "citeRegEx" : "Celikyilmaz et al\\.,? 2020",
      "shortCiteRegEx" : "Celikyilmaz et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems, 32:7059–7069.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Variation and the indexical field 1",
      "author" : [ "Penelope Eckert." ],
      "venue" : "Journal of sociolinguistics, 12(4).",
      "citeRegEx" : "Eckert.,? 2008",
      "shortCiteRegEx" : "Eckert.",
      "year" : 2008
    }, {
      "title" : "Languageagnostic bert sentence embedding",
      "author" : [ "Fangxiaoyu Feng", "Yinfei Yang", "Daniel Cer", "Naveen Arivazhagan", "Wei Wang." ],
      "venue" : "arXiv preprint arXiv:2007.01852.",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin, 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "Towards universality in multilingual text rewriting",
      "author" : [ "Xavier Garcia", "Noah Constant", "Mandy Guo", "Orhan Firat." ],
      "venue" : "arXiv preprint arXiv:2107.14749.",
      "citeRegEx" : "Garcia et al\\.,? 2021",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural syntactic preordering for controlled paraphrase generation",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "Generating sentences by editing prototypes",
      "author" : [ "Kelvin Guu", "Tatsunori B Hashimoto", "Yonatan Oren", "Percy Liang." ],
      "venue" : "Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Guu et al\\.,? 2018",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning sparse prototypes for text generation",
      "author" : [ "Junxian He", "Taylor Berg-Kirkpatrick", "Graham Neubig." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Intelligent writing assistance",
      "author" : [ "George Heidorn." ],
      "venue" : "Handbook of natural language processing, pages 181–207.",
      "citeRegEx" : "Heidorn.,? 2000",
      "shortCiteRegEx" : "Heidorn.",
      "year" : 2000
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "you sound just like your father” commercial machine translation systems include stylistic biases",
      "author" : [ "Dirk Hovy", "Federico Bianchi", "Tommaso Fornaciari." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hovy et al\\.,? 2020",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic detection of generated text is easiest when humans are fooled",
      "author" : [ "Daphne Ippolito", "Daniel Duckworth", "Chris CallisonBurch", "Douglas Eck." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Ippolito et al\\.,? 2020",
      "shortCiteRegEx" : "Ippolito et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "Shakespearizing modern language using copy-enriched sequence to sequence models",
      "author" : [ "Harsh Jhamtani", "Varun Gangal", "Eduard Hovy", "Eric Nyberg." ],
      "venue" : "Proceedings of the Workshop on Stylistic Variation, pages 10–19.",
      "citeRegEx" : "Jhamtani et al\\.,? 2017",
      "shortCiteRegEx" : "Jhamtani et al\\.",
      "year" : 2017
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Hindi, volume 12",
      "author" : [ "Yamuna Kachru." ],
      "venue" : "John Benjamins Publishing.",
      "citeRegEx" : "Kachru.,? 2006",
      "shortCiteRegEx" : "Kachru.",
      "year" : 2006
    }, {
      "title" : "A new measure of rank correlation",
      "author" : [ "Maurice G Kendall." ],
      "venue" : "Biometrika, 30(1/2):81–93.",
      "citeRegEx" : "Kendall.,? 1938",
      "shortCiteRegEx" : "Kendall.",
      "year" : 1938
    }, {
      "title" : "Gluecos: An evaluation benchmark for codeswitched nlp",
      "author" : [ "Simran Khanuja", "Sandipan Dandapat", "Anirudh Srinivasan", "Sunayana Sitaram", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Khanuja et al\\.,? 2020",
      "shortCiteRegEx" : "Khanuja et al\\.",
      "year" : 2020
    }, {
      "title" : "Grammatical error correction and style transfer via zero-shot monolingual translation",
      "author" : [ "Elizaveta Korotkova", "Agnes Luhtaru", "Maksym Del", "Krista Liin", "Daiga Deksne", "Mark Fishel." ],
      "venue" : "arXiv preprint arXiv:1903.11283.",
      "citeRegEx" : "Korotkova et al\\.,? 2019",
      "shortCiteRegEx" : "Korotkova et al\\.",
      "year" : 2019
    }, {
      "title" : "Reformulating unsupervised style transfer as paraphrase generation",
      "author" : [ "Kalpesh Krishna", "John Wieting", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737–762, Online. Asso-",
      "citeRegEx" : "Krishna et al\\.,? 2020",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2020
    }, {
      "title" : "Developing politeness annotated corpus of Hindi blogs",
      "author" : [ "Ritesh Kumar." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 1275– 1280, Reykjavik, Iceland. European Language Re-",
      "citeRegEx" : "Kumar.,? 2014",
      "shortCiteRegEx" : "Kumar.",
      "year" : 2014
    }, {
      "title" : "Neural data augmentation via example extrapolation",
      "author" : [ "Kenton Lee", "Kelvin Guu", "Luheng He", "Tim Dozat", "Hyung Won Chung." ],
      "venue" : "arXiv preprint arXiv:2102.01335.",
      "citeRegEx" : "Lee et al\\.,? 2021",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the 10",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Scientific credibility of machine translation research: A meta-evaluation of 769 papers",
      "author" : [ "Benjamin Marie", "Atsushi Fujita", "Raphael Rubino." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th In-",
      "citeRegEx" : "Marie et al\\.,? 2021",
      "shortCiteRegEx" : "Marie et al\\.",
      "year" : 2021
    }, {
      "title" : "Evaluating style transfer for text",
      "author" : [ "Remi Mir", "Bjarke Felbo", "Nick Obradovich", "Iyad Rahwan." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Mir et al\\.,? 2019",
      "shortCiteRegEx" : "Mir et al\\.",
      "year" : 2019
    }, {
      "title" : "Language detection library for java",
      "author" : [ "Shuyo Nakatani" ],
      "venue" : null,
      "citeRegEx" : "Nakatani.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nakatani.",
      "year" : 2010
    }, {
      "title" : "Controlling neural machine translation formality with synthetic supervision",
      "author" : [ "Xing Niu", "Marine Carpuat." ],
      "venue" : "Association for the Advancement of Artificial Intelligence.",
      "citeRegEx" : "Niu and Carpuat.,? 2020",
      "shortCiteRegEx" : "Niu and Carpuat.",
      "year" : 2020
    }, {
      "title" : "Multi-task neural models for translating between styles within and across languages",
      "author" : [ "Xing Niu", "Sudha Rao", "Marine Carpuat." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1008–1021.",
      "citeRegEx" : "Niu et al\\.,? 2018",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards actual (not operational) textual style transfer auto-evaluation",
      "author" : [ "Richard Yuanzhe Pang." ],
      "venue" : "Proceedings of the 5th Workshop on Noisy Usergenerated Text (W-NUT 2019).",
      "citeRegEx" : "Pang.,? 2019",
      "shortCiteRegEx" : "Pang.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Adapterhub: A framework for adapting transformers",
      "author" : [ "Jonas Pfeiffer", "Andreas Rücklé", "Clifton Poth", "Aishwarya Kamath", "Ivan Vulić", "Sebastian Ruder", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020a",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing, Online.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020b",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Style transfer through back-translation",
      "author" : [ "Shrimai Prabhumoye", "Yulia Tsvetkov", "Ruslan Salakhutdinov", "Alan W Black." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Prabhumoye et al\\.,? 2018",
      "shortCiteRegEx" : "Prabhumoye et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating the morphosyntactic well-formedness of generated texts",
      "author" : [ "Adithya Pratapa", "Antonios Anastasopoulos", "Shruti Rijhwani", "Aditi Chaudhary", "David R. Mortensen", "Graham Neubig", "Yulia Tsvetkov." ],
      "venue" : "arXiv preprint arXiv:2103.16590.",
      "citeRegEx" : "Pratapa et al\\.,? 2021",
      "shortCiteRegEx" : "Pratapa et al\\.",
      "year" : 2021
    }, {
      "title" : "Language modeling for code-mixing: The role of linguistic theory based synthetic data",
      "author" : [ "Adithya Pratapa", "Gayatri Bhat", "Monojit Choudhury", "Sunayana Sitaram", "Sandipan Dandapat", "Kalika Bali." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the As-",
      "citeRegEx" : "Pratapa et al\\.,? 2018",
      "shortCiteRegEx" : "Pratapa et al\\.",
      "year" : 2018
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Samanantar: The largest publicly available parallel corpora collection for 11 indic languages",
      "author" : [ "van", "Anoop Kunchukuttan", "Pratyush Kumar", "Mitesh Shantadevi Khapra" ],
      "venue" : null,
      "citeRegEx" : "van et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "van et al\\.",
      "year" : 2021
    }, {
      "title" : "Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss’ fixed-marginal multirater kappa",
      "author" : [ "Justus J Randolph." ],
      "venue" : "Online submission.",
      "citeRegEx" : "Randolph.,? 2005",
      "shortCiteRegEx" : "Randolph.",
      "year" : 2005
    }, {
      "title" : "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
      "author" : [ "Sudha Rao", "Joel Tetreault." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Rao and Tetreault.,? 2018",
      "shortCiteRegEx" : "Rao and Tetreault.",
      "year" : 2018
    }, {
      "title" : "Obfuscating gender in social media writing",
      "author" : [ "Sravana Reddy", "Kevin Knight." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 17–26.",
      "citeRegEx" : "Reddy and Knight.,? 2016",
      "shortCiteRegEx" : "Reddy and Knight.",
      "year" : 2016
    }, {
      "title" : "A recipe for arbitrary text style transfer with large language models",
      "author" : [ "Emily Reif", "Daphne Ippolito", "Ann Yuan", "Andy Coenen", "Chris Callison-Burch", "Jason Wei." ],
      "venue" : "arXiv preprint arXiv:2109.03910.",
      "citeRegEx" : "Reif et al\\.,? 2021",
      "shortCiteRegEx" : "Reif et al\\.",
      "year" : 2021
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512–4525,",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "TextSETTR: Few-shot text style extraction and tunable targeted restyling",
      "author" : [ "Parker Riley", "Noah Constant", "Mandy Guo", "Girish Kumar", "David Uthus", "Zarana Parekh." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Riley et al\\.,? 2021",
      "shortCiteRegEx" : "Riley et al\\.",
      "year" : 2021
    }, {
      "title" : "A hierarchical vae for calibrating attributes while generating text using normalizing flow",
      "author" : [ "Bidisha Samanta", "Mohit Agrawal", "Niloy Ganguly." ],
      "venue" : "ACL, page 2405–2415.",
      "citeRegEx" : "Samanta et al\\.,? 2021",
      "shortCiteRegEx" : "Samanta et al\\.",
      "year" : 2021
    }, {
      "title" : "A deep generative model for code-switched text",
      "author" : [ "Bidisha Samanta", "Sharmila Reddy", "Hussain Jagirdar", "Niloy Ganguly", "Soumen Chakrabarti." ],
      "venue" : "arXiv preprint arXiv:1906.08972.",
      "citeRegEx" : "Samanta et al\\.,? 2019",
      "shortCiteRegEx" : "Samanta et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised text style transfer: Cross projection in latent space",
      "author" : [ "Mingyue Shang", "Piji Li", "Zhenxin Fu", "Lidong Bing", "Dongyan Zhao", "Shuming Shi", "Rui Yan." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Shang et al\\.,? 2019",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2019
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Advances in neural information processing systems, pages 6830–6841.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "A4nt: author attribute anonymity by adversarial training of neural machine translation",
      "author" : [ "Rakshith Shetty", "Bernt Schiele", "Mario Fritz." ],
      "venue" : "27th {USENIX} Security Symposium ({USENIX} Security 18), pages 1633–1650.",
      "citeRegEx" : "Shetty et al\\.,? 2018",
      "shortCiteRegEx" : "Shetty et al\\.",
      "year" : 2018
    }, {
      "title" : "Controlling style in generated dialogue",
      "author" : [ "Eric Michael Smith", "Diana Gonzalez-Rico", "Emily Dinan", "Y-Lan Boureau." ],
      "venue" : "arXiv preprint arXiv:2009.10855.",
      "citeRegEx" : "Smith et al\\.,? 2020",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2020
    }, {
      "title" : "Multiple-attribute text style transfer",
      "author" : [ "Sandeep Subramanian", "Guillaume Lample", "Eric Michael Smith", "Ludovic Denoyer", "Marc’Aurelio Ranzato", "Y-Lan Boureau" ],
      "venue" : "In Proceedings of the International Conference",
      "citeRegEx" : "Subramanian et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2019
    }, {
      "title" : "Sounds wilde",
      "author" : [ "Aleksey Tikhonov", "Ivan P Yamshchikov." ],
      "venue" : "phonetically extended embeddings for author-stylized poetry generation. In Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology,",
      "citeRegEx" : "Tikhonov and Yamshchikov.,? 2018",
      "shortCiteRegEx" : "Tikhonov and Yamshchikov.",
      "year" : 2018
    }, {
      "title" : "Style transfer for texts: Retrain, report errors, compare with rewrites",
      "author" : [ "Alexey Tikhonov", "Viacheslav Shibaev", "Aleksander Nagaev", "Aigul Nugmanova", "Ivan P Yamshchikov." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Tikhonov et al\\.,? 2019",
      "shortCiteRegEx" : "Tikhonov et al\\.",
      "year" : 2019
    }, {
      "title" : "Controllable unsupervised text attribute transfer via editing entangled latent representation",
      "author" : [ "Ke Wang", "Hang Hua", "Xiaojun Wan." ],
      "venue" : "Advances in Neural Information Processing Systems, 32:11036–11046.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Inequalities between multirater kappas",
      "author" : [ "Matthijs J Warrens." ],
      "venue" : "Advances in data analysis and classification, 4(4):271–286.",
      "citeRegEx" : "Warrens.,? 2010",
      "shortCiteRegEx" : "Warrens.",
      "year" : 2010
    }, {
      "title" : "ParaNMT50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
      "author" : [ "John Wieting", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Wieting and Gimpel.,? 2018",
      "shortCiteRegEx" : "Wieting and Gimpel.",
      "year" : 2018
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1904.12848.",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "On variational learning of controllable representations for text without supervision",
      "author" : [ "Peng Xu", "Jackie Chi Kit Cheung", "Yanshuai Cao." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Problems in current text simplification research: New data can help",
      "author" : [ "Wei Xu", "Chris Callison-Burch", "Courtney Napoles." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:283–297.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Paraphrasing for style",
      "author" : [ "Wei Xu", "Alan Ritter", "Bill Dolan", "Ralph Grishman", "Colin Cherry." ],
      "venue" : "Proceedings of International Conference on Computational Linguistics.",
      "citeRegEx" : "Xu et al\\.,? 2012",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Byt5: Towards a tokenfree future with pre-trained byte-to-byte models",
      "author" : [ "Linting Xue", "Aditya Barua", "Noah Constant", "Rami AlRfou", "Sharan Narang", "Mihir Kale", "Adam Roberts", "Colin Raffel." ],
      "venue" : "arXiv preprint arXiv:2105.13626.",
      "citeRegEx" : "Xue et al\\.,? 2021a",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "Conference of the North American Chapter of the",
      "citeRegEx" : "Xue et al\\.,? 2021b",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "Multilingual universal sentence encoder for semantic retrieval",
      "author" : [ "Yinfei Yang", "Daniel Cer", "Amin Ahmad", "Mandy Guo", "Jax Law", "Noah Constant", "Gustavo Hernandez Abrego", "Steve Yuan", "Chris Tar", "Yun-hsuan Sung", "Brian Strope", "Ray Kurzweil" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving massively multilingual neural machine translation and zero-shot translation",
      "author" : [ "Biao Zhang", "Philip Williams", "Ivan Titov", "Rico Sennrich." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628–",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "2021b) further introduced XFORMAL, the first formality transfer evaluation dataset in French, Brazilian Portugese and Italian.16",
      "author" : [ "Tikhonov", "2018 Yamshchikov", "2019 Korotkova et al", "2018). Briakou Niu et al" ],
      "venue" : null,
      "citeRegEx" : "Tikhonov et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Tikhonov et al\\.",
      "year" : 2021
    }, {
      "title" : "The Universal Rewriter is trained on Englishcentric translation data from the high-resource languages in OPUS-100 (Zhang et al., 2020)",
      "author" : [ "LCE(ȳ" ],
      "venue" : null,
      "citeRegEx" : "LCE.ȳ and y,? \\Q2020\\E",
      "shortCiteRegEx" : "LCE.ȳ and y",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 55,
      "context" : "While most prior literature assumes access to large stylelabelled corpora, recent work (Riley et al., 2021) has attempted “few-shot” style transfer using only 3-10 sentences at inference for extracting the target style.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "It has many applications such as writing assistance (Heidorn, 2000), controlling generation for attributes",
      "startOffset" : 52,
      "endOffset" : 67
    }, {
      "referenceID" : 70,
      "context" : "like simplicity, formality or persuasion (Xu et al., 2015; Smith et al., 2020; Niu and Carpuat, 2020), data augmentation (Xie et al.",
      "startOffset" : 41,
      "endOffset" : 101
    }, {
      "referenceID" : 61,
      "context" : "like simplicity, formality or persuasion (Xu et al., 2015; Smith et al., 2020; Niu and Carpuat, 2020), data augmentation (Xie et al.",
      "startOffset" : 41,
      "endOffset" : 101
    }, {
      "referenceID" : 39,
      "context" : "like simplicity, formality or persuasion (Xu et al., 2015; Smith et al., 2020; Niu and Carpuat, 2020), data augmentation (Xie et al.",
      "startOffset" : 41,
      "endOffset" : 101
    }, {
      "referenceID" : 68,
      "context" : ", 2020; Niu and Carpuat, 2020), data augmentation (Xie et al., 2019; Lee et al., 2021), and author obfuscation (Shetty et al.",
      "startOffset" : 50,
      "endOffset" : 86
    }, {
      "referenceID" : 34,
      "context" : ", 2020; Niu and Carpuat, 2020), data augmentation (Xie et al., 2019; Lee et al., 2021), and author obfuscation (Shetty et al.",
      "startOffset" : 50,
      "endOffset" : 86
    }, {
      "referenceID" : 26,
      "context" : "Most prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to large corpus of unpaired sentences with style labels (Prabhumoye et al.",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 45,
      "context" : ", 2017), or access to large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018; Subramanian et al., 2019).",
      "startOffset" : 75,
      "endOffset" : 126
    }, {
      "referenceID" : 62,
      "context" : ", 2017), or access to large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018; Subramanian et al., 2019).",
      "startOffset" : 75,
      "endOffset" : 126
    }, {
      "referenceID" : 59,
      "context" : "Most standard “unpaired” style transfer datasets have been carefully curated (Shen et al., 2017) or were originally parallel (Xu et al.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 71,
      "context" : ", 2017) or were originally parallel (Xu et al., 2012; Rao and Tetreault, 2018).",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 51,
      "context" : ", 2017) or were originally parallel (Xu et al., 2012; Rao and Tetreault, 2018).",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 32,
      "context" : "To further boost performance we propose DIFFUR,3 an algorithm using the recent finding that paraphrasing leads to stylistic changes (Krishna et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 69,
      "context" : "Few-shot methods are a recent development in English style transfer, with prior work using variational autoencoders (Xu et al., 2020), or prompting large pretrained language models at inference (Reif et al.",
      "startOffset" : 116,
      "endOffset" : 133
    }, {
      "referenceID" : 53,
      "context" : ", 2020), or prompting large pretrained language models at inference (Reif et al., 2021).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "Recently, the Universal Rewriter (Garcia et al., 2021) extended TextSETTR to 101 languages, developing a joint model for translation, few-shot style transfer and stylized translation.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 73,
      "context" : "Consider fenc, fdec to be encoder & decoder Transformers initialized with mT5 (Xue et al., 2021b), which are composed to form the model fur.",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 73,
      "context" : "This objective is used on the mC4 dataset (Xue et al., 2021b) with 101 languages.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 75,
      "context" : "To build a general-purpose rewriter which can do translation as well as style transfer, the model is additionally trained on two objectives: (1) supervised machine translation using the OPUS-100 parallel dataset (Zhang et al., 2020), and (2) a self-supervised objective to learn effective stylecontrolled translation; more details in Appendix C.",
      "startOffset" : 212,
      "endOffset" : 232
    }, {
      "referenceID" : 18,
      "context" : "Moreover, token-level noise does not differentiate between content / function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020).",
      "startOffset" : 141,
      "endOffset" : 166
    }, {
      "referenceID" : 67,
      "context" : "Prior work has shown that backtranslation is effective for paraphrasing (Wieting and Gimpel, 2018; Iyyer et al., 2018) and style transfer (Prabhumoye et al.",
      "startOffset" : 72,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "Prior work has shown that backtranslation is effective for paraphrasing (Wieting and Gimpel, 2018; Iyyer et al., 2018) and style transfer (Prabhumoye et al.",
      "startOffset" : 72,
      "endOffset" : 118
    }, {
      "referenceID" : 32,
      "context" : "Prior work (Krishna et al., 2020) has shown that paraphrasing leads to stylistic changes, and denoising can be considered a style re-insertion process.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "DIFFUR is related to neural editor models (Guu et al., 2018; He et al., 2020), where language models are decomposed into a probabilistic space of edit vectors over prototype sentences.",
      "startOffset" : 42,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "DIFFUR is related to neural editor models (Guu et al., 2018; He et al., 2020), where language models are decomposed into a probabilistic space of edit vectors over prototype sentences.",
      "startOffset" : 42,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "Automatic evaluation of style transfer is challenging (Pang, 2019; Mir et al., 2019; Tikhonov et al., 2019), and the lack of resources (such as evaluation datasets, style classifiers) make evaluation trickier for Indic languages.",
      "startOffset" : 54,
      "endOffset" : 107
    }, {
      "referenceID" : 37,
      "context" : "Automatic evaluation of style transfer is challenging (Pang, 2019; Mir et al., 2019; Tikhonov et al., 2019), and the lack of resources (such as evaluation datasets, style classifiers) make evaluation trickier for Indic languages.",
      "startOffset" : 54,
      "endOffset" : 107
    }, {
      "referenceID" : 64,
      "context" : "Automatic evaluation of style transfer is challenging (Pang, 2019; Mir et al., 2019; Tikhonov et al., 2019), and the lack of resources (such as evaluation datasets, style classifiers) make evaluation trickier for Indic languages.",
      "startOffset" : 54,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "Since automatic metrics in generation are imperfect (Celikyilmaz et al., 2020), we complement our results with human evaluation (Section 5.",
      "startOffset" : 52,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "We resort to zero-shot cross lingual transfer techniques (Conneau and Lample, 2019), where large multilingual pretrained models are first fine-tuned on English classification data, and then applied to other languages at inference.",
      "startOffset" : 57,
      "endOffset" : 83
    }, {
      "referenceID" : 44,
      "context" : "We experiment with three such techniques, and find MAD-X classifiers with language adapters (Pfeiffer et al., 2020b) have the highest accuracy of 81% on our Hindi data from Section 5.",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "However, MAD-X classifiers were only available for Hindi, so we use the next best XLM RoBERTa-base (Conneau et al., 2020) for other languages, which has 75%-82% accuracy on annotated data; details in Appendix E.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 36,
      "context" : "Following recent recommendations (Marie et al., 2021; Krishna et al., 2020), we avoid n-gram overlap metrics like BLEU (Papineni et al.",
      "startOffset" : 33,
      "endOffset" : 75
    }, {
      "referenceID" : 32,
      "context" : "Following recent recommendations (Marie et al., 2021; Krishna et al., 2020), we avoid n-gram overlap metrics like BLEU (Papineni et al.",
      "startOffset" : 33,
      "endOffset" : 75
    }, {
      "referenceID" : 42,
      "context" : ", 2020), we avoid n-gram overlap metrics like BLEU (Papineni et al., 2002).",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "LaBSE (Feng et al., 2020), a language-agnostic semantic similarity model based on multilingual BERT (Devlin et al.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : ", 2020), a language-agnostic semantic similarity model based on multilingual BERT (Devlin et al., 2019).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "We then compute the style calibration to λ (CALIB), or how often does increasing λ lead to a style score increase? We measure this with a statistic similar to Kendall’s τ (Kendall, 1938), counting concordant pairs in Λ,",
      "startOffset" : 171,
      "endOffset" : 186
    }, {
      "referenceID" : 35,
      "context" : "Besides formality transfer and code-mixing addition, we transfer several other attributes: sentiment (Li et al., 2018), simplicity (Xu et al.",
      "startOffset" : 101,
      "endOffset" : 118
    }, {
      "referenceID" : 70,
      "context" : ", 2018), simplicity (Xu et al., 2015), anonymity (Anandan et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : ", 2015), anonymity (Anandan et al., 2012) and gender neutrality (Reddy and Knight, 2016); more outputs in Appendix J.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 52,
      "context" : ", 2012) and gender neutrality (Reddy and Knight, 2016); more outputs in Appendix J.",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : "Recent work has highlighted issues of stylistic bias in text generation systems, specifically machine translation systems (Hovy et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "• UR: the Universal Rewriter (Garcia et al., 2021), which is our main baseline (Section 3);",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 73,
      "context" : "To train the UR-INDIC model, we use mC4 (Xue et al., 2021b) for the self-supervised objectives and Samanantar (Ramesh et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "Our models are implemented in JAX (Bradbury et al., 2018) using the T5X library.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 73,
      "context" : "(2021) and initialize the model with mT5-XL (Xue et al., 2021b), which has 3.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 51,
      "context" : "com/google-research/ google-research/tree/master/flax_models/ t5x (15)We do not use GYAFC (Rao and Tetreault, 2018) and XFORMAL (Briakou et al.",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "com/google-research/ google-research/tree/master/flax_models/ t5x (15)We do not use GYAFC (Rao and Tetreault, 2018) and XFORMAL (Briakou et al., 2021b) due to reasons in footnote 4.",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 58,
      "context" : "(2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019; Tikhonov and Yamshchikov, 2018; Korotkova et al., 2019; Niu et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 169
    }, {
      "referenceID" : 63,
      "context" : "(2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019; Tikhonov and Yamshchikov, 2018; Korotkova et al., 2019; Niu et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 169
    }, {
      "referenceID" : 31,
      "context" : "(2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019; Tikhonov and Yamshchikov, 2018; Korotkova et al., 2019; Niu et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 169
    }, {
      "referenceID" : 40,
      "context" : "(2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019; Tikhonov and Yamshchikov, 2018; Korotkova et al., 2019; Niu et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 169
    }, {
      "referenceID" : 28,
      "context" : "16 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006; Agnihotri, 2013; Kumar, 2014) and codemixing (Bali et al.",
      "startOffset" : 75,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "16 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006; Agnihotri, 2013; Kumar, 2014) and codemixing (Bali et al.",
      "startOffset" : 75,
      "endOffset" : 119
    }, {
      "referenceID" : 33,
      "context" : "16 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006; Agnihotri, 2013; Kumar, 2014) and codemixing (Bali et al.",
      "startOffset" : 75,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "16 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006; Agnihotri, 2013; Kumar, 2014) and codemixing (Bali et al., 2014).",
      "startOffset" : 135,
      "endOffset" : 154
    }, {
      "referenceID" : 47,
      "context" : "Due to its prevalence in India, English-Hindi code-mixing has seen work in language modeling (Pratapa et al., 2018; Samanta et al., 2019) and core NLP tasks (Khanuja et al.",
      "startOffset" : 93,
      "endOffset" : 137
    }, {
      "referenceID" : 57,
      "context" : "Due to its prevalence in India, English-Hindi code-mixing has seen work in language modeling (Pratapa et al., 2018; Samanta et al., 2019) and core NLP tasks (Khanuja et al.",
      "startOffset" : 93,
      "endOffset" : 137
    }, {
      "referenceID" : 65,
      "context" : "A few prior works build models which can control the degree of style transfer using a scalar input (Wang et al., 2019; Samanta et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 140
    }, {
      "referenceID" : 56,
      "context" : "A few prior works build models which can control the degree of style transfer using a scalar input (Wang et al., 2019; Samanta et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 140
    }, {
      "referenceID" : 75,
      "context" : "The Universal Rewriter is trained on Englishcentric translation data from the high-resource languages in OPUS-100 (Zhang et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 12,
      "context" : "Due to the absence of a style classification dataset in Indic languages, we built our multilingual classifier drawing inspiration from recent research in zero-shot cross-lingual transfer (Conneau et al., 2018; Conneau and Lample, 2019; Pfeiffer et al., 2020b).",
      "startOffset" : 187,
      "endOffset" : 259
    }, {
      "referenceID" : 44,
      "context" : "Due to the absence of a style classification dataset in Indic languages, we built our multilingual classifier drawing inspiration from recent research in zero-shot cross-lingual transfer (Conneau et al., 2018; Conneau and Lample, 2019; Pfeiffer et al., 2020b).",
      "startOffset" : 187,
      "endOffset" : 259
    }, {
      "referenceID" : 51,
      "context" : "These models have been trained on the English GYAFC formality classification dataset (Rao and Tetreault, 2018), and have been shown to be effective on the XFORMAL dataset (Briakou et al.",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "These models have been trained on the English GYAFC formality classification dataset (Rao and Tetreault, 2018), and have been shown to be effective on the XFORMAL dataset (Briakou et al., 2021b) for formality classification in Italian, French and Brazilian Portuguese.",
      "startOffset" : 171,
      "endOffset" : 194
    }, {
      "referenceID" : 43,
      "context" : "Unfortunately, AdapterHub (Pfeiffer et al., 2020a) has XLM-R language adapters available only for Hindi & Tamil (among Indic languages).",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 74,
      "context" : ", 2020); (2) m-USE (Yang et al., 2020); (3) multilingual Sentence-BERT (Reimers and Gurevych, 2020), the knowledge-distilled variant",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 54,
      "context" : ", 2020); (3) multilingual Sentence-BERT (Reimers and Gurevych, 2020), the knowledge-distilled variant",
      "startOffset" : 40,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "However, in terms of Semantic Textual Similarity (STS) benchmarks (Cer et al., 2017) for English, Arabic & Spanish, m-USE and Sentence-BERT outperform LaBSE (Table 1 in Reimers and Gurevych, 2020).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "We measure Fleiss Kappa (Fleiss, 1971), Randolph Kappa (Randolph, 2005; Warrens, 2010), the fraction of sentence pairs with total agreement between the three annotators and the fraction of sentence pairs with no agreement.",
      "startOffset" : 24,
      "endOffset" : 38
    }, {
      "referenceID" : 50,
      "context" : "We measure Fleiss Kappa (Fleiss, 1971), Randolph Kappa (Randolph, 2005; Warrens, 2010), the fraction of sentence pairs with total agreement between the three annotators and the fraction of sentence pairs with no agreement.",
      "startOffset" : 55,
      "endOffset" : 86
    }, {
      "referenceID" : 66,
      "context" : "We measure Fleiss Kappa (Fleiss, 1971), Randolph Kappa (Randolph, 2005; Warrens, 2010), the fraction of sentence pairs with total agreement between the three annotators and the fraction of sentence pairs with no agreement.",
      "startOffset" : 55,
      "endOffset" : 86
    }, {
      "referenceID" : 41,
      "context" : "(2020); (3) several prior works (Pang, 2019; Mir et al., 2019; Krishna et al., 2020) have recommended against using perplexity of style language models for fluency evaluation since it is unbounded and favours unnatural sentences with common words; (4) large language models are known to produce fluent text as perceived by humans (Ippolito et al.",
      "startOffset" : 32,
      "endOffset" : 84
    }, {
      "referenceID" : 37,
      "context" : "(2020); (3) several prior works (Pang, 2019; Mir et al., 2019; Krishna et al., 2020) have recommended against using perplexity of style language models for fluency evaluation since it is unbounded and favours unnatural sentences with common words; (4) large language models are known to produce fluent text as perceived by humans (Ippolito et al.",
      "startOffset" : 32,
      "endOffset" : 84
    }, {
      "referenceID" : 32,
      "context" : "(2020); (3) several prior works (Pang, 2019; Mir et al., 2019; Krishna et al., 2020) have recommended against using perplexity of style language models for fluency evaluation since it is unbounded and favours unnatural sentences with common words; (4) large language models are known to produce fluent text as perceived by humans (Ippolito et al.",
      "startOffset" : 32,
      "endOffset" : 84
    }, {
      "referenceID" : 24,
      "context" : ", 2020) have recommended against using perplexity of style language models for fluency evaluation since it is unbounded and favours unnatural sentences with common words; (4) large language models are known to produce fluent text as perceived by humans (Ippolito et al., 2020; Akoury et al., 2020), reducing the need for this evaluation.",
      "startOffset" : 253,
      "endOffset" : 297
    }, {
      "referenceID" : 1,
      "context" : ", 2020) have recommended against using perplexity of style language models for fluency evaluation since it is unbounded and favours unnatural sentences with common words; (4) large language models are known to produce fluent text as perceived by humans (Ippolito et al., 2020; Akoury et al., 2020), reducing the need for this evaluation.",
      "startOffset" : 253,
      "endOffset" : 297
    }, {
      "referenceID" : 46,
      "context" : "A potential tool for fluency evaluation in future work is LAMBRE (Pratapa et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 48,
      "context" : "We build two metrics to measure output diversity compared to the input, which have been previously used for extractive question answering evaluation (Rajpurkar et al., 2016).",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 22,
      "context" : "We experiment with five decoding schemes on the Hindi formality validation set — beam search with beam size 1, 4 and top-p sampling (Holtzman et al., 2020) with p = 0.",
      "startOffset" : 132,
      "endOffset" : 155
    } ],
    "year" : 0,
    "abstractText" : "Style transfer is the task of rewriting an input sentence into a target style while approximately preserving its content. While most prior literature assumes access to large stylelabelled corpora, recent work (Riley et al., 2021) has attempted “few-shot” style transfer using only 3-10 sentences at inference for extracting the target style. In this work we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available. We find that existing fewshot methods perform this task poorly, with a strong tendency to copy inputs verbatim. We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work using automatic and human evaluations, our model achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across seven languages. Moreover, our method is better able to control the amount of style transfer using an input scalar knob. We report promising qualitative results for several attribute transfer directions, including sentiment transfer, text simplification, gender neutralization and text anonymization, all without retraining the model. Finally we found model evaluation to be difficult due to the lack of evaluation datasets and metrics for many languages. To facilitate further research in formality transfer for Indic languages, we crowdsource annotations for 4000 sentence pairs in four languages, and use this dataset1 to design our automatic evaluation suite.",
    "creator" : null
  }
}
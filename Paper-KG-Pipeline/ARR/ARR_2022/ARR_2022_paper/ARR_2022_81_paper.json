{
  "name" : "ARR_2022_81_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multimodal Dialogue State Tracking",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The main goal of dialogue research is to develop intelligent agents that can assist humans through conversations. For example, a dialogue agent can be tasked to help users to find a restaurant based on their preferences of price ranges and food choices. A crucial part of a dialogue system is Dialogue State Tracking (DST), which is\nresponsible for tracking and updating user goals in the form of dialogue states, including a set of (slot, value) pairs such as (price, “moderate”) and (food, “japanese”). Numerous machine learning approaches have been proposed to tackle DST, including fixed-vocabulary models (Ramadan et al., 2018; Lee et al., 2019) and open-vocabulary models (Lei et al., 2018b; Wu et al., 2019; Le et al., 2020c), for either single-domain (Wen et al., 2017) or multi-domain dialogues (Eric et al., 2017; Budzianowski et al., 2018).\nHowever, the research of DST has largely limited the scope of dialogue agents to unimodality. In this setting, the slots and slot values are defined by the knowledge domains (e.g. restaurant domain) and database schema (e.g. data tables for restaurant entities). The ultimate goal of dialogue research towards building artificial intelligent assistants necessitates DST going beyond unimodal systems. In this paper, we propose Multimodal Dialogue State Tracking (MM-DST) that extends the DST task in a multimodal world. Specifically, MM-DST extends the scope of dialogue states by defining slots and slot values for visual objects that are mentioned in visually-grounded dialogues. For research purposes, following (Alamri et al., 2019), we limited visually-grounded dialogues as ones with a grounding video input and the dialogues contain multiple turns of (question, answer) pairs about this video. Each new utterance in such dialogues may focus on a new video segment, new visual objects, or new object attributes, and the tracker is required to update the dialogue state accordingly at each turn. An example of MM-DST can be seen in Figure 1.\nToward MM-DST, we developed a synthetic benchmark based on the CATER universe (Girdhar and Ramanan, 2020). We also introduced VideoDialogue Transformer Network (VDTN), a neural network architecture that combines both objectlevel features and segment-level features in video and learns contextual dependencies between videos\nand dialogues. Specifically, we maintained the information granularity of visual objects, embedded by object classes and their bounding boxes and injected with segment-level visual context. VDTN enables interactions between each visual object representation and word-level representation in dialogues to decode dialogue states. To decode multimodal dialogue states, we adopted a decoding strategy inspired by the Markov decision process in traditional DST (Young et al., 2010). In this strategy, a model learns to decode the state at a dialogue turn based on the predicted/ observed dialogue state available from the last dialogue turn.\nCompared to the conventional DST, MM-DST involves the new modality from visual inputs. Our experiments show that simply combining visual and language representations in traditional DST models results in poor performance. Towards this challenge, we enhanced VDTN with selfsupervised video understanding tasks which recovers object-based or segment-based representations. Benchmarked against strong unimodal DST models, we observed significant performance gains from VDTN. We provided comprehensive ablation analysis to study the efficacy of VDTN models. Interestingly, we also showed that using decoded states brought performance gains in a dialogue response prediction task, supporting our motivation for introducing multimodality into DST research."
    }, {
      "heading" : "2 Multimodal Dialogue State Tracking",
      "text" : "Traditional DST. As defined by (Mrkšić et al., 2017), the traditional DST includes an input of dialogue D and a set of slots S to be tracked from\nturn to turn. At each dialogue turn t, we denote the dialogue context as Dt, containing all utterances up to the current turn. The objective of DST is for each turn t, predict a value vti of each slot si from a predefined set S, conditioned by the dialogue context Dt. We denote the dialogue state at turn t as Bt = {(si, vti)}| i=|S| i=1 . Note that a majority of traditional DST models assume slots are conditionally independent, given the dialogue context (Zhong et al., 2018; Budzianowski et al., 2018; Wu et al., 2019; Lee et al., 2019; Gao et al., 2019). The learning objective is defined as:\nB̂t = argmax Bt P (Bt|Dt, θ)\n= argmax Bt |S|∏ i P (vti |si,Dt, θ) (1)\nMotivation to Multimodality. Yet, the above definition of DST are still limited to unimodality and our ultimate goal of building intelligent dialogue agents, ideally with similar level of intelligence as humans, inspires us to explore mulitmodality. In neuroscience literature, several studies have analyzed how humans can perceive the world in visual context. (Bar, 2004; Xu and Chun, 2009) found that humans can recognize multiple visual objects and how their contexts, often embedded with other related objects, facilitate this capacity.\nOur work is more related to the recent study (Fischer et al., 2020) which focuses on human capacity to create temporal stability across multiple objects. The multimodal DST task is designed to develop multimodal dialogue systems that are capable of\nmaintaining discriminative representations of visual objects over a period of time, segmented by dialogue turns. While computer science literature has focused on related human capacities in intelligent systems, they are mostly limited to vision-only tasks e.g. (He et al., 2016; Ren et al., 2015) or QA tasks e.g. (Antol et al., 2015; Jang et al., 2017) but not in a dialogue task.\nMost related work in the dialogue domain is (Pang and Wang, 2020) and almost concurrent to our work is (Kottur et al., 2021). However, (Kottur et al., 2021) is limited to a single object per dialogue, and (Pang and Wang, 2020) extends to multiple objects but does not require to maintain an information state with component slots for each object. Our work aims to complement these directions and address their limitations with a novel definition of multimodal dialogue state.\nMultimodal DST (MM-DST). To this end, we proposed to extend conventional dialogue states. First, we use visual object identities themselves as a component of the dialogue state to enable the perception of multiple objects. A dialogue state might have one or more objects and a dialogue system needs to update the object set as the dialogue carries on. Secondly, for each object, we define slots that represent the information state of objects in dialogues (as denoted by (Fischer et al., 2020) as “content” features of objects memorized by humans). The value of each slot is subject-specific and updated based on the dialogue context of the corresponding object. This definition of DST is closely based on the above well-studied human capacities while complementing the conventional dialogue research (Young et al., 2010; Mrkšić et al., 2017), and more lately multimodal dialogue research (Pang and Wang, 2020; Kottur et al., 2021).\nWe denote a grounding visual input in the form of a video V with one or more visual objects oj . We assume these objects are semantically different enough (by appearance, by characters, etc.) such that each object can be uniquely identified (e.g. by an object detection module ω). The objective of MM-DST is for each dialogue turn t, predict a value vti of each slot si ∈ S for each object oj ∈ O. We denote the dialogue state at turn t as Bt = |{(oj , si, vti,j)}| i=|S|,j=|O| i=1,j=1 . Assuming all slots are conditionally independent given dialogue and video context, the learning objective is extended from Eq. (1):\nB̂t = argmax Bt P (Bt|Dt,V, θ)\n= argmax Bt |O|∏ j |S|∏ i P (vti,j |si, oj ,Dt,V, θ)P (oj |V, ω)\nOne limitation of the current representation is the absence of temporal placement of objects in time. Naturally humans are able to associate objects and their temporal occurrence over a certain period. Therefore, we defined two temporal-based slots: sstart and send, denoting the start time and end time of the video segment that an object can be located by each dialogue turn. In this work, we assume that a dialogue turn is limited to a single continuous time span, and hence, sstart and send can be defined turn-wise, identically for all objects. While this is a strong assumption, we believe it covers a large portion of natural conversational interactions. An example of multimodal dialogue state can be seen in Figure 1."
    }, {
      "heading" : "3 Video-Dialogue Transformer Network",
      "text" : "A naive adaptation of conventional DST to MMDST is to directly combine visual features extracted by a pretrained 3D-CNN model. However, as shown in our experiments, this extension of conventional DST results in poor performance and does not address the challenge of MM-DST. In this paper, we established a new baseline, denoted as Video-Dialogue Transformer Network (VDTN) (Refer to Fig. 2 for an overview):"
    }, {
      "heading" : "3.1 Visual Perception and Encoder",
      "text" : "Visual Perception. This module encodes videos at both frame-level and segment-level representations. Specifically, we used a pretrained Faster R-CNN model (Ren et al., 2015) to extract object representations. We used this model to output the bounding boxes and object identifiers (object classes) in each video frame of the video. For an object oj , we denoted the four values of its bounding boxes as bbj = (x1j , y 1 j , x 2 j , y 2 j ) and oj as the object class itself. We standardized the video features by extracting features of up to Nobj = 10 objects per frame and normalizing all bounding box coordinates by the frame size. Secondly, we used a pretrained ResNeXt model (Xie et al., 2017) to extract the segment-level representations of videos, denoted as zm ∈ R2048 for a segment m. Practically, we followed the best practice in computer\nvision by using a temporal sliding window with strides to sample video segments and passed segments to ResNeXt model to extract features. To standardize visual features, we use the same striding configuration Nstride to sub-sample segments for ResNeXt and frames for Faster R-CNN models.\nVisual Representation. Note that we do not finetuned the visual feature extractors in VDTN and keep the extracted features fixed. To transform these features into VDTN embedding space, we first concatenated all object identities tokens of OBJ<class>) of all frames, separated by a special token FRAME<number>, where <number> is the temporal order of the frame. This results in a sequence of tokens Xobj of length Lobj = (Nobj+1)×(|V|/Nstride) where |V| is the number of video frames. Correspondingly, we concatenated bounding boxes of all objects, and used a zero vector in positions of FRAME<number> tokens. We denoted this sequence as Xbb ∈ RLobj×4. Similarly, we stacked each ResNeXt feature vector by (Nobj + 1) for each segment, and obtained a sequence Xcnn ∈ RLobj×2048.\nVisual Encoding. We passed each of Xbb and Xcnn to a linear layer with ReLU activation to map their feature dimension to a uniform dimension d. We used a learnable embedding matrix to embed each object identity in Xobj , resulting in embed-\nding features of dimensions d. The final video input representation is the summation of above vectors, denoted as ZV = Zobj + Zbb + Zcnn ∈ RLobj×d."
    }, {
      "heading" : "3.2 Dialogue and State Encoder",
      "text" : "Dialogue Encoding. Another encoder encodes dialogue into continuous representations. Given a dialogue context Dt, We tokenized all dialogue utterances into sequences of words, separated by special tokens USR for human utterance and SYS for system utterance. We used a trainable embedding matrix and sinusoidal positional embeddings to embed this sequence into d-dimensional vectors.\nFlattening State into Sequence. Similar to the recent work in traditional DST (Lei et al., 2018b; Le et al., 2020b; Zhang et al., 2020), we are motivated by the DST decoding strategy following a Markov principle and used the dialogue state of the last dialogue turn Bt−1 as an input to generate the current state Bt. Using the same notations from Section 2, we can represent Bt into a sequence of oj , si, and vti,j tokens, such as “OBJ4 shape cube OBJ24 size small color red”. This sequence is then concatenated with utterances from Dt, separated by a special token PRIOR_STATE. We denoted the resulting sequence as Xctx which is passed to the embedding matrix and positional encoding as described above. As we showed in our experiments, to encode dialogue context, this strategy needs only\na few dialogue utterances (that is closer to the current turn t) and Bt−1, rather than the full dialogue history from turn 1. Therefore, dialogue representations Zctx have more compressed dimensions of |Xctx| × d where |Xctx| < |Dt|."
    }, {
      "heading" : "3.3 Multimodal Transformer Network",
      "text" : "We concatenated both video and dialogue representations, denoted as ZV D = [ZV ;ZD]. ZV D has a length of Lobj + Lctx and embedding dimension d. We pased ZV D to a vanilla Transformer network (Vaswani et al., 2017) through multiple multi-head attention layers with normalization (Ba et al., 2016) and residual connections (He et al., 2016). Each layer allows multimodal interactions between object-level representations from videos and word-level representations from dialogues."
    }, {
      "heading" : "3.4 Dialogue State Decoder and",
      "text" : "Self-supervised Video Denoising Decoder\nState Decoding. This module decodes dialogue state sequence auto-regressively, i.e. each token is conditioned on all dialogue and video representations as well as all tokens previously decoded. At the first decoding position, a special token STATE is embedded into dimension d (by a learned embedding layer and sinusoidal positional encoding) and concatenated to ZV D. The resulting sequence is passed to the Transformer network and the output representations of STATE are passed to a linear that transforms representations to state vocabulary embedding space. The decoder applies the same procedure for the subsequent positions to decode dialogue states auto-regressively. Denoting bk,t as the kth token in Bt, i.e. token of slot, object identity, or slot value, we defined the DST loss function as the negative log-likelihood:\nLdst = − ∑ logP (bk,t|b<k,t, Xctx, Xobj)\nNote that this decoder design partially avoids the assumption of conditionally independent slots. During test time, we applied beam search to decode states with the maximum length of 25 tokens in all models and a beam size 5. An END_STATE token is used to mark the end of each sequence.\nVisual Denoising Decoding. Finally, moving away from conventional unimodal DST, we proposed to enhance our DST model with a Visual Decoder that learns to recovers visual representations in a self-supervised learning task to improve video representation learning. Specifically, during\ntraining time, we randomly sampled visual representations and masked each of them with a zero vector. At the object level, in the mth video frame, we randomly masked a row from Xbb(m) ∈ RNobj×4. Since each row represents an object, we selected a row to mask by a random object index j ∈ [1, Nobj ] such that the same object has not been masked in the preceding frame or following frame. We denote the Transformer output representations from video inputs as Z ′V ∈ RLobj×d. This vector is passed to a linear mapping fbb to bounding box features R4. We defined the learning objective as:\nLobj = ∑ o 1masked × l(fbb(Z ′V,o), Xbb,o)\nwhere l is a loss function and 1masked = {0, 1} is a masking indicator. We experimented with both L1 and L2 loss and reported the results. Similarly, at the segment level, we randomly selected a segment to mask such that the preceding or following segments have not been chosen for masking:\nLseg = ∑ s 1masked × l(fcnn(Z ′V,s), Xcnn,s)"
    }, {
      "heading" : "4 Experiments",
      "text" : "DVD-DST Benchmark. In existing popular benchmarks of multimodal dialogues such as VisDial (Das et al., 2017a), we observed that a large number of data samples contain strong distribution bias in dialogue context, in which dialogue agents can simply ignore the whole dialogue and rely on imageonly features (Kim et al., 2020), or annotation bias, in which the causal link connecting dialogue history and current turn question is actually harmful (Qi et al., 2020). These biases would obviate the need for a DST task.\nWe noticed a recent research work (Le et al., 2021b) that can address both biases. (Le et al., 2021b) proposed to synthetically create dialogues that are grounded on videos from CATER (Shamsian et al., 2020). The videos contain visually simple yet highly varied objects. The dialogues are synthetically designed with both short-term and long-term object references. These specifications remove the annotation bias in terms of object appearances in visual context and cross-turn dependencies in dialogue context.\nWe generated new dialogues following (Le et al., 2021b) procedures but based on an extended CATER video split (Shamsian et al., 2020) rather than the original CATER video data (Girdhar\nand Ramanan, 2020). The extended CATER split (Shamsian et al., 2020) includes additional annotations of ground-truth bounding box boundaries of visual objects in video frames. This annotation facilitates experiments with Faster-RCNN finetuned on CATER objects and experiments with models of perfect visual perception, i.e. P (oj |V, ω) ≈ 1. As shown in (Le et al., 2021b), objects can be uniquely referred in utterances based on their appearance by one or more following aspects: “size”, “color”, “material”, and “shape”. We directly reuse these and define them as slots in our dialogue states, in addition to 2 temporal slots for sstart and send. We denote the new benchmark as DVD-DST and summarize the dataset in Table 1 (for more detail, please refer to Appendix B).\nBaselines. We benchmarked VDTN on DVDDST with the following baseline models: (1) Qretrieval (tf-idf), for each test sample, directly retrieves the training sample the with most similar question utterance and use its state as the predicted state; (2) State prior selects the most common tuple of (object, slot, value) in training split and uses it as predicted states; (3) Object (random), for each test sample, randomly selects one object predicted by the visual perception model and a random (slot, value) tuple (with slots and values inferred from object classes) as the predicted state; (4) Object (all) is similar to (3) but selects all possible objects and all possible (slot, value) tuples as the predicted state; (5) RNN(+Att) uses RNN as encoder and an MLP as decoder with a vanilla dot-product attention; We experimented with strong unimodal DST baselines, including: (6) TRADE (Wu et al., 2019); (7) UniConv (Le et al., 2020b); and (8) NADST (Le et al., 2020c). We implemented baselines (5) to (8) and tested them on dialogues with or without videos. When video inputs are applied, we embedded both object and segment-level features using the same method as described in Section 3.1. The embedded features are integrated into baselines in the same techniques in which the original models treat dialogue representations. Refer to Appendix C for our training details.\nEvaluation. We followed the unimodal DST\ntask (Budzianowski et al., 2018; Henderson et al., 2014a) and used the state accuracy metric. The prediction is counted as correct only when all the component values exactly match the oracle values. In multimodal states, there are both discrete slots (object attributes) as well as continuous slots (temporal start and end time). For continuous slots, we followed (Hu et al., 2016; Gao et al., 2017) by using Intersection-over-Union (IoU) between predicted temporal segment and ground-truth segment. The predicted segment is counted as correct if its IoU with the oracle is more than p, where we chose p = {0.5, 0.7}. We reported the joint state accuracy of discrete slots only (“Joint Acc”) as well as all slot values (“Joint Acc IoU@p”). We also reported the performance of component state predictions, including predictions of object identities oj , object slot tuples (oj , si, vi,j), and object state tuples (oj , si, vi,j)∀si ∈ S. Since a model may simply output all possible object identities and slot values and achieve 100% component accuracies, we reported the F1 metric for each of these component predictions.\nOverall results. From Table 2, we have the following observations: (1) we noted that simply using naive retrieval models such as Q-retrieval achieved zero joint state accuracy only. State prior achieved only about 15% and 8% F1 on object identities and object slots, showing that a model cannot simply rely on distribution bias of dialogue states. (2) The results of Object (random/all) show that in DVD-DST, dialogues often focus on a subset of visual objects and an object perception model alone\ncannot predict dialogue states well. (3) The performance gains of RNN models show the benefits of neural network models compared to retrieval models. The higher results of RNN(D) against RNN(V) showed the dialogue context is essential and reinforced the above observation (2). (4) Comparing TRADE and UniConv, we noted that TRADE performed slightly better in component predictions, but was outperformed in joint state prediction metrics. This showed the benefits of UniConv which avoids the assumptions of conditionally independent slots and learns to extract the dependencies between slot values. (5) Results of TRADE, UniConv, and NADST all displayed minor improvement when adding video inputs to dialogue inputs, displaying their weakness when exposed to crossmodality learning. (6) VDTN achieves significant performance gains and achieves the SOTA results in all component or joint prediction metrics.\n(7) We also experimented with a version of VDTN in which the transformer network (Section 3.3) was initialized from a GPT2-base model (Radford et al., 2019) with a pretrained checkpoint released by HuggingFace1. Asides from using BPE to encode text sequences to match GPT2 embedding indices, we keep other components of the model the same. VDTN+GPT2 is about 36× bigger than our default VDTN model. As shown in Table 2, the performance gains of VDTN+GPT2 indicates the benefits of large-scale language models (LMs). Another benefit of using pretrained GPT2 is faster training time as we observed the VDTN+GPT2 converged much earlier than training it from scratch. From these observations, we are excited to see more future adaptation of large pretrained LMs, such as (Brown et al., 2020; Raffel et al., 2020), or of pretrained multimodal transformer models, such as (Lu et al., 2019; Zhou et al., 2020), in the MM-DST task.\nImpacts of self-supervised video representation learning. From Table 3, we noted that compared to a model trained only with the DST objective Ldst, models enhanced with self-supervised video understanding objectives can improve the results. However, we observe that L1 loss works more consistently than L2 loss in most cases. Since L2 loss minimizes the squared differences between predicted and ground-truth values, it may be susceptible to outliers (of segment features or bounding boxes) in the dataset. Since we could not\n1https://huggingface.co/gpt2\ncontrol these outliers, an L1 loss is more suitable. We also tested with Lobj (tracking), in which we used oracle bounding box labels during training, and simply passed the features of all objects to VDTN for an object tracking task. All output representations are used to predict the ground-truth bounding box coordinates of all objects. Interestingly, we found Lobj (tracking) only improves the results significantly, as compared to the selfsupervised learning objective Lobj . This indicates that our self-supervised learning tasks do not heavily rely on object boundary annotations. Finally, we found combining both segment and object-level self-supervision is not useful. This is possible due to our current masking strategy that masks object and segment features independently. Therefore, the resulting context features might not be sufficient for recovering masked representations.\nImpacts of video features and time-based slots. Table 4 shows the results of different variants of VDTN models. We observed that: (1) We observed that segment-based learning is marginally more powerful than object-based learning. (2) By considering the temporal placement of objects and defining time-based slots, we noted the performance gains by “Joint Obj State Acc” (B vs. B\\time). The performance gains show the interesting relationships between temporal slots and discrete-only slots and the benefits of modelling both in dialogue states. (3) Finally, even with only object-level features Xbb, we still observed performance gains from using self-supervised loss Lobj , confirming the benefits of better visual representation learning.\nAblation analysis by turn positions. Figure 3 reported the results of VDTN predictions of states that are separated by the corresponding dialogue positions. The results are from the VDTN model trained with both Ldst and Lseg. As expected, we observed a downward trend of results as the turn\nposition increases. We noted that state accuracy reduces more dramatically (as shown by “Joint Acc”) than the F1 metrics of component predictions. For instance, “Object Identity F1” shows almost stable performance lines through dialogue turns. Interestingly, we noted that the prediction performance of dialogue states with temporal slots only deteriorates dramatically after turn 2 onward. We expected that VDTN is able to learn short-term dependencies ( 1-turn distance) between temporal slots, but failed to deal with long-term dependencies (> 1-turn distance) between temporal slots. In all metrics, we observed VDTN outperforms both RNN baseline and UniConv (Le et al., 2020b), across all turn positions. However, future work is needed to close the performance gaps between lower and higher turn positions.\nImpacts on downstream response prediction task. Finally, we tested the benefits of studying multimodal DST for a response prediction task. Specifically, we used the best VDTN model to decode dialogue states across all samples in DVDDST. We then used the predicted slots, including object identities and temporal slots, to filter the video inputs by objects and segments. We then used these filtered videos as input to train new VDTN models with an MLP as the response prediction\nlayer. Note that these models are trained only with a cross-entropy loss to predict answer candidates. From Table 5, we observed the benefits of visual inputs filtered by states, resulting in accuracy improvement of up to 5.9% accuracy score. Note that there are more sophisticated approaches such as neural module networks (Hu et al., 2018) and symbolic reasoning (Chen et al., 2020) to fully exploit the decoded dialogue states. We leave these extensions for future research.\nFor more experiment results, analysis, and qualitative examples, please refer to Appendix D."
    }, {
      "heading" : "5 Discussion and Conclusion",
      "text" : "Compared to conventional DST (Mrkšić et al., 2017; Lei et al., 2018b; Gao et al., 2019; Le et al., 2020c), we show that the scope of DST can be further extended to a multimodal world. Compared to prior work in multimodal dialogues (Das et al., 2017a; Hori et al., 2019; Thomason et al., 2019) which focuses more on vision-language interactions, our work was inspired from a dialogue-based strategy with a formulation of a dialogue state tracking task. For more comparison to related work, please refer to Appendix A.\nWe noted the current work are limited to a synthetic benchmark with a limited video domain (3D objects). However, we expect that MM-DST task is still applicable and can be extended to other video domains (e.g. videos of humans). We expect that MM-DST is useful in dialogues centered around a “focus group” of objects. For further discussion of limitations, please refer to Appendix E.\nIn summary, in this work, we introduced a novel MM-DST task that tracks visual objects and their attributes mentioned in dialogues. For this task, we experimented on a synthetic benchmark with videos simulated in a 3D environment and dialogues grounded on these objects. Finally we proposed VDTN, a Transformer-based model with self-supervised learning objectives on object and segment-level visual representations. Our results indicate the multimodal reasoning capacities of our baseline and the benefits of MM-DST in an end-toend dialogue system."
    }, {
      "heading" : "6 Broader Impacts",
      "text" : "During the research of this work, there is no human subject involved and hence, no ethical concerns regarding the experimental procedures and results. The data is used from a synthetically developed dataset, in which all videos are simulated in a 3D environment with synthetic non-human visual objects. We intentionally chose this dataset to minimize any distribution bias and make fair comparisons between all baseline models.\nHowever, we wanted to emphasize on ethical usage of any potential adaptation of our methods in real applications. Considering the development of AI in various industries, the technology introduced in this paper may be used in practical applications, such as dialogue agents with human users. In these cases, the adoption of the MM-DST task or VDTN should be strictly used to improve the model performance and only for legitimate and authorized purposes. It is crucial that any plan to apply or extend MM-DST in real systems should consider carefully all potential stakeholders as well as the risk profiles of application domains. For instance, in case a dialogue state is extended to human subjects, any information used as slots should be clearly informed and approved by the human subjects before the slots are tracked."
    }, {
      "heading" : "A Details of Related Work",
      "text" : "Our work is related to the following domains:\nA.1 Dialogue State Tracking\nDialogue State Tracking (DST) research aims to develop models that can track essential information conveyed in dialogues between a dialogue agent and human (defined as hidden information state by (Young et al., 2010) or belief state by (Mrkšić et al., 2017)). DST research has evolved largely within the domain of task-oriented dialogue systems. DST is conventionally designed in a modular dialogue system (Wen et al., 2017; Budzianowski et al., 2018; Le et al., 2020b) and preceded by a Natural Language Understanding (NLU) component. NLU learns to label sequences of dialogue utterances and provides a tag for each word token (often in the form of In-Out-Begin representations) (Kurata et al., 2016; Shi et al., 2016; Rastogi et al., 2017). To avoid credit assignment problems and streamline the modular designs, NLU and DST have been integrated into a single module (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018). These DST approaches can be roughly categorized into two types: fixed-vocabulary or open-vocabulary. Fixedvocabulary approaches are designed for classification tasks which assume a fixed set of (slot, value) candidates and directly retrieve items from this set to form dialogue states during test time (Henderson et al., 2014b; Ramadan et al., 2018; Lee et al., 2019). More recently, we saw more approaches toward open-vocabulary strategies which learn to generate candidates based on input dialogue context (Lei et al., 2018b; Gao et al., 2019; Wu et al., 2019; Le et al., 2020c). Our work is more related to open-vocabulary DST, but we essentially redefined the DST task with multimodality. Based on our literature review, we are the first to formally extend DST and bridge the gap between traditional task-oriented dialogues and multimodal dialogues.\nA.2 Visually-grounded Dialogues\nA novel challenge to machine intelligence, the intersection of vision and language research has expanded considerably in the past few years. Earlier benchmarks test machines to perceive visual inputs, and learn to generate captions (Farhadi et al., 2010; Lin et al., 2014; Rohrbach et al., 2015), ground text phrases and objects (Kazemzadeh et al., 2014; Plummer et al., 2015), and answer questions about the visual contents (Antol et al., 2015; Zhu et al.,\n2016; Jang et al., 2017; Lei et al., 2018a). As an orthogonal development from Visual Question Answering problems, we noted recent work that targets vision-language in dialogue context, in which an image or video is given and the dialogue utterances are centered around its visual contents (De Vries et al., 2017; Das et al., 2017a; Chattopadhyay et al., 2017; Hori et al., 2019; Thomason et al., 2019; Le et al., 2021b). Recent work has addressed different challenges in visually-grounded dialogues, including multimodal integration (Hori et al., 2019; Le et al., 2019; Li et al., 2021), crossturn dependencies (Das et al., 2017b; Schwartz et al., 2019; Le et al., 2021a), visual understanding (Le et al., 2020a), and data distribution bias (Qi et al., 2020). Our work is more related to the challenge of visual object reasoning (Seo et al., 2017; Kottur et al., 2018), but focused on a multi-turn tracking task over multiple turns of dialogue context. The prior approaches are not well designed to track objects and maintain a recurring memory or state of these objects from turn to turn. This challenge becomes more obvious when a dialogue involves multiple objects of similar characters or appearance. We directly tackles this challenge as we formulated a novel multimodal state tracking task and leveraged the research development from DST in task-oriented dialogue systems. As shown in our experiments, baseline models that use attention strategies similar to (Seo et al., 2017; Kottur et al., 2018) did not perform well in MM-DST.\nA.3 Multimodal DST\nWe noted a few studies have attempted to integrate some forms of state tracking in multimodal dialogues. In (Mou et al., 2020), however, we are not convinced that a dialogue state tracking task is a major focus, or correctly defined. In (Pang and Wang, 2020), we noted that some form of object tracking is introduced throughout dialogue turns. The tracking module is used to decide which object the dialogue centers around. This method extends to multi-object tracking but the objects are only limited within static images, and there is no recurring information state (object attributes) maintained at each turn. Compared to our work, their tracking module only requires object identity as a single-slot state from turn to turn. Almost concurrent to our work, we noted (Kottur et al., 2021) which formally, though very briefly, focuses on multimodal DST. However, the work is limited to the task-oriented\ndomain, and each dialogue is only limited to a single goal-driven object in a synthetic image. While this definition is useful in the task-oriented dialogue domain, it does not account for the DST of multiple visual objects as defined in our work."
    }, {
      "heading" : "B DVD-DST Dataset Details",
      "text" : "For each of CATER videos from the extended split (Shamsian et al., 2020), we generated up to 10 turns for each CATER video. In total, DVD-DST contains more than 13k dialogues, resulting in more 130k (human, system) utterance pairs and corresponding dialogue states. A comparison of statistics of DVD-DST and prior DST benchmarks can be seen in Table 6. We observed that DVD-DST contains a larger scale data than the related DST benchmark. Even though the number of slots in DVD-DST is only 6, lower than prior state tracking datasets, our experiments indicate that most current conventional DST models perform poorly on DVD-DST.\nCATER universe. Figure 4 displays the configuration of visual objects in the CATER universe. In total, there are 3 object sizes, 9 colors, 2 materials, and 5 shapes. These attributes are combined randomly to synthesize objects in each CATER video. We directly adopted these attributes as slots in dialogue states, and each dialogue utterance frequently refers to these objects by one or more attributes. In total, there are 193 (size, color, material, shape)\nvalid combinations, each of which corresponds to an object class in our models.\nSample dialogues. Please refer to Figure 5, Table 14 and Table 15.\nUsage. We want to highlight that the DVD-DST dataset should only be used for its intended purpose, i.e. to diagnose dialogue systems on their tracking abilities. Any derivatives of the data should be limited within the research contexts of MM-DST."
    }, {
      "heading" : "C Training Details",
      "text" : "We trained VDTN by jointly minimizing Ldst and Lbb/cnn. In practice, we applied label smoothing (Szegedy et al., 2016) on state sequence labels to regularize the training. As the segment-level representations are stacked by the number of objects, we randomly selected only one vector per masked segment to apply Lseg. We tested both L1 and L2 losses on Lbb/cnn. We trained all models using the Adam optimizer (Kingma and Ba, 2015) with a warm-up learning rate period of 1 epoch and the learning rate decays up to 160 epochs. Models are selected based on the average Ldst on the validation set. All model parameters, except pretrained visual perception models, are initialized by a uniform distribution (Glorot and Bengio, 2010). To standardize model sizes, we selected embedding dimension d = 128 for all models, and experimented with both shallow (N = 1) and deep networks (N = 3) (by stacking attention or RNN blocks),\nand 8 attention heads in Transformer backbones. For fair comparison among baselines, all models use both object-level and segment-level feature representations, encoded by the same method as Describe in Section 3.1. In TRADE, the video representations are passed to an RNN encoder, and the output hidden states are concatenated to the dialogue hidden states. Both are passed to the original pointer-based decoder. In UniConv and NADAST, we stacked another Transformer attention layer to attend on video representations before the original state-to-dialogue attention layer. We all baseline models, we replaced the original (domain, slot) embeddings as (object class, slot) embeddings and kept the original model designs.\nNote that in our visual perception model, we adopted the finetuned Faster R-CNN model used by (Shamsian et al., 2020). The model was finetuned to predict object bounding boxes and object classes. The object classes are derived based on object appearance, based on the four attributes of size, color, material, and shape. In total, there are 193 object classes. For segment embeddings, we adopted the ResNeXt-101 model (Xie et al., 2017) finetuned on Kinetics dataset (Kay et al., 2017). For all models (except for VDTN ablation analysis), we standardized Nobj = 10 and Nstride = 12 to sub-sample object and segment-level embeddings.\nResources. Note that all experiments did not require particularly large computing resources as we limited all model training to a single GPU, specifically on a Tesla V100 GPU of 16G configuration."
    }, {
      "heading" : "D Additional Results",
      "text" : "Greedy vs. Beam Search Decoding. Table 7 shows the results of different variants of VDTN models. We observed that compared to greedy decoding, beam search decoding improves the performance in all models. As beam search decoding selects the best decoded state by the joint probabilities of tokens, this observation indicates the benefits of considering slot values to be co-dependent and their relationships should be modelled. This is consistent with similar observations in later work of unimodal DST (Lei et al., 2018b; Le et al., 2020c).\nAblation analysis by component predictions. From Table 8, we have the following observations: (1) In ablation results by component predictions, we noted that models can generally detect object identities well with F1 about 80%. However, when considering object and slot tuples, F1 reduces to\n48 − 60%, indicating the gaps are caused by slot value predictions. (2) By individual slots, we noted “color” and “shape” slots are easier to track than “size” and “material” slots. We noted that in the CATER universe, the latter two slots have lower visual variances (less possible values) than the others. As a result, objects are more likely to share the same size or material and hence, discerning objects by those slots and tracking them in dialogues become more challenging.\nTable 9 and 10 display the ablation results by component predictions, using precision and recall metrics. We still noted consistent observations as described in Section 4. Notably, we found that current VDTN models are better in tuning the correct predictions (as shown by high precision metrics) but still fail to select all components as a set (as shown by low recall metrics). This might be caused by the upstream errors coming from the visual perception models, which may fail to visually perceive all objects and their attributes.\nResults by turn positions. Table 11 reported the results of VDTN predictions of states that are separated by the corresponding dialogue positions. The results are from the VDTN model trained with both Ldst and Lseg. As expected, we observed a downward trend of results as the turn position increases.\nImpacts of dialogue context encoder. In Table 12a, we observed the benefits of using the Markov process to decode dialogue states based on the dialogue states of the last turn Bt−1. This strategy allow us to discard parts of dialogue history that is already represented by the state. We noted that the optimal design is to use at least 1 last dialogue turn as the dialogue history. In a hypothetical scenario, we applied the oracle Bt−1 during test time, and noted the performance is improved significantly. This observation indicates the sensitivity of VDTN to a turn-wise auto-regressive decoding process.\nImpacts of frame-level and segment-level sampling. As expected, Table 12b displays higher performance with higher object limits Nobj , which increases the chance of detecting the right visual objects in videos. We noted performance gains when sampling strides increase up to 24 frames. However, in the extreme case, when sampling stride is 300 frames, the performance on temporal slots reduce (as shown by “Joint State IoU@p”). This raises the issue to sample data more efficiently by balancing between temporal sparsity in videos and\nstate prediction performance. We also observed that in a hypothetical scenario with a perfect object perception model, the performance improves significantly, especially on the predictions of discrete slots, although less effect on temporal slots.\nImpacts of object-level representation. Table 13 reported the results when only segment-level features are used. We observed that both VDTN and RNN(V+D) are affected significantly, specifically by 24% and 3.1% “Joint Obj State Acc” score respectively. Interestingly, we noted that RNN(V), using only video inputs, are not affected by the removal of object-level features. These observations indicate that current MM-DST requires object-level information. We expected that existing 3DCNN models such as ResNeXt still fail to capture such level of granularity.\nQualitative analysis. Table 14 and 15 display 2 sample dialogues and state predictions. We displayed the corresponding video screenshots for these dialogues in Figure 5. To cross-reference between videos and dialogues, we displayed the bounding boxes and their object classes in video screenshots. These object classes are indicated in ground-truth and decoded dialogue states in dialogues. Overall, we noted that VDTN generated\ntemporal slots of start and end time such that the resulting periods better match the ground-truth temporal segments. VDTN also showed to maintain the dialogue states better from turn to turn."
    }, {
      "heading" : "E Further Discussion",
      "text" : "Synthetic datasets result in overestimation of real performance and don’t translate to realworld usability. We agree that the current state accuracy seems to be quite low at about 28%. However, we want to highlight that state accuracy used in this paper is a very strict metric, which only considers a prediction as correct if it completely matches the ground truth. In DVD, assuming the average 10 objects per video with the set of attributes as in Figure 4 (+ ‘none’ value in each slot), we can roughly equate the multimodal DST as a 7200-class classification task, each class is a distinct set of objects, each with all possible attribute combinations. Combined with the cascading error from object perception models, we think the current reported results are reasonable.\nMoreover, we want to highlight that the reported performance of baselines reasonably matches their own capacities in unimodal DST. We can consider Object State F1 as the performance on single-object state and it can closely correlate with the joint\nstate accuracy in unimodal DST (remember that unimodal DST such as MultiWOZ (Budzianowski et al., 2018) is only limited to a single object/entity per dialogue). As seen in Table 2, the Object State F1 results of TRADE (Wu et al., 2019), UniConv (Le et al., 2020b), and NADST (Le et al., 2020c) are between 46-50%. This performance range is indeed not very far off from the performance of these baseline models in unimodal DST in the MultiWOZ benchmark (Budzianowski et al., 2018).\nFinally, we also want to highlight that like other synthetic benchmarks such as CLEVR (Johnson et al., 2017), we want to use DVD in this work as a test bed to study and design better multimodal dialogue systems. However, we do not intend to use it as a training data for practical systems. The DVDDST benchmark should be used to supplement realworld video-grounded dialogue datasets.\nMM-DST in practical applications e.g. with videos of humans. While we introduced MMDST task and VDTN as a new baseline, we noted that the existing results are limited to the synthetic benchmark. For instance, in the real world, there would be many identical objects with the same (size, color, material, shape) tuples, which would make the current formulation of dialogue states difficult. In such object-driven conversations, we would recommend a dialogue agent not focus on all possible objects in each video frame, but only on a “focus group” of objects. These objects, required\nto be semantically different, are topical subjects of the conversations.\nSay we want to scale to a new domain e.g. videos of humans, the first challenge from the current study is the recognition of human objects, which often have higher visual complexity than moving objects as in DVD. We also noted that it is impossible to define all human object classes as in CATER object classes, each of which is unique by its own appearance. To overcome this limitation, we would want to explore multimodal DST with the research of human object tracking, e.g. (Fernando et al., 2018), and consider human object identities uniquely defined per video. Another limitation is the definition of slots to track in each human object. While this requires careful considerations, for both practical and ethical reasons, we noted several potential papers that investigate human attributes in dialogues such as human emotions (Wang et al., 2021). Along these lines, we are excited to see interesting adaptations of multimodal dialogue states grounded on videos of humans.\nTurn Position Obj Identity F1 Obj Slot F1 Obj State F1 Joint Obj State Acc Joint State IoU@0.5 Joint State IoU@0.7\n1 88.8% 84.0% 82.4% 74.0% 40.5% 34.6% 2 86.9% 81.1% 77.2% 60.0% 37.5% 33.6% 3 84.9% 77.6% 71.0% 41.6% 22.8% 19.5% 4 84.2% 75.6% 66.5% 29.0% 15.2% 12.5% 5 84.0% 74.0% 63.1% 21.3% 11.3% 9.4% 6 84.3% 73.0% 60.2% 17.1% 9.6% 8.2% 7 83.9% 71.6% 57.1% 12.7% 6.1% 5.3% 8 84.1% 70.6% 54.9% 10.2% 4.7% 3.9% 9 84.0% 69.1% 51.8% 7.9% 3.6% 2.6%\n10 84.1% 68.0% 49.5% 6.0% 2.3% 1.7% Average 84.9% 74.5% 63.4% 28.0% 15.3% 13.1%\n(a) dialogue encoding by prior states and dialogue sizes: ∗ denotes using oracle values.\n(b) video encoding by number of objects and sampling strides: ∗ denotes perfect object perception.\nTable 12: Ablation results by encoding strategies: All models are trained only with Ldst.\nXbb +Xcnn Xcnn only\nModel Joint ObjState Acc Joint State IoU@0.5 Joint State IoU@0.7 Joint Obj State Acc Joint State IoU@0.5 Joint State IoU@0.7 VDTN 28.0% 15.3% 13.1% 4.0% 2.2% 2.0% RNN(V) 1.0% 0.1% 0.1% 1.5% 0.4% 0.4% RNN(V+D) 6.8% 2.6% 2.3% 3.7% 1.8% 1.6%\nTable 13: Results with and without object representations"
    } ],
    "references" : [ {
      "title" : "Audio-visual scene-aware dialog",
      "author" : [ "Huda Alamri", "Vincent Cartillier", "Abhishek Das", "Jue Wang", "Stefan Lee", "Peter Anderson", "Irfan Essa", "Devi Parikh", "Dhruv Batra", "Anoop Cherian", "Tim K. Marks", "Chiori Hori." ],
      "venue" : "Proceedings of the IEEE Conference on",
      "citeRegEx" : "Alamri et al\\.,? 2019",
      "shortCiteRegEx" : "Alamri et al\\.",
      "year" : 2019
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2425–2433.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Visual objects in context",
      "author" : [ "Moshe Bar." ],
      "venue" : "Nature Reviews Neuroscience, 5(8):617–629.",
      "citeRegEx" : "Bar.,? 2004",
      "shortCiteRegEx" : "Bar.",
      "year" : 2004
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual dialog",
      "author" : [ "Dhruv Batra." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Batra.,? 2017a",
      "shortCiteRegEx" : "Batra.",
      "year" : 2017
    }, {
      "title" : "2017b. Learning cooperative",
      "author" : [ "Lee", "Dhruv Batra" ],
      "venue" : null,
      "citeRegEx" : "Lee and Batra.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lee and Batra.",
      "year" : 2017
    }, {
      "title" : "2017. Key-value retrieval",
      "author" : [ "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Manning.,? \\Q2017\\E",
      "shortCiteRegEx" : "Manning.",
      "year" : 2017
    }, {
      "title" : "Context information supports serial dependence of multiple visual objects across memory episodes",
      "author" : [ "Cora Fischer", "Stefan Czoschke", "Benjamin Peters", "Benjamin Rahm", "Jochen Kaiser", "Christoph Bledowski." ],
      "venue" : "Nature communications, 11(1):1–11.",
      "citeRegEx" : "Fischer et al\\.,? 2020",
      "shortCiteRegEx" : "Fischer et al\\.",
      "year" : 2020
    }, {
      "title" : "Tall: Temporal activity localization via language query",
      "author" : [ "Jiyang Gao", "Chen Sun", "Zhenheng Yang", "Ram Nevatia." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 5267– 5275.",
      "citeRegEx" : "Gao et al\\.,? 2017",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2017
    }, {
      "title" : "Dialog state tracking: A neural reading comprehension approach",
      "author" : [ "Shuyang Gao", "Abhishek Sethi", "Sanchit Aggarwal", "Tagyoung Chung", "Dilek Hakkani-Tur." ],
      "venue" : "arXiv preprint arXiv:1908.01946.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Cater: A diagnostic dataset for compositional actions and temporal reasoning",
      "author" : [ "Rohit Girdhar", "Deva Ramanan." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Girdhar and Ramanan.,? 2020",
      "shortCiteRegEx" : "Girdhar and Ramanan.",
      "year" : 2020
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256.",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "The second dialog state tracking challenge",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Jason D Williams." ],
      "venue" : "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263–272.",
      "citeRegEx" : "Henderson et al\\.,? 2014a",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Steve J. Young." ],
      "venue" : "2014 IEEE Spoken Language Technology Workshop (SLT), pages 360–365.",
      "citeRegEx" : "Henderson et al\\.,? 2014b",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Endto-end audio visual scene-aware dialog using multimodal attention-based video features",
      "author" : [ "C. Hori", "H. Alamri", "J. Wang", "G. Wichern", "T. Hori", "A. Cherian", "T.K. Marks", "V. Cartillier", "R.G. Lopes", "A. Das", "I. Essa", "D. Batra", "D. Parikh." ],
      "venue" : "ICASSP",
      "citeRegEx" : "Hori et al\\.,? 2019",
      "shortCiteRegEx" : "Hori et al\\.",
      "year" : 2019
    }, {
      "title" : "Explainable neural computation via stack neural module networks",
      "author" : [ "Ronghang Hu", "Jacob Andreas", "Trevor Darrell", "Kate Saenko." ],
      "venue" : "Proceedings of the European conference on computer vision (ECCV), pages 53–69.",
      "citeRegEx" : "Hu et al\\.,? 2018",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2018
    }, {
      "title" : "Natural language object retrieval",
      "author" : [ "Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4555–4564.",
      "citeRegEx" : "Hu et al\\.,? 2016",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Tgif-qa: Toward spatiotemporal reasoning in visual question answering",
      "author" : [ "Yunseok Jang", "Yale Song", "Youngjae Yu", "Youngjin Kim", "Gunhee Kim." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758–2766.",
      "citeRegEx" : "Jang et al\\.,? 2017",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "author" : [ "Justin Johnson", "Bharath Hariharan", "Laurens Van Der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE conference",
      "citeRegEx" : "Johnson et al\\.,? 2017",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "The kinetics human action video",
      "author" : [ "Will Kay", "Joao Carreira", "Karen Simonyan", "Brian Zhang", "Chloe Hillier", "Sudheendra Vijayanarasimhan", "Fabio Viola", "Tim Green", "Trevor Back", "Paul Natsev" ],
      "venue" : null,
      "citeRegEx" : "Kay et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kay et al\\.",
      "year" : 2017
    }, {
      "title" : "Referitgame: Referring to objects in photographs of natural scenes",
      "author" : [ "Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara Berg." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787–",
      "citeRegEx" : "Kazemzadeh et al\\.,? 2014",
      "shortCiteRegEx" : "Kazemzadeh et al\\.",
      "year" : 2014
    }, {
      "title" : "Modality-balanced models for visual dialogue",
      "author" : [ "Hyounghun Kim", "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8091–8098.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederick P Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "SIMMC 2.0: A taskoriented dialog dataset for immersive multimodal conversations",
      "author" : [ "Satwik Kottur", "Seungwhan Moon", "Alborz Geramifard", "Babak Damavandi" ],
      "venue" : "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-",
      "citeRegEx" : "Kottur et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2021
    }, {
      "title" : "Visual coreference resolution in visual dialog using neural module networks",
      "author" : [ "Satwik Kottur", "José MF Moura", "Devi Parikh", "Dhruv Batra", "Marcus Rohrbach." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 153–169.",
      "citeRegEx" : "Kottur et al\\.,? 2018",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2018
    }, {
      "title" : "Leveraging sentence-level information with encoder LSTM for semantic slot filling",
      "author" : [ "Gakuto Kurata", "Bing Xiang", "Bowen Zhou", "Mo Yu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2077–2083,",
      "citeRegEx" : "Kurata et al\\.,? 2016",
      "shortCiteRegEx" : "Kurata et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning reasoning paths over semantic graphs for videogrounded dialogues",
      "author" : [ "Hung Le", "Nancy F. Chen", "Steven Hoi." ],
      "venue" : "International Conference on Learning Representations. 10",
      "citeRegEx" : "Le et al\\.,? 2021a",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2021
    }, {
      "title" : "Multimodal transformer networks for end-toend video-grounded dialogue systems",
      "author" : [ "Hung Le", "Doyen Sahoo", "Nancy Chen", "Steven Hoi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5612–5623, Flo-",
      "citeRegEx" : "Le et al\\.,? 2019",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2019
    }, {
      "title" : "BiST: Bi-directional spatio-temporal reasoning for video-grounded dialogues",
      "author" : [ "Hung Le", "Doyen Sahoo", "Nancy Chen", "Steven C.H. Hoi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Le et al\\.,? 2020a",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "UniConv: A unified conversational neural architecture for multi-domain task-oriented dialogues",
      "author" : [ "Hung Le", "Doyen Sahoo", "Chenghao Liu", "Nancy Chen", "Steven C.H. Hoi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Le et al\\.,? 2020b",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "DVD: A diagnostic dataset for multi-step reasoning in video grounded dialogue",
      "author" : [ "Hung Le", "Chinnadhurai Sankar", "Seungwhan Moon", "Ahmad Beirami", "Alborz Geramifard", "Satwik Kottur." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for",
      "citeRegEx" : "Le et al\\.,? 2021b",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2021
    }, {
      "title" : "Non-autoregressive dialog state tracking",
      "author" : [ "Hung Le", "Richard Socher", "Steven C.H. Hoi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Le et al\\.,? 2020c",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "Sumbt: Slot-utterance matching for universal and scalable belief tracking",
      "author" : [ "Hwaran Lee", "Jinsik Lee", "Tae yoon Kim." ],
      "venue" : "ACL.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "TVQA: Localized, compositional video question answering",
      "author" : [ "Jie Lei", "Licheng Yu", "Mohit Bansal", "Tamara Berg." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1369–1379, Brussels, Belgium.",
      "citeRegEx" : "Lei et al\\.,? 2018a",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
      "author" : [ "Wenqiang Lei", "Xisen Jin", "Min-Yen Kan", "Zhaochun Ren", "Xiangnan He", "Dawei Yin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for",
      "citeRegEx" : "Lei et al\\.,? 2018b",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "Bridging text and video: A universal multimodal transformer for video-audio scene-aware dialog",
      "author" : [ "Zekang Li", "Zongjia Li", "Jinchao Zhang", "Yang Feng", "Jie Zhou." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, pages 1–1.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "C Lawrence Zitnick" ],
      "venue" : "In European conference on computer vision,",
      "citeRegEx" : "Zitnick.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zitnick.",
      "year" : 2014
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multimodal dialogue state tracking by qa approach with data augmentation",
      "author" : [ "Xiangyang Mou", "Brandyn Sigouin", "Ian Steenstra", "Hui Su." ],
      "venue" : "arXiv preprint arXiv:2007.09903.",
      "citeRegEx" : "Mou et al\\.,? 2020",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural belief tracker: Data-driven dialogue state tracking",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "Visual dialogue state tracking for question generation",
      "author" : [ "Wei Pang", "Xiaojie Wang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11831–11838.",
      "citeRegEx" : "Pang and Wang.,? 2020",
      "shortCiteRegEx" : "Pang and Wang.",
      "year" : 2020
    }, {
      "title" : "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
      "author" : [ "Bryan A Plummer", "Liwei Wang", "Chris M Cervantes", "Juan C Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Plummer et al\\.,? 2015",
      "shortCiteRegEx" : "Plummer et al\\.",
      "year" : 2015
    }, {
      "title" : "Two causal principles for improving visual dialog",
      "author" : [ "Jiaxin Qi", "Yulei Niu", "Jianqiang Huang", "Hanwang Zhang." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10860–10869.",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog,",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Large-scale multi-domain belief tracking with knowledge sharing",
      "author" : [ "Osman Ramadan", "Paweł Budzianowski", "Milica Gasic." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, volume 2, pages 432–437.",
      "citeRegEx" : "Ramadan et al\\.,? 2018",
      "shortCiteRegEx" : "Ramadan et al\\.",
      "year" : 2018
    }, {
      "title" : "Scalable multi-domain dialogue state tracking",
      "author" : [ "Abhinav Rastogi", "Dilek Z. Hakkani-Tür", "Larry P. Heck." ],
      "venue" : "2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 561– 568.",
      "citeRegEx" : "Rastogi et al\\.,? 2017",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2017
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "Advances in neural information processing systems, 28:91–99.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "A dataset for movie description",
      "author" : [ "Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3202–3212.",
      "citeRegEx" : "Rohrbach et al\\.,? 2015",
      "shortCiteRegEx" : "Rohrbach et al\\.",
      "year" : 2015
    }, {
      "title" : "Factor graph attention",
      "author" : [ "Idan Schwartz", "Seunghak Yu", "Tamir Hazan", "Alexander G Schwing." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2039–2048.",
      "citeRegEx" : "Schwartz et al\\.,? 2019",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2019
    }, {
      "title" : "Visual reference resolution using attention memory for visual dialog",
      "author" : [ "Paul Hongsuck Seo", "Andreas Lehrmann", "Bohyung Han", "Leonid Sigal." ],
      "venue" : "Advances in neural information processing systems, pages 3719–3729.",
      "citeRegEx" : "Seo et al\\.,? 2017",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning object permanence from video",
      "author" : [ "Aviv Shamsian", "Ofri Kleinfeld", "Amir Globerson", "Gal Chechik." ],
      "venue" : "European Conference on Computer Vision, pages 35–50. Springer.",
      "citeRegEx" : "Shamsian et al\\.,? 2020",
      "shortCiteRegEx" : "Shamsian et al\\.",
      "year" : 2020
    }, {
      "title" : "Recurrent support vector machines for slot tagging in spoken language understanding",
      "author" : [ "Yangyang Shi", "Kaisheng Yao", "Hu Chen", "Dong Yu", "YiCheng Pan", "Mei-Yuh Hwang." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the",
      "citeRegEx" : "Shi et al\\.,? 2016",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Vision-and-dialog navigation",
      "author" : [ "Jesse Thomason", "Michael Murray", "Maya Cakmak", "Luke Zettlemoyer." ],
      "venue" : "Conference on Robot Learning (CoRL).",
      "citeRegEx" : "Thomason et al\\.,? 2019",
      "shortCiteRegEx" : "Thomason et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A contextual attention network for multimodal emotion recognition in conversation",
      "author" : [ "Tana Wang", "Yaqing Hou", "Dongsheng Zhou", "Qiang Zhang." ],
      "venue" : "2021 International Joint Conference on Neural Networks (IJCNN), pages 1–7. IEEE.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "A network-based end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "David Vandyke", "Nikola Mrkšić", "Milica Gašić", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young" ],
      "venue" : null,
      "citeRegEx" : "Wen et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2017
    }, {
      "title" : "Transferable multi-domain state generator for task-oriented dialogue systems",
      "author" : [ "Chien-Sheng Wu", "Andrea Madotto", "Ehsan Hosseini-Asl", "Caiming Xiong", "Richard Socher", "Pascale Fung." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Aggregated residual transformations for deep neural networks",
      "author" : [ "Saining Xie", "Ross Girshick", "Piotr Dollár", "Zhuowen Tu", "Kaiming He." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500.",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "An end-to-end approach for handling unknown slot values in dialogue state tracking",
      "author" : [ "Puyang Xu", "Qi Hu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1448–1457. Association",
      "citeRegEx" : "Xu and Hu.,? 2018",
      "shortCiteRegEx" : "Xu and Hu.",
      "year" : 2018
    }, {
      "title" : "Selecting and perceiving multiple visual objects",
      "author" : [ "Yaoda Xu", "Marvin M Chun." ],
      "venue" : "Trends in cognitive sciences, 13(4):167–174.",
      "citeRegEx" : "Xu and Chun.,? 2009",
      "shortCiteRegEx" : "Xu and Chun.",
      "year" : 2009
    }, {
      "title" : "The hidden information state model: A practical framework for pomdp-based spoken dialogue management",
      "author" : [ "Steve Young", "Milica Gašić", "Simon Keizer", "François Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu." ],
      "venue" : "Computer Speech and Language,",
      "citeRegEx" : "Young et al\\.,? 2010",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2010
    }, {
      "title" : "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking",
      "author" : [ "Jianguo Zhang", "Kazuma Hashimoto", "Chien-Sheng Wu", "Yao Wang", "Philip Yu", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the Ninth Joint Confer-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Global-locally self-attentive encoder for dialogue state tracking",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1458–",
      "citeRegEx" : "Zhong et al\\.,? 2018",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2018
    }, {
      "title" : "Unified visionlanguage pre-training for image captioning and vqa",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason Corso", "Jianfeng Gao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13041–13049.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual7w: Grounded question answering in images",
      "author" : [ "Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li FeiFei." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995–5004.",
      "citeRegEx" : "Zhu et al\\.,? 2016",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    }, {
      "title" : "Rohrbach et al., 2015), ground text phrases and objects (Kazemzadeh et al., 2014; Plummer et al., 2015), and answer questions about the visual contents",
      "author" : [ "Lin" ],
      "venue" : "(Antol et al.,",
      "citeRegEx" : "Lin,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin",
      "year" : 2014
    }, {
      "title" : "2018a). As an orthogonal development from Visual Question Answering problems, we noted recent work that targets vision-language in dialogue context, in which an image or video is given and the dialogue",
      "author" : [ "Jang et al", "Lei" ],
      "venue" : null,
      "citeRegEx" : "2017 and Lei,? \\Q2018\\E",
      "shortCiteRegEx" : "2017 and Lei",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 47,
      "context" : "DST, including fixed-vocabulary models (Ramadan et al., 2018; Lee et al., 2019) and open-vocabulary models (Lei et al.",
      "startOffset" : 39,
      "endOffset" : 79
    }, {
      "referenceID" : 34,
      "context" : "DST, including fixed-vocabulary models (Ramadan et al., 2018; Lee et al., 2019) and open-vocabulary models (Lei et al.",
      "startOffset" : 39,
      "endOffset" : 79
    }, {
      "referenceID" : 36,
      "context" : ", 2019) and open-vocabulary models (Lei et al., 2018b; Wu et al., 2019; Le et al., 2020c), for either single-domain (Wen et al.",
      "startOffset" : 35,
      "endOffset" : 89
    }, {
      "referenceID" : 60,
      "context" : ", 2019) and open-vocabulary models (Lei et al., 2018b; Wu et al., 2019; Le et al., 2020c), for either single-domain (Wen et al.",
      "startOffset" : 35,
      "endOffset" : 89
    }, {
      "referenceID" : 33,
      "context" : ", 2019) and open-vocabulary models (Lei et al., 2018b; Wu et al., 2019; Le et al., 2020c), for either single-domain (Wen et al.",
      "startOffset" : 35,
      "endOffset" : 89
    }, {
      "referenceID" : 59,
      "context" : ", 2020c), for either single-domain (Wen et al., 2017) or multi-domain dialogues (Eric et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "For research purposes, following (Alamri et al., 2019), we limited visually-grounded dialogues as ones with a grounding video input and the dialogues contain multiple turns of (question, answer) pairs about this video.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "Toward MM-DST, we developed a synthetic benchmark based on the CATER universe (Girdhar and Ramanan, 2020).",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 64,
      "context" : "strategy inspired by the Markov decision process in traditional DST (Young et al., 2010).",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 41,
      "context" : "As defined by (Mrkšić et al., 2017), the traditional DST includes an input of dialogue D and a set of slots S to be tracked from turn to turn.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 66,
      "context" : "Note that a majority of traditional DST models assume slots are conditionally independent, given the dialogue context (Zhong et al., 2018; Budzianowski et al., 2018; Wu et al., 2019; Lee et al., 2019; Gao et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 218
    }, {
      "referenceID" : 60,
      "context" : "Note that a majority of traditional DST models assume slots are conditionally independent, given the dialogue context (Zhong et al., 2018; Budzianowski et al., 2018; Wu et al., 2019; Lee et al., 2019; Gao et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 218
    }, {
      "referenceID" : 34,
      "context" : "Note that a majority of traditional DST models assume slots are conditionally independent, given the dialogue context (Zhong et al., 2018; Budzianowski et al., 2018; Wu et al., 2019; Lee et al., 2019; Gao et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 218
    }, {
      "referenceID" : 10,
      "context" : "Note that a majority of traditional DST models assume slots are conditionally independent, given the dialogue context (Zhong et al., 2018; Budzianowski et al., 2018; Wu et al., 2019; Lee et al., 2019; Gao et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 218
    }, {
      "referenceID" : 3,
      "context" : "(Bar, 2004; Xu and Chun, 2009) found that humans can recognize multiple visual objects and how their contexts, often embedded with other related objects, facilitate this capacity.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 63,
      "context" : "(Bar, 2004; Xu and Chun, 2009) found that humans can recognize multiple visual objects and how their contexts, often embedded with other related objects, facilitate this capacity.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "Our work is more related to the recent study (Fischer et al., 2020) which focuses on human capacity to create temporal stability across multiple objects.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 42,
      "context" : "(Pang and Wang, 2020) and almost concurrent to our work is (Kottur et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 25,
      "context" : "(Pang and Wang, 2020) and almost concurrent to our work is (Kottur et al., 2021).",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "However, (Kottur et al., 2021) is limited to a single object per dialogue, and (Pang and Wang, 2020) extends to multiple objects but does not require to maintain",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 42,
      "context" : ", 2021) is limited to a single object per dialogue, and (Pang and Wang, 2020) extends to multiple objects but does not require to maintain",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "Secondly, for each object, we define slots that represent the information state of objects in dialogues (as denoted by (Fischer et al., 2020) as “content” features of objects memorized by hu-",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 64,
      "context" : "dialogue research (Young et al., 2010; Mrkšić et al., 2017), and more lately multimodal dialogue research (Pang and Wang, 2020; Kottur et al.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 41,
      "context" : "dialogue research (Young et al., 2010; Mrkšić et al., 2017), and more lately multimodal dialogue research (Pang and Wang, 2020; Kottur et al.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 42,
      "context" : ", 2017), and more lately multimodal dialogue research (Pang and Wang, 2020; Kottur et al., 2021).",
      "startOffset" : 54,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : ", 2017), and more lately multimodal dialogue research (Pang and Wang, 2020; Kottur et al., 2021).",
      "startOffset" : 54,
      "endOffset" : 96
    }, {
      "referenceID" : 49,
      "context" : "Specifically, we used a pretrained Faster R-CNN model (Ren et al., 2015) to extract object representations.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 61,
      "context" : "Secondly, we used a pretrained ResNeXt model (Xie et al., 2017) to extract the segment-level representations of videos, denoted as zm ∈ R2048 for a segment m.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 36,
      "context" : "Similar to the recent work in traditional DST (Lei et al., 2018b; Le et al., 2020b; Zhang et al., 2020), we are motivated by the DST decoding strategy following a Markov principle and used the dialogue state of the last dialogue turn Bt−1 as an input to generate the current state Bt.",
      "startOffset" : 46,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Similar to the recent work in traditional DST (Lei et al., 2018b; Le et al., 2020b; Zhang et al., 2020), we are motivated by the DST decoding strategy following a Markov principle and used the dialogue state of the last dialogue turn Bt−1 as an input to generate the current state Bt.",
      "startOffset" : 46,
      "endOffset" : 103
    }, {
      "referenceID" : 65,
      "context" : "Similar to the recent work in traditional DST (Lei et al., 2018b; Le et al., 2020b; Zhang et al., 2020), we are motivated by the DST decoding strategy following a Markov principle and used the dialogue state of the last dialogue turn Bt−1 as an input to generate the current state Bt.",
      "startOffset" : 46,
      "endOffset" : 103
    }, {
      "referenceID" : 57,
      "context" : "We pased ZV D to a vanilla Transformer network (Vaswani et al., 2017) through multiple multi-head attention layers with normalization (Ba et al.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : ", 2017) through multiple multi-head attention layers with normalization (Ba et al., 2016) and residual connections (He et al.",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 23,
      "context" : "of data samples contain strong distribution bias in dialogue context, in which dialogue agents can simply ignore the whole dialogue and rely on imageonly features (Kim et al., 2020), or annotation bias, in which the causal link connecting dialogue history and current turn question is actually harmful (Qi et al.",
      "startOffset" : 163,
      "endOffset" : 181
    }, {
      "referenceID" : 44,
      "context" : ", 2020), or annotation bias, in which the causal link connecting dialogue history and current turn question is actually harmful (Qi et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 145
    }, {
      "referenceID" : 32,
      "context" : "We noticed a recent research work (Le et al., 2021b) that can address both biases.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 32,
      "context" : "(Le et al., 2021b) proposed to synthetically create dialogues that are grounded on videos from CATER (Shamsian et al.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 53,
      "context" : ", 2021b) proposed to synthetically create dialogues that are grounded on videos from CATER (Shamsian et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 32,
      "context" : "We generated new dialogues following (Le et al., 2021b) procedures but based on an extended CATER video split (Shamsian et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 53,
      "context" : ", 2021b) procedures but based on an extended CATER video split (Shamsian et al., 2020) rather than the original CATER video data (Girdhar",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 53,
      "context" : "The extended CATER split (Shamsian et al., 2020) includes additional annota-",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 32,
      "context" : "As shown in (Le et al., 2021b), objects can be uniquely referred in utterances based on their appearance by one or more following aspects: “size”, “color”, “material”, and “shape”.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 60,
      "context" : "of (object, slot, value) in training split and uses it as predicted states; (3) Object (random), for each test sample, randomly selects one object predicted by the visual perception model and a random (slot, value) tuple (with slots and values inferred from object classes) as the predicted state; (4) Object (all) is similar to (3) but selects all possible objects and all possible (slot, value) tuples as the predicted state; (5) RNN(+Att) uses RNN as encoder and an MLP as decoder with a vanilla dot-product attention; We experimented with strong unimodal DST baselines, including: (6) TRADE (Wu et al., 2019); (7) UniConv (Le et al.",
      "startOffset" : 595,
      "endOffset" : 612
    }, {
      "referenceID" : 31,
      "context" : ", 2019); (7) UniConv (Le et al., 2020b); and (8) NADST (Le et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "task (Budzianowski et al., 2018; Henderson et al., 2014a) and used the state accuracy metric.",
      "startOffset" : 5,
      "endOffset" : 57
    }, {
      "referenceID" : 45,
      "context" : "3) was initialized from a GPT2-base model (Radford et al., 2019) with a pretrained checkpoint released by HuggingFace1.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "From these observations, we are excited to see more future adaptation of large pretrained LMs, such as (Brown et al., 2020; Raffel et al., 2020), or of pretrained multimodal transformer models, such as (Lu et al.",
      "startOffset" : 103,
      "endOffset" : 144
    }, {
      "referenceID" : 46,
      "context" : "From these observations, we are excited to see more future adaptation of large pretrained LMs, such as (Brown et al., 2020; Raffel et al., 2020), or of pretrained multimodal transformer models, such as (Lu et al.",
      "startOffset" : 103,
      "endOffset" : 144
    }, {
      "referenceID" : 39,
      "context" : ", 2020), or of pretrained multimodal transformer models, such as (Lu et al., 2019; Zhou et al., 2020), in the MM-DST task.",
      "startOffset" : 65,
      "endOffset" : 101
    }, {
      "referenceID" : 67,
      "context" : ", 2020), or of pretrained multimodal transformer models, such as (Lu et al., 2019; Zhou et al., 2020), in the MM-DST task.",
      "startOffset" : 65,
      "endOffset" : 101
    }, {
      "referenceID" : 31,
      "context" : "In all metrics, we observed VDTN outperforms both RNN baseline and UniConv (Le et al., 2020b), across all turn positions.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "Note that there are more sophisticated approaches such as neural module networks (Hu et al., 2018) and",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 41,
      "context" : "Compared to conventional DST (Mrkšić et al., 2017; Lei et al., 2018b; Gao et al., 2019; Le et al., 2020c), we show that the scope of DST can be fur-",
      "startOffset" : 29,
      "endOffset" : 105
    }, {
      "referenceID" : 36,
      "context" : "Compared to conventional DST (Mrkšić et al., 2017; Lei et al., 2018b; Gao et al., 2019; Le et al., 2020c), we show that the scope of DST can be fur-",
      "startOffset" : 29,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Compared to conventional DST (Mrkšić et al., 2017; Lei et al., 2018b; Gao et al., 2019; Le et al., 2020c), we show that the scope of DST can be fur-",
      "startOffset" : 29,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "Compared to conventional DST (Mrkšić et al., 2017; Lei et al., 2018b; Gao et al., 2019; Le et al., 2020c), we show that the scope of DST can be fur-",
      "startOffset" : 29,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "Compared to prior work in multimodal dialogues (Das et al., 2017a; Hori et al., 2019; Thomason et al., 2019) which focuses more on vision-language interactions, our work was inspired from a dialogue-based",
      "startOffset" : 47,
      "endOffset" : 108
    }, {
      "referenceID" : 56,
      "context" : "Compared to prior work in multimodal dialogues (Das et al., 2017a; Hori et al., 2019; Thomason et al., 2019) which focuses more on vision-language interactions, our work was inspired from a dialogue-based",
      "startOffset" : 47,
      "endOffset" : 108
    } ],
    "year" : 0,
    "abstractText" : "Designed for tracking user goals in dialogues, a dialogue state tracker is an essential component in a dialogue system. However, the research of dialogue state tracking has largely been limited to unimodality, in which slots and slot values are limited by knowledge domains (e.g. restaurant domain with slots of restaurant name and price range) and are defined by specific database schema. In this paper, we propose to extend the definition of dialogue state tracking to multimodality. Specifically, we introduce a novel dialogue state tracking task to track the information of visual objects that are mentioned in video-grounded dialogues. Each new dialogue utterance may introduce a new video segment, new visual objects, or new object attributes and a state tracker is required to update these information slots accordingly. We created a new synthetic benchmark and designed a novel baseline, Video-Dialogue Transformer Network (VDTN), for this task. VDTN combines both object-level features and segment-level features and learns contextual dependencies between videos and dialogues to generate multimodal dialogue states. We optimized VDTN for a state generation task as well as a self-supervised video understanding task which recovers video segment or object representations. Finally, we trained VDTN to use the decoded states in a response prediction task. Together with comprehensive ablation and qualitative analysis, we discovered interesting insights towards building more capable multimodal dialogue systems.",
    "creator" : null
  }
}
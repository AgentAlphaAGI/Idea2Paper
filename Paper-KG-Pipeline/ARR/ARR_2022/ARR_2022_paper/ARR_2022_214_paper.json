{
  "name" : "ARR_2022_214_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained Language Models (PLMs), such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2020) and XLNet (Yang et al., 2019) have shown remarkable abilities to match and even surpass human performances on many Natural Languages Understanding (NLU) tasks (Rajpurkar et al., 2018; Wang et al., 2018, 2019). However, the deployment of these models in real world applications (e.g. edge devices) come with challenges, mainly due to large model size and inference time.\nIn this regard, several model compression techniques such as quantization (Shen et al., 2019;\nZafrir et al., 2019), pruning (Guo et al., 2019; Gordon et al., 2020; Michel et al., 2019), optimizing the Transformer architecture (Fan et al., 2019; Wu et al., 2020b; Lu et al., 2020), and knowledge distillation (Sanh et al., 2019a; Jiao et al., 2019; Sun et al., 2020b; Wang et al., 2020a; Rashid et al., 2021; Passban et al., 2021; Jafari et al., 2021; Kamalloo et al., 2021) have been developed to reduce the model size and latency, while maintaining comparable performance to the original model.\nKD, which is the main focus of this work, is a neural model compression approach that involves training a small student model with the guidance of a large pre-trained teacher model. In the original KD technique (Buciluǎ et al., 2006; Hinton et al., 2014; Turc et al., 2019), the teacher output predictions are used as soft labels for supervising the training of the student. There has been several attempts in the literature to reduce the teacherstudent performance gap by leveraging data augmentation (Fu et al., 2020; Li et al., 2021; Jiao et al., 2019), adversarial training (Zaharia et al., 2021; Rashid et al., 2020, 2021), and intermediate layer distillation (ILD) (Wang et al., 2020b,a; Ji et al., 2021; Passban et al., 2021).\nWhen it comes to BERT compression, ILD leads to clear gains in performances (Sanh et al., 2019a; Jiao et al., 2019; Wang et al., 2020a) due to its ability to enhance the knowledge transfer beyond logits matching. This is done by mapping intermediate layer representations of both models to a common space1, and then matching them via regression (Sun et al., 2019) or cosine similarity (Sanh et al., 2019a) losses. One major problem with ILD is the absence of an appropriate strategy to select layers to be matched on both sides, reacting to the skip and search problem (Passban et al., 2021). Some solutions in the literature mostly rely on layer combination (Wu et al., 2020a), attention-based layer\n1In some cases, the representations are directly matched if the teacher and student have the same hidden size.\nprojection (Passban et al., 2021) and contrastive learning (Sun et al., 2020a). While these solutions are all effective to some extent, to the best of our knowledge, there is no work in the literature doing a comprehensive evaluation of these techniques in terms of both efficiency and performance.\nA case in point is that the aforementioned solutions to the layer skip and search problem do not scale to very deep networks. We propose RAIL-KD (RAndom Intermediate Layer KD), a simple yet effective method for intermediate layer mapping which randomly selects k out of n intermediate layers of the teacher at each epoch to be distilled to the corresponding student layers. Since the layer selection is done randomly, all the intermediate layers of the teacher will have a chance to be selected for distillation. Our method adds no computational cost to the training, still outperforming all aforementioned methods on the GLUE benchmark (Wang et al., 2018). Moreover, we observe larger gains when distilling from large teacher models, as well as when student models are evaluated on out-ofdomain datasets. Last, we report the results on 5 random seeds in order to verify the contribution of the random selection process, thus making the comparison fair with previous methods. The main contributions of our paper are as follows:\n• We introduce RAIL-KD, a more efficient and scalable intermediate layer distillation approach.\n• To the best of our knowledge, we are the first to perform a comprehensive study of the ILD techniques in terms of both efficiency and performance.\n• We consider the distillation of models such as BERT and RoBERTa, and compare different up-to-date distillation techniques on out-ofdomain test sets."
    }, {
      "heading" : "2 Related Work",
      "text" : "Recent years, have seen a wide range of methods have witnessed to expand knowledge transfer of transformer-based (Vaswani et al., 2017) NLU models beyond logits matching. DistillBERT (Sanh et al., 2019a) added a cosine similarity loss between teacher and student embeddings layers. TinyBERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and selfattention distributions of the teacher and the student.\nIn PKD, Sun et al. (2019) used deterministic mapping strategies to distill a 12-layer BERT teacher to a 6-layer BERT student. PKD-LAST and PKD-SKIP refer to matching layers {1− 5} of the student with layers {7 − 11} and {2, 4, 6, 8, 10} of the teacher respectively. However, these works ignored the impact of layer selection, as they used a fixed layer-wise mapping.2\nResearchers have found that tuning the layer mapping scheme can significantly improve the performance of ILD techniques (Sun et al., 2019). Nevertheless, finding the optimal mapping can be challenging, which is referred to as the layer skip and search problems by Passban et al. (2021). To address the layer skip problem, CKD (Wu et al., 2020a) is built on top of PKD by partitioning all the intermediate layers of the teacher to the number of student layers. Then, the combined representation of the layers of each partition is distilled into a number of subset corresponding to the number of student layers. However, finding the optimal partitioning scheme requires running exhaustive experiments. Given teacher and student BERT models with n and m layers respectively (where n >> m),\n2e.g. matching the first (or last) k layers of the student with their corresponding teacher layers.\nit is not trivial to choose the teacher layers that can be incorporated in the distillation process and how we should map them to the student layers (search).\nALP-KD (Passban et al., 2021) overcomes this issue by computing attention weights between each student layer and all the intermediate layers of the teacher. The learned attention weights for each student layer are used to obtain a weighted representation of all teacher layers. Although ALP-KD has shown promising results on 12-layer BERT-based compression, attending to all layers of the teacher adds considerable computational overhead to the training phase. This can become computationally prohibitive when scaling to very large models such as RoBERTa-large (Liu et al., 2020) or GPT-2 (Radford et al., 2019). Alternatively, CODIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping. Similar to ALP-KD, this approach also requires excessive training time due to the contrastive loss calculation and the use of negative samples from a memory bank.\nTable 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD (Sun et al., 2019), CKD (Wu et al., 2020a), and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD. First, PKD and CKD treat the\nmapping as an extra hyperparameter that requires extensive experiments in order to find the optimal mapping. Second, ALP-KD (Passban et al., 2021) and CoDIR (Sun et al., 2020a) use the attention mechanism and contrastive learning respectively to address the issue, but at the expense of extra computational cost.\nOur proposed RAIL-KD method does not add any computational cost to the distillation process, while empirically outperforming previous methods. For instance, RAIL-KD is roughly twice faster than CoDIR in a 24 to 6 layer compression. In addition, it does not require extensive experiments to find the optimal mapping scheme. In this work, we position ourselves to works that tackle the skip and search problem 3. Otherwise said, we don’t compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching. However, we expect that these methods, as well as state-of-the-art (Rashid et al., 2021; He et al., 2021) ones can take full advantage of RAIL-KD, since they use a deterministic layer mapping scheme."
    }, {
      "heading" : "3 RAIL-KD",
      "text" : "The RAIL-KD method is sketched in Figure 1. RAIL-KD transfers intermediate knowledge of a\n3This only concerns works that perform intermediate layer distillation\npre-trained teacher T with n intermediate layers to a student model Sθ with m intermediate layers. In contrast to traditional intermediate layer distillation techniques which keep the selected layers of the teacher for distillation fixed during training, in RAIL-KD, at each epoch, a few intermediate layers from the teacher model are selected randomly for distillation. Here for simplicity, we set the number of selected intermediate layers of the teacher model equal to that of the student model.\nLet (X, y) denote a training sample X = (x0, · · · , xL−1) which is a sequence of L tokens and y its corresponding label. In Figure 1, our Random Selection operator is applied to the intermediate layers of the teacher to randomly select m out of n layers. The intermediate layer representations of the m selected layers of the teacher and the student model corresponding to the X input can be described as HTX = {HT1,X , · · · , HTm,X} and HSθX = {H Sθ 1,X , · · · , H Sθ m,X} respectively, where HTi,X = ∪ L−1 k=0{h T i,xk } ∈ RL×d1 and HSθi,X = ∪L−1k=0{h Sθ i,xk } ∈ RL×d2 . Here, d1 and d2 indicate the hidden dimension of the layers of the teacher and the student models respectively. To obtain HTi,X and H S j,X , we need to find the individual representation of each token xk at each layer i, which is indicated as hTi,xk and h S i,xk\nfor the teacher and student networks respectively.\nAt this stage, we need to obtain an aggregated representation of the sequence X at each layer. In this regard, one can either use the <CLS> token representation or use the mean-pooling of the sequence representations of the layer. Since in (Sun et al., 2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer. Meanpooling is a row-wise average over HTi,X , H Sθ i,X to get h̄Ti,X ∈ Rd1 h̄ Sθ i,X ∈ Rd2 (Sun et al., 2020a):\nh̄Ti,X = 1\nL L−1∑ k=0 hTi,xk ; h̄ Sθ i,X = 1 L L−1∑ k=0 hSθi,xk (1)\nAfter obtaining aggregated layer representations for both the student and teacher networks, our RAILKD proposal is to randomly select m layer representations from the teacher through training to perform the intermediate layer distillation (ILD). RAIL-KD does ILD in two different forms: using layer-wise distillation (see Fig. 1(a)) or by concatenating layer representations (see Fig. 1(b)) which are described in the following two sub-sections."
    }, {
      "heading" : "3.1 Layer-wise RAIL-KD",
      "text" : "In this setting, the representations h̄Ti,X ∈ Rd1 and h̄Sθi,X ∈ Rd2 are projected into a same-size lowerdimensional space ĥTi,X , ĥ Sθ i,X ∈ Ru using (d1 × u) and (d2×u) linear mappings respectively. Assume that the set A = {aκ|aκ ∼ {1, 2, ..., n}, 1 ≤ κ ≤ m} contains indices of selected m layers from the teacher, then to calculate the layer-wise loss we have:\nLRAIL-KDl =∑ X∈X ∑ i∈A αi ( || ĥTi,X ||ĥTi,X ||2 − ĥSθi,X ||ĥSθi,X ||2 ||22 ) (2)\nwhere X denotes the training set, and αi is a hyperparameter to assign a custom weights to the layerwise distillation loss. It is worth mentioning that in our experiments we set αi = 1."
    }, {
      "heading" : "3.2 Concatenated RAIL-KD",
      "text" : "In this setting, intermediate layer representations are concatenated and then distilled: h̄TX = [h̄Ti,X ]i∈A, h̄ Sθ X = [h̄ Sθ j,X ] m j=1 which are further mapped into the same lower-dimensional space ĥTX , ĥ Sθ X ∈ Ru using (md1 × u) and (md2 × u) linear mappings to calculate the concatenated distillation loss.\nLRAIL-KDc = ∑ X∈X || ĥTX ||ĥTX ||2 − ĥSθX ||ĥSθX ||2 ||22 (3)\nAny type of loss such as contrastive (Sun et al., 2020a), or mean-square-error (MSE) (Passban et al., 2021; Sun et al., 2019) can be applied for our RAIL-KD approach."
    }, {
      "heading" : "3.3 Training Loss",
      "text" : "The intermediate representation distillation loss LRAIL-KD is combined with the original KD loss LKD, which is used to distill the knowledge from the output logits of the teacher model T to the output logits of the student model Sθ, and the original cross-entropy loss LCE. The total loss function for training the student model is:\nL = λ1LCE + λ2LKD + λ3LRAIL-KDl/c (4)\nwhere λ1, λ2, and λ3 are hyper-parameters of our model to minimize the total loss, and λ1 + λ2 + λ3 = 1."
    }, {
      "heading" : "4 Experimental Protocol",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Evaluation",
      "text" : "We evaluate RAIL-KD on 8 tasks from the GLUE benchmark (Wang et al., 2018): 2 single-sentence (CoLA and SST-2) and 5 sentence-pair (MRPC, RTE, QQP, QNLI, and MNLI) classification tasks, and 1 regression task (STS-B). Following prior works (Sun et al., 2019; Passban et al., 2021; Jiao et al., 2019; Sun et al., 2020a), we use the same metrics as the GLUE benchmark for evaluation. Moreover, to further show the generalization capability of our RAIL-KD method on out-of-domain (OOD) across tasks, we use Scitail (Khot et al., 2018), PAWS (Paraphrase Adversaries from Word Scrambling) (Zhang et al., 2019), and IMDb (Internet Movie Database) (Maas et al., 2011) test sets to evaluate the models fine-tuned on MNLI, QQP, and SST-2 tasks respectively."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We run extensive experiments on 3 different teachers in order to ensure a fair comparison with a wide range of prior works, and also to show the effectiveness of RAIL-KD. We experiment with the 12-layer BERT-base-uncased (Devlin et al., 2019) model as teacher (BERT12) and the 6-layer DistilBERT (Sanh et al., 2019a) as student (DistillBERT6) to compare\nwith PKD (Sun et al., 2019) and ALP-KD (Passban et al., 2021). Also, we consider 24-layer RoBERTa-large (Liu et al., 2020) and 6-layer DistilRoberta (Sanh et al., 2019b) as the backbone for teacher (RoBERTa24) and student (DistilRoberta6) respectively to compare results when n >> m. Furthermore, we perform evaluation using the 12 layers RoBERTa-base (RoBERTa12) model as a teacher to be able to directly compare our figures with the ones of CoDIR. Hyperparameters selection and other implementation details can be found in Appendix."
    }, {
      "heading" : "5 Results",
      "text" : "Table 2 shows the performances of models trained on GLUE tasks, and evaluated on their respective DEV and TEST sets for 12-layer to 6-layer distillation. BERT12 and DistilBERT6 are used as backbone for the teacher and student models respectively. The baselines are fine-tuned without KD (w/o KD) and with Vanilla KD. Moreover, we directly compare RAIL-KDlc results with PKD and ALP-KD as more competing techniques.\nFirst, we observe that the performance gap between ILD methods and vanilla-KD is tight (0.8% and 0.3% on DEV and TEST sets respectively). Moreover, as we expect, ALP-KD performs better on DEV and similar on TEST compared to PKD\nwith 0.2% improvement on the DEV results. Second, results show that RAIL-KD outperforms the best ILD methods by a margin of 0.5% and 0.3% on average on DEV and TEST sets respectively. We notice that, except on RTE TEST, our RAIL-KD variants obtained the highest per-task performances. Third, we observe that RAIL-KD variants perform very similarly, which indicates that our method is effective on concatenated as well as layer-wise distillation.\nSimilar trends are seen on the 24- to 6-layer model compression experiments, which are reported in Table 3. In this experiment, we used Roberta24 and DistillRoberta6 as teacher and students models respectively. Overall, RAIL-KD outperforms the best baseline by 1.2% and 0.3% on DEV and TEST sets respectively. Interestingly, the gap on DEV compared with PKD and ALP-KD is larger than the one reported on BERT12 experiments, and PKD TEST socres are much lower from that of ALP and RAIL-KD. This might be because PKD skips a large number of intermediate layers on RoBERTa24, and the computational cost of ALP-KD attention weights over a large number of teacher layers might produce smaller weights on Roberta24 compared to BERT12.\nFurthermore, we demonstrate the effectiveness of RAIL-KD by directly comparing it with\nCoDIR (Sun et al., 2020a), the current state-ofthe-art ILD method. It uses the contrastive objective and a memory bank to extract a large number of negative samples for contrastive loss calculations. Table 4 shows GLUE test results of both approaches when distilling RoBERTa12 to DistillRoberta6. CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost twice faster as shown in the next section."
    }, {
      "heading" : "5.1 Training Speed up",
      "text" : "Table 5 shows the training time speed up against the teacher of different techniques on 8 GLUE tasks. We measured the speed up by calculating the student_train_time/teacher_train_time using RoBERTa24 and DistilRoBERTa6 as backbone for teacher and student respectively. We used this configuration because CODIR pretrained student models are not available and we can only run CODIR code out-of-the-box.\nOur results indicate that random layer mapping not only delivers consistently better results than the deterministic mapping technique such as PKD, but it has less computational overhead during training\n(twice faster than CODIR), while avoiding extensive search experiments to find an optimal mapping. Furthermore, using attention for layer selection (ALP-KD) or contrastive learning (CoDIR) leads to slightly worse performance result than random selection."
    }, {
      "heading" : "5.2 Impact of Random Layer Selection",
      "text" : "To evaluate the impact of random layer selection on the performance of RAIL-KD, we report the standard deviation of the DistilBERT6 student models (Table 2 models) on the 8 GLUE tasks. As Table 6 shows, the variances of RAIL-KD is in the same range for each task, for instance, RAIL-KD variance is at the same scale compared with PKD and ALP-KD on CoLA and MRPC, and even lower on RTE. This indicates that the gains of RAIL-KD are significant, and are not due to chance in our random selection of layers to distill."
    }, {
      "heading" : "5.3 Out-of-Distribution Test",
      "text" : "We further validate the generalization ability of student models by measuring their robustness to in-domain and out-of-domain evaluation. We do so by evaluating models finetuned on MLI, QQP and SST-2 and then evaluated on SciTail, PAWS, and IMDB respectively. These datasets contains counterexamples to biases found in the training data (McCoy et al., 2019; Schuster et al., 2019; Clark et al., 2019). Performances of BERT12/Roberta24 teacher and DistilBERT6/DistilRoBERTa6 student variants are\nreported in Table 7. Also, we compute the unweighted average score of the three tasks.\nFirst, we notice high variability in models rank and some inconsistencies in performances across tasks when compared with in-domain results. This was also reported in prior works on out-of-domain training and evaluation (Clark et al., 2019; Mahabadi et al., 2020; Utama et al., 2020; Sanh et al., 2020). Still, RAIL-KD clearly outperforms all baselines across tasks. Surprisingly, we observe that PKD and ALP-KD perform poorly (on all three tasks) compared to the Vanilla KD baseline.\nInterestingly, we observe that RAIL-KDl performs consistently better (1.1% on average) than RAIL-KDc on Roberta24 compression, while RAIL-KDc performs better (1.1% on average) on BERT12. These results suggest that layer-wise distillation approach is more effective than concatenated distillation when we have a large capacity gap (layer number) between the teacher and the student, and vice versa."
    }, {
      "heading" : "6 Analysis",
      "text" : "We run extensive analysis to better understand why RAIL-KD performs better than the other baselines. We visualize the layer-wise cosine similarity between the intermediate representations of the teacher and the student networks. Figure 2 shows the cosine similarity score between three intermediate layer representations of BERT12 teacher (i.e. layers 2, 4 and 6) and the first three layer representations of the student for PKD, ALP-KD, RAILKDl/c students on 100 samples randomly selected from the SST-2 dataset. Due to space constraints, we only plot the scores for the first three layers of\nthe student model. The similar trend are seen from the other layers.\nWe found that RAIL-KD allows the student to mimic teacher layers similar to PKD and much better than ALP-KD, despite that the mapping scheme varies at each epoch. Moreover, we observe that ALP-KD results in less similarity scores in the upper intermediate layers. PKD gives lower similarity scores in the lower layers while improving in the upper layers. In contrast, our approach gives more stable similarity scores for all layers while getting closer to the teacher representation in the upper layers.\nWe further investigate the attention weights learned by ALP-KD, and find out that they mostly focus on few layers (sparse attention). Figure 3 illustrates the distribution of weights, averaged on all training samples of DistilBERT6 ALP-KD studnet on CoLA (left), RTE (middle), and MRPC (right) 4. The figure clearly shows (light colors) that most of ALP weights are concentrated on top layers of the teacher. For instance, layers 1,2,5 of the three students mostly attend to the last layer of BERT12. This is an indicator that ALP-KD overfits to the information driven from last layers. In contrast, the randomness in layer selection of RAIL-KD ensures a uniform focus on teacher layers. This may explain the poor performance of ALP-KD on out-\n4Similar trends found on other datasets.\nof-domain evaluation compared with RAIL-KD. From Figure 3, we see clearly that ALP-KD mostly prefers the upper layers of the teacher. On the other hand, the deterministic nature of PDK allows it to match better particular layers of the teacher (e.g. bottom ones as shown in Figure 2), but PKD never sees the layers that are skipped by the mapping. Consequently, it is expected that even though PDK can mimic bottom layers well, it is worse overall because it completely ignores some layers of the teacher. Random layer selection allow RAIL-KD to mimic all teacher layers while delivering high performances."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "We introduced a novel, simple, and efficient intermediate layer KD approach that outperforms the conventional approaches with performance improvement and efficient training time. RAIL-KD selects random intermediate layers from the teacher which equals to the number of intermediate layers of the student model. The selected intermediate layers are then sorted to distill their representations into the student model. RAIL-KD yields better regularization, which helps performance. Furthermore, our approach shows better performance for larger model distillation with faster training time, which opens up an avenue to investigate our approach for super-large models."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Implementation details We re-implement PKD (Sun et al., 2019) and ALPKD (Passban et al., 2021) approaches using the default settings proposed in the respective papers. We used early stopping based on performance on the development set, while making sure that the figures are in line with the ones reported in the papers. More precisely, the best layer setting for PKD teacher BERT12 is {2, 4, 6, 8, 10} to distill into DistilBERT6. For DistilRoBERTa6, we choose the intermediate layers 4, 8, 12, 16, 20 from the teacher RoBERTa24 model for distillation that we found to work the best on the development set.\nUsing ALP-KD, we compute attention weights for the intermediate layers of the teacher (i.e., 1 to 11 for BERT12 and 1 to 23 for RoBERTa24 models) to calculate the weighted intermediate representations of the teacher for each intermediate layer of the student model (i.e., 1 to 5 layers of the student models). Since, the hidden dimensions of the RoBERTa24 and DistilRoBERTa6 are different, we linearly transform them into same lowerdimensional space. We train the PKD and ALP-KD models following (Sun et al., 2019; Passban et al., 2021).\nFor RAIL-KDl, at each epoch we randomly select 5 layers from the intermediate layers of the teacher (i.e., from layers 1 to 11 for BERT12 model and 1 to 23 for RoBERTa24 model). Then, we sort the layer indexes and perform layer-wise distillation (Figure 1(a)) for RAIL-KDl. For RAIL-KDc, we concatenated the representations of the sorted randomly selected intermediate layers and then perform concatenated representation distillation (Figure 1(b)).\nWe use a linear transformation to map the intermediate representations (layer-wise or concatenated representations) into 128-dimensional space (u = 128) and normalize them before computing the loss LRAIL-KDl/c for both BERT12 and RoBERTa24 distillations. We fixed αi = 1, λ1, λ2, λ3 = 1/3 for our proposed approaches 5. We search learning rate from {1e-5, 2e-5, 5e-5, 4e6}, batch size from {8, 16, 32}, and fixed the epoch number to 40 for all the experiments. we run all experiments 5 times and report average score, in order to validate the credibility of our results. We ran all the experiments on a single NVIDIA V100 GPU\n5We didn’t find a significant improvement when changing these values.\nusing mixed-precision training (Micikevicius et al., 2018) and PyTorch (Paszke et al., 2019) framework."
    } ],
    "references" : [ {
      "title" : "Model compression",
      "author" : [ "Cristian Buciluǎ", "Rich Caruana", "Alexandru Niculescu-Mizil." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541.",
      "citeRegEx" : "Buciluǎ et al\\.,? 2006",
      "shortCiteRegEx" : "Buciluǎ et al\\.",
      "year" : 2006
    }, {
      "title" : "Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases",
      "author" : [ "Christopher Clark", "Mark Yatskar", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "https://arxiv.org/abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Reducing transformer depth on demand with structured dropout",
      "author" : [ "Angela Fan", "Edouard Grave", "Armand Joulin." ],
      "venue" : "arXiv preprint arXiv:1909.11556.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Role-wise data augmentation for knowledge distillation",
      "author" : [ "Jie Fu", "Xue Geng", "Zhijian Duan", "Bohan Zhuang", "Xingdi Yuan", "Adam Trischler", "Jie Lin", "Chris Pal", "Hao Dong." ],
      "venue" : "arXiv preprint arXiv:2004.08861.",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Compressing bert: Studying the effects of weight pruning on transfer learning",
      "author" : [ "Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews" ],
      "venue" : "arXiv preprint arXiv:2002.08307",
      "citeRegEx" : "Gordon et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2020
    }, {
      "title" : "Reweighted proximal pruning for large-scale language representation",
      "author" : [ "Fu-Ming Guo", "Sijia Liu", "Finlay S Mungall", "Xue Lin", "Yanzhi Wang." ],
      "venue" : "arXiv preprint arXiv:1909.12486.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Generate, annotate, and learn: Generative models advance selftraining and knowledge distillation",
      "author" : [ "Xuanli He", "Islam Nassar", "Jamie Kiros", "Gholamreza Haffari", "Mohammad Norouzi." ],
      "venue" : "arXiv preprint arXiv:2106.06168.",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff. Dean." ],
      "venue" : "NIPS Workshop, https://arxiv.org/abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2014",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2014
    }, {
      "title" : "Annealing knowledge distillation",
      "author" : [ "Aref Jafari", "Mehdi Rezagholizadeh", "Pranav Sharma", "Ali Ghodsi." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2493–2504,",
      "citeRegEx" : "Jafari et al\\.,? 2021",
      "shortCiteRegEx" : "Jafari et al\\.",
      "year" : 2021
    }, {
      "title" : "Show, attend and distill: Knowledge distillation via attention-based feature matching",
      "author" : [ "Mingi Ji", "Byeongho Heo", "Sungrae Park." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ji et al\\.,? 2021",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2021
    }, {
      "title" : "Not far away, not so close: Sample efficient nearest neighbour data augmentation via minimax",
      "author" : [ "Ehsan Kamalloo", "Mehdi Rezagholizadeh", "Peyman Passban", "Ali Ghodsi." ],
      "venue" : "arXiv preprint arXiv:2105.13608.",
      "citeRegEx" : "Kamalloo et al\\.,? 2021",
      "shortCiteRegEx" : "Kamalloo et al\\.",
      "year" : 2021
    }, {
      "title" : "Scitail: A textual entailment dataset from science question answering",
      "author" : [ "Tushar Khot", "Ashish Sabharwal", "Peter Clark." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Khot et al\\.,? 2018",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2018
    }, {
      "title" : "How to select one among all? an extensive empirical study towards the robustness of knowledge distillation in natural language understanding",
      "author" : [ "Tianda Li", "Ahmad Rashid", "Aref Jafari", "Pranav Sharma", "Ali Ghodsi", "Mehdi Rezagholizadeh" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Roberta: A robustly optimized fbertg pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynabert: Dynamic bert with adaptive width and depth",
      "author" : [ "Hou Lu", "Huang Zhiqi", "Shang Lifeng", "Jiang Xin", "Chen Xiao", "Liu Qun." ],
      "venue" : "arXiv preprint arXiv:2004.04037.",
      "citeRegEx" : "Lu et al\\.,? 2020",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "ACL.",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "End-to-end bias mitigation by modelling biases in corpora",
      "author" : [ "Rabeeh Karimi Mahabadi", "Yonatan Belinkov", "James Henderson." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8706–8716. Asso-",
      "citeRegEx" : "Mahabadi et al\\.,? 2020",
      "shortCiteRegEx" : "Mahabadi et al\\.",
      "year" : 2020
    }, {
      "title" : "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Are sixteen heads really better than one? in neurips",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Michel et al\\.,? 2019",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Mixed precision training",
      "author" : [ "Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu." ],
      "venue" : "In International Conference on",
      "citeRegEx" : "Micikevicius et al\\.,? 2018",
      "shortCiteRegEx" : "Micikevicius et al\\.",
      "year" : 2018
    }, {
      "title" : "ALP-KD: attention-based layer projection for knowledge distillation",
      "author" : [ "Peyman Passban", "Yimeng Wu", "Mehdi Rezagholizadeh", "Qun Liu." ],
      "venue" : "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Appli-",
      "citeRegEx" : "Passban et al\\.,? 2021",
      "shortCiteRegEx" : "Passban et al\\.",
      "year" : 2021
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards zero-shot knowledge distillation for natural language processing",
      "author" : [ "Ahmad Rashid", "Vasileios Lioutas", "Abbas Ghaddar", "Mehdi Rezagholizadeh." ],
      "venue" : "arXiv preprint arXiv:2012.15495.",
      "citeRegEx" : "Rashid et al\\.,? 2020",
      "shortCiteRegEx" : "Rashid et al\\.",
      "year" : 2020
    }, {
      "title" : "Mate-kd: Masked adversarial text, a companion to knowledge distillation",
      "author" : [ "Ahmad Rashid", "Vasileios Lioutas", "Mehdi Rezagholizadeh." ],
      "venue" : "arXiv preprint arXiv:2105.05912.",
      "citeRegEx" : "Rashid et al\\.,? 2021",
      "shortCiteRegEx" : "Rashid et al\\.",
      "year" : 2021
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019a",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilroberta, a distilled version of roberta: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "https://huggingface.co/ distilroberta-base.",
      "citeRegEx" : "Sanh et al\\.,? 2019b",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning from others’ mistakes: Avoiding dataset biases without modeling them",
      "author" : [ "Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush." ],
      "venue" : "arXiv preprint arXiv:2012.01300.",
      "citeRegEx" : "Sanh et al\\.,? 2020",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards debiasing fact verification models",
      "author" : [ "Tal Schuster", "Darsh Shah", "Yun Jie Serene Yeo", "Daniel Roberto Filizzola Ortiz", "Enrico Santus", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Schuster et al\\.,? 2019",
      "shortCiteRegEx" : "Schuster et al\\.",
      "year" : 2019
    }, {
      "title" : "Q-bert: Hessian based ultra low precision quantization of bert",
      "author" : [ "Sheng Shen", "Zhen Dong", "Jiayu Ye", "Linjian Ma", "Zhewei Yao", "Amir Gholami", "Michael W Mahoney", "Kurt Keutzer." ],
      "venue" : "arXiv preprint arXiv:1909.05840.",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "https://arxiv.org/abs/1908.09355.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Contrastive distillation on intermediate representations for language model compression",
      "author" : [ "Siqi Sun", "Zhe Gan", "Yu Cheng", "Yuwei Fang", "Shuohang Wang", "Jingjing Liu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Sun et al\\.,? 2020a",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Mobilebert: a compact task-agnostic bert for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "arXiv preprint arXiv:2004.02984.",
      "citeRegEx" : "Sun et al\\.,? 2020b",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Contrastive representation distillation",
      "author" : [ "Yonglong Tian", "Dilip Krishnan", "Phillip Isola." ],
      "venue" : "arXiv preprint arXiv:1910.10699.",
      "citeRegEx" : "Tian et al\\.,? 2019",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2019
    }, {
      "title" : "Well-read students learn better: On the importance of pre-training compact models",
      "author" : [ "Iulia Turc", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1908.08962.",
      "citeRegEx" : "Turc et al\\.,? 2019",
      "shortCiteRegEx" : "Turc et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards debiasing nlu models from unknown biases",
      "author" : [ "Prasetya Ajie Utama", "Nafise Sadat Moosavi", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7597–7610.",
      "citeRegEx" : "Utama et al\\.,? 2020",
      "shortCiteRegEx" : "Utama et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Minilmv2: Multihead self-attention relation distillation for compressing pretrained transformers",
      "author" : [ "Wenhui Wang", "Hangbo Bao", "Shaohan Huang", "Li Dong", "Furu Wei." ],
      "venue" : "arXiv preprint arXiv:2012.15828.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
      "author" : [ "Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:2002.10957.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Why skip if you can combine: A simple knowledge distillation technique for intermediate layers",
      "author" : [ "Yimeng Wu", "Peyman Passban", "Mehdi Rezagholizadeh", "Qun Liu." ],
      "venue" : "https://arxiv.org/abs/2010.03034.",
      "citeRegEx" : "Wu et al\\.,? 2020a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Lite transformer with long-short range attention",
      "author" : [ "Zhanghao Wu", "Zhijian Liu", "Ji Lin", "Yujun Lin", "Song Han." ],
      "venue" : "arXiv preprint arXiv:2004.11886.",
      "citeRegEx" : "Wu et al\\.,? 2020b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "NeuRIPS.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Q8bert: Quantized 8bit bert",
      "author" : [ "Ofir Zafrir", "Guy Boudoukh", "Peter Izsak", "Moshe Wasserblat." ],
      "venue" : "arXiv preprint arXiv:1910.06188.",
      "citeRegEx" : "Zafrir et al\\.,? 2019",
      "shortCiteRegEx" : "Zafrir et al\\.",
      "year" : 2019
    }, {
      "title" : "Dialect identification through adversarial learning and knowledge distillation on romanian BERT",
      "author" : [ "George-Eduard Zaharia", "Andrei-Marius Avram", "Dumitru-Clementin Cercel", "Traian Rebedea." ],
      "venue" : "Proceedings of the Eighth Workshop",
      "citeRegEx" : "Zaharia et al\\.,? 2021",
      "shortCiteRegEx" : "Zaharia et al\\.",
      "year" : 2021
    }, {
      "title" : "Paws: Paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He." ],
      "venue" : "arXiv preprint arXiv:1904.01130.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Pre-trained Language Models (PLMs), such as BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : ", 2019), RoBERTa (Liu et al., 2020) and XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 45,
      "context" : ", 2020) and XLNet (Yang et al., 2019) have shown remarkable abilities to match and even surpass human performances on many Natural Languages Understanding (NLU) tasks (Rajpurkar et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : "In this regard, several model compression techniques such as quantization (Shen et al., 2019; Zafrir et al., 2019), pruning (Guo et al.",
      "startOffset" : 74,
      "endOffset" : 114
    }, {
      "referenceID" : 46,
      "context" : "In this regard, several model compression techniques such as quantization (Shen et al., 2019; Zafrir et al., 2019), pruning (Guo et al.",
      "startOffset" : 74,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : ", 2019), pruning (Guo et al., 2019; Gordon et al., 2020; Michel et al., 2019), optimizing the Transformer architecture (Fan et al.",
      "startOffset" : 17,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : ", 2019), pruning (Guo et al., 2019; Gordon et al., 2020; Michel et al., 2019), optimizing the Transformer architecture (Fan et al.",
      "startOffset" : 17,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : ", 2019), pruning (Guo et al., 2019; Gordon et al., 2020; Michel et al., 2019), optimizing the Transformer architecture (Fan et al.",
      "startOffset" : 17,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : ", 2019), optimizing the Transformer architecture (Fan et al., 2019; Wu et al., 2020b; Lu et al., 2020), and knowledge distillation (Sanh et al.",
      "startOffset" : 49,
      "endOffset" : 102
    }, {
      "referenceID" : 44,
      "context" : ", 2019), optimizing the Transformer architecture (Fan et al., 2019; Wu et al., 2020b; Lu et al., 2020), and knowledge distillation (Sanh et al.",
      "startOffset" : 49,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : ", 2019), optimizing the Transformer architecture (Fan et al., 2019; Wu et al., 2020b; Lu et al., 2020), and knowledge distillation (Sanh et al.",
      "startOffset" : 49,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "mentation (Fu et al., 2020; Li et al., 2021; Jiao et al., 2019), adversarial training (Zaharia et al.",
      "startOffset" : 10,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "mentation (Fu et al., 2020; Li et al., 2021; Jiao et al., 2019), adversarial training (Zaharia et al.",
      "startOffset" : 10,
      "endOffset" : 63
    }, {
      "referenceID" : 47,
      "context" : ", 2019), adversarial training (Zaharia et al., 2021; Rashid et al., 2020, 2021), and intermediate layer distillation (ILD) (Wang et al.",
      "startOffset" : 30,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : ", 2020, 2021), and intermediate layer distillation (ILD) (Wang et al., 2020b,a; Ji et al., 2021; Passban et al., 2021).",
      "startOffset" : 57,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : ", 2020, 2021), and intermediate layer distillation (ILD) (Wang et al., 2020b,a; Ji et al., 2021; Passban et al., 2021).",
      "startOffset" : 57,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "When it comes to BERT compression, ILD leads to clear gains in performances (Sanh et al., 2019a; Jiao et al., 2019; Wang et al., 2020a) due to its ability to enhance the knowledge transfer beyond logits matching.",
      "startOffset" : 76,
      "endOffset" : 135
    }, {
      "referenceID" : 41,
      "context" : "When it comes to BERT compression, ILD leads to clear gains in performances (Sanh et al., 2019a; Jiao et al., 2019; Wang et al., 2020a) due to its ability to enhance the knowledge transfer beyond logits matching.",
      "startOffset" : 76,
      "endOffset" : 135
    }, {
      "referenceID" : 32,
      "context" : "This is done by mapping intermediate layer representations of both models to a common space1, and then matching them via regression (Sun et al., 2019) or cosine similarity (Sanh et al.",
      "startOffset" : 132,
      "endOffset" : 150
    }, {
      "referenceID" : 27,
      "context" : ", 2019) or cosine similarity (Sanh et al., 2019a) losses.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "One major problem with ILD is the absence of an appropriate strategy to select layers to be matched on both sides, reacting to the skip and search problem (Passban et al., 2021).",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 43,
      "context" : "Some solutions in the literature mostly rely on layer combination (Wu et al., 2020a), attention-based layer",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 32,
      "context" : "PKD (Sun et al., 2019) Extra Hyperparameter O(m) Extensive Search CKD (Wu et al.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 43,
      "context" : ", 2019) Extra Hyperparameter O(m) Extensive Search CKD (Wu et al., 2020a) Extra Hyperparameter O(m) Extensive Search ALP-KD (Passban et al.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : ", 2020a) Extra Hyperparameter O(m) Extensive Search ALP-KD (Passban et al., 2021) Attention O(m× n) Slow Training time CoDIR (Sun et al.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : ", 2021) Attention O(m× n) Slow Training time CoDIR (Sun et al., 2020a) Contrastive Learning O(K ×m) Slow Training time RAIL-KDl (our) Random Selection O(m) RAIL-KDc (our) Random Selection O(m) -",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "projection (Passban et al., 2021) and contrastive learning (Sun et al.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 40,
      "context" : "tioned methods on the GLUE benchmark (Wang et al., 2018).",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 38,
      "context" : "Recent years, have seen a wide range of methods have witnessed to expand knowledge transfer of transformer-based (Vaswani et al., 2017) NLU models beyond logits matching.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 34,
      "context" : ", 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 42,
      "context" : ", 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and self-",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 32,
      "context" : "Researchers have found that tuning the layer mapping scheme can significantly improve the performance of ILD techniques (Sun et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 138
    }, {
      "referenceID" : 43,
      "context" : "To address the layer skip problem, CKD (Wu et al., 2020a) is built on top of PKD by partitioning all the intermediate layers of the teacher to the number of student layers.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 21,
      "context" : "ALP-KD (Passban et al., 2021) overcomes this issue by computing attention weights between each student layer and all the intermediate layers of the teacher.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "This can become computationally prohibitive when scaling to very large models such as RoBERTa-large (Liu et al., 2020) or GPT-2 (Radford et al.",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : "Alternatively, CODIR (Sun et al., 2020a) exploited contrastive learning (Tian et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 35,
      "context" : ", 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : "Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD (Sun et al., 2019), CKD (Wu et al.",
      "startOffset" : 125,
      "endOffset" : 143
    }, {
      "referenceID" : 43,
      "context" : ", 2019), CKD (Wu et al., 2020a), and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "Second, ALP-KD (Passban et al., 2021) and CoDIR (Sun et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 33,
      "context" : ", 2021) and CoDIR (Sun et al., 2020a) use the attention",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 42,
      "context" : ", 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 26,
      "context" : "However, we expect that these methods, as well as state-of-the-art (Rashid et al., 2021; He et al., 2021) ones can take full advantage of RAIL-KD, since they use a deterministic layer mapping scheme.",
      "startOffset" : 67,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "However, we expect that these methods, as well as state-of-the-art (Rashid et al., 2021; He et al., 2021) ones can take full advantage of RAIL-KD, since they use a deterministic layer mapping scheme.",
      "startOffset" : 67,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "Since in (Sun et al., 2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 33,
      "context" : "Meanpooling is a row-wise average over HT i,X , H Sθ i,X to get h̄i,X ∈ Rd1 h̄ Sθ i,X ∈ Rd2 (Sun et al., 2020a):",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 33,
      "context" : "Any type of loss such as contrastive (Sun et al., 2020a), or mean-square-error (MSE) (Passban et al.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : ", 2020a), or mean-square-error (MSE) (Passban et al., 2021; Sun et al., 2019) can be applied for our RAIL-KD approach.",
      "startOffset" : 37,
      "endOffset" : 77
    }, {
      "referenceID" : 32,
      "context" : ", 2020a), or mean-square-error (MSE) (Passban et al., 2021; Sun et al., 2019) can be applied for our RAIL-KD approach.",
      "startOffset" : 37,
      "endOffset" : 77
    }, {
      "referenceID" : 40,
      "context" : "We evaluate RAIL-KD on 8 tasks from the GLUE benchmark (Wang et al., 2018): 2 single-sentence (CoLA and SST-2) and 5 sentence-pair (MRPC, RTE, QQP, QNLI, and MNLI) classification tasks,",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 32,
      "context" : "Following prior works (Sun et al., 2019; Passban et al., 2021; Jiao et al., 2019; Sun et al., 2020a), we use the same metrics as the GLUE benchmark for evaluation.",
      "startOffset" : 22,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "Following prior works (Sun et al., 2019; Passban et al., 2021; Jiao et al., 2019; Sun et al., 2020a), we use the same metrics as the GLUE benchmark for evaluation.",
      "startOffset" : 22,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : "Following prior works (Sun et al., 2019; Passban et al., 2021; Jiao et al., 2019; Sun et al., 2020a), we use the same metrics as the GLUE benchmark for evaluation.",
      "startOffset" : 22,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "Moreover, to further show the generalization capability of our RAIL-KD method on out-of-domain (OOD) across tasks, we use Scitail (Khot et al., 2018), PAWS (Paraphrase Adversaries from Word Scrambling) (Zhang et al.",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 48,
      "context" : ", 2018), PAWS (Paraphrase Adversaries from Word Scrambling) (Zhang et al., 2019), and IMDb (Internet Movie Database) (Maas et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : ", 2019), and IMDb (Internet Movie Database) (Maas et al., 2011) test sets to evaluate the models fine-tuned on MNLI, QQP, and SST-2 tasks respectively.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "We experiment with the 12-layer BERT-base-uncased (Devlin et al., 2019) model as teacher (BERT12) and the 6-layer DistilBERT (Sanh et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : ", 2019) model as teacher (BERT12) and the 6-layer DistilBERT (Sanh et al., 2019a) as student (DistillBERT6) to compare with PKD (Sun et al.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 32,
      "context" : ", 2019a) as student (DistillBERT6) to compare with PKD (Sun et al., 2019) and ALP-KD (Passban et al.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "Also, we consider 24-layer RoBERTa-large (Liu et al., 2020) and 6-layer DistilRoberta (Sanh et al.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : ", 2020) and 6-layer DistilRoberta (Sanh et al., 2019b) as the backbone for",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "Results of CoDIR are directly copied from their paper (Sun et al., 2020a).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 33,
      "context" : "Furthermore, we demonstrate the effectiveness of RAIL-KD by directly comparing it with CoDIR (Sun et al., 2020a), the current state-ofthe-art ILD method.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 18,
      "context" : "These datasets contains counterexamples to biases found in the training data (McCoy et al., 2019; Schuster et al., 2019; Clark et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 140
    }, {
      "referenceID" : 30,
      "context" : "These datasets contains counterexamples to biases found in the training data (McCoy et al., 2019; Schuster et al., 2019; Clark et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "These datasets contains counterexamples to biases found in the training data (McCoy et al., 2019; Schuster et al., 2019; Clark et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "This was also reported in prior works on out-of-domain training and evaluation (Clark et al., 2019; Mahabadi et al., 2020; Utama et al., 2020; Sanh et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 161
    }, {
      "referenceID" : 17,
      "context" : "This was also reported in prior works on out-of-domain training and evaluation (Clark et al., 2019; Mahabadi et al., 2020; Utama et al., 2020; Sanh et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 161
    }, {
      "referenceID" : 37,
      "context" : "This was also reported in prior works on out-of-domain training and evaluation (Clark et al., 2019; Mahabadi et al., 2020; Utama et al., 2020; Sanh et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 161
    }, {
      "referenceID" : 29,
      "context" : "This was also reported in prior works on out-of-domain training and evaluation (Clark et al., 2019; Mahabadi et al., 2020; Utama et al., 2020; Sanh et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 161
    } ],
    "year" : 0,
    "abstractText" : "Intermediate layer knowledge distillation (KD) can improve the standard KD technique (which only targets the output of teacher and student models) especially over large pre-trained language models. However, intermediate layer distillation suffers from excessive computational burdens and engineering efforts required for setting up a proper layer mapping. To address these problems, we propose a RAndom Intermediate Layer Knowledge Distillation (RAIL-KD) approach in which, intermediate layers from the teacher model are selected randomly to be distilled into the intermediate layers of the student model. This randomized selection enforces that all teacher layers are taken into account in the training process, while reducing the computational cost of intermediate layer distillation. Also, we show that it acts as a regularizer for improving the generalizability of the student model. We perform extensive experiments on GLUE tasks as well as on out-of-domain test sets. We show that our proposed RAIL-KD approach outperforms other state-of-the-art intermediate layer KD methods considerably in both performance and training-time.",
    "creator" : null
  }
}
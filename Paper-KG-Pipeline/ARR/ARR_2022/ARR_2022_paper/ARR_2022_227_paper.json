{
  "name" : "ARR_2022_227_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Ditch the Gold Standard: Re-evaluating Conversational Question Answering",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Conversational question answering (CQA) aims to build machines to answer questions in conversations, and has the promise to revolutionize the way humans interact with machines for informationseeking. With recent development of large-scale datasets (Choi et al., 2018; Saeidi et al., 2018; Reddy et al., 2019; Campos et al., 2020), rapid progress has been made in better modeling of conversational QA systems.\nCurrent datasets are collected by crowdsourcing human-human conversations, where the questioner asks questions based on an evidence passage and conversational history and the answerer provides\ncorresponding answers. When evaluating CQA systems, a set of held-out conversations are used for asking models questions in turn. Since the evaluation builds on pre-collected conversations, the gold history of the conversation is always provided, regardless of models’ actual predictions (Figure 1). Despite the extremely competitive performance of current systems on this static evaluation, it is questionable whether this can faithfully reflect models’ true performance in real-world applications. To what extent do human-machine conversations deviate from human-human conversations? What will happen if models have no access to ground-truth answers in a conversation?\nTo answer these questions and better understand the performance of CQA systems, we carry out the first large-scale human evaluation with four state-of-the-art models on the QuAC dataset (Choi et al., 2018), by having human evaluators converse with the models and judge the correctness of their answers. We collected 1,446 human-machine conversations in total, with 15,059 question-answer pairs. Through a careful analysis, we identify a significant distribution shift from human-human conversations and discover a clear inconsistency of model performance between current evaluation protocol and human evaluation.\nThis finding motivates us to improve the automatic evaluation so that it is better aligned with human evaluation. Mandya et al. (2020); Siblini et al. (2021) identify a similar issue in gold-history evaluation and propose to use models’ own predictions for automatic evaluation. However, predictedhistory evaluation poses another challenge: since all the questions have been collected beforehand, using predicted history will invalidate some of the questions because of changes in the conversational history (see Figure 1 for an example).\nBased on this insight, we propose a question rewriting mechanism, which automatically detects and rewrites invalid questions with predicted his-\ntory (Figure 4). We use a coreference resolution model (Lee et al., 2018) to detect coreference inconsistency of question text between predicted history and gold history, and then rewrite those questions by substituting with the correct mentions, so that the questions are resolvable in the context. Compared to predicted-history evaluation, we find that incorporating rewriting better aligns with human judgements and reflects models’ true performance.\nFinally, we also investigate the impact of different modeling strategies on human evaluation. We find that accurately detecting unanswerable questions and explicitly modeling question dependencies on the context are crucial in model performance. Equipped with all the insights, we discuss directions for CQA modeling. We will release our human evaluation dataset and hope that our findings can shed light on future development of better conversational QA systems."
    }, {
      "heading" : "2 Preliminary",
      "text" : ""
    }, {
      "heading" : "2.1 Evaluation of Conversational QA",
      "text" : "In conversational question answering, there is an evidence passage P , a (human) questionerH that has no access to P , and a model M that has access to P . The questioner asks questions about P and the model answers them based on P and the conversational history so far (see an example in Figure 1). Formally, for the i-th turn, the human asks a question based on the previous conversation,\nQi ∼ H(Q1, A1, ..., Qi−1, Ai−1),\nand then the model answers it based on both the history and the passage,\nAi ∼M(P,Q1, A1, ..., Qi−1, Ai−1, Qi),\nwhere Qi and Ai represent the question and the answer at the i-th turn. If the question is unanswerable from P , Ai = CANNOT ANSWER. The model M is evaluated by the correctness of answers.\nEvaluating CQA systems requires having human in the loop and it is expensive to collect the judgments. Instead, current CQA benchmarks use automatic evaluation with gold history (Auto-Gold). For example, QuAC (Choi et al., 2018) collects a set of human-human conversations for automatic evaluation. For each passage, one annotator asks questions without seeing the passage, while the other annotator provides the answers. Denote the collected questions and answers as Q∗i and A ∗ i (gold answers). In gold-history evaluation, we inquire the model with pre-defined questions Q∗i :"
    }, {
      "heading" : "Ai ∼M(P,Q∗1, A∗1, ..., Q∗i−1, A∗i−1, Q∗i ),",
      "text" : "and we evaluate the model by comparing Ai to A∗i (measured by word-level F1). This process does not require human effort, but it can’t truly reflect the distribution of real human-machine conversations, since the questioner may ask different questions based on different models’ predictions.\nIn this work, we choose the QuAC dataset for our main evaluation, since it is closer to real-world information-seeking conversations, where the questioner cannot see the evidence passage. It prevents the questioner asking questions that simply overlaps with the passage and encourages truly unanswerable questions. QuAC also adopts extractive question answering that restricts the answer as a span of text, and more modeling work has been done than in free-form question answering."
    }, {
      "heading" : "2.2 Models",
      "text" : "For human evaluation and analysis, we choose the following four representative CQA models:\nBERT. A simple BERT baseline, which concatenates the passage, the previous two turns of question-answer pairs, and the question as the input and predicts the answer as in Devlin et al. (2019).1\nGraphFlow. Chen et al. (2020) propose a recurrent graph neural network on top of BERT embeddings to model the question dependencies on the history and the passage.\nHAM. Qu et al. (2019) propose a history attention mechanism (HAM) to softly select the most relevant previous turns.\nExCorD. Kim et al. (2021) train a question rewriting model on CANARD (Elgohary et al., 2019) to generate context-independent questions, and then use both the original and the generated questions to train the QA model. This model achieves the current state-of-the-art on QuAC (67.7 F1).\nFor all the models except for BERT, we use the original implementations for a direct comparison."
    }, {
      "heading" : "3 Human Evaluation",
      "text" : "In this section, we carry out a large-scale human evaluation with the four models discussed above."
    }, {
      "heading" : "3.1 Conversation Collection",
      "text" : "We collect human-machine conversations using 100 passages from the QuAC development set on Amazon Mechanical Turk.2 We also design a set of qualification questions to make sure that the annotators fully understand our annotation guideline. For each model and each passage, we collect three conversations from three different annotators.\nWe collect each conversation in two steps: (1) The annotator has no access to the passage and asks questions. The model extracts the answer span from the passage in a human-machine conversation interface.3 We provide the title, the section title, the background of the passage, and the first question from QuAC as a prompt to annotators. Annotators are required to ask at least 8 and at most 12 questions. We encourage context-dependent questions, but also allow open questions like “What\n1We use bert-base-uncased as the encoder. 2We restrict the annotators from English-speaking countries, having finished at least 1,000 HITS with an acceptance rate at least 95% for high quality.\n3We used ParlAI (Miller et al., 2017) to build the interface.\nelse is interesting” if asking a follow-up question is difficult. (2) After the conversation ends, the annotator is shown the passage and asked to check whether the model’s answers are correct or not.\nWe noticed that the annotators are biased when evaluating the correctness of answers. For questions to which the model answered CANNOT ANSWER, annotators tend to mark the answer as incorrect without checking if the question is answerable. Additionally, for answers with the correct types (for example, a date as an answer to “When was it?”), annotators tend to mark it as \"correct\" without verifying from the passage. Therefore, we asked another group of annotators to verify question answerability and answer correctness."
    }, {
      "heading" : "3.2 Answer Validation",
      "text" : "For each collected conversation, we ask two additional annotators to validate annotated answers. First, each annotator reads the passage before seeing the conversation. Then, the annotator sees the question (and question only) and selects whether the question is (a) ungrammatical, (b) unanswerable, or (c) answerable. If the annotator chooses “answerable”, the interface then reveals the answer and asks about its correctness. If the answer is “incorrect”, the annotator selects the answer span from the passage. We discard all questions that both annotators find “ungrammatical” and the correctness is taken as the majority of the 3 annotations.\nIn total, we collected 1,446 human-machine conversations and 15,059 question-answer pairs. The data distribution is very different from the humanhuman conversations (the QuAC dataset): we see more open questions and unanswerable questions, due to less fluent conversation flow caused by model mistakes, and that models cannot provide feedback to questioners like human annotators do. (see more detailed analysis in §6.2). This collection not only supports our comparison and analysis, but also complements existing datasets."
    }, {
      "heading" : "3.3 Annotator Agreement",
      "text" : "Deciding the correctness of answers is challenging for humans in some cases, especially when questions are relatively short and ambiguous. We measure annotators’ agreement and calculate the Fleiss’ Kappa (Fleiss, 1971) on the agreement between annotators in the validation phase. For deciding one turn is unanswerable, correct, or incorrect, we achieve κ = 0.598 (moderate agreement). For deciding whether one turn is unanswerable, we have\nκ = 0.679 (substantial agreement)."
    }, {
      "heading" : "4 Disagreements between Human and Gold History Evaluation",
      "text" : "We now compare the results from the human evaluation and the automatic evaluation with gold history. Note that the two sets of numbers are not directly comparable: (1) the human evaluation reports accuracy, while the automatic evaluation reports F1 scores; (2) the absolute numbers of human evaluation are much higher than those of automatic evaluations. In automatic evaluations, the gold answers cannot capture all possible correct answers to open-ended questions or questions with multiple answers. However, the annotators can evaluate the correctness of answers easily in human evaluations. Nevertheless, we can compare the rankings and the relative gaps between models.\nFigure 2 shows different trends between human evaluation and Auto-Gold. Current standard evaluation cannot reflect model performance in humanmachine conversations: (1) Human evaluation and Auto-Gold rank BERT and GraphFlow differently; (2) The gap between HAM and ExCorD is significant in Auto-Gold but the two models perform on par in human evaluation."
    }, {
      "heading" : "5 Strategies for Automatic Evaluation",
      "text" : "The inconsistency between human evaluation and gold-history evaluation suggests that we need better ways to evaluate and develop our CQA models. When placed in realistic settings, the models never have access to the ground truth (gold answers) and are only exposed to the conversational history and\nthe passage. Intuitively, we can simply replace gold answers by the predicted answers of models and we name this as predicted-history evaluation (Auto-Pred). Formally, the model makes predictions based on the questions and their own answers:\nAi ∼M(P,Q∗1, A1, ..., Q∗i−1, Ai−1, Q∗i ).\nThis evaluation has been suggested by several recent works (Mandya et al., 2020; Siblini et al., 2021) which reported a significant performance drop using predicted history. We observe the same performance degradation, shown in Table 1.\nHowever, another issue naturally arises with predicted history: Q∗i s were written by the dataset annotators based on (Q∗1, A ∗ 1, ..., Q ∗ i−1, A ∗ i−1) which may become unnatural or invalid if we change the history to (Q∗1, A1, ..., Q ∗ i−1, Ai−1). We investigate this issue in depth next."
    }, {
      "heading" : "5.1 Predicted History Invalidates Questions",
      "text" : "We examined 100 QuAC conversations with the best-performing model (ExCorD) and identified three categories of invalid questions caused by predicted history. We find that 23% of the questions become invalid after using the predicted history. We summarize the types of invalid questions as follows (see Figure 3 for examples):\n• Unresolved coreference (44.0%). The question becomes invalid for containing either a pronoun\nor a definite noun phrase that refers to an entity unresolvable without the gold history.\n• Incoherence (39.1%). The question is incoherent with the conversation flow (e.g., mentioning an entity non-existent in predictions). While humans may still answer the question using the passage, this leads to an unnatural conversation and a train-test discrepancy for models.\n• Correct answer changed (16.9%). The answer to this question with the predicted history changes from when it is based on the gold history.\nWe further analyze the reasons for the biggest “unresolved coreference” category and find that the model either gives an incorrect answer to the previous question (“incorrect prediction”, 17.5%), or the model predicts a different (yet correct) answer to an open question (“open question”, 16.3%), or the model returns CANNOT ANSWER incorrectly (“no prediction”, 4.2%), or the gold answer is longer than prediction and the next question depends on the extra part (“extra gold information”, 6.0%).\nInvalid questions result in compounding errors as the model is not able to parse them correctly, which may further affect how the model interprets the next questions. Since “unresolved coreference” accounts for most of invalid questions, we aim to address them with a better automatic evaluation."
    }, {
      "heading" : "5.2 Evaluation with Question Rewriting",
      "text" : "Among all the invalid question categories, “unresolved coreference” questions are the most critical ones. They lead to incorrect interpretations\nof questions and hence wrong answers. We propose to improve our evaluation by incorporating a state-of-the-art coreference resolution system to automatically detect invalid questions categorized as \"unresolved coreference\". More specifically, we use the coreference model from Lee et al. (2018) in AllenNLP (Gardner et al., 2018). We make the assumption that if the coreference model resolves mentions in Q∗i differently using gold history (Q∗1, A ∗ 1, ..., A ∗ i−1, Q ∗ i ) and predicted history (Q∗1, A1, ..., Ai−1, Q ∗ i ), thenQ ∗ i is identified as having an unresolved coreference issue.\nDetecting invalid questions. The inputs to the coreference model for Q∗i are the following:\nS∗i = [BG;Q ∗ i−k;A ∗ i−k;Q ∗ i−k+1;A ∗ i−k+1; ...;Q ∗ i ] Si = [BG;Q ∗ i−k;Ai−k;Q ∗ i−k+1;Ai−k+1; ...;Q ∗ i ],\nwhere BG is the background 4, S∗i and Si denote the inputs for gold and predicted history. We are only interested in the entities mentioned in the current question Q∗t and we filter out named entities (e.g., the National Football League) because they can be understood without coreference resolution. After the coreference model returns entity cluster information given S∗i and Si, we extract a list of entities E∗ = {e∗1, ..., e∗|E∗|} and E = {e1, ..., e|E|}. We say Q∗i is valid only if E ∗ = E, that is,\n|E∗| = |E| and e∗j = ej ,∀ej ∈ E,\nassuming e∗j and ej has a shared mention in Q ∗ i . We determine whether e∗j = ej by checking if F1(s∗j,1, sj,1) > 0, where s ∗ j,1 is the first mention of e∗j and sj,1 is the first mention of ej , and F1 is the word-level F1 score, i.e. e∗j = ej as long as their first mentions have word overlap.\nQuestion rewriting through entity substitution. Our first strategy is to substitute the entity names in Q∗i with entities in E\n∗, if Q∗i is invalid. The rewritten question, instead of the original one, will be used in the conversation history and fed into the model. We denote this evaluation method as rewritten-question evaluation (Auto-Rewrite), and Figure 4 illustrates a concrete example. Our algorithm rewrites ∼12% of the questions for all of the models. An analysis of rewritten questions’ quality is provided in Appendix B.\n4QuAC provides a short background for each passage, which is the first paragraph of the article the passage is from. It is empirically helpful for associating different spans in the conversation to the same entity mention.\nQuestion replacement using CANARD. Alternative to automatically rewriting questions, we also tried replacing the invalid questions with its humanwritten context-independent counterpart from CANARD (Elgohary et al., 2019), which we denote as replaced-question evaluation (Auto-Replace). Since collecting context-independent questions is expensive, Auto-Replace is limited to evaluating models trained with QuAC, thus we do not treat this as a generic method for CQA evaluation."
    }, {
      "heading" : "6 Automatic vs Human Evaluation",
      "text" : "In this section, we compare human evaluation results with all the automatic evaluations we have introduced: gold-history evaluation (Auto-Gold), predicted-history evaluation (Auto-Pred), and our proposed Auto-Rewrite and Auto-Replace. We first explain how we compare different evaluation results and then discuss the findings."
    }, {
      "heading" : "6.1 Agreement Metrics",
      "text" : "Model performance and rankings. We first consider using model performance reported by different evaluation methods. Considering numbers of automatic and human evaluations are not directly comparable, we also calculate models’ rankings and compare whether the rankings are consistent between automatic and human evaluations. Model performance is reported in Table 1. In human evaluation, GraphFlow < BERT < HAM ≈ ExCorD; in Auto-Gold, BERT < GraphFlow < HAM < ExCorD; in other automatic evaluations, GraphFlow < BERT < HAM < ExCorD.\nUnanswerable statistics. Percentage of unanswerable questions is an important attribute for conversations. Automatic evaluations using static datasets have a fixed number of unanswerable questions, while in human evaluation, the amount of unan-"
    }, {
      "heading" : "34.6 20.6 34.1 33.2 20.2",
      "text" : "swerable questions asked by human annotators varies with different models. The statistics of unanswerable questions is shown in Table 2.\nPairwise agreement. For a more fine-grained evaluation, we perform a passage-level comparison for every pair of models. More specifically, for every single passage we use one automatic metric to decide whether model A outperforms model B (or vice versa) and examine the percentage of passages that the automatic metric agrees with human evaluation. For example, if the pairwise agreement of BERT/ExCorD between human evaluation and Auto-Gold is 52%, it means that Auto-Gold and human evaluation agree on 52% passages in terms of\nwhich model is better. Higher agreement means the automatic evaluation is closer to human evaluation. Figure 5 shows the results of pairwise agreement."
    }, {
      "heading" : "6.2 Key Findings",
      "text" : "Automatic evaluations have a significant distribution shift from human evaluation. We draw this conclusion from the three following points.\n• Human evaluation shows a much higher model performance than all automatic evaluations, as shown in Table 1. Two reasons caused this huge discrepancy: (a) Many CQA questions have multiple possible answers, and it’s hard for the static dataset in automatic evaluations to capture all the answers. It is not an issue in human evaluation for all answers are judged by human evaluators. (b) There are more unanswerable questions and open questions in human evaluation (reason discussed in the next paragraph), which are relatively easy.\n• Human evaluation has a much higher unanswerable question rate, as shown in Table 2. The reason is that in human-human data collection, the answers are usually correct and the questioners can ask followup questions upon the highquality conversation; in human-machine interactions, since the models can make mistakes, the conversation flow is less fluent and it is harder to have followup questions. Thus, questioners chatting with models tend to ask more open or unanswerable questions. This also suggests that current CQA models are far from perfection.\n• All automatic evaluation methods have a pairwise agreement lower than 70% with human evaluation, as demonstrated in Figure 2.\nAuto-Rewrite is closer to human evaluation. First, we can clearly see that among all automatic evaluations, Auto-Gold deviates the most from the\nhuman evaluation. From Table 1, only Auto-Gold shows different rankings from human evaluation, while Auto-Pred, Auto-Rewrite, and Auto-Replace show consistent rankings to human judgments.\nIn Figure 2, we see that Auto-Gold has the lowest agreement with human evaluation; among others, Auto-Rewrite better agrees with human evaluation for most model pairs. Surprisingly, Auto-Rewrite is even better than Auto-Replace – which uses human annotated context independent questions – in most cases. It shows that our rewriting policy can better reflect the real-world CQA performance."
    }, {
      "heading" : "7 Towards Better Conversational QA",
      "text" : "With insights drawn from human evaluation and comparison with automatic evaluations, we discuss the impact of different modeling strategies, as well as future directions towards better CQA systems.\nModeling question dependencies on conversational context. When we focus on answerable questions (Table 1), we notice that GraphFlow, HAM and ExCorD perform much better than BERT. We compare the modeling differences of the four systems in Figure 6, and identify that all the three better systems explicitly model the question dependencies on the conversation history and the passage: both GraphFlow and HAM highlight repeated mentions in questions and conversation history by special embeddings (turn marker and PosHAE) and use attention mechanism to select the most relevant part from the context; ExCorD adopts a question rewriting module that generates context-independent questions given the history and passage. All those designs help models better understand the question in a conversational context.\nUnanswerable question detection. Table 3 demonstrates models’ performance in detecting unanswerable questions. We notice that GraphFlow predicts much fewer unanswerable questions\nthan the other three models, and has a high precision and a low recall in unanswerable detection. This is because GraphFlow uses a separate network for predicting unanswerable questions, which is harder to calibrate, while the other models jointly predict unanswerable questions and answer spans.\nThis behavior has two effects: (a) GraphFlow’s overall performance is dragged down by its poor unanswerable detection result (Table 1). (b) In human evaluation, annotators ask fewer unanswerable questions with GraphFlow (Table 2) – when the model outputs more, regardless of correctness, the human questioner has a higher chance to ask passage-related followup questions. Both suggest that how well the model detects unanswerable questions significantly affects its performance and the flow in human-machine conversations.\nOptimizing towards the new testing protocols. Most existing works on CQA modeling focus on optimizing towards Auto-Gold evaluation. Since Auto-Gold has a large gap from the real world evaluation, more efforts are needed in optimizing towards the human evaluation, or Auto-Rewrite, which better reflects human evaluation. One potential direction is to improve models’ robustness given noisy conversation history, which simulates the inaccurate history in real conversations that consists of models’ own predictions. In fact, prior works (Mandya et al., 2020; Siblini et al., 2021) that used predicted history in training showed that it benefits the models in predicted-history evaluation."
    }, {
      "heading" : "8 Related Work",
      "text" : "Conversational question answering. In recent years, several conversational question answering datasets have emerged, such as QuAC (Choi et al., 2018), CoQA (Reddy et al., 2019), and DoQA (Campos et al., 2020). Different from singleturn QA datasets (Rajpurkar et al., 2016), CQA\nrequires the model to understand the question in the context of conversational history. There have been many methods proposed to improve CQA performance (Ohsugi et al., 2019; Chen et al., 2020; Qu et al., 2019; Kim et al., 2021) and significant improvement has been made on CQA benchmarks.\nBesides text-based CQA tasks, there also exist CQA benchmarks that require other forms of modeling ability, such as combining textual evidence with background knowledge (Saeidi et al., 2018), utilizing structured knowledge base (Saha et al., 2018; Guo et al., 2018), as well as CQA in other modalities (Das et al., 2017).\nEvaluation with predicted history. Only recently has it been noticed that the current method of evaluating CQA models is flawed. Mandya et al. (2020); Siblini et al. (2021) point out that using gold answers in history is not consistent with the realworld scenario and propose to use predicted history for evaluation. Different from prior work, in this paper, we conduct a large scale human evaluation to support our claims, identify the issues with predicted history, and propose rewriting questions to further mitigate the gap to human evaluation."
    }, {
      "heading" : "9 Conclusion",
      "text" : "In this work, we carry out the first large-scale human evaluation on CQA systems. We show that current standard automatic evaluation with gold history cannot reflect models’ performance in human evaluation, and that human-machine conversations have a large distribution shift from static CQA datasets of human-human conversations. To tackle these problems, we propose to use predicted history with rewriting invalid questions for evaluation, which reduces the gap between automatic evaluations and the real-world human evaluation. We also use the human evaluation results to analyze current CQA systems and identify promising directions for future development."
    }, {
      "heading" : "A Human Evaluation Statistics",
      "text" : "Table 4 shows the human evaluation statistics, including numbers of conversations and questions regarding each model."
    }, {
      "heading" : "B Quality of Rewriting Questions",
      "text" : "To analyze how well Auto-Rewrite does in detecting and rewriting questions, we manually check 100 conversations of ExCorD from the QuAC development set. We find that Auto-Rewrite can detect invalid questions with a precision of 72% and a recall of 72%. We notice that the coreference model sometimes detects the pronoun of the main character in the passage, which almost shows up in every question, as insolvable. This issue causes the low precision but is not a serious problem in our case – whether rewriting the pronoun of the main character does not affect models’ prediction much, because the model always sees the passage and knows who the main character is.\nAmong all correctly detected invalid questions, we further check the quality of rewriting, and in 68% of the times Auto-Rewrite gives a correct context-independent questions. The most common error is being ungrammatical: For example, using the gold history of \"... Dee Dee claimed that Spector once pulled a gun on him\", the original question \"Did they arrest him for doing this?\" was rewritten to \"Did they arrest Phillip Harvey Spector for doing pulled?\" While this creates a distribution shift on question formats, it is still better than putting an invalid question in the flow.\nC Importance of Explicit Dependency Modeling.\nFigure 7 gives an example where GraphFlow, HAM and ExCorD correctly resolve the question phrase from long conversation history while BERT failed. This is caused by BERT’s lack of explicit question dependency modeling."
    } ],
    "references" : [ {
      "title" : "DoQA - accessing domain-specific FAQs via conversational QA",
      "author" : [ "Jon Ander Campos", "Arantxa Otegi", "Aitor Soroa", "Jan Deriu", "Mark Cieliebak", "Eneko Agirre." ],
      "venue" : "Association for Computational Linguistics (ACL), pages 7302–7314.",
      "citeRegEx" : "Campos et al\\.,? 2020",
      "shortCiteRegEx" : "Campos et al\\.",
      "year" : 2020
    }, {
      "title" : "Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension",
      "author" : [ "Yu Chen", "Lingfei Wu", "Mohammed J Zaki." ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI).",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "QuAC: Question answering in context",
      "author" : [ "Eunsol Choi", "He He", "Mohit Iyyer", "Mark Yatskar", "Wentau Yih", "Yejin Choi", "Percy Liang", "Luke Zettlemoyer." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 2174–2184.",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Visual dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José M.F. Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1080–1089.",
      "citeRegEx" : "Das et al\\.,? 2017",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Can you unpack that? learning to rewrite questions-in-context",
      "author" : [ "Ahmed Elgohary", "Denis Peskov", "Jordan BoydGraber." ],
      "venue" : "Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Elgohary et al\\.,? 2019",
      "shortCiteRegEx" : "Elgohary et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin, 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "AllenNLP: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of Workshop for",
      "citeRegEx" : "Gardner et al\\.,? 2018",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialog-to-action: Conversational question answering over a large-scale knowledge base",
      "author" : [ "Daya Guo", "Duyu Tang", "Nan Duan", "Ming Zhou", "Jian Yin." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 2942–2951.",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "Learn to resolve conversational dependency: A consistency training framework for conversational question answering",
      "author" : [ "Gangwoo Kim", "Hyunjae Kim", "Jungsoo Park", "Jaewoo Kang." ],
      "venue" : "Association for Computational Linguistics (ACL), pages 6130–",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Higher-order coreference resolution with coarse-tofine inference",
      "author" : [ "Kenton Lee", "Luheng He", "Luke Zettlemoyer." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 687–",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Do not let the history haunt you: Mitigating compounding errors in conversational question answering",
      "author" : [ "Angrosh Mandya", "James O’ Neill", "Danushka Bollegala", "Frans Coenen" ],
      "venue" : "In International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Mandya et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mandya et al\\.",
      "year" : 2020
    }, {
      "title" : "ParlAI: A dialog research software platform",
      "author" : [ "Alexander Miller", "Will Feng", "Dhruv Batra", "Antoine Bordes", "Adam Fisch", "Jiasen Lu", "Devi Parikh", "Jason Weston." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP): System Demonstra-",
      "citeRegEx" : "Miller et al\\.,? 2017",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2017
    }, {
      "title" : "A simple but effective method to incorporate multi-turn context with BERT for conversational machine comprehension",
      "author" : [ "Yasuhito Ohsugi", "Itsumi Saito", "Kyosuke Nishida", "Hisako Asano", "Junji Tomita." ],
      "venue" : "Proceedings of the First Workshop on",
      "citeRegEx" : "Ohsugi et al\\.,? 2019",
      "shortCiteRegEx" : "Ohsugi et al\\.",
      "year" : 2019
    }, {
      "title" : "Attentive history selection for conversational question answering",
      "author" : [ "Chen Qu", "Liu Yang", "Minghui Qiu", "Yongfeng Zhang", "Cen Chen", "W Bruce Croft", "Mohit Iyyer." ],
      "venue" : "ACM International Conference on Information and Knowledge Management",
      "citeRegEx" : "Qu et al\\.,? 2019",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "CoQA: A conversational question answering challenge",
      "author" : [ "Siva Reddy", "Danqi Chen", "Christopher D. Manning." ],
      "venue" : "Transactions of the Association of Computational Linguistics (TACL), pages 249–266.",
      "citeRegEx" : "Reddy et al\\.,? 2019",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2019
    }, {
      "title" : "Interpretation of natural language rules in conversational machine reading",
      "author" : [ "Marzieh Saeidi", "Max Bartolo", "Patrick Lewis", "Sameer Singh", "Tim Rocktäschel", "Mike Sheldon", "Guillaume Bouchard", "Sebastian Riedel." ],
      "venue" : "Empirical Methods in Natural Lan-",
      "citeRegEx" : "Saeidi et al\\.,? 2018",
      "shortCiteRegEx" : "Saeidi et al\\.",
      "year" : 2018
    }, {
      "title" : "Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph",
      "author" : [ "Amrita Saha", "Vardaan Pahuja", "Mitesh M. Khapra", "Karthik Sankaranarayanan", "Sarath Chandar." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Saha et al\\.,? 2018",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards a more robust evaluation for conversational question answering",
      "author" : [ "Wissam Siblini", "Baris Sayil", "Yacine Kessaci." ],
      "venue" : "Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-",
      "citeRegEx" : "Siblini et al\\.,? 2021",
      "shortCiteRegEx" : "Siblini et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "With recent development of large-scale datasets (Choi et al., 2018; Saeidi et al., 2018; Reddy et al., 2019; Campos et al., 2020), rapid progress has been made in better modeling of conversational QA systems.",
      "startOffset" : 48,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "With recent development of large-scale datasets (Choi et al., 2018; Saeidi et al., 2018; Reddy et al., 2019; Campos et al., 2020), rapid progress has been made in better modeling of conversational QA systems.",
      "startOffset" : 48,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "With recent development of large-scale datasets (Choi et al., 2018; Saeidi et al., 2018; Reddy et al., 2019; Campos et al., 2020), rapid progress has been made in better modeling of conversational QA systems.",
      "startOffset" : 48,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "With recent development of large-scale datasets (Choi et al., 2018; Saeidi et al., 2018; Reddy et al., 2019; Campos et al., 2020), rapid progress has been made in better modeling of conversational QA systems.",
      "startOffset" : 48,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "To answer these questions and better understand the performance of CQA systems, we carry out the first large-scale human evaluation with four state-of-the-art models on the QuAC dataset (Choi et al., 2018), by having human evaluators converse with the models and judge the correctness of their answers.",
      "startOffset" : 186,
      "endOffset" : 205
    }, {
      "referenceID" : 10,
      "context" : "We use a coreference resolution model (Lee et al., 2018) to detect coreference inconsistency of question text between predicted history and gold history, and then rewrite those questions by substituting with the correct mentions, so that the questions are resolvable in the context.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "For example, QuAC (Choi et al., 2018) collects a set of human-human conversations for automatic evaluation.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "(2021) train a question rewriting model on CANARD (Elgohary et al., 2019) to generate context-independent questions, and then use both the original and the generated questions to train the QA model.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "(3)We used ParlAI (Miller et al., 2017) to build the interface.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "We measure annotators’ agreement and calculate the Fleiss’ Kappa (Fleiss, 1971) on the agreement between annotators in the validation phase.",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "This evaluation has been suggested by several recent works (Mandya et al., 2020; Siblini et al., 2021) which reported a significant performance drop using predicted history.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "This evaluation has been suggested by several recent works (Mandya et al., 2020; Siblini et al., 2021) which reported a significant performance drop using predicted history.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Alternative to automatically rewriting questions, we also tried replacing the invalid questions with its humanwritten context-independent counterpart from CANARD (Elgohary et al., 2019), which we denote as replaced-question evaluation (Auto-Replace).",
      "startOffset" : 162,
      "endOffset" : 185
    }, {
      "referenceID" : 11,
      "context" : "In fact, prior works (Mandya et al., 2020; Siblini et al., 2021) that used predicted history in training showed that it benefits the models in predicted-history evaluation.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "In fact, prior works (Mandya et al., 2020; Siblini et al., 2021) that used predicted history in training showed that it benefits the models in predicted-history evaluation.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "In recent years, several conversational question answering datasets have emerged, such as QuAC (Choi et al., 2018), CoQA (Reddy et al.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : ", 2018), CoQA (Reddy et al., 2019), and DoQA (Campos et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "Different from singleturn QA datasets (Rajpurkar et al., 2016), CQA requires the model to understand the question in the context of conversational history.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "There have been many methods proposed to improve CQA performance (Ohsugi et al., 2019; Chen et al., 2020; Qu et al., 2019; Kim et al., 2021) and significant improvement has been made on CQA benchmarks.",
      "startOffset" : 65,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "There have been many methods proposed to improve CQA performance (Ohsugi et al., 2019; Chen et al., 2020; Qu et al., 2019; Kim et al., 2021) and significant improvement has been made on CQA benchmarks.",
      "startOffset" : 65,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "There have been many methods proposed to improve CQA performance (Ohsugi et al., 2019; Chen et al., 2020; Qu et al., 2019; Kim et al., 2021) and significant improvement has been made on CQA benchmarks.",
      "startOffset" : 65,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "There have been many methods proposed to improve CQA performance (Ohsugi et al., 2019; Chen et al., 2020; Qu et al., 2019; Kim et al., 2021) and significant improvement has been made on CQA benchmarks.",
      "startOffset" : 65,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "Besides text-based CQA tasks, there also exist CQA benchmarks that require other forms of modeling ability, such as combining textual evidence with background knowledge (Saeidi et al., 2018), utilizing structured knowledge base (Saha et al.",
      "startOffset" : 169,
      "endOffset" : 190
    }, {
      "referenceID" : 18,
      "context" : ", 2018), utilizing structured knowledge base (Saha et al., 2018; Guo et al., 2018), as well as CQA in other modalities (Das et al.",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : ", 2018), utilizing structured knowledge base (Saha et al., 2018; Guo et al., 2018), as well as CQA in other modalities (Das et al.",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ", 2018), as well as CQA in other modalities (Das et al., 2017).",
      "startOffset" : 44,
      "endOffset" : 62
    } ],
    "year" : 0,
    "abstractText" : "Conversational question answering (CQA) systems aim to provide natural-language answers to users in information-seeking conversations. Existing benchmarks compare CQA models with pre-collected human-human conversations, using ground-truth answers provided in conversational history. It remains unclear whether we can rely on this static evaluation for model development, and whether current systems can well generalize to real-world human-machine conversations. In this work, we conduct the first large-scale human evaluation of state-of-the-art CQA systems, where human evaluators converse with models and judge the correctness of their answers. We find that the distribution of human-machine conversations drastically differs from that of humanhuman conversations, and evaluating with gold answers is inconsistent with human evaluation. We further investigate how to improve automatic evaluations and propose a question rewriting mechanism based on predicted history, which better correlates with human judgments. Finally, we analyze the impact of various modeling strategies. We hope our findings can shed light to how to develop better CQA systems in the future.",
    "creator" : null
  }
}
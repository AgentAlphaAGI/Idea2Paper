{
  "name" : "ARR_2022_140_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, knowledge graphs (KGs) such as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015) and NELL (Mitchell et al., 2018) have been widely used in many knowledge-intensive applications, including question answering (Sun et al., 2020; Saxena et al., 2020), dialogue systems (Yang et al., 2020; Zhou et al., 2018) and recommender systems (Wang et al., 2021, 2019). However, the KGs constructed manually or automatically are inevitably incomplete, requiring KGC to infer new facts.\nThe previous KGC models can be classified into three main streams: (1) Rule learning-based models mine logic rules for induction reasoning, such as AMIE+ (Galárraga et al., 2015),\nDRUM (Sadeghian et al., 2019) and AnyBurl (Meilicke et al., 2019). (2) Path-based models (Liu et al., 2020; Xiong et al., 2017; Lin et al., 2018) search paths for multi-hop reasoning. (3) KGE models such as TransE (Bordes et al., 2013) and its variants (Sun et al., 2019; Zhang et al., 2019a, 2020) learn the embeddings of entities and relations to score the plausibility of triples for link prediction.\nAmong all the existing KGC models, KGE approaches achieve higher efficiency and better performance. Specifically, the KGE-based KGC pipeline can be divided into two stages: learning knowledge graph (KG) embeddings at the training and link prediction at the inference. Learning KG embeddings relies on a basic procedure of negative sampling (Li et al., 2021). Link prediction aims to infer the missing entity or relation in a triple via ranking the candidate triples’ scores in virtue of the learned KG embeddings.\nHowever, the two separate stages both have draw-\nbacks: (1) Invalid negative sampling: all the previous NS (Wang et al., 2014; Cai and Wang, 2018; Sun et al., 2019; Zhang et al., 2019b; Denis et al., 2015) cannot avoid sampling the falsenegative triples and low-quality negative triples, simultaneously. For instance, given the positive triple (Los Angeles, LocatedIn,California) as shown in Figure 1, the existing NS strategies might sample the corrupted triples such as (San Francisco, LocatedIn,California), which is actually a missing correct triple namely false-negative triple. On the other hand, the quality of some generated negative triples such as (San Francisco, LocatedIn,Apple P ie) is too poor to make little sense for training the KGE models. (2) Uncertainty of fact-view link prediction: performing link prediction solely based on facts in a data-driven fashion suffers from uncertainty due to the deviation of KG embeddings compared to the symbolic representations, limiting the accuracy of KGC. Take the tail entity prediction (David,Nationality, ?) in Figure 1 as an instance. The correct tail entity should belong to the concept Country in the view of commonsense. Whereas the entity California that is inconsistent with commonsense even ranks highest via scoring the candidate triples with KG embeddings.\nLast but not least, although some KGE approaches exploit external information, including entity types (Xie et al., 2016b), textual descriptions (Xie et al., 2016a) and images of entities (Xie et al., 2017). Such auxiliary information is hard to access and enhances the single representation of entities rather than providing the semantics of commonsense. However, the valuable commonsense is always acquired by the expensive hand annotation (Rajani et al., 2019), so its high cost leads to relatively low coverage. Besides, the existing large-scale commonsense KGs such as ConceptNet (Speer et al., 2017) only contain the concepts without the links to the corresponding entities, causing them unavailable to the KGC task.\nTo address the above challenges, we propose a novel and scalable Commonsense-Aware Knowledge Embedding (CAKE) framework to improve the NS in the training of KGE and boost the performance of KGC benefited from the selfsupervision of commonsense. In specific, we attempt to automatically construct explicit commonsense via an instance abstraction technique from KGs. Then, contrary to random sampling, we pur-\nposefully generate the high-quality negative triples by taking advantage of the commonsense together with the characteristics of complex relations. Furthermore, a multi-view link prediction is conducted to determine the entity candidates that belong to the correct concepts in the commonsense view and predict the answer entities with the learned KG embeddings from the perspective of fact. In summary, the contributions of our work are three-fold:\n• We propose a scalable KGC framework with an automatic commonsense generation mechanism to extract valuable commonsense from factual triples and entity concepts.\n• We develop a commonsense-aware negative sampling strategy for generating valid and high-quality negative triples. Meanwhile, a multi-view link prediction mechanism is proposed to improve the accuracy of KGC.\n• Extensive experiments on four benchmark datasets illustrate the effectiveness and the scalability of our whole framework and each module. We promise to release all the codes and datasets when this paper is published."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 KGC Models",
      "text" : "The existing KGC models can be classified into three main categories: (1) Rule learning-based algorithms such as AMIE+ (Galárraga et al., 2015), DRUM (Sadeghian et al., 2019) and AnyBurl (Meilicke et al., 2019) automatically mine logic rules from KGs and apply these rules for inductive link prediction. However, these models are inefficient due to the time-consuming rule searching and evaluation. (2) Path-based models search paths linking head and tail entities, including path ranking approaches (Lao et al., 2011; Liu et al., 2020) and reinforcement learning-based models (Xiong et al., 2017; Lin et al., 2018). Whereas, multi-hop pathbased models also spend much time in path searching. (3) KG embedding (KGE) models such as TransE (Bordes et al., 2013), RESCAL (Nickel et al., 2011), ComplEx (Trouillon et al., 2016), RotatE (Sun et al., 2019) and HAKE (Zhang et al., 2020) learn the embeddings of entities and relations to score the plausibility of triples for predicting the missing triples efficiently. KGE approaches achieve higher efficiency and better performance on KGC compared with the others. However, the natural uncertainty of embeddings limits the precision\nof KGC relying solely on facts. More specifically, the KGE models generally need a primary negative sampling (NS) procedure to randomly or purposely sample some triples that are not observed in the KG as negative triples for training (Li et al., 2021)."
    }, {
      "heading" : "2.2 Negative Sampling of KGE",
      "text" : "Following the local closed-world assumption (Dong et al., 2014), the existing NS techniques for KGE can be classified into five categories: (1) Randomly and uniformly sampling: the majority of the KGE models generate negative triples via randomly replacing an entity or relation in a positive triple from a uniform distribution (Wang et al., 2014). (2) Adversarial-based sampling: KBGAN (Cai and Wang, 2018) integrates the KGE model with softmax probabilities to select the high-quality negative triples in an adversarial training framework. Selfadversarial sampling (Sun et al., 2019) performs similar to KBGAN, but it utilizes a self-scoring function without a generator. (3) Domain-based sampling: domain-based NS (Y. et al., 2019) and type-constrained NS (Denis et al., 2015) both leverage domain or type constraints on sampling the corrupted entities that belong to the correct domain. (4) Efficient sampling: NSCaching (Zhang et al., 2019b) employs cache containing candidates of negative triples to improve the efficiency of sampling. (5) None-sampling: NS-KGE (Li et al., 2021) eliminates the NS procedure by converting loss functions of KGE into a unified square loss.\nHowever, all the previous NS algorithms cannot address the issue of false-negative triples since these NS techniques, except for none sampling, would attempt to sample the corrupted triples with higher probability while they might be correct and just missing in the KG. Domain-based NS relies heavily on the constraint of the single type rather than the commonsense, limiting the diversity of negative triples. KBGAN introduces generative adversarial networks (GAN) in the NS framework, making the original model more complex and hard to train. None sampling eliminates the negative triples and has to convert each original KGE model into square loss, which weakens the performance of KGE models. These drawbacks of the NS strategies degrade the training of KGE and further limit the performance of KGC."
    }, {
      "heading" : "2.3 Commonsense Knowledge Graph",
      "text" : "Different from the factual triples, commonsense could inject rich abstract knowledge into KGs.\nHowever, the valuable commonsense is hard to access due to the costly hand annotation. In recent years, many researches attempt to construct general commonsense graphs such as ConceptNet (Speer et al., 2017), Microsoft Concept Graph (Ji et al., 2019) and ATOMIC (Sap et al., 2019). However, these commonsense graphs only contain the concepts without the links to the corresponding entities, causing them inapplicable to the KGC task. On the other hand, although some KGE models such as JOIE (Hao et al., 2019) employ the ontology built-in most of the KGs, i.e., NELL (Mitchell et al., 2018) and DBpedia (Lehmann et al., 2015), the relations in ontology such as isA, partOf and relatedTo mainly represent the type hierarchy but not the explicit commonsense. Such relations are useless for KGC because there are few overlaps between the ontological and the factual relations."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we introduce our novel and scalable CAKE framework. As shown in Figure 2, the entire pipeline consists of three developed modules: the automatic commonsense generation (ACG) module, the commonsense-aware negative sampling (CANS) module and the multi-view link prediction (MVLP) module. Firstly, the ACG module extracts the commonsense from the factual triples with the entity concepts via an instance abstraction mechanism (§ 3.2). Then, the CANS module employs the generated commonsense to produce the high-quality negative triples, which takes the characteristics of complex relations into account (§ 3.3). Afterwards, our approach feeds the positive and the weighted negative triples into the KGE model for learning entity and relation embeddings (§ 3.4). Finally, the MVLP module conducts link prediction in a coarse-to-fine fashion by filtering the candidates in the view of commonsense and predicting the answer entities with KG embeddings from the candidates in the view of fact (§ 3.5)."
    }, {
      "heading" : "3.1 Notations and Problem Formalization",
      "text" : "Commonsense. Commonsense has gained widespread attraction from its successful use in understanding high-level semantics, which is generally represented as the concepts with their ontological relations in some well-known commonsense graphs such as ConceptNet (Speer et al., 2017) and Microsoft Concept Graph (Ji et al., 2019). Notably, we extend the commonsense in two forms: the in-\ndividual form C1 and the set form C2. Both C1 and C2 are the sets of triples while each triple in C1 is constituted of a head entity’s concept ch and a tail entity’s concept ct associated with their instancelevel relation r, which can be written as follows:\nC1 = {(ch, r, ct)} (1) On the contrary, each triple in C2 consists of a relation r linking the corresponding head concept set Ch and tail concept set Ct, which is shown as:\nC2 = {(Ch, r, Ct)} (2)\nThe detailed description of commonsense generation is introduced in section 3.2.\nKGE Score Function. We could leverage any KGE model to learn the entity and relation embeddings owing to our scalable framework independent of the KGE model. Thus, we define a uniform symbol E(h, r, t) to represent the score function of any KEG model for evaluating the plausibility of a triple (h, r, t). More specifically, the three most typical score function patterns are given as follows:\n(1) The translation-based score function, such as TransE (Bordes et al., 2013):\nE(h, r, t) = ∥h + r − t∥ (3)\nwhere h, r and t denote the embeddings of head entity h, relation r and tail entity t, respectively.\n(2) The rotation-based score function, such as RotatE (Sun et al., 2019):\nE(h, r, t) = ∥h ◦ r − t∥ (4)\nwhere ◦ indicates the hardmard product. (3) The tensor decomposition-based score function, such as DistMult (Yang et al., 2015):\nE(h, r, t) = h⊤diag(Mr)t (5)\nwhere diag(Mr) represents the diagonal matrix of the relation r.\nLink Prediction. Following most of the previous KGC models, we regard link prediction as an entity prediction task. Given a triple query with an entity missing (h, r, ?) or (?, r, t), link prediction takes every entity as a candidate. It calculates the score of each candidate triple by employing the learned KG embeddings and the score function. Then, we rank the candidate entities in light of their scores and output the top n entities as results."
    }, {
      "heading" : "3.2 Automatic Commonsense Generation",
      "text" : "In terms of the representation of commonsense defined in section 3.1, our approach could theoretically generate commonsense from any KG automatically as long as there exist some concepts linked to the entities in the KG. Specifically, we develop an entity-to-concept converter to replace the entities in each factual triple with corresponding concepts. Meanwhile, the relations in commonsense entail the instance-level relations in factual KGs. Take an instance in Figure 2, the factual triple (David,Nationality, U.S.A.) can be transformed to a concept-level triple (Person, Nationality, Country). Particularly, the commonsense in the individual form C1 is achieved by wiping out the reduplicated concept-level triples. Afterwards, we merge the concept-level triples that contain the same relation into a set to construct the commonsense in the set form C2."
    }, {
      "heading" : "3.3 Commonsense-Aware Negative Sampling",
      "text" : "Intuitively, the negative triples satisfying commonsense are more challenging to distinguish from positive triples, contributing to more effective training signals. Therefore, we try to sample the negative\ntriples that conform to the commonsense. To reduce the false-negative triples, we exploit the characteristics of complex relations, namely 1-1, 1-N, N-1, and N-N defined in TransH (Wang et al., 2014) for negative sampling, where 1 implies that the entity is unique when given the relation and another entity, on the contrary, N denotes that there might be multiple entities in this case (non-unique entity). Based on this observation, two specific sampling strategies are proposed: (1) uniqueness sampling: in terms of corrupting a unique entity such as the tail entity of the N-1 relation, the corrupted triples except for the original positive one are definitely actual negative triples. Furthermore, the corrupted entities that share at least one concept with the correct entity are regarded as high-quality negative triples, contributing to a more consistent training signal. (2) None-unique sampling: for corrupting a non-unique entity such as a head entity linked by the N-1 relation, the entities belonging to the same concept(s) with the correct entity are more likely to be false-negative due to the nonuniqueness of the head entity. Thus, the weights of these negative triples being false-negative should be as low as possible in training. Meanwhile, we try to sample the triples conforming to the commonsense C2 for high quality.\nFor a better understanding, an example of generating high-quality negative triples with an N-1 relation is shown in Figure 3. The whole NS procedure can be divided into two steps. Step 1: selecting the\ncandidate concepts with commonsense C2. The candidate head concepts city, county and island are determined according to commonsense C2 and nonunique sampling. Besides, based on the uniqueness sampling strategy, the candidate tail concept is selected as the same concept stateprovince as that of Georgia. Step 2: attentive concept-to-entity converting. To reduce false-negative while ensuring the high quality of the negative triples, the corrupted entities belonging to the candidate concepts are sampled from the following distribution:\nw(h′j , r, t) = 1− p((h′j , r, t)|{(hi, ri, ti)})\n= 1− expαE(h′j , r, t)∑ i expαE(h ′ i, r, t)\n(6)\nw(h, r, t′j) = p((h, r, t ′ j)|{(hi, ri, ti)})\n= expαE(h, r, t′j)∑ i expαE(h, r, t ′ i)\n(7)\nwhere h′i and t ′ i are the corrupted head and tail entities obtained by non-unique sampling and uniqueness sampling. w and p denote the weight and the probability of the negative triple, respectively. α is the temperature of sampling motivated by the selfadversarial sampling (Sun et al., 2019). Remarkably, considering that a triple with a higher probability is more likely to be a positive one, the weight of a negative triple containing the corrupted head entity such is defined as Eq. 6 to prevent the issue of false-negative. Besides, the negative triples containing the corrupted tail entities with higher probability are endowed with higher-quality weight since there is no false-negative issue. Thus, both the corrupted head entity greenland and the corrupted tail entity tennessee with the high weights are selected to generate high-quality negative triples."
    }, {
      "heading" : "3.4 Traning the KGE Model",
      "text" : "Based on the negative triples obtained by CANS, we train the KGE model to learn the entity and relation embeddings for enlarging the gap between the scores of the positive and high-quality negative triples. In this work, we employ the following loss function as our optimization objective:\nL =− logσ(γ − E(h, r, t))\n− n∑ i 0.5[w(h′i, r, t)logσ(E(h ′ i, r, t)− γ)\n+ w(h, r, t′i)logσ(E(h, r, t ′ i)− γ)]\n(8)\nin which γ is the margin. [x]+ indicates the larger value between 0 and x. σ is the sigmoid function."
    }, {
      "heading" : "3.5 Multi-View Link Prediction",
      "text" : "Benefiting from the same relations among commonsense and facts, commonsense could directly provide a definite range for link prediction results. Hence we develop a novel multi-view link prediction (MVLK) mechanism in a coarse-to-fine paradigm to facilitate more likely predicted results. Firstly, at the coarse prediction stage, we pick out the candidate entities in the view of commonsense. Specifically, take a query (h, r, ?) for an example, commonsense C1 is employed for filtering the reasonable concepts of the tail entity. The candidate concept set of tail entity is defined as\nC1t = {cti|(chi, r, cti) ∈ C1} (9)\nwhere chi is the i-th concept of h, and cti denotes the tail concept in the commonsense (chi, r, cti). Then, the entities belonging to the concept set C1t can be determined as the candidate entities since they satisfy commonsense and are more likely to be the correct tail entities from the perspective of commonsense compared with other entities.\nThen, at the fine prediction stage, we score each candidate entity ei derived from the coarse prediction stage in the view of fact as following\nscore(ei) = E(h, r, ei) (10)\nwhere E(h, r, ei) denotes the score function employed for training the KGE model. Subsequently, the prediction results will rank the scores of candidate entities in ascending order and output the entities with higher ranks."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "In this section, we perform extensive experiments of KGC on four widely-used KG datasets containing concepts. We firstly describe datasets, baseline models, implementation details and evaluation protocol. Then, the effectiveness of our proposed framework CAKE and each module is demonstrated by compared with several baselines. Furthermore, we conduct further experiments, including the ablation study and the case study."
    }, {
      "heading" : "4.1 Experiment Settings",
      "text" : "Datasets. Four real-world datasets containing ontological concepts are utilized for experiments, including FB15K (Bordes et al., 2013), FB15K237 (Toutanova and Chen, 2015), NELL995 (Xiong et al., 2017) and DBpedia-242. Particularly, DBpedia-242 is extracted from DBpedia (Lehmann et al., 2015) which contains totally\n242 concepts. The statistics of the datasets are summarized in Table 1. Notably, the entities in FB15K and FB15K237 always belong to more than one concept while each entity in NELL-995 and DBpedia-242 has only one concept. Baselines. We compare our CAKE model with three state-of-the-art KGE models, including TransE (Bordes et al., 2013), RotatE (Sun et al., 2019) and HAKE (Zhang et al., 2020). Meanwhile, these baselines are also the basic models integrated with our framework. It is unnecessary to use many baselines since the focus of this work is to observe the impact of applying our CAKE framework to original KGE models instead of defeating all the SOTA models. We provide the results of baselines by running their source codes with the suggested parameters. Note that all the existing type-based and ontology-based models are not chosen as baselines since they are specific to a few KGs and cannot work on most of the datasets in our experiment. Implementation Details. Each complex relation is labelled in the same way as in TransH (Wang et al., 2014). We use Adam optimizer for the training and tune the hyper-parameters of our model by grid search on the validation sets. Specifically, the embedding size and the batch size are the same as those of each basic model for a fair comparison. The learning rate is chosen from 0.0001 to 0.01. The margin is tuned in {9, 12, 18, 24, 30}. The sampling temperature is adjusted in {0.5, 1.0}. The entity and relation embeddings are initialized randomly. All the experiments are conducted in Pytorch and on GeForce GTX 2080Ti GPUs. Evaluation Protocol. Following the procedure of MVLP in Section 3.5, we can obtain the rank of the correct entity for each test example. Then, the performance of link prediction is evaluated by three commonly-used metrics: mean rank (MR), mean reciprocal rank (MRR), and proportion of the correct entities ranked in the top n (Hits@N). All the metrics are in the filtered setting by wiping out the candidate triples already exist in the datasets."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "Table 2 exhibits the evaluation results of link prediction on the four datasets. We can observe that both CANS and MVLP modules effectively improve the performance of each basic model on each dataset. Moreover, the entire CAKE framework further facilitates more performance gains than each separate module and outperforms all the baselines consistently and significantly. Compared with the performance average of the three baseline models, our CAKE model improves MRR by 7.0%, 11.5%, 16.2% and 16.7% on FB15K, FB15K237, DBpedia242 and NELL-995. These results demonstrate the superiority and effectiveness of integrating commonsense with the original KGE models.\nWe compare our CANS module with various\ntypes of NS techniques, including uniform sampling (Bordes et al., 2013), none sampling (Li et al., 2021), NSCaching (Zhang et al., 2019b), domainbased sampling (Y. et al., 2019) and self-adversarial sampling (Sun et al., 2019). The comparison results are obtained by combining these NS techniques with the most classical KGE model TransE(Bordes et al., 2013). From the results shown in Table 3, our CANS module significantly outperforms all the other NS techniques on all the datasets. Specifically, domain-based NS, self-adversarial sampling and our CANS module consistently outperform the others due to the consideration of the quality of negative triples. Furthermore, our CANS module performs better than domain-based NS and selfadversarial sampling since CANS could reduce false-negative. These results illustrate the superior ability of our CANS module to generate more high-quality negative triples for enhancing the performance of any KGE model."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "We verify the effectiveness of each contribution via integrating the whole framework CAKE and\nthe following ablated models into the basic model HAKE: (1) neglecting the characteristics of complex relations in CANS (-CRNS), (2) removing the commonsense in CANS while retaining the characteristics of complex relations (-CSNS), and (3) omitting the commonsense-view prediction from MVLP (-MVLP). The results in Table 4 demonstrate that our whole model CAKE performs better than all the ablated models on each dataset. It illustrates that introducing commonsense and the characteristics of complex relations both make sense in the NS process for generating more effective negative triples. Besides, MVLP facilitates link prediction performance benefited from determining the reasonable candidate entities by prior commonsense. In general, each contribution plays a pivotal role in our approach."
    }, {
      "heading" : "4.4 Case Study",
      "text" : "We provide the case study of explainable link prediction with commonsense as shown in Figure 4. Given a query with the tail entity missing (rockets, teamplaysinleague, ?) on NELL995, our model could output the answer entities and provide the corresponding entity concepts together with the commonsense specific to the query. We can observe that all the top5 entities including the correct entity nba belong to the concept sportsleague which satisfies the commonsense (rockets, teamplaysinleague, sportsleague). More interestingly, the commonsense and the entity concepts could explain the rationality of the predicted answer entities to enhance the users’ credibility of the answers."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a novel and scalable commonsense-aware knowledge embedding framework, which could automatically generate commonsense from KGs with entity concepts for the KGC task. We exploit the generated commonsense to produce effective and high-quality negative triples. On the other hand, we design a multi-view link prediction technique in a coarse-to-fine paradigm to filter the candidate entities in the view of commonsense and output the predicted results from the perspective of fact. The experiments on four datasets demonstrate the effectiveness and the scalability of our proposed framework and each module compared with the state-of-the-art baselines. Furthermore, our framework could explain link prediction results and potentially assemble new KGE models to improve their performance."
    } ],
    "references" : [ {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Georg Gottlob", "Sergio Flesca." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247–1250.",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Processing of the 27th Annual Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "KBGAN: Adversarial learning for knowledge graph embeddings",
      "author" : [ "Liwei Cai", "William Yang Wang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Cai and Wang.,? 2018",
      "shortCiteRegEx" : "Cai and Wang.",
      "year" : 2018
    }, {
      "title" : "Type-constrained representation learning in knowledge graphs",
      "author" : [ "KrompaßDenis", "Stephan Baier", "Volker Tresp." ],
      "venue" : "Proceedings of the 14th International Conference on The Semantic Web - ISWC 2015 - Volume 9366, page 640–655.",
      "citeRegEx" : "KrompaßDenis et al\\.,? 2015",
      "shortCiteRegEx" : "KrompaßDenis et al\\.",
      "year" : 2015
    }, {
      "title" : "Knowledge vault: A web-scale approach to probabilistic knowledge fusion",
      "author" : [ "Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang." ],
      "venue" : "Proceedings of the 20th ACM",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast rule mining in ontological knowledge bases with amie+",
      "author" : [ "Luis Galárraga", "Christina Teflioudi", "Katja Hose", "Fabian Suchanek." ],
      "venue" : "The VLDB Journal, 24(6):707–730.",
      "citeRegEx" : "Galárraga et al\\.,? 2015",
      "shortCiteRegEx" : "Galárraga et al\\.",
      "year" : 2015
    }, {
      "title" : "Universal representation learning of knowledge bases by jointly embedding instances and ontological concepts",
      "author" : [ "Junheng Hao", "Muhao Chen", "Wenchao Yu", "Yizhou Sun", "Wei Wang." ],
      "venue" : "Proceedings of the 25th ACM SIGKDD International Conference",
      "citeRegEx" : "Hao et al\\.,? 2019",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft concept graph: Mining semantic concepts for short text understanding",
      "author" : [ "L. Ji", "Y. Wang", "B. Shi", "D. Zhang", "Z. Wang", "J. Yan." ],
      "venue" : "Data Intelligence, 1:262–294.",
      "citeRegEx" : "Ji et al\\.,? 2019",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2019
    }, {
      "title" : "Random walk inference and learning in a large scale knowledge base",
      "author" : [ "Ni Lao", "Tom Mitchell", "William W. Cohen." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529–539.",
      "citeRegEx" : "Lao et al\\.,? 2011",
      "shortCiteRegEx" : "Lao et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient non-sampling knowledge graph embedding",
      "author" : [ "Zelong Li", "Jianchao Ji", "Zuohui Fu", "Yingqiang Ge", "Shuyuan Xu", "Chong Chen", "Yongfeng Zhang." ],
      "venue" : "Proceedings of the Web Conference 2021, page 1727–1736.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-hop knowledge graph reasoning with reward shaping",
      "author" : [ "Xi Victoria Lin", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3243–3253.",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "Path ranking with attention to type hierarchies",
      "author" : [ "Weiyu Liu", "Angel Daruna", "Zsolt Kira", "Sonia Chernova." ],
      "venue" : "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 2893– 2900.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Anytime bottom-up rule learning for knowledge graph completion",
      "author" : [ "Christian Meilicke", "Melisachew Wudage Chekol", "Daniel Ruffinelli", "Heiner Stuckenschmidt." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelli-",
      "citeRegEx" : "Meilicke et al\\.,? 2019",
      "shortCiteRegEx" : "Meilicke et al\\.",
      "year" : 2019
    }, {
      "title" : "A three-way model for collective learning on multi-relational data",
      "author" : [ "Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel." ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning, page 809–816.",
      "citeRegEx" : "Nickel et al\\.,? 2011",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2011
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "Drum: End-to-end differentiable rule mining on knowledge graphs",
      "author" : [ "Ali Sadeghian", "Mohammadreza Armandpour", "Patrick Ding", "Daisy Zhe Wang." ],
      "venue" : "Proceedings of the Thirty-third Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Sadeghian et al\\.,? 2019",
      "shortCiteRegEx" : "Sadeghian et al\\.",
      "year" : 2019
    }, {
      "title" : "ATOMIC: an atlas of machine commonsense for if-then reasoning",
      "author" : [ "Maarten Sap", "Ronan Le Bras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "The Thirty-Third AAAI Con-",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving multi-hop question answering over knowledge graphs using knowledge base embeddings",
      "author" : [ "Apoorv Saxena", "Aditay Tripathi", "Partha Talukdar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4498–",
      "citeRegEx" : "Saxena et al\\.,? 2020",
      "shortCiteRegEx" : "Saxena et al\\.",
      "year" : 2020
    }, {
      "title" : "2017. ConceptNet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of the ThirtyFirst AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "SPARQA: skeleton-based semantic parsing for complex questions over knowledge bases",
      "author" : [ "Yawei Sun", "Lingling Zhang", "Gong Cheng", "Yuzhong Qu." ],
      "venue" : "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 8952–8959.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "RotatE: Knowledge graph embedding by relational rotation in complex space",
      "author" : [ "Zhiqing Sun", "Zhi-Hong Deng", "Jian-Yun Nie", "Jian Tang." ],
      "venue" : "Proceedings of International Conference on Learning Representations.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Observed versus latent features for knowledge base and text inference",
      "author" : [ "Kristina Toutanova", "Danqi Chen." ],
      "venue" : "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pages 57–66.",
      "citeRegEx" : "Toutanova and Chen.,? 2015",
      "shortCiteRegEx" : "Toutanova and Chen.",
      "year" : 2015
    }, {
      "title" : "Complex embeddings for simple link prediction",
      "author" : [ "Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard." ],
      "venue" : "Proceedings of the 33rd International Conference on Machine Learning, page 2071–2080.",
      "citeRegEx" : "Trouillon et al\\.,? 2016",
      "shortCiteRegEx" : "Trouillon et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge-aware graph neural networks with label smoothness regularization for recommender systems",
      "author" : [ "Hongwei Wang", "Fuzheng Zhang", "Mengdi Zhang", "Jure Leskovec", "Miao Zhao", "Wenjie Li", "Zhongyuan Wang." ],
      "venue" : "Proceedings of the 25th ACM",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning intents behind interactions with knowledge graph for recommendation",
      "author" : [ "Xiang Wang", "Tinglin Huang", "Dingxian Wang", "Yancheng Yuan", "Zhenguang Liu", "Xiangnan He", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the Web Conference, pages 878–887.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Knowledge graph embedding by translating on hyperplanes",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, page 1112–1119.",
      "citeRegEx" : "Wang et al\\.,? 2014",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Representation learning of knowledge graphs with entity descriptions",
      "author" : [ "Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, page 2659–2665.",
      "citeRegEx" : "Xie et al\\.,? 2016a",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2016
    }, {
      "title" : "Image-embodied knowledge representation learning",
      "author" : [ "Ruobing Xie", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pages 3140–3146.",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "Representation learning of knowledge graphs with hierarchical types",
      "author" : [ "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, page 2965–2971.",
      "citeRegEx" : "Xie et al\\.,? 2016b",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2016
    }, {
      "title" : "DeepPath: A reinforcement learning method for knowledge graph reasoning",
      "author" : [ "Wenhan Xiong", "Thien Hoang", "William Yang Wang" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Xiong et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "Leveraging lexical semantic information for learning conceptbased multiple embedding representations for knowledge graph completion",
      "author" : [ "Y. Wang", "Y. Liu", "H. Zhang", "H. Xie" ],
      "venue" : "APWeb-WAIM, pages 382–397.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "Bishan Yang", "Wen tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng." ],
      "venue" : "Proceedings of International Conference on Learning Representations.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "GraphDialog: Integrating graph knowledge into endto-end task-oriented dialogue systems",
      "author" : [ "Shiquan Yang", "Rui Zhang", "Sarah Erfani." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Quaternion knowledge graph embeddings",
      "author" : [ "Shuai Zhang", "Yi Tay", "Lina Yao", "Qi Liu." ],
      "venue" : "Proceedings of the 33rd Conference on Neural Information Processing Systems, pages 2731–2741.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Nscaching: Simple and efficient negative sampling for knowledge graph embedding",
      "author" : [ "Yongqi Zhang", "Quanming Yao", "Yingxia Shao", "Lei Chen." ],
      "venue" : "2019 IEEE 35th International Conference on Data Engineering, pages 614–625.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning hierarchy-aware knowledge graph embeddings for link prediction",
      "author" : [ "Zhanqiu Zhang", "Jianyu Cai", "Yongdong Zhang", "Jie Wang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, pages 3065–3072.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Commonsense knowledge aware conversation generation with graph attention",
      "author" : [ "Hao Zhou", "Tom Young", "Minlie Huang", "Haizhou Zhao", "Xiaoyan Zhu." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, pages",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In recent years, knowledge graphs (KGs) such as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : ", 2018) have been widely used in many knowledge-intensive applications, including question answering (Sun et al., 2020; Saxena et al., 2020), dialogue systems (Yang et al.",
      "startOffset" : 101,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : ", 2018) have been widely used in many knowledge-intensive applications, including question answering (Sun et al., 2020; Saxena et al., 2020), dialogue systems (Yang et al.",
      "startOffset" : 101,
      "endOffset" : 140
    }, {
      "referenceID" : 32,
      "context" : ", 2020), dialogue systems (Yang et al., 2020; Zhou et al., 2018) and recommender systems (Wang et al.",
      "startOffset" : 26,
      "endOffset" : 64
    }, {
      "referenceID" : 36,
      "context" : ", 2020), dialogue systems (Yang et al., 2020; Zhou et al., 2018) and recommender systems (Wang et al.",
      "startOffset" : 26,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "The previous KGC models can be classified into three main streams: (1) Rule learning-based models mine logic rules for induction reasoning, such as AMIE+ (Galárraga et al., 2015), (Los Angeles, LocatedIn, California) Positive triple:",
      "startOffset" : 154,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "(2) Path-based models (Liu et al., 2020; Xiong et al., 2017; Lin et al., 2018) search paths for multi-hop reasoning.",
      "startOffset" : 22,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "(2) Path-based models (Liu et al., 2020; Xiong et al., 2017; Lin et al., 2018) search paths for multi-hop reasoning.",
      "startOffset" : 22,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "(2) Path-based models (Liu et al., 2020; Xiong et al., 2017; Lin et al., 2018) search paths for multi-hop reasoning.",
      "startOffset" : 22,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "(3) KGE models such as TransE (Bordes et al., 2013) and its variants (Sun et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 20,
      "context" : ", 2013) and its variants (Sun et al., 2019; Zhang et al., 2019a, 2020) learn the embeddings of entities and relations to score the plausibility of triples for link prediction.",
      "startOffset" : 25,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "Learning KG embeddings relies on a basic procedure of negative sampling (Li et al., 2021).",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "backs: (1) Invalid negative sampling: all the previous NS (Wang et al., 2014; Cai and Wang, 2018; Sun et al., 2019; Zhang et al., 2019b; Denis et al., 2015) cannot avoid sampling the falsenegative triples and low-quality negative triples, simultaneously.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "backs: (1) Invalid negative sampling: all the previous NS (Wang et al., 2014; Cai and Wang, 2018; Sun et al., 2019; Zhang et al., 2019b; Denis et al., 2015) cannot avoid sampling the falsenegative triples and low-quality negative triples, simultaneously.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 20,
      "context" : "backs: (1) Invalid negative sampling: all the previous NS (Wang et al., 2014; Cai and Wang, 2018; Sun et al., 2019; Zhang et al., 2019b; Denis et al., 2015) cannot avoid sampling the falsenegative triples and low-quality negative triples, simultaneously.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 34,
      "context" : "backs: (1) Invalid negative sampling: all the previous NS (Wang et al., 2014; Cai and Wang, 2018; Sun et al., 2019; Zhang et al., 2019b; Denis et al., 2015) cannot avoid sampling the falsenegative triples and low-quality negative triples, simultaneously.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 28,
      "context" : "Last but not least, although some KGE approaches exploit external information, including entity types (Xie et al., 2016b), textual descriptions (Xie et al.",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : ", 2016b), textual descriptions (Xie et al., 2016a) and images of entities (Xie et al.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "However, the valuable commonsense is always acquired by the expensive hand annotation (Rajani et al., 2019), so its high cost leads to relatively low coverage.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "Besides, the existing large-scale commonsense KGs such as ConceptNet (Speer et al., 2017) only contain the concepts without the links to the corresponding entities, causing them unavailable to the KGC task.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "The existing KGC models can be classified into three main categories: (1) Rule learning-based algorithms such as AMIE+ (Galárraga et al., 2015), DRUM (Sadeghian et al.",
      "startOffset" : 119,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : ", 2015), DRUM (Sadeghian et al., 2019) and AnyBurl (Meilicke et al.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : ", 2019) and AnyBurl (Meilicke et al., 2019) automatically mine logic rules from KGs and apply these rules for inductive link prediction.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "(2) Path-based models search paths linking head and tail entities, including path ranking approaches (Lao et al., 2011; Liu et al., 2020) and reinforcement learning-based models (Xiong et al.",
      "startOffset" : 101,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "(2) Path-based models search paths linking head and tail entities, including path ranking approaches (Lao et al., 2011; Liu et al., 2020) and reinforcement learning-based models (Xiong et al.",
      "startOffset" : 101,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : ", 2020) and reinforcement learning-based models (Xiong et al., 2017; Lin et al., 2018).",
      "startOffset" : 48,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : ", 2020) and reinforcement learning-based models (Xiong et al., 2017; Lin et al., 2018).",
      "startOffset" : 48,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "(3) KG embedding (KGE) models such as TransE (Bordes et al., 2013), RESCAL (Nickel et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : ", 2013), RESCAL (Nickel et al., 2011), ComplEx (Trouillon et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : ", 2011), ComplEx (Trouillon et al., 2016), RotatE (Sun et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : ", 2016), RotatE (Sun et al., 2019) and HAKE (Zhang et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 35,
      "context" : ", 2019) and HAKE (Zhang et al., 2020) learn the embeddings of entities and relations to score the plausibility of triples for predicting the missing triples efficiently.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "More specifically, the KGE models generally need a primary negative sampling (NS) procedure to randomly or purposely sample some triples that are not observed in the KG as negative triples for training (Li et al., 2021).",
      "startOffset" : 202,
      "endOffset" : 219
    }, {
      "referenceID" : 4,
      "context" : "Following the local closed-world assumption (Dong et al., 2014), the existing NS techniques for KGE can be classified into five categories: (1) Randomly and uniformly sampling: the majority of the KGE models generate negative triples via randomly replacing an entity or relation in a positive triple from a uniform distribution (Wang et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : ", 2014), the existing NS techniques for KGE can be classified into five categories: (1) Randomly and uniformly sampling: the majority of the KGE models generate negative triples via randomly replacing an entity or relation in a positive triple from a uniform distribution (Wang et al., 2014).",
      "startOffset" : 272,
      "endOffset" : 291
    }, {
      "referenceID" : 2,
      "context" : "(2) Adversarial-based sampling: KBGAN (Cai and Wang, 2018) integrates the KGE model with softmax probabilities to select the high-quality negative triples in an adversarial training framework.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "Selfadversarial sampling (Sun et al., 2019) performs similar to KBGAN, but it utilizes a self-scoring function without a generator.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 34,
      "context" : "(4) Efficient sampling: NSCaching (Zhang et al., 2019b) employs cache containing candidates of negative triples to improve the efficiency of sampling.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "(5) None-sampling: NS-KGE (Li et al., 2021) eliminates the NS procedure by converting loss functions of KGE into a unified square loss.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 18,
      "context" : "In recent years, many researches attempt to construct general commonsense graphs such as ConceptNet (Speer et al., 2017), Microsoft Concept Graph (Ji et al.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : ", 2017), Microsoft Concept Graph (Ji et al., 2019) and ATOMIC (Sap et al.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "On the other hand, although some KGE models such as JOIE (Hao et al., 2019) employ the ontology built-in most of the KGs, i.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "Commonsense has gained widespread attraction from its successful use in understanding high-level semantics, which is generally represented as the concepts with their ontological relations in some well-known commonsense graphs such as ConceptNet (Speer et al., 2017) and Microsoft Concept Graph (Ji et al.",
      "startOffset" : 245,
      "endOffset" : 265
    }, {
      "referenceID" : 7,
      "context" : ", 2017) and Microsoft Concept Graph (Ji et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "More specifically, the three most typical score function patterns are given as follows: (1) The translation-based score function, such as TransE (Bordes et al., 2013):",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "(2) The rotation-based score function, such as RotatE (Sun et al., 2019):",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "(3) The tensor decomposition-based score function, such as DistMult (Yang et al., 2015):",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 25,
      "context" : "To reduce the false-negative triples, we exploit the characteristics of complex relations, namely 1-1, 1-N, N-1, and N-N defined in TransH (Wang et al., 2014) for negative sampling, where 1 implies that the entity is unique when given the relation and another entity, on the contrary, N denotes that there might be multiple entities in this case (non-unique entity).",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 20,
      "context" : "α is the temperature of sampling motivated by the selfadversarial sampling (Sun et al., 2019).",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "Four real-world datasets containing ontological concepts are utilized for experiments, including FB15K (Bordes et al., 2013), FB15K237 (Toutanova and Chen, 2015), NELL995 (Xiong et al.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : ", 2013), FB15K237 (Toutanova and Chen, 2015), NELL995 (Xiong et al.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 29,
      "context" : ", 2013), FB15K237 (Toutanova and Chen, 2015), NELL995 (Xiong et al., 2017) and DBpedia-242.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "We compare our CAKE model with three state-of-the-art KGE models, including TransE (Bordes et al., 2013), RotatE (Sun et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : ", 2013), RotatE (Sun et al., 2019) and HAKE (Zhang et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "Each complex relation is labelled in the same way as in TransH (Wang et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "We compare our CANS module with various types of NS techniques, including uniform sampling (Bordes et al., 2013), none sampling (Li et al.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : ", 2013), none sampling (Li et al., 2021), NSCaching (Zhang et al.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 34,
      "context" : ", 2021), NSCaching (Zhang et al., 2019b), domainbased sampling (Y.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : ", 2019) and self-adversarial sampling (Sun et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "The comparison results are obtained by combining these NS techniques with the most classical KGE model TransE(Bordes et al., 2013).",
      "startOffset" : 109,
      "endOffset" : 130
    } ],
    "year" : 0,
    "abstractText" : "Knowledge graphs store a large number of factual triples while they are still incomplete, inevitably. The previous knowledge graph completion (KGC) models predict missing links between entities merely relying on fact-view data, ignoring the valuable commonsense knowledge. The previous knowledge graph embedding (KGE) techniques suffer from invalid negative sampling and the uncertainty of fact-view link prediction, limiting KGC’s performance. To address the above challenges, we propose a novel and scalable Commonsense-Aware Knowledge Embedding (CAKE) framework to automatically extract commonsense from factual triples with entity concepts. The generated commonsense augments effective selfsupervision to facilitate both high-quality negative sampling (NS) and joint commonsense and fact-view link prediction. Experimental results on the KGC task demonstrate that assembling our framework could enhance the performance of the original KGE models, and the proposed commonsense-aware NS module is superior to other NS techniques. Besides, our proposed framework could be easily adaptive to various KGE models and explain the predicted results.",
    "creator" : null
  }
}
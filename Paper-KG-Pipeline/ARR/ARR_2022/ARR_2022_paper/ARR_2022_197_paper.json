{
  "name" : "ARR_2022_197_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Grapheme-to-Phoneme Conversion for Thai using Neural Regression Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Grapheme-to-phoneme conversion (G2P) is the task of converting grapheme sequences into corresponding phoneme sequences. Many languages have the difficulty that some grapheme sequences correspond to more than one different phoneme sequence depending on the context.\nG2P plays a key role in speech and text processing systems, especially in text-to-speech (TTS) systems. These systems have to produce speech sounds for every word or phrase, even those not contained in a dictionary. In low-resource languages, it is fundamentally difficult to obtain large vocabulary dictionaries with pronunciations. Therefore, pronunciations need to be predicted from character sequences.\nIn many languages, each word is composed of syllables and each syllable is composed of characters following the orthography rules of that language.\nThis means that G2P can be formulated as the task of selecting the best path in a lattice generated for a given input word or phrase if we prepare enough orthography rules to make sure that any lattice generated almost certainly includes the path for the correct pronunciation.\nAs the result of some effort, we prepared Thai orthography rules. Almost all possible paths in a lattice can be generated from these, and each path needs to be evaluated using a phonological language model to select the best path. With this in mind, we propose a novel G2P method based on a neural regression model that is trained using neural networks to predict how similar a pronunciation candidate is to the correct pronunciation. After generating a set of candidates for an input word or phrase using the orthography rules, this model selects the best-similarity pronunciation from the candidates. In the following sections, we describe the proposed method and explain experiments on a dataset of Thai vocabulary entries with pronunciations collected from Wiktionary. After that, we show that the proposed method outperforms encoder-decoder sequence models in terms of the difference between correct and predicted pronunciations, and demonstrate that incorrect, strange output sometimes occurs when using encoder-decoder sequence models while error is within the expected range when using the proposed method. The code is available at https://github.com/T0106661."
    }, {
      "heading" : "2 Related Work",
      "text" : "For G2P, converting words into pronunciations does not require as much expertise as needed to prepare correspondences between graphemes and phonemes. Therefore, a method of learning correspondences from a large number of wordpronunciation pairs has been used (van den Bosch and Daelemans, 1993). To solve the problem that graphemes and phonemes often have many-to-many correspondence, a hidden Markov model-based method (Jiampojamarn et al., 2007) and weighted finite-state transducer-based methods (Novak et al., 2012a,b) have been developed. In addition, there have been some attempts to apply encoder-decoder models to learn end-to-end G2P models. For example, Toshniwal and Livescu (2016) applied a sequence-to-sequence architecture (Sutskever et al., 2014; Luong et al., 2015). Yolchuyeva et al. (2020) applied a transformer architecture (Vaswani et al., 2017) to train a single model that can deal with a large number of languages."
    }, {
      "heading" : "3 Proposed Method",
      "text" : "It is well known that word segmentation (WS) is a necessary preprocessing for languages without word delimiters, such as Chinese, Japanese, and Thai. To solve WS and homograph disambiguation for Thai simultaneously, Tesprasit et al. (2003) enumerated pronunciation candidates for text data with ambiguity and trained a model to select the correct pronunciation from candidates based on the context in which the text data appear.\nIn contrast, the proposed method trains a model that predicts how similar each candidate is to the correct pronunciation. Although the main process of the proposed method is language independent, we use Thai as an example language in this section.\nFirst, we prepared correspondences between graphemes and phonemes by combining Thai characters consisting of 44 consonants, 15 vowels, and several symbols, resulting in an approximately 300- line program and more than 180,000 entries. The prepared entries consisted of 77 characters, 5,772 syllables, and 31 phoneme symbols, and each syllable was composed of up to 7 symbols. Suppose that a dataset of vocabulary entries with pronunciations = {(E8 , ?8) | 8 = 0, . . .} is given. We begin by tracing characters in each vocabulary E8 one at a time to generate a lattice of nodes corresponding to entries. We then enumerate all possible paths in the lattice from one\nend to the other, and obtain a set of pronunciation candidates 8 = {28 9 | 9 = 0, . . .} by joining syllables assigned to nodes. For example, when we have E8 = “กลางคืน”(night), we obtain the set 8 = {/kaĂ£ laaĂ£ NaĂ£ khWWnĂ£/, /klaaĂ£ NaĂ£ khWWnĂ£/, /klaaNĂ£ khWWnĂ£/}.\nAfter that, we calculate the similarity\nB8 9 = B(?8 , 28 9) = 1 − 3 (?8 , 28 9)\nmax( |?8 |, |28 9 |)\nto the correct pronunciation ?8 for each candidate 28 9 , where 3 (·, ·) denotes symbol-based edit distance. This B8 9 takes a maximum of 1 when 28 9 = ?8 and approaches a minimum of 0 as 28 9 diverges from ?8. For the previous example, we obtained ?8 = /klaaNĂ£ khWWnĂ£/; thus B(/klaaNĂ£ khWWnĂ£/, /klaaNĂ£ khWWnĂ£/) = 1, B(/klaaNĂ£ khWWnĂ£/, /klaaĂ£ NaĂ£ khWWnĂ£/) = 13/16, for example, were obtained. Using the similarity defined above, we train a neural regression model that predicts how similar each candidate is to the correct pronunciation. More specifically, we train the model to return the similarity B8 9 from the encoded vectors of E8 and 28 9 using RNNs. Figure 2 shows the model architecture. Vocabulary E is converted into a 3E -dimensional vector by a character embedding layer and a bi-directional GRU (Bi-GRU). Candidate 2 is converted into a 32- dimensional vector by a syllable embedding layer, a phoneme embedding layer, and Bi-GRUs, like the network of Lample et al. (2016). Finally, both vectors are concatenated and converted into similarity B by two dense layers. Each layer dimension can be changed depending on the target language.\nThemodel trained in this way is expected to represent the phonological nature of the target language. Therefore, we can predict the pronunciation ?′ for a given vocabulary E′ as follows. As in the training phase, we trace characters in each vocabulary E′, generate a lattice, and obtain a set of candidates\n′ = {2′ 9 | 9 = 0, . . .}. Next, we calculate the\nsimilarity B′ 9 for each pair (E′, 2′ 9 ) using this model, and find 9max = argmax 9 B′9 . Finally, we output the predicted pronunciation ?′ = 2′\n9max .\nAs can be seen, both training and predicting processes are language independent. In other words, the proposed method can be applied to languages other than Thai simply by preparing enough orthography rules. However, one potential problem with the proposed method is that the number of candidates increases exponentially with input length, which can be undesirable for long words and phrases. This is considered in the next section."
    }, {
      "heading" : "4 Experimental Setup and Results",
      "text" : "We collected 18,066 Thai vocabulary entries with pronunciations fromWiktionary as an experimental dataset. The vocabulary consisted of not only words but also phrases. We then converted the pronunciations described in a Thai-specific way into International Phonetic Alphabet sequences. For entries without pronunciations but with syllable boundaries, we determined their pronunciations when all candidates for each boundary were uniquely generated. For entries where the pronunciations were unable to be determined, two workers fluent in Thai described the correct pronunciations. The average length of each data was 7.42 in characters, 2.47 in syllables, and 12.48 in phoneme symbols. First, we examined the number of candidates generated and the coverage rate of the correct pronunciations. Figure 3 shows the distribution of the number of candidates. As seen in the figure, the number is usually less than 10 and seldom greater than 100. In fact, the minimum, mode, median, mean, and maximum were 1, 2, 4, 19.6, and 19,242 respectively. This means that the average stayed slightly less than 20, although the lattice generated for longer input tends to have many branches and\ncan cause an exponential increase in candidates. For the coverage rate, 17,720 of 18,066 correct pronunciations were included in the sets of candidates and 346 were not.\nNext, we evaluated our method compared with three G2P baseline models available from SIGMORPHON (2020), namely, a pair ngram model (fst) and two encoder-decoder sequence models (encoder-decoder and transfomer, abbreviated as enc-dec and xformer). Table 2 shows the results of 10-fold cross-validation on the test data. For each fold in the experiments, we used 8/10 of entries for training, 1/10 for validation, and 1/10 for testing. We also used accuracy and the difference between correct and predicted pronunciations counted by symbols as evaluation metrics. In other words, accuracy is the percentage of 0-difference entries. As a result, 1.17 × 106 parameters were trained. Each training run was composed of about 40 epochs and each epoch took about 13 minutes on 1 GPU (Titan V, 11GB). The absence of decoders in our model is a possible reason why the number of parameters is small and the training time is short. Table 2 shows that our method achieved high accuracy, small average difference, and small maximum difference, and the accuracy in particular is\ncomparable to those of enc-dec and xformer. In contrast, the low accuracy and large average difference of fst indicate that an ngram model was not able to sufficiently learn the Thai phonological nature, and large maximum differences of enc-dec and xformer indicate the well-known problem of neural network models returning good output when they work, but sometimes making mistakes when they do not. As shown in Table 3, further investigation revealed that the outputs of the encoder-decoder sequence models sometimes included unnatural syllable repetitions and sometimes lacked syllables in the middle, which are undesirable for TTS systems because they might give the impression that the system is failing. In contrast, our method was able to reduce such mistakes because all candidates followed the orthography rules."
    }, {
      "heading" : "4.1 Additional Experiments",
      "text" : "To confirm that the main process of our method is language independent, we performed additional experiments on the Japanese Hiragana dataset available from SIGMORPHON (2021). This dataset consisted of 10,000 entries and the average length of each data was 4.21 in characters, 6.53 in syllables, and 16.43 in phoneme symbols.\nAs the result of some effort, we prepared Japanese Hiragana orthography rules. The prepared entries consisted of 85 characters, 405 syllables, and 45 phoneme symbols, and each syllable was composed of up to 6 symbols.\nTable 4 shows performance comparison with three G2P baseline models. As can be seen, our method also achieved high accuracy and small average difference. However, the maximum differences of enc-dec and xformer are comparable to that of our method. A possible reason why the encoderdecoder sequence models worked well is that the number of long inputs in this dataset was smaller compared with Thai."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this study, we proposed a novel Thai G2P method based on neural regression models. We confirmed that the model trained using neural networks to predict the similarity was able to select the correct pronunciations from candidates. The accuracy was .931 and the difference between correct and predicted pronunciations was .217 on average and 8.7 at maximum.\nThis means that the performance of our proposed method was comparable to that of encoder-decoder sequence models and superior in terms of the difference between correct and predicted pronunciations. In particular, error is within the expected range when using our proposed method. Use of neural regression models not only for G2P but also for summarization and generation opens the possibility that neural network models could reduce strange mistakes.\nOur proposed method has the strength that it can be applied to any language by preparing enough orthography rules. However, it also has the weakness of the number of candidates increasing exponentially with input length, which can be a concern for languages with many exceptional orthography rules, such as English.\nA method for reducing candidates might thus be needed. There may be an efficient solution to find the correct pronunciation using a given candidate and the predicted similarity. However, these studies and experiments are left as future work."
    } ],
    "references" : [ {
      "title" : "Applying many-to-many alignments and hiddenMarkov models to letter-to-phoneme conversion",
      "author" : [ "Sittichai Jiampojamarn", "Grzegorz Kondrak", "Tarek Sherif." ],
      "venue" : "Human Language Technologies 2007: The Conference of the North American Chapter of",
      "citeRegEx" : "Jiampojamarn et al\\.,? 2007",
      "shortCiteRegEx" : "Jiampojamarn et al\\.",
      "year" : 2007
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "ThangLuong", "Hieu Pham", "andChristopherD.Manning" ],
      "venue" : "InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "ThangLuong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "ThangLuong et al\\.",
      "year" : 2015
    }, {
      "title" : "WFST-based grapheme-to-phoneme conversion: Open source tools for alignment, modelbuilding and decoding",
      "author" : [ "Josef R. Novak", "Nobuaki Minematsu", "Keikichi Hirose." ],
      "venue" : "Proceedings of the 10th International Workshop on Finite State Methods",
      "citeRegEx" : "Novak et al\\.,? 2012a",
      "shortCiteRegEx" : "Novak et al\\.",
      "year" : 2012
    }, {
      "title" : "Improving wfst-based G2P conversion with alignment constraints and RNNLM n-best rescoring",
      "author" : [ "Josef R. Novak", "Nobuaki Minematsu", "Keikichi Hirose", "Chiori Hori", "Hideki Kashioka", "Paul R. Dixon." ],
      "venue" : "INTERSPEECH 2012, 13th Annual Conference",
      "citeRegEx" : "Novak et al\\.,? 2012b",
      "shortCiteRegEx" : "Novak et al\\.",
      "year" : 2012
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A context-sensitive homograph disambiguation in Thai text-to-speech synthesis",
      "author" : [ "Virongrong Tesprasit", "Paisarn Charoenpornsawat", "Virach Sornlertlamvanich." ],
      "venue" : "Companion Volume of the Proceedings of HLT-NAACL 2003 - Short Papers, pages 103–105.",
      "citeRegEx" : "Tesprasit et al\\.,? 2003",
      "shortCiteRegEx" : "Tesprasit et al\\.",
      "year" : 2003
    }, {
      "title" : "Jointly learning to align and convert graphemes to phonemes with neural attention models",
      "author" : [ "Shubham Toshniwal", "Karen Livescu." ],
      "venue" : "CoRR, abs/1610.06540.",
      "citeRegEx" : "Toshniwal and Livescu.,? 2016",
      "shortCiteRegEx" : "Toshniwal and Livescu.",
      "year" : 2016
    }, {
      "title" : "Data-oriented methods for grapheme-to-phoneme",
      "author" : [ "Antal van den Bosch", "Walter Daelemans" ],
      "venue" : null,
      "citeRegEx" : "Bosch and Daelemans.,? \\Q1993\\E",
      "shortCiteRegEx" : "Bosch and Daelemans.",
      "year" : 1993
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Transformer based grapheme-tophoneme conversion",
      "author" : [ "Sevinj Yolchuyeva", "Géza Németh", "Bálint GyiresTóth." ],
      "venue" : "CoRR, abs/2004.06338.",
      "citeRegEx" : "Yolchuyeva et al\\.,? 2020",
      "shortCiteRegEx" : "Yolchuyeva et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "correspondence, a hidden Markov model-based method (Jiampojamarn et al., 2007) and weighted finite-state transducer-based methods (Novak et al.",
      "startOffset" : 51,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "For example, Toshniwal and Livescu (2016) applied a sequence-to-sequence architecture (Sutskever et al., 2014; Luong et al., 2015).",
      "startOffset" : 86,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "(2020) applied a transformer architecture (Vaswani et al., 2017) to train a single model that can deal with a large number of languages.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "correspondence, a hidden Markov model-based method (Jiampojamarn et al., 2007) and weighted finite-state transducer-based methods (Novak et al., 2012a,b) have been developed. In addition, there have been some attempts to apply encoder-decoder models to learn end-to-end G2P models. For example, Toshniwal and Livescu (2016) applied a sequence-to-sequence architecture (Sutskever et al.",
      "startOffset" : 52,
      "endOffset" : 324
    }, {
      "referenceID" : 0,
      "context" : "correspondence, a hidden Markov model-based method (Jiampojamarn et al., 2007) and weighted finite-state transducer-based methods (Novak et al., 2012a,b) have been developed. In addition, there have been some attempts to apply encoder-decoder models to learn end-to-end G2P models. For example, Toshniwal and Livescu (2016) applied a sequence-to-sequence architecture (Sutskever et al., 2014; Luong et al., 2015). Yolchuyeva et al. (2020) applied a transformer architecture (Vaswani et al.",
      "startOffset" : 52,
      "endOffset" : 439
    }, {
      "referenceID" : 6,
      "context" : "To solve WS and homograph disambiguation for Thai simultaneously, Tesprasit et al. (2003) enumerated pronunciation candidates for text data with ambiguity and trained a model to select the correct pronunciation from candidates based on the context in which the text data appear.",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "Candidate 2 is converted into a 32dimensional vector by a syllable embedding layer, a phoneme embedding layer, and Bi-GRUs, like the network of Lample et al. (2016). Finally, both vectors are concatenated and converted into similarity B by two dense layers.",
      "startOffset" : 144,
      "endOffset" : 165
    } ],
    "year" : 0,
    "abstractText" : "We propose a novel Thai grapheme-tophoneme conversion method based on a neural regression model that is trained using neural networks to predict the similarity between a candidate and the correct pronunciation. After generating a set of candidates for an input word or phrase using the orthography rules, this model selects the best-similarity pronunciation from the candidates. This method can be applied to languages other than Thai simply by preparing enough orthography rules, and can reduce the mistakes that neural network models often make. We show that the accuracy of the proposed method is .931, which is comparable to that of encoder-decoder sequence models. We also demonstrate that the proposed method is superior in terms of the difference between correct and predicted pronunciations because incorrect, strange output sometimes occurs when using encoder-decoder sequence models but the error is within the expected range when using the proposed method.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_80_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Many Hands Make Light Work: Using Essay Traits to Automatically Score Essays",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "An essay is a piece of text that is written in response to a topic, called a prompt (Mathias and Bhattacharyya, 2020). Qualitative evaluation of the essay consumes a lot of time and resources. Hence, in 1966, Page proposed a method of automatically scoring essays using computers (Page, 1966), giving rise to the domain of Automatic Essay Grading.\nEssay traits are different aspects of the essay that can aid in explaining the score assigned to the essay. Examples of essay traits include content (how much information is present in the essay) (Page, 1966), organization (how well the essay is structured) (Persing et al., 2010), style (how well written the essay is) (Page, 1966), prompt adherence (how much the essay stays on topic for the essay prompt) (Persing and Ng, 2014), etc.\nMost of the research work done in the field of AEG is geared toward scoring the essay holistically,\nrather than studying the importance of essay traits in the overall essay score. In this paper, we ask the question:\n“Can we use information learnt from scoring essay traits to score an essay holistically?” In our paper, not only do we score essays holistically, but we also describe how to score essay traits simultaneously in a multi-task learning framework. Scoring essay traits is essential as it could help in explaining why the essay was scored the way it was, as well as providing valuable insights to the writer about what aspects of the essay were well-written and what the writer needs to improve.\nMulti-task learning is a machine learning technique where we use information from multiple auxiliary tasks to perform a primary task (Caruana, 1997). In our experiments, scoring the individual essay traits is the auxiliary task, and scoring the essay holistically is the primary task. In addition to this, we also study the impact of scoring an essay trait as the primary task while the other traits and overall essay score are auxiliary tasks.\nContributions. In this paper, we describe a way to simultaneously score essay traits and the essay itself using multi-task learning. We evaluate our system against different types of essays and essay traits. We also share our code and the data for reproducibility and further research1."
    }, {
      "heading" : "2 Motivation",
      "text" : "Most of the work done in the area of automatic essay grading is in the area of holistic AEG - where we provide a single score for the entire essay based on its quality. However, for writers of an essay, a holistic score alone would not be enough. Providing trait-specific scores will tell the writer which aspects of the essay need improvement.\n1We have uploaded the code and data as part of the supplementary material. We will put the URLs in the camera-ready version of the paper upon acceptance.\nIn our dataset, we observe that writers of good essays usually have a lot of content, appropriate word choice, very few errors, etc. Essays that are poorly written often lack one or more of these qualities (i.e. they are either too short, have lots of errors, etc.). We, therefore, observe a high correlation between individual trait scores and the overall essay score (Pearson correlation trait scores and overall essay score > 0.7 across all essay sets in our dataset). Hence, we believe that using essay trait scores will benefit in scoring the essay holistically, as their scores will provide more relevant information to the AEG system."
    }, {
      "heading" : "3 Related Work",
      "text" : ""
    }, {
      "heading" : "3.1 Holistic Essay Grading",
      "text" : "Holistic essay grading involves assigning an overall score for an essay (Mathias and Bhattacharyya, 2020). The first AEG system was designed by Page (1966). In the decade of the 2000s there were a lot of AEG systems which were developed commercially (see Shermis and Burstein (2013) for more details).\nAfter the release of Kaggle’s Automatic Student Assessment Prize’s (ASAP) Automatic Essay Grading (AEG) dataset in 20122, there has been a lot of research on holistic essay grading. Initial approaches, such as those of Phandi et al. (2015) and Zesch et al. (2015) used feature engineering techniques and domain adaptation in scoring the essays. More recent papers look at using a number of deep learning approaches, such as LSTMs (Taghipour and Ng, 2016; Tay et al., 2018) and CNNs (Dong and Zhang, 2016) or both (Dong et al., 2017; Zhang and Litman, 2018, 2020). Zhang and Litman (2020) describe a way to extract important information, called topical components, from a source-dependent response3."
    }, {
      "heading" : "3.2 Trait-specific Essay Grading",
      "text" : "In the last decade or so, there has been some work done in scoring essay traits such as sentence fluency (Chae and Nenkova, 2009), organization (Persing et al., 2010; Taghipour, 2017; Mathias et al., 2018; Song et al., 2020), thesis clarity (Persing and Ng, 2013; Ke et al., 2019) coherence (Somasundaran et al., 2014; Mathias et al., 2018), prompt adherence (Persing and Ng, 2014), argument strength\n2https://www.kaggle.com/c/asap-aes 3We define what a source-dependent response is in the\nDataset Section (i.e. Section 5).\n(Persing and Ng, 2015; Taghipour, 2017), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b) and narrative quality (Somasundaran et al., 2018). None of the above work, however, uses trait information to score the essay holistically.\nThere has also been work on scoring multiple essay traits (Taghipour, 2017; Mathias and Bhattacharyya, 2018a, 2020). (Rama and Vajjala, 2021) discuss solutions across multiple languages (German, Czech and Italian). Mathias and Bhattacharyya (2020) describes work on the use of neural networks for scoring essay traits. Our work combines the scores of essay traits for holistic essay grading. We focus on using trait-specific essay grading to improve the performance of an automatic essay grading system. We also show how using multi-task learning - simultaneously scoring both the essay and its traits - we are able to speed up the training of our system without too much of a loss in scoring the essay traits. (Ridley et al., 2021) describe a multi-task learning approach to grade essays and their traits using a neural network. Our system differs from theirs with respect to the shared layers and trait-specific layers. While Ridley et al. (2021) share the embedding and word-specific layers (to get sentence representations), we share only the embedding layer."
    }, {
      "heading" : "3.3 Multi-task Learning",
      "text" : "Multitask Learning was proposed by Caruana (1997) where the argument was that training signals from related tasks could help in a better generalization of the model. Collobert et al. (2011) successfully demonstrated how tasks like Part-of-Speech tagging, chunking and Named Entity Recognition can help each other when trained jointly using deep neural networks. Song et al. (2020) described a multi-task learning approach to score organization in essays, where the auxiliary tasks were classifying the sentences and paragraphs, and the primary task was scoring the essay’s organization. Cao et al. (2020) also use a domain adaptive MTL approach to grade essays, where their auxiliary tasks are sentence reordering, noise identification, as well as domain adversarial training. However, they also use all the other essay sets as part of their training, whereas we use only the essays present in the respective essay set for training."
    }, {
      "heading" : "4 System Architecture",
      "text" : ""
    }, {
      "heading" : "4.1 STL Essay Grading Stack",
      "text" : "For scoring the essays, we use essay grading stacks. Each stack is used for scoring a single essay trait. The architecture of the stack is based on the architecture of the holistic essay grading system proposed by Dong et al. (2017). The essay grading stack takes the essay as input (split into tokens and sentences) and returns the score of the essay / essay trait as the output. Figure 1 shows the architecture for the essay grading stack.\nFor each essay, we first split the essay into tokens and sentences. This is given as an input to the essay grading stack. In the word embedding layer, we look up the word embeddings of each token. Just like Taghipour and Ng (2016), Dong et al. (2017), Tay et al. (2018), Mathias and Bhattacharyya (2020) and Mathias et al. (2020), we use the most frequent 4000 words of the training data as the vocabulary with all other words mapping to a special unknown token. This is done mainly to capture out-of-vocabulary words, as well words that generally don’t belong in the topic4. If the vocabulary size is too small, then a number of words will be marked as spelling errors. On the other hand, if the vocabulary size is too large, a lot of spurious words would also be learnt as important ones.\n4Using the word “relay” when writing about something you like instead of “really”. “Relay” could be a valid word in the context of athletics, but not in the context of an argumentative essay on libraries! These valid words are learnt from the training data.\nThis sequence of word embeddings is then sent to the next layer - the 1 dimension CNN layer - to get local information from nearby words. The output of this layer is aggregated using attention pooling to get the sentence representation of the sentence. This is done for all sentences in the essay.\nEach of the sentence representations are then sent through a recurrent layer. We experiment on two different types of recurrent layers - a unidirectional LSTM (Hochreiter and Schmidhuber, 1997) and bidirectional LSTM (BiLSTM) - as the recurrent layer. The outputs of the recurrent layer are pooled using attention pooling to get the representation for the essay. This essay representation is then sent through a fully-connected Dense layer with a sigmoid activation function to score the essay either holistically or a particular essay trait. For our experiments, we minimize the mean squared error loss.\nPrior to input, we scale the scores to the range of [0, 1] using min-max normalization. The output of the sigmoid function is a scalar in the range of [0, 1] which is rescaled back up to a score in the original score range and rounded off to get the score for the essay. This essay stack is used for the scoring of the single-task learning (STL) models."
    }, {
      "heading" : "4.2 MTL Model",
      "text" : "The architecture of our MTL model for an essay of M traits is shown in Figure 2. Here, the word embedding layer is shared across all the tasks. In the multi-task learning framework, each stack is\nused to learn an essay representation for each essay trait. In a similar manner, the essay representation for the overall score is learnt and it is concatenated with the predicted trait scores before being sent to a Dense layer with a sigmoid activation function to score the essay holistically. For calculating each score - both overall and trait scores - we use the mean squared error loss function. We experimented with multiple weights for the loss function for the essay trait scoring task, but settled on uniform weights for all the traits and the overall scoring task5."
    }, {
      "heading" : "5 Dataset Used",
      "text" : "For our experiments, we use the Automated Student’s Assessment Prize (ASAP) Automatic Essay Grading (AEG) dataset. The dataset has a total of 8 essay sets - where each essay set has a number of essays written in response to the same essay prompt. In total, there are nearly 13,000 English essays in the dataset, written by American high school students from classes 7 to 10.\nTable 1 gives the properties of each of the essay sets in our dataset. It reports the overall essay scoring range, traits scoring, average word count, number of traits, number of essays and essay type.\nWe use the overall scores directly from the ASAP AEG dataset. Since the original dataset only provided trait-specific scores for Prompts 7\n5This is done because we want to get accurate predictions of the traits scores which are used for predicting the overall score.\n& 8, we use the trait-specific scores provided by Mathias and Bhattacharyya (2018a).\nDepending on the type of prompt for the essay set, each essay set has a different set of traits. Argumentative / Persuasive essays are essays which the writer is prompted to take a stand on a topic and argue for their stance. These essay sets have traits like content, organization, word choice, sentence fluency, and conventions. Source-dependent responses (Zhang and Litman, 2018) are essays where the writer reads a piece of text and answers a question based on the text that they just read6. These essay sets have traits like content, prompt adherence (Persing and Ng, 2014), language and narrativity (Somasundaran et al., 2018). Narrative / Descriptive essays are essays where the writer has to narrate a story or incident or anecdote. They have traits like content, organization, style, conventions, voice, word choice, and sentence fluency7. Table 2 lists the different essay traits for each essay set.\n6A sample prompt is “Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt.” It involves the writer reading the excerpt from The Empire State Building by Marcia Amidon Lusted before writing the essay.\n7Neither the original ASAP dataset, nor Mathias and Bhattacharyya (2018a) have scored narrativity for the narrative essays."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Evaluation Metric",
      "text" : "We use Cohen’s Kappa with quadratic weights (Cohen, 1968) (QWK) as the evaluation metric. This is done for the following reasons. Firstly, the final scores predicted by the system are distinct numbers/grades, rather than continuous values; so we cannot use the Pearson Correlation Coefficient or Mean Squared Error. Secondly, evaluation metrics like F-Score and accuracy do not take into account chance agreements. For example, if we are to grade every essay with the mean score or most frequent score, we would get F-Score and accuracy as high as 60% or more, whereas the Kappa score will be 0! Thirdly, the fact that the scores given are ordered (i.e. 0 < 1 < 2 < 3...) means that we need to use weighted Kappa to capture the distance between the actual and predicted scores. Between linear weighted Kappa and QWK, we choose QWK because it rewards matches and punishes mismatches more distinctly than linear weighted Kappa."
    }, {
      "heading" : "6.2 Evaluation Method",
      "text" : "We evaluate our experiments using five-fold cross validation. We use the same data splits as used by Taghipour and Ng (2016). To avoid overfitting, we choose the model which gives the best result on the\nvalidation set for evaluating on the test set, and we report the mean value of all 5 folds."
    }, {
      "heading" : "6.3 Experiment Configuration",
      "text" : "Table 3 gives the different hyperparameters used in our systems. For the sake of uniformity, we use these hyperparameters irrespective of the network configuration (STL vs MTL, or LSTM vs BiLSTM).\nTo evaluate the performance of our systems in scoring the essay overall, we use 4 different configurations - STL-LSTM, STL-BiLSTM, MTLLSTM, and MTL-BiLSTM. In addition to the\nabove systems, we also compare our approach with a state-of-the-art string kernel system designed by Cozma et al. (2018), using the same splits for training, testing, and validation8, as well as a baseline transformer-based implementation (BERT-STL), using the BERT-base-uncased model. We run this baseline model for 100 epochs and a batch size of 30, all other hyperparameters remaining default.\nWe also study the effect of using our system to grade an essay trait as the primary task, and score the other traits and the essay overall as auxiliary tasks (MTL*).\nIn the STL configurations, we train our system to predict a single score at a time- either the overall essay score or the score for any of the essay traits. In the MTL configurations, our system learns to score the essay and all its traits simultaneously. The LSTM configurations use only a forward direction LSTM, while the BiLSTM configurations use a bidirectional (i.e. forward and reverse) LSTM."
    }, {
      "heading" : "7 Results and Analysis",
      "text" : "In this section, we report our results and analyze them for different experiments."
    }, {
      "heading" : "7.1 Performance on Holistic Essay Scoring",
      "text" : "Table 4 gives the QWK scores of each of our systems as they score each essay set holistically. The different systems used are the Single Task Learning (STL) (only scoring the essay overall) and Multitask Learning (MTL) (scoring the essay and the traits simultaneously). The first column lists out the different essay sets (Prompts 1 to 8). The\n8Cozma et al. (2018) do not provide their folds, so we run their system on our training/validation/test split, as given by Taghipour and Ng (2016).\nnext three columns report results for STL using both LSTM and BiLSTM, as well as results using the string kernel-based approach of Cozma et al. (2018). The next two columns report results for the MTL systems using both LSTM and BiLSTM. The last column shows the results using the baseline BERT-STL system.\nFrom the table, we see that the MTL-BiLSTM performs the best of all the non-transformer systems (almost as good as the results of our BERTSTL system). In order to see if the improvements observed are statistically significant, we run the Paired T-Test for each of the essay sets and compare the results using a p-value of p < 0.05."
    }, {
      "heading" : "7.2 Performance on Scoring Essay Traits",
      "text" : "We also look at how our system performs in the auxiliary tasks - namely scoring the different essay traits. Figure 3 gives the results of our experiments in scoring the essay traits, using the String Kernel (HISK) (Cozma et al., 2018), CNN-LSTM (STL) (Dong et al., 2017), and Our Systems (MTL and MTL*). We use the same evaluation method, which we used for scoring essay traits, with the same data splits. For the STL systems, we train them for every essay trait individually. MTL* is the results of using our system to score essay trait as the primary task, and score the other traits and the essay overall as auxiliary tasks.\nWe compare the results with that of our MTLBiLSTM system, which was trained to score the essay traits as auxiliary tasks. Figure 3 gives the results of our experiments. From the figure, we see that, while the STL-LSTM system is able to outperform our MTL-BiLSTM system, the MTL* system (where the traits are primary tasks) outperforms the STL-LSTM system. While the STL system optimizes for scoring only a trait, the MTL* system learns information from other traits to score the given essay trait.\nOne of the reasons for the different trait performance depends on how easy (or difficult) it is to score the individual trait, as well which all essay traits have the particular trait. For example, prompt adherence has a higher average QWK than the other traits because it is present mainly in the source-dependent essays (which have a mean QWK over 1% higher than the mean QWK across all essay sets). Similarly, Voice has the lowest QWK mainly because it is present only in Prompt # 8, which has a very low holistic QWK."
    }, {
      "heading" : "7.3 Scoring Traits as the Primary Task",
      "text" : "An interesting question for analysis is “What if we score the traits as the primary task?” In order to do that, we changed our system to make scoring one of the trait as the primary task, and scoring the rest of the traits as well as the essay overall, as auxiliary tasks. The comparison of these results are shown in Figure 3 (in the MTL* column). We see that our MTL* system outperforms the STL-based system on scoring individual traits, although it would take a lot longer time to train (as it would be equivalent to running the MTL system between 4 to 6 times)."
    }, {
      "heading" : "7.4 Ablation Tests",
      "text" : "In order to know which trait is most important for each essay set, we run a series of ablation tests.\nFor each essay set, we ablate one essay trait at a time before scoring the essay. Table 5 reports the results of the ablation test. The values in the table correspond to the drop in performance in scoring the essay holistically. We find that the Content is the most important essay trait for 3 of the essay sets. Prompt Adherence and Word Choice are the most important traits for 2 of the essay sets where they are scored."
    }, {
      "heading" : "7.5 Error Analysis",
      "text" : "As we have seen, the MTL model generally helps over the STL model when it comes to holistic essay scoring, especially if there is no well-defined rule (Example: Holistic Score = Sum of trait scores) for scoring the essay holistically.\nA possible scenario where STL could help over MTL is if the holistic score is a well-defined function of the trait scores AND the STL system can predict the trait scores with a good deal of accuracy. The essay sets corresponding to Prompts 7 & 8 are two such essay sets, where the overall score is a function of the individual trait scores. To verify this, we ran the experiments in a pipelined manner - first scoring the essay traits, then calculating the holistic score using the predicted trait scores and comparing it with the gold standard holistic scores. We found no difference in QWK for Prompt 7 (a QWK of 0.796 vs. 0.795), but a much lesser performance with Prompt 8 (a QWK of 0.684 vs. 0.699) as compared to our MTL-based system. One of the main reasons for this is due to the poor performance in predicting the trait scores as single tasks."
    }, {
      "heading" : "7.6 Runtime Analysis",
      "text" : "We also ran experiments to see how much resources and time our approaches will take. Table 6 gives the total training time (in hours). The total training time is the total time taken to train our system to score the essay holistically as well as all the traits in that essay set for all 100 epochs. We also report the speed-up when using the MTL approach as compared to the STL approach. From our results, we observe a 2.30 to 3.70 speed-up in using the MTL models as compared to using the STL models. The BERT-STL experiments ran for about 5 days (113 hours).\nWe also report the average number of training parameters per system in Table 7. For the STL systems, the number of trainable parameters is the same irrespective of essay set. For the MTL systems, the models, the number of training parame-\nters varies based on the number of essay traits in the essay set. Prompts 3 to 7, which have only 4 traits, have about 1.38 million training parameters. On the other hand, Prompt 8, which has 6 essay traits, has over 1.85 million training parameters.\nAll our experiments were run on an Nvidia GeForce GTX 1080 Ti Graphics Card with 12GB of GPU memory, using Python version 3.5.2, Keras version 2.2.4 and Tensorflow version 1.14."
    }, {
      "heading" : "7.7 Comparison with Transformer Models",
      "text" : "Most modern NLP systems have started to use attention-based transformer networks and large pretrained language models. Yang et al. (2020), Cao et al. (2020), and Uto et al. (2020) use the BERTbase-uncased (Devlin et al., 2019) pre-trained language model to perofrm automatic essay grading achieving QWKs in the range of 0.79 to 0.805. However, BERT has about 110 million parameters (compared to our largest model with just under 2 million parameters). Another limiting factor with\nusing BERT is the fact that we can only input 512 tokens. This is a problem, especially for Prompt 8, where the average essay length is about 650 words. Mayfield and Black (2020) describe some of the other limitations of using BERT for scoring essays."
    }, {
      "heading" : "8 Conclusion and Future Work",
      "text" : "In this paper, we described an approach to use multi-task learning to automatically score essays and their traits. We achieve this by concatenating a representation of the essay with the trait scores - predicted as an auxiliary task. We compared our results with single-task learning models as well. We found out that the MTL system with the BiDirectional LSTM outperforms the STL-based systems and has results comparable with a baseline BERT-STL system. We then ran an ablation test and found out which essay trait was important for the corresponding essay sets. We also report our system’s performance, which shows a 2.30 to 3.70 speed-up of using the multi-task learning system, compared to using a single task learning system.\nAn exciting avenue of future work is using trait scoring to aid in providing text feedback to the writer, like showing where the low score for the trait originates, similar to Hellman et al. (2020) (for content scoring), rather than a trait-specific score only. We also plan to investigate using ALBERT (Lan et al., 2020), in lieu of the essay stack, to grade essays and their traits simultaneously. We also plan to explore how to extend our approach in generalizing our system, training it on essays written in response to one set of source prompts, and tested it on essays written for another prompt."
    } ],
    "references" : [ {
      "title" : "Domain-adaptive neural automated essay scoring",
      "author" : [ "Yue Cao", "Hanqi Jin", "Xiaojun Wan", "Zhiwei Yu." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1011–1020.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Predicting the fluency of text with shallow structural features: Case studies of machine translation and human-written text",
      "author" : [ "Jieun Chae", "Ani Nenkova." ],
      "venue" : "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages",
      "citeRegEx" : "Chae and Nenkova.,? 2009",
      "shortCiteRegEx" : "Chae and Nenkova.",
      "year" : 2009
    }, {
      "title" : "Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Psychological bulletin, 70(4):213.",
      "citeRegEx" : "Cohen.,? 1968",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1968
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research, 12(Aug):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Automated essay scoring with string kernels and word embeddings",
      "author" : [ "Mădălina Cozma", "Andrei Butnaru", "Radu Tudor Ionescu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
      "citeRegEx" : "Cozma et al\\.,? 2018",
      "shortCiteRegEx" : "Cozma et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic features for essay scoring – an empirical study",
      "author" : [ "Fei Dong", "Yue Zhang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1072–1077, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Dong and Zhang.,? 2016",
      "shortCiteRegEx" : "Dong and Zhang.",
      "year" : 2016
    }, {
      "title" : "Attentionbased recurrent convolutional neural network for automatic essay scoring",
      "author" : [ "Fei Dong", "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 153–162, Vancouver,",
      "citeRegEx" : "Dong et al\\.,? 2017",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2017
    }, {
      "title" : "Multiple instance learning for content feedback localization without annotation",
      "author" : [ "Scott Hellman", "William Murray", "Adam Wiemerslage", "Mark Rosenstein", "Peter Foltz", "Lee Becker", "Marcia Derr." ],
      "venue" : "Proceedings of the Fifteenth Workshop on Innovative",
      "citeRegEx" : "Hellman et al\\.,? 2020",
      "shortCiteRegEx" : "Hellman et al\\.",
      "year" : 2020
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Give me more feedback II: Annotating thesis strength and related attributes in student essays",
      "author" : [ "Zixuan Ke", "Hrishikesh Inamdar", "Hui Lin", "Vincent Ng." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Ke et al\\.,? 2019",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2019
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "ASAP++: Enriching the ASAP automated essay grading dataset with essay attribute scores",
      "author" : [ "Sandeep Mathias", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018),",
      "citeRegEx" : "Mathias and Bhattacharyya.,? 2018a",
      "shortCiteRegEx" : "Mathias and Bhattacharyya.",
      "year" : 2018
    }, {
      "title" : "Thank “goodness”! a way to measure style in student essays",
      "author" : [ "Sandeep Mathias", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 35–41, Melbourne, Australia. As-",
      "citeRegEx" : "Mathias and Bhattacharyya.,? 2018b",
      "shortCiteRegEx" : "Mathias and Bhattacharyya.",
      "year" : 2018
    }, {
      "title" : "Can neural networks automatically score essay traits",
      "author" : [ "Sandeep Mathias", "Pushpak Bhattacharyya" ],
      "venue" : "In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,",
      "citeRegEx" : "Mathias and Bhattacharyya.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mathias and Bhattacharyya.",
      "year" : 2020
    }, {
      "title" : "Eyes are the windows to the soul: Predicting the rating of text quality using gaze behaviour",
      "author" : [ "Sandeep Mathias", "Diptesh Kanojia", "Kevin Patel", "Samarth Agrawal", "Abhijit Mishra", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the 56th Annual Meeting",
      "citeRegEx" : "Mathias et al\\.,? 2018",
      "shortCiteRegEx" : "Mathias et al\\.",
      "year" : 2018
    }, {
      "title" : "Happy are those who grade without seeing: A multitask learning approach to grade essays using gaze behaviour",
      "author" : [ "Sandeep Mathias", "Rudra Murthy", "Diptesh Kanojia", "Abhijit Mishra", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the 1st Conference of the",
      "citeRegEx" : "Mathias et al\\.,? 2020",
      "shortCiteRegEx" : "Mathias et al\\.",
      "year" : 2020
    }, {
      "title" : "Should you fine-tune BERT for automated essay scoring",
      "author" : [ "Elijah Mayfield", "Alan W Black" ],
      "venue" : "In Proceedings of the Fifteenth Workshop on Innovative Use",
      "citeRegEx" : "Mayfield and Black.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mayfield and Black.",
      "year" : 2020
    }, {
      "title" : "The imminence of... grading essays by computer",
      "author" : [ "Ellis B Page" ],
      "venue" : "The Phi Delta Kappan,",
      "citeRegEx" : "Page.,? \\Q1966\\E",
      "shortCiteRegEx" : "Page.",
      "year" : 1966
    }, {
      "title" : "Modeling organization in student essays",
      "author" : [ "Isaac Persing", "Alan Davis", "Vincent Ng." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229–239, Cambridge, MA. Association for Computational Linguis-",
      "citeRegEx" : "Persing et al\\.,? 2010",
      "shortCiteRegEx" : "Persing et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling thesis clarity in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 260– 269, Sofia, Bulgaria. Association for Computational",
      "citeRegEx" : "Persing and Ng.,? 2013",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2013
    }, {
      "title" : "Modeling prompt adherence in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1534–1543, Baltimore, Maryland. Association for",
      "citeRegEx" : "Persing and Ng.,? 2014",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2014
    }, {
      "title" : "Modeling argument strength in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol-",
      "citeRegEx" : "Persing and Ng.,? 2015",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2015
    }, {
      "title" : "Modeling stance in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2174–2184, Berlin, Germany. Association for Computational Lin-",
      "citeRegEx" : "Persing and Ng.,? 2016",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2016
    }, {
      "title" : "Flexible domain adaptation for automated essay scoring using correlated linear regression",
      "author" : [ "Peter Phandi", "Kian Ming A. Chai", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 431–",
      "citeRegEx" : "Phandi et al\\.,? 2015",
      "shortCiteRegEx" : "Phandi et al\\.",
      "year" : 2015
    }, {
      "title" : "Are pretrained text representations useful for multilingual and multi-dimensional language proficiency modeling",
      "author" : [ "Taraka Rama", "Sowmya Vajjala" ],
      "venue" : null,
      "citeRegEx" : "Rama and Vajjala.,? \\Q2021\\E",
      "shortCiteRegEx" : "Rama and Vajjala.",
      "year" : 2021
    }, {
      "title" : "Automated cross-prompt scoring of essay traits",
      "author" : [ "Robert Ridley", "Liang He", "Xin-yu Dai", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13745–13753.",
      "citeRegEx" : "Ridley et al\\.,? 2021",
      "shortCiteRegEx" : "Ridley et al\\.",
      "year" : 2021
    }, {
      "title" : "Handbook of automated essay evaluation: Current applications and new directions",
      "author" : [ "Mark D Shermis", "Jill Burstein." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Shermis and Burstein.,? 2013",
      "shortCiteRegEx" : "Shermis and Burstein.",
      "year" : 2013
    }, {
      "title" : "Lexical chaining for measuring discourse coherence quality in test-taker essays",
      "author" : [ "Swapna Somasundaran", "Jill Burstein", "Martin Chodorow." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Techni-",
      "citeRegEx" : "Somasundaran et al\\.,? 2014",
      "shortCiteRegEx" : "Somasundaran et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards evaluating narrative quality in student writing",
      "author" : [ "Swapna Somasundaran", "Michael Flor", "Martin Chodorow", "Hillary Molloy", "Binod Gyawali", "Laura McCulla." ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Somasundaran et al\\.,? 2018",
      "shortCiteRegEx" : "Somasundaran et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical multi-task learning for organization evaluation of argumentative student essays",
      "author" : [ "Wei Song", "Ziyao Song", "Lizhen Liu", "Ruiji Fu." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Robust trait-specific essay scoring using neural networks and density estimators",
      "author" : [ "Kaveh Taghipour." ],
      "venue" : "Ph.D. thesis, National University of Singapore (Singapore).",
      "citeRegEx" : "Taghipour.,? 2017",
      "shortCiteRegEx" : "Taghipour.",
      "year" : 2017
    }, {
      "title" : "A neural approach to automated essay scoring",
      "author" : [ "Kaveh Taghipour", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1882–1891, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Taghipour and Ng.,? 2016",
      "shortCiteRegEx" : "Taghipour and Ng.",
      "year" : 2016
    }, {
      "title" : "Skipflow: Incorporating neural coherence features for end-to-end automatic text scoring",
      "author" : [ "Yi Tay", "Minh Phan", "Luu Anh Tuan", "Siu Cheung Hui." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), pages 5948–5955.",
      "citeRegEx" : "Tay et al\\.,? 2018",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural automated essay scoring incorporating handcrafted features",
      "author" : [ "Masaki Uto", "Yikuan Xie", "Maomi Ueno." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 6077–6088, Barcelona, Spain (Online). Inter-",
      "citeRegEx" : "Uto et al\\.,? 2020",
      "shortCiteRegEx" : "Uto et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing automated essay scoring performance via fine-tuning pre-trained language models with combination of regression and ranking",
      "author" : [ "Ruosong Yang", "Jiannong Cao", "Zhiyuan Wen", "Youzheng Wu", "Xiaodong He." ],
      "venue" : "Findings of the Association for Computa-",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Task-independent features for automated essay grading",
      "author" : [ "Torsten Zesch", "Michael Wojatzki", "Dirk ScholtenAkoun." ],
      "venue" : "Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 224–232, Denver,",
      "citeRegEx" : "Zesch et al\\.,? 2015",
      "shortCiteRegEx" : "Zesch et al\\.",
      "year" : 2015
    }, {
      "title" : "Co-attention based neural network for source-dependent essay",
      "author" : [ "Haoran Zhang", "Diane Litman" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Litman.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang and Litman.",
      "year" : 2018
    }, {
      "title" : "Automated topical component extraction using neural network attention scores from source-based essay scoring",
      "author" : [ "Haoran Zhang", "Diane Litman." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8569–",
      "citeRegEx" : "Zhang and Litman.,? 2020",
      "shortCiteRegEx" : "Zhang and Litman.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "An essay is a piece of text that is written in response to a topic, called a prompt (Mathias and Bhattacharyya, 2020).",
      "startOffset" : 84,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "Hence, in 1966, Page proposed a method of automatically scoring essays using computers (Page, 1966), giving rise to the domain of Automatic Essay Grading.",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Examples of essay traits include content (how much information is present in the essay) (Page, 1966), organization (how well the essay is structured) (Persing et al.",
      "startOffset" : 88,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "Examples of essay traits include content (how much information is present in the essay) (Page, 1966), organization (how well the essay is structured) (Persing et al., 2010), style (how well written the essay is) (Page, 1966), prompt adherence (how much the essay stays on topic for the essay prompt) (Persing and Ng, 2014), etc.",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : ", 2010), style (how well written the essay is) (Page, 1966), prompt adherence (how much the essay stays on topic for the essay prompt) (Persing and Ng, 2014), etc.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : ", 2010), style (how well written the essay is) (Page, 1966), prompt adherence (how much the essay stays on topic for the essay prompt) (Persing and Ng, 2014), etc.",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 15,
      "context" : "Holistic essay grading involves assigning an overall score for an essay (Mathias and Bhattacharyya, 2020).",
      "startOffset" : 72,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "More recent papers look at using a number of deep learning approaches, such as LSTMs (Taghipour and Ng, 2016; Tay et al., 2018) and CNNs (Dong and Zhang, 2016) or both (Dong et al.",
      "startOffset" : 85,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : "More recent papers look at using a number of deep learning approaches, such as LSTMs (Taghipour and Ng, 2016; Tay et al., 2018) and CNNs (Dong and Zhang, 2016) or both (Dong et al.",
      "startOffset" : 85,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : ", 2018) and CNNs (Dong and Zhang, 2016) or both (Dong et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : ", 2018) and CNNs (Dong and Zhang, 2016) or both (Dong et al., 2017; Zhang and Litman, 2018, 2020).",
      "startOffset" : 48,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "In the last decade or so, there has been some work done in scoring essay traits such as sentence fluency (Chae and Nenkova, 2009), organization (Persing et al.",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : "In the last decade or so, there has been some work done in scoring essay traits such as sentence fluency (Chae and Nenkova, 2009), organization (Persing et al., 2010; Taghipour, 2017; Mathias et al., 2018; Song et al., 2020), thesis clarity (Persing and Ng, 2013; Ke et al.",
      "startOffset" : 144,
      "endOffset" : 224
    }, {
      "referenceID" : 32,
      "context" : "In the last decade or so, there has been some work done in scoring essay traits such as sentence fluency (Chae and Nenkova, 2009), organization (Persing et al., 2010; Taghipour, 2017; Mathias et al., 2018; Song et al., 2020), thesis clarity (Persing and Ng, 2013; Ke et al.",
      "startOffset" : 144,
      "endOffset" : 224
    }, {
      "referenceID" : 16,
      "context" : "In the last decade or so, there has been some work done in scoring essay traits such as sentence fluency (Chae and Nenkova, 2009), organization (Persing et al., 2010; Taghipour, 2017; Mathias et al., 2018; Song et al., 2020), thesis clarity (Persing and Ng, 2013; Ke et al.",
      "startOffset" : 144,
      "endOffset" : 224
    }, {
      "referenceID" : 31,
      "context" : "In the last decade or so, there has been some work done in scoring essay traits such as sentence fluency (Chae and Nenkova, 2009), organization (Persing et al., 2010; Taghipour, 2017; Mathias et al., 2018; Song et al., 2020), thesis clarity (Persing and Ng, 2013; Ke et al.",
      "startOffset" : 144,
      "endOffset" : 224
    }, {
      "referenceID" : 21,
      "context" : ", 2020), thesis clarity (Persing and Ng, 2013; Ke et al., 2019) coherence (Somasundaran et al.",
      "startOffset" : 24,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : ", 2020), thesis clarity (Persing and Ng, 2013; Ke et al., 2019) coherence (Somasundaran et al.",
      "startOffset" : 24,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : ", 2019) coherence (Somasundaran et al., 2014; Mathias et al., 2018), prompt adherence (Persing and Ng, 2014), argument strength",
      "startOffset" : 18,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : ", 2019) coherence (Somasundaran et al., 2014; Mathias et al., 2018), prompt adherence (Persing and Ng, 2014), argument strength",
      "startOffset" : 18,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : ", 2018), prompt adherence (Persing and Ng, 2014), argument strength",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "(Persing and Ng, 2015; Taghipour, 2017), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b) and narrative quality (Somasundaran et al.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "(Persing and Ng, 2015; Taghipour, 2017), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b) and narrative quality (Somasundaran et al.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "(Persing and Ng, 2015; Taghipour, 2017), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b) and narrative quality (Somasundaran et al.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "(Persing and Ng, 2015; Taghipour, 2017), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b) and narrative quality (Somasundaran et al.",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 30,
      "context" : "(Persing and Ng, 2015; Taghipour, 2017), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b) and narrative quality (Somasundaran et al., 2018).",
      "startOffset" : 135,
      "endOffset" : 162
    }, {
      "referenceID" : 26,
      "context" : "(Rama and Vajjala, 2021) discuss solutions across multiple languages (German, Czech and Italian).",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 27,
      "context" : "(Ridley et al., 2021) describe a multi-task learning approach to grade essays and their traits using a neural network.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "We experiment on two different types of recurrent layers - a unidirectional LSTM (Hochreiter and Schmidhuber, 1997) and bidirectional LSTM (BiLSTM) - as the recurrent layer.",
      "startOffset" : 81,
      "endOffset" : 115
    }, {
      "referenceID" : 38,
      "context" : "Source-dependent responses (Zhang and Litman, 2018) are essays",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "These essay sets have traits like content, prompt adherence (Persing and Ng, 2014), language and narrativity (Somasundaran et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 30,
      "context" : "These essay sets have traits like content, prompt adherence (Persing and Ng, 2014), language and narrativity (Somasundaran et al., 2018).",
      "startOffset" : 109,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "The trait scores are taken from the original ASAP dataset, as well as from ASAP++ (Mathias and Bhattacharyya, 2018a).",
      "startOffset" : 82,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "We use Cohen’s Kappa with quadratic weights (Cohen, 1968) (QWK) as the evaluation metric.",
      "startOffset" : 44,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "The different systems are Histogram Intersection String Kernel (HISK) using ν support vector regressor (Cozma et al., 2018), LSTM-CNN Single-task Learning System (STL) (Dong et al.",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : ", 2018), LSTM-CNN Single-task Learning System (STL) (Dong et al., 2017), our system (MTL) where the traits are auxiliary tasks, and our system (MTL*) where the traits are the primary task.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "Figure 3 gives the results of our experiments in scoring the essay traits, using the String Kernel (HISK) (Cozma et al., 2018), CNN-LSTM (STL) (Dong et al.",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : ", 2018), CNN-LSTM (STL) (Dong et al., 2017), and Our Systems (MTL and MTL*).",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "(2020) use the BERTbase-uncased (Devlin et al., 2019) pre-trained language model to perofrm automatic essay grading achieving QWKs in the range of 0.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "We also plan to investigate using ALBERT (Lan et al., 2020), in lieu of the essay stack, to grade essays and their traits simultaneously.",
      "startOffset" : 41,
      "endOffset" : 59
    } ],
    "year" : 0,
    "abstractText" : "Most research in the area of automatic essay grading (AEG) is geared towards scoring the essay holistically while there has also been little work done on scoring individual essay traits. In this paper, we describe a way to score essays using a multi-task learning (MTL) approach, where scoring the essay holistically is the primary task, and scoring the essay traits is the auxiliary task. We compare our results with a single-task learning (STL) approach, using both LSTMs and BiLSTMs. To find out which traits work best for different types of essays, we conduct ablation tests for each of the essay traits. We also report the runtime and number of training parameters for each system. We find that MTL-based BiLSTM system gives the best results for scoring the essay holistically, as well as performing well on scoring the essay traits. The MTL systems also give a speed-up of between 2.30 to 3.70 times the speed of the STL system, when it comes to scoring the essay and all the traits.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_343_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Why don’t people use character-level machine translation?",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The progress in natural language processing (NLP) brought by deep learning is often narrated as removing assumptions about the input data and letting the models learn everything end-to-end. One of the assumptions about input data that seems to resist this trend is (at least partially) linguistically motivated segmentation of input data in machine translation (MT) and NLP in general.\nFor NMT, several papers have claimed parity of character-based methods with subword models, highlighting advantageous features of such systems. Very recent examples include Gao et al. (2020); Banar et al. (2020); Li et al. (2021). Despite this, character-level methods are rarely used as strong baselines in research papers and shared task submissions, suggesting that character-level models might have drawbacks that are not sufficiently addressed in the literature.\nIn this paper, we examine what the state of the art in character-level MT really is. We survey existing methods and conduct a meta-analysis of the\ninput segmentation methods used in WMT shared task submissions. We then systematically compare the most recent character-processing architectures, some of them taken from general NLP research and used for the first time in MT. Further, we propose an alternative two-step decoder architecture that unlike standard decoders does not suffer from a slow-down due to the length of character sequences. Following the recent findings on MT decoding, we evaluate different decoding strategies in the character-level context.\nMany previous studies on character-level MT drew their conclusions from experiments on rather small datasets and focused only on quantitatively assessed translation quality without further analysis. To compensate for this, we revisit and systematically evaluate the state-of-the-art approaches to character-level neural MT and identify their major strengths and weaknesses on large datasets."
    }, {
      "heading" : "2 Character-Level Neural MT",
      "text" : "The original sequence-to-sequence models used word-based vocabularies of a limited size and thus relatively frequent occurrence of out-of-vocabulary tokens. A typical solution to that problem is subword segmentation (Sennrich et al., 2016; Kudo and Richardson, 2018), which keeps frequent tokens intact and splits less frequent ones into smaller units.\nModeling language on the character level is attractive because it can help overcome several problems of subword models. One-hot representations of words or subwords do not reflect systematic character-level relations between words, potentially harming morphologically rich languages. With subwords, minor typos on the source side lead to radically different input representations resulting in low robustness towards source-side noise (Provilkov et al., 2020; Libovický and Fraser, 2020).\nModels using recurrent neural networks (RNNs) showed early success with character-level segmen-\ntation on the decoder side (Chung et al., 2016). Using character-level processing on the encoder side proved harder which was attributed to the features of the attention mechanism which can presumably benefit from semantically rich units (such as subwords) in the encoder. Following this line of thinking, Lee et al. (2017) introduced 1D convolutions with max-pooling that pre-process the character sequence into a sequence of latent wordlike states. Coupled with a character-level decoder, they claimed to match the state-of-the-art subwordbased models. Even though this architecture works well on the character level, it does not generalize further to the byte level (Costa-jussà et al., 2017). Hybrid approaches combining tokenization into words followed by the computation of characterbased word representations were successfully used with RNNs (Luong and Manning, 2016; Grönroos et al., 2017; Ataman et al., 2019). Later, Cherry et al. (2018) showed that with sufficiently large models RNNs do not need architecture modification and perform on par with subword models.\nCharacter-level modeling with Transformers appears to be more difficult. Gupta et al. (2019) used Transparent Attention (Bapna et al., 2018) to train deep character-level models and needed up to 32 layers to close the gap between the BPE and character models, which makes the model too large for practical use. Libovický and Fraser (2020) narrowed the gap between subword and character modeling using curriculum learning by finetuning subword models to character-level.\nGao et al. (2020) proposed adding a convolutional sub-layer in the Transformer layers. At the cost of a 30% increase in parameter count, they managed to narrow the gap between subword- and character-based models by half. Banar et al. (2020) reused the convolutional preprocessing layer with constant step segments of Lee et al. (2017) in a Transformer model for translation into English. With no changes to the decoder, they reached comparable, but usually slightly worse, translation quality as BPE-based models.\nShaham and Levy (2021a) revisited characterand byte-level MT on rather small IWSLT datasets. Their results show that although character-level and byte-level models are usually worse than BPE models, byte-based models without embedding layers often outperform BPE-based models in the out-ofEnglish direction. Using similarly small datasets, Li et al. (2021) claim that character-level modeling"
    }, {
      "heading" : "2016 2017 2018 2019 2020 2021",
      "text" : "outperforms BPE when translating into fusional, agglutinative, and introflexive languages.\nLittle is known about other properties of character-level MT beyond the overall translation quality. Sennrich (2017) prepared a set of contrastive English-German sentence pairs and tested them using shallow RNN-based models. They observed that character-based models transliterated better, but captured morphosyntactic agreement worse. Libovický and Fraser (2020) evaluated Transformer-based character-level models using MorphEval and came to mixed conclusions.\nGupta et al. (2019) and Libovický and Fraser (2020) make claims about the noise robustness of the character-level models using synthetic noise. Li et al. (2021) evaluated domain robustness by training models on small domain-specific datasets and evaluating them on unrelated domains, claiming the superiority of character-level models in this setup. On the other hand, Gupta et al. (2019) evaluated the domain robustness in a more natural setup and did not observe higher robustness when evaluating general domain models on domain-specific tests compared to BPE.\nAnother consideration is longer training and inference times. Character-level systems are significantly slower due to the increased sequence length. Libovický and Fraser (2020) reported a 5.6-fold slowdown at training time and a 4.7-fold slowdown at inference time compared to subword models.\nRecent research on character-level modeling goes beyond MT. Pre-trained multilingual representations are a particularly active area. Clark et al. (2021) propose CANINE. The model shrinks character sequences into less hidden states (similar to Lee et al., 2017). They use local self-attention and strided convolutions (instead of highway layers and max-pooling as in Lee’s work). Their model is either trained using the masked-language-modeling objective (Devlin et al., 2019) with subword super-\nvision, or in an encoder-decoder setup similar to Raffel et al. (2020). Both reach a representation quality comparable to similar subword models.\nByT5 (Xue et al., 2021a) and Charformer (Tay et al., 2021) are based on the mT5 model (Xue et al., 2021b) which uses sequence-to-sequence denoising pre-training. Whereas byT5 only uses byte sequences instead of subwords and differs in hyperparameters, Charformer uses convolution and combines character blocks to obtain latent subword representations. These models mostly reach similar results to sub-word models, occasionally outperforming few of them, in the case of Charformer without a significant slowdown."
    }, {
      "heading" : "3 WMT submissions",
      "text" : "The Conference on Machine Translation (WMT) organizes annual shared tasks in various use cases of MT. The shared task submissions focus on translation quality rather than the novelty of presented ideas, as most other research papers do. Therefore, we assume that, if character-level models were a fully-fledged alternative to subword models, at least some systems submitted to the shared tasks would use character-level models.\nWe annotated recent system description papers with what input and output segmentation they used. We focused on information about experiments with character-level models. Since we are primarily interested in the Transformer architecture that became the standard after 2017, we only included system description papers from 2018–2020 (Bojar et al., 2018; Barrault et al., 2019, 2020). Transformers were used in 81%, 87%, and 97% of the systems in the respective years. We included the main task on WMT, news translation, and two minor tasks where character-level methods might help: translation robustness (Li et al., 2019; Specia et al., 2020) and translation between similar languages (ibid.).\nAlmost all systems use subword-based vocabulary (BPE: 81%, 71%, 66% in the respective years; SentencePiece: None in 2018, 9% and 25% in the\nfollowing ones). Purely word-based (none in 2018, 2% and 3% in the later years) or morphological segmentation (4%, 2%, 3% in the respective years) are rarely used. The average vocabulary size decreases over time (see Figure 2) with a median size remaining at 32k in the last two years. The reason for the decreasing average is probably a higher proportion of systems for low-resource languages, where a smaller vocabulary leads to better translation quality (Sennrich and Zhang, 2019).\nAmong the 145 annotated system description papers, there were only two that used characterlevel segmentation. Mahata et al. (2018) used a character-level model for Finnish-to-English translation. This system however makes many suboptimal design choices and ended up as the last one in the manual evaluation. Scherrer et al. (2019) experimented with character-level systems for similar language translation and observed that characters outperform other segmentations for SpanishPortuguese translation, but not for Czech-Polish.\nKnowles et al. (2020) experimented with different subword vocabulary sizes for English-Inuktikut translation and reached the best results using a subword vocabulary of size 1k, which makes it close to the character level. Most of the papers do not even mention character-level segmentation as a viable alternative they would like to pursue in future work (7% in 2018, 2% in 2019, none in 2020).\nCharacter-level methods were more frequently used in WMT17 with RNN-based systems, especially for translation of Finnish (Escolano et al., 2017; Östling et al., 2017) and less successfully for Chinese (Holtz et al., 2017) and the automatic post-editing task (Variš and Bojar, 2017).\nOn the other hand, Figure 1 shows that the research interest in character-level methods remains approximately the same, or may have slightly increased. For practical solutions in WMT systems, we clearly show that system designers in the WMT community have avoided character-level models.\nWe speculate that the main reasons for not considering character-level modeling are its lower efficiency and the fact that the literature shows no clear improvement of translation quality. Most of the submissions use back-translation (85%, 82%, and 94% in the respective years), often iterated several times (11%, 20%, 16%), which requires both training and inference on large datasets. With the approximately 5-fold slowdown, WMT-scale experiments on character models are not easily tractable."
    }, {
      "heading" : "4 Evaluated Models",
      "text" : "We evaluate several Transformer-based architectures for character-level MT. A major issue with character-level sequence processing is the sequence length and low information density compared to subword sequences. Architectures for characterlevel sequence processing typically address this issue by locally processing and shrinking the sequences into latent word-like units. In our experiments, we explore several ways to do this.\nFirst, we directly use character embeddings as input to the Transformer. Second, following Banar et al. (2020), we use the convolutional character processing layers proposed by Lee et al. (2017). Third, we replace the convolutions with local selfattention as proposed in the CANINE model (Clark et al., 2021). Finally, we use the recently proposed Charformer architecture (Tay et al., 2021).\nLee-style encoding. Lee et al. (2017) process the sequence of character embeddings with convolutions of different kernel sizes and output channels. In the original paper, this was followed by 4 highway layers (Srivastava et al., 2015). In our preliminary experiments, we observed that a too deep stack of highway layers leads to diminishing gradients and we replaced the two Highway layers with feedforward sublayers as used in the Transformer architecture (Vaswani et al., 2017).\nCANINE. Clark et al. (2021) experiment with character-level pre-trained sentence representations. The character-processing architecture is in principle similar to Lee et al. (2017) but uses more modern building blocks. Character embeddings are processed by a Transformer layer with local selfattention which only allows the states to attend to states in their neighborhood. This is followed by downsampling using strided convolution.\nCharformer. Unlike previous approaches, Charformer (Tay et al., 2021) does not apply nonlinearity on the embeddings and gets latent subword representations by repeated averaging of character embeddings. First, it processes the sequence using a 1D convolution, so the states are aware of their mutual local positions. Second, nonoverlapping character n-grams of length up to N are represented by averages of the respective embeddings. For each character, there is a vector that represents the character as a member of n-grams of length 1 to N . In the third step, the character\nblocks are scored with a scoring function (a linear transformation), which can be interpreted as attention over the N different n-gram lengths. The attention scores are used to compute a weighted average over the n-gram representations. Finally, the sequence is downsampled using mean-pooling with window size N .\nWhereas Lee-style encoding allows using lowdimensional character embeddings and keeps most parameters in the convolutional layers, CANINE and Charformer need the character representation to have the same dimension as the following Transformer layer stack.\nTwo-step decoding. The architectures mentioned above allow the Transformer layers to operate more efficiently with a shorter and more information-dense sequence of states. However, while decoding, we need to generate the target character sequence in the original length, by outputting a block of characters in each decoding step. Our preliminary experiments showed that generating blocks of characters non-autoregressively leads to incoherent output. Therefore, we propose a twostep decoding architecture where the stack of Transformer layers operating over the downsampled sequence is followed by a lightweight LSTM autoregressive decoder (see Figure 3).\nThe input to the LSTM decoder is a concatenation of the embedding of the previously generated character and a projection of the Transformer decoder output state. At inference time, the LSTM decoder generates a block of characters and inputs them to the character-level processing layer. The Transformer decoder computes an output state that the LSTM decoder uses to generate another character block. More details are in Appendix A.\nFirst, we conduct all our experiments on the small IWSLT datasets. Then we evaluate the most\npromising architectures on larger datasets."
    }, {
      "heading" : "5 Experiments on Small Data",
      "text" : "We implement the models using Huggingface Transformers (Wolf et al., 2020). We take the CANINE layer from Huggingface Transformers and use an independent implementation of Charformer1. Our source code is attached to the submission. Hyperparameters and other experimental details can be found in Appendix B."
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "We evaluate the models on translation between English paired with German, French, and Arabic (with English as both input and output) using the IWSLT 2017 datasets (Cettolo et al., 2017) with a training data size of around 200k sentences for each language pair (see Appendix B for details).\nFor the subword models, we tokenize the input using the Moses tokenizer (Koehn et al., 2007) and then further split the words into subword units using BPE (Sennrich et al., 2016) with 16k merge operations. For the character models, we limit the vocabulary to 300 UTF-8 characters.\nWe use the Transformer Base architecture (Vaswani et al., 2017) in all experiments. We make no changes to it in the subword and baseline character experiments. In the later experiments, we replace the embedding lookup with the character processing architectures. For the Lee-style encoder, we chose similar hyperparameters as related work (Banar et al., 2020). For experiments with Charformer and CANINE models, we set the hyperparameters such that they cover the same character span before downsampling as the Lee-style encoder, which causes the models to have fewer parameters than a Lee-style encoder. Note however that for both the Charformer and the CANINE models, the number of parameters is almost independent of the character window width. For all three character processing architectures, we experiment with downsampling factors of 3 and 5 (a 16k BPE vocabulary corresponds to a downsampling factor of about 4 in English)."
    }, {
      "heading" : "5.2 Translation Quality",
      "text" : "We evaluate the translation quality using the BLEU score (Papineni et al., 2002), the chrF score (Popović, 2015) (as implemented in SacreBLEU;\n1https://github.com/lucidrains/charformer-pytorch\nPost, 2018),2 and the COMET score (Rei et al., 2020). We run each experiment 4 times and report the mean value and standard deviation.\nThe results are presented in Table 1. Except for translation into Arabic (which is consistent with the findings of Shaham and Levy, 2021a and Li et al., 2021), where character methods outperform BPEs, subword methods are always better than characters.\nThe Lee-style encoder outperforms the two more recent methods and the method of using the character embeddings directly. Charformer performs similarly to using character embeddings directly, CANINE is significantly worse. The results are mostly consistent across the language pairs.\nIncreasing the downsampling from 3 to 5 degrades the translation quality for all architectures. Employing the two-step decoder matches the decoding speed of subword models. However, the overall translation quality is much worse.\nThe three metrics that we use give consistent results in most cases. Often, relatively small differences in BLEU and chrF scores correspond to much bigger differences in the COMET score."
    }, {
      "heading" : "5.3 Inference",
      "text" : "Inference algorithm for neural MT have been discussed extensively (Meister et al., 2020; Massarelli et al., 2020; Shi et al., 2020; Shaham and Levy, 2021b) for the subword models. Subword translation quality quickly degrades beyond a certain beam width unless heuristically defined length normalization is applied.\nAs an alternative, Eikema and Aziz (2020) recently proposed Minimum Bayes Risk (MBR; Goel and Byrne 2000) estimation as an alternative. Assuming that similar sentences should be similarly probable, they propose repeatedly sampling from the model and selecting a sentence that is most similar to other samples. With subword models, MBR performs comparably to beam search.\nWe explore what decoding strategies are best suited for the character-level models. We compare the translation quality of beam search decoding with different degrees of length normalization.3 Further, we compare length-normalized\n2BLEU score signature nrefs:1|case:mixed| eff:no|tok:13a|smooth:exp|version:2.0.0 chrF score signature nrefs:1|case:mixed|eff:yes| nc:6|nw:0|space:no|version:2.0.0\n3As we increase beam size, the number of search errors is decreasing, but here we are evaluating modeling errors, not search errors.\nbeam search decoding with MBR (with 100 samples), greedy decoding, and random sampling. We use the chrF as a comparison metric which allows pre-computing the character n-grams and thus faster sentence pair comparison than the originally proposed METEOR (Denkowski and Lavie, 2011).\nFigure 4 shows the translation quality of the selected models for different beam sizes. The dotted lines denoting the translation quality without length normalization show that the quality of the subword models quickly deteriorates without length normalization, whereas vanilla and Lee-style characterlevel models do not seem to suffer from this problem."
    }, {
      "heading" : "3 3 0.430 0.523 0.540 0.526-0.657 -0.015 0.065 -0.105",
      "text" : ""
    }, {
      "heading" : "3 3 0.227 0.462 0.540 0.412-1.720 -0.424 0.036 -1.090",
      "text" : ""
    }, {
      "heading" : "3 3 0.253 0.516 0.534 0.413-1.680 -0.097 -0.034 -1.130",
      "text" : "Table 2 presents the translation quality for different decoding methods. In all cases, beam search is the best strategy. Sampling from character-level models leads to very poor translation quality that in turn also influences the MBR decoding that leads to much worse results than beam search.\nOur experiments show that beam search with\nlength normalization is the best inference algorithm for character-level models. They also seem to be more resilient towards the beam search curse compared to subword models."
    }, {
      "heading" : "6 Experiments on WMT Data",
      "text" : "Based on the results of the experiments with the IWSLT data, we further experiment only with the Lee-style encoder using a downsampling factor of 3 on the source side. Additionally, we experiment with hybrid systems with a subword encoder and character decoder. We train translation systems of competitive quality on two high-resource language pairs, English-Czech and English-German, and perform an extensive evaluation."
    }, {
      "heading" : "6.1 Experimental Setup",
      "text" : "For English-to-Czech translation, we use the CzEng 2.0 corpus (Kocmi et al., 2020b) that aggregates and curates all sources for this language pair. We use all 66M authentic parallel sentence pairs and 50M back-translated Czech sentences.\nFor the English-to-German translation, we use a subset of the training data used by Chen et al. (2021). The data consists of 66M authentic sentence pairs filtered from the available data for WMT and 52M back-translated German sentences from News Crawl 2020.\nWe tag the back-translation data (Caswell et al., 2019). We use the Transformer Big architecture for all experiments with hyperparameters following Popel and Bojar (2018). For the Lee-style encoder, we double the hidden layer sizes compared to the IWSLT experiments (following the hidden size increase between the Transormer Base and Big archtiectures). In contrast to the previous set of experiments, we use Fairseq (Ott et al., 2019). Our code and systems outputs are attached to the submission.\nWe evaluate the systems not only on WMT20 test sets but also on data that often motivated the research of character-level methods. We evaluate the out-of-domain performance of the models on the NHS test set from the WMT17 Biomedical Task (Jimeno Yepes et al., 2017) and on the WMT16 IT Domain test set (Bojar et al., 2016). We use the same evaluation metrics as for the IWSLT experiments. We estimate the confidence intervals using bootstrap resampling (Koehn, 2004).\nWe also assess the gender bias of the systems (Stanovsky et al., 2019; Kocmi et al., 2020a), using\na dataset of sentence pairs with stereotypical and non-stereotypical English sentences. We measure the accuracy of gendered nouns and pronouns using word alignment and morphological analysis.\nMorphological generalization is often mentioned among the motivations for character-level modeling. Therefore, we evaluate our models using MorphEval (Burlot and Yvon, 2017; Burlot et al., 2018). Similar to the gender evaluation, MorphEval also uses contrastive sentence pairs that differ in exactly one morphological feature. Accuracy on the sentences is measured. Besides, we assess how well the models handle lemmas and forms that were unseen at training time. We tokenize and lemmatize all data with UDPipe (Straka and Straková, 2017). On the WMT20 test set, we compute the recall of test lemmas that were not in the training set and the recall of word forms that were not in the training data, but forms of the same lemma were. Note that not generating a particular lemma or form is not necessarily an error. Therefore, we report the recall in contrast with the recall of lemmas and forms that were represented in the training data.\nCharacter-level models are also supposed to be more robust towards source-side noise. We evaluate the noise robustness of the systems using synthetic noise. We use TextFlint (Wang et al., 2021) to generate synthetic noise in the source text with simulated typos and spelling errors. We generate 20 noisy versions of the WMT20 test set and report the average chrF score."
    }, {
      "heading" : "6.2 Results",
      "text" : "The main results are presented in Table 3. The main trends in the translation quality are the same as in the case of IWSLT data: subword models outperform character models. Using Lee-style encoding narrows the quality gap and performs similarly to models with subword tokens on the source side. Although domain robustness often motivates character-level experiments, our experiments show that the trends are domain-independent, except for English-German IT Domain translation.\nThe similar performance of the subword encoder and the Lee-style encoder suggests that the hidden states of the Lee-style encoder can efficiently emulate the subword segmentation. We speculate that the main weaknesses remain on the decoder side.\nIn the English-to-Czech direction, the characterlevel models perform worse in gender bias evaluation, although they better capture grammatical gen-\nder agreement according to the MorphEval benchmark. On the other hand, character-level models make more frequent errors in the tense of coordinated verbs. There are no major differences in recall of novel forms and lemmas.\nFor the English-to-German translation, characterlevel methods reach better results on the gender benchmark. We speculate that getting gender correct in German might be easier because unlike Czech it does not require subject-verb agreement. The average performance on the MorphEval benchmark is also slightly better for character models. Detailed results on MorphEval are in Tables 7 and 8 in the Appendix. Recall of novel forms suggest also slightly better morphological generalization.\nThe only consistent advantage of the characterlevel models is their robustness towards source side noise. Here, the character-level models outperform both the fully subword model and the subword encoder."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In our extensive literature survey, we found evidence that character-level methods should reach comparative translation quality as subword methods, typically at the expense of much higher computation costs. We speculate that the computational cost is the reason why virtually none of the recent WMT systems used character-level methods nor mention them as a reasonable alternative.\nRecently, most innovations in character-level modeling were introduced in the context of pretrained representations. In our comparison of character processing architectures (two of them used for the first time in the context of MT), we showed that 1D convolutions followed by highway layers still deliver the best results for MT.\nCharacter-level systems are still mostly worse than subword systems. Moreover, the recent character-level architectures do not show advantages over vanilla character models, other than improved speed.\nTo overcome efficiency issues, we proposed a two-step decoding architecture that matches the speed of subword models, however at the expense of a further drop in translation quality.\nFurthermore, we found that conclusions of recent literature on decoding in MT do not generalize for character models. Character models do not suffer from the beam search curse and decoding methods based on sampling perform poorly.\nEvaluation on competitively large datasets showed that there is still a small quality gap between character and subword models. Character models do not show better domain robustness, and only slightly better morphological generalization in German, although this is often mentioned as important motivation for character-level modeling. The only clear advantage of character models is high robustness towards source-side noise.\nIn contrast to earlier work on character-level MT, which claimed that decoding is straightforward and which focused on the encoder part of the model, our conclusions are that Lee-style encoding is comparable to subword encoders. Even now, most modeling innovations focus on encoding. Both accurate and efficient character-level decoding remains an open research question."
    }, {
      "heading" : "A Two-step decoder",
      "text" : "Here, we describe details of the architecture of the two step decoder shown in Figure 3. The input\nof the decoder are hidden states of the character processing architecture, i.e., for a downsampling factor s, a sequence that is s times shorter than the input sequence. The output of the Transformer stack is a sequence of the same length.\nFor each Transformer decoder state hi, the decoder needs to produce s characters. This is done by a light-weight autoregressive LSTM decoder. In each step, it has two inputs: the embedding of the previously decoded character and a projection of the decoder state hi. There are s different linear projections for each of the output character generated from a single Transformer state.\nAt inference time, the LSTM decoder gets one Transformer state and generates s output characters. The characters are fed to the character processing architecture, which is in turn used to generate the next Transformer decoder state.\nB IWSLT Experiments\nB.1 Dataset details\nWe used the tst2010 part of the dataset for validation and tst2015 for testing and did not use any other test sets. The data sizes are presented in Table 4.\nB.2 Model Hyperparameters\nAll models are trained with initial learning rate: 5 · 10−4 with 4k warmup steps. The batch size is 20k tokens for both BPE and character experiments with update after 3 batches. Label smoothing is set to 0.1.\nLee-style. The character embedding dimension is 64. The original paper used kernel sizes from 1 to 8. For ease of implementation, we only use even-sized kernels up to size 9. The encoder uses 1D convolutions of kernel size 1, 3, 5, 7, 9 with 128, 256, 512, 512, 256 filters. Their output is concatenated and projected to the model dimension, followed by 2 highway layers and 2 Transformer feed-forward layers.\nCANINE. The local self-attention span in the encoder is 4× the downsampling factor, in the encoder, equal to the downsampling factor.\nTwo-step decoder. The decoder uses character embeddings with dimension of 64, which is also the size of the projection of the Transformer decoder state. The hidden state size of the LSTM is 128.\nB.3 Validation Performance The validation BLEU and chrF scores and training and inference times are in Table 5. The training times were measured on machines with GeForce GTX 1080 Ti GPUs and with Intel Xeon E5– 2630v4 CPUs (2.20GHz), a single GPU was used.\nNote that the experiments on IWSLT were not optimized for speed and are thus not comparable with the times reported on the larger datasets."
    }, {
      "heading" : "C WMT Experiments",
      "text" : "C.1 Training Details We use the Transformer Big architecture as defined FairSeq’s standard transformer_wmt_en_de_big_t2t. The Lee-style encoder uses filters sizes 1, 3, 5, 7, 9 of dimensions 256, 512, 1024, 1024, 512. The other parameters remains the same as in the IWSLT experiments.\nWe set the beta parameters of Adam optimizer to 0.9 and 0.998 and gradient clipping to 5. The learning rate is 5 · 10−4 with 16k warmup steps. Early stopping is with respect to negative log likelihood with patience 10. We save 5 best checkpoints and do checkpoint averaging before evaluation. The maximum batch size is 1800 tokens for the BPE experiments and 500 for character-level experiments. We train the models on 4 GPUs, so the effective batch size is 4 times bigger.\nC.2 Validation Performance During training, we evaluated the models by measuring the cross-entropy on the validation set. After model training, we use grid search to estimate the best value of length normalization on a validation set. The translation quality on the validation data is tabulated in Table 6.\nC.3 Detailed Results The detailed results on the MorphEval benchmark are in Tables 7 (Czech) and 8 (German). The details of the noise evaluation are in Table 9."
    } ],
    "references" : [ {
      "title" : "On the importance of word boundaries in character-level neural machine translation",
      "author" : [ "Duygu Ataman", "Orhan Firat", "Mattia A. Di Gangi", "Marcello Federico", "Alexandra Birch." ],
      "venue" : "Proceedings of the 8",
      "citeRegEx" : "Ataman et al\\.,? 2019",
      "shortCiteRegEx" : "Ataman et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-level transformer-based neural machine translation",
      "author" : [ "Nikolay Banar", "Walter Daelemans", "Mike Kestemont." ],
      "venue" : "NLPIR 2020: 4th International Conference on Natural Language Processing and Information Retrieval, Seoul, Republic",
      "citeRegEx" : "Banar et al\\.,? 2020",
      "shortCiteRegEx" : "Banar et al\\.",
      "year" : 2020
    }, {
      "title" : "Training deeper neural machine translation models with transparent attention",
      "author" : [ "Ankur Bapna", "Mia Chen", "Orhan Firat", "Yuan Cao", "Yonghui Wu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Bapna et al\\.,? 2018",
      "shortCiteRegEx" : "Bapna et al\\.",
      "year" : 2018
    }, {
      "title" : "Findings of the 2020 conference on machine translation (WMT20)",
      "author" : [ "Monz", "Makoto Morishita", "Masaaki Nagata", "Toshiaki Nakazawa", "Santanu Pal", "Matt Post", "Marcos Zampieri." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages",
      "citeRegEx" : "Monz et al\\.,? 2020",
      "shortCiteRegEx" : "Monz et al\\.",
      "year" : 2020
    }, {
      "title" : "Findings of the 2016 conference on machine translation",
      "author" : [ "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume",
      "citeRegEx" : "Rubino et al\\.,? 2016",
      "shortCiteRegEx" : "Rubino et al\\.",
      "year" : 2016
    }, {
      "title" : "Findings of the 2018 conference on machine translation (WMT18)",
      "author" : [ "Ondřej Bojar", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Philipp Koehn", "Christof Monz." ],
      "venue" : "Proceedings of the Third Conference on Machine Trans-",
      "citeRegEx" : "Bojar et al\\.,? 2018",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2018
    }, {
      "title" : "The WMT’18 morpheval test suites for English-Czech, English-German, English-Finnish",
      "author" : [ "Franck Burlot", "Yves Scherrer", "Vinit Ravishankar", "Ondřej Bojar", "Stig-Arne Grönroos", "Maarit Koponen", "Tommi Nieminen", "François Yvon" ],
      "venue" : null,
      "citeRegEx" : "Burlot et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Burlot et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating the morphological competence of machine translation systems",
      "author" : [ "Franck Burlot", "François Yvon." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 43–55, Copenhagen, Denmark. Association for Computational",
      "citeRegEx" : "Burlot and Yvon.,? 2017",
      "shortCiteRegEx" : "Burlot and Yvon.",
      "year" : 2017
    }, {
      "title" : "Tagged back-translation",
      "author" : [ "Isaac Caswell", "Ciprian Chelba", "David Grangier." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 53–63, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Caswell et al\\.,? 2019",
      "shortCiteRegEx" : "Caswell et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the iwslt 2017 evaluation campaign",
      "author" : [ "Mauro Cettolo", "Marcello Federico", "Luisa Bentivogli", "Niehues Jan", "Stüker Sebastian", "Sudoh Katsuitho", "Yoshino Koichiro", "Federmann Christian." ],
      "venue" : "International Workshop on Spoken Language Trans-",
      "citeRegEx" : "Cettolo et al\\.,? 2017",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2017
    }, {
      "title" : "The University of Edinburgh’s English-German and English-Hausa",
      "author" : [ "Pinzhen Chen", "Jindřich Helcl", "Ulrich Germann", "Laurie Burchell", "Nikolay Bogoychev", "Antonio Valerio Miceli Barone", "Jonas Waldendorf", "Alexandra Birch", "Kenneth Heafield" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Revisiting character-based neural machine translation with capacity and compression",
      "author" : [ "Colin Cherry", "George Foster", "Ankur Bapna", "Orhan Firat", "Wolfgang Macherey." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Cherry et al\\.,? 2018",
      "shortCiteRegEx" : "Cherry et al\\.",
      "year" : 2018
    }, {
      "title" : "A character-level decoder without explicit segmentation for neural machine translation",
      "author" : [ "Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Chung et al\\.,? 2016",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "CANINE: pre-training an efficient tokenization-free encoder for language representation",
      "author" : [ "Jonathan H. Clark", "Dan Garrette", "Iulia Turc", "John Wieting." ],
      "venue" : "CoRR, abs/2103.06874.",
      "citeRegEx" : "Clark et al\\.,? 2021",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2021
    }, {
      "title" : "Byte-based neural machine translation",
      "author" : [ "Marta R. Costa-jussà", "Carlos Escolano", "José A.R. Fonollosa." ],
      "venue" : "Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 9",
      "citeRegEx" : "Costa.jussà et al\\.,? 2017",
      "shortCiteRegEx" : "Costa.jussà et al\\.",
      "year" : 2017
    }, {
      "title" : "Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems",
      "author" : [ "Michael Denkowski", "Alon Lavie" ],
      "venue" : "In Proceedings of the Sixth Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Denkowski and Lavie.,? \\Q2011\\E",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2011
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Is MAP decoding all you need? the inadequacy of the mode in neural machine translation",
      "author" : [ "Bryan Eikema", "Wilker Aziz." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 4506–4520, Barcelona, Spain (Online).",
      "citeRegEx" : "Eikema and Aziz.,? 2020",
      "shortCiteRegEx" : "Eikema and Aziz.",
      "year" : 2020
    }, {
      "title" : "The TALP-UPC neural machine translation system for German/Finnish-English using the inverse direction model in rescoring",
      "author" : [ "Carlos Escolano", "Marta R. Costa-jussà", "José A.R. Fonollosa." ],
      "venue" : "Proceedings of the Second Conference on Machine",
      "citeRegEx" : "Escolano et al\\.,? 2017",
      "shortCiteRegEx" : "Escolano et al\\.",
      "year" : 2017
    }, {
      "title" : "Character-level translation with self-attention",
      "author" : [ "Yingqiang Gao", "Nikola I. Nikolov", "Yuhuang Hu", "Richard H.R. Hahnloser." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1591–1604, Online. As-",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Minimum bayes-risk automatic speech recognition",
      "author" : [ "Vaibhava Goel", "William J. Byrne." ],
      "venue" : "Comput. Speech Lang., 14(2):115–135.",
      "citeRegEx" : "Goel and Byrne.,? 2000",
      "shortCiteRegEx" : "Goel and Byrne.",
      "year" : 2000
    }, {
      "title" : "Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis",
      "author" : [ "Stig-Arne Grönroos", "Sami Virpioja", "Mikko Kurimo." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 296–",
      "citeRegEx" : "Grönroos et al\\.,? 2017",
      "shortCiteRegEx" : "Grönroos et al\\.",
      "year" : 2017
    }, {
      "title" : "Character-based NMT with transformer",
      "author" : [ "Rohit Gupta", "Laurent Besacier", "Marc Dymetman", "Matthias Gallé." ],
      "venue" : "CoRR, abs/1911.04997.",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "University of Rochester WMT 2017 NMT system submission",
      "author" : [ "Chester Holtz", "Chuyang Ke", "Daniel Gildea." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 310–314, Copenhagen, Denmark. Association for Computa-",
      "citeRegEx" : "Holtz et al\\.,? 2017",
      "shortCiteRegEx" : "Holtz et al\\.",
      "year" : 2017
    }, {
      "title" : "Findings of the WMT 2017 biomedical translation shared task",
      "author" : [ "Saskia Trescher." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 234–247, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Trescher.,? 2017",
      "shortCiteRegEx" : "Trescher.",
      "year" : 2017
    }, {
      "title" : "NRC systems for the 2020 Inuktitut-English news translation task",
      "author" : [ "Rebecca Knowles", "Darlene Stewart", "Samuel Larkin", "Patrick Littell." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 156–170, Online. Association for Computa-",
      "citeRegEx" : "Knowles et al\\.,? 2020",
      "shortCiteRegEx" : "Knowles et al\\.",
      "year" : 2020
    }, {
      "title" : "Gender coreference and bias evaluation at WMT 2020",
      "author" : [ "Tom Kocmi", "Tomasz Limisiewicz", "Gabriel Stanovsky." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 357–364, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Kocmi et al\\.,? 2020a",
      "shortCiteRegEx" : "Kocmi et al\\.",
      "year" : 2020
    }, {
      "title" : "Announcing czeng 2.0 parallel corpus with over 2 gigawords",
      "author" : [ "Tom Kocmi", "Martin Popel", "Ondřej Bojar" ],
      "venue" : null,
      "citeRegEx" : "Kocmi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kocmi et al\\.",
      "year" : 2020
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computa-",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Fully character-level neural machine translation without explicit segmentation",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Thomas Hofmann." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:365–378.",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "When is char better than subword: A systematic study of segmentation algorithms for neural machine translation",
      "author" : [ "Jiahuan Li", "Yutong Shen", "Shujian Huang", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Findings of the first shared task on machine translation robustness",
      "author" : [ "Xian Li", "Paul Michel", "Antonios Anastasopoulos", "Yonatan Belinkov", "Nadir Durrani", "Orhan Firat", "Philipp Koehn", "Graham Neubig", "Juan Pino", "Hassan Sajjad." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards reasonably-sized character-level transformer NMT by finetuning subword systems",
      "author" : [ "Jindřich Libovický", "Alexander Fraser." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Libovický and Fraser.,? 2020",
      "shortCiteRegEx" : "Libovický and Fraser.",
      "year" : 2020
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models",
      "author" : [ "Minh-Thang Luong", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Luong and Manning.,? 2016",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "JUCBNMT at WMT2018 news translation task: Character based neural machine translation of Finnish to English",
      "author" : [ "Sainik Kumar Mahata", "Dipankar Das", "Sivaji Bandyopadhyay." ],
      "venue" : "Proceedings of the Third Conference on Machine Trans-",
      "citeRegEx" : "Mahata et al\\.,? 2018",
      "shortCiteRegEx" : "Mahata et al\\.",
      "year" : 2018
    }, {
      "title" : "How decoding strategies affect the verifiability of generated text",
      "author" : [ "Luca Massarelli", "Fabio Petroni", "Aleksandra Piktus", "Myle Ott", "Tim Rocktäschel", "Vassilis Plachouras", "Fabrizio Silvestri", "Sebastian Riedel." ],
      "venue" : "Findings of the Association for Compu-",
      "citeRegEx" : "Massarelli et al\\.,? 2020",
      "shortCiteRegEx" : "Massarelli et al\\.",
      "year" : 2020
    }, {
      "title" : "If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173–2185, Online",
      "author" : [ "Clara Meister", "Ryan Cotterell", "Tim Vieira." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Meister et al\\.,? 2020",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2020
    }, {
      "title" : "The Helsinki neural machine translation system",
      "author" : [ "Robert Östling", "Yves Scherrer", "Jörg Tiedemann", "Gongbo Tang", "Tommi Nieminen." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 338–347, Copenhagen, Denmark.",
      "citeRegEx" : "Östling et al\\.,? 2017",
      "shortCiteRegEx" : "Östling et al\\.",
      "year" : 2017
    }, {
      "title" : "fairseq: A fast, extensible",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli" ],
      "venue" : null,
      "citeRegEx" : "Ott et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Training Tips for the Transformer Model",
      "author" : [ "Martin Popel", "Ondřej Bojar." ],
      "venue" : "The Prague Bulletin of Mathematical Linguistics, 110:43–70.",
      "citeRegEx" : "Popel and Bojar.,? 2018",
      "shortCiteRegEx" : "Popel and Bojar.",
      "year" : 2018
    }, {
      "title" : "chrF: character n-gram F-score for automatic MT evaluation",
      "author" : [ "Maja Popović." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.",
      "citeRegEx" : "Popović.,? 2015",
      "shortCiteRegEx" : "Popović.",
      "year" : 2015
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "BPE-dropout: Simple and effective subword regularization",
      "author" : [ "Ivan Provilkov", "Dmitrii Emelianenko", "Elena Voita." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1882–1892, Online. Association for",
      "citeRegEx" : "Provilkov et al\\.,? 2020",
      "shortCiteRegEx" : "Provilkov et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "COMET: A neural framework for MT evaluation",
      "author" : [ "Ricardo Rei", "Craig Stewart", "Ana C Farinha", "Alon Lavie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Associa-",
      "citeRegEx" : "Rei et al\\.,? 2020",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2020
    }, {
      "title" : "The University of Helsinki submissions to the WMT19 similar language translation task",
      "author" : [ "Yves Scherrer", "Raúl Vázquez", "Sami Virpioja." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day",
      "citeRegEx" : "Scherrer et al\\.,? 2019",
      "shortCiteRegEx" : "Scherrer et al\\.",
      "year" : 2019
    }, {
      "title" : "How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs",
      "author" : [ "Rico Sennrich." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Sennrich.,? 2017",
      "shortCiteRegEx" : "Sennrich.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Revisiting lowresource neural machine translation: A case study",
      "author" : [ "Rico Sennrich", "Biao Zhang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 211– 221, Florence, Italy. Association for Computational",
      "citeRegEx" : "Sennrich and Zhang.,? 2019",
      "shortCiteRegEx" : "Sennrich and Zhang.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation without embeddings",
      "author" : [ "Uri Shaham", "Omer Levy." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 181–186, On-",
      "citeRegEx" : "Shaham and Levy.,? 2021a",
      "shortCiteRegEx" : "Shaham and Levy.",
      "year" : 2021
    }, {
      "title" : "What do you get when you cross beam search with nucleus sampling? CoRR, abs/2107.09729",
      "author" : [ "Uri Shaham", "Omer Levy" ],
      "venue" : null,
      "citeRegEx" : "Shaham and Levy.,? \\Q2021\\E",
      "shortCiteRegEx" : "Shaham and Levy.",
      "year" : 2021
    }, {
      "title" : "Why neural machine translation prefers empty outputs",
      "author" : [ "Xing Shi", "Yijun Xiao", "Kevin Knight." ],
      "venue" : "CoRR, abs/2012.13454.",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "Highway networks",
      "author" : [ "Rupesh Kumar Srivastava", "Klaus Greff", "Jürgen Schmidhuber." ],
      "venue" : "CoRR, abs/1505.00387.",
      "citeRegEx" : "Srivastava et al\\.,? 2015",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluating gender bias in machine translation",
      "author" : [ "Gabriel Stanovsky", "Noah A. Smith", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684, Florence, Italy. Association for",
      "citeRegEx" : "Stanovsky et al\\.,? 2019",
      "shortCiteRegEx" : "Stanovsky et al\\.",
      "year" : 2019
    }, {
      "title" : "Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe",
      "author" : [ "Milan Straka", "Jana Straková" ],
      "venue" : "In Proceedings of the CoNLL",
      "citeRegEx" : "Straka and Straková.,? \\Q2017\\E",
      "shortCiteRegEx" : "Straka and Straková.",
      "year" : 2017
    }, {
      "title" : "Charformer: Fast character transformers via gradient-based subword tokenization",
      "author" : [ "Yi Tay", "Vinh Q. Tran", "Sebastian Ruder", "Jai Prakash Gupta", "Hyung Won Chung", "Dara Bahri", "Zhen Qin", "Simon Baumgartner", "Cong Yu", "Donald Metzler." ],
      "venue" : "CoRR,",
      "citeRegEx" : "Tay et al\\.,? 2021",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2021
    }, {
      "title" : "CUNI system for WMT17 automatic post-editing task",
      "author" : [ "Dušan Variš", "Ondřej Bojar." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 661–666, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Variš and Bojar.,? 2017",
      "shortCiteRegEx" : "Variš and Bojar.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "TextFlint: Unified multilingual robustness evaluation toolkit",
      "author" : [ "Yuan Hu", "Qiyuan Bian", "Zhihua Liu", "Shan Qin", "Bolin Zhu", "Xiaoyu Xing", "Jinlan Fu", "Yue Zhang", "Minlong Peng", "Xiaoqing Zheng", "Yaqian Zhou", "Zhongyu Wei", "Xipeng Qiu", "Xuanjing Huang" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Byt5: Towards a tokenfree future with pre-trained byte-to-byte models",
      "author" : [ "Linting Xue", "Aditya Barua", "Noah Constant", "Rami AlRfou", "Sharan Narang", "Mihir Kale", "Adam Roberts", "Colin Raffel." ],
      "venue" : "CoRR, abs/2105.13626.",
      "citeRegEx" : "Xue et al\\.,? 2021a",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Xue et al\\.,? 2021b",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 49,
      "context" : "A typical solution to that problem is subword segmentation (Sennrich et al., 2016; Kudo and Richardson, 2018), which keeps frequent tokens intact and splits less frequent ones into smaller units.",
      "startOffset" : 59,
      "endOffset" : 109
    }, {
      "referenceID" : 29,
      "context" : "A typical solution to that problem is subword segmentation (Sennrich et al., 2016; Kudo and Richardson, 2018), which keeps frequent tokens intact and splits less frequent ones into smaller units.",
      "startOffset" : 59,
      "endOffset" : 109
    }, {
      "referenceID" : 44,
      "context" : "With subwords, minor typos on the source side lead to radically different input representations resulting in low robustness towards source-side noise (Provilkov et al., 2020; Libovický and Fraser, 2020).",
      "startOffset" : 150,
      "endOffset" : 202
    }, {
      "referenceID" : 33,
      "context" : "With subwords, minor typos on the source side lead to radically different input representations resulting in low robustness towards source-side noise (Provilkov et al., 2020; Libovický and Fraser, 2020).",
      "startOffset" : 150,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "well on the character level, it does not generalize further to the byte level (Costa-jussà et al., 2017).",
      "startOffset" : 78,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "(2019) used Transparent Attention (Bapna et al., 2018) to train deep character-level models and needed up to 32 layers to close the gap between the BPE and char-",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "ther trained using the masked-language-modeling objective (Devlin et al., 2019) with subword super-",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 57,
      "context" : ", 2021a) and Charformer (Tay et al., 2021) are based on the mT5 model (Xue",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "came the standard after 2017, we only included system description papers from 2018–2020 (Bojar et al., 2018; Barrault et al., 2019, 2020).",
      "startOffset" : 88,
      "endOffset" : 137
    }, {
      "referenceID" : 32,
      "context" : "on WMT, news translation, and two minor tasks where character-level methods might help: translation robustness (Li et al., 2019; Specia et al., 2020) and translation between similar languages (ibid.",
      "startOffset" : 111,
      "endOffset" : 149
    }, {
      "referenceID" : 50,
      "context" : "proportion of systems for low-resource languages, where a smaller vocabulary leads to better translation quality (Sennrich and Zhang, 2019).",
      "startOffset" : 113,
      "endOffset" : 139
    }, {
      "referenceID" : 18,
      "context" : "cially for translation of Finnish (Escolano et al., 2017; Östling et al., 2017) and less successfully for Chinese (Holtz et al.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 38,
      "context" : "cially for translation of Finnish (Escolano et al., 2017; Östling et al., 2017) and less successfully for Chinese (Holtz et al.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : ", 2017) and less successfully for Chinese (Holtz et al., 2017) and the automatic post-editing task (Variš and Bojar, 2017).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 58,
      "context" : ", 2017) and the automatic post-editing task (Variš and Bojar, 2017).",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "Third, we replace the convolutions with local selfattention as proposed in the CANINE model (Clark et al., 2021).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 57,
      "context" : "Finally, we use the recently proposed Charformer architecture (Tay et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 54,
      "context" : "In the original paper, this was followed by 4 highway layers (Srivastava et al., 2015).",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 59,
      "context" : "ents and we replaced the two Highway layers with feedforward sublayers as used in the Transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 57,
      "context" : "Unlike previous approaches, Charformer (Tay et al., 2021) does not apply nonlinearity on the embeddings and gets latent subword representations by repeated averaging of character embeddings.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "2017 datasets (Cettolo et al., 2017) with a training data size of around 200k sentences for each language pair (see Appendix B for details).",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 49,
      "context" : ", 2007) and then further split the words into subword units using BPE (Sennrich et al., 2016) with 16k merge operations.",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "encoder, we chose similar hyperparameters as related work (Banar et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 40,
      "context" : "We evaluate the translation quality using the BLEU score (Papineni et al., 2002), the chrF score (Popović, 2015) (as implemented in SacreBLEU;",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 42,
      "context" : ", 2002), the chrF score (Popović, 2015) (as implemented in SacreBLEU;",
      "startOffset" : 24,
      "endOffset" : 39
    }, {
      "referenceID" : 46,
      "context" : "com/lucidrains/charformer-pytorch Post, 2018),2 and the COMET score (Rei et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 37,
      "context" : "Inference algorithm for neural MT have been discussed extensively (Meister et al., 2020; Massarelli et al., 2020; Shi et al., 2020; Shaham and Levy, 2021b) for the subword models.",
      "startOffset" : 66,
      "endOffset" : 155
    }, {
      "referenceID" : 36,
      "context" : "Inference algorithm for neural MT have been discussed extensively (Meister et al., 2020; Massarelli et al., 2020; Shi et al., 2020; Shaham and Levy, 2021b) for the subword models.",
      "startOffset" : 66,
      "endOffset" : 155
    }, {
      "referenceID" : 53,
      "context" : "Inference algorithm for neural MT have been discussed extensively (Meister et al., 2020; Massarelli et al., 2020; Shi et al., 2020; Shaham and Levy, 2021b) for the subword models.",
      "startOffset" : 66,
      "endOffset" : 155
    }, {
      "referenceID" : 15,
      "context" : "We use the chrF as a comparison metric which allows pre-computing the character n-grams and thus faster sentence pair comparison than the originally proposed METEOR (Denkowski and Lavie, 2011).",
      "startOffset" : 165,
      "endOffset" : 192
    }, {
      "referenceID" : 39,
      "context" : "In contrast to the previous set of experiments, we use Fairseq (Ott et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 55,
      "context" : "We also assess the gender bias of the systems (Stanovsky et al., 2019; Kocmi et al., 2020a), using a dataset of sentence pairs with stereotypical and non-stereotypical English sentences.",
      "startOffset" : 46,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "We also assess the gender bias of the systems (Stanovsky et al., 2019; Kocmi et al., 2020a), using a dataset of sentence pairs with stereotypical and non-stereotypical English sentences.",
      "startOffset" : 46,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "Therefore, we evaluate our models using MorphEval (Burlot and Yvon, 2017; Burlot et al., 2018).",
      "startOffset" : 50,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Therefore, we evaluate our models using MorphEval (Burlot and Yvon, 2017; Burlot et al., 2018).",
      "startOffset" : 50,
      "endOffset" : 94
    }, {
      "referenceID" : 56,
      "context" : "We tokenize and lemmatize all data with UDPipe (Straka and Straková, 2017).",
      "startOffset" : 47,
      "endOffset" : 74
    } ],
    "year" : 0,
    "abstractText" : "We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in characterlevel natural language processing, characterlevel MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.",
    "creator" : null
  }
}
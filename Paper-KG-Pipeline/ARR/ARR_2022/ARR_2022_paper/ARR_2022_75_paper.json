{
  "name" : "ARR_2022_75_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multilingual pre-trained language models (PLMs) (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks. However, the zero-shot transfer method based on multilingual PLM has limited effectiveness for low-resource languages MRC tasks. This type of multilingual MRC model could roughly detect answer spans but fail to predict the precise boundaries of answers (Yuan et al., 2020). Moreover, the recent development in multilingual MRC evaluation datasets (Artetxe et al., 2020; Lewis et al., 2020; Clark et al., 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al., 2020; Liu et al., 2020; Huang et al., 2021; Wu et al., 2021).\nFor the inaccuracy of boundary detection, existing methods mainly solve this issue by introducing external knowledge. Based on the finding that 70% of the answer spans are language-specific phrases in MLQA (Lewis et al., 2020) dataset, Yuan et al. (2020) proposes an additional language-specific knowledge phrase masking (LAKM) task to enhance the boundary detection performance for lowresource languages. Liang et al. (2021) propose a separate model for boundary calibration based on the output of a base zero-shot transfer model, introducing a phrase boundary recovery task to pretrain calibration module on large-scale multilingual datasets synthesized from Wikipedia documents. The two works rely on external corpora, which are hard to be accessed widely.\nAs illustrated in Figure 1(b), the transfer model may violate syntactic constraints for answer spans on the target language (e.g., the predicted answer \"月光不住\" crossing the boundaries of two subtrees). An intuitive assumption is that the majori-\nty of answer spans respect syntactic constituency boundaries (i.e., syntactic constraint, illustrated by the case in Figure 1(a)). On four multilingual MRC evaluation datasets, we use Stanford CoreNLP1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries. As shown in Table 1, over 87% of answer spans respect syntactic constraint.\nOn the bilingual parallel MRC corpus BiPaR (Jing et al., 2019), we have compared two MRC models: a monolingual MRC model trained on the Chinese data of BiPaR vs. an mBERT-based MRC model trained on the English data of BiPaR and adapted to Chinese via zero-shot transfer. For answers that the monolingual model correctly predicts and respect syntactic constraint, 23.15% of them, which are incorrectly predicted by the transfer model, violate the syntactic constraint, illustrated by the case in Figure 1(b). It suggests that the source language syntax may have a negative impact on the answer boundary detection on the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.\nHowever, the linguistic discrepancies are diverse and impossible to learn. We propose to decouple semantics from syntax in pre-trained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information. Specifically, we propose a Siamese Semantic Disentanglement Model (S2DM) that utilises two latent variables to learn semantic and syntactic vectors in multilingual pre-trained representations. As shown in Figure 2(a), stacking a linear output layer for MRC over the disentangled semantic representation layer, we can finetune the multilingual PLMs on the rich-resource source language and transfer only disentangled semantic knowledge into the target language MRC. Our model aims to reduce the negative impact of the source language syntax on answer boundary detection in the target language.\nBesides, the multilingual pre-trained models have learned an amount of linguistic structure of\n1https://stanfordnlp.github.io/ CoreNLP/\nmultiple languages (Chi et al., 2020). The semantic can not directly disentangle from complex syntactic information in PLMs. To disassociate semantic and syntactic information well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination and losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees), respectively. We use a publicly available multilingual sentence-level parallel corpus with syntactic labels to train S2DM.\nTo summarize, our main contributions are as follows.\n• We propose a multilingual MRC framework that explicitly transfers semantic knowledge of the source language to the target language to reduce the negative impact of source syntax on answer span detection in the target language MRC.\n• We propose a siamese semantic disentanglement model that can effectively separate semantic from syntactic information of multilingual PLMs with semantics/syntax-oriented losses.\n• We demonstrate the generalization of S2DM through theoretical analysis and experimental verification. In other words, our model is suitable even for low-resource languages without training data.\nFinally, experimental results on three multilingual MRC datasets ( XQuAD, MLQA, and TyDi QA) demonstrate that our model can significantly improve performance over two strong baselines (averagely 3.13 and 2.53 EM points on three datasets, respectively)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Cross-lingual/Multilingual Machine Reading Comprehension Hsu et al. (2019) investigate cross-lingual transfer capability of multilingual BERT (mBERT) on MRC tasks and find that zeroshot learning based on PLM is feasible, even between distant languages, such as English and Chinese. Various approaches have been proposed on top of multilingual MRC based on PLMs. Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC. In order to effectively leverage translation\ndata and reduce the impact of noise in translations, Liu et al. (2020) propose a cross-lingual training approach based on knowledge distillation for multilingual MRC. Yuan et al. (2020) present two auxiliary tasks: mixMRC and LAKM to introduce additional phrase boundary supervision into the fine-tuning stage. Liang et al. (2021) propose a pretrained boundary calibration module based on the output of a base zero-shot transfer model, refining the boundary of the initial answer.\nDifferent from the above studies, we mainly consider the impact of syntactic divergences between the source and target language in zero-shot crosslingual transfer based on multilingual PLMs, and attempt to disassociate semantics from syntax and only transfer semantic to the target language.\nDisentangled Representation Learning Recently, there has been a growing amount of work on learning disentangled latent representations in NLP tasks (Zhang et al., 2019; Hu et al., 2017; Yin et al., 2018). In this aspect, the most related work to our syntax-semantics decoupling method is the vMF-Gaussian Variational Autoencoder (VGVAE) model proposed by Chen et al. (2019). It is a generative model using two latent variables to represent semantics and syntax of the sentence, developed for monolingual setting and trained with paraphrases. It uses paraphrase reconstruction loss and a discriminative paraphrase loss to learn semantic representations and word order information for syntactic representations. We adapt this model to multilingual syntax-semantics disentanglement. We use bilingual sentence pairs to train our model with a\ncross-lingual reconstruction loss and semantic discrimination loss. To better disentangle semantics from complex and diverse syntax in multilingual PLMs, we introduce two additional syntax-related losses for incorporating POS tags and syntax trees."
    }, {
      "heading" : "3 Approach",
      "text" : "Figure 2 shows the architecture of our multilingual MRC framework with the proposed siamese semantic disentanglement model."
    }, {
      "heading" : "3.1 Multilingual MRC Framework",
      "text" : "Our multilingual MRC framework consists of three essential components: the multilingual PLM layer, the siamese semantic disentanglement module, and the linear output layer. The output representations from the multilingual PLM are fed into S2DM to disassociate semantic and syntactic information. Only the disentangled semantic representations are input to the linear output layer for predicting answer spans in passages.\nIn order to facilitate the zero-shot cross-lingual transfer of only semantic knowledge from the richresource source language to the low-resource target language, we take a two-stage training strategy. First, we pre-train S2DM with parallel data (see Section 3.2) while the parameters of the multilingual PLM are frozen. Once S2DM is trained, only the output of source language MLP network input to linear output layer for MRC. In the second step, we freeze the parameters of the S2DM and finetune the entire multilingual MRC framework on MRC data of the source language."
    }, {
      "heading" : "3.2 Siamese Semantic Disentanglement Model",
      "text" : "In S2DM, we assume that a sentence x is generated by a semantic and syntactic variable, i.e., y and z, independently. We follow VGVAE Chen et al. (2019) to use the von Mises-Fisher (vMF) distribution for the semantic variable and the Gaussian distribution for the syntactic variable. Formally, the joint probability of the sentence and its two latent variables can be factorized as:\npθ(x, y, z) = pθ(y)pθ(z)pθ(x|y, z) (1)\nwhere pθ(x|y, z) is a generative model. In our model, we use a bag-of-words decoder as the generator.\nThe variational inference process of VGVAE uses a factorized approximated posterior qφ(y|x)qφ(z|x) = qφ(y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:\nLV GV AE = LRL +KL(qφ(z|x)||pθ(z)) + KL(qφ(y|x)||pθ(y)),\n(2)\nLRL = E y∼qφ(y|x) z∼qφ(z|x)\n[ − log pθ(x|y, z) ] (3)\nwhere qφ(y|x) follows vMF(µα(x), κα(x)) and qφ(z|x) follows N(µβ(x), diag(κβ(x))). The prior pθ(y) and pθ(z) follows the uniform distribution vMF(·, 0) and a standard Gaussian distribution respectively. Eq.(3) is the reconstruction loss (RL) of the generator. In our model, we adopt a multilayer perceptron (MLP) network to learn the mean (µ) and variance (κ) of two distributions. The pretrained representations of sentence are contextuallyencoded token vectors, so the latent variable vectors obtained by sampling from the distributions need to be averaged, then output sentence-level semantic vector and syntactic vector.\nSince S2DM uses a Siamese network for both the source and target language, the disentanglement between semantics and syntax is conducted for the two languages simultaneously with two parametershared subnetworks, as shown in Figure 2(b).\nWe attempt to extract rich semantic information from multilingual representations which is universal for multiple languages and contains less syntactic information. Except for the conventional reconstruction loss, we propose two additional losses on parallel data to encourage the latent variable y to capture semantic information: a Cross-lingual Reconstruction Loss (CRL) and Semantic Discrimination Loss (SDL). The former estimates the cross-entropy loss when we use the semantic representation yt of the target language to reconstruct the\nsource input and use the source semantic representation ys for target reconstruction. The latter is used to force the learned source semantic representation ys to be as close as possible to the target semantic representation yt since the semantic meanings of the parallel source and target sentence is equivalent to each other. The two losses are estimated as follows:\nLCRL = E yt∼qφ(y|xt) zs∼qφ(z|xs)\n[ − log pθ(xs|yt, zs) ] + E ys∼qφ(y|xs)\nzt∼qφ(z|xt)\n[ − log pθ(xt|ys, zt) ] , (4)\nLSDL = max { 0, δ − sim(ys, yt) + sim(ys, nt) } +max { 0, δ − sim(ys, yt) + sim(ns, yt)\n} (5) where sim(·, ·) is a cosine similarity score function. The margin δ is a hyperparameter to control the gap between parallel sentence pair (ys, yt) and two nonparallel sentence pairs (ys, nt) and (ns, yt). ns is the semantic vector of a negative sample, which has the highest cosine similarity to ys. Specially, as partial sentences in our corpus are parallel in more than two languages, we limit the data range of negative sampling to only 2-way parallel pairs. nt are obtained in the similar way to ns.\nIn order to guide S2DM to disassociate syntactic information into the syntactic latent variable z, we also define three types of losses tailored for capturing different types of syntactic information. First, we employ Word Position Loss (WPL) , as follow:\nLWPL = Ez∼qφ(z|x) [ − ∑ i log softmax(f(hi))i ] , (6) where softmax(·)i indicates the probability of the ith word at position i, and f(·) is a three-layer feedforward neural network with input hi = [ei; z] that from the concatenation of the syntactic variable z and the embedding vector ei of the multilingual PLM for the ith token in the input sentence.\nIn addition, we define a Part-of-Speech and syntax tree loss to encourage S2DM to isolate deeper syntactic information from pre-trained representations. POS tagging is a sequence labeling task, which can be regarded as a multi-class classification problem for each token in a sentence. Hence, we define Part-of-Speech (POS) Loss as a crossentropy style loss as follows:\nLPOS = ∑ i [ m∑ j=1 −log softmax(g(hi))j ]\n(7)\nwhere g(·) is a linear layer, softmax(·)j estimates the probability of POS tag j, m is the number of different POS tags.\nFor learning structural information, we design Syntax Tree Loss (STL). Many studies have found that PLMs can encode syntactic structures of sentences (Hewitt and Manning (2019); Chi et al. (2020)). Inspired by Hewitt and Manning (2019), we formulate syntactic parsing from pretrained word representations as two independent tasks: depth prediction of a word and distance prediction of two words in the parse tree. Given a matrix B ∈ Rk×m as a linear transformation, the losses of these two subtasks are defined as:\nLdepth = ∑ i (‖wi‖ − ‖Bhi‖22), (8)\nLdistance = ∑ i,j ∣∣dT (wi, wj)− dB(hi, hj)∣∣ (9) where ‖wi‖ is the parse depth of a word defined as the number of edges from the root of the parse tree to wi, and ‖Bhi‖2 is the tree depth L2 norm of the vector space under the linear transformation. dT (wi, wj) is the number of edges in the path between the ith and jth word in the parse tree T . As for dB(hi, hj), it can be defined as the squared L2 distance after transformation by B:\ndB(hi, hj) = (B(hi − hj))T (B(hi − hj)) (10)\nTo induce parse trees, we minimize the summation of the above two losses Ldepth and Ldistance, and LSTL is defined as:\nLSTL = Ldepth + Ldistance (11)\nAccording to the different syntactic tasks, we train two S2DM variants: S2DM_POS and S2DM_SP (SP for syntactic parsing), where their training objectives are defined as follows:\nL1 = LV GV AE + LCRL + LSDL + LWPL + LPOS ,\nL2 = LV GV AE + LCRL + LSDL + LWPL + LSTL"
    }, {
      "heading" : "3.3 Analysis",
      "text" : "In this section, we analyze the generalization of our decoupling-based multilingual MRC model.\nBy two reconstruction losses Eq.(3) and Eq.(4), we will prove that the syntactic and semantic vectors obtained by S2DM are language-agnostic. Since the structure of Eq.(3) and Eq.(4) are the same, we take one part of Eq.(4) for analysis. Due to zs and yt are independent of each other, pθ(xs, zs|yt) = pθ(xs, zs). We obtain:\nE yt∼qφ(y|xt) zs∼qφ(z|xs)\n[ − log pθ(xs|yt, zs) ] = Eyt∼qφ(y|xt)\n( ∑ zs∼qφ(z|xs) pθ(zs)log pθ(zs) pθ(xs, zs|yt) )\n= KL(pθ(zs)||pθ(xs, zs))\nSimilarly,\nE ys∼qφ(y|xs) zt∼qφ(z|xt) [−log pθ(xt|ys, zt)] = KL(pθ(zt)||pθ(xt, zt))\nLRL = KL(pθ(ys)||pθ(xs, ys)) + KL(pθ(yt)||pθ(xt, yt))\nSince minimizing KL(qφ(z|x)||pθ(z)) and KL(qφ(y|x)||pθ(y)), pθ(xs, zs) and pθ(xt, zt) will eventually get the same fitted distribution. In the same way, pθ(xs, ys) and pθ(xt, yt) also fit to the same distribution, no matter what the target language is. This is consistent with our motivation to use the siamese network.\nFurthermore, the semantic discrimination loss Eq.(5) guarantees that the semantic vectors of the source language and the target language are similar. Minimizing Eq.(5) can be equivalent to:{\nsim(ys, yt) > sim(ys, nt) + δ sim(ys, yt) > sim(ns, yt) + δ\nwhich is to maximize sim(ys, yt) to encourages the target semantic vector to approach parallel source semantic vector.\nIn summary, S2DM can obtain languageagnostic semantic and syntactic vectors. Therefore, our multilingual MRC model is suitable even for low-resource languages without training data of decoupling model."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "To verify the effectiveness of our multilingual MRC model, we conducted experiments on three multilingual question answering benchmarks:\nXQuAD (Artetxe et al., 2020) consists of 11 languages corpus translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, include : Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).\nMLQA (Lewis et al., 2020) consists of over 5K extractive MRC instances in 7 languages: English (en), Arabic (ar), German (de), Spanish (es), Hindi (hi), Vietnamese (vi) and Chinese (zh). MLQA is also highly parallel, with MRC instances parallel across 4 different languages on average.\nTyDi QA-GoldP is the gold passage task in TyDi QA (Clark et al., 2020) covers 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te). It is a more challenging MRC benchmark as questions have been written without seeing the answers,\nleading to 3 and 2 times less lexical overlap than XQuAD and MLQA, respectively (Hu et al., 2020)."
    }, {
      "heading" : "4.2 Baseline Models",
      "text" : "We used the following two multilingual PLMs to build our MRC model to conduct experiments:\nmBERT is the multilingual version of BERT Devlin et al. (2019), with 177M parameters, is pretrained on the Wikipedia of 104 languages to optimize the masked language modeling objective.\nXLM-100 uses a pre-training objective similar to that of mBERT but with a larger number of parameters (578M) and a larger shared vocabulary than mBERT, and is trained on the same Wikipedia data covering 100 languages as mBERT."
    }, {
      "heading" : "4.3 Setup",
      "text" : "For S2DM, we collected approximately 26,000+ labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set. The training set covers 20 languages and overlap with 13 languages of three MRC evaluation datasets. We used Universal POS tags and HEAD tags in UD 2.7 for the POS tagging and syntactic parsing task. We chose data from the Chinese semantic textual similarity (STS) task (Tang et al., 2016) as the development set. For hyper-parameters in S2DM, the learning rate was set to 5e-5, the margin δ was 0.4, and the latent variable dimensions was 200.\nFor our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated the test data of the three multilingual MRC datasets. For models based on mBERT, we fine-tuned them for 3 epochs with a training batch size of 32 and a learning rate of 2e-5. We fine-tuned models based on XLM-100 for 2 epochs with a training batch size of 16 and a learning rate of 3e-5."
    }, {
      "heading" : "4.4 Experiment Results",
      "text" : "The overall experimental results are shown in Table 2. All our tests were conducted under the conditions of zero-shot transfer. Our models (S2DM_POS, S2DM_SP combined with XLM100 or mBERT) significantly outperform both XLM-100 and mBERT baselines on three datasets. S2DM_SP achieves the best performance, indicating that the learning of deeper syntax information is compelling. Especially, compared with baselines on TyDi QA-Gold datasets, S2DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM\nrespectively improvements on average across 9 languages.\nThe results of 12 languages in XQuAD and MLQA are shown in Table 3. For cross-lingual transfer performance, our models are better than the two baselines on EM or F1 of all 11 low-resource target languages. The TyDi QA-GoldP dataset is more challenging than XQuAD and MLQA. The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines on EM or F1 of all 8 low-resource target languages. Significantly, XLM+S2DM_SP outperforms the XLM-100 baselines by 8.4%, 9.5% in EM for Finnish (fi), Russian(ru), respectively. The language families of these two languages are different from that of English. The evaluation results on these three datasets verify the effectiveness of our proposed method.\nIn Section 3.3, we theoretically analyze the generalization of our model. The results in three datasets show the effectiveness of five languages not included in the training target languages for S2DM. The five languages are Romanian (ro), Vietnamese (vi) in XQuAD and Bengali (bg), Swahili (sw), Telugu (te) in TyDi QAGoldP, which are resource-scarce and have different language families from English. Significantly, mBERT+S2DM_SP outperforms the mBERT baseline by 13.6% in EM for Swahili (sw)."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Ablation Study",
      "text" : "We further conducted an ablation study based on the mBERT and VGVAE model with different combinations of losses (introduced in the Section.3.2). The results are reported in Table 3. Our mBERT+S2DM_SP MRC model achieves the strongest performance among all variants, surpassing the model w/ all losses. According to the results shown in Figure 3, we can summarize that each loss is essential and suitable to our model.\nThe results without POS and STL loss (e.g., w/ CRL+SDL+WPL) on the MLQA dataset validate the effectiveness of our losses (POS or STL loss)\ntailored for capturing syntactic information. The performance of models that only contain two losses in CRL, SDL, and WPL drops significantly compared with the w/ CRL+SDL+WPL model. The results of models that only contain one of the losses in CRL, SDL drop slightly, but the EM of the model with only WPL is better than w/ CRL+WPL and w/ SDL+WPL, which further demonstrates the importance of the syntax-oriented loss. All ablation models do not exceed our best model, illustrating the importance of all our losses.\n5.2 Why Use a Siamese Network in S2DM?\nIn order to separate semantic information from PLMs, an alternative way is to train a single net-\nwork based on the VGVAE model as shown in Figure 4. Compared with S2DM, the single-network model does not use the CRL and SDL loss and only requires labeled monolingual data. Corresponding to S2DM, there are also two single-network variants: S2DM_single_POS and S2DM_single_SP. Since there is no explicit semantics learning across the source and target language, we conjecture that the single-network S2DM will affect the quality of learned semantic vectors and the degree of semantics-syntax decoupling. As shown in Table 5, the performance of the single-network S2DM is worse than the siamese-network model.\n5.3 Why S2DM Works?\nOur method mainly aims to reduce the potential negative impact of syntactic differences of languages in the zero-shot transfer process by explicitly isolating semantics from syntax in representations from multilingual pre-trained models. There-\nfore, we hope to obtain multilingual semantic representations with rich semantic information to guide the machine to read and understand texts. In order to examine (1) whether semantic vectors y in S2DM encode rich semantic information, and (2) whether semantics is sufficiently separated from syntax, and (3) whether semantic disentanglement can improve predicted answer spans in matching syntactic structures of the target language, we conducted additional experiments and analyses.\nHere we used three datasets of cross-lingual semantic textual similarity (STS) in SemEval-20172 to evaluate the quality of semantic vectors learned by S2DM. The three datasets are for Arabic to English (ar-en), Spanish to English (es-en), and Turkish to English (tr-en) cross-lingual STS. We report the results of our models in Figure 5 based on mBERT. We also evaluated learned syntactic vectors in cross-lingual STS, hoping that the performance gap between semantic vectors (i.e., y in S2DM) and syntactic vectors (i.e., z in S2DM) is as large as possible. As shown in Figure 5, disentangled semantic representations significantly improve Pearson correlation over the baseline in ar-en, es-en, and tr-en by 11.46%, 3.40%, 4.98%, respectively. Additionally, disentangled syntactic representations are negatively correlated to STS in most cases. These results suggest that disentangled semantic vectors indeed learn rich universal semantic information.\nWe visualize hidden representations of the last layer of mBERT and semantic representations of mBERT+S2DM_POS and mBERT+S2DM_SP in Figure 6, in which the parallel sentences are from a 15-way parallel corpus (Conneau et al., 2018). It is clear to see that disentangled semantic representations learned by S2DM make parallel sentences in 15 languages (semantically equivalent to each other) closer to one another in space, blending language boundaries clearly seen from mBERT\n2https://alt.qcri.org/semeval2017/ task1/\nrepresentations (Figure 6(a)). Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S2DM can efficiently disassociate semantics from syntax.\nFinally, we evaluated the degree of consistency to syntactic constituents of predicted answer spans. As described in Section 1, 23.15% of the nontransfer predicted correct answers violate syntactic constraint of the target language during the raw zero-shot cross-lingual transfer on BiPaR. By contrast, S2DM_POS and S2DM_SP drop this percentage to 12.98% and 6.60%, respectively. Moreover, on the entire test set of BiPaR (Jing et al., 2019) in Chinese, 93.27% answers predicted by S2DM_SP exactly span syntactic constituents, which is 8.14% higher than the mBERT model."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we have presented a novel multilingual MRC model for zero-shot cross-lingual transfer, which can disentangle semantic from syntactic representations and explicitly transfer semantic information from rich-resource language to low-resource languages, reducing the influence of syntactic differences between languages on the answer span prediction of the target language. To disassociate semantics from syntax in multilingual pre-trained representations, we propose the siamese semantic disentanglement model that semantics/syntax-oriented losses to guide latent variables to learn corresponding information. For low-resource languages without training data of decoupling model, we theoretically analyze and experiment verify the generalization of our multilingual MRC model. Further in-depth analyses suggest that the proposed S2DM can efficiently disentangle semantics from syntax and significantly improve syntactic consistency of answer predictions on the target language after zero-shot crosslingual transfer."
    } ],
    "references" : [ {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, pages 4623–4637.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "A multi-task approach for disentangling syntax and semantics in sentence representations",
      "author" : [ "Mingda Chen", "Qingming Tang", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Finding universal grammatical relations in multilingual BERT",
      "author" : [ "Ethan A. Chi", "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, pages 5564–5577.",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "Tydi QA: A benchmark for information-seeking question answering in typologically diverse languages",
      "author" : [ "Jonathan H. Clark", "Jennimaria Palomaki", "Vitaly Nikolaev", "Eunsol Choi", "Dan Garrette", "Michael Collins", "Tom Kwiatkowski." ],
      "venue" : "Trans. Assoc. Com-",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Proceedings of the 2019 Annual Conference on Neural Information Processing Systems, NeurIPS 2019, pages 7057–7067.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel R. Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Method-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Cross-lingual machine reading comprehension",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Cui et al\\.,? 2019",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Zero-shot reading comprehension by crosslingual transfer learning with multi-lingual language representation model",
      "author" : [ "Tsung-Yuan Hsu", "Chi-Liang Liu", "Hung-yi Lee." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Hsu et al\\.,? 2019",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2019
    }, {
      "title" : "XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of the 37th International Conference on",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P. Xing." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 1587–1596.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving cross-lingual reading comprehension with self-training",
      "author" : [ "Wei-Cheng Huang", "Chien-yu Huang", "Hung-yi Lee." ],
      "venue" : "arXiv preprint arXiv:2105.03627.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "BiPaR: A bilingual parallel dataset for multilingual and cross-lingual reading comprehension on novels",
      "author" : [ "Yimin Jing", "Deyi Xiong", "Yan Zhen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Jing et al\\.,? 2019",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2019
    }, {
      "title" : "MLQA: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick S.H. Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Calibrenet: Calibration networks for multilingual sequence labeling",
      "author" : [ "Shining Liang", "Linjun Shou", "Jian Pei", "Ming Gong", "Wanli Zuo", "Daxin Jiang." ],
      "venue" : "WSDM ’21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtu-",
      "citeRegEx" : "Liang et al\\.,? 2021",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2021
    }, {
      "title" : "Cross-lingual machine reading comprehension with language branch knowledge distillation",
      "author" : [ "Junhao Liu", "Linjun Shou", "Jian Pei", "Ming Gong", "Min Yang", "Daxin Jiang." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, pages",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Chinese semantic text similarity trainning dataset",
      "author" : [ "Shancheng Tang", "Yunyue Bai", "Fuyu Ma." ],
      "venue" : "Xi’an University of Science and Technology.",
      "citeRegEx" : "Tang et al\\.,? 2016",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving low-resource reading comprehension via cross-lingual transposition rethinking",
      "author" : [ "Gaochen Wu", "Bin Xu", "Yuxin Qin", "Fei Kong", "Bangchang Liu", "Hongwen Zhao", "Dejie Chang." ],
      "venue" : "arXiv preprint arXiv:2107.05002.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing",
      "author" : [ "Pengcheng Yin", "Chunting Zhou", "Junxian He", "Graham Neubig." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, A-",
      "citeRegEx" : "Yin et al\\.,? 2018",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2018
    }, {
      "title" : "Enhancing answer boundary detection for multilingual machine reading comprehension",
      "author" : [ "Fei Yuan", "Linjun Shou", "Xuanyu Bai", "Ming Gong", "Yaobo Liang", "Nan Duan", "Yan Fu", "Daxin Jiang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the As-",
      "citeRegEx" : "Yuan et al\\.,? 2020",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal dependencies 2.7. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles Universi",
      "author" : [ "Daniel Zeman", "Joakim Nivre", "Mitchell Abrams" ],
      "venue" : null,
      "citeRegEx" : "Zeman et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zeman et al\\.",
      "year" : 2020
    }, {
      "title" : "Syntax-infused variational autoencoder for text generation",
      "author" : [ "Xinyuan Zhang", "Yi Yang", "Siyang Yuan", "Dinghan Shen", "Lawrence Carin." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, pages 2069–",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Multilingual pre-trained language models (PLMs) (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks.",
      "startOffset" : 48,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "Multilingual pre-trained language models (PLMs) (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks.",
      "startOffset" : 48,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Multilingual pre-trained language models (PLMs) (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks.",
      "startOffset" : 48,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : "This type of multilingual MRC model could roughly detect answer spans but fail to predict the precise boundaries of answers (Yuan et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "Moreover, the recent development in multilingual MRC evaluation datasets (Artetxe et al., 2020; Lewis et al., 2020; Clark et al., 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al.",
      "startOffset" : 73,
      "endOffset" : 135
    }, {
      "referenceID" : 15,
      "context" : "Moreover, the recent development in multilingual MRC evaluation datasets (Artetxe et al., 2020; Lewis et al., 2020; Clark et al., 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al.",
      "startOffset" : 73,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "Moreover, the recent development in multilingual MRC evaluation datasets (Artetxe et al., 2020; Lewis et al., 2020; Clark et al., 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al.",
      "startOffset" : 73,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : ", 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al., 2020; Liu et al., 2020; Huang et al., 2021; Wu et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : ", 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al., 2020; Liu et al., 2020; Huang et al., 2021; Wu et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 183
    }, {
      "referenceID" : 22,
      "context" : ", 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al., 2020; Liu et al., 2020; Huang et al., 2021; Wu et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 183
    }, {
      "referenceID" : 17,
      "context" : ", 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al., 2020; Liu et al., 2020; Huang et al., 2021; Wu et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 183
    }, {
      "referenceID" : 13,
      "context" : ", 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al., 2020; Liu et al., 2020; Huang et al., 2021; Wu et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 183
    }, {
      "referenceID" : 20,
      "context" : ", 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al., 2020; Liu et al., 2020; Huang et al., 2021; Wu et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "(a) An example from XQuAD (Artetxe et al., 2020) where the ground-truth answer is a syntactic constituent.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by the model transferred from English to Chinese violates syntactic constituent boundaries.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "Based on the finding that 70% of the answer spans are language-specific phrases in MLQA (Lewis et al., 2020) dataset, Yuan et al.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "On the bilingual parallel MRC corpus BiPaR (Jing et al., 2019), we have compared two MRC models: a monolingual MRC model trained on the Chinese data of BiPaR vs.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : "Disentangled Representation Learning Recently, there has been a growing amount of work on learning disentangled latent representations in NLP tasks (Zhang et al., 2019; Hu et al., 2017; Yin et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 203
    }, {
      "referenceID" : 12,
      "context" : "Disentangled Representation Learning Recently, there has been a growing amount of work on learning disentangled latent representations in NLP tasks (Zhang et al., 2019; Hu et al., 2017; Yin et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 203
    }, {
      "referenceID" : 21,
      "context" : "Disentangled Representation Learning Recently, there has been a growing amount of work on learning disentangled latent representations in NLP tasks (Zhang et al., 2019; Hu et al., 2017; Yin et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 203
    }, {
      "referenceID" : 0,
      "context" : "To verify the effectiveness of our multilingual MRC model, we conducted experiments on three multilingual question answering benchmarks: XQuAD (Artetxe et al., 2020) consists of 11 languages corpus translated from the SQuAD v1.",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "1 (Rajpurkar et al., 2016) development set, include : Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "MLQA (Lewis et al., 2020) consists of over 5K extractive MRC instances in 7 languages: English (en), Arabic (ar), German (de), Spanish (es), Hindi (hi), Vietnamese (vi) and Chinese (zh).",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "TyDi QA-GoldP is the gold passage task in TyDi QA (Clark et al., 2020) covers 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "leading to 3 and 2 times less lexical overlap than XQuAD and MLQA, respectively (Hu et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "We chose data from the Chinese semantic textual similarity (STS) task (Tang et al., 2016) as the development set.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "1 (Rajpurkar et al., 2016) and evaluated the test data of the three multilingual MRC datasets.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "We visualize hidden representations of the last layer of mBERT and semantic representations of mBERT+S2DM_POS and mBERT+S2DM_SP in Figure 6, in which the parallel sentences are from a 15-way parallel corpus (Conneau et al., 2018).",
      "startOffset" : 207,
      "endOffset" : 229
    }, {
      "referenceID" : 14,
      "context" : "on the entire test set of BiPaR (Jing et al., 2019) in Chinese, 93.",
      "startOffset" : 32,
      "endOffset" : 51
    } ],
    "year" : 0,
    "abstractText" : "Multilingual pre-trained models are able to zero-shot transfer knowledge from richresource to low-resource languages in machine reading comprehension (MRC). However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language. In this paper, we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model (SDM) to disassociate semantics from syntax in representations learned by multilingual pre-trained models. To explicitly transfer only semantic knowledge to the target language, we propose two groups of losses tailored for semantic and syntactic encoding and disentanglement. Moreover, we theoretically analyze the generalization of our decoupling model, even for low-resource languages without training corpus. Experimental results on three multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the effectiveness of our proposed approach over models based on mBERT and XLM-100.",
    "creator" : null
  }
}
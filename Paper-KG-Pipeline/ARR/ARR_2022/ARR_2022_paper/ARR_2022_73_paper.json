{
  "name" : "ARR_2022_73_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Restoring Hebrew Diacritics Without a Dictionary",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The vast majority of modern Hebrew texts are written in a letter-only version of the Hebrew script, one which omits the diacritics present in the full diacritized, or dotted variant.1 Since most vowels are encoded via diacritics, the pronunciation of words in the text is left underspecified, and a considerable mass of tokens becomes ambiguous. This ambiguity forces readers and learners to infer the intended reading using syntactic and semantic context, as well as common sense (Bentin and Frost, 1987; Abu-Rabia, 2001). In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007; Goldberg and Elhadad, 2010; Tsarfaty et al., 2019). As an example, the sentence in Table 1 (a) will be resolved by a typical reader as (b) in most reasonable contexts, knowing that the word “softly” may characterize landings. In contrast, an automatic system processing Hebrew text may not be as sensitive to this kind of grammatical knowledge and instead interpret the undotted token as the more frequent word in (c), harming downstream performance.\n1Also known as pointed text, or via the Hebrew term for the diacritic marks, nikkud/niqqud.\nOne possible way to overcome this problem is by adding diacritics to undotted text, or dotting, implemented using data-driven algorithms trained on dotted text. Obtaining such data is not trivial, even given correct pronunciation: the standard Tiberian diacritic system contains several sets of identicallyvocalized forms, so while most Hebrew speakers easily read dotted text, they are unable to produce it. Moreover, the process of manually adding diacritics in either handwritten script or through digital input devices is mechanically cumbersome. Thus, the overwhelming majority of modern Hebrew text is undotted, and manually dotting it requires expertise. The resulting scarcity of available dotted text in modern Hebrew contrasts with Biblical and Rabbinical texts which, while dotted, manifest a very different language register. This state of affairs allows individuals and companies to offer dotting as paid services, either by experts or automatically, e.g. the Morfix engine by Melingo.2 Such usage practices also force a disconnect in the NLP pipeline, requiring an API call into an external service whose parameters cannot be updated.\nExisting computational approaches to dotting\n2https://nakdan.morfix.co.il/\nare manifested as complex, multi-resourced systems which perform morphological analysis on the undotted text and look undotted words up in handcrafted dictionaries as part of the dotting process. Dicta’s Nakdan (Shmidman et al., 2020), the current state-of-the-art, applies such methods in addition to applying multiple neural networks over different levels of the text, requiring manual annotation not only for dotting but also for morphology. Among the resources it uses are a diacritized corpus of 3M tokens and a POS-tagged corpus of 300K tokens. Training the model takes several weeks.3\nIn this work, we set out to simplify the dotting task as much as possible to standard modules. Our system, NAKDIMON, accepts the undotted character sequence as its input, consults no external resources or lexical components, and produces diacritics for each character, resulting in dotted text whose quality is comparable to that of the commercial Morfix, on both character-level and word-level accuracy. Our model is easy to integrate within larger systems that perform end-to-end Hebrew processing tasks, as opposed to the existing proprietary dotters. To our knowledge, this is the first attempt at a “light” model for Hebrew dotting since early HMM-based systems (Kontorovich, 2001; Gal, 2002). We introduce a novel test set for Modern Hebrew dotting, derived from larger and more diverse sources than existing datasets. In experiments over our novel dataset, we show that our system is particularly useful in the main use case of modern dotting, which is to convey the desired pronunciation to a reader, and that the errors it makes should be more easily detectable by non-professionals than Dicta’s.4"
    }, {
      "heading" : "2 Dotting as Sequence Labeling",
      "text" : "The input to the dotting task consists of a sequence of characters. Each of the characters is assigned three values, from three separate diacritic categories: one category for the dot distinguishing shin (!שׁ) from sin (!שׂ), two consonants sharing a base character !ש; another for the presence of dagesh/mappiq, a central dot affecting pronunciation of some consonants, e.g. !פּ /p/ from !|פ /f/, but also present elsewhere; and one for all other diacritic marks, which mostly determine vocalization, e.g. !´ד /da/ vs. !«ד /de/. Diacritics of different categories may co-occur on single letters, e.g. !µ , or\n3Private communication. 4The system is a available at anonymized.org.\nmay be absent altogether.\nFull script Hebrew script written without intention of dotting typically employs a compensatory variant known colloquially as full script (ktiv male, !אלמ ביתכ), which adds instances of the letters !י and !ו in some places where they can aid pronunciation, but are incompatible with the rules for dotted script. In our formulation of dotting as a sequence tagging problem, and in collecting our test set from raw text, these added letters may conflict with the dotting standard. For the sake of input integrity, and unlike some other systems, we opt not to remove these characters, but instead employ a dotting policy consistent with full script. See Appendix A for further details.\nNew test set Shmidman et al. (2020) provide a benchmark dataset for dotting modern Hebrew documents. However, it is relatively small and nondiverse: all 22 documents in the dataset originate in a single source, namely Hebrew Wikipedia articles. Therefore, we created a new test set5 from a larger variety of texts, including high-quality Wikipedia articles and edited news stories, as well as usergenerated blog posts. This set consists of ten documents from each of eleven sources (5x Dicta’s test set), and totals 20,796 Hebrew tokens, roughly 3.5x Dicta’s."
    }, {
      "heading" : "3 Character-LSTM Dotter",
      "text" : "NAKDIMON embeds the input characters and passes them through a two-layer BiLSTM (Hochreiter and Schmidhuber, 1997). The LSTM output is fed into a single linear layer, which then feeds three linear layers, one for each diacritic category (see §2). Each character then receives a prediction for each category independently, and decoding is performed greedily. In training, we sum the cross-entropy loss from all categories. Trivial decisions, such as the label for the shin/sin diacritic for any non-!ש letter, are masked.\nTraining corpora Dotted modern Hebrew text is scarce, since speakers usually read and write undotted text, with the occasional diacritic added for disambiguation when context does not suffice. As we are unaware of legally-obtainable dotted modern corpora, we use a combination of dotted pre-modern texts and semi-automatically dotted modern sources to train NAKDIMON:\n5Provided as supplementary material.\nThe PRE-MODERN portion is obtained from two main sources: A combination of late pre-modern text from Project Ben-Yehuda, mostly texts from the late 19th century and the early 20th century;6 and rabbinical texts from the medieval period, the most important of which is Mishneh Torah.7 This portion contains roughly 2.6M Hebrew tokens, most of which are dotted, with a varying level of accuracy, varying dotting styles, and varying degree of similarity to Modern Hebrew.\nThe AUTOMATIC portion contains 547 short stories taken from the short story project.8 The stories are dotted using Dicta without manual validation. The corpus contains roughly 1.3M Hebrew tokens.\nLastly, the MODERN portion contains manually collected text in Modern Hebrew, mostly from undotted sources, which we dot using Dicta and follow up by manually fixing errors, either using Dicta’s API or via automated scripts which catch common mistakes. We use the same technique and style for dotting this corpus as we do for our test corpus (§2), but the documents were collected in different ways. We made an effort to collect a diverse set of sources: news, opinion columns, paragraphs from books, short stories, Wikipedia articles, governmental publications, blog posts and forums expressing various domains and voices, and more. Our MODERN corpus contains roughly 300,000 Hebrew tokens, and is much more consistent and similar to the expectation of a native Hebrew speaker than the PRE-MODERN or the AUTOMATIC corpora, and more accurately dotted than the AUTOMATIC corpus. The sources and statistics of this dataset are presented in Table 2."
    }, {
      "heading" : "4 Experiments",
      "text" : "We compare the performance of NAKDIMON against Dicta (retrieved 2022-1-9), Snopi,9 and Morfix (Kamir et al., 2002), on our new test set (§2).10 We report the following metrics: decision accuracy (DEC) is computed over the entire set of individual possible decisions: dagesh/mappiq for letters that allow it, sin/shin dot for the letter !ש, and all other diacritics for letters that allow them; char-\n6Obtained from the Ben-Yehuda Project www.benyehuda.org.\n7Obtained from Project Mamre www.mechon-mamre. org.\n8www.shortstoryproject.com/he/ 9http://www.nakdan.com/Nakdan.aspx\n10Results on Dicta’s test set (Shmidman et al., 2020) are presented in Appendix C.\nacter accuracy (CHA) is the portion of characters in the text that end up in their intended final form (which may combine two or three decisions, e.g. dagesh + vowel); word accuracy (WOR) is the portion of words with no mistakes; and vocalization accuracy (VOC) is the portion of words where any dotting errors do not cause incorrect pronunciation among mainstream Israeli Hebrew speakers.11\nWe train NAKDIMON over PRE-MODERN for a single epoch, followed by two epochs over the AUTOMATIC corpus, and then by three epochs over the MODERN corpus. We pre-process the input by removing all but Hebrew characters, spaces and punctuation; digits are converted to a dedicated symbol, as are Latin characters. We strip all existing diacritic marks. We split the documents into chunks of at most 80 characters12 bounded at whitespace, ignoring sentence boundaries. We optimize using Adam (Kingma and Ba, 2014). For the PRE-MODERN corpus we use a cyclical learning rate schedule (Smith, 2017) which we found to be more useful than a constant learning rate. For each of AUTOMATIC and MODERN corpora we use epoch-wise decreasing learning rate. Based on tuning experiments, we set both character embedding and LSTM hidden dimensions to 400, and apply a dropout rate of 0.1. Further tuning experiments are detailed in Appendix B, and an evaluation of a preliminary version of NAKDIMON over the Dicta test set is in Appendix C.\n11These are: the sin/shin dot, vowel distinctions across the a/e/i/o/u/null sets, and dagesh in the !ב/ !|כ/ !|פ characters. We do not distinguish between kamatz gadol/kamatz katan, and schwa is assumed to always be null.\n12Selected to trade off hardware capacity against effective modeling."
    }, {
      "heading" : "4.1 Results",
      "text" : "We provide document-level macro-averaged accuracy percentage results for a single run over our test set in Table 3. NAKDIMON outperforms Morfix on character-level metrics but not on word-level metrics, mostly since Morfix ignores certain words altogether, incurring errors on multiple characters.\nWe note the substantial improvement our model achieves on the VOC metric compared to the WOR metric: 18.43% of word-level errors are attributable to vocalization-agnostic dotting, compared to 13.80% for Dicta and 10.41% for Snopi (but 20.91% for Morfix). Considering that the central use case for dotting modern Hebrew text is to facilitate pronunciation to learners and for reading, and that undotted homograph ambiguity typically comes with pronunciation differences, we believe this measure to be no less important than WOR."
    }, {
      "heading" : "4.2 Error analysis",
      "text" : "In Table 4 we present examples of words dotted incorrectly, or correctly, only by NAKDIMON, compared with Morfix and Dicta. The largest category for NAKDIMON-only errors (∼18% of 90 sampled) are ones where a fused preposition+determiner character is dotted to only include the preposition, perhaps due to its inability to detect the explicit determiner clitic !ה in neighboring words, on which the complex systems apply morphological segmentation. In other cases (∼15%), NAKDIMON creates unreadable vocalization sequences, as it has no lexical component and is decoded greedily. These types of errors are more friendly to the typical use cases of a dotting system, as they are likely to stand out to a reader. In contrast, a large portion of cases where NAKDIMON was exclusively correct (∼13% of 152) are foreign names and terms. This may be the result of such words not yet appearing in dictionaries, or not being easily separable from an adjoining clitic, while character-level information can capture pronunciation patterns from similar words (e.g. !Nופלט ‘telephone’, for the example)."
    }, {
      "heading" : "5 Related Work",
      "text" : "Existing work on diacritizing Hebrew is not common, and all efforts build on word-level features. In Arabic, diacritization serves a comparable purpose to that in Hebrew, but not exclusively: most diacritic marks differentiate letters from each other (which only the sin/shin dot does in Hebrew), while vocalization marks are in a one-to-one relationship with their phonetic realizations. Dictionary-less Arabic diacritization has been attempted using a 3- layer Bi-LSTM (Belinkov and Glass, 2015). Abandah et al. (2015) use a Bi-LSTM where characters are assigned either one or more diacritic symbols. Our system differs from theirs by virtue of separating the diacritization categories. Mubarak et al. (2019) tackled Arabic diacritization as a sequenceto-sequence problem, tasking the model with reproducing the characters as well as the marks.\nZalmout and Habash (2017) have made the case against RNN-only systems, arguing for the importance of morphological analyzers in Arabic NLP systems. We concede that well-curated systems perform better than ours, noting that they are difficult to train for individual use-cases and are burdensome to deploy as services."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Learning directly from plain diacritized text can go a long way, even with relatively limited resources. NAKDIMON demonstrates that a simple architecture for diacritizing Hebrew text as a sequence tagging problem can achieve performance on par with much more complex systems. We also introduce and release a corpus of dotted Hebrew text, as well as a source-balanced test set. In the future, we wish to evaluate the utility of dotting as a feature for downstream tasks such as question answering and speech generation, taking advantage of the fact that our simplified model can be easily integrated in an end-to-end Hebrew processing system."
    }, {
      "heading" : "A Full Script Reconciliation",
      "text" : "We apply the following resolution tactics for added letters in undotted text: (a) We almost never remove or add letters to the original text (unless it is completely undiacritizable). (b) We keep dagesh in letters that follow a shuruk which replaces a kubuts, and similarly for yod (hirik male replacing hirik haser). (c) When we have double vav or double yod, the second letter is usually left undotted, except when it is impossible to have the correct vocalization this way.\nResolving ktiv haser discrepancies from Morfix outputs is done by adding missing vowel letters, or removing superfluous vowel letters, in such a way that would not count as an error if it is correct according to Academy regulations."
    }, {
      "heading" : "B Development Experiments",
      "text" : "While developing NAKDIMON, we performed several evaluations over a held-out validation set of 40 documents with 27,681 tokens, on which Dicta performs at 91.56% WOR accuracy. Figure 1 shows the favorable effect of training NAKDIMON over an increasing amount of MODERN text.\nWe tried to further improve NAKDIMON by initializing its parameters from a language model trained to predict masked characters in a large undotted Wikipedia corpus (440MB, 30% mask rate), but were only able to achieve an improvement of 0.07%. Attempted architectural modifications, including substituting a Transformer (Vaswani et al., 2017) for the LSTM; adding a CRF layer to the decoding process; and adding a residual connection between the character LSTM layers, yielded no substantial benefits in these experiments. Similarly, varying the number of LSTM layers between 2 and 5 (keeping the total number of parameters roughly constant, close to the 5, 313, 223 parameters of our final model) has little to no impact on the accuracy\non the validation set."
    }, {
      "heading" : "C Dicta Test Set",
      "text" : "We present results for the Dicta test set in Table 5. In order to provide fair comparison and to preempt overfitting on this test data, we ran this test in a preliminary setup on a variant of NAKDIMON which was not tuned or otherwise unfairly trained. This system, NAKDIMON0, differs from our final variant in three main aspects: it is not trained on the Dicta portion of our training corpus (§3), it is not trained on the AUTOMATIC corpus, and it employs a residual connection between the two character Bi-LSTM layers. Testing on the Dicta test set required some minimal evaluation adaptations resulting from encoding constraints (for example, we do not distinguish between kamatz katan and kamatz gadol). Thus, we copy the results reported in Shmidman et al. (2020) as well as our replication.\nWe see that the untuned NAKDIMON0 performs on par with the proprietary Morfix, which uses word-level dictionary data, consistent with our main results on our novel test set."
    } ],
    "references" : [ {
      "title" : "Automatic diacritization of Arabic text using recurrent neural networks",
      "author" : [ "Gheith Abandah", "Alex Graves", "Balkees Al-Shagoor", "Alaa Arabiyat", "Fuad Jamour", "Majid Al-Taee." ],
      "venue" : "International Journal on Document Analysis and Recognition (IJDAR),",
      "citeRegEx" : "Abandah et al\\.,? 2015",
      "shortCiteRegEx" : "Abandah et al\\.",
      "year" : 2015
    }, {
      "title" : "The role of vowels in reading semitic scripts: Data from Arabic and Hebrew",
      "author" : [ "Salim Abu-Rabia." ],
      "venue" : "Reading and Writing, 14(1-2):39–59.",
      "citeRegEx" : "Abu.Rabia.,? 2001",
      "shortCiteRegEx" : "Abu.Rabia.",
      "year" : 2001
    }, {
      "title" : "Arabic diacritization with recurrent neural networks",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2281– 2285, Lisbon, Portugal. Association for Computa-",
      "citeRegEx" : "Belinkov and Glass.,? 2015",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2015
    }, {
      "title" : "Processing lexical ambiguity and visual word recognition in a deep orthography",
      "author" : [ "Shlomo Bentin", "Ram Frost." ],
      "venue" : "Memory & Cognition, 15(1):13–23.",
      "citeRegEx" : "Bentin and Frost.,? 1987",
      "shortCiteRegEx" : "Bentin and Frost.",
      "year" : 1987
    }, {
      "title" : "An HMM approach to vowel restoration in Arabic and Hebrew",
      "author" : [ "Ya’akov Gal" ],
      "venue" : "In Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages,",
      "citeRegEx" : "Gal.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gal.",
      "year" : 2002
    }, {
      "title" : "Easy-first dependency parsing of modern Hebrew",
      "author" : [ "Yoav Goldberg", "Michael Elhadad." ],
      "venue" : "Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103–107, Los Angeles, CA, USA. Associa-",
      "citeRegEx" : "Goldberg and Elhadad.,? 2010",
      "shortCiteRegEx" : "Goldberg and Elhadad.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A comprehensive NLP system for modern standard Arabic and modern Hebrew",
      "author" : [ "Dror Kamir", "Naama Soreq", "Yoni Neeman." ],
      "venue" : "Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages, Philadelphia, Pennsylvania,",
      "citeRegEx" : "Kamir et al\\.,? 2002",
      "shortCiteRegEx" : "Kamir et al\\.",
      "year" : 2002
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Problems in Semitic NLP: Hebrew vocalization using HMMs",
      "author" : [ "Leonid Kontorovich." ],
      "venue" : "Problems in Semitic NLP, NIPS Workshop on Machine Learning Methods for Text and Images.",
      "citeRegEx" : "Kontorovich.,? 2001",
      "shortCiteRegEx" : "Kontorovich.",
      "year" : 2001
    }, {
      "title" : "Highly effective Arabic diacritization using sequence to sequence modeling",
      "author" : [ "Hamdy Mubarak", "Ahmed Abdelali", "Hassan Sajjad", "Younes Samih", "Kareem Darwish." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Mubarak et al\\.,? 2019",
      "shortCiteRegEx" : "Mubarak et al\\.",
      "year" : 2019
    }, {
      "title" : "Morphological disambiguation of Hebrew: A case study in classifier combination",
      "author" : [ "Danny Shacham", "Shuly Wintner." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
      "citeRegEx" : "Shacham and Wintner.,? 2007",
      "shortCiteRegEx" : "Shacham and Wintner.",
      "year" : 2007
    }, {
      "title" : "Nakdan: Professional Hebrew diacritizer",
      "author" : [ "Avi Shmidman", "Shaltiel Shmidman", "Moshe Koppel", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 197–",
      "citeRegEx" : "Shmidman et al\\.,? 2020",
      "shortCiteRegEx" : "Shmidman et al\\.",
      "year" : 2020
    }, {
      "title" : "Cyclical learning rates for training neural networks",
      "author" : [ "Leslie N Smith." ],
      "venue" : "2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 464–472. IEEE.",
      "citeRegEx" : "Smith.,? 2017",
      "shortCiteRegEx" : "Smith.",
      "year" : 2017
    }, {
      "title" : "What’s wrong with Hebrew NLP? and how to make it right",
      "author" : [ "Reut Tsarfaty", "Shoval Sadde", "Stav Klein", "Amit Seker." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Tsarfaty et al\\.,? 2019",
      "shortCiteRegEx" : "Tsarfaty et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Don’t throw those morphological analyzers away just yet: Neural morphological disambiguation for Arabic",
      "author" : [ "Nasser Zalmout", "Nizar Habash." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 704–",
      "citeRegEx" : "Zalmout and Habash.,? 2017",
      "shortCiteRegEx" : "Zalmout and Habash.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "This ambiguity forces readers and learners to infer the intended reading using syntactic and semantic context, as well as common sense (Bentin and Frost, 1987; Abu-Rabia, 2001).",
      "startOffset" : 135,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "This ambiguity forces readers and learners to infer the intended reading using syntactic and semantic context, as well as common sense (Bentin and Frost, 1987; Abu-Rabia, 2001).",
      "startOffset" : 135,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007; Goldberg and Elhadad, 2010; Tsarfaty et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 232
    }, {
      "referenceID" : 5,
      "context" : "In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007; Goldberg and Elhadad, 2010; Tsarfaty et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 232
    }, {
      "referenceID" : 14,
      "context" : "In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007; Goldberg and Elhadad, 2010; Tsarfaty et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 232
    }, {
      "referenceID" : 12,
      "context" : "Dicta’s Nakdan (Shmidman et al., 2020), the current state-of-the-art, applies such methods in addition to applying multiple neural networks over",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "To our knowledge, this is the first attempt at a “light” model for Hebrew dotting since early HMM-based systems (Kontorovich, 2001; Gal, 2002).",
      "startOffset" : 112,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "To our knowledge, this is the first attempt at a “light” model for Hebrew dotting since early HMM-based systems (Kontorovich, 2001; Gal, 2002).",
      "startOffset" : 112,
      "endOffset" : 142
    }, {
      "referenceID" : 6,
      "context" : "and passes them through a two-layer BiLSTM (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 43,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "We compare the performance of NAKDIMON against Dicta (retrieved 2022-1-9), Snopi,9 and Morfix (Kamir et al., 2002), on our new test set (§2).",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "aspx (10)Results on Dicta’s test set (Shmidman et al., 2020) are presented in Appendix C.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "For the PRE-MODERN corpus we use a cyclical learning rate schedule (Smith, 2017) which we found to be more useful than a constant learning rate.",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "Dictionary-less Arabic diacritization has been attempted using a 3layer Bi-LSTM (Belinkov and Glass, 2015).",
      "startOffset" : 80,
      "endOffset" : 106
    } ],
    "year" : 0,
    "abstractText" : "We demonstrate that it is feasible to diacritize Hebrew script without any human-curated resources other than plain diacritized text. We present NAKDIMON, a two-layer characterlevel LSTM, that performs on par with much more complicated curation-dependent systems, across a diverse array of modern Hebrew sources. Its lightweight architecture allows it to be integrated into end-to-end Hebrew processing systems with little overhead, and with the capacity for further tuning on downstream tasks.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_278_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, abundant research have been performed on modelling variations other than the input text in synthesized speech such as background noise, speaker information, and prosody, as those directly influence the naturalness and expressiveness of the generated audio. Prosody, as the focus of this paper, collectively refers to the stress, intonation, and rhythm in speech, and has been an increasingly popular research aspect in end-to-end TTS systems (van den Oord et al., 2016; Wang et al., 2017; Stanton et al., 2018; Elias et al., 2021; Chen et al., 2021). Some previous work captured prosody features explicitly using either style tokens or variational autoencoders (VAEs) (Kingma and Welling, 2014; Hsu et al., 2019a) which encapsulate prosody information into latent representations. Recent work\nachieved fine-grained prosody modelling and control by extracting prosody features at phoneme or word-level (Lee and Kim, 2019; Sun et al., 2020a,b). However, the VAE-based TTS system lacks control over the latent space where the sampling is performed from a standard Gaussian prior during inference. Therefore, recent research (Dahmani et al., 2019; Karanasou et al., 2021) employed a conditional VAE (CVAE) (Sohn et al., 2015) to synthesize speech from a conditional prior. Meanwhile, pre-trained language model (LM) such as bidirectional encoder representation for Transformers (BERT) (Devlin et al., 2019) has also been applied to TTS systems (Hayashi et al., 2019; Kenter et al., 2020; Jia et al., 2021; Futamata et al., 2021; Cong et al., 2021) to estimate prosody attributes implicitly from pre-trained text representations within the utterance or the segment. Efforts have been devoted to include cross-utterance information in the input features to improve the prosody modelling of auto-regressive TTS (Xu et al., 2021).\nTo generate more expressive prosody, while maintaining high fidelity in synthesized speech, a cross-utterance conditional VAE (CUC-VAE) component is proposed, which is integrated into and jointly optimised with FastSpeech 2 (Ren et al., 2021), a commonly used non-autoregressive end-toend TTS system. Specifically, the CUC-VAE TTS system consists of cross-utterance embedding (CUembedding) and cross-utterance enhanced CVAE (CU-enhanced CVAE). The CU-embedding takes BERT sentence embeddings from surrounding utterances as inputs and generates phoneme-level CUembedding using a multi-head attention (Vaswani et al., 2017) layer where attention weights are derived from the encoder output of each phoneme as well as the speaker information. The CU-enhanced CVAE is proposed to improve prosody variation and to address the inconsistency between the standard Gaussian prior, which the VAE-based TTS system is sampled from, and the true prior of\nspeech. Specifically, the CU-enhanced CVAE is a fine-grained VAE that estimates the posterior of latent prosody features for each phoneme based on acoustic features, cross-utterance embedding, and speaker information. It improves the encoder of standard VAE with an utterance-specific prior. To match the inference with training, the utterancespecific prior, jointly optimised with the system, is conditioned on the output of CU-embedding. Latent prosody features are sampled from the derived utterance-specific prior instead of a standard Gaussian prior during inference.\nThe proposed CUC-VAE TTS system was evaluated on the LJ-Speech read English data and the LibriTTS English audiobook data. In addition to the sample naturalness measured via subjective listening tests, the intelligibility is measured using word error rate (WER) from an automatic speech recognition (ASR) system, and diversity in prosody was measured by calculating standard deviations of prosody attributes among all generated audio samples of an utterance. Experimental results showed that the system with CUC-VAE achieved a much better prosody diversity while improving both the naturalness and intelligibility compared to the standard FastSpeech 2 baseline and two variants.\nThe rest of this paper is organised as follows. Section 2 introduces the background and related work. Section 3 illustrates the proposed CUC-VAE TTS system. Experimental setup and results are shown in Section 4 and Section 5, with conclusions in Section 6."
    }, {
      "heading" : "2 Background",
      "text" : "Non-Autoregressive TTS. Promising progress has taken place in non-autoregressive TTS systems to synthesize audio with high efficiency and high fidelity thank to the advancement in deep learning. A non-autoregressive TTS system maps the input text sequence into an acoustic feature or waveform sequence without using the autoregressive decomposition of output probabilities. FastSpeech (Ren et al., 2019) and ParaNet (Peng et al., 2019) requires distillation from an autoregressive model, while more recent non-autoregressive TTS systems, including FastPitch (La’ncucki, 2021), AlignTTS (Zeng et al., 2020) and FastSpeech 2 (Ren et al., 2021), do not rely on any form of knowledge distillation from a pre-trained TTS system. In this paper, the proposed CUC-VAE TTS system is based on FastSpeech 2. FastSpeech 2\nreplaces the knowledge distillation for the length regulator in FastSpeech with mean-squared error training based on duration labels, which are obtained from frame-to-phoneme alignment to simplify the training process. Additionally, FastSpeech 2 predicts pitch and energy from the encoder output, which is also supervised with pitch contours and L2-norm of signal amplitudes as labels respectively. The pitch and energy prediction injects additional prosody information, which improves the naturalness and expressiveness in the synthesized speech.\nPre-trained Representation in TTS. It is believed that prosody can also be inferred from language information in both current and surrounding utterances (Shen et al., 2018; Fang et al., 2019; Xu et al., 2021; Zhou et al., 2021). Such information is often entailed in vector representations from a pre-trained LM, such as BERT (Devlin et al., 2019). Some existing work incorporated BERT embeddings at word or subword-level into autoregressive TTS models (Shen et al., 2018; Fang et al., 2019).More recent work (Xu et al., 2021) used the chunked and paired sentence patterns from BERT. Besides, a relational gated graph network with pretrained BERT embeddings as node inputs (Zhou et al., 2021) was used to extract word-level semantic representations, thus enhancing expressiveness.\nVAEs in TTS. VAEs have been widely adopted in TTS systems to explicit model prosody variation. The training objective of VAE is to maximise pθ(x), the data likelihood parameterised by θ, which can be regarded as the marginalisation w.r.t. the latent vector z as shown in Eq. (1).\npθ(x) = ∫ pθ(x | z)p(z)dz. (1)\nTo make this calculation tractable, the marginalisation is approximated using evidence lower bound (ELBO):\nLELBO(x) = Eqϕ(z|x)[log pθ(x|z)] − βDKL (qϕ(z|x)∥p(z)) , (2)\nwhere qϕ(z|x) is the posterior distribution of the latent vector parameterized by ϕ, β is a hyperparameter, and DKL(·) is the Kullback-Leibler divergence. The first term measures the expected reconstruction performance of the data from the latent vector and is approximated by Monte Carlo sampling of z according to the posterior distribution. The reparameterization trick is applied to make the sampling differentiable. The second term\nencourages the posterior distribution to approach the prior distribution which is sampled from during inference, and β weighs this term’s contribution.\nA large body of previous work on VAE-based TTS used VAEs to capture and disentangle data variations in different aspects in the latent space. Works by Akuzawa et al. (2018) leveraged VAE to model the speaking style of an utterance. Meanwhile, Hsu et al. (2019a,b) explored the disentanglement between prosody variation and speaker information using VAE together with adversarial training. Recently, fine-grained VAE (Sun et al., 2020a,b) was adopted to model prosody in the latent space for each phoneme or word. Moreover, vector-quantised VAE was also applied to discrete duration modelling by Yasuda et al. (2021).\nCVAE is a variant of VAE when the data generation is conditioned on some other information y. In CVAE, both prior and posterior distributions are conditioned on additional variables, and the data likelihood calculation is modified as shown below:\npθ(x | y) = ∫ pθ(x | z,y)pϕ(z | y)dz. (3)\nSimilar to VAE, this intractable calculation can be converted to the ELBO form as\nLELBO(x | y) = Eqϕ(z|x,y)[log pθ(x | z,y)] − βDKL (qϕ(z | x,y)∥p(z | y)) .\nTo model the conditional prior, a density network is usually used to predict the mean and variance based on the conditional input y."
    }, {
      "heading" : "3 CUC-VAE TTS System",
      "text" : "The proposed CUC-VAE TTS system, which is adapted from FastSpeech 2 as shown in Fig. 1, aims to synthesize speech with more expressive prosody. Fig. 1 describes the model architecture, which has two components: CU-embedding and CU-enhanced CVAE. The CUC-VAE TTS system takes as input [ui−L, · · · ,ui, · · · ,ui+L], si and xi, where [ui−L, · · · ,ui, · · · ,ui+L] is the crossutterance set that includes the current utterance ui and the L utterances before and after ui. Each u represents the text content of an utterance. Note that si is the speaker ID, and xi is the reference mel-spectrogram of the current utterance ui. In this section, the two main components of the CUCVAE TTS system will be introduced in detail."
    }, {
      "heading" : "3.1 Cross-Utterance Embedding",
      "text" : "The CU-embedding encodes not only the phoneme sequence and speaker information but also crossutterance information into a sequence of mixture encodings in place of a standard embedding. As shown in Fig. 1, the first L utterances and the last L utterances surrounding the current one, ui, are used as text input in addition to the current utterance and speaker information. Same as the standard embedding, an extra G2P conversion is first performed to convert the current utterance into phonemes Pi = [p1, p2, · · · , pT ], where T is the number of phonemes. Then, a Transformer encoder is used to encode the phoneme sequence into a sequence of phoneme encodings. Besides, speaker\ninformation is encoded into a speaker embedding si which is directly added to each phoneme encoding to form the mixture encodings Fi of the phoneme sequence.\nFi = [fi(p1),fi(p2), · · · ,fi(pT )], (4)\nwhere f represents resultant vector from the addition of each phoneme encoding and speaker embedding.\nTo supplement the text information from the current utterance to generate natural and expressive audio, cross-utterance BERT embeddings together with a multi-head attention layer are used to capture contextual information. To begin with, 2L cross-utterance pairs, denoted as Ci, are derived from 2L + 1 neighboring utterances [ui−L, · · · ,ui, · · · ,ui+L] as:\nCi = [c(ui−L,ui−L+1), · · · , c(ui−1,ui), · · · , c(ui+L−1,ui+L)], (5)\nwhere c(uk, uk+1) = {[CLS],uk, [SEP],uk+1}, which adds a special token [CLS] at the beginning of each pair and inserts another special token [SEP] at the boundary of each sentence to keep track of BERT. Then, the 2L cross-utterance pairs are fed to the BERT to capture cross-utterance information, which yields 2L BERT embedding vectors by taking the output vector at the position of the [CLS] token and projecting each to a 768-dim vector for each cross-utterance pair, as shown below:\nBi = [b−L, b−L+1, · · · , bL−1],\nwhere each vector bk in Bi represents the BERT embedding of the cross-utterance pair c(uk,uk+1). Next, to extract CU-embedding vectors for each phoneme specifically, a multi-head attention layer is added to combine the 2L BERT embeddings into one vector as shown in Eq. (6).\nGi = MHA(FiW Q,BiW K,BiW V), (6)\nwhere MHA(·) denotes the multi-head attention layer, W Q, W K and W V are linear projection matrices, and Fi denotes the sequence of mixture encodings for the current utterance which acts as the query in the attention mechanism. For simplicity, we denote Eq. (6) as Gi = [g1, g2, · · · , gT ] from the multi-head attention being of length T and each of them is then concatenated with its corresponding mixture encoding. The concatenated vectors are projected by another linear layer to\nform the final output Hi of the CU-embedding, Hi = [h1,h2, · · · ,hT ] of the current utterance, as shown in Eq. (7).\nht = [gt,f(pt)]W , (7)\nwhere W is a linear projection matrix. Moreover, an additional duration predictor takes Hi as inputs and predicts the duration Di of each phoneme."
    }, {
      "heading" : "3.2 Cross-Utterance Enhanced CVAE",
      "text" : "In addition to the CU-embedding, a CU-enhanced CVAE is proposed to conquer the lack of prosody variation of FastSpeech 2 and the inconsistency between the standard Gaussian prior distribution sampled by the VAE based TTS system and the true prior distribution of speech. Specifically, the CU-enhanced CVAE consists of an encoder module and a decoder module, as shown in Fig. 1. The utterance-specific prior in the encoder aims to learn the prior distribution zp from the CU-embedding output H and predicts duration D. For convenience, the subscript i is omitted in this subsection. Furthermore, the posterior module in the encoder takes as input reference mel-spectrogram x, then model the approximate posterior z conditioned on utterance-specific conditional prior zp. Sampling is done from the estimated prior by the utterancespecific prior module and is reparameterized as:\nz = µ⊕ σ ⊗ zp, (8) where µ and σ are estimated from conditional posterior module to approximate posterior distribution N (µ,σ), zp is sampled from the learned utterance-specific prior, and ⊕,⊗ are elementwise addition and multiplication operation. Furthermore, the utterance-specific conditional prior module is conducted to learn utterance-specific prior with CU-embedding output H and D. The reparameterization is as follows:\nzp = µp ⊕ σp ⊗ ϵ, (9)\nwhere µp,σp are learned from the utterancespecific prior module, and ϵ is sampled from the standard Gaussian N (0, 1). By substituting Eq. (9) into Eq. (8), the following equation can be derived for the total sampling process:\nz = µ⊕ σ ⊗ µp ⊕ σ ⊗ σp ⊗ ϵ. (10)\nDuring inference, sampling is done from the learned utterance-specific conditional prior distribution N (µp,σp) from CU-embedding instead of\na standard Gaussian distribution N (0, 1). For simplicity, we can formulate the data likelihood calculation as follows, where the intermediate variable utterance-specific prior zp from D,H to obtain z is omitted:\npθ(x | H,D) = ∫ pθ(x | z,H,D)pϕ(z | H,D)dz,\n(11) In Eq. (11), ϕ, θ are the encoder and decoder module parameters of the CUC-VAE TTS system.\nMoreover, the decoder in CU-enhanced CVAE is adapted from FastSpeech 2. An additional projection layer is firstly added to project z to a high dimensional space so that z could be added to H . Next, a length regulator expands the length of input according to the predicted duration D of each phoneme. The rest of Decoder is same as the Decoder module in FastSpeech 2 to convert the hidden sequence into an mel-spectrogram sequence via parallelized calculation.\nTherefore, the ELBO objective of the CUC-VAE can be expressed as,\nL(x | H,D) = Eqϕ(z|D,H)[log pθ(x | z,D,H)]\n− β1 T∑\nn=1\nDKL ( qϕ1 ( zn | znp ,x ) ∥qϕ2 ( znp | D,H )) − β2\nT∑ n=1 DKL ( qϕ2 ( znp | D,H ) ∥p(znp ) ) ,\n(12) where ϕ1, ϕ2 are two parts of CUC-VAE encoder ϕ to obtain z from zp,x and zp from D,H respectively, β1, β2 are two balance constants, p(znp ) is chosen to be standard Gaussian N (0, 1). Meanwhile, zn and znp correspond to the latent representation for the n-th phoneme, and T is the length of the phoneme sequence."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "To evaluate the proposed CUC-VAE TTS system, a series of experiments were conducted on a single speaker dataset and a multi-speaker dataset. For the single speaker setting, the LJ-Speech read English data (Ito and Johnson, 2017) was used which consists of 13,100 audio clips with a total duration of approximately 24 hours. A female native English speaker read all the audio clips, and the scripts were selected from 7 non-fiction books. For the multispeaker setting, the train-clean-100 and train-clean360 subsets of the LibriTTS English audiobook data (Zen et al., 2019) were used. These subsets used here consist of 1151 speakers (553 female\nspeakers and 598 male speakers) and about 245 hours of audio. All audio clips were re-sampled at 22.05 kHz in experiments for consistency.\nThe proposed CU-embedding in our system learns the cross-utterance representation from surrounding utterances. However, unlike LJ-Speech, transcripts of LibriTTS utterances are not arranged as continuous chunks of text in their corresponding book. Therefore, transcripts of the LibriTTS dataset were pre-processed to find the location of each utterance in the book, so that the first L and last L utterances of the current one can be efficiently obtained during training and inference. The pre-processed scripts and our code are available 1."
    }, {
      "heading" : "4.2 System Specification",
      "text" : "The proposed CUC-VAE TTS system was based on the framework of FastSpeech 2. The CUembedding utilised a Transformer to learn the current utterance representation, where the dimension of phoneme embeddings and the size of the selfattention were set to 256. To explicitly extract speaker information, 256-dim speaker embeddings were also added to the Transformer output. Meanwhile, the pre-trained BERT model to extract crossutterance information had 12 Transformer blocks and 12-head attention layers with 110 million parameters. The size of the derived embeddings of each cross-utterance pair was 768-dim. Note that the BERT model and corresponding embeddings were fixed when training the TTS system. Network in CU-enhanced CVAE consisted of four 1Dconvolutional (1D-Conv) layers with kernel sizes of 1 to predict the mean and variance of 2-dim latent features. Then a linear layer was added to transform the sampled latent feature to a 256-dim vector. The duration predictor which consisted of two convolutional blocks and an extra linear layer to predict the duration of each phoneme for the length regulator in FastSpeech 2 was adapted to take in CU-embedding outputs. Each convolutional block was comprised of a 1D-Conv network with ReLU activation followed by a layer normalization and dropout layer. The Decoder adopted four feed-forward Transformer blocks to convert hidden sequences into 80-dim mel-spectrogram sequence, similar to FastSpeech 2. Finally, HifiGAN (Kong et al., 2020) was used to synthesize waveform from the predicted mel-spectrogram.\n1https://anonymous.4open.science/r/co de-2708"
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "In order to evaluate the performance of our proposed component, both subjective and objective tests were performed. First of all, a subjective listening test was performed over 11 synthesized audios with 23 volunteers asked to rate the naturalness of speech samples on a 5-scale mean opinion score (MOS) evaluation. The MOS results were reported with 95% confidence intervals. In addition, an AB test was conducted to compare the CU-enhanced CVAE with utterance-specific prior and normal CVAE with standard Gaussian prior. 23 volunteers were asked to choose the preference audio generated by different models in the AB test.\nFor the objective evaluation, F0 frame error (FFE) (Chu and Alwan, 2009) and mel-cepstral distortion (MCD) (Kubichek, 1993) were used to measure the reconstruction performance of different VAEs. FFE combined the Gross Pitch Error (GPE) and the Voicing Decision Error (VDE) and was used to evaluate the reconstruction of the F0 track. MCD evaluated the timbral distortion, which was computed from the first 13 MFCCs in our experiments. Moreover, word error rates (WER) from an ASR model trained on the real speech from the LibriTTS training set were reported. Complementary to naturalness, the WER metric showed both the intelligibility and the degree of inconsistency between synthetic speech and real speech. The ASR system used in this paper was an attention-based encoder-decoder model trained on Librispeech 960- hour data, with a WER of 4.4% on the test-clean set. Finally, the diversity of samples was evaluated by measuring the standard deviation of two prosody attributes of each phoneme: relative energy (E) and fundamental frequency (F0), similar to Sun et al. (2020b). Relative energy was calculated as the ratio of the average signal amplitude within a phoneme to the average amplitude of the entire sentence, and fundamental frequency was measured using a pitch tracker. In this paper, the average standard deviation of E and F0 of three phonemes in randomly selected 11 utterances was reported to evaluate the diversity of generated speech."
    }, {
      "heading" : "5 Results",
      "text" : "This section presents the series of experiments for the proposed CUC-VAE TTS system. First, ablation studies were performed to progressively show the influence of different parts in the CUC-VAE TTS system based on MOS and WER. Next, the\nreconstruction performance of CUC-VAE was evaluated by FFE and MCD. Then, the naturalness and prosody diversity using CUC-VAE were compared to FastSpeech 2 and other VAE techniques. At last, a case study illustrated the prosody variations with different cross-utterance information as an example. The audio examples are available on the demo page 2."
    }, {
      "heading" : "5.1 Ablation Studies",
      "text" : "Ablation studies in this section were conducted on the LJ-Speech data based on the subjective test and WER. First, to investigate the effect of the different number of neighbouring utterances, CUC-VAE TTS systems built with L = 1, 3, 5 were evaluated using MOS scores, as shown in Table 1.\nThe effect of the different number of neighbouring utterances on the naturalness of the synthesized speech can be observed by comparing MOS scores which is the higher the better. The CUC-VAE with L = 5 achieved highest score 3.95 compared to system with L = 1 and L = 3. Since only marginal MOS improvements were obtained using more than 5 neighbouring utterances, the rest of experiments were performed using L = 5.\nThen we investigated the influence of each part of CUC-VAE on performance. The baseline was our implementation of Fastspeech 2. For the system denoted as Baseline + fine-grained VAE which served as a stronger baseline, the pitch predictor and energy predictor of FastSpeech 2 were replaced with a fine-grained VAE with 2-dim latent space. Based on the fine-grained VAE baseline, the CVAE was added without the CU-embedding to the system, referred to as Baseline+CVAE to verify the function of CVAE on the system, which conditions on the current utterance. Again, MOS was compared among these systems as shown in Table 2.\nAs shown in Table 2, MOS progressively increased when fine-grained VAE, CVAE, and CUembedding were added in consecutively. The proposed CUC-VAE TTS system achieved the highest\n2https://bit.ly/cuc-vae-tts-demo\nMOS 3.95 compared to baselines. The results indicated that CUC-VAE module played a crucial role in generating more natural audio.\nTo verify the importance of the utterancespecific prior to the synthesized audio, the same CUC-VAE system was used, and the only difference is whether to sample latent prosody features from the utterance-specific prior or from a standard Gaussian distribution. A subjective AB test was performed which required 23 volunteers to provide their preference between audios synthesized from the 2 approaches. Moreover, WER was also compared here to show the intelligibility of the synthesized audio. As shown in Table 3, the preference rate of using the utterance-specific prior is 0.52 higher than its counterpart, and a 4.9% absolute WER reduction was found, which confirmed the importance of the utterance-specific prior in our CUC-VAE TTS system."
    }, {
      "heading" : "5.2 Reconstruction Performance",
      "text" : "FFE and MCD were used to measure the reconstruction performance of VAE systems. An utterance-level prosody modelling baseline which extract one latent prosody feature vector for an utterance was added for more comprehensive comparison, and is referred to as the Global VAE.\nTable. 4 shows the reconstruction performance\non the LJ-Speech dataset and LibriTTS dataset, respectively. Baseline had the highest value of FFE and MCD on the LJ-Speech dataset and LibriTTS dataset. The value of FFE and MCD decreased when the global VAE was added and was further reduced when the fine-grained VAE was added to the baseline. Our proposed CUC-VAE TTS system achieved the lowest FFE and MCD across the table on both the LJ-Speech and LibriTTS datasets. This indicated that richer prosody-related information entailed in both cross-utterance and conditional inputs was captured by CUC-VAE."
    }, {
      "heading" : "5.3 Sample Naturalness and Diversity",
      "text" : "Next, sample naturalness and intelligibility were measured using MOS and WER respectively on both LJ-Speech and LibriTTS datasets. Complementary to the naturalness, the diversity of generated speech from the conditional prior was evaluated by comparing the standard deviation of E and F0 similar to (Sun et al., 2020b).\nLJ-Speech experiments were shown in left part of Table. 5. Compared to the global VAE and finegrained VAE, the proposed CUC-VAE received the highest MOS and achieved the lowest WER. Although both F0 and E of the CUC-VAE TTS system were lower than the baseline + fine-grained VAE, the proposed system achieved a clearly higher prosody diversity than the baseline and baseline + global VAE systems. The fine-grained VAE achieved the highest prosody variation as its latent prosody features were sampled from a standard Gaussian distribution, which lacks the constraint of language information from both the current and the neighbouring utterances. This caused extreme prosody variations to occur which impaired both the naturalness and the intelligibility of synthesized audios. As a result, the CUC-VAE TTS system was able to achieve high prosody diversity without hurting the naturalness of the generated speech. In fact, the adequate increase in prosody diversity im-\nTable 5: Sample naturalness and diversity results on LJ-Speech and LibriTTS datasets. Three metrics are reported for each dataset, namely MOS, WER, and Prosody Std. The Prosody Std. includes standard deviations of relative energy (E) and fundamental frequency (F0) in Hertz within each phonene.\nLJ-Speech LibriTTS\nMOS WER Prosody Std. MOS WER Prosody Std.\nF0 E F0 E\nGround Truth 4.31 ± 0.06 8.8 - - 4.10 ± 0.07 5.0 - - Baseline 3.85 ± 0.07 10.8 1.86× 10−13 6.78× 10−7 3.53 ± 0.08 6.0 2.13× 10−13 7.22× 10−7\nBaseline+Global VAE 3.82 ± 0.07 10.4 1.46 0.0004 3.59 ± 0.08 10.8 2.01 0.0054 Baseline+Fine-grained VAE 3.55 ± 0.08 12.8 49.60 0.0670 3.43 ± 0.08 5.6 63.64 0.0901\nCUC-VAE 3.95 ± 0.07 9.9 26.35 0.0184 3.63 ± 0.08 5.5 30.28 0.0217\nproved the expressiveness of the synthesized audio, and hence increased the naturalness.\nThe right part of Table. 5 showed the results on LibriTTS dataset. Similar to the LJ-Speech experiments, the CUC-VAE TTS system achieved the best naturalness measured by MOS, the best intelligibility measured by WER, and the secondhighest prosody diversity across the table. Overall, consistent improvements in both naturalness and prosody diversity were observed on both singlespeaker and multi-speaker datasets."
    }, {
      "heading" : "5.4 A Case Study",
      "text" : "To better illustrate how the utterance-specific prior influenced the naturalness of the synthesized speech under a given context, a case study was performed by synthesizing an example utterance, “Mary asked the time”, with two different neighbouring utterances: “Who asked the time? Mary asked the time.” and “Mary asked the time, and was told it was only five.” Based on the linguistic knowledge, to answer the question in the first setting, an emphasis should be put on the word “Mary”, while in the second setting, the focus of the sentence is “asked the time”. The model trained on LJ-Speech dataset was used to synthesize the utterance and the results were shown in Fig. 2.\nFig. 2 showed the energy and pitch of the two utterance. Energy of the first word “Mary” in Fig. 2(a) changed significantly (energy of “Ma-” was much higher than “-ry”), which reflected an emphasis on the word “Mary”, whereas in Fig. 2(b), energy of “Mary” had no obvious change, i.e., the word was not emphasized. On the other hand, the fundamental frequency of words “asked” and “time” stayed at a high level for a longer time in the second audio than the first one, reflecting another type of emphasis on those words which was also coherent with the given context. Therefore, the difference of energy and pitch between the two utterances demonstrated that the speech synthesized\n(a) Who asked the time? Mary asked the time.\n(b) Mary asked the time, and was told it was only five.\nFigure 2: Comparisons between the energy and pitch contour of same text “Mary asked the time\" but different neighbouring utterances, generated by CUC-VAE TTS trained on LJ-Speech.\nby our model is sufficiently contextualized."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, a non-autoregressive CUC-VAE TTS system was proposed to synthesize speech with better naturalness and more prosody diversity. CUCVAE TTS system estimated the posterior distribution of latent prosody features for each phone based on cross-utterance information in addition to the acoustic features and speaker information. The generated audio was sampled from an utterancespecific prior distribution, approximated based on cross-utterance information. Experiments were conducted to evaluate the proposed CUC-VAE TTS system with metrics including MOS, preference rate, WER, and the standard deviation of prosody attributes. Experiment results showed that the proposed CUC-VAE TTS system improved both the naturalness and prosody diversity in the generated audio samples, which outperformed the baseline in all metrics with clear margins."
    } ],
    "references" : [ {
      "title" : "Expressive speech synthesis via modeling expressions with variational autoencoder",
      "author" : [ "K. Akuzawa", "Yusuke Iwasawa", "Y. Matsuo." ],
      "venue" : "ArXiv, abs/1804.02135.",
      "citeRegEx" : "Akuzawa et al\\.,? 2018",
      "shortCiteRegEx" : "Akuzawa et al\\.",
      "year" : 2018
    }, {
      "title" : "Speech bert embedding for improving prosody in neural tts",
      "author" : [ "Liping Chen", "Yan Deng", "Xi Wang", "F. Soong", "Lei He." ],
      "venue" : "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6563–6567.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
      "author" : [ "Wei Chu", "A. Alwan." ],
      "venue" : "2009 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 3969–3972.",
      "citeRegEx" : "Chu and Alwan.,? 2009",
      "shortCiteRegEx" : "Chu and Alwan.",
      "year" : 2009
    }, {
      "title" : "Controllable context-aware conversational speech synthesis",
      "author" : [ "Jian Cong", "Shan Yang", "Na Hu", "Guangzhi Li", "Lei Xie", "Dan Su." ],
      "venue" : "ArXiv, abs/2106.10828.",
      "citeRegEx" : "Cong et al\\.,? 2021",
      "shortCiteRegEx" : "Cong et al\\.",
      "year" : 2021
    }, {
      "title" : "Conditional variational auto-encoder for text-driven expressive audiovisual speech synthesis",
      "author" : [ "Sara Dahmani", "Vincent Colotte", "Valérian Girard", "S. Ouni." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Dahmani et al\\.,? 2019",
      "shortCiteRegEx" : "Dahmani et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "J. Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel tacotron 2: A non-autoregressive neural tts model with differentiable duration modeling",
      "author" : [ "Isaac Elias", "H. Zen", "Jonathan Shen", "Yu Zhang", "Jia Ye", "R. Skerry-Ryan", "Yonghui Wu." ],
      "venue" : "ArXiv, abs/2103.14574.",
      "citeRegEx" : "Elias et al\\.,? 2021",
      "shortCiteRegEx" : "Elias et al\\.",
      "year" : 2021
    }, {
      "title" : "Towards transfer learning for end-to-end speech synthesis from deep pre-trained language models",
      "author" : [ "Wei Fang", "Yu-An Chung", "J. Glass." ],
      "venue" : "ArXiv, abs/1906.07307.",
      "citeRegEx" : "Fang et al\\.,? 2019",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2019
    }, {
      "title" : "Phrase break prediction with bidirectional encoder representations in japanese text-to-speech synthesis",
      "author" : [ "Kosuke Futamata", "Byeong-Cheol Park", "Ryuichi Yamamoto", "Kentaro Tachibana." ],
      "venue" : "ArXiv, abs/2104.12395.",
      "citeRegEx" : "Futamata et al\\.,? 2021",
      "shortCiteRegEx" : "Futamata et al\\.",
      "year" : 2021
    }, {
      "title" : "Pretrained text embeddings for enhanced text-to-speech synthesis",
      "author" : [ "Tomoki Hayashi", "Shinji Watanabe", "T. Toda", "K. Takeda", "Shubham Toshniwal", "Karen Livescu." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Hayashi et al\\.,? 2019",
      "shortCiteRegEx" : "Hayashi et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical generative modeling for controllable speech synthesis",
      "author" : [ "Wei-Ning Hsu", "Y. Zhang", "Ron J. Weiss", "H. Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Z. Chen", "Jonathan Shen", "P. Nguyen", "Ruoming Pang." ],
      "venue" : "ArXiv, abs/1810.07217.",
      "citeRegEx" : "Hsu et al\\.,? 2019a",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2019
    }, {
      "title" : "Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization",
      "author" : [ "Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Yu-An Chung", "Yuxuan Wang", "Yonghui Wu", "James R. Glass." ],
      "venue" : "ICASSP 2019 - 2019 IEEE",
      "citeRegEx" : "Hsu et al\\.,? 2019b",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2019
    }, {
      "title" : "The lj speech dataset",
      "author" : [ "Keith Ito", "Linda Johnson." ],
      "venue" : "https://keithito.com/LJ-Sp eech-Dataset/.",
      "citeRegEx" : "Ito and Johnson.,? 2017",
      "shortCiteRegEx" : "Ito and Johnson.",
      "year" : 2017
    }, {
      "title" : "Png bert: Augmented bert on phonemes and graphemes for neural tts",
      "author" : [ "Ye Jia", "H. Zen", "Jonathan Shen", "Yu Zhang", "Yonghui Wu." ],
      "venue" : "ArXiv, abs/2103.15060.",
      "citeRegEx" : "Jia et al\\.,? 2021",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2021
    }, {
      "title" : "A learned conditional prior for the vae acoustic space of a tts system",
      "author" : [ "Panagiota Karanasou", "S. Karlapati", "A. Moinet", "Arnaud Joly", "Ammar Abbas", "Simon Slangen", "Jaime LorenzoTrueba", "Thomas Drugman." ],
      "venue" : "ArXiv, abs/2106.10229.",
      "citeRegEx" : "Karanasou et al\\.,? 2021",
      "shortCiteRegEx" : "Karanasou et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving the prosody of rnn-based english text-tospeech synthesis by incorporating a bert model",
      "author" : [ "Tom Kenter", "Manish Sharma", "R. Clark." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Kenter et al\\.,? 2020",
      "shortCiteRegEx" : "Kenter et al\\.",
      "year" : 2020
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P. Kingma", "M. Welling." ],
      "venue" : "CoRR, abs/1312.6114.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "author" : [ "Jungil Kong", "Jaehyeon Kim", "Jaekyoung Bae." ],
      "venue" : "ArXiv, abs/2010.05646.",
      "citeRegEx" : "Kong et al\\.,? 2020",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "Mel-cepstral distance measure for objective speech quality assessment",
      "author" : [ "R. Kubichek." ],
      "venue" : "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing, 1:125–128 vol.1.",
      "citeRegEx" : "Kubichek.,? 1993",
      "shortCiteRegEx" : "Kubichek.",
      "year" : 1993
    }, {
      "title" : "Fastpitch: Parallel text-tospeech with pitch prediction",
      "author" : [ "Adrian La’ncucki" ],
      "venue" : "In ICASSP",
      "citeRegEx" : "La.ncucki.,? \\Q2021\\E",
      "shortCiteRegEx" : "La.ncucki.",
      "year" : 2021
    }, {
      "title" : "Robust and finegrained prosody control of end-to-end speech synthesis",
      "author" : [ "Younggun Lee", "Taesu Kim." ],
      "venue" : "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5911–5915.",
      "citeRegEx" : "Lee and Kim.,? 2019",
      "shortCiteRegEx" : "Lee and Kim.",
      "year" : 2019
    }, {
      "title" : "Parallel neural text-to-speech",
      "author" : [ "Kainan Peng", "Wei Ping", "Z. Song", "Kexin Zhao." ],
      "venue" : "ArXiv, abs/1905.08459.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Fastspeech 2: Fast and high-quality end-to-end text to speech",
      "author" : [ "Yi Ren", "Chenxu Hu", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu." ],
      "venue" : "ArXiv, abs/2006.04558.",
      "citeRegEx" : "Ren et al\\.,? 2021",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2021
    }, {
      "title" : "Fastspeech: Fast, robust and controllable text to speech",
      "author" : [ "Yi Ren", "Yangjun Ruan", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural tts synthesis by conditioning",
      "author" : [ "Jonathan Shen", "Ruoming Pang", "Ron J. Weiss", "M. Schuster", "Navdeep Jaitly", "Zongheng Yang", "Z. Chen", "Yu Zhang", "Yuxuan Wang", "R. Skerry-Ryan", "R. Saurous", "Yannis Agiomyrgiannakis", "Yonghui Wu" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Sohn et al\\.,? 2015",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Predicting expressive speaking style from text in end-to-end speech synthesis",
      "author" : [ "Daisy Stanton", "Yuxuan Wang", "R. Skerry-Ryan." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pages 595– 602.",
      "citeRegEx" : "Stanton et al\\.,? 2018",
      "shortCiteRegEx" : "Stanton et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating diverse and natural text-to-speech samples using a quantized fine-grained vae and autoregressive prosody prior",
      "author" : [ "G. Sun", "Y. Zhang", "Ron J. Weiss", "Yuan Cao", "H. Zen", "A. Rosenberg", "B. Ramabhadran", "Yonghui Wu." ],
      "venue" : "ICASSP 2020 - 2020",
      "citeRegEx" : "Sun et al\\.,? 2020a",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Fully-hierarchical finegrained prosody modeling for interpretable speech synthesis",
      "author" : [ "G. Sun", "Y. Zhang", "Ron J. Weiss", "Yuanbin Cao", "H. Zen", "Yonghui Wu." ],
      "venue" : "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Process-",
      "citeRegEx" : "Sun et al\\.,? 2020b",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Wavenet: A generative model for raw audio",
      "author" : [ "Aäron van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "Oriol Vinyals", "A. Graves", "Nal Kalchbrenner", "A. Senior", "K. Kavukcuoglu." ],
      "venue" : "SSW.",
      "citeRegEx" : "Oord et al\\.,? 2016",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam M. Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Tacotron: Towards end-to-end speech synthesis",
      "author" : [ "Yuxuan Wang", "R. Skerry-Ryan", "Daisy Stanton", "Yonghui Wu", "Ron J. Weiss", "Navdeep Jaitly", "Zongheng Yang", "Y. Xiao", "Z. Chen", "Samy Bengio", "Quoc V. Le", "Yannis Agiomyrgiannakis", "R. Clark", "R. Saurous." ],
      "venue" : "In",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving prosody modelling with cross-utterance bert embeddings for end-to-end speech synthesis",
      "author" : [ "Guanghui Xu", "Wei Song", "Zhengchen Zhang", "C. Zhang", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "End-to-end text-to-speech using latent duration based on vq-vae",
      "author" : [ "Yusuke Yasuda", "Xin Wang", "J. Yamagishi." ],
      "venue" : "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5694–5698.",
      "citeRegEx" : "Yasuda et al\\.,? 2021",
      "shortCiteRegEx" : "Yasuda et al\\.",
      "year" : 2021
    }, {
      "title" : "Libritts: A corpus derived from librispeech for text-tospeech",
      "author" : [ "H. Zen", "Viet-Trung Dang", "R. Clark", "Yu Zhang", "Ron J. Weiss", "Ye Jia", "Z. Chen", "Yonghui Wu." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Zen et al\\.,? 2019",
      "shortCiteRegEx" : "Zen et al\\.",
      "year" : 2019
    }, {
      "title" : "Aligntts: Efficient feed-forward text-to-speech system without explicit alignment",
      "author" : [ "Zhen Zeng", "Jianzong Wang", "Ning Cheng", "Tian Xia", "Jing Xiao." ],
      "venue" : "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Zeng et al\\.,? 2020",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2020
    }, {
      "title" : "Dependency parsing based semantic representation learning with graph neural network for enhancing expressiveness of text-to-speech",
      "author" : [ "Yixuan Zhou", "C. Song", "Jingbei Li", "Zhiyong Wu", "H. Meng." ],
      "venue" : "ArXiv, abs/2104.06835.",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Prosody, as the focus of this paper, collectively refers to the stress, intonation, and rhythm in speech, and has been an increasingly popular research aspect in end-to-end TTS systems (van den Oord et al., 2016; Wang et al., 2017; Stanton et al., 2018; Elias et al., 2021; Chen et al., 2021).",
      "startOffset" : 185,
      "endOffset" : 292
    }, {
      "referenceID" : 26,
      "context" : "Prosody, as the focus of this paper, collectively refers to the stress, intonation, and rhythm in speech, and has been an increasingly popular research aspect in end-to-end TTS systems (van den Oord et al., 2016; Wang et al., 2017; Stanton et al., 2018; Elias et al., 2021; Chen et al., 2021).",
      "startOffset" : 185,
      "endOffset" : 292
    }, {
      "referenceID" : 6,
      "context" : "Prosody, as the focus of this paper, collectively refers to the stress, intonation, and rhythm in speech, and has been an increasingly popular research aspect in end-to-end TTS systems (van den Oord et al., 2016; Wang et al., 2017; Stanton et al., 2018; Elias et al., 2021; Chen et al., 2021).",
      "startOffset" : 185,
      "endOffset" : 292
    }, {
      "referenceID" : 1,
      "context" : "Prosody, as the focus of this paper, collectively refers to the stress, intonation, and rhythm in speech, and has been an increasingly popular research aspect in end-to-end TTS systems (van den Oord et al., 2016; Wang et al., 2017; Stanton et al., 2018; Elias et al., 2021; Chen et al., 2021).",
      "startOffset" : 185,
      "endOffset" : 292
    }, {
      "referenceID" : 16,
      "context" : "Some previous work captured prosody features explicitly using either style tokens or variational autoencoders (VAEs) (Kingma and Welling, 2014; Hsu et al., 2019a) which encapsulate prosody information into latent representations.",
      "startOffset" : 117,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "Some previous work captured prosody features explicitly using either style tokens or variational autoencoders (VAEs) (Kingma and Welling, 2014; Hsu et al., 2019a) which encapsulate prosody information into latent representations.",
      "startOffset" : 117,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "Therefore, recent research (Dahmani et al., 2019; Karanasou et al., 2021) employed a conditional VAE (CVAE) (Sohn et al.",
      "startOffset" : 27,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "Therefore, recent research (Dahmani et al., 2019; Karanasou et al., 2021) employed a conditional VAE (CVAE) (Sohn et al.",
      "startOffset" : 27,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : ", 2021) employed a conditional VAE (CVAE) (Sohn et al., 2015) to synthesize speech from a conditional prior.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "while, pre-trained language model (LM) such as bidirectional encoder representation for Transformers (BERT) (Devlin et al., 2019) has also been applied to TTS systems (Hayashi et al.",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 32,
      "context" : "Efforts have been devoted to include cross-utterance information in the input features to improve the prosody modelling of auto-regressive TTS (Xu et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 160
    }, {
      "referenceID" : 22,
      "context" : "To generate more expressive prosody, while maintaining high fidelity in synthesized speech, a cross-utterance conditional VAE (CUC-VAE) component is proposed, which is integrated into and jointly optimised with FastSpeech 2 (Ren et al., 2021), a commonly used non-autoregressive end-toend TTS system.",
      "startOffset" : 224,
      "endOffset" : 242
    }, {
      "referenceID" : 30,
      "context" : "The CU-embedding takes BERT sentence embeddings from surrounding utterances as inputs and generates phoneme-level CUembedding using a multi-head attention (Vaswani et al., 2017) layer where attention weights are derived from the encoder output of each phoneme as well as the speaker information.",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 23,
      "context" : "FastSpeech (Ren et al., 2019) and ParaNet (Peng et al.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : ", 2019) and ParaNet (Peng et al., 2019) requires distillation from an autoregressive model, while more recent non-autoregressive TTS systems, including FastPitch (La’ncucki, 2021), AlignTTS (Zeng et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : ", 2019) requires distillation from an autoregressive model, while more recent non-autoregressive TTS systems, including FastPitch (La’ncucki, 2021), AlignTTS (Zeng et al.",
      "startOffset" : 130,
      "endOffset" : 147
    }, {
      "referenceID" : 35,
      "context" : ", 2019) requires distillation from an autoregressive model, while more recent non-autoregressive TTS systems, including FastPitch (La’ncucki, 2021), AlignTTS (Zeng et al., 2020) and FastSpeech 2 (Ren et al.",
      "startOffset" : 158,
      "endOffset" : 177
    }, {
      "referenceID" : 22,
      "context" : ", 2020) and FastSpeech 2 (Ren et al., 2021), do not rely on any form of knowledge distillation from a pre-trained TTS system.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : "It is believed that prosody can also be inferred from language information in both current and surrounding utterances (Shen et al., 2018; Fang et al., 2019; Xu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "It is believed that prosody can also be inferred from language information in both current and surrounding utterances (Shen et al., 2018; Fang et al., 2019; Xu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 192
    }, {
      "referenceID" : 32,
      "context" : "It is believed that prosody can also be inferred from language information in both current and surrounding utterances (Shen et al., 2018; Fang et al., 2019; Xu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 192
    }, {
      "referenceID" : 36,
      "context" : "It is believed that prosody can also be inferred from language information in both current and surrounding utterances (Shen et al., 2018; Fang et al., 2019; Xu et al., 2021; Zhou et al., 2021).",
      "startOffset" : 118,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "Such information is often entailed in vector representations from a pre-trained LM, such as BERT (Devlin et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "Some existing work incorporated BERT embeddings at word or subword-level into autoregressive TTS models (Shen et al., 2018; Fang et al., 2019).",
      "startOffset" : 104,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "Some existing work incorporated BERT embeddings at word or subword-level into autoregressive TTS models (Shen et al., 2018; Fang et al., 2019).",
      "startOffset" : 104,
      "endOffset" : 142
    }, {
      "referenceID" : 36,
      "context" : "Besides, a relational gated graph network with pretrained BERT embeddings as node inputs (Zhou et al., 2021) was used to extract word-level semantic representations, thus enhancing expressiveness.",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "For the single speaker setting, the LJ-Speech read English data (Ito and Johnson, 2017) was used which consists of 13,100 audio clips with a total duration of approximately 24 hours.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 34,
      "context" : "For the multispeaker setting, the train-clean-100 and train-clean360 subsets of the LibriTTS English audiobook data (Zen et al., 2019) were used.",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "Finally, HifiGAN (Kong et al., 2020) was used to synthesize waveform from the predicted mel-spectrogram.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "For the objective evaluation, F0 frame error (FFE) (Chu and Alwan, 2009) and mel-cepstral distortion (MCD) (Kubichek, 1993) were used to measure the reconstruction performance of different VAEs.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "For the objective evaluation, F0 frame error (FFE) (Chu and Alwan, 2009) and mel-cepstral distortion (MCD) (Kubichek, 1993) were used to measure the reconstruction performance of different VAEs.",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 28,
      "context" : "Complementary to the naturalness, the diversity of generated speech from the conditional prior was evaluated by comparing the standard deviation of E and F0 similar to (Sun et al., 2020b).",
      "startOffset" : 168,
      "endOffset" : 187
    } ],
    "year" : 0,
    "abstractText" : "Modelling prosody variation is critical for synthesizing natural and expressive speech in endto-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUCVAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences. At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody. The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes. Experimental results on LJSpeech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins.",
    "creator" : null
  }
}
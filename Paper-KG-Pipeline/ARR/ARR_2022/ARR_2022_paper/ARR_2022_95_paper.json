{
  "name" : "ARR_2022_95_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fantastic Questions and Where to Find Them: FairytaleQA— An Authentic Dataset for Narrative Comprehension",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should\nbe valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students’ performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop mod-\nels to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what subskills are tested. As a consequence, QG models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a consistent way.\nTo bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset.\nWe employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education\ndomain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions’ reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills, even for models trained on standard QA datasets (NarrativeQA (Kočiskỳ et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models’ reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality."
    }, {
      "heading" : "2 Related Work",
      "text" : "This section reports a survey on closely related popular QA datasets that 1) focus on narratives and/or 2) are designed for educational purposes (dataset features in Table 2)1."
    }, {
      "heading" : "2.1 QA Datasets Focusing on Narratives",
      "text" : "Despite the large number of datasets on reading comprehension, fewer focus on comprehension of narrative text. NarrativeQA (Kočiskỳ et al., 2018) is one of the representative datasets. It was generated by crowd-source workers who wrote QA pairs according to summaries of books or movie scripts, while the task takers are supposed to answer these questions based on their reading of original books or movie scripts. As such, this dataset is posited to evaluate a person’s understanding of the underlying narrative. Indeed, a study (Mou et al., 2021) confirmed that NarrativeQA contains a significant amount of questions that focus on narrative events and the relationship among events. However, NarrativeQA simply instructed crowd-sourced workers to generate questions as if they were to \"test students\" without using a detailed annotation protocol.\n1It is worth noting that this review focuses on the purpose of reading-related education. Therefore, datasets assessing the education of natural science (Clark et al., 2018; Dalvi et al., 2018) are not covered.\nIt is questionable whether these workers actually had experiences in testing students in the first place, and the lack of protocol may have imposed too little control over the coverage of reading sub-skills being assessed.\nBookTest (Bajgar et al., 2016) is an automatically constructed cloze-style QA dataset based on a collection of narrative texts retrieved from project Gutenberg. The questions were generated by automatically removing a noun or entity in a sentence that has appeared in the preceding context. While cloze-style tests can be a valid instrument for assessing reading comprehension, its validity depends on of the careful selection of words to be removed so that filling them in requires proper comprehension (Gellert and Elbro, 2013). It is unlikely that automatically constructed cloze tests would meet such standard.\nAnother dataset, TellMeWhy (Lal et al., 2021), aims to facilitate and assess understanding of causal relationships. This dataset contains \"why\" questions that are relatively challenging, given that they require additional information not directly provided in the text. However, TellMeWhy only addresses one narrative component type (i.e., causal relationship), whereas FairytaleQA provides seven evaluation components. Moreover, TellMeWhy was built upon ROCStories (Mostafazadeh et al., 2016) and thus only examine comprehension on incomplete story sections, which may have limited the dataset’s ability to assess macro-level summarization and inference making."
    }, {
      "heading" : "2.2 QA Datasets for Reading Education",
      "text" : "There are several benchmarks derived from sources for education purposes (e.g., exams or curricula). RACE (Lai et al., 2017) is a large-scale dataset consisting of comprehension questions from English exams for Chinese middle and high school\nstudents. RACE uses a mixture of narrative and informational paragraphs. These two genres require slightly different comprehension skills (Liebfreund, 2021) and students perform differently based on what genres of text they read (Denton et al., 2015). Mixing these two together in one dataset without annotating the specific genre of each story/question obscures the ability to offer a precise assessment. Moreover, RACE is purely in multiple-choice format and the paragraphs are usually shorter. These two characteristics may make the RACE dataset less challenging; and recent models have demonstrated close-to-human performance2.\nCLOTH (Xie et al., 2017) is a cloze-style dataset also collected from English exams. Each question in CLOTH is fill-in-the-blank with multiple options to choose from. CLOTH can be advantageous for educational QG as each question is labeled with the level of reasoning it involves, including grammar, short-term reasoning, paraphrasing, and long-term reasoning. However, this dataset shares certain limitations inherent to multiple choice formats (Klufa, 2015)."
    }, {
      "heading" : "2.3 Non-QA Datasets for Narrative Comprehension",
      "text" : "There are some datasets that are designed for assessing narrative comprehension skills but do not use QA as a form of evaluation. Several datasets, such as NovelChapters (Ladhak et al., 2020) and BookSum (Kryściński et al., 2021), evaluate models’ comprehension through summarization tasks. However, there have been debates of whether comprehension can be assessed solely through summarization (Head et al., 1989), as summarization poses a high demand on writing that confounds the\n2http://www.qizhexie.com/data/RACE_ leaderboard.html\nreading skills intended to be assessed. Two other recent datasets focus on singular specific elements in narratives. The LiSCU dataset (Brahman et al., 2021) targets readers’ understanding of characters, and Sims et al. (2019) propose a dataset for detecting events in narratives. Yet given their focus on single narrative elements, these two datasets may not provide a comprehensive evaluation of narrative comprehension.\n3 FairytaleQA\nThe FairytaleQA contains 10,580 QA pairs from 278 classic fairytale stories. In the remainder of this section, we report the dataset construction process and its key statistics."
    }, {
      "heading" : "3.1 Source Texts",
      "text" : "The narrative texts utilized in the dataset are classic fairytales with clear narrative structures. We gathered the text from the Project Gutenberg website3, using “fairytale” as the search term. Due to large number of fairytales found within the Gutenberg project, we used the most popular stories based on the number of downloads, since these stories presumably have more engaging plots and higherquality of writing.\nTo ensure the readability of the text, we made a small number of minor revisions to some obviously outdated vocabulary (e.g., changing “ere” to “before”) and the unconventional use of punctuation (e.g., changing consecutive semi-colons to periods). For each story, we evaluated the reading difficulty level using the textstat4 Python package, primarily based on sentence length, word length, and commonness of words. We excluded stories that are at 10th grade level or above.\nThese texts were broken down into small sections based on their semantic content by our annotators. Most of the resulting sections were one single natural paragraph of the original text. However, sometimes several paragraphs were combined (usually multiple exchanges of dialogues); and some exceptionally long paragraphs that contained more than one focal event were divided into multiple sections. On average, there are 15 sections per story, and each section has an average of 150 words.\n3https://www.gutenberg.org/ 4https://pypi.org/project/textstat/"
    }, {
      "heading" : "3.2 Schema for Question Annotation",
      "text" : "Categorization via Narrative Elements or Relations FairytaleQA is intended to include QA pairs that capture the seven narrative elements/relations that are verified in prior educational research (Paris and Paris, 2003). Definitions of question types are shown below. Example questions for each type are in Appendix C. • Character questions ask test takers to identify\nthe character of the story or describe characteristics of characters. • Setting questions ask about a place or time\nwhere/when story events take place and typically start with \"Where\" or \"When.\" • Action questions ask about characters’ behav-\niors or additional information about that behavior. • Feeling questions ask about the character’s emo-\ntional status or reaction to certain events and are typically worded as \"How did/does/do . . . feel\" • Causal relationship questions focus on two\nevents that are causally related where the prior events causally lead to the latter event in the question. This type of questions usually begins with \"Why\" or \"What made/makes.\" • Outcome resolution questions ask for identi-\nfying outcome events that are causally led to by the prior event in the question. This type of questions are usually worded as \"What happened/happens/has happened...after...\" • Prediction questions ask for the unknown out-\ncome of a focal event, which is predictable based on the existing information in the text.\nCategorization via Source of Answers Orthogonal to the aforementioned question categories, questions in FairytaleQA are also categorized based on whether or not the answer source can be directly found in the text, namely explicit versus implicit questions. Generally speaking, explicit questions revolve around a specific story fact and implicit questions require summarizing and making an inference based on information that is only implicit in the text. Using a combination of explicit and implicit questions yield an assessment with more balanced difficulty (Raphael, 1986). In our data, explicit and implicit questions are defined as below (Examples in Appendix C): • Explicit questions ask for answers that can be\ndirectly found in the stories. In other words, the source of answer are spans of text. • Implicit questions ask for answers that cannot\nbe directly found in the text. Answering the questions require either reformulating language or making inference. In other words, the answer source is \"free-form\", meaning that the answers can be any free-text, and there is no limit to where the answer comes from."
    }, {
      "heading" : "3.3 Annotation Process",
      "text" : "Five annotators were involved in the annotation of QA pairs. All of these annotators have a B.A. degree in education, psychology, or cognitive science and have substantial experience in teaching and assessing students’ reading skills. These annotators were supervised by three experts in literacy education.\nAnnotation Guidelines The annotators were instructed to imagine that they were writing questions to test elementary school students who are in the process of reading a complete story. We required the annotators to generate only natural, open-ended questions that started with \"wh-\", avoiding \"yes\" or “no-” questions. We also instructed them to provide a diverse set of questions about different narrative elements and include both implicit and explicit questions. Each question in the dataset has a label on the narrative element/relation to be assessed and whether it is implicit or explicit.\nWe asked the annotators to also generate answers for each of their questions. We asked them to provide the shortest possible answers but did not restrict them to either complete sentences or short phrases. For explicit questions (i.e., span), annotators extracted the shortest phrase from the text as the answer. For implicit questions (i.e., free-form), annotators provided at least two possible answers for each question. We also asked the annotators to label which section(s) the question and answer were from.\nAnnotator Training and Cross-Checking All annotators received a two-week training in which each of them was familiarized with the coding template (described in the section below), and conducted practice coding on the same five stories. The practice QA pairs were then reviewed by the other annotators and the three experts, and discrepancies among annotators were discussed. At then end of the training session, the five annotators had little disagreement with the questions generated by other coders. During the annotation process, the team met once every week to review and discuss each member’s work. All QA pairs were cross-checked"
    }, {
      "heading" : "Category Count Percentage (%)",
      "text" : "by two annotators, and 10% of the QA pairs were additionally checked by the expert supervisor. This process is to ensure that the questions focused on key information to the narrative and the answers to the questions were correct.\nAgreement among Annotators The questions generated by the five coders showed a consistent pattern. All coders’ questions have similar length (average length ranging from 8 to 10 words among the coders) and have similar readability level (average readability between fourth to fifth grade among the coders). The distributions in narrative elements focused as well as implicit/explicit questions were also consistent. A detailed description of the distributions by coders is displayed in Appendix D.\nSecond Answer Annotation For the 46 stories used as the evaluation set, we annotate a second reference answer by asking an annotator to independently read the story and answer the questions generated by others. All questions were judged as answerable and thus answered by the second annotator. The second answers are used for both human QA performance estimation, and for providing multiple references in automatic QA evaluation.\n3.4 Statistics of FairytaleQA\nOverall, the resulting FairytaleQA dataset contained 10,580 questions from 278 fairytale stories. The description of story and question characteristics is presented in Table3. In FairytaleQA, action and causal relationship questions are the two most common types in the FairytaleQA, which constituting 31.6% and 27.8%, respectively, of all questions. Outcome resolution, character, and feeling questions each constitutes about 10% of all questions. Setting and prediction questions are about 5% each. Our dataset contains about 75% explicit questions and 25% implicit questions. See Table 4 for details.\nValidation of FairytaleQA for Comprehension Assessment We validated the questions in FairytaleQA using established procedures in educational assessment development (Özdemir and Akyol, 2019) and have proven that our questions have high reliability and validity. Specifically, we sampled a small subset of the questions in our dataset (11 questions generated for one story) and tested them among 120 students in kindergarten. The Cronbach’s coefficient alpha was 0.83 for the items in this story comprehension assessment, suggesting was a high internal reliability. We also linked children’s performance answering our questions to another validated language assessment (Martin and Brownell, 2011), and the correlation was strong 0.76 (p<.001), suggesting an excellent external validity."
    }, {
      "heading" : "4 Baseline Benchmark: Question Answering",
      "text" : "In the following sections, we present a couple of baseline benchmarks on both the Question Answering (QA) task and the Question Generation (QG) task with FairytaleQA. We leveraged both pretrained neural models and models fine-tuned on different QA datasets, including NarrativeQA and our dataset, FairytaleQA. The baseline results show that our FairytaleQA demonstrates challenging problems to existing approaches and those models fine-tuned on FairytaleQA can benefit from the annotations a lot to achieve significant performance improvement. We also report human performance by scoring one reference answer to the other."
    }, {
      "heading" : "4.1 Question Answering Task and Model",
      "text" : "Question Answering (QA) is a straight-forward task that our FairytaleQA dataset can contribute"
    }, {
      "heading" : "Pre-trained Models",
      "text" : ""
    }, {
      "heading" : "Fine-tuned Models",
      "text" : "to. We leveraged the commonly-used Rouge-L F1 score for the evaluation of QA performances. For each QA instance, we compared the generated answer with each of the two ground-truth answers and took the higher Rouge-L F1 score."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "Here in Table 5, we show the QA performance of a few pretrained SOTA neural-model architectures: BERT (Devlin et al., 2018), BART (Lewis et al., 2019), and DistilBERT(Sanh et al., 2019). The quality of answers generated by these pre-trained models is on par with each other. Since BART outperformed(Mou et al., 2021) other model architectures in the QA task of NarrativeQA, we decided to use BART as the backbone for our fine-tuned models.\nWe report the performance of fine-tuned BART models with the following settings: BART finetuned on NarrativeQA, which is the SOTA model reported in (Mou et al., 2021), one BART model fine-tuned on FairytaleQA only, and another BART model fine-tuned on both NarrativeQA and FairytaleQA. We note that for the QA task, the model that was fine-tuned on both large scale datasets performs much better than the other settings, and outperforms the model that fine-tuned on FairytaleQA-only by at least 6%. This result leaves around 12% on both splits between human performance and the model fine-tuned with FairytaleQA, which demonstrates that QA task is still a challenging problem for existing works on our FairytaleQA dataset."
    }, {
      "heading" : "4.3 Analysis",
      "text" : "Performance Decomposition FairytaleQA has question type annotations on all the questionanswer pairs. Therefore, it supports the decom-\nposition of performance on different types, thus gives a comprehensive picture of which reading skills the models lack the most.\nFigure 1 gives the QA performance decomposition as a wind rose chart. (the full results on both validation and test sets can be found in Table 10 in Appendix A). From the results, compared to the model trained on NarrativeQA, our FairytaleQA helped most on dimensions of Setting and Feeling with more than 10% improvement. The Character and Prediction dimensions were also improved with a large margin (7-8%). It can be seen that these four dimensions cover important fundamental elements of children’s understanding of stories. The large improvement shows that despite narrative domain focus of the NarrativeQA dataset, it fails to cover these fundamental elements, probably due to typical crowd-source workers’ limited knowledge in reading assessment. By comparison, on dimensions of Action, Causal Relationship and Outcome Resolution, our FairytaleQA brings small advantage. This is consistent with the human study in (Mou et al., 2021), which showed that most of the NarrativeQA questions are about event arguments and causal or temporal relation between events.\nOur performance decomposition also reveals ma-\njor gaps between existing state-of-the-art (SOTA) models and humans. From the results, humans were 15-20% better on Causal Relationship, Outcome Resolution and Prediction. While the gaps on the former two dimensions reflect the deficiency of current NLP models in understanding story plots, the third dimension asks the models to envision what will come next in the text, which requires connecting commonsense knowledge with the content of the text.\nThe gaps on Character and Settingwere also considerable, showing that the understanding of these fundamental reading elements is still far from accurate. Finally, it is interesting to see that the model trained on our dataset outperformed humans on the Feeling dimension. This is mainly because the answers of these Feeling questions were mostly explicitly described in the story. Therefore, it did not actually require reasoning of character’s mental states, but rather understanding which parts of the texts express the feelings.\nLearning Curve Finally, we show the learning curve of the BART QA model on our FairytaleQA. Figure 2 plots the model performance on the validation set with different sizes of training data. The curve becomes flatter after training with 6,000 QA pairs in our dataset. This shows that our dataset has a reasonably good size for fine-tuning a stateof-the-art pre-trained model; and the performance gap between models and humans requires more sophisticated reading model design rather than solely augmenting the training examples."
    }, {
      "heading" : "5 Baseline Benchmark: Question Generation",
      "text" : ""
    }, {
      "heading" : "5.1 Question Generation Task and Model",
      "text" : "In terms of the QG performance on FairytaleQA, the task was to generate question-answer pairs that\nreflect the assessed reading comprehension skills. We adopted the method from (Yao et al., 2021), which first used a rule-based method to generate over-complete answer candidates that are entities or event mentions. A BART-based model was then used to generate a question conditioned on each answer candidate. Finally, a ranker model was trained to score each question-answer pair to verify if it could be inferred from the background story section. Both the second and the third modules needed to be trained on a QA dataset. Similarly to the QA experiment, we compare the models trained on NarrativeQA versus FairytaleQA.\nWe compared the generated questions for each section against the ground truth questions for the same section. The questions were concatenated according to the order of the the appearance of their evidence in the original story. We used ROUGE-L F1 score as the evaluation metric."
    }, {
      "heading" : "5.2 Results and Analysis",
      "text" : "Table 7 gives the QG results. We observed the same pattern, where the model trained on FairytaleQA demonstrated a clear advantage on Rouge-L. Further analysis in Table 8 presented the distribution of generated question types according to the beginning word of a question (wh- words). The model trained on our dataset was able to mimic the education experts’ strategy of asking questions that assess the seven elements of reading comprehension. This can be further seen in the qualitative examples in Table 9. By comparison, the model trained on NarrativeQA tended to ask general ques-"
    }, {
      "heading" : "Ground-truth Question",
      "text" : ""
    }, {
      "heading" : "Outputs",
      "text" : ""
    }, {
      "heading" : "Ground-truth Question",
      "text" : ""
    }, {
      "heading" : "Outputs",
      "text" : "tions, which reflects the distribution of annotation behaviors of crowd-source workers. Furthermore, the crowd workers only read the abstracts to create QA-pairs in NarrativeQA, while we asked our coders to read the complete story. This may have lead to an issue where the evidence of the answer in the original text content is not detailed and obvious enough for QA-pairs in NarrativeQA. We also find from Table 11 in Appendix B that the model trained on NarrativeQA may generate questions with formats that seem to be correct, but suffer from fact error."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In summary, we constructed a large scale dataset, FairytaleQA, for the context of children’s narrative comprehension. The dataset was generated through a rigorous labeling process with educational domain experts. Through benchmark testing and qualitative analysis, our dataset is proved to add unique educational values to the narrative comprehension research and the future development of educational applications with QG and QA capacities, thus contributing to both NLP and education community. Upon paper acceptance, we will release the dataset and organize shared tasks to invite the community members to advance research in narrative comprehension."
    }, {
      "heading" : "A Decomposed QA results on 7 narrative",
      "text" : "Table 10 shows the full decomposed QA results on 7 narrative elements for both validation and test splits, in terms of BART fine-tuned on NarrativeQA, BART fine-tuned on FairytaleQA, and human performance for the experts created groundtruth QA-pairs."
    }, {
      "heading" : "B QG examples by benchmark models on event-based answers",
      "text" : "Table 11 shows two QG examples that have input of event-related ground-truth answers. We may notice that BART fine-tuned on NarrativeQA is able to generate questions that seem to be in a correct format, but suffer from fact error, while BART fine-tuned on FairytaleQA is able to generate questions that very alike ground-truth questions, and are semantically correct. Since the crowd workers only read the abstracts to create QA-pairs in NarrativeQA, in comparison, we ask our coders to read the complete story. This may leads to an issue with models fine-tuned on NarrativeQA where the evidence of the answer in the original text content is not detailed and obvious enough for QA-pairs in NarrativeQA, so that the QG model fine-tuned on NarrativeQA is not ad good as models fine-tuned on FairytaleQA in locating evidences."
    }, {
      "heading" : "C Example questions by category in",
      "text" : "FairytaleQA\nTable 12 shows example QA-pairs for different annotations in FairytaleQA dataset. There is one"
    }, {
      "heading" : "Ground-truth Question",
      "text" : ""
    }, {
      "heading" : "Outputs",
      "text" : ""
    }, {
      "heading" : "Ground-truth Question",
      "text" : ""
    }, {
      "heading" : "Outputs",
      "text" : "example QA-pair for each narrative element as well as for implicit and explicit.\nD Proportion of Each Question Type"
    } ],
    "references" : [ {
      "title" : "They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension",
      "author" : [ "Julie Alonzo", "Deni Basaraba", "Gerald Tindal", "Ronald S Carriveau." ],
      "venue" : "Assessment for Effective Intervention, 35(1):34–44.",
      "citeRegEx" : "Alonzo et al\\.,? 2009",
      "shortCiteRegEx" : "Alonzo et al\\.",
      "year" : 2009
    }, {
      "title" : "Embracing data abundance: Booktest dataset for reading comprehension",
      "author" : [ "Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst." ],
      "venue" : "arXiv preprint arXiv:1610.00956.",
      "citeRegEx" : "Bajgar et al\\.,? 2016",
      "shortCiteRegEx" : "Bajgar et al\\.",
      "year" : 2016
    }, {
      "title" : " let your characters tell their story\": A dataset for character-centric narrative understanding",
      "author" : [ "Faeze Brahman", "Meng Huang", "Oyvind Tafjord", "Chao Zhao", "Mrinmaya Sachan", "Snigdha Chaturvedi." ],
      "venue" : "arXiv preprint arXiv:2109.05438.",
      "citeRegEx" : "Brahman et al\\.,? 2021",
      "shortCiteRegEx" : "Brahman et al\\.",
      "year" : 2021
    }, {
      "title" : "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "author" : [ "Peter Clark", "Isaac Cowhey", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Carissa Schoenick", "Oyvind Tafjord." ],
      "venue" : "arXiv preprint arXiv:1803.05457.",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension",
      "author" : [ "Bhavana Dalvi", "Lifu Huang", "Niket Tandon", "Wen-tau Yih", "Peter Clark." ],
      "venue" : "Proceedings of the 2018 Conference of the North American",
      "citeRegEx" : "Dalvi et al\\.,? 2018",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic question generation and answer assessment: a survey",
      "author" : [ "Bidyut Das", "Mukta Majumder", "Santanu Phadikar", "Arif Ahmed Sekh." ],
      "venue" : "Research and Practice in Technology Enhanced Learning, 16(1):1–15.",
      "citeRegEx" : "Das et al\\.,? 2021",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2021
    }, {
      "title" : "Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narra",
      "author" : [ "Carolyn A Denton", "Mischa Enos", "Mary J York", "David J Francis", "Marcia A Barnes", "Paulina A Kulesz", "Jack M Fletcher", "Suzanne Carter" ],
      "venue" : null,
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Dimensions affecting the assessment of reading comprehension",
      "author" : [ "David J Francis", "Jack M Fletcher", "Hugh W Catts", "J Bruce Tomblin." ],
      "venue" : "Children’s reading comprehension and assessment, pages 387– 412. Routledge.",
      "citeRegEx" : "Francis et al\\.,? 2005",
      "shortCiteRegEx" : "Francis et al\\.",
      "year" : 2005
    }, {
      "title" : "Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension",
      "author" : [ "Anna S Gellert", "Carsten Elbro." ],
      "venue" : "Journal of Psychoeducational Assessment, 31(1):16–28.",
      "citeRegEx" : "Gellert and Elbro.,? 2013",
      "shortCiteRegEx" : "Gellert and Elbro.",
      "year" : 2013
    }, {
      "title" : "One’s remembered past: Narrative thinking, emotion, and the external perspective",
      "author" : [ "Peter Goldie." ],
      "venue" : "Philosophical Papers, 32(3):301–319.",
      "citeRegEx" : "Goldie.,? 2003",
      "shortCiteRegEx" : "Goldie.",
      "year" : 2003
    }, {
      "title" : "An examination of summary writing as a measure of reading comprehension",
      "author" : [ "Martha H Head", "John E Readence", "Ray R Buss." ],
      "venue" : "Literacy Research and Instruction, 28(4):1–11.",
      "citeRegEx" : "Head et al\\.,? 1989",
      "shortCiteRegEx" : "Head et al\\.",
      "year" : 1989
    }, {
      "title" : "Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier)",
      "author" : [ "Young-Suk Grace Kim." ],
      "venue" : "Scientific Studies of Reading, 21(4):310–333.",
      "citeRegEx" : "Kim.,? 2017",
      "shortCiteRegEx" : "Kim.",
      "year" : 2017
    }, {
      "title" : "Multiple choice question tests– advantages and disadvantages",
      "author" : [ "Jindrich Klufa." ],
      "venue" : "3rd International Conference on Education and Modern Educational Technologies (EMET), pages 39–42.",
      "citeRegEx" : "Klufa.,? 2015",
      "shortCiteRegEx" : "Klufa.",
      "year" : 2015
    }, {
      "title" : "The narrativeqa reading comprehension challenge",
      "author" : [ "Tomáš Kočiskỳ", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gábor Melis", "Edward Grefenstette." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:317–328.",
      "citeRegEx" : "Kočiskỳ et al\\.,? 2018",
      "shortCiteRegEx" : "Kočiskỳ et al\\.",
      "year" : 2018
    }, {
      "title" : "Booksum: A collection of datasets for longform narrative summarization",
      "author" : [ "Wojciech Kryściński", "Nazneen Rajani", "Divyansh Agarwal", "Caiming Xiong", "Dragomir Radev." ],
      "venue" : "arXiv preprint arXiv:2105.08209.",
      "citeRegEx" : "Kryściński et al\\.,? 2021",
      "shortCiteRegEx" : "Kryściński et al\\.",
      "year" : 2021
    }, {
      "title" : "A systematic review of automatic question generation for educational purposes",
      "author" : [ "Ghader Kurdi", "Jared Leo", "Bijan Parsia", "Uli Sattler", "Salam Al-Emari." ],
      "venue" : "International Journal of Artificial Intelligence in Education, 30(1):121–204.",
      "citeRegEx" : "Kurdi et al\\.,? 2020",
      "shortCiteRegEx" : "Kurdi et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring content selection in summarization of novel chapters",
      "author" : [ "Faisal Ladhak", "Bryan Li", "Yaser Al-Onaizan", "Kathleen McKeown." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5043–5054.",
      "citeRegEx" : "Ladhak et al\\.,? 2020",
      "shortCiteRegEx" : "Ladhak et al\\.",
      "year" : 2020
    }, {
      "title" : "Race: Large-scale reading comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1704.04683.",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Tellmewhy: A dataset for answering why-questions in narratives",
      "author" : [ "Yash Kumar Lal", "Nathanael Chambers", "Raymond Mooney", "Niranjan Balasubramanian." ],
      "venue" : "arXiv preprint arXiv:2106.06132.",
      "citeRegEx" : "Lal et al\\.,? 2021",
      "shortCiteRegEx" : "Lal et al\\.",
      "year" : 2021
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Cognitive and motivational predictors of narrative and informational text comprehension",
      "author" : [ "Meghan D Liebfreund." ],
      "venue" : "Reading Psychology, 42(2):177– 196.",
      "citeRegEx" : "Liebfreund.,? 2021",
      "shortCiteRegEx" : "Liebfreund.",
      "year" : 2021
    }, {
      "title" : "The development of narrative comprehension and its relation to other early reading skills",
      "author" : [ "Julie S Lynch", "Paul Van Den Broek", "Kathleen E Kremer", "Panayiota Kendeou", "Mary Jane White", "Elizabeth P Lorch." ],
      "venue" : "Reading Psychology, 29(4):327–365.",
      "citeRegEx" : "Lynch et al\\.,? 2008",
      "shortCiteRegEx" : "Lynch et al\\.",
      "year" : 2008
    }, {
      "title" : "Expressive one-word picture vocabulary test-4 (EOWPVT4)",
      "author" : [ "Nancy A Martin", "Rick Brownell." ],
      "venue" : "Academic Therapy Publications.",
      "citeRegEx" : "Martin and Brownell.,? 2011",
      "shortCiteRegEx" : "Martin and Brownell.",
      "year" : 2011
    }, {
      "title" : "A corpus and evaluation framework for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study",
      "author" : [ "Xiangyang Mou", "Chenghao Yang", "Mo Yu", "Bingsheng Yao", "Xiaoxiao Guo", "Saloni Potdar", "Hui Su." ],
      "venue" : "arXiv preprint arXiv:2106.03826.",
      "citeRegEx" : "Mou et al\\.,? 2021",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2021
    }, {
      "title" : "The development of a reading comprehension test",
      "author" : [ "Ezgi Çetinkaya Özdemir", "Hayati Akyol." ],
      "venue" : "Universal Journal of Educational Research, 7(2):563– 570.",
      "citeRegEx" : "Özdemir and Akyol.,? 2019",
      "shortCiteRegEx" : "Özdemir and Akyol.",
      "year" : 2019
    }, {
      "title" : "Assessing narrative comprehension in young children",
      "author" : [ "Alison H Paris", "Scott G Paris." ],
      "venue" : "Reading Research Quarterly, 38(1):36–76.",
      "citeRegEx" : "Paris and Paris.,? 2003",
      "shortCiteRegEx" : "Paris and Paris.",
      "year" : 2003
    }, {
      "title" : "Teaching question answer relationships, revisited",
      "author" : [ "Taffy E Raphael." ],
      "venue" : "The reading teacher, 39(6):516– 522.",
      "citeRegEx" : "Raphael.,? 1986",
      "shortCiteRegEx" : "Raphael.",
      "year" : 1986
    }, {
      "title" : "Reliability and validity in research",
      "author" : [ "Paula Roberts", "Helena Priest." ],
      "venue" : "Nursing standard, 20(44):41–",
      "citeRegEx" : "Roberts and Priest.,? 2006",
      "shortCiteRegEx" : "Roberts and Priest.",
      "year" : 2006
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Literary event detection",
      "author" : [ "Matthew Sims", "Jong Ho Park", "David Bamman." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3623–3634.",
      "citeRegEx" : "Sims et al\\.,? 2019",
      "shortCiteRegEx" : "Sims et al\\.",
      "year" : 2019
    }, {
      "title" : "Large-scale cloze test dataset created by teachers",
      "author" : [ "Qizhe Xie", "Guokun Lai", "Zihang Dai", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1711.03225.",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "It is ai’s turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset",
      "author" : [ "Bingsheng Yao", "Dakuo Wang", "Tongshuang Wu", "Tran Hoang", "Branda Sun", "Toby Jia-Jun Li", "Mo Yu", "Ying Xu." ],
      "venue" : "arXiv",
      "citeRegEx" : "Yao et al\\.,? 2021",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Reading comprehension is a complex, multidimensional cognitive process (Kim, 2017).",
      "startOffset" : 71,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : ", text matching, paraphrasing, or memorization) (Roberts and Priest, 2006).",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students’ performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005).",
      "startOffset" : 265,
      "endOffset" : 287
    }, {
      "referenceID" : 16,
      "context" : "els to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021).",
      "startOffset" : 127,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : ", 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003).",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 27,
      "context" : "We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009).",
      "startOffset" : 199,
      "endOffset" : 243
    }, {
      "referenceID" : 0,
      "context" : "We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009).",
      "startOffset" : 199,
      "endOffset" : 243
    }, {
      "referenceID" : 27,
      "context" : "Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment.",
      "startOffset" : 194,
      "endOffset" : 217
    }, {
      "referenceID" : 14,
      "context" : "First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills, even for models trained on standard QA datasets (NarrativeQA (Kočiskỳ et al., 2018)).",
      "startOffset" : 334,
      "endOffset" : 356
    }, {
      "referenceID" : 14,
      "context" : "NarrativeQA (Kočiskỳ et al., 2018) is one of the representative datasets.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "Indeed, a study (Mou et al., 2021) confirmed that NarrativeQA contains a significant amount of questions that focus on narrative events and the relationship among events.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "Therefore, datasets assessing the education of natural science (Clark et al., 2018; Dalvi et al., 2018) are not covered.",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "Therefore, datasets assessing the education of natural science (Clark et al., 2018; Dalvi et al., 2018) are not covered.",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "BookTest (Bajgar et al., 2016) is an automatically constructed cloze-style QA dataset based on a collection of narrative texts retrieved from project Gutenberg.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "Another dataset, TellMeWhy (Lal et al., 2021), aims to facilitate and assess understanding of",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "Moreover, TellMeWhy was built upon ROCStories (Mostafazadeh et al., 2016) and thus only examine comprehension on incomplete story sections, which may have limited the dataset’s ability to assess macro-level summarization and inference making.",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "RACE (Lai et al., 2017) is a large-scale dataset consisting of comprehension questions from English exams for Chinese middle and high school students.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "These two genres require slightly different comprehension skills (Liebfreund, 2021) and students perform differently based on what genres of text they read (Denton et al.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "CLOTH (Xie et al., 2017) is a cloze-style dataset also collected from English exams.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "However, this dataset shares certain limitations inherent to multiple choice formats (Klufa, 2015).",
      "startOffset" : 85,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "Several datasets, such as NovelChapters (Ladhak et al., 2020) and BookSum (Kryściński et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : ", 2020) and BookSum (Kryściński et al., 2021), evaluate models’ comprehension through summarization tasks.",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "However, there have been debates of whether comprehension can be assessed solely through summarization (Head et al., 1989), as summarization poses a high demand on writing that confounds the",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "The LiSCU dataset (Brahman et al., 2021) targets readers’ understanding of characters, and Sims et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "Categorization via Narrative Elements or Relations FairytaleQA is intended to include QA pairs that capture the seven narrative elements/relations that are verified in prior educational research (Paris and Paris, 2003).",
      "startOffset" : 195,
      "endOffset" : 218
    }, {
      "referenceID" : 28,
      "context" : "Using a combination of explicit and implicit questions yield an assessment with more balanced difficulty (Raphael, 1986).",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "(Martin and Brownell, 2011), and the correlation was strong 0.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "few pretrained SOTA neural-model architectures: BERT (Devlin et al., 2018), BART (Lewis et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : ", 2018), BART (Lewis et al., 2019), and DistilBERT(Sanh et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "outperformed(Mou et al., 2021) other model architectures in the QA task of NarrativeQA, we decided to use BART as the backbone for our fine-tuned models.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "We report the performance of fine-tuned BART models with the following settings: BART finetuned on NarrativeQA, which is the SOTA model reported in (Mou et al., 2021), one BART model fine-tuned on FairytaleQA only, and another BART model fine-tuned on both NarrativeQA and FairytaleQA.",
      "startOffset" : 148,
      "endOffset" : 166
    }, {
      "referenceID" : 25,
      "context" : "This is consistent with the human study in (Mou et al., 2021), which showed that most of the NarrativeQA questions are about event arguments and causal or temporal relation between events.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 33,
      "context" : "We adopted the method from (Yao et al., 2021), which first used a rule-based method to generate over-complete answer candidates that are entities or event mentions.",
      "startOffset" : 27,
      "endOffset" : 45
    } ],
    "year" : 0,
    "abstractText" : "Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing from education domains where QA is also used to train children’s narrative comprehension, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements/relations. Our dataset is valuable in two folds: First, with annotations on particular reading skills required for answering each question, FairytaleQA decomposes the otherwise scarce performance into multiple analysis dimensions that are consistent to human-language-learning assessment. We ran existing QA models on our dataset, and confirmed that this annotation helps assess models’ fine-grained learning skills. Second, the dataset supports generating questions (QG) in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.",
    "creator" : null
  }
}
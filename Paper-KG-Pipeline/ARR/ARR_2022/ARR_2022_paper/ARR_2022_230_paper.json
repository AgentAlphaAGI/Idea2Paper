{
  "name" : "ARR_2022_230_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Text categorization is the task of assigning topical categories to text units such as documents, social media postings, or news articles. Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019).\nThere are approaches based on a Bag of Words (BoW) that perform text categorization purely on the basis of a multiset of tokens. Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText (Bojanowski et al., 2017), which uses a linear layer on top of pretrained word embed-\ndings. These models count the occurrence of all tokens in the input sequence, while disregarding word position and order, and then rely on word embeddings and fully connected feedforward layer(s). We call these BoW-based models.\nAmong the most popular recent methods for text categorization are graph-based models such as TextGCN (Yao et al., 2019) that first induce a synthetic word-document co-occurence graph over the corpus and subsequently apply a graph neural network (GNN) to perform the classification task. Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN (Liu et al., 2020), and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.\nFinally, there is the well-known Transformer (Vaswani et al., 2017) universe with models such as BERT (Devlin et al., 2019) and its sizereduced variants such as DistilBERT (Sanh et al., 2019). Here, the input is a (fixed-length) sequence of tokens, which is then fed into multiple layers of self-attention. Lightweight versions such as DistilBERT and others (Tay et al., 2020; Fournier et al., 2021) use less parameters but operate on the same type of input. Together with recurrent models such as LSTMs, we call these sequence-based models.\nIn this paper, we hypothesize that text categorization can be very well conducted by simple but effective BoW-based models. We investigate this research question in three steps: First, we conduct an in-depth analysis of the literature. We review the key research in the field of text categorization. From this analysis, we derive the different families of methods, the established benchmark datasets, and identify the top performing methods. We decide for which models we report numbers from the literature and which models we run on our own. Overall, we compare 16 different methods from the families of BoW-based models (8 methods), sequence-based models (3 methods), and graphbased models (5 methods). We run our own experi-\nments for 7 of these methods on 5 text categorization datasets, while we report the results from the literature for the remaining methods.\nThe result is surprising: Our own BoW-based MLP, called the WideMLP, with only one wide hidden layer, outperforms many of the recent graphbased models for inductive text categorization (Yao et al., 2019; Liu et al., 2020; Ragesh et al., 2021). Moreover, we did not find any reported scores for BERT-based methods from the sequence-based family. Thus, we fine-tuned our own BERT (Devlin et al., 2019) and DistilBERT (Sanh et al., 2019). These models set a new state of the art. On a metalevel, our study shows that MLPs have largely been ignored as competitor methods in experiments. It seems as if MLPs have been forgotten as baseline in the literature, which instead is focusing mostly on other advanced Deep Learning architectures. Considering strong baselines is, however, an important means to argue about true scientific advancement (Shen et al., 2018; Dacrema et al., 2019). Simple models are also often preferred in industry due to lower operational and maintenance costs.\nBelow, we introduce our methodology and results from the literature study. Subsequently, we introduce the families of models in Section 3. Thereafter, we describe the experimental procedure in Section 4. We present the results of our experiments in Section 5 and discuss our findings in Section 6, before we conclude."
    }, {
      "heading" : "2 Literature on Text Categorization",
      "text" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016). These cover the range from shallow to deep classification models. Second, we have screened for literature in key NLP and AI venues. Finally, we have complemented our search by checking results and papers on paperswithcode.com. On the basis of this input, we have determined three families of methods and benchmark datasets (see Table 2). We focus our analysis on identifying models per family showing strong performance and select the methods to include in our study. For all models, we have verified that the same train-test split is used. We check whether modified versions of the datasets have been used (e. g., less classes), to avoid bias and wrongfully giving advantages.\nBoW-based Models Classical machine learning models that operate on a BoW-based input are extensively discussed in two surveys (Kowsari et al., 2019; Kadhim, 2019) and other comparison studies (Galke et al., 2017). Iyyer et al. (2015) proposed DAN, which combine word embeddings and deep feedforward networks. It is an MLP with 1-6 hidden layers, non-linear activation, dropout, and AdaGrad as optimization method. The results suggest to use pretrained embeddings such as GloVe (Pennington et al., 2014) over a randomly initialized neural bag of-words (Kalchbrenner et al., 2014) as input. In fastText (Bojanowski et al., 2017; Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classification. Furthermore, Shen et al. (2018) explore embedding pooling variants and find that SWEM can rival approaches based on recurrent (RNN) and convolutional neural networks (CNN). We consider fastText, SWEM, and a DAN-like deeper MLP in our comparison.\nNote that those approaches that rely on logistic regression on top of pretrained word embeddings, e. g., fastText, share a similar architecture as an MLP with one hidden layer. However, the standard training protocol involves pretraining the word embedding on large amounts of unlabeled text and then freezing the word embeddings while training the logistic regression (Mikolov et al., 2013).\nGraph-based Models Using graphs induced from text for the task of text categorization has a long history in the community. An early work is the term co-occurrence graph of the KeyGraph algorithm (Ohsawa et al., 1998). The graph is split into segments, representing the key concepts in the document. Co-occurence graphs have also been used for automatic keyword extraction such as in RAKE (Rose et al., 2010) and can be also used for classification (Zhang et al., 2021).\nModern approaches exploit this idea in combination with graph neural networks (GNN) (Hamilton, 2020). Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN (Liu et al., 2020) as well as HeteGCN (Ragesh et al., 2021), HyperGAT (Ding et al., 2020), and DADGNN (Liu et al., 2020). We briefly discuss these models: In TextGCN, the authors set up a graph based on word-word connections given by window-based pointwise mutual information and word-document TF-IDF scores. They use a one-hot encoding as node features and apply a\ntwo-layer graph convolutional network (Kipf and Welling, 2017) on the graph to carry out the node classification task. HeteGCN combines ideas from Predictive Text Embedding (Tang et al., 2015) and TextGCN and split the adjacency matrix into its word-document and word-word sub-matrices and fuse the different layers’ representations when required. TensorGCN uses multiple ways of converting text data into graph data including a semantic graph created with an LSTM, a syntactic graph created by dependency parsing, and a sequential graph based on word co-occurrence. HyperGAT extended the idea of text-induced graphs for text classification to hypergraphs. The model uses graph attention and two kinds of hyperedges. Sequential hyperedges represent the relation between sentences and their words. Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2003). Finally, DADGNN is a graph-based approach that uses attention diffusion and decoupling techniques to tackle oversmoothing of the GNN and to be able to stack more layers.\nIn TextGCN’s original transductive formulation, the entire graph including the test set needs to be known for training. This may be prohibitive in practical applications as each batch of new documents would require retraining the model. When these methods are adapted for inductive learning, where the test set is unseen, they achieve notably lower scores (Ragesh et al., 2021). GNNs for text classification use corpus statistics, e. g., pointwise mutual information (PMI), to connect related words in a graph (Yao et al., 2019). When these were omitted, the GNNs would collapse to bag-of-words MLPs. Thus, GNNs have access to more information than BoW-MLPs. GloVe (Pennington et al., 2014) also captures PMI corpus statistics, which is why we include an MLP on GloVe input representations.\nSequence models: RNN and CNN Recurrent neural networks (RNN) are a natural choice for any NLP task However, it turned out to be challenging to find numbers reported on text categorization in the literature that can be used as references. The bidirectional LSTM with two-dimensional max pooling BLSTM-2DCNN (Zhou et al., 2016) has been applied on a stripped-down to 4 classes version of the 20ng dataset. Thus, the high score of 96.5 reported for 4ng cannot be compared with papers applied on the full 20ng dataset. Also TextRCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major\ncategories in the 20ng dataset. The results of TextRCNN is identical with BLSTM-2DCNN. For the MR dataset, BLSTM-2DCNN provides no information on the specific splitting of the dataset. RNNCapsule (Wang et al., 2018) is a sentiment analysis method reaching an accuracy of 83.8 on the MR dataset, but with a different train-test split.Lyu and Liu (2020) combine a 2D-CNN with bidirectional RNN. Another work applying a combination of a convolutional layer and an LSTM layer is by Wang et al. (2019b). The authors experiment with five English and two Chinese datasets, which are not in the set of representative datasets we identified. The authors report that their approach outperforms existing models like fastText on two of the five English datasets and both Chinese datasets.\nSequence models: Transformers Surprisingly, only few works consider Transformer models for topical text classification. A recent work shows that BERT outperforms classic TF-IDF BoW approaches on English, Chinese, and Portuguese text classification datasets (González-Carvajal and Garrido-Merchán, 2020). We have not found any results of transformer-based models reported on those text categorization datasets that are commonly used in the graph-based approaches.\nTherefore, we fine-tune BERT (Devlin et al., 2019) and DistilBERT (Sanh et al., 2019) on those datasets ourselves. BERT is a large pretrained language model on the basis of Transformers. DistilBERT (Sanh et al., 2019) is a distilled version of BERT with 40% reduced parameters while retaining 97% of BERT’s language understanding capabilities. TinyBERT (Jiao et al., 2020) and MobileBERT (Sun et al., 2020) would be similarly suitable alternatives, among others. We chose DistilBERT because it can be fine-tuned independently from the BERT teacher. and its inference times are 60% faster than BERT, which makes it is more likely to be reusable by labs with limited resources.\nSummary From our literature survey, we see that all recent methods are based on graphs. BoW-based methods are hardly found in experiments, while, likewise surprisingly, Transformer-based sequence models are extremely scarce in the literature on topical text categorization. The recent surveys on text categorization include both classical and Deep Learning models, but none considered a simple MLP except for the inclusion of DAN (Iyyer et al., 2015) in Li et al. (2020)."
    }, {
      "heading" : "3 Models for Text Categorization",
      "text" : "We formally introduce the three families of models for text categorization, namely the BoW-based, graph-based, and sequence-based models. Table 1 summarizes the key properties of the approaches: whether they require a synthetic graph, whether word position is reflected in the model, whether the model can deal with arbitrary length text, and whether the model is capable of inductive learning."
    }, {
      "heading" : "3.1 BoW-Based Text Categorization",
      "text" : "Under pure BoW-based text categorization, we denote approaches that are not order-aware and operate only on the multiset of words from the input document. Given paired training examples (x, y) ∈ D, each consisting of a bag-of-words x ∈ Rnvocab and a class label y ∈ Y, the goal is to learn a generalizable function ŷ = f (BoW)θ (x) with parameters θ such that argmax(ŷ) preferably equals the true label y for input x.\nAs BoW-based models, we consider a one hidden layer WideMLP (i. e., two layers in total). We further experiment with pure BoW, TF-IDF weighted, or averaged GloVe input representations and two hidden layers WideMLP-2. We also list the numbers for fastText, SWEM, and logistic regression from Ding et al. (2020) in our comparison."
    }, {
      "heading" : "3.2 Graph-Based Text Categorization",
      "text" : "Graph-based text categorization approaches first set up a synthetic graph on the basis of the text corpus D in the form of an adjacency matrix Â := make-graph(D). For instance, TextGCN the graph is set up in two parts: word-word connections modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document. Then, a parameterized function f (graph)θ (X, Â) is learned that uses the graph as input, where X are the node features. The graph is composed of word and document nodes, each receiving its own embedding (by set-\nting X = I). In inductive learning, however, there is no embedding of the test documents. Note that the graph-based approaches from the current literature such as TextGCN also disregard word order, similar to the BoW-based models described above. A detailed discussion of the connection between TextGCN and MLP is provided in the Appendix B.\nWe consider top performing graph-based models from the literature, namely TextGCN along with its successors HeteGCN, TensorGCN, HyperGAT, DADGNN, as well as simplified GCN (SGC) (Wu et al., 2019). We do not run our own experiments for the graph-based models but rely on the original work and extensive studies by Ding et al. (2020) and Ragesh et al. (2021)."
    }, {
      "heading" : "3.3 Sequence-Based Text Categorization",
      "text" : "We consider RNNs, LSTMs, and Transformers as sequence-based models. These models are aware of the order of the words in the input text in the sense that they are able to exploit word order information. Thus, the key difference to the BoW-based and graph-based families is that the word order is reflected by sequence-based model. The model signature is ŷ = f (sequence)θ ((x1, x1, . . . , xk)), where k is the (maximum) sequence length. Word position is modeled by a dedicated positional encoding. For instance, in BERT each position is associated with an embedding vector that is added to the word embedding at input level.\nFor the sequence-based models, we run our own experiments with BERT and DistilBERT, while reporting the scores of a pretrained LSTM from Ding et al. (2020) for comparison."
    }, {
      "heading" : "4 Experimental Apparatus",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We use the same datasets and train-test split as in TextGCN (Yao et al., 2019). Those datasets are 20ng, R8, R52, ohsumed, and MR. Twenty\nNewsgroups (20ng)1 (bydate version) contains long posts categorized into 20 newsgroups. The mean sequence length is 551 words with a standard deviation of 2,047. R8 and R52 are subsets of the Reuters 21578 news dataset with 8 and 52 classes, respectively. The mean sequence length and SD is 119± 128 words for R8, and 126± 133 words for R52. Ohsumed2 is a corpus of medical abstracts from the MEDLINE database that are categorized into diseases (one per abstract). The mean sequence length is 285 ± 123 words. Movie Reviews (MR)3 (Pang and Lee, 2005), split by Tang et al. (2015), is a binary sentiment analysis dataset on sentence level (mean sequence length and SD: 25± 11). Table 2 shows the dataset characteristics."
    }, {
      "heading" : "4.2 Inductive and Transductive Setups",
      "text" : "We distinguish between a transductive and an inductive setup for text categorization. In the transductive setup, as used in TextGCN, the test documents are visible and actually used for the preprocessing step. In the inductive setting, the test documents remain unseen until test time (i. e., they are not available for preprocessing). We report the scores of the graph-based models for both setups from the literature, where available. BoW-based and sequence-based models are inherently inductive. Ragesh et al. (2021) have evaluated a variant of TextGCN that is capable of inductive learning, which we include in our results, too."
    }, {
      "heading" : "4.3 Procedure and Hyperparameter Settings",
      "text" : "We have extracted accuracy scores from the literature according to our systematic selection from Section 2. Below, we provide a detailed description of the procedure for the models that we have run ourselves. We borrow the tokenization strategy\n1http://qwone.com/~jason/20Newsgroups/ 2http://disi.unitn.it/moschitti/\ncorpora.htm 3https://www.cs.cornell.edu/people/ pabo/movie-review-data/\nfrom BERT (Devlin et al., 2019) along with its uncased vocabulary. The tokenizer relies primarily on WordPiece (Wu et al., 2016) for a high coverage while maintaining a small vocabulary.\nTraining our BoW-Models. Our WideMLP has one hidden layer with 1,024 rectified linear units (one input-to-hidden and one hidden-to-output layer). We apply dropout after each hidden layer, notably also after the initial embedding layer. Only for GloVe+WideMLP, neither dropout nor ReLU is applied to the frozen pretrained embeddings but only on subsequent layers. The variant WideMLP2 has two ReLU-activated hidden layers (three layers in total) with 1, 024 hidden units each. While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018; Nakkiran et al., 2020). In pre-experiments, we realized that MLPs are not very sensitive to hyperparameter choices. Therefore, we optimize crossentropy with Adam (Kingma and Ba, 2015) and its default learning rate of 10−3, a linearly decaying learning rate schedule and train for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity, along with a dropout ratio of 0.5.\nFine-tuning our BERT models. For BERT and DistilBERT, we fine-tune for 10 epochs with a linearly decaying learning rate of 5 · 10−5 and an effective batch size of 128 via gradient accumulation of 8 x 16 batches. We truncate all inputs to 512 tokens. To isolate the influence of word order on BERT’s performance, we conduct two further ablations. First, we set all position embeddings to zero and disable their gradient (BERT w/o pos ids). By doing this, we force BERT to operate on a bag-ofwords without any notion of word order or position. Second, we shuffle each sequence to augment the training data. We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuffle augment)."
    }, {
      "heading" : "4.4 Measures",
      "text" : "We report accuracy as evaluation metric, which is equivalent to Micro-F1 in single-label classification (see Appendix C). We repeat all experiments five times with different random initialization of the parameters and report the mean and standard deviation of these five runs."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Effectiveness",
      "text" : "Table 3 shows the accuracy scores for the text categorization models on the five datasets. All graphbased models in the transductive setting show similar accuracy scores (maximum difference is 2 points). As expected, the scores decrease in the inductive setting up to a point where they are matched or even outperformed by our WideMLP.\nIn the inductive setting, the WideMLP models perform best among the BoW models, in particular, TFIDF+WideMLP and WideMLP on an unweighted BoW. The best-performing graph-based model is HyperGAT, yet DADGNN has a slight advantage on R8, R52, and MR. For the sequencebased models, BERT attains the highest scores, closely followed by DistilBERT.\nThe strong performance of WideMLP rivals all graph-based techniques reported in the literature, in particular, the recently published graph-inducing methods. MLP only falls behind HyperGAT, which relies on topic models to set up the graph. Another observation is that 1 hidden layer (but wide) is sufficient for the tasks, as the scores for MLP variants with 2 hidden layers are lower. We further observe that both pure BoW and TF-IDF weighted BoW lead to better results than approaches that exploit pretrained word embeddings such as GloVe-MLP, fastText, and SWEM.\nWith its immense pretraining, BERT yields the overall highest scores, closely followed by DistilBERT. DistilBERT outperforms HyperGAT by 7 points on the MR dataset while being on par on the others. BERT outperforms the strongest graphbased competitor, HyperGAT, by 8 points on MR, 1.5 points on ohsumed, 1 point on R52 and R8, and 0.5 points on 20ng.\nOur results further confirm that position embeddings are important for BERT with a notable decrease when those are omitted. Augmenting the data with shuffled sequences has led to neither a consistent decrease nor increase in performance."
    }, {
      "heading" : "5.2 Efficiency",
      "text" : "Parameter Count of the Models Table 4 lists the parameter counts of the models. Even though the MLP is fully-connected on top of a bag-ofwords with the dimensionality of the vocabulary size, it has only half of the parameters as DistilBERT and a quarter of the parameters of BERT. Using TF-IDF does not change the number of\nmodel parameters. Due to the high vocabulary size, GloVe-based models have a high number of parameters, but the majority of those is frozen, i. e., does not get gradient updates during training.\nRuntime Performance of the Models We provide the total running times in Table 5 as observed while conducting the experiments on a single NVIDIA A100-SXM4-40GB card. All WideMLP variants are an order of magnitude faster than DistilBERT when considering the average runtime per epoch. DistilBERT is twice as fast as the original BERT. The transformers are only faster than BoW models on the MR dataset. This is because the sequences in the MR dataset are much shorter and less O(L2) attention weights have to be computed."
    }, {
      "heading" : "6 Discussion",
      "text" : "Key Insights Our experiments show that our MLP models using BoW outperform the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting. Furthermore, the MLP models are comparable to HyperGAT. Only transformer-based BERT and DistilBERT models outperform our MLP and set a new state-of-the-art. This result is important for two reasons: First, the strong performance of a pure BoW-MLP questions the added value of synthetic graphs in models like TextGCN to the text categorization task. Only HyperGAT, which uses the expensive Latent Dirichlet Allocation for computing the graph, slightly outperforms our BoWWideMLP in two out of five datasets. Thus, we argue that using strong baseline models for text classification is important to assess the true scientific advancement (Dacrema et al., 2019).\nSecond, in contrast to conventional wisdom (Iyyer et al., 2015), we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer. Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity. Furthermore, we experiment with more hidden layers (see WideMLP-2), but do not observe any improvement when the single hidden layer is sufficiently wide. A possible explanation is that already a single hidden layer is sufficient to approximate any compact function to an arbitrary degree of accuracy depending on the width of the hidden layer (Cybenko, 1989).\nFinally, a new state-of-the-art is set by the transformer model BERT, which is not very surpris-\ning. However, as our efficiency analysis shows, the MLPs require only a fraction of the parameters and are faster in their combined training and inference time except for the MR dataset. The attention mechanism of (standard) Transformers is quadratic in the sequence length, which leads to slower processing of long sequences. With larger batches, the speed of the MLP could be increased even further.\nDetailed Discussion of Results Graph-based models come with high training costs, as not only the graph has to be first computed, but also a GNN has to be trained. For standard GNN methods, the whole graph has to fit into the GPU memory and mini-batching is nontrivial, but possible with dedicated sampling techniques for GNNs (Fey et al., 2021). Furthermore, the original TextGCN is inher-\nently transductive, i. e., it has to be retrained whenever new documents appear. Strictly transductive models are effectively useless in practice (Lu et al., 2019) except for applications, in which a partially labeled corpus needs to be fully annotated. However, recent extensions such as HeteGCN, HyperGAT, and DADGNN already relax this constraint and enable inductive learning. Nevertheless, worddocument graphs require O(N2) space, where N is the number of documents plus the vocabulary size, which is a hurdle for large-scale applications.\nThere are also tasks where the natural structure of the graph data provides more information than the mere text, e. g., citations networks or connections in social graphs. In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017; Veličković et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018). GNNs also find application in various NLP tasks, other than classification (Wu et al., 2021).\nAn interesting factor is the ability of the models to capture word order. BoW models disregard word order entirely and yield good results, but still fall behind order-aware Transformer models. In an extensive study, Conneau et al. (2018) have shown\nthat memorizing the word content (which words appear at all) is most indicative of downstream task performance. Sinha et al. (2021) have experimented with pretraining BERT by disabling word order during pretraining and show that it makes surprisingly little difference for fine-tuning. In their study, word order is preserved during fine-tuning. In our experiments, we have conducted complementary experiments: we have used a BERT model that is pretrained with word order, but we have deactivated the position encoding during fine-tuning. Our results show that there is a notable drop in performance but the model does not fail completely.\nOther NLP tasks such as question answering (Rajpurkar et al., 2016) or natural language inference (Wang et al., 2019a) can also be regarded as text classification on a technical level. Here, the positional information of the sequence is more important than for pure topical text categorization. One can expect that BoW-based models perform worse than sequence-based models.\nGeneralizability We expect that similar observations would be made on other text classification datasets because we have already covered a range of different characteristics: long and short texts, topical categorization (20ng, Reuters, and Ohsumed) and sentiment prediction (MR) in the domains of forum postings, news, movie reviews, and medical abstracts. Our results are in line with those from other fields, who have reported a resurgence of MLPs. For example, in business prediction, an MLP baseline outperforms various other Deep Learning models (Venugopal et al., 2021; Yedida et al., 2021). In computer vision, Tolstikhin et al. (2021) and Melas-Kyriazi (2021) attentionfree MLP models are on par with the Vision Transformer (Dosovitskiy et al., 2021). In natural language processing, Liu et al. (2021a) show similar results, while acknowledging that a small attention module is necessary for some tasks.\nThreats to Validity We acknowledge that the experimental datasets are limited to English. While word order is important in the English language, it is notable that methods that discard word order still work well for text categorization. Another possible bias is the comparability of the results. However, we carefully checked all relevant parameters such as the train/test split, the number of classes in the datasets, if datasets have been pre-processed in such a way that, e. g., makes a task easier like reducing the number of classes, the training procedure, and the reported evaluation metrics. Regarding our efficency analysis, we made sure to report numbers for the parameter count and a measure for the speed other than FLOPs, as recommended by Dehghani et al. (2021). Since runtime is heavily dependant on training parameters such as batch size, we complement this with asymptotic complexity.\nPractical Impact and Future Work Our study has an immediate impact on practitioners who seek to employ robust text categorization models in research projects and in industrial operational environments. Furthermore, we advocate to use an MLP baseline in future text categorization research, for which we provide concrete guidelines in Appendix A. As future work, it would be interesting to analyze multi-label classification tasks and to compare with hierarchical text categorization methods (Peng et al., 2018; Xiao et al., 2019). Another interesting yet challenging setting would be few-shot classification (Brown et al., 2020)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We argue that a wide multi-layer perceptron enhanced with today’s best practices should be considered as a strong baseline for text classification tasks. In fact, the experiments show that our WideMLP is oftentimes on-par or even better than recently proposed models that synthesize a graph structure from the text. We make the source code available upon acceptance of the paper."
    }, {
      "heading" : "A Practical Guidelines for Designing a WideMLP",
      "text" : "On the basis of our results, we provide recommendations for designing a WideMLP baseline.\nTokenization We recommend using modern subword tokenizers such as BERT-like WordPiece or SentencePiece that yield a high coverage while needing a relatively small vocabulary.\nInput representation In contrast to conventional wisdom (Iyyer et al., 2015), we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer. Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity.\nDepth vs width In text classification, width seems more important than depth. We recommend to use a single, wide hidden layer, i. e., one input-tohidden and one hidden-to-output layer, e. g., with 1,024 hidden units and ReLU activation. While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018; Nakkiran et al., 2020).\nHaving a fully-connected layer on-top of a bagof-words leads to a high number of learnable parameters. Still, the wide first input-to-hidden layer can be implemented efficiently by using an embedding layer followed by aggregation, which avoids large matrix multiplications.\nIn our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by Iyyer et al. (2015), but it might help for other, more challenging, datasets.\nOptimization and Regularization We seek to find an optimization strategy that does not require dataset-specific hyperparameter tuning. This comprises optimizing cross-entropy with Adam (Kingma and Ba, 2015) and default learning rate 10−3, a linearly decaying learning rate schedule and training for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity. For regularization during this prolonged training, we suggest to use a high dropout ratio of 0.5. Regarding initialization, we rely on framework defaults, i. e., N (0, 1) for the initial embedding layer\nand random uniform U(− √ dinput, √ doutput) for subsequent layers’ weight and bias parameters."
    }, {
      "heading" : "B Connection between BoW-MLP and TextGCN",
      "text" : "TextGCN uses the PMI matrix to set up edge weights for word-word connections. A single layer Text-GCN is a bow MLP, except for the document embedding. The one hop neighbors are words which are aggregated after a nonlinear transform. The basic GCN equation reveals that the order of transformation and neighborhood aggregation is equivalent. The document embedding implies that TextGCN is a semisupervised technique. Truly new documents would need a special treatment such as using an all zero embedding vector.\nA two-layer MLP can be characterized by the equation ŷ = W (2)σ(W (1)x+ b(1)) + b(2). The first layer can be replaced by an embedding layer such that H = XE, where X is the weighted term-document matrix.\nThe first layer of Text-GCN is equivalent to aggregating embedding vectors. A standard GCN layer with shared weights has the form (assuming self-loops have been inserted)\nhi = ∑ j∈N(i) αijW (1)xj + b\nNow in Text-GCN node features are given by the identity, such that xj is one-hot. Then we can rewrite the first layer of Text-GCN as an aggregation of embeddings E. We gain\nhi = ∑ j∈N(i) αijEj\nas Wx+ b may be replaced by an embedding matrix if applied to one hot vectors x. Now E contains two types of embedding vectors: word embeddings and document embeddings corresponding to word nodes and document nodes. We see that the first layer of Text-GCN is essentially an aggregation of word embeddings plus the document embedding. Only with a second layer, TextGCN considers the embedding of other documents whose words are related to the present documents’ words."
    }, {
      "heading" : "C Equivalence of Micro-F1 and Accuracy in Multiclass Classification",
      "text" : "In multiclass classification, we have a single true label for each instance and the predictions are constrained to a single prediction per instance. As a\nconsequence, the measures accuracy and Micro-F1 coincide to the same formula.\nMicro F1 aggregates true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) globally. It can be expressed as: Micro− F1 = 2 ∑\ncTPc 2 ∑ cTPc + ∑ c FPc + ∑ c FNc ,\nwhere c iterates over all classes. While the accuracy can be expressed as:\nAcc =\n∑ cTPc + ∑ cTNc∑\ncTPc + ∑ cTNc + ∑ c FPc + ∑ c FNc\nIn multiclass classification, every true positive is also a true negative for all other classes. When summing those up over the entire dataset, we obtain∑\nc TPc = ∑ c TNc.\nThus, we can rewrite 2 ∑ c TPc = ∑ c TPc + ∑ c TNc\nand see that the Micro-F1 and accuracy are equivalent in the multiclass (a.k.a. single-label) case."
    } ],
    "references" : [ {
      "title" : "A survey on data augmentation for text classification",
      "author" : [ "Markus Bayer", "Marc-André Kaufhold", "Christian Reuter." ],
      "venue" : "CoRR, abs/2107.03158.",
      "citeRegEx" : "Bayer et al\\.,? 2021",
      "shortCiteRegEx" : "Bayer et al\\.",
      "year" : 2021
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "J. Mach. Learn. Res., 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomás Mikolov." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "What you can cram into a single \\$&!#* vector: Probing sentence embeddings for linguistic properties",
      "author" : [ "Alexis Conneau", "Germán Kruszewski", "Guillaume Lample", "Loïc Barrault", "Marco Baroni." ],
      "venue" : "ACL (1), pages 2126–2136. ACL.",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "George Cybenko." ],
      "venue" : "Math. Control. Signals Syst., 2(4):303–314.",
      "citeRegEx" : "Cybenko.,? 1989",
      "shortCiteRegEx" : "Cybenko.",
      "year" : 1989
    }, {
      "title" : "Are we really making much progress? A worrying analysis of recent neural recommendation approaches",
      "author" : [ "Maurizio Ferrari Dacrema", "Paolo Cremonesi", "Dietmar Jannach." ],
      "venue" : "RecSys, pages 101– 109. ACM.",
      "citeRegEx" : "Dacrema et al\\.,? 2019",
      "shortCiteRegEx" : "Dacrema et al\\.",
      "year" : 2019
    }, {
      "title" : "The efficiency misnomer",
      "author" : [ "Mostafa Dehghani", "Anurag Arnab", "Lucas Beyer", "Ashish Vaswani", "Yi Tay." ],
      "venue" : "CoRR, abs/2110.12894.",
      "citeRegEx" : "Dehghani et al\\.,? 2021",
      "shortCiteRegEx" : "Dehghani et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT (1), pages 4171–4186. ACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Be more with less: Hypergraph attention networks for inductive text classification",
      "author" : [ "Kaize Ding", "Jianling Wang", "Jundong Li", "Dingcheng Li", "Huan Liu." ],
      "venue" : "EMNLP (1), pages 4927–4936. ACL.",
      "citeRegEx" : "Ding et al\\.,? 2020",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2020
    }, {
      "title" : "Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings",
      "author" : [ "Matthias Fey", "Jan Eric Lenssen", "Frank Weichert", "Jure Leskovec." ],
      "venue" : "ICML, volume 139 of Proceedings of Machine Learning Research, pages 3294–3304.",
      "citeRegEx" : "Fey et al\\.,? 2021",
      "shortCiteRegEx" : "Fey et al\\.",
      "year" : 2021
    }, {
      "title" : "A practical survey on faster and lighter transformers",
      "author" : [ "Quentin Fournier", "Gaétan Marceau Caron", "Daniel Aloise." ],
      "venue" : "CoRR, abs/2103.14636.",
      "citeRegEx" : "Fournier et al\\.,? 2021",
      "shortCiteRegEx" : "Fournier et al\\.",
      "year" : 2021
    }, {
      "title" : "Using titles vs",
      "author" : [ "Lukas Galke", "Florian Mai", "Alan Schelten", "Dennis Brunsch", "Ansgar Scherp." ],
      "venue" : "fulltext as source for automated semantic document annotation. In K-CAP, pages 20:1–20:4. ACM.",
      "citeRegEx" : "Galke et al\\.,? 2017",
      "shortCiteRegEx" : "Galke et al\\.",
      "year" : 2017
    }, {
      "title" : "Comparing BERT against traditional machine learning text classification",
      "author" : [ "Santiago González-Carvajal", "Eduardo C. GarridoMerchán." ],
      "venue" : "CoRR, abs/2005.13012.",
      "citeRegEx" : "González.Carvajal and GarridoMerchán.,? 2020",
      "shortCiteRegEx" : "González.Carvajal and GarridoMerchán.",
      "year" : 2020
    }, {
      "title" : "Graph Representation Learning",
      "author" : [ "William L. Hamilton" ],
      "venue" : null,
      "citeRegEx" : "Hamilton.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hamilton.",
      "year" : 2020
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan L. BoydGraber", "Hal Daumé III." ],
      "venue" : "ACL (1), pages 1681–1691. ACL.",
      "citeRegEx" : "Iyyer et al\\.,? 2015",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Tinybert: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pages 4163–",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomás Mikolov." ],
      "venue" : "EACL (2), pages 427–431. ACL.",
      "citeRegEx" : "Joulin et al\\.,? 2017",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2017
    }, {
      "title" : "Survey on supervised machine learning techniques for automatic text classification",
      "author" : [ "Ammar Ismael Kadhim." ],
      "venue" : "Artif. Intell. Rev., 52(1):273–292.",
      "citeRegEx" : "Kadhim.,? 2019",
      "shortCiteRegEx" : "Kadhim.",
      "year" : 2019
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "ACL (1), pages 655–665. ACL.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR (Poster).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "ICLR (Poster). OpenReview.net.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Text classification algorithms: A survey",
      "author" : [ "Kamran Kowsari", "Kiana Jafari Meimandi", "Mojtaba Heidarysafa", "Sanjana Mendu", "Laura E. Barnes", "Donald E. Brown." ],
      "venue" : "Inf., 10(4):150.",
      "citeRegEx" : "Kowsari et al\\.,? 2019",
      "shortCiteRegEx" : "Kowsari et al\\.",
      "year" : 2019
    }, {
      "title" : "Recurrent convolutional neural networks for text classification",
      "author" : [ "Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao." ],
      "venue" : "AAAI, pages 2267–2273. AAAI Press.",
      "citeRegEx" : "Lai et al\\.,? 2015",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2015
    }, {
      "title" : "A survey on text classification: From shallow to deep learning",
      "author" : [ "Qian Li", "Hao Peng", "Jianxin Li", "Congying Xia", "Renyu Yang", "Lichao Sun", "Philip S. Yu", "Lifang He." ],
      "venue" : "CoRR, abs/2008.00364.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Pay attention to mlps",
      "author" : [ "Hanxiao Liu", "Zihang Dai", "David R. So", "Quoc V. Le." ],
      "venue" : "CoRR, abs/2105.08050.",
      "citeRegEx" : "Liu et al\\.,? 2021a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Tensor graph convolutional networks for text classification",
      "author" : [ "Xien Liu", "Xinxin You", "Xiao Zhang", "Ji Wu", "Ping Lv." ],
      "venue" : "AAAI, pages 8409–8416. AAAI Press.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep attention diffusion graph neural networks for text classification",
      "author" : [ "Yonghao Liu", "Renchu Guan", "Fausto Giunchiglia", "Yanchun Liang", "Xiaoyue Feng." ],
      "venue" : "EMNLP (1), pages 8142–8152. Association for Computational Linguistics.",
      "citeRegEx" : "Liu et al\\.,? 2021b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Graph star net for generalized multi-task learning",
      "author" : [ "Haonan Lu", "Seth H. Huang", "Tian Ye", "Xiuyan Guo." ],
      "venue" : "CoRR, abs/1906.12330.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Combine convolution with recurrent networks for text classification",
      "author" : [ "Shengfei Lyu", "Jiaqi Liu." ],
      "venue" : "CoRR, abs/2006.15795.",
      "citeRegEx" : "Lyu and Liu.,? 2020",
      "shortCiteRegEx" : "Lyu and Liu.",
      "year" : 2020
    }, {
      "title" : "Do you even need attention? A stack of feed-forward layers does surprisingly well on imagenet",
      "author" : [ "Luke Melas-Kyriazi." ],
      "venue" : "CoRR, abs/2105.02723.",
      "citeRegEx" : "Melas.Kyriazi.,? 2021",
      "shortCiteRegEx" : "Melas.Kyriazi.",
      "year" : 2021
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep double descent: Where bigger models and more data hurt",
      "author" : [ "Preetum Nakkiran", "Gal Kaplun", "Yamini Bansal", "Tristan Yang", "Boaz Barak", "Ilya Sutskever." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Nakkiran et al\\.,? 2020",
      "shortCiteRegEx" : "Nakkiran et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards understanding the role of over-parametrization in generalization of neural networks",
      "author" : [ "Behnam Neyshabur", "Zhiyuan Li", "Srinadh Bhojanapalli", "Yann LeCun", "Nathan Srebro." ],
      "venue" : "CoRR, abs/1805.12076.",
      "citeRegEx" : "Neyshabur et al\\.,? 2018",
      "shortCiteRegEx" : "Neyshabur et al\\.",
      "year" : 2018
    }, {
      "title" : "Keygraph: Automatic indexing by cooccurrence graph based on building construction metaphor",
      "author" : [ "Yukio Ohsawa", "Nels E. Benson", "Masahiko Yachida." ],
      "venue" : "ADL, pages 12–18. IEEE Computer Society.",
      "citeRegEx" : "Ohsawa et al\\.,? 1998",
      "shortCiteRegEx" : "Ohsawa et al\\.",
      "year" : 1998
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 115–124, Ann",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Large-scale hierarchical text classification with recursively regularized deep graph-cnn",
      "author" : [ "Hao Peng", "Jianxin Li", "Yu He", "Yaopeng Liu", "Mengjiao Bao", "Lihong Wang", "Yangqiu Song", "Qiang Yang." ],
      "venue" : "WWW, pages 1063–1072. ACM.",
      "citeRegEx" : "Peng et al\\.,? 2018",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2018
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 1532–1543. ACL.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "HeteGCN: Heterogeneous graph convolutional networks for text classification",
      "author" : [ "Rahul Ragesh", "Sundararajan Sellamanickam", "Arun Iyer", "Ramakrishna Bairi", "Vijay Lingam." ],
      "venue" : "WSDM, pages 860– 868. ACM.",
      "citeRegEx" : "Ragesh et al\\.,? 2021",
      "shortCiteRegEx" : "Ragesh et al\\.",
      "year" : 2021
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "EMNLP, pages 2383–2392. ACL.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic keyword extraction from individual documents",
      "author" : [ "Stuart Rose", "Dave Engel", "Nick Cramer", "Wendy Cowley." ],
      "venue" : "Michael W. Berry and Jacob Kogan, editors, Text Mining. Applications and Theory, pages 1–20. John Wiley and Sons, Ltd.",
      "citeRegEx" : "Rose et al\\.,? 2010",
      "shortCiteRegEx" : "Rose et al\\.",
      "year" : 2010
    }, {
      "title" : "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "CoRR, abs/1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Pitfalls of graph neural network evaluation",
      "author" : [ "Oleksandr Shchur", "Maximilian Mumme", "Aleksandar Bojchevski", "Stephan Günnemann." ],
      "venue" : "CoRR, abs/1811.05868.",
      "citeRegEx" : "Shchur et al\\.,? 2018",
      "shortCiteRegEx" : "Shchur et al\\.",
      "year" : 2018
    }, {
      "title" : "Baseline needs more love: On simple wordembedding-based models and associated pooling",
      "author" : [ "Dinghan Shen", "Guoyin Wang", "Wenlin Wang", "Martin Renqiang Min", "Qinliang Su", "Yizhe Zhang", "Chunyuan Li", "Ricardo Henao", "Lawrence Carin" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
      "author" : [ "Koustuv Sinha", "Robin Jia", "Dieuwke Hupkes", "Joelle Pineau", "Adina Williams", "Douwe Kiela." ],
      "venue" : "To appear in: EMNLP 2021. ACL.",
      "citeRegEx" : "Sinha et al\\.,? 2021",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "Mobilebert: a compact task-agnostic BERT for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "ACL, pages 2158–2170. ACL.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "PTE: predictive text embedding through large-scale heterogeneous text networks",
      "author" : [ "Jian Tang", "Meng Qu", "Qiaozhu Mei." ],
      "venue" : "KDD, pages 1165– 1174. ACM.",
      "citeRegEx" : "Tang et al\\.,? 2015",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient transformers: A survey",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler." ],
      "venue" : "CoRR, abs/2009.06732.",
      "citeRegEx" : "Tay et al\\.,? 2020",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Mlp-mixer: An all-mlp",
      "author" : [ "Ilya O. Tolstikhin", "Neil Houlsby", "Alexander Kolesnikov", "Lucas Beyer", "Xiaohua Zhai", "Thomas Unterthiner", "Jessica Yung", "Andreas Steiner", "Daniel Keysers", "Jakob Uszkoreit", "Mario Lucic", "Alexey Dosovitskiy" ],
      "venue" : null,
      "citeRegEx" : "Tolstikhin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Tolstikhin et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "A comparison of deeplearning methods for analysing and predicting business processes",
      "author" : [ "Ishwar Venugopal", "Jessica Töllich", "Michael Fairbank", "Ansgar Scherp." ],
      "venue" : "IJCNN. IEEE.",
      "citeRegEx" : "Venugopal et al\\.,? 2021",
      "shortCiteRegEx" : "Venugopal et al\\.",
      "year" : 2021
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "ICLR (Poster). OpenReview.net.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Convolutional recurrent neural networks for text classification",
      "author" : [ "Ruishuang Wang", "Zhao Li", "Jian Cao", "Tong Chen", "Lei Wang." ],
      "venue" : "IJCNN, pages 1–",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment analysis by capsules",
      "author" : [ "Yequan Wang", "Aixin Sun", "Jialong Han", "Ying Liu", "Xiaoyan Zhu." ],
      "venue" : "WWW, pages 1165–1174. ACM.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Simplifying graph convolutional networks",
      "author" : [ "Felix Wu", "Amauri H. Souza Jr.", "Tianyi Zhang", "Christopher Fifty", "Tao Yu", "Kilian Q. Weinberger." ],
      "venue" : "ICML, volume 97 of Proceedings of Machine Learning Research, pages 6861–6871. PMLR.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph neural networks for natural language processing: A survey",
      "author" : [ "Lingfei Wu", "Yu Chen", "Kai Shen", "Xiaojie Guo", "Hanning Gao", "Shucheng Li", "Jian Pei", "Bo Long." ],
      "venue" : "CoRR, abs/2106.06090.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient path prediction for semi-supervised and weakly supervised hierarchical text classification",
      "author" : [ "Huiru Xiao", "Xin Liu", "Yangqiu Song." ],
      "venue" : "WWW, pages 3370–3376. ACM.",
      "citeRegEx" : "Xiao et al\\.,? 2019",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph convolutional networks for text classification",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "AAAI, pages 7370–7377. AAAI Press.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "When SIMPLE is better than complex: A case study on deep learning for predicting bugzilla issue close time",
      "author" : [ "Rahul Yedida", "Xueqi Yang", "Tim Menzies." ],
      "venue" : "CoRR, abs/2101.06319.",
      "citeRegEx" : "Yedida et al\\.,? 2021",
      "shortCiteRegEx" : "Yedida et al\\.",
      "year" : 2021
    }, {
      "title" : "Bayesian performance comparison of text classifiers",
      "author" : [ "Dell Zhang", "Jun Wang", "Emine Yilmaz", "Xiaoling Wang", "Yuxin Zhou." ],
      "venue" : "SIGIR, pages 15–24. ACM.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Weakly-supervised text classification based on keyword graph",
      "author" : [ "Lu Zhang", "Jiandong Ding", "Yi Xu", "Yingyao Liu", "Shuigeng Zhou." ],
      "venue" : "EMNLP (1), pages 2803–2813. Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling",
      "author" : [ "Peng Zhou", "Zhenyu Qi", "Suncong Zheng", "Jiaming Xu", "Hongyun Bao", "Bo Xu." ],
      "venue" : "COLING, pages 3485–3495. ACL.",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey on text classification and its applications",
      "author" : [ "Xujuan Zhou", "Raj Gururajan", "Yuefeng Li", "Revathi Venkataraman", "Xiaohui Tao", "Ghazal Bargshady", "Prabal Datta Barua", "Srinivas KondalsamyChennakesavan." ],
      "venue" : "Web Intell., 18(3):205–",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019).",
      "startOffset" : 119,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019).",
      "startOffset" : 119,
      "endOffset" : 211
    }, {
      "referenceID" : 63,
      "context" : "Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019).",
      "startOffset" : 119,
      "endOffset" : 211
    }, {
      "referenceID" : 22,
      "context" : "Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019).",
      "startOffset" : 119,
      "endOffset" : 211
    }, {
      "referenceID" : 18,
      "context" : "Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019).",
      "startOffset" : 119,
      "endOffset" : 211
    }, {
      "referenceID" : 15,
      "context" : "Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 43,
      "context" : ", 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText (Bojanowski et al.",
      "startOffset" : 135,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : ", 2018) that explores different pooling strategies for pretrained word embeddings, and fastText (Bojanowski et al., 2017), which uses a linear layer on top of pretrained word embeddings.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 58,
      "context" : "text categorization are graph-based models such as TextGCN (Yao et al., 2019) that first induce a synthetic word-document co-occurence graph over the corpus and subsequently apply a graph neural network (GNN) to perform the classification task.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 38,
      "context" : "Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN (Liu et al.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : ", 2021), TensorGCN (Liu et al., 2020), and HyperGAT (Ding et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : ", 2020), and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 49,
      "context" : "Finally, there is the well-known Transformer (Vaswani et al., 2017) universe with models",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "such as BERT (Devlin et al., 2019) and its sizereduced variants such as DistilBERT (Sanh et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 41,
      "context" : ", 2019) and its sizereduced variants such as DistilBERT (Sanh et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 47,
      "context" : "BERT and others (Tay et al., 2020; Fournier et al., 2021) use less parameters but operate on the same type of input.",
      "startOffset" : 16,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "BERT and others (Tay et al., 2020; Fournier et al., 2021) use less parameters but operate on the same type of input.",
      "startOffset" : 16,
      "endOffset" : 57
    }, {
      "referenceID" : 58,
      "context" : "MLP, called the WideMLP, with only one wide hidden layer, outperforms many of the recent graphbased models for inductive text categorization (Yao et al., 2019; Liu et al., 2020; Ragesh et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 198
    }, {
      "referenceID" : 26,
      "context" : "MLP, called the WideMLP, with only one wide hidden layer, outperforms many of the recent graphbased models for inductive text categorization (Yao et al., 2019; Liu et al., 2020; Ragesh et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 198
    }, {
      "referenceID" : 38,
      "context" : "MLP, called the WideMLP, with only one wide hidden layer, outperforms many of the recent graphbased models for inductive text categorization (Yao et al., 2019; Liu et al., 2020; Ragesh et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "Thus, we fine-tuned our own BERT (Devlin et al., 2019) and DistilBERT (Sanh et al.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 43,
      "context" : "tant means to argue about true scientific advancement (Shen et al., 2018; Dacrema et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "tant means to argue about true scientific advancement (Shen et al., 2018; Dacrema et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 239
    }, {
      "referenceID" : 24,
      "context" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 239
    }, {
      "referenceID" : 63,
      "context" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 239
    }, {
      "referenceID" : 22,
      "context" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 239
    }, {
      "referenceID" : 18,
      "context" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 239
    }, {
      "referenceID" : 12,
      "context" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 239
    }, {
      "referenceID" : 60,
      "context" : "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Bayer et al., 2021; Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019; Galke et al., 2017; Zhang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 239
    }, {
      "referenceID" : 22,
      "context" : "BoW-based Models Classical machine learning models that operate on a BoW-based input are extensively discussed in two surveys (Kowsari et al., 2019; Kadhim, 2019) and other comparison studies (Galke et al.",
      "startOffset" : 126,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "BoW-based Models Classical machine learning models that operate on a BoW-based input are extensively discussed in two surveys (Kowsari et al., 2019; Kadhim, 2019) and other comparison studies (Galke et al.",
      "startOffset" : 126,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : ", 2019; Kadhim, 2019) and other comparison studies (Galke et al., 2017).",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 37,
      "context" : "The results suggest to use pretrained embeddings such as GloVe (Pennington et al., 2014) over a randomly initialized neural bag of-words (Kalchbrenner et al.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : ", 2014) over a randomly initialized neural bag of-words (Kalchbrenner et al., 2014) as input.",
      "startOffset" : 56,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "In fastText (Bojanowski et al., 2017; Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classification.",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "In fastText (Bojanowski et al., 2017; Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classification.",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 34,
      "context" : "An early work is the term co-occurrence graph of the KeyGraph algorithm (Ohsawa et al., 1998).",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 40,
      "context" : "Co-occurence graphs have also been used for automatic keyword extraction such as in RAKE (Rose et al., 2010) and can be also used for classification (Zhang et al.",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 61,
      "context" : ", 2010) and can be also used for classification (Zhang et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "Modern approaches exploit this idea in combination with graph neural networks (GNN) (Hamilton, 2020).",
      "startOffset" : 84,
      "endOffset" : 100
    }, {
      "referenceID" : 58,
      "context" : "Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN (Liu et al.",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 26,
      "context" : ", 2019) and its successor TensorGCN (Liu et al., 2020) as well as HeteGCN (Ragesh et al.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 38,
      "context" : ", 2020) as well as HeteGCN (Ragesh et al., 2021), HyperGAT (Ding et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : ", 2021), HyperGAT (Ding et al., 2020), and DADGNN (Liu et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "two-layer graph convolutional network (Kipf and Welling, 2017) on the graph to carry out the node classification task.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 46,
      "context" : "HeteGCN combines ideas from Predictive Text Embedding (Tang et al., 2015) and TextGCN and split the adjacency matrix into its word-document and word-word sub-matrices and fuse the different layers’ representations when re-",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "connections are derived from topic models (Blei et al., 2003).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 38,
      "context" : "When these methods are adapted for inductive learning, where the test set is unseen, they achieve notably lower scores (Ragesh et al., 2021).",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 58,
      "context" : "information (PMI), to connect related words in a graph (Yao et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 62,
      "context" : "The bidirectional LSTM with two-dimensional max pooling BLSTM-2DCNN (Zhou et al., 2016) has been applied on a stripped-down to 4 classes version of the 20ng dataset.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "Also TextRCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 54,
      "context" : "RNNCapsule (Wang et al., 2018) is a sentiment analysis method reaching an accuracy of 83.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 41,
      "context" : "2019) and DistilBERT (Sanh et al., 2019) on those datasets ourselves.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 41,
      "context" : "DistilBERT (Sanh et al., 2019) is a distilled version of BERT with 40% reduced parameters while retaining 97% of BERT’s language understanding capabilities.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "TinyBERT (Jiao et al., 2020) and MobileBERT (Sun et al.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 45,
      "context" : ", 2020) and MobileBERT (Sun et al., 2020) would be similarly suitable alternatives, among others.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "The recent surveys on text categorization include both classical and Deep Learning models, but none considered a simple MLP except for the inclusion of DAN (Iyyer et al., 2015) in Li et al.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 55,
      "context" : "We consider top performing graph-based models from the literature, namely TextGCN along with its successors HeteGCN, TensorGCN, HyperGAT, DADGNN, as well as simplified GCN (SGC) (Wu et al., 2019).",
      "startOffset" : 178,
      "endOffset" : 195
    }, {
      "referenceID" : 58,
      "context" : "We use the same datasets and train-test split as in TextGCN (Yao et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 35,
      "context" : "Movie Reviews (MR)3 (Pang and Lee, 2005), split by Tang et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "pabo/movie-review-data/ from BERT (Devlin et al., 2019) along with its uncased vocabulary.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 33,
      "context" : "on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018; Nakkiran et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 123
    }, {
      "referenceID" : 32,
      "context" : "on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018; Nakkiran et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "entropy with Adam (Kingma and Ba, 2015) and its default learning rate of 10−3, a linearly decaying learning rate schedule and train for a high amount of steps (Nakkiran et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "entropy with Adam (Kingma and Ba, 2015) and its default learning rate of 10−3, a linearly decaying learning rate schedule and train for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient",
      "startOffset" : 159,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "Thus, we argue that using strong baseline models for text classification is important to assess the true scientific advancement (Dacrema et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 15,
      "context" : "Second, in contrast to conventional wisdom (Iyyer et al., 2015), we find that pretrained embeddings, e.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "A possible explanation is that already a single hidden layer is sufficient to approximate any compact function to an arbitrary degree of accuracy depending on the width of the hidden layer (Cybenko, 1989).",
      "startOffset" : 189,
      "endOffset" : 204
    }, {
      "referenceID" : 10,
      "context" : "For standard GNN methods, the whole graph has to fit into the GPU memory and mini-batching is nontrivial, but possible with dedicated sampling techniques for GNNs (Fey et al., 2021).",
      "startOffset" : 163,
      "endOffset" : 181
    }, {
      "referenceID" : 21,
      "context" : "In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017; Veličković et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al.",
      "startOffset" : 80,
      "endOffset" : 129
    }, {
      "referenceID" : 50,
      "context" : "In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017; Veličković et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al.",
      "startOffset" : 80,
      "endOffset" : 129
    }, {
      "referenceID" : 42,
      "context" : ", 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 56,
      "context" : "GNNs also find application in various NLP tasks, other than classification (Wu et al., 2021).",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 52,
      "context" : ", 2016) or natural language inference (Wang et al., 2019a) can also be regarded as text classification on a technical level.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 51,
      "context" : "For example, in business prediction, an MLP baseline outperforms various other Deep Learning models (Venugopal et al., 2021; Yedida et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 145
    }, {
      "referenceID" : 59,
      "context" : "For example, in business prediction, an MLP baseline outperforms various other Deep Learning models (Venugopal et al., 2021; Yedida et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 145
    }, {
      "referenceID" : 36,
      "context" : "As future work, it would be interesting to analyze multi-label classification tasks and to compare with hierarchical text categorization methods (Peng et al., 2018; Xiao et al., 2019).",
      "startOffset" : 145,
      "endOffset" : 183
    }, {
      "referenceID" : 57,
      "context" : "As future work, it would be interesting to analyze multi-label classification tasks and to compare with hierarchical text categorization methods (Peng et al., 2018; Xiao et al., 2019).",
      "startOffset" : 145,
      "endOffset" : 183
    } ],
    "year" : 0,
    "abstractText" : "Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today’s state of the art. We show that a simple multi-layer perceptron (MLP) using a “Bag of Words” (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importance of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an O(N) graph, where N is the vocabulary plus corpus size. Finally, since Transformers need to compute O(L) attention weights with L sequence length, the MLP models show higher training and inference speeds on datasets with long sequences.",
    "creator" : null
  }
}
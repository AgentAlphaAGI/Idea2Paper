{
  "name" : "ARR_2022_44_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ACTUNE: Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : null,
    "references" : [ {
      "title" : "Pseudolabeling and confirmation bias in deep semisupervised learning",
      "author" : [ "Eric Arazo", "Diego Ortego", "Paul Albert", "Noel E O’Connor", "Kevin McGuinness" ],
      "venue" : "In 2020 International Joint Conference on Neural Networks (IJCNN),",
      "citeRegEx" : "Arazo et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Arazo et al\\.",
      "year" : 2020
    }, {
      "title" : "Kmeans++: The advantages of careful seeding",
      "author" : [ "David Arthur", "Sergei Vassilvitskii." ],
      "venue" : "Proceedings of the Eighteenth Annual ACMSIAM Symposium on Discrete Algorithms, page 1027–1035, USA.",
      "citeRegEx" : "Arthur and Vassilvitskii.,? 2007",
      "shortCiteRegEx" : "Arthur and Vassilvitskii.",
      "year" : 2007
    }, {
      "title" : "Deep batch active learning by diverse, uncertain gradient lower bounds",
      "author" : [ "Jordan T. Ash", "Chicheng Zhang", "Akshay Krishnamurthy", "John Langford", "Alekh Agarwal." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ash et al\\.,? 2020",
      "shortCiteRegEx" : "Ash et al\\.",
      "year" : 2020
    }, {
      "title" : "Self-supervised meta-learning for few-shot natural language classification tasks",
      "author" : [ "Trapit Bansal", "Rishikesh Jha", "Tsendsuren Munkhdalai", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Bansal et al\\.,? 2020",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2020
    }, {
      "title" : "SciBERT: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Flex: Unifying evaluation for few-shot nlp",
      "author" : [ "Jonathan Bragg", "Arman Cohan", "Kyle Lo", "Iz Beltagy." ],
      "venue" : "Advances in Neural Information Processing Systems, 34.",
      "citeRegEx" : "Bragg et al\\.,? 2021",
      "shortCiteRegEx" : "Bragg et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are fewshot learners",
      "author" : [ "Tom Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared D Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
      "author" : [ "Franck Dernoncourt", "Ji Young Lee." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
      "citeRegEx" : "Dernoncourt and Lee.,? 2017",
      "shortCiteRegEx" : "Dernoncourt and Lee.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
      "author" : [ "Jesse Dodge", "Gabriel Ilharco", "Roy Schwartz", "Ali Farhadi", "Hannaneh Hajishirzi", "Noah A. Smith." ],
      "venue" : "CoRR, abs/2002.06305.",
      "citeRegEx" : "Dodge et al\\.,? 2020",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2020
    }, {
      "title" : "The hitchhiker’s guide to testing statistical significance in natural language processing",
      "author" : [ "Rotem Dror", "Gili Baumer", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Dror et al\\.,? 2018",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-training improves pre-training for natural language understanding",
      "author" : [ "Jingfei Du", "Edouard Grave", "Beliz Gunel", "Vishrav Chaudhary", "Onur Celebi", "Michael Auli", "Veselin Stoyanov", "Alexis Conneau." ],
      "venue" : "Proceedings of the 2021 Conference of",
      "citeRegEx" : "Du et al\\.,? 2021",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2021
    }, {
      "title" : "Active Learning for BERT: An Empirical Study",
      "author" : [ "Liat Ein-Dor", "Alon Halfon", "Ariel Gera", "Eyal Shnarch", "Lena Dankin", "Leshem Choshen", "Marina Danilevsky", "Ranit Aharonov", "Yoav Katz", "Noam Slonim." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Ein.Dor et al\\.,? 2020",
      "shortCiteRegEx" : "Ein.Dor et al\\.",
      "year" : 2020
    }, {
      "title" : "Bayesian convolutional neural networks with bernoulli approximate variational inference",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani." ],
      "venue" : "CoRR, abs/1506.02158.",
      "citeRegEx" : "Gal and Ghahramani.,? 2015",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2015
    }, {
      "title" : "Deep bayesian active learning with image data",
      "author" : [ "Yarin Gal", "Riashat Islam", "Zoubin Ghahramani." ],
      "venue" : "International Conference on Machine Learning, pages 1183–1192. PMLR.",
      "citeRegEx" : "Gal et al\\.,? 2017",
      "shortCiteRegEx" : "Gal et al\\.",
      "year" : 2017
    }, {
      "title" : "Consistency-based semi-supervised active learning: Towards minimizing labeling cost",
      "author" : [ "Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan Ö Arık", "Larry S Davis", "Tomas Pfister." ],
      "venue" : "European Conference on Computer Vision, pages 510–526.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Semisupervised active learning for semi-supervised models: Exploit adversarial examples with graph-based",
      "author" : [ "Jiannan Guo", "Haochen Shi", "Yangyang Kang", "Kun Kuang", "Siliang Tang", "Zhuoren Jiang", "Changlong Sun", "Fei Wu", "Yueting Zhuang" ],
      "venue" : null,
      "citeRegEx" : "Guo et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2021
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Entropy-based active learning for object recognition",
      "author" : [ "Alex Holub", "Pietro Perona", "Michael C Burl." ],
      "venue" : "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1–8. IEEE.",
      "citeRegEx" : "Holub et al\\.,? 2008",
      "shortCiteRegEx" : "Holub et al\\.",
      "year" : 2008
    }, {
      "title" : "Active learning with partial feedback",
      "author" : [ "Peiyun Hu", "Zack Lipton", "Anima Anandkumar", "Deva Ramanan." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated variable weighting in k-means type clustering",
      "author" : [ "Joshua Zhexue Huang", "Michael K Ng", "Hongqiang Rong", "Zichen Li." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 27(5):657–668.",
      "citeRegEx" : "Huang et al\\.,? 2005",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2005
    }, {
      "title" : "Billion-scale similarity search with gpus",
      "author" : [ "Jeff Johnson", "Matthijs Douze", "Hervé Jégou." ],
      "venue" : "IEEE Transactions on Big Data.",
      "citeRegEx" : "Johnson et al\\.,? 2019",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards realistic practices in lowresource natural language processing: The development set",
      "author" : [ "Katharina Kann", "Kyunghyun Cho", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Kann et al\\.,? 2019",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2019
    }, {
      "title" : "Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering",
      "author" : [ "Siddharth Karamcheti", "Ranjay Krishna", "Li Fei-Fei", "Christopher Manning." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association",
      "citeRegEx" : "Karamcheti et al\\.,? 2021",
      "shortCiteRegEx" : "Karamcheti et al\\.",
      "year" : 2021
    }, {
      "title" : "Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning",
      "author" : [ "Andreas Kirsch", "Joost Van Amersfoort", "Yarin Gal." ],
      "venue" : "Advances in neural information processing systems, 32:7026– 7037.",
      "citeRegEx" : "Kirsch et al\\.,? 2019",
      "shortCiteRegEx" : "Kirsch et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the biocreative VI chemical-protein interaction track",
      "author" : [ "Martin Krallinger", "Obdulia Rabal", "Saber A Akhondi" ],
      "venue" : "In BioCreative evaluation Workshop,",
      "citeRegEx" : "Krallinger et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krallinger et al\\.",
      "year" : 2017
    }, {
      "title" : "Temporal ensembling for semi-supervised learning",
      "author" : [ "Samuli Laine", "Timo Aila." ],
      "venue" : "arXiv preprint arXiv:1610.02242.",
      "citeRegEx" : "Laine and Aila.,? 2016",
      "shortCiteRegEx" : "Laine and Aila.",
      "year" : 2016
    }, {
      "title" : "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "author" : [ "Dong-Hyun Lee." ],
      "venue" : "ICML Workshop on challenges in representation learning, volume 3, page 896.",
      "citeRegEx" : "Lee.,? 2013",
      "shortCiteRegEx" : "Lee.",
      "year" : 2013
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Xin Li", "Dan Roth." ],
      "venue" : "The 19th International Conference on Computational Linguistics.",
      "citeRegEx" : "Li and Roth.,? 2002",
      "shortCiteRegEx" : "Li and Roth.",
      "year" : 2002
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Bayesian active learning with pretrained language models",
      "author" : [ "Katerina Margatina", "Loic Barrault", "Nikolaos Aletras." ],
      "venue" : "arXiv preprint arXiv:2104.08320.",
      "citeRegEx" : "Margatina et al\\.,? 2021a",
      "shortCiteRegEx" : "Margatina et al\\.",
      "year" : 2021
    }, {
      "title" : "Active learning by acquiring contrastive examples",
      "author" : [ "Katerina Margatina", "Giorgos Vernikos", "Loïc Barrault", "Nikolaos Aletras." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650–663, Online",
      "citeRegEx" : "Margatina et al\\.,? 2021b",
      "shortCiteRegEx" : "Margatina et al\\.",
      "year" : 2021
    }, {
      "title" : "On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines",
      "author" : [ "Marius Mosbach", "Maksym Andriushchenko", "Dietrich Klakow." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Mosbach et al\\.,? 2021",
      "shortCiteRegEx" : "Mosbach et al\\.",
      "year" : 2021
    }, {
      "title" : "Uncertainty-aware self-training for few-shot text classification",
      "author" : [ "Subhabrata Mukherjee", "Ahmed Awadallah." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Mukherjee and Awadallah.,? 2020",
      "shortCiteRegEx" : "Mukherjee and Awadallah.",
      "year" : 2020
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Snorkel: Rapid training data creation with weak supervision",
      "author" : [ "Alexander Ratner", "Stephen H Bach", "Henry Ehrenberg", "Jason Fries", "Sen Wu", "Christopher Ré." ],
      "venue" : "Proceedings of the VLDB Endowment., volume 11, page 269.",
      "citeRegEx" : "Ratner et al\\.,? 2017",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2017
    }, {
      "title" : "In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning",
      "author" : [ "Mamshad Nayeem Rizve", "Kevin Duarte", "Yogesh S Rawat", "Mubarak Shah." ],
      "venue" : "International Conference on Learning Rep-",
      "citeRegEx" : "Rizve et al\\.,? 2021",
      "shortCiteRegEx" : "Rizve et al\\.",
      "year" : 2021
    }, {
      "title" : "Semi-supervised self-training of object detection models",
      "author" : [ "Chuck Rosenberg", "Martial Hebert", "Henry Schneiderman." ],
      "venue" : "Proceedings of the IEEE Workshops on Application of Computer Vision, pages 29–36.",
      "citeRegEx" : "Rosenberg et al\\.,? 2005",
      "shortCiteRegEx" : "Rosenberg et al\\.",
      "year" : 2005
    }, {
      "title" : "Deep bayesian active semisupervised learning",
      "author" : [ "Matthias Rottmann", "Karsten Kahl", "Hanno Gottschalk." ],
      "venue" : "2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 158–164. IEEE.",
      "citeRegEx" : "Rottmann et al\\.,? 2018",
      "shortCiteRegEx" : "Rottmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Active sentence learning by adversarial uncertainty sampling in discrete space",
      "author" : [ "Dongyu Ru", "Jiangtao Feng", "Lin Qiu", "Hao Zhou", "Mingxuan Wang", "Weinan Zhang", "Yong Yu", "Lei Li." ],
      "venue" : "Findings of the Association for Computational Linguistics:",
      "citeRegEx" : "Ru et al\\.,? 2020",
      "shortCiteRegEx" : "Ru et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
      "citeRegEx" : "Schick and Schütze.,? 2021",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "Active learning for convolutional neural networks: A core-set approach",
      "author" : [ "Ozan Sener", "Silvio Savarese." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sener and Savarese.,? 2018",
      "shortCiteRegEx" : "Sener and Savarese.",
      "year" : 2018
    }, {
      "title" : "Active learning for sequence tagging",
      "author" : [ "Artem Shelmanov", "Dmitri Puzyrev", "Lyubov Kupriyanova", "Denis Belyakov", "Daniil Larionov", "Nikita Khromov", "Olga Kozlova", "Ekaterina Artemova", "Dmitry V. Dylov", "Alexander Panchenko" ],
      "venue" : null,
      "citeRegEx" : "Shelmanov et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Shelmanov et al\\.",
      "year" : 2021
    }, {
      "title" : "Rethinking deep active learning: Using unlabeled data at model training",
      "author" : [ "Oriane Siméoni", "Mateusz Budnik", "Yannis Avrithis", "Guillaume Gravier." ],
      "venue" : "the 25th International Conference on Pattern Recognition (ICPR), pages 1220–1227. IEEE.",
      "citeRegEx" : "Siméoni et al\\.,? 2020",
      "shortCiteRegEx" : "Siméoni et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Combining mixmatch and active learning for better accuracy with fewer labels",
      "author" : [ "Shuang Song", "David Berthelot", "Afshin Rostamizadeh." ],
      "venue" : "arXiv preprint arXiv:1912.00594.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Semisupervised active learning for sequence labeling",
      "author" : [ "Katrin Tomanek", "Udo Hahn." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Tomanek and Hahn.,? 2009",
      "shortCiteRegEx" : "Tomanek and Hahn.",
      "year" : 2009
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(11).",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Cost-effective active learning for deep image classification",
      "author" : [ "Keze Wang", "Dongyu Zhang", "Ya Li", "Ruimao Zhang", "Liang Lin." ],
      "venue" : "IEEE Transactions on Circuits and Systems for Video Technology, 27(12):2591–2600.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach",
      "author" : [ "Yue Yu", "Simiao Zuo", "Haoming Jiang", "Wendi Ren", "Tuo Zhao", "Chao Zhang." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Yu et al\\.,? 2021",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2021
    }, {
      "title" : "Cold-start active learning through self-supervised language modeling",
      "author" : [ "Michelle Yuan", "Hsuan-Tien Lin", "Jordan BoydGraber." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Yuan et al\\.,? 2020",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2020
    }, {
      "title" : "WRENCH: A comprehensive benchmark for weak supervision",
      "author" : [ "Jieyu Zhang", "Yue Yu", "Yinghao Li", "Yujing Wang", "Yaming Yang", "Mao Yang", "Alexander Ratner." ],
      "venue" : "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Bench-",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Revisiting few-sample bert fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "arXiv preprint arXiv:2006.05987.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in neural information processing systems, 28:649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Active learning approaches to enhancing neural machine translation",
      "author" : [ "Yuekai Zhao", "Haoran Zhang", "Shuchang Zhou", "Zhihua Zhang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1796–1806, Online. Associa-",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "2008) for dimension reduction, and the black triangle stands for the queried samples while other circles stands for the unlabeled data. Different colors stands for different classes",
      "author" : [ "Hinton" ],
      "venue" : "From the comparision,",
      "citeRegEx" : "Hinton,? \\Q2008\\E",
      "shortCiteRegEx" : "Hinton",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Fine-tuning pre-trained language models (PLMs) has achieved enormous success in natural language processing (NLP) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al.",
      "startOffset" : 114,
      "endOffset" : 173
    }, {
      "referenceID" : 30,
      "context" : "Fine-tuning pre-trained language models (PLMs) has achieved enormous success in natural language processing (NLP) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al.",
      "startOffset" : 114,
      "endOffset" : 173
    }, {
      "referenceID" : 6,
      "context" : "Fine-tuning pre-trained language models (PLMs) has achieved enormous success in natural language processing (NLP) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al.",
      "startOffset" : 114,
      "endOffset" : 173
    }, {
      "referenceID" : 3,
      "context" : ", 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al., 2020; Gao et al., 2021).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : ", 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al., 2020; Gao et al., 2021).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "Besides, the performance of few-shot PLM fine-tuning can vary substantially with different sets of training data (Bragg et al., 2021).",
      "startOffset" : 113,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Towards this goal, researchers have resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020; Margatina et al., 2021a,b; Yuan et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 240
    }, {
      "referenceID" : 53,
      "context" : "Towards this goal, researchers have resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020; Margatina et al., 2021a,b; Yuan et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 240
    }, {
      "referenceID" : 11,
      "context" : "Nevertheless, they usually neglect unlabeled data, which can be useful for improving label efficiency for PLM fine-tuning (Du et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 139
    }, {
      "referenceID" : 50,
      "context" : "To leverage those unlabeled data to improve label efficiency of active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016; Rottmann et al., 2018; Siméoni et al., 2020), but the proposed query strategies can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.",
      "startOffset" : 154,
      "endOffset" : 218
    }, {
      "referenceID" : 40,
      "context" : "To leverage those unlabeled data to improve label efficiency of active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016; Rottmann et al., 2018; Siméoni et al., 2020), but the proposed query strategies can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.",
      "startOffset" : 154,
      "endOffset" : 218
    }, {
      "referenceID" : 45,
      "context" : "To leverage those unlabeled data to improve label efficiency of active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016; Rottmann et al., 2018; Siméoni et al., 2020), but the proposed query strategies can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.",
      "startOffset" : 154,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "This phenomenon can be even more severe for PLMs, as the finetuning process often suffers from the instability issue caused by different weight initialization and data orders (Dodge et al., 2020).",
      "startOffset" : 175,
      "endOffset" : 195
    }, {
      "referenceID" : 12,
      "context" : "Inspired by the fact that existing uncertainty-based AL methods often end up choosing uncertain yet repetitive data (Ein-Dor et al., 2020; Margatina et al., 2021b), we design a region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.",
      "startOffset" : 116,
      "endOffset" : 163
    }, {
      "referenceID" : 33,
      "context" : "Inspired by the fact that existing uncertainty-based AL methods often end up choosing uncertain yet repetitive data (Ein-Dor et al., 2020; Margatina et al., 2021b), we design a region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.",
      "startOffset" : 116,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "Our method is motivated by the fact that fine-tuning PLMs suffer from instability issues — distinct initializations and data orders can result in a large variance of the task performance (Dodge et al., 2020; Zhang et al., 2020; Mosbach et al., 2021).",
      "startOffset" : 187,
      "endOffset" : 249
    }, {
      "referenceID" : 55,
      "context" : "Our method is motivated by the fact that fine-tuning PLMs suffer from instability issues — distinct initializations and data orders can result in a large variance of the task performance (Dodge et al., 2020; Zhang et al., 2020; Mosbach et al., 2021).",
      "startOffset" : 187,
      "endOffset" : 249
    }, {
      "referenceID" : 34,
      "context" : "Our method is motivated by the fact that fine-tuning PLMs suffer from instability issues — distinct initializations and data orders can result in a large variance of the task performance (Dodge et al., 2020; Zhang et al., 2020; Mosbach et al., 2021).",
      "startOffset" : 187,
      "endOffset" : 249
    }, {
      "referenceID" : 27,
      "context" : "we propose a momentum updating method to dynamically aggregate the predictions from preceding rounds (Laine and Aila, 2016) and select low-uncertainty samples based on aggregated prediction.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "However, directly sampling the most uncertain samples gives suboptimal result since uncertainty-based sampling tends to query repetitive data (Ein-Dor et al., 2020) and results in poor representativeness of the overall data distribution.",
      "startOffset" : 142,
      "endOffset" : 164
    }, {
      "referenceID" : 21,
      "context" : "Specifically, in the tth round, we first conduct the weighted K-means clustering (Huang et al., 2005), which weights samples based on their uncertainty.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "The weighted K-means first initializes the center of each each cluster μi(1 ≤ i ≤ K) via K-Means++ (Arthur and Vassilvitskii, 2007).",
      "startOffset" : 99,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "As PLMs are sensitive to the stochasticity involved in fine-tuning, the model suffers from the instability issue — different weight initialization and data orders may result in different predictions on the same dataset (Dodge et al., 2020).",
      "startOffset" : 219,
      "endOffset" : 239
    }, {
      "referenceID" : 18,
      "context" : "To effectively mitigate the noise and stabilize the active self-training process, we maintain a dynamic memory bank to save the results from previous rounds, and use momentum update (He et al., 2020; Laine and Aila, 2016) to aggregate the results from both the previous and current rounds.",
      "startOffset" : 182,
      "endOffset" : 221
    }, {
      "referenceID" : 27,
      "context" : "To effectively mitigate the noise and stabilize the active self-training process, we maintain a dynamic memory bank to save the results from previous rounds, and use momentum update (He et al., 2020; Laine and Aila, 2016) to aggregate the results from both the previous and current rounds.",
      "startOffset" : 182,
      "endOffset" : 221
    }, {
      "referenceID" : 22,
      "context" : "Note that the clustering can be efficiently implemented with FAISS (Johnson et al., 2019), and will not excessively increase the total running time.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "For self-training, the size of the memory bank g(x; θ) is proportional to |Xu|, while the extra computation of maintaining this dictionary is ignorable since we do not inference over the unlabeled data for multiple times in each round as BALD (Gal et al., 2017) does.",
      "startOffset" : 243,
      "endOffset" : 261
    }, {
      "referenceID" : 46,
      "context" : "In our main experiments, we study over 4 benchmark datasets, including SST-2 (Socher et al., 2013) for sentiment analysis, AGNews (Zhang et al.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 56,
      "context" : ", 2013) for sentiment analysis, AGNews (Zhang et al., 2015) for news topic classification, Pubmed-RCT (Dernoncourt and Lee, 2017) for medical abstract classification, and DBPe-",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : ", 2015) for news topic classification, Pubmed-RCT (Dernoncourt and Lee, 2017) for medical abstract classification, and DBPe-",
      "startOffset" : 50,
      "endOffset" : 77
    }, {
      "referenceID" : 56,
      "context" : "dia (Zhang et al., 2015) for wikipedia topic classification.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 29,
      "context" : "For weakly-supervised text classification, we choose 2 datasets, namely TREC (Li and Roth, 2002) and Chemprot (Krallinger et al.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "For weakly-supervised text classification, we choose 2 datasets, namely TREC (Li and Roth, 2002) and Chemprot (Krallinger et al., 2017) from the WRENCH benchmark (Zhang et al.",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 54,
      "context" : ", 2017) from the WRENCH benchmark (Zhang et al., 2021) for evaluation.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 53,
      "context" : "Following (Yuan et al., 2020), we set the number of rounds T = 10, the overall budget for all datasets b = 1000 and the initial size of the labeled |Xl| is set to 100.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "Since large development sets are impractical in low-resource settings (Kann et al., 2019), we keep the size of development set as 1000, which is the same as the labeling budget3.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 30,
      "context" : "We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : ", 2020) as the backbone for ACTUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific corpora.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 35,
      "context" : "(2) UST (Mukherjee and Awadallah, 2020; Rizve et al., 2021): It is an uncertainty-based self-training method that only uses low-uncertainty data for selftraining.",
      "startOffset" : 8,
      "endOffset" : 59
    }, {
      "referenceID" : 38,
      "context" : "(2) UST (Mukherjee and Awadallah, 2020; Rizve et al., 2021): It is an uncertainty-based self-training method that only uses low-uncertainty data for selftraining.",
      "startOffset" : 8,
      "endOffset" : 59
    }, {
      "referenceID" : 52,
      "context" : "(3) COSINE (Yu et al., 2021): It uses self-training to fine-tune LM with weakly-labeled data, which achieves SOTA performance on various text datasets in WRENCH benchmark (Zhang et al.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 54,
      "context" : ", 2021): It uses self-training to fine-tune LM with weakly-labeled data, which achieves SOTA performance on various text datasets in WRENCH benchmark (Zhang et al., 2021).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 19,
      "context" : "(2) Entropy (Holub et al., 2008): It is an uncertainty-based method that acquires annotations on samples with the highest predictive entropy.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "(3) BALD (Gal et al., 2017): It is also an uncertainty-based method, which calculates model uncertainty using MC Dropout (Gal and Ghahramani, 2015).",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : ", 2017): It is also an uncertainty-based method, which calculates model uncertainty using MC Dropout (Gal and Ghahramani, 2015).",
      "startOffset" : 101,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "(4) BADGE (Ash et al., 2020): It first selects high uncertainty samples then uses KMeans++ over the gradient embedding to sample data.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 53,
      "context" : "(5) ALPS (Yuan et al., 2020): It uses the masked language model (MLM) loss of BERT to query labels for samples.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 33,
      "context" : "(6) CAL (Margatina et al., 2021b) is the most recent AL method for pretrained LMs.",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 48,
      "context" : "Semi-supervised Active Learning (SSAL) Methods: (1) ASST (Tomanek and Hahn, 2009; Siméoni et al., 2020) is an active semi-supervised learning method that jointly queries labels for AL and samples pseudo labels for self-training.",
      "startOffset" : 57,
      "endOffset" : 103
    }, {
      "referenceID" : 45,
      "context" : "Semi-supervised Active Learning (SSAL) Methods: (1) ASST (Tomanek and Hahn, 2009; Siméoni et al., 2020) is an active semi-supervised learning method that jointly queries labels for AL and samples pseudo labels for self-training.",
      "startOffset" : 57,
      "endOffset" : 103
    }, {
      "referenceID" : 50,
      "context" : "(2) CEAL (Wang et al., 2016) acquires annotations on informative samples, and uses high-confidence samples with predicted pseudo labels for weights updating.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 40,
      "context" : "(3) BASS (Rottmann et al., 2018) is similar to CEAL, but use MC dropout for querying",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "(4) REVIVAL (Guo et al., 2021) is the most recent SSAL method, which uses an adversarial loss to query samples and leverage label propagation to exploit adversarial examples.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 45,
      "context" : "Different from studies in the computer vision (CV) domain (Siméoni et al., 2020) where the model does not perform well in the low-data regime, pre-trained LM has achieved competitive performance with only a few labeled data, which makes further improvements to the vanilla fine-tuning challenging.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 37,
      "context" : "We conduct experiments on the TREC and Chemprot dataset4, where we first use Snorkel (Ratner et al., 2017) to obtain weak label set Xl, then fine-tune the pre-trained LM f(θ(0)) on Xl.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "For larger K, the performance also drops as some of the high-uncertainty regions can be outliers and sampling from them would hurt the model performance (Karamcheti et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 178
    }, {
      "referenceID" : 53,
      "context" : "Active learning has been widely applied to various NLP tasks (Yuan et al., 2020; Zhao et al., 2020; Shelmanov et al., 2021; Karamcheti et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 148
    }, {
      "referenceID" : 57,
      "context" : "Active learning has been widely applied to various NLP tasks (Yuan et al., 2020; Zhao et al., 2020; Shelmanov et al., 2021; Karamcheti et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 148
    }, {
      "referenceID" : 44,
      "context" : "Active learning has been widely applied to various NLP tasks (Yuan et al., 2020; Zhao et al., 2020; Shelmanov et al., 2021; Karamcheti et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : "Active learning has been widely applied to various NLP tasks (Yuan et al., 2020; Zhao et al., 2020; Shelmanov et al., 2021; Karamcheti et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 148
    }, {
      "referenceID" : 41,
      "context" : ", 2021a,b), diversitybased methods (Ru et al., 2020; Sener and Savarese, 2018) and hybrid methods (Yuan et al.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 43,
      "context" : ", 2021a,b), diversitybased methods (Ru et al., 2020; Sener and Savarese, 2018) and hybrid methods (Yuan et al.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 53,
      "context" : ", 2020; Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020; Ash et al., 2020; Kirsch et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : ", 2020; Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020; Ash et al., 2020; Kirsch et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 111
    }, {
      "referenceID" : 25,
      "context" : ", 2020; Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020; Ash et al., 2020; Kirsch et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 111
    }, {
      "referenceID" : 39,
      "context" : "Self-training first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability (Rosenberg et al., 2005; Lee, 2013).",
      "startOffset" : 156,
      "endOffset" : 191
    }, {
      "referenceID" : 28,
      "context" : "Self-training first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability (Rosenberg et al., 2005; Lee, 2013).",
      "startOffset" : 156,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "However, it is known to be vulnerable to error propagation (Arazo et al., 2020; Rizve et al., 2021).",
      "startOffset" : 59,
      "endOffset" : 99
    }, {
      "referenceID" : 38,
      "context" : "However, it is known to be vulnerable to error propagation (Arazo et al., 2020; Rizve et al., 2021).",
      "startOffset" : 59,
      "endOffset" : 99
    } ],
    "year" : 0,
    "abstractText" : "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop ACTUNE, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self training. ACTUNE switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from lowuncertainty regions are used for model selftraining. Additionally, we design (1) a regionaware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model’s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average.",
    "creator" : null
  }
}
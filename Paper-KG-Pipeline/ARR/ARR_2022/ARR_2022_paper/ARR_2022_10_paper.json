{
  "name" : "ARR_2022_10_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The impact of lexical and grammatical processing on generating code from natural language",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Translating natural language program descriptions to actual code is meant to help programmers to ease writing reliable code efficiently by means of a set of advanced code completion mechanisms.\nThere are mainly two classes of methods for obtaining code corresponding to a query expressed in natural language. The first one is code retrieval, which consists of searching and retrieving an appropriate code snippet from a code database. The second one is code generation, where the goal is to generate code fragments from a natural language description, generating potentially previously unseen code. In this work, we are interested in Python code generation. Code generation features a mismatch between an ambiguous and noisy natural language input and the structured nature of the generated code. Although Python’s vocabulary has a finite number of keywords, the set of value that can be assigned to a variable is infinite and constitutes one of the issues in predicting code corresponding to natural language.\nLike many other NLP tasks, current architectures for natural language to code generally take advantage of pre-trained language models such as BERT (Devlin et al., 2019) or GPT (Brown et al., 2020) based on the transformer architecture (Vaswani\net al., 2017). In particular, these architectures are used for code generation where parallel data is limited due to the human expertise required for alignment. The best results on code generation are reached by pretraining seq2seq models on external sources, then by fine-tuning those models on smaller data sets. For instance, Orlanski and Gittens (2021) fine-tunes BART (Lewis et al., 2020) on data pairs of natural language and code and by taking advantage of external informations. Similarly, Norouzi et al. (2021) used BERT and a transformer decoder in a semi-supervised way by taking advantage of a large amount of additional monolingual data. Another popular method is to train large language models on code (Austin et al., 2021; Hendrycks et al., 2021). Notably, GPT-3 has been finetuned on a large quantity of data from Github to obtain a powerful language model named Codex (Chen et al., 2021) that powers Github Copilot, a tool to help developers.\nOverall the above mentioned solutions aim to take advantage of large amounts of training data available nowadays, but few of them care about generating code that is guaranteed to be syntactically correct nor well typed. Let us mention some exceptions from semantic parsing like Dong and Lapata (2016); Rabinovich et al. (2017); Yin and Neubig (2017) that rely on grammatical constraints to ensure that the generated code can be executable.\nIn this work, we study variations around the TranX seq2seq architecture (Yin and Neubig, 2018) for translating natural language to code. Rather than generating directly code tokens from natural language, the architecture generates an Abstract Syntax Tree (AST) constrained by the programming language grammar.\nThe paper reports state of the art results on the task and specifically introduces:\n• A formalization of the grammar constrained code generator relying on the Earley (1970) parser transition system.\n• A study of the impact of key components of the architecture on the performance of the system: we study the impact of the grammatical component itself, the impact of the language model chosen, the impact of variable naming and typing and the impact of the input/output copy mechanisms.\nIt is structured as follows. Section 2 formalizes the symbolic transition system used for generating the grammatically correct code, Section 3 describes a family of variants around the TranX architecture that will be used to study the impact of these variations in the experimental part of the paper (Section 4)."
    }, {
      "heading" : "2 A transition system for code generation",
      "text" : "Among the models tested in the paper, some are generating syntactically constrained code. In the context of our study, we propose a transition model that meets two objectives: the code generated is grammatically valid in terms of syntax and the whole translation process still reduces to a seq2seq transduction mechanism that allows us to leverage standard machine learning methods.\nTo this end we introduce a transition system for code generation that generates an AST as a sequence of actions. The derivations can then be translated into ASTs and in actual Python code by means of deterministic functions. The set of valid ASTs is a set of trees that are generated by an ASDL grammar (Wang et al., 1997). An ASDL grammar is essentially a context free grammar abstracting away from low level syntactic details of the programming language and aims to ease the semantic interpretation of the parse trees. To this end ASDL grammar rules come with additional decorators called constructors and field names (Figure 1).\nOur transition system generates derivations, or sequences of actions, that can be translated to a syntactically correct Python code. We adapt to code generation the transition system of the Earley parser (Earley, 1970) as formalized in Figure 2. The generator state is a stack of dotted rules. A dotted rule is a rule of the formA→ α•Xβ where α is a sequence of grammar symbols whose subtrees are already generated and Xβ is a sequence of grammar symbols for which the subtrees are yet to be generated. The •X symbol is the dotted symbol or the next symbol for which the system has to generate the subtree. The Python ASDL grammar\nincludes rules with star (∗) qualifiers allowing zero or more occurrences of the starred symbol. The transition system uses an additional set of starred actions and a CLOSE action to stop these iterations (Figure 2).\nEach PREDICT(C) action starts the generation of a new subtree from its parent. The GENERATE action adds a new leaf to a tree. The COMPLETE action finishes the generation of a subtree and continues the generation process with its parent. The set of PREDICT actions is parametrized by the ASDL rule constructor (C), thus there are as many predict actions as there are constructors in the ASDL grammar. Constructors are required in order to generate the actual ASTs from the derivations.\nGENERATE(V) actions are actions responsible for generating the terminal or primitive symbols. The Python ASDL grammar generates ASTs with primitive leaf types (identifier, int, string, constant) that have to be filled with actual values for the AST to be useful. To generate actual primitive values the set of generate actions is also parametrized by the actual values V for the primitive types. The set of such values is infinite and consequently the set of generate actions is also infinite.\nNon determinism comes from the use of PREDICT(C), GENERATE(V) and CLOSE rules. By contrast the application of the COMPLETE action is entirely deterministic: once the generator has a completed dotted rule on the top of its stack, it has no other choice than applying the complete rule.\nThe sequential generation process is illustrated in Figure 3. Given a start state, at each time step, the generator has to decide which action to perform according to the current state of the stack and updates the stack accordingly. Once the generator reaches the goal state, we collect the list of actions performed (the derivation) in order to build the AST that we finally translate into actual Python code1."
    }, {
      "heading" : "3 Factors influencing code prediction",
      "text" : "All architectures analyzed in this study are variations around a seq2seq architecture. We describe the several variants of this architecture used in this paper both on the encoder and decoder side. We identify key factors that have an impact on the natural-language-to-code translation architecture\n1We use the astor2 library to this end.\nand we formalize a family of models that allow to test variations of these factors.\nWe consider a family of models generating Python code y from a natural language description x, that have the generic form:\np(y|x) = ∏ t p(yt|y<t, x) (1)\ny is either a sequence of code tokens in case we do not use a grammar, or a sequence of actions from a derivation in case we use a grammar. The decoding objective aims to find the most-probable hypothesis among all candidate hypotheses by solving the following optimization problem:\nŷ = argmax y p(y|x) (2)\nThe family of models varies according to four key qualitative factors that we identify in the TranX architecture. First we describe a substitution procedure managing variables and lists names in section 3.1). Second, in section 3.2, we test the architectural variations for encoding the natural language sequence. Third, in section 3.3, we describe variations related to constraining the generated code with grammatical constraints and architectural variations that allow to copy symbols from the natural language input to the generated code."
    }, {
      "heading" : "3.1 Substitution",
      "text" : "Programming languages come with a wide range of variable names and constant identifiers that make the set of lexical symbols infinite. Rather than learning statistics on a set of ad-hoc symbols, we rather normalize variable and constant names with a pre-processing method, reusing the method of Yin and Neubig (2018).\nPreprocessing amounts to substitute the actual names of the variables with a normalized set of predefined names known to the statistical model. The substitution step renames all variables both in the natural language and in the code with conventional names such as var_0, var_1, etc. for variables and lst_0,lst_1, etc. for lists. A post processing step substitutes back the predicted names with the original variable names in the system output. For example, given the natural language intent:\ncreate list `done` containing permutations of each element in list `[a, b, c, d]` with variable `x` as tuples\nis transformed into:\ncreate list var_0 containing permutations of each element in list lst_0 with variable var_1 as tuples\nThe predicted code such as var_0 = [(el, var_1) for el in [lst_0]] is transformed back into done = [(el, x) for el in [a, b, c, d]].\nModels using variable replacement as illustrated above, are identified with the notation SUBSTITUTION = TRUE in section 4. Implementing this heuristic is made easy by the design of the CoNaLa data set where all such names are explicitly quoted in the data while for Django we had to define our own heuristic."
    }, {
      "heading" : "3.2 Encoder",
      "text" : "We switched between a classic bi-LSTM and a pretrained BERTBASE to encode the input natural language {xi, i ∈ J1, nK} of n words into a vectorial representations {h(enc)i , i ∈ J1, nK} which are later used to compute the attention mechanism. We set the BERT factor to TRUE when using it and FALSE when using the bi-LSTM."
    }, {
      "heading" : "3.3 Decoder",
      "text" : "At each time step t, the LSTM decoder computes its internal hidden state h(dec)t :\nh (dec) t = LSTM([et−1 : ãt−1], h (dec) t−1 ) (3)\nwhere et−1 is the embedding from the previous prediction, ãt−1 is the attentional vector.\nWe compute the attentional vector ãt as in Luong et al. (2015) combining the weighted average over all the source hidden state ct and the decoder hidden state h(dec)t :\nãt =Wa[ct : h (dec) t ] (4)\nIt is the attention vector ãt which is the key to determine the next prediction yt.\nWe use several variants of the code generator, that we describe by order of increasing complexity. The basic generator is a feed forward that uses the attention vector to generate a code token v from a vocabulary V :\np(yt = GENERATE[v]|x, e<t) = softmax(e>v ·Wg · ãt)\n(5)\nThese models are not constrained by the Python grammar and we identify these models with GRAMMAR = FALSE.\nWe also use a pointer network that may either copy symbols from input to output or generate symbols from V . Then the probability of generating the symbol v is given by the marginal probability:\np(yt = GENERATE[v]|x, e<t) = p(gen|x, e<t)p(v|gen, x, e<t)\n+p(copy|x, e<t)p(v|copy, x, e<t) (6)\nThe probabilities p(gen|.) and p(copy|.) sum to 1 and are computed with softmax(W · ãt). The probability of generating v from the vocabulary V p(v|gen, .) is defined in the same way as (5). We use the pointer net architecture (Vinyals et al., 2015) to compute the probability p(v|copy, .) of copying an element from the natural language x. Models that use a pointer network are identified with PN = TRUE, otherwise with PN = FALSE .\nFinally we use a set of models that are constrained by the Python grammar and that rely on the transition system from section 2. Rather than directly generating Python code, these models generate a derivation whose actions are predicted using two prediction tasks. When the generator is in a state where the dot of the item on the top of the stack points on a nonterminal symbol, the PREDRULE is used. This task either\noutputs a PREDICT(C) action or the CLOSE action:\np(yt = PREDRULE[c]|x, e<t) = softmax(e>r ·Wp · ãt)\n(7)\nWhen the generator is in a state where the dot of the item on the top of the stack points on a terminal symbol, the generate task is used. This amounts to reuse either equation (5) or equation (6) according to the model at hand. Models constrained by the grammar are labelled with GRAMMAR = TRUE. Recall that the COMPLETE action of the transition system is called deterministically (Section 2)."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section we describe the characteristics of the data sets on which we have tested our different setups and the underlying experimental parameters3."
    }, {
      "heading" : "4.1 Data sets",
      "text" : "In this study we use two available data sets, Django and CoNaLa, to perform our code generation task.\nThe Django data set provides line-by-line comments with code from the Django web framework. About 70% of the 18805 examples are simple\n3The code of our experiments is public and available at anonymized adress\nPython operation ranging from function declarations to package imports, and including exception handling. Those examples strongly share the natural language structure (e.g. call the function cache.close → cache.close()). More than 26% of the words in the natural language are also present in the code, BLEU score between the natural language and code is equal to 19.4.\nCoNaLa is made up of 600k NL-code pairs from StackOverflow, among which 2879 examples have been been manually cleaned up by developers. All results are reported on the manually curated examples, unless stated otherwise. The natural language descriptions are actual developer queries (e.g. Delete an element 0 from a dictionary ‘a‘) and the associated code is diverse and idiomatic (e.g. {i: a[i] for i in a if (i != 0)}). Compared to Django, the code is much more challenging to generate. Especially because the number of words shared between the NL and the code is much lower (BLEU = 0.32). Also, the code is longer and more complex with an AST depth of 7.1 on average against 5.1 for Django."
    }, {
      "heading" : "4.2 Vocabulary generation",
      "text" : "The vocabulary of natural language and code is essential. Usually, this vocabulary is created by adding all the words present in the training data set. There are however exceptions that are detailed in this section.\nThe natural language vocabulary relies on a byte pair encoding tokenizer when BERT = TRUE. As explained in section 3.1, the variable names are replaced with special tokens var_i and lst_i. These new tokens are crucial to our problem, and added to the BERT vocabulary. We can then finetune BERT with this augmented vocabulary on our data sets.\nFor the decoder part, when GRAMMAR = TRUE, the vocabulary of grammatical actions is fixed, while the vocabulary of AST leaves has to be built. This associated vocabulary can be composed of built-in Python functions, libraries with their associated functions or variable names. Its creation is consequently a major milestone in the generation process.\nTo create this external vocabulary, we proceed as in TranX. From the code, we create the derivation sequence composed of the action of the grammar as well as the primitives. All primitives of the action sequences are incorporated into our external\nvocabulary."
    }, {
      "heading" : "4.3 Setup",
      "text" : "When BERT = FALSE, the size of the representations is kept small to prevent overfitting. Encoder and decoder embedding size is set to 128. The hidden layer size of the encoder and decoder bi-LSTM is set to 256 and the resulting attention vector size is 300. We have two dropout layers: for embeddings and at the output of the attention. We use Adam optimizer with learning rate α = 5.10−3.\nWhen BERT = TRUE, encoder embeddings have a natural size of 756 with BERT. We therefore apply a linear transformation to its output to get an embedding size equal to 512. The size of LSTM decoder hidden state and attention vector are set to 512. We regularize only the attentional vector in that case. We use Adam optimizer with learning rate α = 5.10−5. In both cases, we use a beam search size of 15 for decoding.\nEvaluation We report the standard evaluation metric for each data set: exact match accuracy and corpus-level BLEU.\nPython version As the grammar slightly changes between Python versions, let us mention that all our experiments have been carried out with Python 3.7."
    }, {
      "heading" : "4.4 Ablation study",
      "text" : "To highlight the contribution of the different factors, SUBSTITUTION, BERT, GRAMMAR, PN on the Django and CoNaLa data sets we report a detailed study of their impact in Table 1.\nThe results are analyzed by distinguishing lexical and grammatical aspects and by identifying relations between the different factors. We start by a comparison of the marginal mean of the BLEU score for each of our variables in both conditions. Figure 5 highlights the mean difference between the conditions by contrasting the case where the value is TRUE with the case where the value is FALSE.\nPointer network The pointer network can improve the results, especially when SUBSTITUTION = FALSE. This is because the only way to obtain the name of the variables is to copy them. Combined with substitution, the pointer network offers an additional possibility to predict the var_i, lst_i which allows to achieve the best results with a BLEU score of 39.01 on CoNaLa and an exact match accuracy of 76 on Django.\nSubstitution and Typing The scores are stabilised and much higher with substitution. We gain more than 9 points of BLEU on CoNaLa (respectively 20 points on Django) thanks to substitution. The \"weakest\" configuration where all variables are FALSE except the substitution gives better results than all configurations where SUBSTITUTION = FALSE. The increase in BLEU with substitution can be explained in two ways. On the one hand, we remark that the model has difficulties to memorize the val-\nues to fill the lists with GENERATE. For example, four tokens of code must be generated to predict the list [a, b, c, d]. Using substitution, the model can just predict lst_0 which will be replaced by [a, b, c, d] during postprocessing. This avoids a potential error in the creation of the list and directly gives a valid 4-gram. This contributes to greatly increase the BLEU, which shows the importance of replacing listf. On CoNaLa, BLEU score on the development set drops from an average of 37.99 to an average of 30.66 without list replacement. Besides list replacement, the architecture has also a weakness with respect to variable typing. When using the grammar without substitution, the results are lower than without grammar. This effect is the result of a type checking failure. The model predicts ill-typed AST structures. For instance it predicts an AST whose corresponding code should be 1.append([6,7]). However the AST library we used prevents from generating such ill-typed code. The absence of code generation in such cases explain the decrease in BLEU score.\nThe use of substitution partially corrects for these typing errors because the substituted symbols var_i, lst_i are generally more likely to be predicted and are likely to have the right type thanks to the mapping.\nGrammatical aspect The transition system doesn’t improve the results on average because\nof the empty predictions when SUBSTITUTION = FALSE. The use of the transition system leads to better results when SUBSTITUTION = TRUE but not as drastically as one would have expected. However the real contribution of the grammar associated with substitution is the syntactic validity of the code in 100% of the cases, as tested with our architecture obtaining the best results. In scenarios where we do not use the grammar, it is never the case to have an empty output. But then the proportion of code sequences that are actually syntactically valid in this setup is 92% on average.\nBERT As expected when using BERT to encode the natural language input we get an improvement of about 6 marginal BLEU on CoNaLa (respectively +3 BLEU on Django). More interestingly, this effect is lower than the one of the substitution operation.\nWe conclude that the use of a pre-trained model increases the results but less than substitution, despite what one might think and it suggests that improving the management of variable names and lists is one of the key elements for improving the system. The contribution of grammatical constraints in BLEU may seem detrimental but we could see that this is a side effect of typing constraints in adversarial scenarios. Overall the nonconstrained generated code is syntactically incorrect in 8% of the cases."
    }, {
      "heading" : "4.5 Test",
      "text" : "We compare in table 2 our results with other systems on CoNaLa and Django test sets. We report our best performing models on the development set with and without grammatical constraints. We also use models trained on the full CoNaLa including mined examples to get relevant comparisons.\nAmong the other systems Yin and Neubig (2018) is the only one that uses grammatical constraints.\nOur architecture differs with the use of a BERT encoder whereas Yin and Neubig (2018) use an LSTM. The other systems do not use grammatical constraints but rather try to take advantage of additional data. Orlanski and Gittens (2021) and Norouzi et al. (2021) aim to take advantage of the CoNaLa mined examples. As these mined examples are noisy, Orlanski and Gittens (2021) takes advantage of BART (Lewis et al., 2020), a denoising encoder. They also enrich the natural language input with the results of queries from StackOverflow by adding the title of the post, its associated tags, etc. Norouzi et al. (2021) use BERT as encoder and a transformer decoder. They apply the Target Autoencoding method introduced by Currey et al. (2017). During training, the encoder parameters are frozen and the decoder is trained to reconstruct code examples. They use this method on the mined examples to take maximal advantage of the additional noisy data.\nWe observe that our grammar based model with BERT encoder is state of the art on CoNaLa while the transformer encoder/decoder architecture of Norouzi et al. (2021) performs best on Django. Quite interestingly the exact match accurracy of these models remain weak on CoNaLa."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We formalized a transition system that allows us to guarantee the generation of syntactically correct code. A detailed study of the components of the seq2seq architecture reveals that the models have difficulties at managing accurately variable names and list encodings. The comparison with models trained on larger noisy data sets reveals that our grammatically constrained architecture without explicit denoising remains competitive. This further highlights the importance of grammatical constraints and of specific processes dedicated to manage variables, list naming and typing."
    }, {
      "heading" : "A Additional Qualitative Examples",
      "text" : "We present examples of code generated by our best models with and without grammar.\nSource convert tuple ‘t‘ to list\nGold list(t)\nGrammar [x for x in t for x in t]\nWithout Grammar [i for i in t]\nRemark Problem of CLOSE for the Grammar output. Without grammar the code is correct but with a low BLEU.\nSource get the position of item 1 in ‘testlist‘\nGold [i for i, x in enumerate(testlist) if x == 1]\nGrammar [i for i, v in enumerate(testlist) if v == 1]\nWithout Grammar testlist = [i for i in testlist if i != 1]\nRemark Grammar output is not equal to Gold due to dummy variable.\nSource declare an array\nGold my_list = []\nGrammar x = [0] * 2\nWithout Grammar [(0) for _ in range (10000)]\nRemark Source is not precise enough. Models’ outputs are valid.\nSource append a numpy array ‘b‘ to a numpy array ‘a‘\nGold np.vstack((a, b))\nGrammar a = numpy.array([b, a])\nWithout Grammar z = np.array([b]). reshape((3, 3))\nRemark Gold is not accurate with np undefined before. vstack function not in the external vocabulary.\nSource activate is a lambda function which returns None for any argument x.\nGold activate = lambda x : None\nGrammar activate = lambda x = None : x\nWithout Grammar activate = lambda x : None\nRemark Good BLEU for grammar output while the result is not adequate.\nSource remove first element of text\nGold text = text[1:]\nGrammar text = text[1:]\nWithout Grammar text[1:\nRemark Syntax mistake for the code without grammar.\nSource increment piece by first element of elt\nGold piece += elt[0]\nGrammar piece += elt[1]\nWithout Grammar piece += elt[1]\nRemark First element of a list is zero, not one."
    } ],
    "references" : [ {
      "title" : "Program synthesis with large language models",
      "author" : [ "Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "Henryk Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton." ],
      "venue" : "CoRR, abs/2108.07732.",
      "citeRegEx" : "Austin et al\\.,? 2021",
      "shortCiteRegEx" : "Austin et al\\.",
      "year" : 2021
    }, {
      "title" : "Evaluating large language models trained on code",
      "author" : [ "Morikawa", "Alec Radford", "Matthew Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba." ],
      "venue" : "vol-",
      "citeRegEx" : "Morikawa et al\\.,? 2021",
      "shortCiteRegEx" : "Morikawa et al\\.",
      "year" : 2021
    }, {
      "title" : "Copied monolingual data improves low-resource neural machine translation",
      "author" : [ "Anna Currey", "Antonio Valerio Miceli Barone", "Kenneth Heafield." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark,",
      "citeRegEx" : "Currey et al\\.,? 2017",
      "shortCiteRegEx" : "Currey et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The",
      "citeRegEx" : "Dong and Lapata.,? 2016",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "An efficient context-free parsing algorithm",
      "author" : [ "Jay Earley." ],
      "venue" : "Commun. ACM, 13(2):94–102.",
      "citeRegEx" : "Earley.,? 1970",
      "shortCiteRegEx" : "Earley.",
      "year" : 1970
    }, {
      "title" : "Measuring coding challenge competence with APPS",
      "author" : [ "Dan Hendrycks", "Steven Basart", "Saurav Kadavath", "Mantas Mazeika", "Akul Arora", "Ethan Guo", "Collin Burns", "Samir Puranik", "Horace He", "Dawn Song", "Jacob Steinhardt." ],
      "venue" : "CoRR, abs/2105.09938.",
      "citeRegEx" : "Hendrycks et al\\.,? 2021",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2021
    }, {
      "title" : "BART: denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Code generation from natural language with less prior knowledge and more monolingual data",
      "author" : [ "Sajad Norouzi", "Keyi Tang", "Yanshuai Cao." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Norouzi et al\\.,? 2021",
      "shortCiteRegEx" : "Norouzi et al\\.",
      "year" : 2021
    }, {
      "title" : "Reading stackoverflow encourages cheating: Adding question text improves extractive code generation",
      "author" : [ "Gabriel Orlanski", "Alex Gittens." ],
      "venue" : "CoRR, abs/2106.04447.",
      "citeRegEx" : "Orlanski and Gittens.,? 2021",
      "shortCiteRegEx" : "Orlanski and Gittens.",
      "year" : 2021
    }, {
      "title" : "Abstract syntax networks for code generation and semantic parsing",
      "author" : [ "Maxim Rabinovich", "Mitchell Stern", "Dan Klein." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,",
      "citeRegEx" : "Rabinovich et al\\.,? 2017",
      "shortCiteRegEx" : "Rabinovich et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "pages 2692–2700.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "The zephyr abstract syntax description language",
      "author" : [ "Daniel C. Wang", "Andrew W. Appel", "Jeffrey L. Korn", "Christopher S. Serra." ],
      "venue" : "Proceedings of the Conference on Domain-Specific Languages, October 15-17, 1997, Santa Barbara, California,",
      "citeRegEx" : "Wang et al\\.,? 1997",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 1997
    }, {
      "title" : "A syntactic neural model for general-purpose code generation",
      "author" : [ "Pengcheng Yin", "Graham Neubig." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 9",
      "citeRegEx" : "Yin and Neubig.,? 2017",
      "shortCiteRegEx" : "Yin and Neubig.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Like many other NLP tasks, current architectures for natural language to code generally take advantage of pre-trained language models such as BERT (Devlin et al., 2019) or GPT (Brown et al.",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : ", 2020) based on the transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "For instance, Orlanski and Gittens (2021) fine-tunes BART (Lewis et al., 2020) on data pairs of natural language and code and by taking advantage of external informations.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Another popular method is to train large language models on code (Austin et al., 2021; Hendrycks et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "Another popular method is to train large language models on code (Austin et al., 2021; Hendrycks et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Like many other NLP tasks, current architectures for natural language to code generally take advantage of pre-trained language models such as BERT (Devlin et al., 2019) or GPT (Brown et al., 2020) based on the transformer architecture (Vaswani et al., 2017). In particular, these architectures are used for code generation where parallel data is limited due to the human expertise required for alignment. The best results on code generation are reached by pretraining seq2seq models on external sources, then by fine-tuning those models on smaller data sets. For instance, Orlanski and Gittens (2021) fine-tunes BART (Lewis et al.",
      "startOffset" : 148,
      "endOffset" : 601
    }, {
      "referenceID" : 2,
      "context" : "Like many other NLP tasks, current architectures for natural language to code generally take advantage of pre-trained language models such as BERT (Devlin et al., 2019) or GPT (Brown et al., 2020) based on the transformer architecture (Vaswani et al., 2017). In particular, these architectures are used for code generation where parallel data is limited due to the human expertise required for alignment. The best results on code generation are reached by pretraining seq2seq models on external sources, then by fine-tuning those models on smaller data sets. For instance, Orlanski and Gittens (2021) fine-tunes BART (Lewis et al., 2020) on data pairs of natural language and code and by taking advantage of external informations. Similarly, Norouzi et al. (2021) used BERT and a transformer decoder in a semi-supervised way by taking advantage of a large amount of additional monolingual data.",
      "startOffset" : 148,
      "endOffset" : 764
    }, {
      "referenceID" : 0,
      "context" : "Another popular method is to train large language models on code (Austin et al., 2021; Hendrycks et al., 2021). Notably, GPT-3 has been finetuned on a large quantity of data from Github to obtain a powerful language model named Codex (Chen et al., 2021) that powers Github Copilot, a tool to help developers. Overall the above mentioned solutions aim to take advantage of large amounts of training data available nowadays, but few of them care about generating code that is guaranteed to be syntactically correct nor well typed. Let us mention some exceptions from semantic parsing like Dong and Lapata (2016); Rabinovich et al.",
      "startOffset" : 66,
      "endOffset" : 610
    }, {
      "referenceID" : 0,
      "context" : "Another popular method is to train large language models on code (Austin et al., 2021; Hendrycks et al., 2021). Notably, GPT-3 has been finetuned on a large quantity of data from Github to obtain a powerful language model named Codex (Chen et al., 2021) that powers Github Copilot, a tool to help developers. Overall the above mentioned solutions aim to take advantage of large amounts of training data available nowadays, but few of them care about generating code that is guaranteed to be syntactically correct nor well typed. Let us mention some exceptions from semantic parsing like Dong and Lapata (2016); Rabinovich et al. (2017); Yin and Neubig (2017) that rely on grammatical constraints to ensure that the generated code can be executable.",
      "startOffset" : 66,
      "endOffset" : 636
    }, {
      "referenceID" : 0,
      "context" : "Another popular method is to train large language models on code (Austin et al., 2021; Hendrycks et al., 2021). Notably, GPT-3 has been finetuned on a large quantity of data from Github to obtain a powerful language model named Codex (Chen et al., 2021) that powers Github Copilot, a tool to help developers. Overall the above mentioned solutions aim to take advantage of large amounts of training data available nowadays, but few of them care about generating code that is guaranteed to be syntactically correct nor well typed. Let us mention some exceptions from semantic parsing like Dong and Lapata (2016); Rabinovich et al. (2017); Yin and Neubig (2017) that rely on grammatical constraints to ensure that the generated code can be executable.",
      "startOffset" : 66,
      "endOffset" : 659
    }, {
      "referenceID" : 5,
      "context" : "• A formalization of the grammar constrained code generator relying on the Earley (1970) parser transition system.",
      "startOffset" : 75,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "The set of valid ASTs is a set of trees that are generated by an ASDL grammar (Wang et al., 1997).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "We adapt to code generation the transition system of the Earley parser (Earley, 1970) as formalized in Figure 2.",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Rather than learning statistics on a set of ad-hoc symbols, we rather normalize variable and constant names with a pre-processing method, reusing the method of Yin and Neubig (2018). Preprocessing amounts to substitute the actual names of the variables with a normalized set of predefined names known to the statistical model.",
      "startOffset" : 160,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "We compute the attentional vector ãt as in Luong et al. (2015) combining the weighted average over all the source hidden state ct and the decoder hidden state h t :",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "We use the pointer net architecture (Vinyals et al., 2015) to compute the probability p(v|copy, .",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "As these mined examples are noisy, Orlanski and Gittens (2021) takes advantage of BART (Lewis et al., 2020), a denoising encoder.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "Among the other systems Yin and Neubig (2018) is the only one that uses grammatical constraints.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "Among the other systems Yin and Neubig (2018) is the only one that uses grammatical constraints. Our architecture differs with the use of a BERT encoder whereas Yin and Neubig (2018) use an LSTM.",
      "startOffset" : 24,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "Orlanski and Gittens (2021) and Norouzi et al. (2021) aim to take advantage of the CoNaLa mined examples.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Orlanski and Gittens (2021) and Norouzi et al. (2021) aim to take advantage of the CoNaLa mined examples. As these mined examples are noisy, Orlanski and Gittens (2021) takes advantage of BART (Lewis et al.",
      "startOffset" : 32,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "As these mined examples are noisy, Orlanski and Gittens (2021) takes advantage of BART (Lewis et al., 2020), a denoising encoder. They also enrich the natural language input with the results of queries from StackOverflow by adding the title of the post, its associated tags, etc. Norouzi et al. (2021) use BERT as encoder and a transformer decoder.",
      "startOffset" : 88,
      "endOffset" : 302
    }, {
      "referenceID" : 2,
      "context" : "They apply the Target Autoencoding method introduced by Currey et al. (2017). During training, the encoder parameters are frozen and the decoder is trained to reconstruct code examples.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "We observe that our grammar based model with BERT encoder is state of the art on CoNaLa while the transformer encoder/decoder architecture of Norouzi et al. (2021) performs best on Django.",
      "startOffset" : 142,
      "endOffset" : 164
    } ],
    "year" : 0,
    "abstractText" : "Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. To study the impact of these components, we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided. The paper highlights the importance of the lexical substitution component in the current natural language to code systems.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_356_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data – as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT’s predictions."
    }, {
      "heading" : "1 Introduction",
      "text" : "Much effort has been put into curating data in the area of hate speech, from foundational work (Waseem and Hovy, 2016; Davidson et al., 2017) to more recent, broader (Sap et al., 2020) as well as multilingual (Ousidhoum et al., 2019) approaches. However, the demographics of those targeted by hate speech and those creating datasets are often quite different. For example, in Founta et al. (2018), 66% of annotators are male and in Sap et al. (2020), 82% are white. This may lead to unwanted bias (e.g., disproportionately labeling African American English as hateful (Sap et al., 2019; Davidson et al., 2019a)) and to collection of data that is not representative of the comments directed at target groups; e.g., a white person may not see and not have access to hate speech targeting a particular racial group.\nAn example from our dataset is the Kenyan social media post “…Wewere taught that such horrible things can only be found in Luo Nyanza.” The Luo are an ethnic group in Kenya; Nyanza is a Kenyan province. The post is incendiary because it suggests that the Luo are responsible for horrible things, insinuating that retaliation against them may be justified. Only a group of people deeply rooted in Kenya can collect such examples and understand their significance. XTREMESPEECH. In this paper, we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we empower the local affected communities (as opposed to companies and governments) to collect and annotate the data, thus avoiding the problems inherent in approaches that hire outside groups\nfor hate speech dataset creation. In more detail, we built a team of annotators from fact-checking groups from the four different countries. These annotators both collected and annotated data from channels most appropriate for their respective communities. They were also involved in all phases of the creation of XTREMESPEECH, from designing the annotation scheme to labeling. Our inclusive approach results in a dataset that better represents content targeting these communities and that minimizes bias against them because fact-checkers are trained to be objective and know the local context. Figure 1 gives a high-level overview of data collection and annotation for XTREMESPEECH. XTREMESPEECH also is a valuable resource because existing hate speech resources are not representative for problematic speech on a worldwide scale: they mainly cover Western democracies. In contrast, our selection is more balanced, containing three countries from the Global South and one Western democracy. We present a data statement (see Bender and Friedman (2018)) in Appendix A. Anthropological perspective. It has been argued that the NLP community does not sufficiently engage in interdisciplinary work with other fields that address important aspects of hate speech (Jo and Gebru, 2020). In this work, we take an anthropological perspective: the research we present is a collaboration of anthropologists and computational linguists. Internationally, hate speech mostly affects communities that are defined in terms of ethnicity, race and religion. As the discipline that studies people’s knowledge, customs and institutions in the context of their society and culture, sociocultural anthropology can provide a highlevel framework for investigating and theorizing about the phenomenon of hate speech and its cultural variations. We also take an anthropological perspective for defining the terminology in this paper. Potentially harmful online speech is most often referred to by NLP researchers and general media1 as hate speech. From its original, culturally-grounded meaning, hate speech has evolved into a primarily legal and political term with different definitions, depending on who uses it (Bleich, 2014; Saltman and Russell, 2014; Bakalis, 2018). We therefore use the concept of extreme speech from anthropology and adopt its definition as speech that pushes\n1Classifying and Identifying the Intensity of Hate Speech\nthe boundaries of civil language (Udupa and Pohjonen, 2019; Udupa et al., 2021). In investigating extreme speech, anthropologists focus on cultural variation and historical conditions that shape harmful speech. Extreme speech categories. We differentiate between extreme speech that requires removal (denoted R) and speech for which moderation (denoted M) is sufficient. Extreme speech of the M category consists of derogatory speech – roughly, disrespectful and negative comments about a group that are unlikely to directly translate into specific harm. We further subdivide R extreme speech into exclusionary extreme speech (roughly: speech inciting discrimination) and dangerous extreme speech (roughly: speech inciting violence); definitions are given in §3.2. This distinction is important when considering removal of extreme speech; e.g., dangerous speech may warrant more immediate and drastic action than exclusionary speech. XTREMESPEECH does not contain neutral text, focusing solely on M and R extreme speech. Neutral text has been shown to be easier to label both for humans and models while identifying and subclassifying non-neutral text (i.e., extreme speech) remains the Achilles’ heel of current NLP models (Davidson et al., 2017). Finally, we also annotate the targets of extreme speech; examples are “religious minorities” and “immigrants” (frequent targets in India and Germany). Classification tasks. We define three classification tasks. (i) REMOVAL. The two-way classification M vs. R. (ii) EXTREMITY. The threeway classification according to degree of extremity: derogatory vs. exclusionary vs. dangerous. (iii) TARGET. Target group classification. We propose a series of baselines and show that model performance is mediocre for REMOVAL, poor for EXTREMITY and good for TARGET. Further, we show that BERT-based models are unable to generalize in cross-country and crosslingual settings, confirming the intuition that cultural and world knowledge is needed for this task. Finally, we perform a model interpretability analysis with LIME (Ribeiro et al., 2016) to uncover potential model biases and deficiencies. Contributions. In summary, we (i) establish a community-first framework of data curation, (ii) present XTREMESPEECH, a dataset of 20,297 ex-\ntreme speech passages from Brazil, Germany, India and Kenya, capturing target groups and multiple levels of extremity, (iii) propose a series of tasks and baselines, as the basis for meaningful comparison with future work, (iv) show performance both for models and humans is low across tasks except in target group classification, (v) confirm the intuition that extreme speech is dependent on social and cultural knowledge, with low cross-lingual and cross-country performance."
    }, {
      "heading" : "2 Related Work",
      "text" : "Earlier work in hate speech detection focused on data collection, curation and annotation frameworks (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Recent work has expanded the set of captured labels to include more pertinent information such as targets and other forms of abuse (Sap et al., 2020; Hede et al., 2021; Guest et al., 2021; Grimminger and Klinger, 2021; Ross et al., 2017) as well as benchmarking (Röttger et al., 2021; Mathew et al., 2021). Analysis of datasets has been performed too (Madukwe et al., 2020; Kim et al., 2020; Wiegand et al., 2019; Swamy et al., 2019; Davidson et al., 2019a). Work has also been conducted to expand research to multiple languages (Ousidhoum et al., 2019; Ranasinghe and Zampieri, 2020; Ross et al., 2017; Nozza, 2021; Zoph et al., 2016; Marivate et al., 2020; Nekoto et al., 2020). XTREMESPEECH contributes to this goal by providing Brazilian Portuguese, German, Hindi and Swahili data. Research has also been conducted to investigate annotation bias and annotator pools (Al Kuwatly et al., 2020; Waseem, 2016; Ross et al., 2017; Shmueli et al., 2021; Posch et al., 2018), as well as bias (especially racial) in existing datasets (Davidson et al., 2019b; Laugier et al., 2021). Methods for bias mitigation have also been proposed (Sap et al., 2019; Xia et al., 2020; Han et al., 2021; Zhou et al., 2021). It was found that data can reflect and propagate annotator bias. To address this, we diversify the annotator pool in our work. In another line of work, theoretical foundations are being established, in the form of taxonomies (Banko et al., 2020), definitions (Wiegand et al., 2021; Waseem et al., 2017) and hate speech theory (Price et al., 2020; Laaksonen et al., 2020). We are adding to this with definitions based on fieldwork and grounded research, inspired by anthropological and ethnographic work that investigates the\nsocietal impact of online hate and extreme speech (Boromisza-Habashi, 2013; Donovan and danah boyd, 2021; Haynes, 2019; Udupa and Pohjonen, 2019; Hervik, 2019). Further, strides have been made in the ethics of AI.Who should collect data andwho is responsible for model deployment decisions? Calls have been made for more inclusive pools of annotators and domain experts overseeing NLP projects, as well as exploration of other ethical dilemmas (Leins et al. (2020); Jo and Gebru (2020); Mitchell et al. (2020); Vidgen et al. (2019); Gebru (2019); Mohamed et al. (2020), inter alia). With our focus on community-embedded fact-checkers our framework is more inclusive than previous work."
    }, {
      "heading" : "3 Dataset",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset Description",
      "text" : "XTREMESPEECH consists of 20,297 text passages, each targeted at a particular group (e.g., ethnic minorities), and is platform-agnostic, with text collected from multiple social media platforms, as well as direct messaging (with any private information anonymized). While forms of extreme speech may originate from one place, dissemination to other platforms is swift (Rogers, 2020). Proprietary efforts have also taken the platform-agnostic approach.2 Data is collected from Brazil, Germany, India and Kenya. Passages are written in Portuguese, German, Hindi and Swahili, as well as in English. English can either be used on its own, or in conjunctionwith the local language in the form of code switching. We capture this in the annotation: passages that contain English – even if it is only a hashtag in a tweet – are marked as containing both languages. Figure 1 shows the distribution of languages. Passages were labeled both on content and target levels. On their content they are labeled as derogatory, exclusionary or dangerous. On the target level, we make a distinction between text targeted at protected groups and at institutions of power. We take into account the following protected groups: ethnic minorities, immigrants, religious minorities, sexual minorities, women, racialized groups, historically oppressed caste groups, indigenous groups and large ethnic groups. We also give the annotators the option to input any other group. For institutions of power, possible\n2https://www.perspectiveapi.com/\ntargets are politicians, legacy media and the state. To allow for political discourse, extreme speech against institutions of power should not be filtered out, so such speech was marked as derogatory."
    }, {
      "heading" : "3.2 Extreme Speech Definitions",
      "text" : "Building on Benesch (2018) andUdupa (2021), we define extreme speech labels as follows: Derogatory Extreme Speech: Expressions that do not conform to accepted norms of civility within specific regional/local/national contexts and target people/groups based on racialized categories or protected characteristics (ethnicity, national origin, religious affiliation, sexual orientation, gender, language group) or others (state, media, politicians). Includes derogatory expressions about abstract categories/concepts that they identify target groups with. Exclusionary Extreme Speech: Expressions that call for or imply excluding historically disadvantaged and vulnerable people/groups from the “in-group” based on national origin, gender, sexual orientation, ethnicity, caste, racialized categories, language or religious affiliation. These expressions incite discrimination, abhorrence and delegitimization of target groups. The label does not apply to abstract ideas, ideologies or institutions, except when there are reasonable grounds to believe that attacks against abstract ideas/ideologies/institutions amount to a call for exclusion of vulnerable groups associated with these categories. Dangerous Extreme Speech: Passages that have a reasonable chance to trigger harm and violence against target groups (including ostracism, segregation and deportation). All the following criteria should be met for a passage to be classified as dangerous: (i) content calls for harm, (ii) speaker has high degree of influence over specific audience, (iii) audience has grievances and fears that the speaker can cultivate, (iv) target groups are historically disadvantaged and vulnerable to attacks, (v) influential means to disseminate speech. Whereas derogatory extreme speech is a form of speech that requires moderation but, generally, not removal (denoted with M), exclusionary and dangerous speech do require removal (denoted with R) in most cases to protect users from potential harm. We make a distinction between exclusionary and dangerous speech in order to introduce a more fine-grained scale of extremity that can dictate more focused policy (e.g., more severe punish-\nment may be appropriate for dangerous speech). It has been shown in previous work that while neutral text is easy to detect (Davidson et al., 2017), models find it hard to differentiate between different types of extreme speech (e.g., between our definitions of M or R), a task challenging even for humans. By focusing on the difficult distinctions within non-neutral text, we hope to contribute to research that will be able to classify types of potentially harmful speech correctly in the future, which is both the critical point of extreme speech research and the main obstacle towards effective filtering. Exemplary cases for the three labels (derogatory, exclusionary, dangerous) were discussed in detail with the annotators. We believe our interdisciplinary approach will lead to data more aligned with the real world and will benefit the target groups and communities to greater effect."
    }, {
      "heading" : "3.3 Data Collection",
      "text" : ""
    }, {
      "heading" : "3.3.1 Annotator Profiles",
      "text" : "We selected Brazil, Germany, India and Kenya to cover a range of cultures and communities. There are 8 female and 5 male annotators (per country, female/male counts are 2/1 in Brazil, 4/0 in Germany, 2/2 in India and 0/2 in Kenya). Each annotator is a fact-checker who i) is local, ii) is independent (i.e., not employed by social media companies or large media corporations) and iii) investigates the veracity of news articles, including articles directed at or related to local communities. We see independent fact-checkers as a key stakeholder community that provides a feasible and meaningful gateway into cultural variation in online extreme speech. Through their job as factcheckers, they regularly come in contact with extreme speech, with communities that peddle extreme speech as well as with communities targeted by extreme speech. Further details and background can be found in Appendix B."
    }, {
      "heading" : "3.3.2 Annotation Scheme",
      "text" : "Through an online interface, data is entered as found in online media (including data the annotators themselves receive, for example, through direct messaging). This online interface (in the form of a web page, shown in Figure 2 of the Appendix) serves both as the data entry point and the annotation form. After finding a passage of online extreme speech, annotators insert it in our form and are then prompted to label it (see categories in\n§3.1). Input from the annotators is taken into account in all stages of this process, to ensure smooth and reliable data collection."
    }, {
      "heading" : "3.4 Inter-annotator Agreement",
      "text" : "To verify the quality of XTREMESPEECH, we calculate inter-annotator agreement. The data collected from one annotator is shown to another for verification (details in Appendix D). Only the text passage is shown to annotators, without prior category assignments. The agreement scores we measure are: Cohen’s kappa (κ, McHugh (2012)), Krippendorff’s alpha (α, Krippendorff (2011)), intraclass correlation coefficient (two-way mixed, average score ICC(3, k) for k = 2, Cicchetti (1994)) and accuracy (defined as the percentage of passages where both annotators agreed). For the three extreme speech labels, κ = 0.23, α = 0.24 and ICC(3, k) = 0.41 (considered “fair” (Cicchetti, 1994)). Accuracy is 63% overall, 78% for derogatory, 40% for exclusionary and 19% for dangerous. For the more important distinction of M vs. R, accuracy is 78.4% for M and 46.3% for R. For the classification of the target of extreme speech, κ = 0.69. Scores are low compared to other NLP tasks, which is unfortunately a widespread phenomenon in hate speech research. In Founta et al. (2018), only in 55.9% of passages did at least 4 out of 5 annotators agree. In Sap et al. (2020), theα score was 0.45, with a 76% agreement on “offensiveness”, 75% on “intent to offend” and 74% on “targeted group”. In Davidson et al. (2017), there was a 90% agreement onwhether the languagewas neutral, offensive, or hateful. In Ross et al. (2017), a German dataset, α was between 0.18 and 0.29, while in Ousidhoum et al. (2019), a multilingual dataset, α was between 0.15 and 0.24. We argue that in our work, not only are we dealing with a heavily imbalanced dataset, but also that the task is even more challenging than prior work. While prior work collects both neutral passages and hate speech (e.g., in Davidson et al. (2017)), we only collect extreme speech. So whereas in prior work the annotators need to differentiate between neutral and extreme speech (which, intuitively, is an easier task), our annotators only make decisions on the hard task of determining different degrees of extremity."
    }, {
      "heading" : "3.5 Reannotation",
      "text" : "After discussing inconsistently labeled passages with the annotators, we found that there was disagreement about groups currently in power, specifically, about the Kikuyu and Kalenjin ethnic groups (more information in Appendix E). One annotator considered them ethnic minorities because most other ethnic groups are pitted against them. The other annotator did not view them as minorities because they are (i) the two most populous ethnic groups and (ii) are not in the minority when it comes to representation in positions of power. A consensus was reached to add a new target label, “large ethnic group”, to correctly represent this state of affairs in the annotation. As is common practice, instead of limiting the reannotation to passages the annotators disagreed on, we provided all potentially affected passages for reannotation, i.e., all “indigenous group” and “ethnic minority” passages."
    }, {
      "heading" : "3.6 Dataset Analysis",
      "text" : ""
    }, {
      "heading" : "3.6.1 Extreme Speech Analysis",
      "text" : "XTREMESPEECH contains 20,297 passages from the four countries. From each country, we chose to only collect data on one local language plus English. The distribution of languages is shown in Table 1. While for Germany and Brazil, English is rarely used, in India and Kenya it is much more prominent, both on its own and in code switching. The distribution of labels, shown in Table 2, varies a lot from country to country. For example, in Germany annotators labeled far fewer passages as dangerous speech, reflecting stricter regulatory controls over speech compared to the other countries. Data is also heavily imbalanced in Brazil, with the majority of passages being derogatory. The distribution of targets per country in Table 4 again shows large divergences between coun-\ntries. In Germany, immigrants are the main target group because of right-wing opposition to a wave of recent immigration. In India, religious minorities dominate the target group statistics because of the conflict between Hindus and Muslims. Thus XTREMESPEECH reflects the current social and political situation in a country. Recall that the category “large ethnic groups” was added during the reannotation process for Kenya (§3.5). In the other countries, this is not one of the target group categories."
    }, {
      "heading" : "3.6.2 Word Frequency",
      "text" : "Table 3 shows themost frequent words for the three extreme speech labels for the four countries. We see that words indicative of sociopolitical conflict appear frequently: ‘comunista’ and ‘feminista’ in Brazil; ‘merkel’, ‘deutschland’, ‘deutsche’ and ‘europa’, as well as the word for Jew, ‘jude’ in Germany; words related to Islam (e.g., ‘muslims’) alongside words like ‘hindus’ and ‘india’ in India. In Kenya, political entities (‘Ruto’ and ‘Raila’, names of two Kenyan politicians) as well as ethnic groups (e.g., ‘Kikuyus’, ‘Kalenjins’) are among the most frequent words, with ethnic groups appearing particularly prominently in the two forms of extreme speech that should be removed."
    }, {
      "heading" : "4 Experiments",
      "text" : "We establish XTREMESPEECH baselines for large pretrainedmodels and traditionalmachine learning models (details in Appendix F). As introduced in §1, we address three novel tasks: predicting the extremity of speech (EXTREMITY), whether a passage should be removed or not (REMOVAL) and the target of extreme speech (TARGET). Unless noted otherwise, our measure is microaveraged F1. We randomly split each country subset of XTREMESPEECH 80:10:10 into train:dev:test. In the tables below we are showing dev set results (test set results are shown in Appendix H). Note that in the German subset, there are only 16 dangerous passages, so results for dangerous are of limited utility. We used both multilingual (mBERT and XLM-R (Conneau et al., 2020)) and monolingual (langBERT) models. Each monolingual model was pretrained on the native language of the corresponding country; e.g., the Kenyan model was pretrained on Swahili. For finetuning and classification with BERT-based models, a task-\nspecific head is added which takes as input the [CLS] token representation.\n4.1 EXTREMITY Task Table 5 shows that baseline performance is rather low in three-way classification. In India and Kenya performance is acceptable; in Germany as well if we exclude the dangerous label, which only has 16 passages. In Brazil, however, where the predominant class is derogatory speech (>90%), performance is low, with no model managing to classify exclusionary speech.\nXLM-R performs relatively badly, only scoring competitively in the low-resource Kenyan set. langBERT is competitive for Brazil and Germany, less so for Kenya and performs badly for India. This can be explained by the divergence of pretraining and XTREMESPEECH texts: all langBERT models are pretrained on a single language. In the Brazilian and German sets there is primarily only one language used so langBERT performs better in those sets, while it performs worse in countries where English is more predominant both as a standalone language and in code switching. A major issue plaguing this area of research is that substantial world knowledge is required, especially around sociopolitical power dynamics, something thatmodels struggle to capture (Nguyen et al., 2021; Hovy and Yang, 2021).\n4.2 REMOVAL Task Table 6 shows that results are better for the binary task M (moderation) vs. R (removal). LSTMs perform well, in some instances competitively with transformers. XLM-R does not seem to compute good representations and performs poorly for all languages except for the low-resource Kenyan set. The monolingual langBERTmodels again perform well for Brazil and Germany; this time we see improvements for Kenya too.\n4.3 TARGET Task Table 7 shows that transformers are effective for the 8-way multilabel classification of target. In Table 10 and Table 3, we show top words according to prediction contribution and frequency. Words denoting ethnicity (‘kikuyu’), religion (‘hindu’, ‘Muslim’) and gender (‘puta’, ‘girls’) appear often and, not surprisingly, are reliable indicators of targeted groups, making this task easier than the other two."
    }, {
      "heading" : "4.4 Zero-Shot Cross-Country Classification",
      "text" : ""
    }, {
      "heading" : "4.4.1 All languages",
      "text" : "We evaluate mBERT on zero-shot cross-country transfer, i.e., training on one country and testing on the others (results shown in Table 8). Performance is in general poor, indicating that mBERT is not able to generalize from one country to another. Trained on Brazil, the model is unable to make any inferences on other countries. From Kenya to India, we see some transferability potential, with the model correctly identifying passages in all three classes (although at a non-competitively low rate). These results confirm our intuition that detecting extreme speech depends on social and cultural information, so zero-shot transfer, without access to specific information about the target country, is not a promising approach."
    }, {
      "heading" : "4.4.2 English",
      "text" : "We also investigate cross-country transfer of BERT, an English model. We only experiment with the two countries that have a nontrivial number of English passages, India and Kenya, restricting the datasets to their English part only. While cross-country performance is low for both countries, we see that Kenyaen→Kenyaen performance is high. We note that performance is better in Kenyaen→Kenyaen than in the previously examined Kenyaall→Kenyaall mixed language setup. This shows that for a single language within one country, BERT can indeed classify extreme speech with some accuracy."
    }, {
      "heading" : "4.5 Model Interpretability",
      "text" : "To shed light on predictions of mBERT in the EXTREMITY task (§4.1) we extract topcontributing words with LIME (Ribeiro et al., 2016). Specifically, we compute the words that contribute the most to mBERT’s predictions (alongside their weights) for each example and then average the weights, returning the top 10 words that appeared at least 5 times. This list is shown in Table 10. The Indian and German sets are dominated by\nreligious groups (‘Moslems’, ‘Muslims’). In India, ethnic terms (‘Rohingyas’) are also present while in Germany we see extreme speech targeting politicians (‘Merkel’). In Brazil we see politically divisive terms (‘Ucranizar’, a term originally meaning ‘Ukrainian Brazilian’ which has now been appropriated to depict opponents to the right-wing as ‘communists’) as well as insults like ‘traveco’ (for ‘cross-dresser’, used here as a slur). In Kenya, we see direct insults such as ‘idiot’ and ‘wajinga’ (meaning ‘foolish’), as well as ethnic group mentions such as ‘luo’ and ‘kikuyu’."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have presented XTREMESPEECH, an extreme speech dataset, containing 20,297 passages from Brazil, Germany, India and Kenya. We capture both granular levels of extremity and targets of extreme speech by engaging a team of annotators from within the affected communities. In a collaboration of anthropologists and computational linguists, we established a community-based framework, with the goal of curating data more representative of real-world harms. We introduce baselines for three novel tasks, including extreme speech and target group classification. We give experimental support for the intuition that extreme speech classification is dependent on cultural knowledge and that current NLP models do not capture this. Finally, we perform interpretability analysis on BERT’s predictions to reveal potential deficiencies, showing that models rely heavily on keywords and names of marginalized groups. We hope our community-driven work will contribute to the effective elimination of extreme speech against targeted groups, not just in Western democracies, but in a greater variety of countries worldwide."
    }, {
      "heading" : "6 Discussion and Ethical Considerations",
      "text" : ""
    }, {
      "heading" : "6.1 Ethics Statement",
      "text" : "The data provided here contains extreme speech that can be shocking and harmful. We present this dataset as a way to peel back the veil of extreme speech against under-represented communities around the world. We want to motivate, on one hand, the analysis of this overlooked area as a whole and on the other hand the investigation of the various levels of extreme speech (derogatory, exclusionary and dangerous) as found in online social media. This data is not intended and should not be used for pretraining models applied to real-world tasks, since a model pretrained on this data could potentially exhibit and propagate the extreme speech found in the passages we collected. Further, while we endeavored to include as many communities around the world as possible, the data we collected and the list of communities we included are of course non-exhaustive. For each community, we had a close circle of annotators, therefore it is possible other marginalized groups in these communities were not covered (although we made efforts to keep this to a plausible minimum). Finally, while including more communities is a worthwhile effort, countless communities had to be left out due to the daunting nature of the task."
    }, {
      "heading" : "6.2 Framework for Future Ethical Research",
      "text" : "Previous work in extreme speech research as well as groundwork on data collection ethics have proposed more inclusion of marginalized communities in projects that deal with their peoples and cultures. We further advocate for this direction. Discussions with people from the researched communities were pivotal in uncovering facets of extreme speech and its targets that were previously latent. When bridging NLP with anthropology fields it is vital to draw from theories already developed and established. For example, to define extreme speech we should take inputs from fields that have worked extensively on the subject to make sure our research is in line with a broader corpus of science. The addition of anthropologists to the team brings the necessary expertise to navigate this large body of work. Without this input, we risk working in an NLP ivory tower, cutting off communications from other fields pertinent to the task at hand."
    }, {
      "heading" : "6.3 Difficulties and Limitations",
      "text" : "Due to limitations of both time and budget, we only gathered extreme speech without negative passages (ie. neutral language). These neutral passages form the majority of content on social media (Founta et al., 2018; Sap et al., 2020). Despite the abundance of such passages, annotating them using our current scheme would be time and effort-consuming (our annotators collect data on their own, from their own networks, without us querying and supplying data to them). Thus, to keep control in the hands of annotators while at the same time keeping their workload to a reasonable minimum, we decided to only collect extreme speech passages. In future work, methods to solve this hole in the dataset could be explored. For example, pairing the neutral passages from existing hate speech datasets with the data we provide here could be investigated. For all countries but Kenya (especially in Swahili), this data is readily available (indicatively: English, Germany, India (Mandl et al., 2019), Brazil (de Pelle and Moreira, 2017)). For Kenya, even though there is a great percentage of English passages, English data from existing datasets should not be added unless it is related to Kenya. As shown in Table 4.4.2, English to English transfer does not work well across countries."
    }, {
      "heading" : "6.4 Global or Local Definitions?",
      "text" : "A question that came up during discussions with anthropologists and annotators was whether we should have an overall framework of definitions or whether a localized approach is more suitable. Having definitions for each country separately is appealing, since it would allow formore fine-tuned solutions. For example, in Germany we found that there was very little content that constituted as dangerous speech, while in the other countries it was much more prominent. Unfortunately, in reality local definitions are impractical with current means. Not only are countries and communities too numerous to accommodate, but separate frameworks mean joint research on the overarching problem would be impeded. We opted for a hybrid approach with most of our work under a unified framework while also allowing for country-specific intricacies (for example, the ‘large ethnic group’ that came up in the Kenyan reannotation phase). Nevertheless, this remains an open question."
    }, {
      "heading" : "A Data Statement",
      "text" : "CURATION RATIONAL In our project, we venture to present a dataset on extreme speech across different countries (Brazil, Germany, India and Kenya). Fact-checkers from these countries were employed both as data collectors and annotators. These fact-checkers scoured online platforms and fora that peddle extreme speech in their respective countries. The choice of sources was left to the fact-checkers, since they have intimate knowledge of the spread of extreme speech. Sources include social media (e.g., Twitter), fora (e.g., groups on Telegram) and direct messaging.\nLANGUAGE VARIETY Data was collected for Brazilian Portuguese (pt-BR), German (deDE), Hindi (hi-IN, either in the Devangari or Latin script), Swahili (sw-KE) and English used as a second language alongside these native languages.\nSPEAKER DEMOGRAPHIC Demographics on the speakers were not recorded (and in most cases they were anonymized in the first place). Data was collected from Brazil, Germany, India and Kenya, so a fair assumption is that speakers come from these countries.\nANNOTATOR DEMOGRAPHIC Annotators were accredited fact-checkers in their respective countries. There were 8 female and 5 male annotators (per country, female/male counts are 2/1 in Brazil, 4/0 in Germany, 2/2 in India and 0/2 in Kenya). They were native speakers of (Brazilian) Portuguese, German, Hindi and Swahili. Ages were not recorded. In terms of ethnicities, there were 3 Brazilians, 3 Germans, 4 Indians and 2 Kenyans.\nSPEECH SITUATION Speech consists entirely of text, posted and collected in 2020 and 2021. Text is mainly asynchronous, informal and spontaneous. Certain passages were posted as responses to other text (which was not collected) in a more synchronous manner. Lastly, by the nature of this project, all passages contain a certain level of extremity.\nTEXT CHARACTERISTICS Text comes from online media in the form of comments and interactions between users. Length was limited to approximately two paragraphs (at the discretion of the annotators).\nOTHER The team spanned multiple disciplines, ages and ethnicities."
    }, {
      "heading" : "B Annotator Background",
      "text" : "Our annotators are fact-checkers who are also the targets of online extreme speech themselves, since their public role of verification in defense of vulnerable communities has led to them being targeted by dominant and aggressive actors. These comments vary in content, from veiled criticism to bigoted rhetoric. Therefore, these comments that fact-checkers receive could be seen as a proxy to extreme speech aimed not only at them as a professional group but also at vulnerable local communities whom they implicitly stand for. For example, a Kenyan fact-checker receives insults not only because of their identity as a fact-checker, but also because of other, community-based features, such as ethnicity, age and gender, among others. By involving fact checkers, we draw upon the professional competence of a relatively independent group of experts who are confronted with extreme speech both as part of the data they sieve for disinformation and as targets of extreme speech. As per the logistics of annotation, there are at least two annotators from each country. In some countries, we worked with fact-checker teams which themselves employ multiple fact-checkers. In these instances, annotation work was split according to the requirements and resources of the particular team. We ensured that all involvedmembers were accredited fact-checkers and were interviewed to verify they have been exposed to extreme speech and are capable of identifying it."
    }, {
      "heading" : "C Data Analysis",
      "text" : "C.1 Institutions of Power\nStatistics of institutions of power are shown in Table 15. These groups can only be the target of derogatory speech, since they are in power and we want to avoid censoring of speech aimed at these groups. Across all countries, we see that politicians are the predominant targets.\nC.2 Average Passage Length\nIn Table 11we show the average length of each passage per label for every country. Overall, all countries show similar lengths, except Brazil where passages are overall shorter. Also, the more extreme speech is, the longer the passage is on average. This trend is followed in all countries."
    }, {
      "heading" : "D Annotation",
      "text" : "In Table 12 we show the number of passages cross-annotated by each involved party and in Table 14 we show inter-annotator agreement scores per country. While Germany and Kenya have acceptable scores, the other two countries have low scores. In the case of Brazil this could partly be explained by the imbalance in the data. In Figure 2 we see the interface annotators used to enter data."
    }, {
      "heading" : "E Reannotation",
      "text" : "After discussion with the annotators from Kenya, we found that there was disagreement surrounding two ethnic groups and the power dynamics around them. Namely, the Kikuyu and Kalenjin are two ethnic groups currently in power in Kenya. They make up around 17% (largest group) and 13% (third largest group) of Kenyan population respectively. Because of their position of power, in a lot of sociopolitical issues these two ethnic groups (either jointly or individually) get pitted against the rest of the population. So, in that binary perspective (e.g., Kikuyus vs. “others”), the ethnic group in power is technically a minority. Further, these ethnic groups were labeled as indigenous groups by the other annotator. After a conversation between the annotators with anthropologists acting as intermediaries, a consensus was reached that the ethnic groups in power will be labeled neither as ethnic minorities nor as indigenous groups, but as a new target label: “large ethnic groups”. This also entails that re-labeling of the extremity of these passages should take place."
    }, {
      "heading" : "F Model Details",
      "text" : "Transformer models were finetuned for 3 epochs, LSTMs for 5 and SVMs until convergence. Amaximum length of 128 was used universally. The BERT-based models we used are:3\n1. bert-base-multilingual-cased: https://huggingface.co/ bert-base-multilingual-cased\n2. bert-base-portuguese-cased: https://huggingface.co/neuralmind/ bert-base-portuguese-cased\n3. bert-base-german-cased: https://huggingface.co/ bert-base-german-cased\n4. hindi-bert: https://huggingface.co/ monsoon-nlp/hindi-bert\n5. bert-base-uncased-swahili: https: //huggingface.co/flax-community/ bert-base-uncased-swahili"
    }, {
      "heading" : "G Combined Multilingual Setting",
      "text" : "Weperform an ablation study by combining all sets across countries and repeating our mBERT experiments in this new multilingual task (Table 13). Even though the use of a “catch-all” model that is able to work on all languages sounds enticing, care should be taken to ensure that the model has sufficient understanding for each language and culture instead of making predictions based on dubious statistical cues (McCoy et al., 2019). This is a task out of scope for this work, but we are adding such a model to our baselines for comparison and completion."
    }, {
      "heading" : "H Test Set Results",
      "text" : "In Table 16 are shown the extreme speech label classification results on the test set, in Table 17 the M vs. R results and in Table 18 the extreme speech target results.\n3https://huggingface.co/models"
    } ],
    "references" : [ {
      "title" : "Identifying and measuring annotator bias based on annotators’ demographic characteristics",
      "author" : [ "Hala Al Kuwatly", "Maximilian Wich", "Georg Groh." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 184–190, Online. Associa-",
      "citeRegEx" : "Kuwatly et al\\.,? 2020",
      "shortCiteRegEx" : "Kuwatly et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking cyberhate laws",
      "author" : [ "Chara Bakalis." ],
      "venue" : "Information & Communications Technology Law, 27(1):86–110.",
      "citeRegEx" : "Bakalis.,? 2018",
      "shortCiteRegEx" : "Bakalis.",
      "year" : 2018
    }, {
      "title" : "A unified taxonomy of harmful content",
      "author" : [ "Michele Banko", "Brendon MacKeen", "Laurie Ray." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 125–137, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Banko et al\\.,? 2020",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2020
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Dangerous speech: A practical guide",
      "author" : [ "Susan Benesch" ],
      "venue" : null,
      "citeRegEx" : "Benesch.,? \\Q2018\\E",
      "shortCiteRegEx" : "Benesch.",
      "year" : 2018
    }, {
      "title" : "Freedom of expression versus racist hate speech: Explaining differences between high court regulations in the usa and europe",
      "author" : [ "Erik Bleich." ],
      "venue" : "Journal of Ethnic and Migration Studies, 40(2):283–300.",
      "citeRegEx" : "Bleich.,? 2014",
      "shortCiteRegEx" : "Bleich.",
      "year" : 2014
    }, {
      "title" : "Speaking Hatefully: Culture, Communication, and Political Action in Hungary",
      "author" : [ "David Boromisza-Habashi." ],
      "venue" : "Pennsylvania State University Press.",
      "citeRegEx" : "Boromisza.Habashi.,? 2013",
      "shortCiteRegEx" : "Boromisza.Habashi.",
      "year" : 2013
    }, {
      "title" : "Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology",
      "author" : [ "Domenic V. Cicchetti." ],
      "venue" : "Psychological Assessment, 6:284–290.",
      "citeRegEx" : "Cicchetti.,? 1994",
      "shortCiteRegEx" : "Cicchetti.",
      "year" : 1994
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Pro-",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Racial bias in hate speech and abusive language detection datasets",
      "author" : [ "Thomas Davidson", "Debasmita Bhattacharya", "Ingmar Weber." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 25–35, Florence, Italy. Association for Com-",
      "citeRegEx" : "Davidson et al\\.,? 2019a",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2019
    }, {
      "title" : "Racial bias in hate speech and abusive language detection datasets",
      "author" : [ "Thomas Davidson", "Debasmita Bhattacharya", "Ingmar Weber." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 25–35, Florence, Italy. Association for Com-",
      "citeRegEx" : "Davidson et al\\.,? 2019b",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber." ],
      "venue" : "International AAAI Conference on Web and Social Media.",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "Offensive comments in the brazilian web: a dataset and baseline results",
      "author" : [ "Rogers de Pelle", "Viviane Moreira." ],
      "venue" : "Anais do VI Brazilian Workshop on Social Network Analysis and Mining, Porto Alegre, RS, Brasil. SBC.",
      "citeRegEx" : "Pelle and Moreira.,? 2017",
      "shortCiteRegEx" : "Pelle and Moreira.",
      "year" : 2017
    }, {
      "title" : "Stop the presses? moving from strategic silence to strategic amplification in a networked media ecosystem",
      "author" : [ "Joan Donovan", "danah boyd." ],
      "venue" : "American Behavioral Scientist, 65(2):333–350.",
      "citeRegEx" : "Donovan and boyd.,? 2021",
      "shortCiteRegEx" : "Donovan and boyd.",
      "year" : 2021
    }, {
      "title" : "Large scale crowdsourcing and characterization of twitter abu",
      "author" : [ "Antigoni-Maria Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "andNicolasKourtellis" ],
      "venue" : null,
      "citeRegEx" : "Founta et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta et al\\.",
      "year" : 2018
    }, {
      "title" : "Hate towards the political opponent: A Twitter corpus study of the 2020 US elections on the basis of offensive speech and stance detection",
      "author" : [ "Lara Grimminger", "Roman Klinger." ],
      "venue" : "Proceedings of the Eleventh Workshop on Computational Approaches",
      "citeRegEx" : "Grimminger and Klinger.,? 2021",
      "shortCiteRegEx" : "Grimminger and Klinger.",
      "year" : 2021
    }, {
      "title" : "An expert annotated dataset for the detection of online misogyny",
      "author" : [ "Ella Guest", "Bertie Vidgen", "AlexandrosMittos", "Nishanth Sastry", "Gareth Tyson", "Helen Margetts." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Com-",
      "citeRegEx" : "Guest et al\\.,? 2021",
      "shortCiteRegEx" : "Guest et al\\.",
      "year" : 2021
    }, {
      "title" : "Diverse adversaries for mitigating bias in training",
      "author" : [ "Xudong Han", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2760–",
      "citeRegEx" : "Han et al\\.,? 2021",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2021
    }, {
      "title" : "Writing on the walls: Discourses on bolivian immigrants in chilean meme humor",
      "author" : [ "Nell Haynes." ],
      "venue" : "International Journal of Communication, 13(0).",
      "citeRegEx" : "Haynes.,? 2019",
      "shortCiteRegEx" : "Haynes.",
      "year" : 2019
    }, {
      "title" : "From toxicity in online comments to incivility in American news: Proceed with caution",
      "author" : [ "Anushree Hede", "Oshin Agarwal", "Linda Lu", "Diana C. Mutz", "Ani Nenkova." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association",
      "citeRegEx" : "Hede et al\\.,? 2021",
      "shortCiteRegEx" : "Hede et al\\.",
      "year" : 2021
    }, {
      "title" : "Ritualized opposition in danish practices of extremist language and thought",
      "author" : [ "Peter Hervik." ],
      "venue" : "International Journal of Communication, 13(0).",
      "citeRegEx" : "Hervik.,? 2019",
      "shortCiteRegEx" : "Hervik.",
      "year" : 2019
    }, {
      "title" : "The importance of modeling social factors of language: Theory and practice",
      "author" : [ "Dirk Hovy", "Diyi Yang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hovy and Yang.,? 2021",
      "shortCiteRegEx" : "Hovy and Yang.",
      "year" : 2021
    }, {
      "title" : "Lessons from archives: Strategies for collecting sociocultural data in machine learning",
      "author" : [ "Eun Seo Jo", "Timnit Gebru." ],
      "venue" : "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* ’20, page 306–316, New York, NY,",
      "citeRegEx" : "Jo and Gebru.,? 2020",
      "shortCiteRegEx" : "Jo and Gebru.",
      "year" : 2020
    }, {
      "title" : "Intersectional bias in hate speech and abusive language datasets",
      "author" : [ "Jae-Yeon Kim", "Carlos Ortiz", "Sarah Nam", "Sarah Santiago", "Vivek Datta." ],
      "venue" : "CoRR, abs/2005.05921.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Computing krippendorff’s alpha-reliability",
      "author" : [ "Klaus Krippendorff" ],
      "venue" : null,
      "citeRegEx" : "Krippendorff.,? \\Q2011\\E",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2011
    }, {
      "title" : "The datafication of hate: Expectations and challenges in automated hate speech monitoring",
      "author" : [ "Salla-Maaria Laaksonen", "Jesse Haapoja", "Teemu Kinnunen", "Matti Nelimarkka", "Reeta Pöyhtäri." ],
      "venue" : "Frontiers in Big Data, 3:3.",
      "citeRegEx" : "Laaksonen et al\\.,? 2020",
      "shortCiteRegEx" : "Laaksonen et al\\.",
      "year" : 2020
    }, {
      "title" : "Civil rephrases of toxic texts with self-supervised transformers",
      "author" : [ "Léo Laugier", "John Pavlopoulos", "Jeffrey Sorensen", "Lucas Dixon." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
      "citeRegEx" : "Laugier et al\\.,? 2021",
      "shortCiteRegEx" : "Laugier et al\\.",
      "year" : 2021
    }, {
      "title" : "Give me convenience and give her death: Who should decide what uses of NLP are appropriate, and on what basis",
      "author" : [ "Kobi Leins", "Jey Han Lau", "Timothy Baldwin" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Leins et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Leins et al\\.",
      "year" : 2020
    }, {
      "title" : "In data we trust: A critical analysis of hate speech detection datasets",
      "author" : [ "Kosisochukwu Madukwe", "Xiaoying Gao", "Bing Xue." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 150–161, Online. Association for Computa-",
      "citeRegEx" : "Madukwe et al\\.,? 2020",
      "shortCiteRegEx" : "Madukwe et al\\.",
      "year" : 2020
    }, {
      "title" : "Overview of the hasoc track at fire 2019: Hate speech and offensive content identification in indo-european languages",
      "author" : [ "Thomas Mandl", "Sandip Modha", "Prasenjit Majumder", "Daksh Patel", "Mohana Dave", "Chintak Mandlia", "Aditya Patel." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Mandl et al\\.,? 2019",
      "shortCiteRegEx" : "Mandl et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating an approach for low resource language dataset creation, curation",
      "author" : [ "Vukosi Marivate", "Tshephisho Sefara", "Vongani Chabalala", "KeamogetsweMakhaya", "TumishoMokgonyane", "Rethabile Mokoena", "Abiodun Modupe" ],
      "venue" : null,
      "citeRegEx" : "Marivate et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Marivate et al\\.",
      "year" : 2020
    }, {
      "title" : "Hatexplain: A benchmark dataset for explainable hate speech detection",
      "author" : [ "Binny Mathew", "Punyajoy Saha", "Seid Muhie Yimam", "Chris Biemann", "PawanGoyal", "andAnimeshMukherjee" ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Mathew et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2021
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Interrater reliability: the kappa statistic",
      "author" : [ "Mary L. McHugh." ],
      "venue" : "Biochemia medica, 22:276–82.",
      "citeRegEx" : "McHugh.,? 2012",
      "shortCiteRegEx" : "McHugh.",
      "year" : 2012
    }, {
      "title" : "Diversity and inclusion metrics in subset selection",
      "author" : [ "Margaret Mitchell", "Dylan Baker", "Nyalleng Moorosi", "Emily Denton", "Ben Hutchinson", "Alex Hanna", "Timnit Gebru", "Jamie Morgenstern." ],
      "venue" : "Proceedings of the AAAI/ACM Conference on AI, Ethics, and So-",
      "citeRegEx" : "Mitchell et al\\.,? 2020",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2020
    }, {
      "title" : "Decolonial ai: Decolonial theory as sociotechnical foresight in artificial intelligence",
      "author" : [ "Shakir Mohamed", "Marie-Therese Png", "William Isaac." ],
      "venue" : "Philosophy and Technology, 33(4):659–684.",
      "citeRegEx" : "Mohamed et al\\.,? 2020",
      "shortCiteRegEx" : "Mohamed et al\\.",
      "year" : 2020
    }, {
      "title" : "Participatory research for low-resourced machine translation: A",
      "author" : [ "Chris Chinenye Emezue", "Bonaventure F.P. Dossou", "Blessing Sibanda", "Blessing Bassey", "Ayodele Olabiyi", "Arshath Ramkilowan", "Alp Öktem", "Adewale Akinfaderin", "Abdallah Bashir" ],
      "venue" : null,
      "citeRegEx" : "Emezue et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Emezue et al\\.",
      "year" : 2020
    }, {
      "title" : "On learning and representing social meaning in NLP: a sociolinguistic perspective",
      "author" : [ "Dong Nguyen", "Laura Rosseel", "Jack Grieve." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Nguyen et al\\.,? 2021",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2021
    }, {
      "title" : "Exposing the limits of zeroshot cross-lingual hate speech detection",
      "author" : [ "Debora Nozza." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Nozza.,? 2021",
      "shortCiteRegEx" : "Nozza.",
      "year" : 2021
    }, {
      "title" : "Multilingual and multi-aspect hate speech analysis",
      "author" : [ "Nedjma Ousidhoum", "Zizheng Lin", "Hongming Zhang", "Yangqiu Song", "Dit-Yan Yeung." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ousidhoum et al\\.,? 2019",
      "shortCiteRegEx" : "Ousidhoum et al\\.",
      "year" : 2019
    }, {
      "title" : "Characterizing the global crowd workforce: A cross-country comparison of crowdworker demographics",
      "author" : [ "Lisa Posch", "Arnim Bleier", "Fabian Flöck", "Markus Strohmaier" ],
      "venue" : null,
      "citeRegEx" : "Posch et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Posch et al\\.",
      "year" : 2018
    }, {
      "title" : "Six attributes of unhealthy conversations",
      "author" : [ "Ilan Price", "Jordan Gifford-Moore", "Jory Flemming", "Saul Musker", "Maayan Roichman", "Guillaume Sylvain", "Nithum Thain", "Lucas Dixon", "Jeffrey Sorensen." ],
      "venue" : "Proceedings of the Fourth Workshop on Online",
      "citeRegEx" : "Price et al\\.,? 2020",
      "shortCiteRegEx" : "Price et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual offensive language identification with cross-lingual embeddings",
      "author" : [ "Tharindu Ranasinghe", "Marcos Zampieri." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5838–5844,",
      "citeRegEx" : "Ranasinghe and Zampieri.,? 2020",
      "shortCiteRegEx" : "Ranasinghe and Zampieri.",
      "year" : 2020
    }, {
      "title" : "why should i trust you?”: Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Deplatforming: Following extreme internet celebrities to telegram and alternative social media",
      "author" : [ "Richard Rogers." ],
      "venue" : "European Journal of Communication, 35(3):213–229.",
      "citeRegEx" : "Rogers.,? 2020",
      "shortCiteRegEx" : "Rogers.",
      "year" : 2020
    }, {
      "title" : "Measuring the reliability of hate speech annotations: The case of the european refugee crisis",
      "author" : [ "Björn Ross", "Michael Rist", "Guillermo Carbonell", "Benjamin Cabrera", "Nils Kurowsky", "Michael Wojatzki." ],
      "venue" : "CoRR, abs/1701.08118.",
      "citeRegEx" : "Ross et al\\.,? 2017",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2017
    }, {
      "title" : "HateCheck: Functional tests for hate speech detection models",
      "author" : [ "Paul Röttger", "Bertie Vidgen", "Dong Nguyen", "Zeerak Waseem", "Helen Margetts", "Janet Pierrehumbert." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Röttger et al\\.,? 2021",
      "shortCiteRegEx" : "Röttger et al\\.",
      "year" : 2021
    }, {
      "title" : "White paper – the role of prevent in countering online extremism",
      "author" : [ "Erin Marie Saltman", "Jonathan Russell" ],
      "venue" : null,
      "citeRegEx" : "Saltman and Russell.,? \\Q2014\\E",
      "shortCiteRegEx" : "Saltman and Russell.",
      "year" : 2014
    }, {
      "title" : "The risk of racial bias in hate speech detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668–1678, Florence, Italy. Asso-",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond fair pay: Ethical implications of NLP crowdsourcing",
      "author" : [ "Boaz Shmueli", "Jan Fell", "Soumya Ray", "Lun-Wei Ku." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Shmueli et al\\.,? 2021",
      "shortCiteRegEx" : "Shmueli et al\\.",
      "year" : 2021
    }, {
      "title" : "Studying generalisability across abusive language detection datasets",
      "author" : [ "Steve Durairaj Swamy", "Anupam Jamatia", "Björn Gambäck." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 940–950,",
      "citeRegEx" : "Swamy et al\\.,? 2019",
      "shortCiteRegEx" : "Swamy et al\\.",
      "year" : 2019
    }, {
      "title" : "Digital technology and extreme speech: Approaches to counter online hate",
      "author" : [ "Sahana Udupa" ],
      "venue" : null,
      "citeRegEx" : "Udupa.,? \\Q2021\\E",
      "shortCiteRegEx" : "Udupa.",
      "year" : 2021
    }, {
      "title" : "Artificial intelligence, extreme speech and the challenges of online content moderation",
      "author" : [ "Sahana Udupa", "Elonnai Hickok", "Antonis Maronikolakis", "Hinrich Schuetze", "Laura Csuka", "Axel Wisiorek", "Leah Nann" ],
      "venue" : null,
      "citeRegEx" : "Udupa et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Udupa et al\\.",
      "year" : 2021
    }, {
      "title" : "Extreme speech and global digital cultures",
      "author" : [ "Sahana Udupa", "Matti Pohjonen." ],
      "venue" : "International Journal of Communication, 13(0).",
      "citeRegEx" : "Udupa and Pohjonen.,? 2019",
      "shortCiteRegEx" : "Udupa and Pohjonen.",
      "year" : 2019
    }, {
      "title" : "Challenges and frontiers in abusive content detection",
      "author" : [ "Bertie Vidgen", "Alex Harris", "Dong Nguyen", "Rebekah Tromble", "Scott Hale", "Helen Margetts." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 80–93, Florence, Italy. As-",
      "citeRegEx" : "Vidgen et al\\.,? 2019",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2019
    }, {
      "title" : "Are you a racist or am I seeing things? annotator influence on hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 138– 142, Austin, Texas. Association for Computational",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Understanding abuse: A typology of abusive language detection subtasks",
      "author" : [ "Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 78–84, Vancouver, BC, Canada.",
      "citeRegEx" : "Waseem et al\\.,? 2017",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2017
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88– 93, San Diego, California. Association for Computa-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Implicitly abusive language – what does it actually look like andwhy are we not getting there",
      "author" : [ "Michael Wiegand", "Josef Ruppenhofer", "Elisabeth Eder" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Wiegand et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2021
    }, {
      "title" : "Detection of Abusive Language: the Problem of Biased Datasets",
      "author" : [ "Michael Wiegand", "Josef Ruppenhofer", "Thomas Kleinbauer." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Wiegand et al\\.,? 2019",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2019
    }, {
      "title" : "Demoting racial bias in hate speech detection",
      "author" : [ "Mengzhou Xia", "Anjalie Field", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, pages 7–14, Online. Association for Computational",
      "citeRegEx" : "Xia et al\\.,? 2020",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2020
    }, {
      "title" : "Challenges in automated debiasing for toxic language detection",
      "author" : [ "Xuhui Zhou", "Maarten Sap", "Swabha Swayamdipta", "Yejin Choi", "Noah Smith." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Transfer learning for low-resource neural machine translation",
      "author" : [ "Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1568–1575, Austin,",
      "citeRegEx" : "Zoph et al\\.,? 2016",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 58,
      "context" : "the area of hate speech, from foundational work (Waseem and Hovy, 2016; Davidson et al., 2017) to more recent, broader (Sap et al.",
      "startOffset" : 48,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "the area of hate speech, from foundational work (Waseem and Hovy, 2016; Davidson et al., 2017) to more recent, broader (Sap et al.",
      "startOffset" : 48,
      "endOffset" : 94
    }, {
      "referenceID" : 49,
      "context" : ", 2017) to more recent, broader (Sap et al., 2020) as well as multilingual (Ousidhoum et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 39,
      "context" : ", 2020) as well as multilingual (Ousidhoum et al., 2019) approaches.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 48,
      "context" : ", disproportionately labeling African American English as hateful (Sap et al., 2019; Davidson et al., 2019a)) and to collection of data that is not representative of the comments directed at target groups; e.",
      "startOffset" : 66,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : ", disproportionately labeling African American English as hateful (Sap et al., 2019; Davidson et al., 2019a)) and to collection of data that is not representative of the comments directed at target groups; e.",
      "startOffset" : 66,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "It has been argued that the NLP community does not sufficiently engage in interdisciplinary work with other fields that address important aspects of hate speech (Jo and Gebru, 2020).",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 54,
      "context" : "1Classifying and Identifying the Intensity of Hate Speech the boundaries of civil language (Udupa and Pohjonen, 2019; Udupa et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 137
    }, {
      "referenceID" : 53,
      "context" : "1Classifying and Identifying the Intensity of Hate Speech the boundaries of civil language (Udupa and Pohjonen, 2019; Udupa et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : ", extreme speech) remains the Achilles’ heel of current NLP models (Davidson et al., 2017).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 43,
      "context" : "Finally, we perform a model interpretability analysis with LIME (Ribeiro et al., 2016) to uncover potential model biases and deficiencies.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 58,
      "context" : "Earlier work in hate speech detection focused on data collection, curation and annotation frameworks (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018).",
      "startOffset" : 101,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "Earlier work in hate speech detection focused on data collection, curation and annotation frameworks (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018).",
      "startOffset" : 101,
      "endOffset" : 168
    }, {
      "referenceID" : 14,
      "context" : "Earlier work in hate speech detection focused on data collection, curation and annotation frameworks (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018).",
      "startOffset" : 101,
      "endOffset" : 168
    }, {
      "referenceID" : 49,
      "context" : "Recent work has expanded the set of captured labels to include more pertinent information such as targets and other forms of abuse (Sap et al., 2020; Hede et al., 2021; Guest et al., 2021; Grimminger and Klinger, 2021; Ross et al., 2017) as well as benchmarking (Röttger et al.",
      "startOffset" : 131,
      "endOffset" : 237
    }, {
      "referenceID" : 19,
      "context" : "Recent work has expanded the set of captured labels to include more pertinent information such as targets and other forms of abuse (Sap et al., 2020; Hede et al., 2021; Guest et al., 2021; Grimminger and Klinger, 2021; Ross et al., 2017) as well as benchmarking (Röttger et al.",
      "startOffset" : 131,
      "endOffset" : 237
    }, {
      "referenceID" : 16,
      "context" : "Recent work has expanded the set of captured labels to include more pertinent information such as targets and other forms of abuse (Sap et al., 2020; Hede et al., 2021; Guest et al., 2021; Grimminger and Klinger, 2021; Ross et al., 2017) as well as benchmarking (Röttger et al.",
      "startOffset" : 131,
      "endOffset" : 237
    }, {
      "referenceID" : 15,
      "context" : "Recent work has expanded the set of captured labels to include more pertinent information such as targets and other forms of abuse (Sap et al., 2020; Hede et al., 2021; Guest et al., 2021; Grimminger and Klinger, 2021; Ross et al., 2017) as well as benchmarking (Röttger et al.",
      "startOffset" : 131,
      "endOffset" : 237
    }, {
      "referenceID" : 45,
      "context" : "Recent work has expanded the set of captured labels to include more pertinent information such as targets and other forms of abuse (Sap et al., 2020; Hede et al., 2021; Guest et al., 2021; Grimminger and Klinger, 2021; Ross et al., 2017) as well as benchmarking (Röttger et al.",
      "startOffset" : 131,
      "endOffset" : 237
    }, {
      "referenceID" : 56,
      "context" : "Research has also been conducted to investigate annotation bias and annotator pools (Al Kuwatly et al., 2020; Waseem, 2016; Ross et al., 2017; Shmueli et al., 2021; Posch et al., 2018), as well as bias (especially racial) in existing datasets (Davidson et al.",
      "startOffset" : 84,
      "endOffset" : 184
    }, {
      "referenceID" : 45,
      "context" : "Research has also been conducted to investigate annotation bias and annotator pools (Al Kuwatly et al., 2020; Waseem, 2016; Ross et al., 2017; Shmueli et al., 2021; Posch et al., 2018), as well as bias (especially racial) in existing datasets (Davidson et al.",
      "startOffset" : 84,
      "endOffset" : 184
    }, {
      "referenceID" : 50,
      "context" : "Research has also been conducted to investigate annotation bias and annotator pools (Al Kuwatly et al., 2020; Waseem, 2016; Ross et al., 2017; Shmueli et al., 2021; Posch et al., 2018), as well as bias (especially racial) in existing datasets (Davidson et al.",
      "startOffset" : 84,
      "endOffset" : 184
    }, {
      "referenceID" : 40,
      "context" : "Research has also been conducted to investigate annotation bias and annotator pools (Al Kuwatly et al., 2020; Waseem, 2016; Ross et al., 2017; Shmueli et al., 2021; Posch et al., 2018), as well as bias (especially racial) in existing datasets (Davidson et al.",
      "startOffset" : 84,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : ", 2018), as well as bias (especially racial) in existing datasets (Davidson et al., 2019b; Laugier et al., 2021).",
      "startOffset" : 66,
      "endOffset" : 112
    }, {
      "referenceID" : 26,
      "context" : ", 2018), as well as bias (especially racial) in existing datasets (Davidson et al., 2019b; Laugier et al., 2021).",
      "startOffset" : 66,
      "endOffset" : 112
    }, {
      "referenceID" : 48,
      "context" : "Methods for bias mitigation have also been proposed (Sap et al., 2019; Xia et al., 2020; Han et al., 2021; Zhou et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 125
    }, {
      "referenceID" : 61,
      "context" : "Methods for bias mitigation have also been proposed (Sap et al., 2019; Xia et al., 2020; Han et al., 2021; Zhou et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "Methods for bias mitigation have also been proposed (Sap et al., 2019; Xia et al., 2020; Han et al., 2021; Zhou et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 125
    }, {
      "referenceID" : 62,
      "context" : "Methods for bias mitigation have also been proposed (Sap et al., 2019; Xia et al., 2020; Han et al., 2021; Zhou et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "In another line of work, theoretical foundations are being established, in the form of taxonomies (Banko et al., 2020), definitions (Wiegand et al.",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 59,
      "context" : ", 2020), definitions (Wiegand et al., 2021; Waseem et al., 2017) and hate speech theory (Price et al.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 57,
      "context" : ", 2020), definitions (Wiegand et al., 2021; Waseem et al., 2017) and hate speech theory (Price et al.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 44,
      "context" : "While forms of extreme speech may originate from one place, dissemination to other platforms is swift (Rogers, 2020).",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "It has been shown in previous work that while neutral text is easy to detect (Davidson et al., 2017),",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "We used both multilingual (mBERT and XLM-R (Conneau et al., 2020)) and monolingual (langBERT) models.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 37,
      "context" : "A major issue plaguing this area of research is that substantial world knowledge is required, especially around sociopolitical power dynamics, something thatmodels struggle to capture (Nguyen et al., 2021; Hovy and Yang, 2021).",
      "startOffset" : 184,
      "endOffset" : 226
    }, {
      "referenceID" : 21,
      "context" : "A major issue plaguing this area of research is that substantial world knowledge is required, especially around sociopolitical power dynamics, something thatmodels struggle to capture (Nguyen et al., 2021; Hovy and Yang, 2021).",
      "startOffset" : 184,
      "endOffset" : 226
    }, {
      "referenceID" : 43,
      "context" : "1) we extract topcontributing words with LIME (Ribeiro et al., 2016).",
      "startOffset" : 46,
      "endOffset" : 68
    } ],
    "year" : 0,
    "abstractText" : "Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data – as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT’s predictions.",
    "creator" : null
  }
}
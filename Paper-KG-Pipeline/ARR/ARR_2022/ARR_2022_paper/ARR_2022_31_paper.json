{
  "name" : "ARR_2022_31_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Analysis of Negation in Natural Language Understanding Corpora",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Natural language understanding (NLU) is an umbrella term used to refer to any task that requires text understanding. For example, question answering (Rajpurkar et al., 2016), information extraction (Stanovsky et al., 2018), coreference resolution (Wu et al., 2020), and machine reading (Yang et al., 2019), among many others, are tasks that fall under natural language understanding. The threshold for claiming that a system understands natural language is ever-moving. New corpora are often justified by pointing out that state-of-the-art models do not obtain good results. After years of steady improvements, more powerful models eventually obtain so-called human performance, and at that point new, more challenging corpora are created.\nMany corpora for natural language understanding tasks contain language generated by annotators rather than retrieved from texts created independently of the corpus creation process. These corpora are certainly useful and have facilitated tremendous progress. Annotator-generated examples, however, carry the risk of evaluating systems with synthetic language that is not representative of language in the wild. For example, annotators are\nlikely to use negation when asked to write a text that contradicts something despite contradictions in the wild need not have a negation (Gururangan et al., 2018). Recently, Kwiatkowski et al. (2019) present a large corpus for question answering that consists of natural questions (i.e., asked by somebody with a real information need) in order to encourage research in a more realistic scenario. This contrasts with previous corpora, where the questions were written by annotators after being told the answer (Rajpurkar et al., 2016).\nIn this paper, we explore the role of negation in eight corpora for six popular natural language understanding tasks. Our goal is to check whether negation plays the role it deserves in these tasks. To our surprise, we conclude that negation is virtually ignored by answering the following questions:1\n1. Do NLU corpora contain as many negations as general-purpose texts? (they don’t); 2. Do the (few) negations in NLU corpora play a role in solving the tasks? (they don’t); and 3. Do state-of-the-art transformers trained with NLU corpora face challenges with instances that contain negation? (they do, especially if the negation is important)."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "We work with the eight corpora covering six tasks summarized below and exemplified in Table 2.\nWe select two corpora for question answering: CommonsenseQA (Talmor et al., 2019) and COPA (Roemmele et al., 2011). CommonsenseQA consists of multi-choice questions (5 candidate answers) that require some degree of commonsense. COPA presents a premise (e.g., The man broke his toe) and a question (e.g., What was the cause of this?) and the system must choose between two plausible alternatives (e.g. He got a hole in his sock or He dropped a hammer on his foot).\n1Code and analysis available at anonymous_GitHub_link\nFor textual similarity and paraphrasing, we select QQP2 and STS-B (Cer et al., 2017). QQP consists of pairs of questions and the task is to determine whether they are paraphrases. STS-B consists of pairs of texts and the task is to determine how semantically similar they are with a score from 0 to 5.\nWe select one corpus for the remaining tasks. For inference, we work with QNLI (Rajpurkar et al., 2016), which consists in determining whether a text is a valid answer to a question. We use WiC (Pilehvar and Camacho-Collados, 2019) for word sense disambiguation. WiC consists in determining whether two instances of the same word (in two sentences; italicized in Table 2) are used with the same meaning. For coreference resolution, we choose WSC (Levesque et al., 2012), which consists in determining whether a pronoun and a noun phrase are co-referential (italicized in Table 2). Finally, we work with SST-2 (Socher et al., 2013) for sentiment analysis. The task consists in determining whether a sentence from a collection of movie reviews has positive or negative sentiment.\nFor convenience, we work with the formatted versions of these corpora in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks. The only exception is CommonsenseQA, which is not part of these benchmarks. Related Work Previous work has shown that SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) have annotation artifacts (e.g., negation is a strong indicator of contradictions) (Gururangan et al., 2018). The literature has also shown that simple adversarial attacks including negation cues are very effective (Naik et al., 2018; Wallace et al., 2019). Kovatchev et al. (2019) analyze 11 paraphrasing systems and show that they obtain substantially worse results when negation is present.\nMore recently, Ribeiro et al. (2020) show that negation is one of the linguistic phenomena commercial sentiment analysis struggle with. Several previous works have investigated the (lack of) ability of transformers to make inferences when negation is present. For example, Ettinger (2020) conclude that BERT is unable to complete sentences when negation is present. BERT also faces challenges solving the task of natural language inference (i.e., identifying entailments and contradictions) with monotonicity and negation (Geiger et al., 2020; Yanaka et al., 2019). Warstadt et al.\n2https://www.quora.com/q/quoradata/First-QuoraDataset-Release-Question-Pairs\n(2019) show the limitations of BERT making acceptability judgments with sentences that contain negative polarity items. Most related to out work, Hossain et al. (2020) analyze the role of negation in three natural language inference corpora: RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), SNLI and MNLI. In this paper, we present a similar analysis, but we move beyond natural language inference and work with eight corpora spanning six natural language understanding tasks."
    }, {
      "heading" : "3 Research Questions and Analysis",
      "text" : "Q1: Do natural language understanding corpora contain as many negations as generalpurpose English texts? In order to automatically identify negation cues, we train a negation cue detector with the largest corpus available, ConanDoyle-neg (Morante and Daelemans, 2012). The cue detector is based on the RoBERTa pretrained language model (Liu et al., 2019); we provide details about the architecture and training process in the supplementary materials. Our cue detector obtains the best results to date: F1: 93.79 vs. 92.94 (Khandelwal and Sawant, 2020). ConanDoyle-neg (and thus our cue detector) identifies common negation cues such as no and not, affixal negation cues such as impossible and careless, and lexical negations such as deny.\nTable 1 presents the percentage of sentences that contain negation in (a) the eight corpora we work with and (b) general-purpose English. We take the latter percentage (all sentences) from Hossain et al. (2020), who run a negation cue detector in online reviews, conversations, and books. Additionally, we also present the percentages in questions. Negation is much less common in all natural language understanding corpora but WSC (0.8%–16%) than in general-purpose English (22.6%–29.9%), Note that negation is also underrepresented in corpora that primarily contain questions (general-purpose: 15.8%–20.2%; COPA: 0.8%, QQP: 8.1%). Q2: Do the (few) negations in natural language understanding corpora play a role in solving the tasks? After showing that negation in underrepresented in natural language understanding corpora, we explore whether the few negations they contain are important. Given an instance from any of the corpora, we consider a negation important if removing it changes the ground truth. In other words, a negation is unimportant if one can ignore it and still solve the task at hand. Table 2 presents examples of important and unimportant negations. We manually examine the negations in all instances containing negation from the validation split of each corpus except QQP, for which we examine 1,000 (out of 5,196). Note that COPA does not have any negations in the validation split, and many corpora have few instances containing negation (CommonsenseQA: 184, STS-B: 225, QNLI: 852, WiC: 99, WSC: 52, and SST-2: 263). We choose to work with the validation set because we want to compare results when negation is and is not important (Q3), and the ground truth for the test splits of some corpora are not publicly available. We observe that (a) all negations in WiC and WSC are unimportant, and (b) the percentages of unimportant negations in CommonsenseQA, SST2, QQP, STS-B, and QNLI are substantial: 45.1%, 63%, 97.4%, 95.6%, and 97.7%, respectively. This indicates that one can safely ignore (almost) all negations and still solve the benchmarks. We also analyze the role of two major types of negation: syntactic (not, no, never, etc.) and morphological (i.e., affixes such as un-, im-, and -less). To this end, we work with CommonsenseQA and SST-2 as the percentage of important nega-\ntions in them are the closest to 50% (45.1% and 63%). Perhaps unsurprisingly, syntactic negations are much more common than morphological negations (CommonsenseQA: 88.6% vs 11.4%, SST2: 71.9% vs 28.1%). More importantly, syntactic negations are more often important in SST-2 (42.3% vs 23%), but both syntactic and morphological negation are roughly equaly important in CommonsenseQA (55.2% vs 52.4%) The supplementary materials (Table 6) provide examples of these two types of negation. Q3: Do state-of-the-art transformers trained with NLU corpora face challenges with instances that contain negation? We conduct experiments with RoBERTa (Liu et al., 2019). More specifically, we use the implementation by Phang et al. (2020) and train a model with the training split of each corpus. We refer the reader to the supplementary materials for details about these models and hyperparameters. We chose RoBERTa over other transformers because 4 out of the 10 best submissions to the SuperGLUE benchmark use it.3\nTable 3 presents the results evaluating the models with the corresponding validation splits. RoBERTa obtains slightly worse results with the validation instances that have negation in all corpora; the only exception is QQP (F1: 0.90 vs. 0.91). These results lead to the conclusion that negation may only pose a small challenge to state-of-the-art transformers.\nThe results obtained evaluating with the important and unimportant negations from the samples analyzed in Question 2, however, provide a different picture. Indeed, we observe substantial drops in results in all tasks that have both kinds of negations. More specifically, we obtain 27% lower results with instances containing important negations in QNLI (F1: 0.92 vs. 0.67), 33%/26% lower in STSB, 24% lower in CommonsenseQA, 21% lower in QQP, and 9% lower in SST. We conclude that\n3https://super.gluebenchmark.com/leaderboard\ntransformers trained with existing NLU corpora face challenges with instances that contain negation. These results raise two important questions for future research: Is negation an inherently challenging phenomenon for RoBERTa? How many instances with negation are required to solve a natural language understanding task?"
    }, {
      "heading" : "4 Conclusions",
      "text" : "We have analyzed the role of negation in eight natural language understanding corpora covering six tasks. Our analyses show that (a) all but WSC contain almost no negations or around 31%–54% of the negations found in general-purpose texts, (b) the few negations in these corpora are usually unimportant, and (c) RoBERTa obtains substantially worse results when negation is important.\nOur analyses also provide some evidence that creating models to properly deal with negation may require both new corpora and more powerful models. The need for new corpora stems from the answers to Questions 1 and 2. The justification for powerful models is more subtle. We point out that the percentage of unimportant negations (Section 3) is only a weak indicator of the drop in results with important negations (Table 3). For example, we observe a 24% and 21% drop in results with important negations from CommonsenseQA and QQP despite 45% and 97% of negations are unimportant.\nNegation reverses truth values thus solutions to any natural language understanding task should be robust when negation is present and important. To this end, our future work includes two lines of research. First, we plan to create benchmarks for the six tasks consisting of instances containing negation (50/50 split important/unimportant). Second, we plan to conduct probing experiments to investigate whether (and where) pretrained transformers capture the meaning of negation. Doing so may help us discover potential solutions to understand negation and make inferences."
    }, {
      "heading" : "A Negation Cue Detection",
      "text" : "We develop a negation cue detector (Section 3 in the paper) by utilizing the RoBERTa (base architecture; 12 layers) pre-trained model (Liu et al., 2019). We fine-tune the system on ConanDoyleneg (Morante and Daelemans, 2012) corpus. While fine-training, the negation cues are marked with BIO (B: Beginning of cue, I: Inside of cue, O: Outside of cue) tagging scheme. The contextualized representations from the last layer of RoBERTa are passed to a fully connected (FC) layer. Finally, a conditional random field (CRF) layer produces the output sequence for the labels.\nOur model yields the following results on the test set: 93.26 Precision, 94.32 Recall, and 93.79 F1. The neural model takes about two hours on average to train on a single GPU of NVIDIA Tesla K80. A list of the tuned hyperparameters that the model requires to achieve the above results is provided in Table 4. The code is available at https://anonymouslink."
    }, {
      "heading" : "B Hyperparameters to Fine-tune the System for Each of the NLU Tasks",
      "text" : "We use an implementation by Phang et al. (2020) and fine-tune RoBERTa (base architecture; 12 lay-\ners) (Liu et al., 2019) model separately for each of the eight corpora. We accept the default settings of the hyperparameters, except for a few, when fine-tuning the model on each benchmark. Table 5 shows tuned hyperparameters for each benchmark."
    }, {
      "heading" : "C Examples of Syntactic and Morphological Negations",
      "text" : "We provide additional examples of syntactic and morphological negations in Table 6."
    } ],
    "references" : [ {
      "title" : "The second pascal recognising textual entailment challenge",
      "author" : [ "Roy Bar-Haim", "Ido Dagan", "Bill Dolan", "Lisa Ferro", "Danilo Giampiccolo", "Bernardo Magnini", "Idan Szpektor." ],
      "venue" : "Proceedings of the second PASCAL challenges workshop on recognising",
      "citeRegEx" : "Bar.Haim et al\\.,? 2006",
      "shortCiteRegEx" : "Bar.Haim et al\\.",
      "year" : 2006
    }, {
      "title" : "The fifth pascal recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Danilo Giampiccolo" ],
      "venue" : null,
      "citeRegEx" : "Bentivogli et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Neural natural language inference models partially embed theories of lexical entailment and negation",
      "author" : [ "Atticus Geiger", "Kyle Richardson", "Christopher Potts." ],
      "venue" : "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Net-",
      "citeRegEx" : "Geiger et al\\.,? 2020",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2020
    }, {
      "title" : "The third PASCAL recognizing textual entailment challenge",
      "author" : [ "Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan." ],
      "venue" : "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague. Association",
      "citeRegEx" : "Giampiccolo et al\\.,? 2007",
      "shortCiteRegEx" : "Giampiccolo et al\\.",
      "year" : 2007
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "An analysis of natural language inference benchmarks through the lens of negation",
      "author" : [ "Md Mosharaf Hossain", "Venelin Kovatchev", "Pranoy Dutta", "Tiffany Kao", "Elizabeth Wei", "Eduardo Blanco." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Hossain et al\\.,? 2020",
      "shortCiteRegEx" : "Hossain et al\\.",
      "year" : 2020
    }, {
      "title" : "NegBERT: A transfer learning approach for negation detection and scope resolution",
      "author" : [ "Aditya Khandelwal", "Suraj Sawant." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 5739–5748, Marseille, France. Euro-",
      "citeRegEx" : "Khandelwal and Sawant.,? 2020",
      "shortCiteRegEx" : "Khandelwal and Sawant.",
      "year" : 2020
    }, {
      "title" : "A qualitative evaluation framework for paraphrase identification",
      "author" : [ "Venelin Kovatchev", "M. Antonia Marti", "Maria Salamo", "Javier Beltran." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP",
      "citeRegEx" : "Kovatchev et al\\.,? 2019",
      "shortCiteRegEx" : "Kovatchev et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:452–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "The winograd schema challenge",
      "author" : [ "Hector J. Levesque", "Ernest Davis", "Leora Morgenstern." ],
      "venue" : "Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR’12, page 552–561. AAAI Press.",
      "citeRegEx" : "Levesque et al\\.,? 2012",
      "shortCiteRegEx" : "Levesque et al\\.",
      "year" : 2012
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "ConanDoyle-neg: Annotation of negation cues and their scope in conan doyle stories",
      "author" : [ "Roser Morante", "Walter Daelemans." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12),",
      "citeRegEx" : "Morante and Daelemans.,? 2012",
      "shortCiteRegEx" : "Morante and Daelemans.",
      "year" : 2012
    }, {
      "title" : "Stress test evaluation for natural language inference",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Norman Sadeh", "Carolyn Rose", "Graham Neubig." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340–2353,",
      "citeRegEx" : "Naik et al\\.,? 2018",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2018
    }, {
      "title" : "jiant 2.0: A software toolkit for research on general-purpose text understanding models",
      "author" : [ "Jason Phang", "Phil Yeres", "Jesse Swanson", "Haokun Liu", "Ian F. Tenney", "Phu Mon Htut", "Clara Vania", "Alex Wang", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Phang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2020
    }, {
      "title" : "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
      "author" : [ "Mohammad Taher Pilehvar", "Jose CamachoCollados." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Pilehvar and CamachoCollados.,? 2019",
      "shortCiteRegEx" : "Pilehvar and CamachoCollados.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
      "author" : [ "Melissa Roemmele", "Cosmin Adrian Bejan", "Andrew S Gordon." ],
      "venue" : "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, pages 90–95.",
      "citeRegEx" : "Roemmele et al\\.,? 2011",
      "shortCiteRegEx" : "Roemmele et al\\.",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Supervised open information extraction",
      "author" : [ "Gabriel Stanovsky", "Julian Michael", "Luke Zettlemoyer", "Ido Dagan." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Stanovsky et al\\.,? 2018",
      "shortCiteRegEx" : "Stanovsky et al\\.",
      "year" : 2018
    }, {
      "title" : "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing NLP",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "CorefQA: Coreference resolution as query-based span prediction",
      "author" : [ "Wei Wu", "Fei Wang", "Arianna Yuan", "Fei Wu", "Jiwei Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6953–6963, Online. As-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning",
      "author" : [ "Hitomi Yanaka", "Koji Mineshima", "Daisuke Bekki", "Kentaro Inui", "Satoshi Sekine", "Lasha Abzianidze", "Johan Bos." ],
      "venue" : "Proceedings of the Eighth Joint Con-",
      "citeRegEx" : "Yanaka et al\\.,? 2019",
      "shortCiteRegEx" : "Yanaka et al\\.",
      "year" : 2019
    }, {
      "title" : "Enhancing pre-trained language representations with rich knowledge for machine reading comprehension",
      "author" : [ "An Yang", "Quan Wang", "Jing Liu", "Kai Liu", "Yajuan Lyu", "Hua Wu", "Qiaoqiao She", "Sujian Li." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "For example, question answering (Rajpurkar et al., 2016), information extraction (Stanovsky et al.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : ", 2016), information extraction (Stanovsky et al., 2018), coreference resolution (Wu et al.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : ", 2018), coreference resolution (Wu et al., 2020), and machine reading (Yang",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "For example, annotators are likely to use negation when asked to write a text that contradicts something despite contradictions in the wild need not have a negation (Gururangan et al., 2018).",
      "startOffset" : 165,
      "endOffset" : 190
    }, {
      "referenceID" : 24,
      "context" : "We select two corpora for question answering: CommonsenseQA (Talmor et al., 2019) and COPA (Roemmele et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "For textual similarity and paraphrasing, we select QQP2 and STS-B (Cer et al., 2017).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "For inference, we work with QNLI (Rajpurkar et al., 2016), which consists in determining whether a text is a valid answer to a question.",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "For coreference resolution, we choose WSC (Levesque et al., 2012), which consists in determining whether a pronoun and a noun phrase are co-referential (italicized in Table 2).",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "nally, we work with SST-2 (Socher et al., 2013) for sentiment analysis.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "For convenience, we work with the formatted versions of these corpora in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "Related Work Previous work has shown that SNLI (Bowman et al., 2015) and MNLI (Williams et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : ", 2015) and MNLI (Williams et al., 2018) have annotation artifacts (e.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "The literature has also shown that simple adversarial attacks including negation cues are very effective (Naik et al., 2018; Wallace et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 146
    }, {
      "referenceID" : 25,
      "context" : "The literature has also shown that simple adversarial attacks including negation cues are very effective (Naik et al., 2018; Wallace et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : ", identifying entailments and contradictions) with monotonicity and negation (Geiger et al., 2020; Yanaka et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 30,
      "context" : ", identifying entailments and contradictions) with monotonicity and negation (Geiger et al., 2020; Yanaka et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "Q1: Do natural language understanding corpora contain as many negations as generalpurpose English texts? In order to automatically identify negation cues, we train a negation cue detector with the largest corpus available, ConanDoyle-neg (Morante and Daelemans, 2012).",
      "startOffset" : 238,
      "endOffset" : 267
    }, {
      "referenceID" : 14,
      "context" : "The cue detector is based on the RoBERTa pretrained language model (Liu et al., 2019); we provide details about the architecture and training process in the supplementary materials.",
      "startOffset" : 67,
      "endOffset" : 85
    } ],
    "year" : 0,
    "abstractText" : "This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to generalpurpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important. We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present.",
    "creator" : null
  }
}
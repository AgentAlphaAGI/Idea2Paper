{
  "name" : "ARR_2022_266_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mix and Match: Learning-free Controllable Text Generation using Energy Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Transformer-based language models trained on massive amounts of natural language data found on the internet have demonstrated exceptional ability to learn useful representations of sentences for downstream natural language processing tasks. Autoregressive models like GPT-3 are commonly used to generate high quality natural language text as well. However, effective methods for generating well-formed sequences that satisfy a desired global control attribute (e.g., sentiment, formality, etc.) represent an active area of research. If successful, effective controlled generation techniques might help mitigate bias and prevent generation of hate\nspeech and toxic language (Yang and Klein, 2021; Xu et al.; Gehman et al., 2020).\nMuch of the prior work on controlled generation has focused on autoregressive models like GPT-2 and has involved fine-tuning of these large models on target attributes (Yang and Klein, 2021; Krause et al., 2020), or training entirely separate probabilistic generative models for the target attributes (He et al., 2020), or training specialized attribute models with a restricted structure to heuristically generate attribute-sensitive sequences (Dathathri et al., 2020). Our approach instead focuses on drawing samples from a test-time combination of pretrained blackbox experts that each score a desired property of output text – for example, fluency, attribute sensitivity, or faithfulness to the context. Specifically, we view the product of these blackbox experts as a probabilistic energy model (Hinton, 2002) – i.e., a non-autoregressive, globally normalized language model – and then sample (without further training or fine-tuning) using a specialized Gibbs sampler with a Metropolis-Hastings correction step (Goyal et al., 2021).\nOur full framework, which we entitle Mix and Match LM (depicted in Figure 1), enables generation of high-quality attribute-controlled samples by mixing and matching blackbox models like off-the-shelf pretrained attribute-sensitive discriminators (e.g., sentiment classifiers), large bidirectional pretrained language models like BERT (Devlin et al., 2019), and other modules specializing in capturing desirable features pertaining to faithfulness to any additional context, like hamming distance, Bertscore distance (Zhang et al., 2020), or Bleurt (Sellam et al., 2020) based distance between the sample and the conditioning context. We generate samples from the energy language model assembled from these component experts by using the recently proposed Gibbs-MetropolisHastings scheme (Goyal et al., 2021) for sampling from energy models using a masked language\nmodel as a proposal distribution. In this scheme, an expressive bidirectional language model like BERT is used to make a proposal at each transition step in the Gibbs chain to jump to a sequence x̄ from the current sequence x. This proposal’s fitness is judged by the change in the energy language model’s score, with the sampler accepting proposals with larger energy reductions at a higher rate. This approach yields high-quality diverse samples that respect the distribution induced by the product of expert blackbox models.\nWe demonstrate the flexibility of our approach by performing a variety of controlled generation tasks, such as aspect-based text revision, style transfer, and attribute grounded generation. On all of these tasks, we compare our performance to existing approaches that involve additional fine-tuning of generation or attribute based models, or impose restrictions on the parametrization of specific components. We observe that our approach, which does not require any gradient optimization and is able to combine arbitrary heterogeneous blackbox models, outperforms recent controllable generation and style transfer models on a variety of tasks according to various automated metrics of fluency, quality, and control, as well as human evaluations."
    }, {
      "heading" : "2 Mix-and-match Language Models",
      "text" : "In this section, we describe our approach and motivation behind our method. Specifically, we frame the problem of performing controlled generation as a problem of sampling from a specialized energybased (or globally normalized) sequence model that defines a probability distribution which satisfies the desired constraints we wish to impose in the controlled generation setting. As described below, this energy based model is composed of pretrained components and does not require any further optimization. An energy-based sequence model defines the probability distribution over the space of pos-\nsible sequences X as:1 p(X;θ)= e−E(X;θ)∑ X′∈X e −E(X′;θ) , where E(X; θ) refers to the scalar energy of a sequenceX that is parametrized by θ. Lower energy corresponds to higher likelihood of X . In contrast to the common autoregressive sequence models, exact likelihood computation and efficient sampling from these models is challenging. Despite these challenges, we focus on this paradigm of sequence modeling because energy-based models offer increased flexibility via sequence level features and constraints. As we discuss next, this capability lets us easily define expressive functions for controlled generation of sequences which is not readily offered by the autoregressive modeling paradigm."
    }, {
      "heading" : "2.1 Product of Experts Energy-based Models and Controlled Generation",
      "text" : "Our approach is motivated by the perspective that the task of controlled generation requires concentrating probability mass over small subspace of sequences in X that satisfies various constraints pertaining to fluency, target attributes, and other control variables. Consider the task of generating positive sentiment sentences. This requires satisfaction of two major constraints: (1) The sequence X should be well-formed, (2) The sequenceX should express positive sentiment. If we have access to two separate probability distributions over X , one for modelling well-formedness (p1(X)) and another for modelling positivity (p2(X)), then a natural solution for controlled generation in this setting would be to draw samples from a probability distribution that is a product of these two distributions i.e. pdesire(X) ∝ p1(X) ·p2(X). In our approach, we further relax this requirement by assuming access to expert blackboxes that yield scalar non-probabilistic energy scores E1 and E2 indicating fitness of a sequence w.r.t. well-formedness and positivity respectively. Under the product of experts framework above the desired probability distribution would take the form:\n1For simplicity, we are concerned with a finite set of sequences limited by some maximum length.\nlog pdesire(X) = −(E1(X)+E2(X)) − logZ. This expression shows that when working with scalar scores for the expert blackboxes, the product of expert models yields an energy model whose energy is simply the sum of the scalar energy values obtained from the expert models. Inspired by this, we propose a framework for controlled generation that involves linear combinations of various blackbox experts in order to obtain a distribution whose samples satisfy the requirements of a desired controlled generation task: EM&M(X)= ∑k i=1αiEi(X), where our proposed mix-and-match energy is composed of k expert energy components, which are weighted by scalar hyperparameters α."
    }, {
      "heading" : "2.2 Expert Factors in Mix-and-Match LM",
      "text" : "As shown in Fig. 1, we use the following blackbox experts in our experiments as modules that we can add or remove to produce desired behavior: Emlm(X) : Recent work has shown that large masked language models (MLM) like BERT can discirminate between well-formed and ill-formed sentences (Zhang et al., 2020) and induce an implicit energy function over the sequences (Goyal et al., 2021). Hence, we use BERT-base as a blackbox to model the form and fluency of sentences. Specifically, we use an energy parametrization introduced in Goyal et al. (2021) which is negative of the sum of unnormalized logits at each position obtained via forward pass of the MLM after masking the respective positions iteratively. We refer to this blackbox energy for modeling the overall form of the sentences by Emlm(X). Edisc(X) : This particular expert module refers to the energy obtained via the discriminator for the attributes of interest. What this module returns is the raw logits of the discriminator, for the target attribute. For instance, if we have a sentiment classifier, and want to produce positive sentiment, the Edisc(X)=−log p(+|X). Ehamm(X;X\n′) : For a given sequence X ′, this quantity refers to the hamming distance between the sequence X and X ′. This penalization token level deviation from X ′ which is useful if we are interested in only making minor edits to X ′ as described later. Efuzzy(X;X\n′) : Similar to the hamming distance, this quantity refers to the Bertscore (Zhang et al., 2020) computed between X and X ′ which can be viewed as a fuzzy hamming distance that takes semantic similarity into account. EBleurt(X;X ′) : This energy refers to the negative\nBleurt (Sellam et al., 2020) score between X and X ′. We use this score to get sentence level similarity scores which do not hinge on token level alignment across the two sentences."
    }, {
      "heading" : "2.3 Sampling scheme",
      "text" : "To sample from the energy parametrizations described in the last section, we follow the Metroplolis Hastings (Hastings, 1970) MCMC scheme for sampling from masked language models introduced by Goyal et al. (2021). While the proposal distribution we use is the same as Goyal et al. (2021) i.e. masked language model’s (BERT’s) conditionals, the energy parametrizations we use are more suitably designed for controlled generation.\nWe briefly explain the sampling procedure, which involves forming long Markov chains of sequences starting with a random sequence, and following the MH scheme which uses a proposal distribution to propose a new sequence at each step in a chain which is either accepted or rejected based on its fitness to the energy function. The sequences at the end of these chains correspond to samples from the desired energy-base model. Operationally, at each MCMC step, we mask out a token at a random position in the current sequence X in the chain, and propose a new sequence X̄ to transition to by sampling a token from the MLM conditional softmax at the masked position. This proposed sequence is evaluated by its ability to reduce the energy from the current sequence in the chain and is accepted with the prob-\nability p(X̄;X) =min ( 1,\ne−EM&M(X̄) pmlm(Xi|X\\i) e−EM&M(X) pmlm(X̄i|X\\i)\n) .\nEM&M (X) refers to the product of experts energy either Egen or Erev depending on the task, i refers to the position chosen for masking, pmlm refers to the MLM’s conditional distribution at the [MASK] position. Intuitively, this acceptance probability indicates that the proposed sequence X̄ is more acceptable if it has lower energy than the current sequence X in the chain and is rare or less likely to be proposed by the proposal distribution again."
    }, {
      "heading" : "2.4 Controlled generation Tasks",
      "text" : "We use the expert blackbox factors and the sampling scheme describe above in our framework to perform two kinds of controlled generation tasks. Prompted generation: This task focuses on generating well-formed sentences that start with a specified prompt and also satisfy a target attribute for which we have access to a discriminator. An example task would be to generate positive\nsentiment sequences starting with This movie. The energy function takes the form:\nEgen(X)=Emlm(X) + αEdisc(X) (1)\nα is a hyperparameter that controls the tradeoff between the MLM score and the discriminator’s influence. For MH-based sampling for this task, we initialize the sequence with the starting prompt and rest of the tokens masked out, which creates a seed text of shape the movie[MASK][MASK]... [MASK], for the prompt example of the movie. The number of mask tokens depends on the target generation length, and we constrain the sampler to only produce proposals and revise non-prompt tokens, and mark the prompt tokens as “frozen”. Controlled text revision: This task involves editing a source sequence X ′ in order to satisfy the desired target attributes exhibited by the generated sequence X . The energy function for this task is:\nErev(X)=Egen(X)+β Ehamm(X,X ′)+\nγ Efuzzy(X,X ′)+η EBleurt(X,X ′) (2)\nThis energy function in addition to valuing wellformedness and satisfying target attribute requirements, also focuses on maintaining faithfulness to the source sequence X ′. For sampling with this energy, we initialize the sequence with the sequence X ′ to be edited. This sets the length of the target sequence to be the same as the source. In this setup, the sampler can revise all tokens and is not constrained.\nFor both these tasks, we run a separate MCMC chain for each generated sentence for 8 to 15 epochs, depending on the task. An epoch refers to one masking cycle over all the non-frozen positions (selected randomly) of the sequence."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1 Tasks and Datasets",
      "text" : "Controllable debiasing: ROC story corpus. We use the subset of the ROC story corpus (Mostafazadeh et al., 2016) test-set that is used by PowerTransformer (Ma et al., 2020) for their evaluations. We use this data for controllable debiasing, a text revision task which aims to correct the implicit and potentially undesirable agency biases in character portrayals. This test-set consists of 549 sentences, where 224 sentences have low agency verbs (such as wish, dream, etc.) and the rest have high agency (like pursue, achieve, etc.). The task is to revise the sentences such that the meaning\nis preserved, but the agency of the sentence is changed in the target direction. Sentiment transfer: Yelp. We use Yelp (Shen et al., 2017) dataset’s test-set for the task of sentiment transfer. The test set comprises of 1000 sentences, half with positive and half with negative sentiment. We also have a reference set of hand written sentiment transferred sentences, provided by (He et al., 2020) that we use for reporting evaluation metrics. Formality transfer: GYAFC We use 1051 sentences from the test-set of the GYAFC (Rao and Tetreault, 2018) dataset, which contains formal and informal sentences for the task of formality transfer (both directions of formal to informal and informal to formal). Here we use the entertainment and music domain subset of this data, following the evaluation setup of (He et al., 2020). This dataset also contains parallel data between formal and informal sentences, which we use as reference for reporting evaluation metrics. Prompted generation: To compare with PPLM, another controlled generation method, we set Mix and Match LM to generate text with positive or negative sentiment given prompts (listed in Appendix A.4) by using a Yelp sentiment classifier as discriminator."
    }, {
      "heading" : "3.2 Expert Component Configurations",
      "text" : "We use a Huggingface pre-trained bert-baseuncased model2 as our MLM for yielding Emlm and also providing the proposal distribution in our MH MCMC sampler. For obtaining Edisc, we train BERT-based classifiers on the training-set of our datasets to use as our attribute discriminators. Although we could have used any pre-trained attribute classifier from a model repository like Huggingface for Edisc, we train our own classifier for controlled empirical comparison. As described later, we do use pretrained Huggingface attribute classifiers as external attribute classifiers for fair evaluation against baselines. For experiments in which we add BLEURT (Sellam et al., 2020) and BertScore (Zhang et al., 2020) components to the energy, we download the pre-trained Elron/bleurt-base-512 and robertalarge_L17 models from Huggingface, respectively. We have provided implementation details and hyperparameter ablations of all the experiments in Appendix A.1, A.2, A.3 and A.4.\n2https://huggingface.co/transformers/ pretrained_models.html"
    }, {
      "heading" : "3.3 Baselines",
      "text" : "PowerTransformer. For the task of controllable debiasing (agency revision), we compare our work with PowerTransformer (Ma et al., 2020), an approach that uses paraphrasing and selfsupervision based on a reconstruction loss, building on pre-trained language models, to re-write text and control agency level of sentences. He et al. For style transfer on sentiment an formality domains, we compare our work with He et al. (2020), a generative style transfer framework which uses a variational autoencoder (VAE) built using a sequence-to-sequence LSTM-based model to do unsupervised style transfer. This framework needs to be trained from scratch for each style transfer task. UNMT. As a second baseline for style transfer, we compare our work with UNMT (Lample et al., 2018), an unsupervised machine translation framework that demonstrates high performance for sentiment transfer. PPLM. For the task of controlled generation, we compare our work to Plug-and-Play LM (PPLM) Dathathri et al. (2020), which does attribute controlled generation using the flow of gradients from discriminators trained on the last hidden layer representations of the generator, to guide generation."
    }, {
      "heading" : "3.4 Evaluation Metrics",
      "text" : "We use a variety of evaluation metrics to compare our approach’s performance on two major facets: (1) Quality of generated text, and (2) success on matching the target attribute used for control."
    }, {
      "heading" : "3.4.1 Text Quality and Semantic Similarity",
      "text" : "GPT-2 PPL. We feed our generated test sentences to a Huggingface (Radford et al., 2019) pre-trained GPT-2 xl model, and report its perplexity (PPL), as an automatic measure of fluency. Although this measure is not a perfect indicator of fluency, we find it to be a useful metric alongside human judgements. 3 BLEU. For sentiment (Yelp) and formality (GYAFC) transfer experiments, since we have reference text, we report the BLEU score. For controlled debiasing, we report BLEU between generated text and source, and show it as BLEU (src). BertScore. As a measure of meaning preservation, we use the F1 BertScore metric (Zhang et al., 2020)\n3Due to the high variance in the PPL scores generated across sentences by GPT-2, we report the median score for each system under comparison.\nto compare the semantic similarity of the provided reference sentence with the generated output. Hamming Distance. We also report the hamming distance between the source text and generated text, to measure the extent of the change induced by our framework."
    }, {
      "heading" : "3.4.2 Attribute Quality",
      "text" : "Internal Classifier Accuracy. To evaluate the quality of applying target attributes, we report accuracy of the internal classifier (the discriminator used for generation) on the generated text, assuming the target attribute is the correct label. The higher this accuracy is, the better. External Classifier Accuracy. Since the internal classifier is the one we are sampling from, it is natural that we would get high accuracy on it, compared to our baselines. To create a more fair comparison, we also report classification accuracy using external classifiers, downloaded from Huggingface. For sentiment classification we use textattack/bert-base-uncasedyelp-polarity (Morris et al., 2020), and for formality we use cointegrated/robertabase-formality. Agency Lexicon Accuracy. For the controlled debiasing experiment, we measure the accuracy of the change in agency by comparing the target agency level with that of the generated text, extracted using the connotation frames lexicon, and following the setup from Ma et al. (2020)."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Controllable Debiasing",
      "text" : "Table 1 shows our results for the task of text revision for controlling agency bias which is introduced by Ma et al.. Our baseline for this task is PowerTransformer which has a vanilla (no boost) variant and a variant with vocab boosting. The boosting mechanism up-weights the logits of verbs that belong to the target agency lexicon – during decoding – so as to increase their probability and incentivize generation in that direction. We also measure our metrics on the original test-set, without revision, to provide a better sense of the changes made.\nWe offer different variants of our framework, to provide a fair comparison and to better ablate our proposed method. “Disc” denotes our framework where we add the discriminator expert (Edisc) which is trained to predict the agency level of a sentence, to the energy along with Emlm, and Ehamm\n(Eq. 2). As describedd above, in the text revision task like this hamming distance is computed between the generated proposals and the source sentence. The “Agency Score” variant adds an alternative term to EM&M instead of Edisc, which is the number of target agency verbs according to the connotation frames lexicon (Sap et al., 2017) in the sentence. The “Disc+Agency” variant has both the energy components. We also apply our method in two ways: “Verb Replace” which allows the sampler to propose revisions for only one pre-determined verb (which is provided in the dataset annotations). In this setup all tokens remain frozen, except for the given verb. The conventional mode (M&M LM), however, proposes revisions for all tokens in the sentence and is not constrained.\nTable 1 shows that in the conventional setup, Mix and Match LM (Disc only) has performance similar to that of PowerTransformer, without boosting. With the Agency Score component, our method outperforms PowerTransformer in terms of accuracy of revision as per the agency lexicon accuracy metric, with negligible loss in meaning (BertScore). The reason behind this better performance in terms of applying target agency accuracy is that our method’s sampling is guided by the energy that is directly built on the metrics we care about, as opposed to trying to apply them through paraphrasing and proxies such as vocab boosting, which are employed in the PowerTransformer method.\nAnother important observation here is the difference between “Verb Replace” and conventional modes. This ablation shows that although our method makes few changes (the average hamming distance between source and output sentences are between 1.37 and 2.45), it still outperforms a “static” method that has extra knowledge of the offending verb and focuses on changing only that verb, by a significant margin."
    }, {
      "heading" : "4.2 Style Transfer",
      "text" : "In this section we conduct experiments on the task of unsupervised style transfer for sentiment and formality. The main difference between these two tasks is the number of words that need to be revised to have successful transfer without changing the meaning of the sentence. Sentiment transfer needs fewer changes whereas formality transfer needs more structural change."
    }, {
      "heading" : "4.2.1 Sentiment Transfer",
      "text" : "For this task we include two components in our energy model, the attribute discriminator (Edisc), to induce the target style, and the hamming distance (Edisc), to maintain the meaning of the sentence. We don’t include more complex semantic similarityrelated components Efuzzy and EBleurt, since sentiment transfer can normally be done by making only a few changes to the sentence. We report results with two different variants, one where the discriminator component has a higher coefficient in the energy (Discriminator↑) and one where the hamming distance has a higher coefficient (Hamming↑). In effect, these two show the trade-off between transfer quality and language quality.\nWe see in Table 2 that our method, with the hamming component up-weighted, outperforms both the generative baselines in terms of transfer accuracy (Ext. Clsf.) and semantic similarity (BertScore). We can also see Mix and Match LM has higher BLEU score, with respect to the provided handwritten reference sentences. We hypothesize that this superiority is due to the tendency of our model to make minimal revisions that satisfy the prodduct of experts energy model. Therefore, our model can successfully change the style without changing the meaning of the sentence. The generative baselines however, regenerate the sentence which imposes more change, as can be observed from the hamming\ndistance column (Hamm.(src)) in Table 2."
    }, {
      "heading" : "4.2.2 Formality Transfer",
      "text" : "For this task, we include the formality classifier (Edisc), Hamming distance (Ehamm), and Bertscore (Efuzzy) components in the energy formulation, to permit the transfer of style and also maintain the meaning of the sentence. Efuzzy helps with imposing semantic similarity between source and generated sentences, since Hamming alone isn’t sufficient for judging comparable formal and informal sentences. We show results for two setups of our framework, one where the discriminator coefficient is higher (Discriminator↑) and another where the Bertscore coefficient is higher (BertScore↑).\nTable 3 shows our formality transfer results. For this task, we have broken down the external classifier accuracy for the different transfer directions of formal to informal (→ Inf.) and informal to formal (→ Form.). We do this because for both our method and the baselines, the → Form. task is harder and therefore has lower accuracy. We observe that our method outperforms the baselines in terms of external classifier accuracy, BertScore and BLEU. However, for this task, we can see that the GPT-2 PPL of our generated sentences is higher than those of the baselines. The reason behind this is the format and noise in the data. The samples for this dataset are taken from the music and entertainment industry domain, and contain some symbols and characters similar to emojies (e.g. “:)” and “***”). This is where the tendency of our approach toward\nminimal revisions is hurtful–our revisions of text, often do not get rid of all of these symbols, while the baselines’ generative methods successfully remove all the superfluous chatacters because they rewrite sentences from scratch. This difference reflects in the GPT-2 perplexity scores."
    }, {
      "heading" : "4.3 Prompted Controlled Generation",
      "text" : "For the prompted controlled text generation task, we only use Emlm and Edisc, and perform generation with sentiment as the control attirbute. We generate sequences of different lengths (12, 20 and 50 tokens), given 14 prompts taken from Dathathri et al. (2020) (the prompts are listed in Appendix A.4) with our framework and the baseline (PPLM). We generate 20 sequences, per sentiment, for each prompt, making it an overall of 560 sequences, which use for both automatic and human evaluations. Table 5 shows samples of generated outputs from our method, compared with PPLM.\nTable 4 shows our results for this experiment. Here, we have an additional metric, the MLM energy (lower is better), which, like GPT-2, indicates the quality of generated sentences (Salazar et al., 2020) according to BERT. We report this extra metric here since PPLM uses a GPT model for generation, and it is natural that it would measure better on this metric, compared to our method. The table shows that for all lengths of generated sentences, our method is much better at inducing the target sentiment. However, in terms of GPT-2 PPL, PPLM naturally performs better, as it incorporates\na GPT model but in terms of the MLM score, Mix and Match LM performs better since it uses BERT to propose changes. To enable a more conclusive comparison of the text quality, we report results with human evaluations. For these evaluations, we randomly select 10 generated outputs for each prompt, for each sentiment (making it 2× 14× 10 = 280 sentences per method), and asked three Amazon Turkers per sample pair, which samples they find more fluent. We report the majority vote of the Turkers in the table. The results show that for sequences with lengths 12 and 20, humans found our generations more fluent, with preference rates of 71.1% and 62.9% respectively. However, for length 50, the preference rate for M&M drops to 46.7%, which shows that our method is superior to PPLM for short/medium length generation, however PPLM does better at generating longer sequences."
    }, {
      "heading" : "5 Related Work",
      "text" : "Common approaches for flexible attribute-based generation range from retraining or fine-tuning a large underlying base model for generation on domain-specific data (Ziegler et al., 2019), to modifying the architecture of the large pre-trained model (Keskar et al., 2019). Several style transfer approaches hinge on training large generative models with non-parallel (He et al., 2020; Lample et al., 2018; Shen et al., 2017; Krishna et al., 2020; Reif et al., 2021) data across the domains of interest. Instead of retraining large base models or training new architectures from scratch, recent work has used attribute discriminators to steer the generation (Gu et al., 2017) from a large autoregressive\nlanguage model. Plug-and-Play LM (Dathathri et al., 2020) uses discriminators learned from the LM’s top-level hidden layer to modify the LM’s states toward increasing probability of the desired attribute via gradient ascent at each step. This restricts the parametrization of the discriminator and also requires access to multiple gradients from the discriminator multiple times per generated sentence, making this approach fairly expensive and restrictive. GeDi (Krause et al., 2020) and Fudge (Yang and Klein, 2021) take similar, approaches and guide generation from LM using specially trained generative and future discriminators, respectively. These approaches in addition to requiring some kind of optimization, also rely on heuristics to manipulate the local softmax distributions of autoregressive models and do not enjoy the benefits of incorporating global features into the generation mechanism in a simple probabilistic manner. In constrast, our energy-based formulation is not only optimization-free, but also fully modular, allowing for heterogenous blackbox experts to be combined with each other."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present Mix and Match Language Models (M&M LMs), a training-free framework for controlled text generation that can easily mix heterogeneous expert modules. We show that our framework outperforms prior methods on a suite of text revision and attribute controlled generation tasks. Further, our results indicate that probabilistic energy language models, typically considered intractable, can be used for practical text generation tasks when combined with an appriorate sampling scheme.\nEthical Considerations\nWe have designed our framework with re-usablity and modularity in mind, so as to alleviate the need of multiple training and fine-tuning rounds, and to reduce the negative environmental effects that training large models have. We do however acknowledge that strong controlled generation methods that rely on discriminators can have the potential to regurgitate the training data and produce harmful outputs and toxic language (Xu et al.; Gehman et al., 2020; Wallace et al., 2020). However, if used properly and for good, we anticipate positive impact on debiasing and safe generation."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Controllable Debiasing: Hyper parameters\nFor the results presented in Table 1, we ran the Gibbs chain for 8 epochs (8 iterations over all the tokens) for the conventional mode of our method, and 30 iterations for verb replacement. We used the parameters α=100,β=50,θ=100, where θ is the coefficient assigned to the agency scorer, and α and β are defined in Equations 1 and 2.\nA.2 Sentiment Transfer: Hyperparameters\nIn this section we discuss the hyperparameters used for sampling and see the effects of each one. For the results presented in Table 2, we ran the Gibbs chain for 8 epochs (8 iterations over all the tokens), and used the parameters α=100,β =25 (for Discriminator ↑) and α=100,β=50, for Hamming ↑. α and β are defined in Equations 1 and 2.\nTable 6 shows six different scenarios, with six different coefficeints for the Disciriminator (α), BERT MLM (δ) and Hamming distance (β) components in the energy function, which helps understand the effect each expert has.\nA.3 Formality Transfer: Hyperparameters\nFor the results presented in Table 3, we ran the Gibbs chain for 5 epochs (5 iterations over all the tokens), and used the parameters α= 140,β = 15,γ = 100 (for Discriminator ↑) and α=140,β=50,γ=300, for BertScore ↑. α, β and γ are defined in Equations 1 and 2.\nTable 7 shows four different scenarios, with four different coefficeints for the BLEURT and BertScore components in the energy function, which helps understand the effect each expert has.\nA.4 Prompts and Hyperparameters Used for Controlled Generation\nWe have listed the prompts that we used for controlled text generation (these prompts are taken from Dathathri et al. (2020)): the country, the lake, the chicken, the movie, the pizza, the painting, the year, the city, the book, the potato, the horse, the road, the president, once upon a time. We collect these prompts from PPLMs github repo, available at this url: https: //github.com/uber-research/PPLM/ tree/master/human_annotation/ pplm_labeled_csvs.\nPPLM has multiple knobs to tune for sampling, and after running a greed search we found that gamma=1,num_iterations=10,step_size=0.1,kl_scale=0.01 and gm_scale=0.95 yeild the best results (reported in Table 5). We generated samples by running the command python run_pplm.py -D sentiment, with the mentioned hyperparameters.\nFor our method, we ran the Gibbs chain for 15 epochs, and used hyperparameter α = 40, from Eq. 1. We don’t use any experts other than the yelp sentiment classifier, so we don’t have any other hyperparamters."
    } ],
    "references" : [ {
      "title" : "Plug and play language models: A simple approach to controlled text generation",
      "author" : [ "Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dathathri et al\\.,? 2020",
      "shortCiteRegEx" : "Dathathri et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "author" : [ "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:2009.11462.",
      "citeRegEx" : "Gehman et al\\.,? 2020",
      "shortCiteRegEx" : "Gehman et al\\.",
      "year" : 2020
    }, {
      "title" : "Exposing the implicit energy networks behind masked language models via metropolis-hastings",
      "author" : [ "Kartik Goyal", "Chris Dyer", "Taylor Berg-Kirkpatrick." ],
      "venue" : "ArXiv, abs/2106.02736.",
      "citeRegEx" : "Goyal et al\\.,? 2021",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2021
    }, {
      "title" : "Trainable greedy decoding for neural machine translation",
      "author" : [ "Jiatao Gu", "Kyunghyun Cho", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1968–1978, Copenhagen, Denmark.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Monte carlo sampling methods using markov chains and their applications",
      "author" : [ "W Keith Hastings" ],
      "venue" : null,
      "citeRegEx" : "Hastings.,? \\Q1970\\E",
      "shortCiteRegEx" : "Hastings.",
      "year" : 1970
    }, {
      "title" : "A probabilistic formulation of unsupervised text style transfer",
      "author" : [ "Junxian He", "Xinyi Wang", "Graham Neubig", "Taylor Berg-Kirkpatrick." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "Geoffrey E Hinton." ],
      "venue" : "Neural computation, 14(8):1771–1800.",
      "citeRegEx" : "Hinton.,? 2002",
      "shortCiteRegEx" : "Hinton.",
      "year" : 2002
    }, {
      "title" : "Ctrl: A conditional transformer language model for controllable generation",
      "author" : [ "Nitish Shirish Keskar", "Bryan McCann", "Lav R Varshney", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1909.05858.",
      "citeRegEx" : "Keskar et al\\.,? 2019",
      "shortCiteRegEx" : "Keskar et al\\.",
      "year" : 2019
    }, {
      "title" : "GeDi: Generative Discriminator Guided Sequence Generation",
      "author" : [ "Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "Richard Socher", "Nazneen Fatema Rajani." ],
      "venue" : "arXiv preprint arXiv:2009.06367.",
      "citeRegEx" : "Krause et al\\.,? 2020",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2020
    }, {
      "title" : "Reformulating unsupervised style transfer as paraphrase generation",
      "author" : [ "Kalpesh Krishna", "John Wieting", "Mohit Iyyer." ],
      "venue" : "ArXiv, abs/2010.05700.",
      "citeRegEx" : "Krishna et al\\.,? 2020",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2020
    }, {
      "title" : "Phrasebased & neural unsupervised machine translation",
      "author" : [ "Guillaume Lample", "Myle Ott", "Alexis Conneau", "Ludovic Denoyer", "Marc’Aurelio Ranzato" ],
      "venue" : "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "PowerTransformer: Unsupervised controllable revision for biased language correction",
      "author" : [ "Xinyao Ma", "Maarten Sap", "Hannah Rashkin", "Yejin Choi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
      "author" : [ "John Morris", "Eli Lifland", "Jin Yong Yoo", "Jake Grigsby", "Di Jin", "Yanjun Qi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Morris et al\\.,? 2020",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2020
    }, {
      "title" : "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen." ],
      "venue" : "Proceedings of the 2016 Conference",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer",
      "author" : [ "Sudha Rao", "Joel R. Tetreault." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Rao and Tetreault.,? 2018",
      "shortCiteRegEx" : "Rao and Tetreault.",
      "year" : 2018
    }, {
      "title" : "A recipe for arbitrary text style transfer with large language models",
      "author" : [ "Emily Reif", "Daphne Ippolito", "Ann Yuan", "Andy Coenen", "Chris Callison-Burch", "Jason Wei." ],
      "venue" : "arXiv preprint arXiv:2109.03910.",
      "citeRegEx" : "Reif et al\\.,? 2021",
      "shortCiteRegEx" : "Reif et al\\.",
      "year" : 2021
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q. Nguyen", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712, Online. Association for Computational",
      "citeRegEx" : "Salazar et al\\.,? 2020",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2020
    }, {
      "title" : "Connotation frames of power and agency in modern films",
      "author" : [ "Maarten Sap", "Marcella Cindy Prasettio", "Ari Holtzman", "Hannah Rashkin", "Yejin Choi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Sap et al\\.,? 2017",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleurt: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur P Parikh." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6833–6844.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Imitation attacks and defenses for black-box machine translation systems",
      "author" : [ "Eric Wallace", "Mitchell Stern", "Dawn Xiaodong Song." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wallace et al\\.,? 2020",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2020
    }, {
      "title" : "FUDGE: Controlled text generation with future discriminators",
      "author" : [ "Kevin Yang", "Dan Klein." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Yang and Klein.,? 2021",
      "shortCiteRegEx" : "Yang and Klein.",
      "year" : 2021
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Fine-tuning language models from human preferences",
      "author" : [ "Daniel M Ziegler", "Nisan Stiennon", "Jeffrey Wu", "Tom B Brown", "Alec Radford", "Dario Amodei", "Paul Christiano", "Geoffrey Irving." ],
      "venue" : "arXiv preprint arXiv:1909.08593.",
      "citeRegEx" : "Ziegler et al\\.,? 2019",
      "shortCiteRegEx" : "Ziegler et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "If successful, effective controlled generation techniques might help mitigate bias and prevent generation of hate speech and toxic language (Yang and Klein, 2021; Xu et al.; Gehman et al., 2020).",
      "startOffset" : 140,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "If successful, effective controlled generation techniques might help mitigate bias and prevent generation of hate speech and toxic language (Yang and Klein, 2021; Xu et al.; Gehman et al., 2020).",
      "startOffset" : 140,
      "endOffset" : 194
    }, {
      "referenceID" : 23,
      "context" : "Much of the prior work on controlled generation has focused on autoregressive models like GPT-2 and has involved fine-tuning of these large models on target attributes (Yang and Klein, 2021; Krause et al., 2020), or training entirely separate probabilistic generative models for the target attributes (He et al.",
      "startOffset" : 168,
      "endOffset" : 211
    }, {
      "referenceID" : 9,
      "context" : "Much of the prior work on controlled generation has focused on autoregressive models like GPT-2 and has involved fine-tuning of these large models on target attributes (Yang and Klein, 2021; Krause et al., 2020), or training entirely separate probabilistic generative models for the target attributes (He et al.",
      "startOffset" : 168,
      "endOffset" : 211
    }, {
      "referenceID" : 6,
      "context" : ", 2020), or training entirely separate probabilistic generative models for the target attributes (He et al., 2020), or training specialized attribute models with a restricted structure to heuristically generate attribute-sensitive sequences (Dathathri et al.",
      "startOffset" : 97,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : ", 2020), or training specialized attribute models with a restricted structure to heuristically generate attribute-sensitive sequences (Dathathri et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "Specifically, we view the product of these blackbox experts as a probabilistic energy model (Hinton, 2002) – i.",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : ", a non-autoregressive, globally normalized language model – and then sample (without further training or fine-tuning) using a specialized Gibbs sampler with a Metropolis-Hastings correction step (Goyal et al., 2021).",
      "startOffset" : 196,
      "endOffset" : 216
    }, {
      "referenceID" : 1,
      "context" : ", sentiment classifiers), large bidirectional pretrained language models like BERT (Devlin et al., 2019), and other modules specializing in capturing desirable features pertaining to faithfulness to any additional context, like hamming distance, Bertscore distance (Zhang et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : ", 2019), and other modules specializing in capturing desirable features pertaining to faithfulness to any additional context, like hamming distance, Bertscore distance (Zhang et al., 2020), or Bleurt (Sellam et al.",
      "startOffset" : 168,
      "endOffset" : 188
    }, {
      "referenceID" : 20,
      "context" : ", 2020), or Bleurt (Sellam et al., 2020) based distance between the sample and the conditioning context.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "We generate samples from the energy language model assembled from these component experts by using the recently proposed Gibbs-MetropolisHastings scheme (Goyal et al., 2021) for sampling from energy models using a masked language",
      "startOffset" : 153,
      "endOffset" : 173
    }, {
      "referenceID" : 24,
      "context" : "Emlm(X) : Recent work has shown that large masked language models (MLM) like BERT can discirminate between well-formed and ill-formed sentences (Zhang et al., 2020) and induce an implicit energy function over the sequences (Goyal et al.",
      "startOffset" : 144,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : ", 2020) and induce an implicit energy function over the sequences (Goyal et al., 2021).",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "Efuzzy(X;X ′) : Similar to the hamming distance, this quantity refers to the Bertscore (Zhang et al., 2020) computed between X and X ′ which can be viewed as a fuzzy hamming distance that takes semantic similarity into account.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "EBleurt(X;X ′) : This energy refers to the negative Bleurt (Sellam et al., 2020) score between X and X ′.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "To sample from the energy parametrizations described in the last section, we follow the Metroplolis Hastings (Hastings, 1970) MCMC scheme for sampling from masked language models introduced by Goyal et al.",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "We use the subset of the ROC story corpus (Mostafazadeh et al., 2016) test-set that is used by PowerTransformer (Ma et al.",
      "startOffset" : 42,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : ", 2016) test-set that is used by PowerTransformer (Ma et al., 2020) for their evaluations.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "We use Yelp (Shen et al., 2017) dataset’s test-set for the task of sentiment transfer.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "We also have a reference set of hand written sentiment transferred sentences, provided by (He et al., 2020) that we use for reporting evaluation metrics.",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "Formality transfer: GYAFC We use 1051 sentences from the test-set of the GYAFC (Rao and Tetreault, 2018) dataset, which contains formal and informal sentences for the task of formality",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "Here we use the entertainment and music domain subset of this data, following the evaluation setup of (He et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "For experiments in which we add BLEURT (Sellam et al., 2020) and BertScore (Zhang et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : ", 2020) and BertScore (Zhang et al., 2020) components to the energy, we download the pre-trained Elron/bleurt-base-512 and robertalarge_L17 models from Huggingface, respectively.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "For the task of controllable debiasing (agency revision), we compare our work with PowerTransformer (Ma et al., 2020), an approach that uses paraphrasing and selfsupervision based on a reconstruction loss, building on pre-trained language models, to re-write text and control agency level of sentences.",
      "startOffset" : 100,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "As a second baseline for style transfer, we compare our work with UNMT (Lample et al., 2018), an unsupervised machine translation framework that demonstrates high performance for sentiment transfer.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "We feed our generated test sentences to a Huggingface (Radford et al., 2019) pre-trained GPT-2 xl model, and report its perplexity (PPL), as an automatic measure of fluency.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : "As a measure of meaning preservation, we use the F1 BertScore metric (Zhang et al., 2020)",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "For sentiment classification we use textattack/bert-base-uncasedyelp-polarity (Morris et al., 2020), and for formality we use cointegrated/robertabase-formality.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "The “Agency Score” variant adds an alternative term to EM&M instead of Edisc, which is the number of target agency verbs according to the connotation frames lexicon (Sap et al., 2017) in the sentence.",
      "startOffset" : 165,
      "endOffset" : 183
    }, {
      "referenceID" : 18,
      "context" : "Here, we have an additional metric, the MLM energy (lower is better), which, like GPT-2, indicates the quality of generated sentences (Salazar et al., 2020) according to BERT.",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : "Common approaches for flexible attribute-based generation range from retraining or fine-tuning a large underlying base model for generation on domain-specific data (Ziegler et al., 2019), to modifying the architecture of the large pre-trained model (Keskar et al.",
      "startOffset" : 164,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : ", 2019), to modifying the architecture of the large pre-trained model (Keskar et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "Several style transfer approaches hinge on training large generative models with non-parallel (He et al., 2020; Lample et al., 2018; Shen et al., 2017; Krishna et al., 2020; Reif et al., 2021) data across the domains of interest.",
      "startOffset" : 94,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "Several style transfer approaches hinge on training large generative models with non-parallel (He et al., 2020; Lample et al., 2018; Shen et al., 2017; Krishna et al., 2020; Reif et al., 2021) data across the domains of interest.",
      "startOffset" : 94,
      "endOffset" : 192
    }, {
      "referenceID" : 21,
      "context" : "Several style transfer approaches hinge on training large generative models with non-parallel (He et al., 2020; Lample et al., 2018; Shen et al., 2017; Krishna et al., 2020; Reif et al., 2021) data across the domains of interest.",
      "startOffset" : 94,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : "Several style transfer approaches hinge on training large generative models with non-parallel (He et al., 2020; Lample et al., 2018; Shen et al., 2017; Krishna et al., 2020; Reif et al., 2021) data across the domains of interest.",
      "startOffset" : 94,
      "endOffset" : 192
    }, {
      "referenceID" : 17,
      "context" : "Several style transfer approaches hinge on training large generative models with non-parallel (He et al., 2020; Lample et al., 2018; Shen et al., 2017; Krishna et al., 2020; Reif et al., 2021) data across the domains of interest.",
      "startOffset" : 94,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "Instead of retraining large base models or training new architectures from scratch, recent work has used attribute discriminators to steer the generation (Gu et al., 2017) from a large autoregressive language model.",
      "startOffset" : 154,
      "endOffset" : 171
    }, {
      "referenceID" : 9,
      "context" : "GeDi (Krause et al., 2020) and Fudge (Yang and Klein, 2021) take similar, approaches and guide",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 23,
      "context" : ", 2020) and Fudge (Yang and Klein, 2021) take similar, approaches and guide",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "We do however acknowledge that strong controlled generation methods that rely on discriminators can have the potential to regurgitate the training data and produce harmful outputs and toxic language (Xu et al.; Gehman et al., 2020; Wallace et al., 2020).",
      "startOffset" : 199,
      "endOffset" : 253
    }, {
      "referenceID" : 22,
      "context" : "We do however acknowledge that strong controlled generation methods that rely on discriminators can have the potential to regurgitate the training data and produce harmful outputs and toxic language (Xu et al.; Gehman et al., 2020; Wallace et al., 2020).",
      "startOffset" : 199,
      "endOffset" : 253
    } ],
    "year" : 0,
    "abstractText" : "Due to the unidirectional nature of prevalent autoregressive generation models, recent work on controlled generation based on global text attributes has either required attribute-based finetuning of the base language model, or restricted the parametrization of the attribute prediction model to be compatible with the base LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pretrained blackbox models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the blackbox models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from blackbox models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",
    "creator" : null
  }
}
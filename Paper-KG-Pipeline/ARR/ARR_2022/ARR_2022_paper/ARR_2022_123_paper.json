{
  "name" : "ARR_2022_123_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Empirical Study of Document-to-document Neural Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) has achieved great progress and reached near human-level performance. However, most current sequence-tosequence NMT models translate sentences individually. In such cases, discourse phenomena, such as pronominal anaphora, lexical consistency, and document coherence that depend on long-range context going further than a few previous sentences, are neglected (Bawden et al., 2017). As a result, Läubli et al. (2018) find human raters still show a markedly stronger preference for human translations when evaluating at the level of documents.\nMany methods have been proposed to improve document-level neural machine translation (DNMT). Among them, the mainstream works focus on the model architecture modification, including hierarchical attention (Wang et al., 2017; Miculicich et al., 2018; Tan et al., 2019), additional context extraction encoders or query layers (Jean\net al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al., 2018; Tu et al., 2018).\nThese studies come up with different structures in order to include discourse information, specifically speaking, introducing adjacent sentences into the encoder or decoder as document contexts. Experimental results show effective improvements on universal translation metrics like BLEU (Papineni et al., 2002) and document-level linguistic indices (Tiedemann and Scherrer, 2017; Bawden et al., 2017; Werlen and Popescu-Belis, 2017; Müller et al., 2018; Voita et al., 2018, 2019).\nUnlike previous works, this paper does not aim at introducing a novel method. Instead, we hope to answer the following question: Is the basic sequenceto-sequence model strong enough to directly handle document-level translation? To this end, we head back to the original Transformer and conduct literal document-to-document (Doc2Doc) training. We leverage the full document information, making the model capture the full context in both source and target sides.\nThough many studies report less promising results of naive Doc2Doc translation (Zhang et al., 2018; Liu et al., 2020), we successfully activate it with Multi-resolutional Training, which involves multiple levels of sequences. It turns out that endto-end document translation is not only feasible but also better functioning than sentence-level models and previous works. Furthermore, if assisted by extra sentence-level corpus, which can be much more easily obtained, the model can significantly improve the translation performance and achieve state-of-the-art results. It is worth noting that we do not change the model architecture and need no extra parameters.\nOur experiments are conducted on nine\ndocument-level datasets, including TED (ZH-EN, EN-DE), News (EN-DE, ES-EN, FR-EN, RU-EN), Europarl (EN-DE), Subtitles (EN-RU), and a newly constructed News dataset (ZH-EN). Additionally, two sentence-level datasets are adopted in further experiments, including Wikipedia (EN-DE) and WMT (ZH-EN). Experiment results show that our strategy outperforms previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation. In addition to serving as improvement evidence, our newly proposed document-level datasets and metrics can also be a boosting contribution to the community."
    }, {
      "heading" : "2 Doc2Doc: End-to-End DNMT",
      "text" : "In this section, we attempt to analyze the different training patterns for DNMT. Firstly, let us formulate the problem. Let Dx = {x(1), x(2), · · · , x(M)} be a source-language document containing M source sentences. The goal of the document-level NMT is to translate the document Dx in language x to a document Dy in language y. Dy = {y(1), y(2), · · · , y(N)}. We use L (i) y to denote the sentence length of y(i). Previous works translate a document sentenceby-sentence, regarding DNMT as a step-by-step sentence generating problem (Doc2Sent) as:\nLDoc2Sent = − N∑ i=1 L (i) y∑ j=1 log pθ(y (i) j |y (i) (<j), x (i), S(i), T (i)),\n(1)\nS(i) is the context in the source side, depending on the model architecture and is comprised of only two or three sentences in many works. Most current works focus on S(i), by utilizing hierarchical attention or extra encoders. And T (i) is the context in the target side, which is involved by only a couple of works. They usually make use of a topic model or word cache to form T (i).\nDifferent from Doc2Sent, we propose to resolve document translation with the end-to-end, namely document-to-document (Doc2Doc) pattern as:\nLDoc2Doc = − ∑ Ly∑ i=1 log pθ(yi|y<i, Dx), (2)\nwhere Dx is the complete context in the source side, and y<i is the complete historical context in the target side.\n2.1 Why We Dive into Doc2Doc? Full Source Context: Firstly, though some Doc2Sent works utilize a full source-side context (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), some studies show that more sentences beyond can harm the results (Miculicich et al., 2018; Zhang et al., 2018; Tu et al., 2018). Therefore, many works of Doc2Sent are more of “a couple of sentences to sentence” since they only involve two or three preceding sentences as context. However, broader contexts provide more information, which shall lead to more improvements. We attempt to re-visit involving the full context. Correspondingly, we pick Doc2Doc, as it is required to take account of all the source-side context.\nFull Target Context: Secondly, though some Doc2Sent works utilize a full target-side context (Maruf and Haffari, 2018; Zheng et al., 2020), many previous works abandon the target-side historical context, and some even claim that it is harmful to translation quality (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018). Whether to utilize target-side contexts is controversial for Doc2Sent. However, once the cross-sentence language model is discarded, some problems, such as tense mismatch (especially when the source language is tenseless like Chinese), may occur. We attempt to re-visit involving the full context. Correspondingly, we pick Doc2Doc, as it treats the whole document as a sequence and can naturally take advantage of all the target-side historical context.\nLoose Training: Thirdly, Doc2Sent restricts the training scene. The previous works focus on adjusting the model structure to feed preceding source sentences, so the training data has to be in the form of consecutive sentences so as to meet the model entrance. As a result, it is hard to use large numbers of piecemeal parallel sentences. Such a rigid form of training data also greatly hinders the model potential because the scale of parallel sentences can be tens of times of parallel documents. On the contrary, Doc2Doc can naturally absorb all kinds of sequences, including sentences and documents.\nSimplicity: Lastly, Doc2Sent inevitably introduces extra model modules with extra parameters in order to capture contextual information. It complicates the model architecture, making it hard to renovate or generalize. On the contrary, Doc2Doc does not change the model structure and brings in no additional parameters."
    }, {
      "heading" : "2.2 Multi-resolutional Doc2Doc NMT",
      "text" : "Although Doc2Doc seems more concise and promising in multiple terms, it is not widely recognized. Zhang et al. (2018); Liu et al. (2020) conduct experiments by directly feeding the whole documents into the model. We refer to it as Singleresolutional Training (denoted as SR Doc2Doc). Their experiments report extremely negative results unless pre-trained in advance. The model either has a large drop in performance or does not work at all. As pointed out by Koehn and Knowles (2017), one of the six challenges in neural machine translation is the dramatic drop of quality as the length of the sentences increases.\nHowever, we find that Doc2Doc can be activated on any datasets and obtain better results than Doc2Sent models as long as we employ Multiresolutional Training, mixing documents with shorter segments like sentences or paragraphs (denoted as MR Doc2Doc).\nSpecifically, we split each document averagely into k parts multiple times and collect all the sequences together, k ∈ {1, 2, 4, 8, ...}. For example, a document containing eight sentences will be split into two four-sentences segments, four two-sentences segments, and eight single-sentence segments. Finally, fifteen sequences are all gathered and fed into sequence-to-sequence training (15 = 1 + 2 + 4 + 8).\nIn this way, the model can acquire the ability to translate long documents since it is assisted by easier and shorter segments. As a result, multiresolutional Doc2Doc is able to translate all forms of sequences, including extremely long ones such as a document with more than 2000 tokens, as well as shorter ones like sentences. In the following sections, we conduct the same experiments as the aforementioned studies by translating the whole document directly and atomically."
    }, {
      "heading" : "3 Experiment Settings",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "For our main experiments, we follow the datasets provided by Maruf et al. (2019) and Zheng et al. (2020), including TED (ZH-EN/EN-DE), News (EN-DE), and Europarl (EN-DE). The ChineseEnglish and English-German TED datasets are from IWSLT 2015 and 2017 evaluation campaigns, respectively. For ZH-EN, we use dev2010 as the development set and tst2010-2013 as the test set. For TED (EN-DE), we use tst2016-2017 as the test set and the rest as the development set. For News (EN-DE). the training/develop/test sets are: News Commentary v11, WMT newstest2015, and WMT newstest2016. For Europarl (EN-DE). The corpus is extracted from the Europarl v7 according to the method proposed in Maruf et al. (2019). 1\nExperiments on Spanish, French, Russian to English are also conducted, whose training sets are News Commentary v14 , with the development sets and test sets are newstest2012 / newstest2013 (ESEN), newstest2013 / newstest2014 (FR-EN), newstest2018 / newstest2019 (RU-EN), respectively.\nBesides, two additional sentence-level datasets are also adopted. For EN-DE, we use Wikipedia , a corpus containing 2.4 million pairs of sentences. For ZH-EN, we extract one-tenth of WMT 2019 , around 2 million sentence pairs.\nAdditionally, a document-level dataset with contrastive test sets in EN-RU (Voita et al., 2019) is used to evaluate lexical coherence.\nLastly, we propose a new document-level dataset in this paper, whose source, scales, and benchmark will be illustrated in the subsequent sections.\nFor sentences without any ending symbol inside documents, periods are manually added. For our Doc2Doc experiments, the development and test sets are documents merged by sentences. We list\n1EN-DE datasets are from https://github.com/ sameenmaruf/selective-attn\nall the detailed information of used datasets in Table 1, including languages, scales, and downloading URLs for reproducibility."
    }, {
      "heading" : "3.2 Models",
      "text" : "For the model setting, we follow the base version of Transformers (Vaswani et al., 2017), including 6 layers for both encoders and decoders, 512 dimensions for model, 2048 dimensions for ffn layers, 8 heads for attention. For all experiments, we use subword (Sennrich et al., 2016) with 32K merge operations on both sides and cut out tokens appearing less than five times. The models are trained with a batch size of 32000 tokens on 8 Tesla V100 GPUs. Parameters are optimized by using Adam optimizer (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98, and = 10−9. The learning rate is scheduled according to the method proposed in Vaswani et al. (2017), with warmup_steps = 4000. Label smoothing (Szegedy et al., 2016) of value=0.1 is also adopted. We set dropout = 0.3 for small datasets like TED and News, and dropout = 0.1 for larger datasets like Europarl, unless stated elsewise. Besides, we use Horovod library with RDMA inter-GPU communication (Sergeev and Balso, 2018)."
    }, {
      "heading" : "3.3 Evaluation",
      "text" : "For inference, we generate the translation hypothesis with a beam size of 5. Following previous related works, we adopt tokenized case-insensitive BLEU (Papineni et al., 2002). Specifically, we follow the methods in Liu et al. (2020), which calculate sentence-level BLEU (denoted as s-BLEU)\nand document-level BLEU (denoted as d-BLUE), respectively. For d-BLEU, the computing object is either the concatenation of generated sentences or the directly generated documents. Since our documents are generated atomically and hard to split into sentences, we only report d-BLEU for Doc2Doc."
    }, {
      "heading" : "3.4 Roadmap",
      "text" : "To answer the main question in Section 1, we propose three more detailed questions and organize corresponding experiments, as follows:\n1. Is Doc2Doc translation really feasible and effective? 2. Does the usage of the additional sentencelevel corpus help? 3. Does Doc2Doc truly take advantage of the context and improve the document-level consistency like lexical coherence?"
    }, {
      "heading" : "4 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 MR Doc2Doc Improves Performance",
      "text" : "It can be seen from the upper part of Table 2 that SR Doc2Doc indeed has a severe drop on News and even fails to generate normal results on TED, which accords with the findings of Zhang et al. (2018); Liu et al. (2020). It seems too hard for seq2seq models to learn long-range document translation directly.\nHowever, once equipped with our training technique, MR Doc2Doc can yield the best results, outperforming our strong baseline and previous works on TED and Europarl. We suggest that NMT is able\nto acquire the capacity of translating long-range context, as long as it cooperates with some shorter segments as assistance. With the multi-resolutional help of easier patterns, the model can gradually master how to generate complicated sequences.\nTo show the universality of MR Doc2Doc, we also conduct the experiments on other language pairs: Spanish, French, Russian to English. As shown in Table 3, MR Doc2Doc can be successfully achieved on all language pairs and obtains comparable or better results compared with Sent2Sent.\nIt is worth noting that all our results are obtained without any adjustment of model architecture or any extra parameters."
    }, {
      "heading" : "4.2 Additional Sentence Corpus Helps",
      "text" : "Furthermore, introducing extra sentence-level corpus is also an effective technique. This can be regarded as another form of multi-resolutional training, as it supplements more sentence-level information. This strategy makes an impact in two ways: activating SR Doc2Doc and boosting MR Doc2Doc.\nWe merge the datasets mentioned above and Wikipedia (EN-DE), WMT (ZH-EN), two out-ofdomain sentence-level datasets to do experiments. 2\nAs shown in the lower part of Table 2, on the one hand, SR Doc2Doc models are activated and can reach comparable levels with Sent2Sent models as long as assisted with additional sentences. On the other hand, MR Doc2Doc obtains the best results on all datasets and further widens the gap with the sentence corpus’s boost. Even out-of-domain sentences can leverage the learning ability of document translation. It again proves the importance of multi-resolutional assistance.\nIn addition, as analyzed in the previous section, Doc2sent models are not compatible with\n2Sentences and documents in non-MR settings are oversampled for six times to keep the same data ratio with the MR settings, which is proved helpful to the performance in Appendix A. Due to the larger scale, we find the settings of dropout=0.2 for TED, News and dropout=0.1 for Europarl yield the best results for both Sent2Sent and Doc2Doc.\nsentence-level corpus since the model entrance is specially designed for consecutive sentences. However, Doc2Doc models can naturally draw on the merits of any parallel pairs, including piecemeal sentences. Considering the amount of parallel sentence-level data is much larger than the document-level one, MR Doc2Doc has a powerful application potential compared with Doc2Sent."
    }, {
      "heading" : "4.3 Further Analysis on MR Doc2Doc",
      "text" : ""
    }, {
      "heading" : "4.3.1 Improved Discourse Coherence",
      "text" : "Except for BLEU, whether Doc2Doc truly learns to utilize the context to resolve discourse inconsistencies has to be verified. We use the contrastive test sets proposed by Voita et al. (2019), which include deixis, lexicon consistency, ellipsis (inflection), and ellipsis (verb phrase) on English-Russian. Each instance contains a positive translation and a few negative ones, whose difference is only one specific word. With force decoding, if the score of the positive one is the highest, then this instance is counted as correct.\nAs shown in Table 4, MR Doc2Doc achieves significant improvements and obtain the best results, which proves MR Doc2Doc indeed well captures the context information and maintain the crosssentence coherence."
    }, {
      "heading" : "4.3.2 Strong Context Sensibility",
      "text" : "Li et al. (2020) find the performance of previous context-aware systems does not decrease with intentional incorrect context and suspect the context usage of context encoders. To verify whether Doc2Doc truly takes advantage of the contextual information in the document, we also conduct the inference with the wrong context deliberately. If the model neglects discourse dependency, then there should be no difference in the performance.\nSpecifically, we firstly shuffle the sentence order inside each document randomly, marking it as Local Shuffle. Furthermore, we randomly swap sentences among all the documents to make the context more disordered, marking it as Global Shuffle. As shown in Table 5, the misleading context results\nin a significant drop for the Doc2Doc model in BLEU. Besides, Global Shuffle brings more harm than Local Shuffle, showing that more chaotic contexts lead to more harm. After all, Local Shuffle still reserves some general information, like topic or tense. These experiments prove the usage of the context."
    }, {
      "heading" : "4.3.3 Compatible with Sentences",
      "text" : "The performance with sequence length is also analyzed in this study. Taking Europarl as an example, we randomly split documents into shorter paragraphs in different lengths and evaluate them with our models, as shown in Figure 1. Obviously, the model trained only on sentence-level corpus has a severe drop when translating long sequences, while the model trained only on document-level corpus shows the opposite result, which reveals the importance of data distribution. However, the model trained with our multi-resolutional strategy can sufficiently cope with all situations, breaking the limitation of sequence length in translation. By conducting MR Doc2Doc, we obtain an all-in-one model that is capable of translating sequences of any length, avoiding deploying two systems for sentences and documents, respectively."
    }, {
      "heading" : "5 Further Evidence with Newly Proposed Datasets and Metrics",
      "text" : "To further verify our conclusions and push the development of this field, we also contribute a new dataset along with new metrics. Specifically, we propose a package of a large and diverse parallel document corpus, three deliberately designed metrics, and correspondingly constructed test sets, which will be released soon. On the one hand, they make our conclusions more solid. On the other hand, they may benefit future researches to expand the comparison scenes."
    }, {
      "heading" : "5.1 Parallel Document Corpus",
      "text" : "We crawl bilingual news corpus from two websites3 4 with both English and Chinese content provided. The detailed cleaning procedure is in Appendix B. Finally, 1.39 million parallel sentences within almost 60 thousand parallel documents are collected. The corpus contains large-scale data with internal dependency in different lengths and diverse domains, including politics, finance, health, culture, etc. We name it PDC (Parallel Document Corpus)."
    }, {
      "heading" : "5.2 Metrics",
      "text" : "To inspect the coherence improvement, we sum up three common linguistic features in document corpus that the Sent2Sent model can not handle:\nTense Consistency (TC): If the source language is tenseless (e.g. Chinese), it is hard for Sent2Sent models to maintain the consistency of tense.\nConjunction Presence (CP): Traditional models ignore cross-sentence dependencies, and the sentence-level translation may cause the missing of conjunctions like “And” (Xiong et al., 2018).\nPronoun Translation (PT): In pro-drop languages such as Chinese and Japanese, pronouns are frequently omitted. When translating from a prodrop language into a non-pro-drop language (e.g., Chinese-to-English), invisible dropped pronouns may be missing (Wang et al., 2016b,a, 2018a,b).\nAfterward, we collect documents that contain abundant verbs in the past tense, conjunctions, and pronouns, as test sets. These words, as well as their positions, are labeled. Some cases are in Appendix C.\nFor each word-position pair < w, p >, we check whether w appears in the generated documents\n3https://cn.nytimes.com 4https://cn.ft.com\nwithin a rough span. And we calculate the appearance percentage as the evaluation score, Specifically:\nTC / CP / PT =\n∑n i ∑|Wi| j I(wij ∈ y\nspan i )∑n\ni |Wi| (3)\nspan = [αipij − d, αipij + d] (4)\nn indicates the number of sequences in the test set, Wi indicates the labeled word set of sequencei, w indicates labeled words, yi indicates outputi, pij indicates the labeled position of wij in the referencei, αi indicates the length ratio of translation and reference, d indicates the span radius. We set d = 20 in this paper, and calculate the geometric mean as the overall score denoted as TCP."
    }, {
      "heading" : "5.3 Test Sets",
      "text" : "Along with the filtration of the aforementioned coherence indices, the test sets are built based on websites that are totally different from the training corpus to avoid overfitting. Meanwhile, to alleviate the bias of human translation, the English documents are selected as the reference and manually translated to the Chinese documents as the source. Finally, a total of nearly five thousand sentences within 148 documents is obtained."
    }, {
      "heading" : "5.3.1 Benchmark",
      "text" : "Basic experiments with Sent2Sent and Doc2Doc are conducted based on our new datasets, along with full WMT ZH-EN corpus, a sentence-level dataset containing around 20 million pairs. 5 We use WMT newstest2019 as the development set and evaluate the models with our new test sets as well as metrics. The results are shown in Table 6.\n5We set dropout=0.2 for Sent2Sent and MR Doc2Doc without WMT, and dropout=0.1 for the rest settings according to the performance on the development set. Oversampling is done again, as aforementioned, to enhance the performance for non-MR settings.\nBLEU: In terms of BLEU, MR Doc2Doc outperforms Sent2Sent, illustrating the positive effect of long-range context. Moreover, with extra sentencelevel corpus, Doc2Doc shows significant improvements again.\nFine-grained Metrics: Our metrics show much clearer improvements. Considering the usage of contextual information, tense consistency is better guaranteed with Doc2Doc. Meanwhile, Doc2Doc is much more capable of translating the invisible pronouns by capturing original referent beyond the current sentence. Finally, the conjunction presence shows the same tendency.\nHuman Evaluation: Human evaluation is also conducted to illustrate the reliability of our metrics. One-fifth of translated documents are sampled and scored by linguistics experts from 1 to 5 according to not only translation quality but also translation consistency. As shown in Table 6, human evaluation shows a strong correlation with TCP. More specifically, the Pearson Correlation Coefficient (PCCs) between human scores and TCP is higher than that of BLEU (97.9 vs. 94.1)."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "Table 7 shows an example of document translation. Sent2Sent model neglects the cross-sentence context and mistakenly translate the ambiguous word, which leads to a confusing reading experience. However, the Doc2Doc model can grasp a full picture of the historical context and make accurate decisions.\nAlso, we manually switch the context information in the source side to test the model sensibility, as shown in Table 8. It turns out that Doc2Doc is able to adapt to different contexts."
    }, {
      "heading" : "6 Limitation",
      "text" : "Though multi-resolutional Doc2Doc achieves direct document translation and obtains better results, there still exists a big challenge: efficiency. The computation cost of self-attention in Transformer rises with the square of the sequence length. As we feed the entire document into the model, the memory usage will be a bottleneck for larger model deployment. And the inference speed may be affected if no parallel operation is conducted. Recently, many studies focus on the efficiency enhancement on long-range sequence processing (Correia et al., 2019; Child et al., 2019; Kitaev et al., 2020; Wu et al., 2020; Beltagy et al., 2019; Rae et al., 2020). We leave reducing the computation cost to the future work."
    }, {
      "heading" : "7 Related Work",
      "text" : "Document-level neural machine translation is an important task and has been abundantly studied with multiple datasets as well as methods.\nThe mainstream research in this field is the model architecture improvement. Specifically, several recent attempts extend the Sent2Sent approach to the Doc2Sent-like one. Wang et al. (2017); Miculicich et al. (2018); Tan et al. (2019) make use of hierarchical RNNs or Transformer to summarize previous sentences. Jean et al. (2017); Bawden et al. (2017); Zhang et al. (2018); Voita et al. (2018); Kuang and Xiong (2018); Maruf et al. (2019); Yang et al. (2019); Jiang et al. (2019); Zheng et al. (2020); Yun et al. (2020); Xu et al. (2020) introduce additional encoders or query layers with attention model and feed the history contexts into decoders. Maruf and Haffari (2018); Kuang et al. (2018); Tu et al. (2018) propose to augment NMT models with a cache-like memory network, which generates the translation depending on the decoder history retrieved from the memory.\nBesides, some works intend to resolve this problem in other ways. Jean and Cho (2019) propose a regularization term for encouraging to focus more on the additional context using a multi-level pair-\nwise ranking loss. Yu et al. (2020) utilize a noisy channel reranker with Bayes’ rule. Garcia et al. (2019) extends the beam search decoding process with fusing an attentional RNN with an SSLM by modifying the computation of the final score. Saunders et al. (2020) present an approach for structured loss training with document-level objective functions. Liu et al. (2020); Ma et al. (2020) combine large-scale pre-train model with DNMT. Unanue et al. (2020); Kang et al. (2020) adopt reinforcement learning methods.\nThere are also some works sharing similar ideas with us. Tiedemann and Scherrer (2017); Bawden et al. (2017) explore concatenating two consecutive sentences and generate two sentences directly. Obviously, we leverage greatly longer information and capture the full context. Junczys-Dowmunt (2019) cut documents into long segments and feed them into training like BERT (Devlin et al., 2019). There are at least three main differences. Firstly, they need to add specific boundary tokens between sentences while we directly translate the original documents without any additional processing. Secondly, we propose a novel multi-resolutional training paradigm that shows consistent improvements compared with regular training. Thirdly, for extremely long documents, they restrict the segment length to 1000 tokens or make a truncation while we preserve entire documents and achieve literal document-to-document training and inference.\nFinally, our work is also related to a series of studies in long sequence generation like GPT (Radford, 2018), GPT-2 (Radford et al., 2019), and Transformer-XL (Dai et al., 2019). We all suggest that the deep neural generation models have the potential to well process long-range sequences."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we try to answer the question of whether Document-to-document translation works. It seems naive Doc2Doc can fail in multiple scenes. However, with the multi-resolutional training proposed in this paper, it can be successfully activated. Different from traditional methods of modifying the model architectures, our approach introduces no extra parameters. A comprehensive set of experiments on various metrics show the advantage of MR Doc2Doc. In addition, we contribute a new document-level dataset as well as three new metrics to the community."
    }, {
      "heading" : "A Oversampling Illustration",
      "text" : "When combining document-level datasets with sentence-level datasets (especially out-of-domain corpus), we employ oversampling for non-MR settings. This can keep them the same data ratio with the MR setting and is helpful for their performance. Since the data size of MR is around 6 times of non-MR (≈ log2 64), as shown in Table 9, we mainly oversample for 6 times. The contrastive experiments are in Table 10. We attribute the improvements to the reduction of the proportion of out-of-domain data."
    }, {
      "heading" : "B Clean Procedure on PDC",
      "text" : "We mainly crawl bilingual news corpus from two websites (https://cn.nytimes.com, https://cn.ft.com) with both English and Chinese content provided. Then three steps are followed to clean the corpus.\n1. Deduplication: We deduplicate the documents that include almost the same content. 2. Sentence Segmentation: We use Pragmatic Segmenter 6 to segment paragraphs into sentences. 3. Filtration: We use fast_align 7 to align sentence pairs and label the pairs as misaligned ones if the alignment scores are less than 40%. Documents are finally removed if they contain misaligned sentence pairs.\n6https://github.com/diasks2/pragmatic_ segmenter\n7https://github.com/clab/fast_align\nFinally, we obtain 1.39 million parallel sentences within almost 60 thousand cleaned parallel documents. The dataset contains diverse domains including politics, finance, health, culture, etc."
    }, {
      "heading" : "C Cases of Our Test Sets",
      "text" : "Apart from the statistic number in the main paper, we also provide some cases in our test sets to illustrate the value of our test sets and metrics, as shown in Table 11,12,13."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Bawden et al\\.,? 2017",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2017
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv, abs/2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating long sequences with sparse transformers",
      "author" : [ "Rewon Child", "Scott Gray", "Alec Radford", "Ilya Sutskever." ],
      "venue" : "arXiv, abs/1904.10509.",
      "citeRegEx" : "Child et al\\.,? 2019",
      "shortCiteRegEx" : "Child et al\\.",
      "year" : 2019
    }, {
      "title" : "Adaptively sparse transformers",
      "author" : [ "Gonçalo M Correia", "Vlad Niculae", "André FT Martins." ],
      "venue" : "EMNLP-IJCNLP.",
      "citeRegEx" : "Correia et al\\.,? 2019",
      "shortCiteRegEx" : "Correia et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G. Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov." ],
      "venue" : "ACL.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-aware neural machine translation decoding",
      "author" : [ "Eva Martínez Garcia", "C. Creus", "C. España-Bonet." ],
      "venue" : "DiscoMT@EMNLP.",
      "citeRegEx" : "Garcia et al\\.,? 2019",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextaware learning for neural machine translation",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho." ],
      "venue" : "arXiv, abs/1903.04715.",
      "citeRegEx" : "Jean and Cho.,? 2019",
      "shortCiteRegEx" : "Jean and Cho.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation for cross-lingual pronoun prediction",
      "author" : [ "Sébastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho." ],
      "venue" : "DiscoMT@EMNLP.",
      "citeRegEx" : "Jean et al\\.,? 2017",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2017
    }, {
      "title" : "Document-level neural machine translation with inter-sentence attention",
      "author" : [ "Shu Jiang", "Rui Wang", "Zuchao Li", "Masao Utiyama", "Kehai Chen", "Eiichiro Sumita", "Hai Zhao", "BaoLiang Lu." ],
      "venue" : "arXiv, abs/1910.14528.",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft translator at wmt 2019: Towards large-scale document-level neural machine translation",
      "author" : [ "Marcin Junczys-Dowmunt." ],
      "venue" : "WMT.",
      "citeRegEx" : "Junczys.Dowmunt.,? 2019",
      "shortCiteRegEx" : "Junczys.Dowmunt.",
      "year" : 2019
    }, {
      "title" : "Dynamic context selection for document-level neural machine translation via reinforcement learning",
      "author" : [ "Xiaomian Kang", "Yang Zhao", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kang et al\\.,? 2020",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederick P Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Six challenges for neural machine translation",
      "author" : [ "Philipp Koehn", "Rebecca Knowles." ],
      "venue" : "NMT@ACL.",
      "citeRegEx" : "Koehn and Knowles.,? 2017",
      "shortCiteRegEx" : "Koehn and Knowles.",
      "year" : 2017
    }, {
      "title" : "Fusing recency into neural machine translation with an intersentence gate model",
      "author" : [ "Shaohui Kuang", "Deyi Xiong." ],
      "venue" : "COLING.",
      "citeRegEx" : "Kuang and Xiong.,? 2018",
      "shortCiteRegEx" : "Kuang and Xiong.",
      "year" : 2018
    }, {
      "title" : "Modeling coherence for neural machine translation with dynamic and topic caches",
      "author" : [ "Shaohui Kuang", "Deyi Xiong", "Weihua Luo", "Guodong Zhou." ],
      "venue" : "COLING.",
      "citeRegEx" : "Kuang et al\\.,? 2018",
      "shortCiteRegEx" : "Kuang et al\\.",
      "year" : 2018
    }, {
      "title" : "Has machine translation achieved human parity? a case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "Does multi-encoder help? a case study on contextaware neural machine translation",
      "author" : [ "Bei Li", "Hui Liu", "Ziyang Wang", "Yufan Jiang", "Tong Xiao", "Jingbo Zhu", "Tongran Liu", "Changliang Li." ],
      "venue" : "ACL.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xiongmin Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "arXiv, abs/2001.08210.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple and effective unified encoder for documentlevel machine translation",
      "author" : [ "Shuming Ma", "Dongdong Zhang", "Ming Zhou." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Document context neural machine translation with memory networks",
      "author" : [ "Sameen Maruf", "Gholamreza Haffari." ],
      "venue" : "ACL.",
      "citeRegEx" : "Maruf and Haffari.,? 2018",
      "shortCiteRegEx" : "Maruf and Haffari.",
      "year" : 2018
    }, {
      "title" : "Selective attention for context-aware neural machine translation",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza\" Haffari" ],
      "venue" : null,
      "citeRegEx" : "Maruf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation",
      "author" : [ "Mathias Müller", "Annette Rios Gonzales", "Elena Voita", "Rico Sennrich." ],
      "venue" : "WMT.",
      "citeRegEx" : "Müller et al\\.,? 2018",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford" ],
      "venue" : null,
      "citeRegEx" : "Radford.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Compressive transformers for long-range sequence modelling",
      "author" : [ "Jack W Rae", "Anna Potapenko", "Siddhant M Jayakumar", "Chloe Hillier", "Timothy P Lillicrap." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Rae et al\\.,? 2020",
      "shortCiteRegEx" : "Rae et al\\.",
      "year" : 2020
    }, {
      "title" : "Using context in neural machine translation training objectives",
      "author" : [ "Danielle Saunders", "Felix Stahlberg", "Bill Byrne." ],
      "venue" : "ACL.",
      "citeRegEx" : "Saunders et al\\.,? 2020",
      "shortCiteRegEx" : "Saunders et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Horovod: fast and easy distributed deep learning in TensorFlow",
      "author" : [ "Alexander Sergeev", "Mike Del Balso." ],
      "venue" : "arXiv, abs/1802.05799.",
      "citeRegEx" : "Sergeev and Balso.,? 2018",
      "shortCiteRegEx" : "Sergeev and Balso.",
      "year" : 2018
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical modeling of global context for document-level neural machine translation",
      "author" : [ "Xin Tan", "Longyin Zhang", "Deyi Xiong", "Guodong Zhou." ],
      "venue" : "EMNLP-IJCNLP.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "DiscoMT@EMNLP.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Learning to remember translation history with a continuous cache",
      "author" : [ "Zhaopeng Tu", "Yang P. Liu", "Shuming Shi", "Tong Zhang." ],
      "venue" : "TACL.",
      "citeRegEx" : "Tu et al\\.,? 2018",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2018
    }, {
      "title" : "Leveraging discourse rewards for document-level neural machine translation",
      "author" : [ "Inigo Jauregi Unanue", "Nazanin Esmaili", "Gholamreza Haffari", "Massimo Piccardi." ],
      "venue" : "COLING.",
      "citeRegEx" : "Unanue et al\\.,? 2020",
      "shortCiteRegEx" : "Unanue et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "ACL.",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-aware neural machine translation learns anaphora resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "ACL.",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "Translating pro-drop languages with reconstruction models",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Shuming Shi", "Tong Zhang", "Yvette Graham", "Qun Liu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wang et al\\.,? 2018a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting cross-sentence context for neural machine translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to jointly translate and predict dropped pronouns with a shared reconstruction mechanism",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2018b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "A novel approach to dropped pronoun translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Xiaojun Zhang", "Hang Li", "Andy Way", "Qun Liu." ],
      "venue" : "NAACLHLT.",
      "citeRegEx" : "Wang et al\\.,? 2016a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropped pronoun generation for dialogue machine translation",
      "author" : [ "Longyue Wang", "Xiaojun Zhang", "Zhaopeng Tu", "Hang Li", "Qun Liu." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Wang et al\\.,? 2016b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Validation of an automatic metric for the accuracy of pronoun translation (apt)",
      "author" : [ "Lesly Miculicich Werlen", "Andrei Popescu-Belis." ],
      "venue" : "DiscoMT@EMNLP.",
      "citeRegEx" : "Werlen and Popescu.Belis.,? 2017",
      "shortCiteRegEx" : "Werlen and Popescu.Belis.",
      "year" : 2017
    }, {
      "title" : "Lite transformer with long-short range attention",
      "author" : [ "Zhanghao Wu", "Zhijian Liu", "Ji Lin", "Yujun Lin", "Song Han." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling coherence for discourse neural machine translation",
      "author" : [ "Hao Xiong", "Zhongjun He", "Hua Wu", "Haifeng Wang." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Xiong et al\\.,? 2018",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2018
    }, {
      "title" : "Efficient context-aware neural machine translation with layer-wise weighting and input-aware gating",
      "author" : [ "Hongfei Xu", "Deyi Xiong", "Josef van Genabith", "Qiuhui Liu." ],
      "venue" : "DiscoMT@IJCAI.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing context modeling with a query-guided capsule network for document-level nmt",
      "author" : [ "Zhengxin Yang", "Jinchao Zhang", "Fandong Meng", "Shuhao Gu", "Yang Feng", "Jie Zhou." ],
      "venue" : "EMNLPIJCNLP.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Better document-level machine translation with bayes’ rule",
      "author" : [ "Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer." ],
      "venue" : "TACL.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving context-aware neural machine translation using self-attentive sentence embedding",
      "author" : [ "Hyeongu Yun", "Yongkeun Hwang", "Kyomin Jung." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Yun et al\\.,? 2020",
      "shortCiteRegEx" : "Yun et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving the transformer translation model with document-level context",
      "author" : [ "Jiacheng Zhang", "Huanbo Luan", "Maosong Sun", "Feifei Zhai", "Jingfang Xu", "Min Zhang", "Yang P. Liu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Toward making the most of context in neural machine translation",
      "author" : [ "Zaixiang Zheng", "Xiang Yue", "Shujian Huang", "Jiajun Chen", "Alexandra Birch." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Neural machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) has achieved great progress and reached near human-level performance.",
      "startOffset" : 27,
      "endOffset" : 89
    }, {
      "referenceID" : 38,
      "context" : "Neural machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) has achieved great progress and reached near human-level performance.",
      "startOffset" : 27,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "In such cases, discourse phenomena, such as pronominal anaphora, lexical consistency, and document coherence that depend on long-range context going further than a few previous sentences, are neglected (Bawden et al., 2017).",
      "startOffset" : 202,
      "endOffset" : 223
    }, {
      "referenceID" : 42,
      "context" : "Among them, the mainstream works focus on the model architecture modification, including hierarchical attention (Wang et al., 2017; Miculicich et al., 2018; Tan et al., 2019), additional context extraction encoders or query layers (Jean et al.",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 24,
      "context" : "Among them, the mainstream works focus on the model architecture modification, including hierarchical attention (Wang et al., 2017; Miculicich et al., 2018; Tan et al., 2019), additional context extraction encoders or query layers (Jean et al.",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 34,
      "context" : "Among them, the mainstream works focus on the model architecture modification, including hierarchical attention (Wang et al., 2017; Miculicich et al., 2018; Tan et al., 2019), additional context extraction encoders or query layers (Jean et al.",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 1,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 53,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 40,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 16,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 23,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 50,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 10,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 54,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 52,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 49,
      "context" : ", 2019), additional context extraction encoders or query layers (Jean et al., 2017; Bawden et al., 2017; Zhang et al., 2018; Voita et al., 2018; Kuang and Xiong, 2018; Maruf et al., 2019; Yang et al., 2019; Jiang et al., 2019; Zheng et al., 2020; Yun et al., 2020; Xu et al., 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al.",
      "startOffset" : 64,
      "endOffset" : 281
    }, {
      "referenceID" : 22,
      "context" : ", 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al., 2018; Tu et al., 2018).",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : ", 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al., 2018; Tu et al., 2018).",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : ", 2020), and cache-like memory network (Maruf and Haffari, 2018; Kuang et al., 2018; Tu et al., 2018).",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : "Experimental results show effective improvements on universal translation metrics like BLEU (Papineni et al., 2002) and document-level linguistic indices (Tiedemann and Scherrer, 2017; Bawden et al.",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 35,
      "context" : ", 2002) and document-level linguistic indices (Tiedemann and Scherrer, 2017; Bawden et al., 2017; Werlen and Popescu-Belis, 2017; Müller et al., 2018; Voita et al., 2018, 2019).",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : ", 2002) and document-level linguistic indices (Tiedemann and Scherrer, 2017; Bawden et al., 2017; Werlen and Popescu-Belis, 2017; Müller et al., 2018; Voita et al., 2018, 2019).",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 46,
      "context" : ", 2002) and document-level linguistic indices (Tiedemann and Scherrer, 2017; Bawden et al., 2017; Werlen and Popescu-Belis, 2017; Müller et al., 2018; Voita et al., 2018, 2019).",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 25,
      "context" : ", 2002) and document-level linguistic indices (Tiedemann and Scherrer, 2017; Bawden et al., 2017; Werlen and Popescu-Belis, 2017; Müller et al., 2018; Voita et al., 2018, 2019).",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 53,
      "context" : "Though many studies report less promising results of naive Doc2Doc translation (Zhang et al., 2018; Liu et al., 2020), we successfully activate it with Multi-resolutional Training, which involves multiple levels of sequences.",
      "startOffset" : 79,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "Though many studies report less promising results of naive Doc2Doc translation (Zhang et al., 2018; Liu et al., 2020), we successfully activate it with Multi-resolutional Training, which involves multiple levels of sequences.",
      "startOffset" : 79,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : "Full Source Context: Firstly, though some Doc2Sent works utilize a full source-side context (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), some studies show that more sentences beyond can harm the results (Miculicich et al.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 23,
      "context" : "Full Source Context: Firstly, though some Doc2Sent works utilize a full source-side context (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), some studies show that more sentences beyond can harm the results (Miculicich et al.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 34,
      "context" : "Full Source Context: Firstly, though some Doc2Sent works utilize a full source-side context (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), some studies show that more sentences beyond can harm the results (Miculicich et al.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 24,
      "context" : ", 2019), some studies show that more sentences beyond can harm the results (Miculicich et al., 2018; Zhang et al., 2018; Tu et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 137
    }, {
      "referenceID" : 53,
      "context" : ", 2019), some studies show that more sentences beyond can harm the results (Miculicich et al., 2018; Zhang et al., 2018; Tu et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 137
    }, {
      "referenceID" : 36,
      "context" : ", 2019), some studies show that more sentences beyond can harm the results (Miculicich et al., 2018; Zhang et al., 2018; Tu et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "Full Target Context: Secondly, though some Doc2Sent works utilize a full target-side context (Maruf and Haffari, 2018; Zheng et al., 2020), many previous works abandon the target-side historical context, and some even claim that it is harmful to translation quality (Wang et al.",
      "startOffset" : 93,
      "endOffset" : 138
    }, {
      "referenceID" : 54,
      "context" : "Full Target Context: Secondly, though some Doc2Sent works utilize a full target-side context (Maruf and Haffari, 2018; Zheng et al., 2020), many previous works abandon the target-side historical context, and some even claim that it is harmful to translation quality (Wang et al.",
      "startOffset" : 93,
      "endOffset" : 138
    }, {
      "referenceID" : 42,
      "context" : ", 2020), many previous works abandon the target-side historical context, and some even claim that it is harmful to translation quality (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018).",
      "startOffset" : 135,
      "endOffset" : 191
    }, {
      "referenceID" : 53,
      "context" : ", 2020), many previous works abandon the target-side historical context, and some even claim that it is harmful to translation quality (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018).",
      "startOffset" : 135,
      "endOffset" : 191
    }, {
      "referenceID" : 36,
      "context" : ", 2020), many previous works abandon the target-side historical context, and some even claim that it is harmful to translation quality (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018).",
      "startOffset" : 135,
      "endOffset" : 191
    }, {
      "referenceID" : 39,
      "context" : "5M (Voita et al., 2019) Our New Datasets PDC FT/NYT ZH-EN 1.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 39,
      "context" : "Additionally, a document-level dataset with contrastive test sets in EN-RU (Voita et al., 2019) is used to evaluate lexical coherence.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 54,
      "context" : "Models ZH-EN EN-DE TED TED News Europarl s-BLEU d-BLEU s-BLEU d-BLEU s-BLEU d-BLEU s-BLEU d-BLEU Sent2Sent (Zheng et al., 2020) 17.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 38,
      "context" : "For the model setting, we follow the base version of Transformers (Vaswani et al., 2017), including 6",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : "For all experiments, we use subword (Sennrich et al., 2016) with 32K merge operations on both sides and cut out tokens appearing",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "Parameters are optimized by using Adam optimizer (Kingma and Ba, 2015), with β1 = 0.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "Besides, we use Horovod library with RDMA inter-GPU communication (Sergeev and Balso, 2018).",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "Following previous related works, we adopt tokenized case-insensitive BLEU (Papineni et al., 2002).",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 48,
      "context" : "Conjunction Presence (CP): Traditional models ignore cross-sentence dependencies, and the sentence-level translation may cause the missing of conjunctions like “And” (Xiong et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 186
    }, {
      "referenceID" : 4,
      "context" : "Recently, many studies focus on the efficiency enhancement on long-range sequence processing (Correia et al., 2019; Child et al., 2019; Kitaev et al., 2020; Wu et al., 2020; Beltagy et al., 2019; Rae et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 213
    }, {
      "referenceID" : 3,
      "context" : "Recently, many studies focus on the efficiency enhancement on long-range sequence processing (Correia et al., 2019; Child et al., 2019; Kitaev et al., 2020; Wu et al., 2020; Beltagy et al., 2019; Rae et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 213
    }, {
      "referenceID" : 14,
      "context" : "Recently, many studies focus on the efficiency enhancement on long-range sequence processing (Correia et al., 2019; Child et al., 2019; Kitaev et al., 2020; Wu et al., 2020; Beltagy et al., 2019; Rae et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 213
    }, {
      "referenceID" : 47,
      "context" : "Recently, many studies focus on the efficiency enhancement on long-range sequence processing (Correia et al., 2019; Child et al., 2019; Kitaev et al., 2020; Wu et al., 2020; Beltagy et al., 2019; Rae et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 213
    }, {
      "referenceID" : 2,
      "context" : "Recently, many studies focus on the efficiency enhancement on long-range sequence processing (Correia et al., 2019; Child et al., 2019; Kitaev et al., 2020; Wu et al., 2020; Beltagy et al., 2019; Rae et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 213
    }, {
      "referenceID" : 29,
      "context" : "Recently, many studies focus on the efficiency enhancement on long-range sequence processing (Correia et al., 2019; Child et al., 2019; Kitaev et al., 2020; Wu et al., 2020; Beltagy et al., 2019; Rae et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "Junczys-Dowmunt (2019) cut documents into long segments and feed them into training like BERT (Devlin et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "Finally, our work is also related to a series of studies in long sequence generation like GPT (Radford, 2018), GPT-2 (Radford et al.",
      "startOffset" : 94,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : "Finally, our work is also related to a series of studies in long sequence generation like GPT (Radford, 2018), GPT-2 (Radford et al., 2019), and Transformer-XL (Dai et al.",
      "startOffset" : 117,
      "endOffset" : 139
    } ],
    "year" : 0,
    "abstractText" : "This paper does not aim at introducing a novel method for document NMT. Instead, we head back to the original transformer model with document-level training and hope to answer the following question: Is the capacity of current models strong enough for documentlevel NMT? Interestingly, we observe that the original transformer with appropriate training techniques can achieve strong results for document translation, even with a length of 2000 words. We evaluate this model and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that the original Transformer model outperforms sentencelevel models and many previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.",
    "creator" : null
  }
}
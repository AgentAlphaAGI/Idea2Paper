{
  "name" : "ARR_2022_82_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Knowledge Neurons in Pretrained Transformers",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus (Petroni et al., 2019; Jiang et al., 2020b). In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers. The code is available at http://anonymous.url."
    }, {
      "heading" : "1 Introduction",
      "text" : "Large-scale pretrained Transformers (Devlin et al., 2019; Liu et al., 2019; Dong et al., 2019; Clark et al., 2020; Bao et al., 2020) are usually learned with a language modeling objective on large-scale corpora, such as Wikipedia, where exists oceans of factual knowledge. Pretrained language models naturally play as a free-text knowledge base by predicting texts (Bosselut et al., 2019). Petroni et al. (2019) and Jiang et al. (2020b) probe factual knowledge stored in pretrained language models by fillin-the-blank cloze queries. The evaluation shows that pretrained Transformers have a strong ability to recall factual knowledge without any fine-tuning. Roberts et al. (2020) use closed-book question answering to show that the larger a model is, the more knowledge it can store. However, most previous work focuses on evaluating the overall accuracy of text-form knowledge prediction. In this paper, we\nattempt to look deeper into pretrained Transformers and investigate how factual knowledge is stored.\nAs shown in Figure 1, we propose a knowledge attribution method to identify the neurons that express a relational fact, where such neurons are named knowledge neurons. Specifically, we view feed-forward network (i.e., two-layer perceptron) modules in Transformer as key-value memories (Geva et al., 2020). For the example in Figure 1, the hidden state is fed into the first linear layer and activates knowledge neurons; then, the second linear layer integrates the corresponding memory vectors. The key-value-memory nature (Geva et al., 2020) inspires us to propose the knowledge attribution method, which identifies knowledge neurons in feed-forward networks by computing the contribution of each neuron to the knowledge prediction.\nExtensive analysis shows that the activation of the identified knowledge neurons is positively correlated to the knowledge expression, which shows the effectiveness of the proposed knowledge at-\ntribution method. First, suppressing and amplifying knowledge neurons notably affects the expression of the corresponding knowledge. Second, we find that knowledge neurons of a fact tend to be activated more by corresponding knowledgeexpressing prompts. Third, given the knowledge neurons of a fact, the top activating prompts retrieved from open-domain texts usually express the corresponding fact, while the bottom activating prompts do not express the correct relation.\nIn our case studies, we try to leverage knowledge neurons to explicitly edit factual knowledge in pretrained Transformers without any fine-tuning. We present two preliminary studies: updating facts, and erasing relations. After identifying the knowledge neurons, we perform a knowledge surgery for pretrained Transformers by directly modifying the corresponding parameters in feed-forward networks. Such surgery shows promising results, keeping a moderate influence on other knowledge.\nOur contributions are summarized as follows:\n• We introduce the concept of knowledge neurons and propose a knowledge attribution method to identify the knowledge neurons that express specific factual knowledge in the fillin-the-blank cloze task.\n• We conduct both qualitative and quantitative analysis to show that knowledge neurons are positively correlated to knowledge expression.\n• We present preliminary studies of leveraging knowledge neurons to edit factual knowledge in Transformers, even without any fine-tuning."
    }, {
      "heading" : "2 Background: Transformer",
      "text" : "Transformer (Vaswani et al., 2017) is one of the most popular and effective NLP architectures. A Transformer encoder is stacked with L identical blocks. Each Transformer block mainly contains two modules: a self-attention module, and a feedforward network (abbreviated as FFN) module. Let X ∈ Rn×d denote the input matrix, two modules can be formulated as follows:\nQh = XW Q h ,Kh = XW K h , Vh = XW V h , (1) Self-Atth(X) = softmax ( QhK T h ) Vh, (2)\nFFN(H) = gelu (HW1)W2, (3)\nwhere WQh ,W K h ,W V h ,W1,W2 are parameter matrices; Self-Atth(X) computes a single attention head; H , the hidden state, is given by projecting the concatenation of all heads; gelu denotes the GELU activation function (Hendrycks and Gimpel, 2016). For simplicity, we omit the scaling factor in self-attention and the bias terms.\nConnections Between Self-Attention and FFN Comparing Equation (2) and Equation (3), we notice that the formula of FFN(·) is quite similar to Self-Att(·), except the activation function gelu in FFN and softmax in self-attention. Thus, similar to the query-key-value mechanism in self-attention, it is reasonable to regard the input of the FFN as a query vector, and two linear layers of the FFN as keys and values, respectively. Similar observations are also described in (Geva et al., 2020)."
    }, {
      "heading" : "3 Identifying Knowledge Neurons",
      "text" : "Similar to (Geva et al., 2020), we view FFNs in Transformer as key-value memories as illustrated in Figure 2. We hypothesize that factual knowledge is stored in FFN memories and expressed by knowledge neurons. In this section, we propose a knowledge attribution method and a refining strategy to identify these knowledge neurons."
    }, {
      "heading" : "3.1 Knowledge Assessing Task",
      "text" : "We employ the fill-in-the-blank cloze task to assess whether a pretrained model knows a fact. Following Petroni et al. (2019), each relational fact is in the form of a triplet 〈h, r, t〉, where h is the head entity, t is the tail entity, and r is the relation between them. Given a fact, pretrained models answer the cloze query x that expresses the fact but leaves the tail entity as a blank. For example, given the fact 〈Ireland, capital, Dublin〉, a possible query is “The capital of Ireland is ”. We also call the query a knowledge-expressing prompt. Petroni et al. (2019) describe that a model knows a fact if it can predict the correct answer. In this paper, rather than just examining the model outputs, we identify the specific knowledge neurons that express factual knowledge."
    }, {
      "heading" : "3.2 Knowledge Attribution",
      "text" : "Inspired by Hao et al. (2021), we propose a knowledge attribution method based on integrated gradients (Sundararajan et al., 2017). Our method can evaluate the contribution of each neuron to knowledge predictions. In this paper, we examine FFN intermediate neurons for the masked token, where the answer is predicted.\nGiven an input prompt x, we first define the model output Px(ŵ (l) i ) as the probability of the correct answer predicted by a pretrained model:\nPx(ŵ (l) i ) = p(y ∗|x,w(l)i = ŵ (l) i ), (4)\nwhere y∗ denotes the correct answer; w(l)i denotes the i-th intermediate neuron in the l-th FFN; ŵ(l)i is a given constant that w(l)i is assigned to.\nIn order to calculate the attribution score of a neuron Attr(w(l)i ), we gradually change w (l) i from 0 to its original value w(l)i calculated by the pretrained model, and meanwhile integrate the gradients:\nAttr(w (l) i ) = w (l) i ∫ 1 α=0 ∂ Px(αw (l) i )\n∂w (l) i\ndα, (5)\nwhere ∂ Px(αw (l) i )\n∂w (l) i\ncalculates the gradient of the\nmodel output with regard to w(l)i . Intuitively, as α changes from 0 to 1, by integrating the gradients, Attr(w(l)i ) accumulates the output probability change caused by the change of w(l)i . If the neuron has a great influence on the expression of a fact, the gradient will be salient, which in turn has large integration values. Therefore, the attribution score can measure the contribution of the neuron w(l)i to the factual expressions.\nDirectly calculating continuous integrals is intractable. We instead use Riemman approxima-\ntion ˜Attr(w(l)i ) = w (l) i m ∑m k=1 ∂ Px( k m w (l) i )\n∂w (l) i\n, where\nm = 20 is the number of approximation steps. With the attribution algorithm, we can identify a coarse set of knowledge neurons whose attribution scores are greater than a threshold t."
    }, {
      "heading" : "3.3 Knowledge Neuron Refining",
      "text" : "In order to identify knowledge neurons more accurately, we further propose a refining strategy. Besides “true-positive” knowledge neurons that express factual knowledge, the coarse set of knowledge neurons may contain “false-positive” knowledge neurons that express other information (e.g., syntactic or lexical information). The refining strategy aims to filter out these “false-positive” neurons.\nFor different prompts corresponding to the same fact, we hypothesize that they share the same set of “true-positive” knowledge neurons, since they express the same factual knowledge. Meanwhile, we hypothesize that they do not share the “falsepositive” knowledge neurons as long as the prompts are diverse enough. Therefore, given multiple diverse prompts, we can refine the coarse set of knowledge neurons by retaining only neurons that are widely shared among these prompts.\nSpecifically, given a relational fact, the complete process to identify its knowledge neurons is described as follows: (1) produce n diverse prompts; (2) for each prompt, calculate the knowledge attribution scores of neurons; (3) for each prompt, retain the neurons with attribution scores greater than the attribution threshold t, obtaining the coarse set of knowledge neurons; (4) considering all the coarse sets together, retain the knowledge neurons shared by more than p% prompts."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "We conduct experiments for BERT-base-cased (Devlin et al., 2019), one of the most widely-used pretrained models. It contains 12 Transformer blocks, where the hidden size is 768 and the FFN inner hidden size is 3,072. Notice that our method is not limited to BERT and can be easily generalized to other pretrained models. For each prompt, we set the attribution threshold t to 0.2 times the maximum attribution score. For each relation, we initialize the refining threshold p% (Section 3.3) as 0.7. Then, we increase or decrease it by 0.05 at a time until the average number of knowledge neurons lies in [2, 5]. We run our experiments on NVIDIA Tesla V100 GPUs. On average, it costs 13.3 seconds to identify knowledge neurons for a relational fact with 9 prompts."
    }, {
      "heading" : "4.2 Dataset",
      "text" : "We examine knowledge neurons through the fillin-the-blank cloze task based on the PARAREL dataset (Elazar et al., 2021). PARAREL is curated by experts, containing various prompt templates for 38 relations from the T-REx dataset (ElSahar et al., 2018). We show some example templates in Table 1. For each relational fact, we fill in the head entity in prompt templates and leave the tail entity as a blank to predict. In order to guarantee the template diversity, we filter out relations with fewer than 4 prompt templates and finally keep 34 relations, where each relation has 8.63 different prompt templates on average. These prompt templates produce 253,448 knowledge-expressing prompts in total for 27,738 relational facts."
    }, {
      "heading" : "4.3 Attribution Baseline",
      "text" : "Our baseline method takes the neuron activation value as the attribution score, i.e., Attrbase(w (l) i ) = w (l) i , which measures how sensitive a neuron is to the input. After computing attribution scores, we follow the same pipeline to obtain the refined\ndenotes\nthe intersection of knowledge neurons of fact pairs. “rel.” is the shorthand of relation. Our method identifies more exclusive knowledge neurons.\nknowledge neurons. For a fair comparison, we employ the same method to choose the hyperparameters t and p% for the baseline to ensure the average number of knowledge neurons for each relation lies in [2, 5].\nThe method based on neuron activation is a reasonable baseline. It is motivated by FFNs’s analogy with the self-attention mechanism (as described in Section 2), because self-attention scores are usually used as a strong attribution baseline (Kovaleva et al., 2019; Voita et al., 2019; Hao et al., 2021)."
    }, {
      "heading" : "4.4 Statistics of Knowledge Neurons",
      "text" : "Figure 3 presents the layer distribution of knowledge neurons identified by our knowledge attribution method. We notice that most fact-related neurons are distributed in the topmost layers of pretrained Transformers. The finding also agrees with Tenney et al. (2019) and Geva et al. (2020).\nTable 2 shows statistics of knowledge neurons. On average, we identify 4.13 knowledge neurons for each relational fact using our knowledge attribution method, and 3.96 using the baseline method.\nTheir same order of magnitude guarantees the fairness of the subsequent comparisons in the paper.\nWe also compute the knowledge neuron intersection of different relational facts. Table 2 shows the average number of pair-wise knowledge neuron intersections. For our proposed method, (1) fact pairs with the same relation (intra-relation fact pairs) share 1.23 knowledge neurons on average; (2) fact pairs with different relations (inter-relation fact pairs) share almost no knowledge neurons. In contrast, for the baseline, (3) most identified neurons are shared by intra-relation fact pairs; (4) even a substantial portion of neurons are common for inter-relation fact pairs. The difference in knowledge neuron intersections suggests that our method can identify more exclusive knowledge neurons."
    }, {
      "heading" : "4.5 Knowledge Neurons Affect Knowledge Expression",
      "text" : "We investigate how much knowledge neurons can affect knowledge expression in Figure 4 and Figure 5. Given a relational fact, we manipulate its knowledge neurons in two ways: (1) suppressing knowledge neurons by setting their activations to 0; (2) amplifying knowledge neurons by doubling their activations. Then, for each relation, we plot the average change ratio of the probability for the correct answer, corresponding to the manipulation.\nFor comparison, we also plot the results of manipulating baseline-identified knowledge neurons.\nFigure 4 shows that suppressing knowledge neurons identified by our knowledge attribution method leads to a consistent decrease (29.03% on average) in the correct probability. By contrast, for baseline-identified neurons, the suppressing operation has a negligible influence (1.47% decrease on average) on the correct probability. Notably, for the relation P178 (developer), the correct probability abnormally increases by using the baseline.\nAs shown in Figure 5, we have similar observations for amplifying the knowledge neurons identified by our knowledge attribution. We see a consistent increase (31.17% on average) in the correct probability. By contrast, the baseline even decreases the average correct probability by 1.27%.\nIn summary, the knowledge neurons identified by our knowledge attribution method tend to notably affect knowledge expression. Notice that the above assessment is affected by the distribution of knowledge neurons. For example, if the knowledge neurons for a relation are distributed more widely, we need to manipulate more top-k neurons for better control. We use the above experiments as a proof of concept while leaving precise control for future work."
    }, {
      "heading" : "4.6 Knowledge Neurons are Activated by Knowledge-Expressing Prompts",
      "text" : "In order to study what prompts can activate knowledge neurons, we compare the average activation of knowledge neurons for different types of prompts.\nBINGREL Dataset We build a new dataset BINGREL by crawling the Bing search engine to collect new prompts, for a more extensive comparison beyond the PARAREL dataset. For each of the 27,738 facts in PARAREL, we crawl two types of texts: (1) up to ten texts containing both the head and the tail entities (210,217 texts crawled in total); (2) up to ten texts containing only the head entity without restricting tail entities (266,020 texts crawled in total). Following the distant supervision assumption (Mintz et al., 2009), the first type of texts tends to express the whole relational fact, while the second type does not. We mask tail entities for the first type of texts to obtain knowledge-expressing prompts (T1). In order to conduct a controlled experiment, we mask random words for the second\ntype of texts, forming a control group (T2). Moreover, we employ randomly sampled prompts as another control group (T3).\nResults As shown in Table 4, for our method, the identified knowledge neurons are more significantly activated by knowledge-expressing prompts (T1 = 0.485), compared with the control groups (T2 = 0.019 and T3 = −0.018). By contrast, for the baseline, the activation of identified neurons cannot distinguish three types of prompts. In addition, since our comparison is based on the webcrawled BINGREL dataset, we validate the generalization of knowledge neurons to open-domain texts that are unseen in PARAREL.\nExample Prompts In Table 3, we present example prompts that activate knowledge neurons the most and the least, respectively. Given a fact, we first identify its knowledge neurons with our knowledge attribution method. Then, we calculate the average activation of knowledge neurons for each crawled prompt that contains both the head and the tail entities in BINGREL. Finally, we demonstrate two prompts with the highest average activation values and two with the lowest (denoted as top-2 and bottom-2 activating prompts, respectively).\nAs shown in Table 3, the top-2 activating prompts express exactly the corresponding relational fact. In contrast, despite containing the same head and tail entities, the bottom-2 activating prompts do not express the correct relation. For example, although the bottom-2 activating prompts for 〈Ireland, capital, Dublin〉 express\ninformation like “Dublin is a city in Ireland”, they do not reflect the capital relation. The examples support again that knowledge neurons are activated by corresponding knowledge-expressing prompts."
    }, {
      "heading" : "5 Case Studies",
      "text" : "We present two preliminary studies to demonstrate the potential applications of knowledge neurons. We use the case studies as a proof of concept while leaving precise fact editing for future work."
    }, {
      "heading" : "5.1 Updating Facts",
      "text" : "By leveraging knowledge neurons in pretrained models, we try to update a learned relational fact from 〈h, r, t〉 to 〈h, r, t′〉.\nMethods First, we identify the knowledge neurons of 〈h, r, t〉. Then, we retain the knowledge neurons that are shared by less than 10% of intrarelation facts, to reduce the influence on other facts with the same relation. Finally, we directly modify the corresponding value slots in FFN(val) (i.e., the second linear layer of FFNs; see Figure 2): FFN(val)i = FFN (val) i −λ1t + λ2t′, where FFN (val) i denotes the value slot corresponding to the i-th knowledge neuron; t and t′ are the word embeddings of t and t′, respectively; λ1 and λ2 are set to 1 and 8 in our experiments.\nSetup We conduct experiments on PARAREL. For each relation, we randomly sample ten facts learned by the pretrained model. For each fact 〈h, r, t〉, we randomly choose a different entity t′ with the same type as t (e.g., both t and t′ belong to city), and then update t′ as the target entity. We only manipulate about four top knowledge neurons as in Section 4.4. For reference purposes, we also perform the same update process on the same number of random neurons.\nEvaluation Metrics We report two metrics to evaluate the fact updating: (1) change rate, the ratio that the original prediction t is modified to another; (2) success rate, the ratio that t′ becomes the top prediction. In addition, we measure the influence on other knowledge by the following two metrics: (1) ∆intra-relation PPL, the increase of perplexity on the prompts with the same relation r; (2) ∆inter-relation PPL, the increase of perplexity on the prompts with different relations.\nResults As shown in Table 6, the surgery of knowledge neurons achieves a nontrivial success rate for updating facts, while random neurons are insufficient. Moreover, we find that such manipulation has little negative influence on other knowledge predictions. It is promising that we can change very few (i.e., about four in the above experiments) neurons to affect certain facts in pretrained Transformers. We can further improve the success rate by including more top knowledge neurons in the update process."
    }, {
      "heading" : "5.2 Erasing Relations",
      "text" : "We explore how to leverage knowledge neurons to erase specific relations in pretrained Transformers. Specifically, we take four relations in PARAREL as examples, i.e., place_of_birth, country_of_citizenship, occupation, work_location, that typically express sensitive personal information.\nMethods Given a relation r, we first identify knowledge neurons for all relational facts with r. Then, we retain 20 knowledge neurons that appear most frequently among these facts. Finally, we set the value slots in FFN(val) (see Figure 2) corresponding to these knowledge neurons to 0, i.e., zero vectors.\nResults As shown in Table 5, we report model perplexity before and after knowledge erasing. With the erasing operation, the perplexity of the removed knowledge increases as expected. Moreover, the model perplexity of other relations remains similar. We argue that knowledge neurons provide a promising way to erase undesired knowledge with minimal efforts."
    }, {
      "heading" : "6 Related Work",
      "text" : "Probing Knowledge in Pretrained Models Many pieces of previous work aim to measure knowledge stored in pretrained models. Petroni et al. (2019) propose to retrieve knowledge in pretrained models (such as BERT) using cloze queries. Their experiments show that BERT has a strong ability to recall factual knowledge without any finetuning. Jiang et al. (2020b) improve the cloze queries with mining-based and paraphrasing-based methods. Roberts et al. (2020) propose the closedbook question answering to measure how much knowledge a pretrained model has stored in its parameters. Elazar et al. (2021) measure and improve the consistency of pretrained models with respect to factual knowledge prediction. Rather than examining only the model outputs, we provide an openthe-black-box analysis for the knowledge neurons in pretrained Transformers.\nAttribution Methods In order to open the black boxes of deep learning models, attribution methods aim to attribute the model output to input features using different measures. The product of the gradients (of the output with respect to input features) and feature values is a reasonable baseline (Baehrens et al., 2010; Simonyan et al., 2014). Besides, a set of attribution methods (Shrikumar et al., 2017; Binder et al., 2016; Zeiler and Fergus, 2014; Springenberg et al., 2015) back-propagate the final output to input features. However, as stated by Sundararajan et al. (2017), none of these methods can simultaneously satisfy sensitivity and implementation invariance, two fundamental axioms. Taking the axioms as guidance, Sundarara-\njan et al. (2017) propose the integrated gradient method. Our knowledge attribution method is built upon integrated gradients.\nAnalysis of Transformer As one of the most popular and effective NLP architectures, Transformer (Vaswani et al., 2017) has attracted extensive studies. Most previous work focuses on the self-attention module (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019; Hao et al., 2021). Recently, Wu et al. (2019) and Dong et al. (2021) have pointed out that the feed-forward network module also matters to Transformer. Geva et al. (2020) attempt to connect feed-forward networks with key-value memories by qualitative analysis. In this paper, we identify and analyze knowledge neurons in feed-forward networks for given factual knowledge. Moreover, we present how to leverage knowledge neurons to explicitly edit factual knowledge stored in pretrained Transformers."
    }, {
      "heading" : "7 Conclusion and Future Directions",
      "text" : "We propose an attribution method to identify knowledge neurons that express factual knowledge in pretrained Transformers. We find that suppressing or amplifying the activation of knowledge neurons can accordingly affect the strength of knowledge expression. Moreover, quantitative and qualitative analysis on open-domain texts shows that knowledge neurons tend to be activated by the corresponding knowledge-expressing prompts. In addition, we present two preliminary case studies that attempt to utilize knowledge neurons to update or erase knowledge in pretrained Transformers.\nDespite the effectiveness of identifying knowledge neurons, our current studies still have limitations. First, we examine knowledge neurons based on the fill-in-the-blank cloze task, while knowledge can be expressed in a more implicit way. It is an open question whether Transformer can utilize stored knowledge in a generalized way, such as for reasoning. The interactions between knowledge neurons also remain under explored. Second, we focus on factual knowledge for ease of evaluation, even though our method is also applicable for other types of knowledge. Third, we use the single-word blank in cloze queries for simplicity, which requires multi-word extensions (Jiang et al., 2020a). Besides, an interesting future direction is to figure out how knowledge neurons work in multilingual pretrained Transformers (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021)."
    } ],
    "references" : [ {
      "title" : "How to explain individual classification decisions",
      "author" : [ "David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "KlausRobert Müller." ],
      "venue" : "J. Mach. Learn. Res., 11:1803– 1831.",
      "citeRegEx" : "Baehrens et al\\.,? 2010",
      "shortCiteRegEx" : "Baehrens et al\\.",
      "year" : 2010
    }, {
      "title" : "Unilmv2: Pseudo-masked language models for unified language model pre-training",
      "author" : [ "Hangbo Bao", "Li Dong", "Furu Wei", "Wenhui Wang", "Nan Yang", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Songhao Piao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Layer-wise relevance propagation for neural networks with local renormalization layers",
      "author" : [ "Alexander Binder", "Grégoire Montavon", "Sebastian Lapuschkin", "Klaus-Robert Müller", "Wojciech Samek." ],
      "venue" : "Proceedings of the 25th International Conference",
      "citeRegEx" : "Binder et al\\.,? 2016",
      "shortCiteRegEx" : "Binder et al\\.",
      "year" : 2016
    }, {
      "title" : "COMET: commonsense transformers for automatic knowledge graph construction",
      "author" : [ "Antoine Bosselut", "Hannah Rashkin", "Maarten Sap", "Chaitanya Malaviya", "Asli Celikyilmaz", "Yejin Choi." ],
      "venue" : "Proceedings of the 57th Conference of the Association for",
      "citeRegEx" : "Bosselut et al\\.,? 2019",
      "shortCiteRegEx" : "Bosselut et al\\.",
      "year" : 2019
    }, {
      "title" : "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Chi et al\\.,? 2021",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020. OpenRe-",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, pages 7057–7067.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is not all you need: Pure attention loses rank doubly exponentially with depth",
      "author" : [ "Yihe Dong", "Jean-Baptiste Cordonnier", "Andreas Loukas." ],
      "venue" : "CoRR, abs/2103.03404.",
      "citeRegEx" : "Dong et al\\.,? 2021",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2021
    }, {
      "title" : "Measuring and improving consistency in pretrained language models",
      "author" : [ "Yanai Elazar", "Nora Kassner", "Shauli Ravfogel", "Abhilasha Ravichander", "Eduard H. Hovy", "Hinrich Schütze", "Yoav Goldberg." ],
      "venue" : "CoRR, abs/2102.01017.",
      "citeRegEx" : "Elazar et al\\.,? 2021",
      "shortCiteRegEx" : "Elazar et al\\.",
      "year" : 2021
    }, {
      "title" : "T-rex: A large scale alignment of natural language with knowledge base triples",
      "author" : [ "Hady ElSahar", "Pavlos Vougiouklis", "Arslen Remaci", "Christophe Gravier", "Jonathon S. Hare", "Frédérique Laforest", "Elena Simperl." ],
      "venue" : "Proceedings of the Eleventh Interna-",
      "citeRegEx" : "ElSahar et al\\.,? 2018",
      "shortCiteRegEx" : "ElSahar et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformer feed-forward layers are key-value memories",
      "author" : [ "Mor Geva", "Roei Schuster", "Jonathan Berant", "Omer Levy." ],
      "venue" : "CoRR, abs/2012.14913.",
      "citeRegEx" : "Geva et al\\.,? 2020",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2020
    }, {
      "title" : "Selfattention attribution: Interpreting information interactions inside transformer",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu." ],
      "venue" : "The Thirty-Fifth AAAI Conference on Artificial Intelligence. AAAI Press.",
      "citeRegEx" : "Hao et al\\.,? 2021",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2021
    }, {
      "title" : "Gaussian error linear units (gelus)",
      "author" : [ "Dan Hendrycks", "Kevin Gimpel" ],
      "venue" : null,
      "citeRegEx" : "Hendrycks and Gimpel.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hendrycks and Gimpel.",
      "year" : 2016
    }, {
      "title" : "XFACTR: multilingual factual knowledge retrieval from pretrained language models",
      "author" : [ "Zhengbao Jiang", "Antonios Anastasopoulos", "Jun Araki", "Haibo Ding", "Graham Neubig." ],
      "venue" : "Proceedings 9",
      "citeRegEx" : "Jiang et al\\.,? 2020a",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky." ],
      "venue" : "ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick S.H. Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander H. Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "How much knowledge can you pack into the parameters of a language model",
      "author" : [ "Adam Roberts", "Colin Raffel", "Noam Shazeer" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Roberts et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning important features through propagating activation differences",
      "author" : [ "Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, volume 70 of Proceedings",
      "citeRegEx" : "Shrikumar et al\\.,? 2017",
      "shortCiteRegEx" : "Shrikumar et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman." ],
      "venue" : "2nd International Conference on Learning Representations, ICLR 2014.",
      "citeRegEx" : "Simonyan et al\\.,? 2014",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin A. Riedmiller." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015.",
      "citeRegEx" : "Springenberg et al\\.,? 2015",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2015
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, volume 70 of Proceedings of Machine Learning Research, pages",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing the structure of attention in a transformer language model",
      "author" : [ "Jesse Vig", "Yonatan Belinkov." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63–76, Florence, Italy. As-",
      "citeRegEx" : "Vig and Belinkov.,? 2019",
      "shortCiteRegEx" : "Vig and Belinkov.",
      "year" : 2019
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computa-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann N. Dauphin", "Michael Auli." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019. OpenReview.net.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Matthew D. Zeiler", "Rob Fergus." ],
      "venue" : "Proceedings of the 13th European Conference on Computer Vision, ECCV 2014, volume 8689 of Lecture Notes in Computer Science, pages 818–833.",
      "citeRegEx" : "Zeiler and Fergus.,? 2014",
      "shortCiteRegEx" : "Zeiler and Fergus.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus (Petroni et al., 2019; Jiang et al., 2020b).",
      "startOffset" : 125,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : "Large-scale pretrained Transformers (Devlin et al., 2019; Liu et al., 2019; Dong et al., 2019; Clark et al., 2020; Bao et al., 2020) are usually learned with a language modeling objective on large-scale corpora, such as Wikipedia, where exists oceans of factual knowledge.",
      "startOffset" : 36,
      "endOffset" : 132
    }, {
      "referenceID" : 20,
      "context" : "Large-scale pretrained Transformers (Devlin et al., 2019; Liu et al., 2019; Dong et al., 2019; Clark et al., 2020; Bao et al., 2020) are usually learned with a language modeling objective on large-scale corpora, such as Wikipedia, where exists oceans of factual knowledge.",
      "startOffset" : 36,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "Large-scale pretrained Transformers (Devlin et al., 2019; Liu et al., 2019; Dong et al., 2019; Clark et al., 2020; Bao et al., 2020) are usually learned with a language modeling objective on large-scale corpora, such as Wikipedia, where exists oceans of factual knowledge.",
      "startOffset" : 36,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "Large-scale pretrained Transformers (Devlin et al., 2019; Liu et al., 2019; Dong et al., 2019; Clark et al., 2020; Bao et al., 2020) are usually learned with a language modeling objective on large-scale corpora, such as Wikipedia, where exists oceans of factual knowledge.",
      "startOffset" : 36,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "Large-scale pretrained Transformers (Devlin et al., 2019; Liu et al., 2019; Dong et al., 2019; Clark et al., 2020; Bao et al., 2020) are usually learned with a language modeling objective on large-scale corpora, such as Wikipedia, where exists oceans of factual knowledge.",
      "startOffset" : 36,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Pretrained language models naturally play as a free-text knowledge base by predicting texts (Bosselut et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : ", two-layer perceptron) modules in Transformer as key-value memories (Geva et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "The key-value-memory nature (Geva et al., 2020) inspires us to propose the knowledge attribution method, which identifies knowledge neurons in feed-forward networks by computing the contribution of each neuron to the knowledge prediction.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 29,
      "context" : "Transformer (Vaswani et al., 2017) is one of the most popular and effective NLP architectures.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "where W h ,W K h ,W V h ,W1,W2 are parameter matrices; Self-Atth(X) computes a single attention head; H , the hidden state, is given by projecting the concatenation of all heads; gelu denotes the GELU activation function (Hendrycks and Gimpel, 2016).",
      "startOffset" : 221,
      "endOffset" : 249
    }, {
      "referenceID" : 14,
      "context" : "Similar observations are also described in (Geva et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "(2021), we propose a knowledge attribution method based on integrated gradients (Sundararajan et al., 2017).",
      "startOffset" : 80,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "We conduct experiments for BERT-base-cased (Devlin et al., 2019), one of the most widely-used pretrained models.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "We examine knowledge neurons through the fillin-the-blank cloze task based on the PARAREL dataset (Elazar et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "PARAREL is curated by experts, containing various prompt templates for 38 relations from the T-REx dataset (ElSahar et al., 2018).",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "It is motivated by FFNs’s analogy with the self-attention mechanism (as described in Section 2), because self-attention scores are usually used as a strong attribution baseline (Kovaleva et al., 2019; Voita et al., 2019; Hao et al., 2021).",
      "startOffset" : 177,
      "endOffset" : 238
    }, {
      "referenceID" : 31,
      "context" : "It is motivated by FFNs’s analogy with the self-attention mechanism (as described in Section 2), because self-attention scores are usually used as a strong attribution baseline (Kovaleva et al., 2019; Voita et al., 2019; Hao et al., 2021).",
      "startOffset" : 177,
      "endOffset" : 238
    }, {
      "referenceID" : 15,
      "context" : "It is motivated by FFNs’s analogy with the self-attention mechanism (as described in Section 2), because self-attention scores are usually used as a strong attribution baseline (Kovaleva et al., 2019; Voita et al., 2019; Hao et al., 2021).",
      "startOffset" : 177,
      "endOffset" : 238
    }, {
      "referenceID" : 21,
      "context" : "Following the distant supervision assumption (Mintz et al., 2009), the first type of texts tends to express the whole relational fact, while the second type does not.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "The product of the gradients (of the output with respect to input features) and feature values is a reasonable baseline (Baehrens et al., 2010; Simonyan et al., 2014).",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 25,
      "context" : "The product of the gradients (of the output with respect to input features) and feature values is a reasonable baseline (Baehrens et al., 2010; Simonyan et al., 2014).",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 24,
      "context" : "Besides, a set of attribution methods (Shrikumar et al., 2017; Binder et al., 2016; Zeiler and Fergus, 2014; Springenberg et al., 2015) back-propagate the final output to input features.",
      "startOffset" : 38,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "Besides, a set of attribution methods (Shrikumar et al., 2017; Binder et al., 2016; Zeiler and Fergus, 2014; Springenberg et al., 2015) back-propagate the final output to input features.",
      "startOffset" : 38,
      "endOffset" : 135
    }, {
      "referenceID" : 33,
      "context" : "Besides, a set of attribution methods (Shrikumar et al., 2017; Binder et al., 2016; Zeiler and Fergus, 2014; Springenberg et al., 2015) back-propagate the final output to input features.",
      "startOffset" : 38,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "Besides, a set of attribution methods (Shrikumar et al., 2017; Binder et al., 2016; Zeiler and Fergus, 2014; Springenberg et al., 2015) back-propagate the final output to input features.",
      "startOffset" : 38,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "Analysis of Transformer As one of the most popular and effective NLP architectures, Transformer (Vaswani et al., 2017) has attracted extensive studies.",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 31,
      "context" : "Most previous work focuses on the self-attention module (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019; Hao et al., 2021).",
      "startOffset" : 56,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "Most previous work focuses on the self-attention module (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019; Hao et al., 2021).",
      "startOffset" : 56,
      "endOffset" : 138
    }, {
      "referenceID" : 30,
      "context" : "Most previous work focuses on the self-attention module (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019; Hao et al., 2021).",
      "startOffset" : 56,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "Most previous work focuses on the self-attention module (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019; Hao et al., 2021).",
      "startOffset" : 56,
      "endOffset" : 138
    }, {
      "referenceID" : 17,
      "context" : "Third, we use the single-word blank in cloze queries for simplicity, which requires multi-word extensions (Jiang et al., 2020a).",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "Besides, an interesting future direction is to figure out how knowledge neurons work in multilingual pretrained Transformers (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021).",
      "startOffset" : 125,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "Besides, an interesting future direction is to figure out how knowledge neurons work in multilingual pretrained Transformers (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021).",
      "startOffset" : 125,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "Besides, an interesting future direction is to figure out how knowledge neurons work in multilingual pretrained Transformers (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021).",
      "startOffset" : 125,
      "endOffset" : 191
    } ],
    "year" : 0,
    "abstractText" : "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus (Petroni et al., 2019; Jiang et al., 2020b). In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers. The code is available at http://anonymous.url.",
    "creator" : null
  }
}
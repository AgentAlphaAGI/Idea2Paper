{
  "name" : "ARR_2022_277_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "When Combating Hype, Proceed with Caution",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Over the last few years, natural language processing has seen a wave of surprising negative results overturning previously-reported success stories about what our models can do, and showing that widely-used models are surprisingly brittle (Jia and Liang, 2017; Niven and Kao, 2019; McCoy et al., 2019). This shows that many of our standard practices for evaluation and reporting can lead to unrealistically positive initial claims about what we can do. The resulting hype and overclaiming, whether intentional or not, are a problem. They can encourage the reckless deployment of NLP systems in high-stakes settings where they can do significant harm. They also threaten the health and credibility of NLP as a research field, and thereby threaten our ability to influence applied stakeholders or attract funding.\nFortunately, these results have led to a surge of research and writing that proposes more thorough and cautious practices for the evaluation of model ability (Ribeiro et al., 2020; Gardner et al., 2020;\nKiela et al., 2021; Bowman and Dahl, 2021). While we have only a limited ability to control the public narrative taking place through industry PR and the media, there’s reason to be hopeful that we researchers are getting much better at avoiding the worst forms of overconfidence about our systems. Less fortunately, these discouraging results seem to have produced a trend toward pessimism about model performance that is often ungrounded from real empirical results. This leaves room for the research community’s consensus about a model’s capabilities to fall short of its actual capabilities.\nI call this issue underclaiming, for lack of a better term, and argue that it is more dangerous than it might seem. It risks our credibility and thereby limits our ability to influence stakeholders in cases where our current systems are doing real harm. It also limits our ability to accurately forecast and plan for the impacts that may result from the deployment of more capable systems in the future. If we can truly reach near-human-level performance on many of the core problems of NLP, we should expect enormous impacts—involving valuable new technologies but also severe economic and political disruption and high-stakes alignment or robustness failures—which will be potentially catastrophic if not planned for.\nIn this paper, I lay out four types of underclaiming, focusing especially on writing and citation practices. I then argue that it is a problem. I close by sketching some ways of reducing the prevalence\nof this kind of underclaiming, including straightforward best practices in writing and evaluation, a proposed rule of thumb for writing and reviewing, improvements to tooling for analysis and benchmarking, and research directions in model performance forecasting and test set design."
    }, {
      "heading" : "2 Underclaiming: Case Studies",
      "text" : "This paper addresses the possibility that in our effort to avoid overconfidence, we often err in the opposite direction: often claiming or implying that state-of-the-art systems are less capable than they actually are. This has taken several forms, including misleading presentations of valid negative results from weak or dated baseline models, misleading claims about the limits of what is conceptually possible with machine learning, misleading reporting of results on adversarially collected data."
    }, {
      "heading" : "2.1 Negative Results on Weaker Models",
      "text" : "In spite of many surprises and setbacks, NLP research seems to have made genuine progress on many problems over the last few years. In light of this, discussions about the limitations of systems from past years don’t straightforwardly apply to present systems. The first two cases that I present involve failures to contextualize claims about the failures of weaker past systems:\nAdversarial Examples for SQuAD Jia and Liang (2017) published one of the first prominent demonstrations of adversarial failures in neuralnetwork-based NLP, showing that a simple algorithm could automatically augment examples from the SQuAD benchmark (Rajpurkar et al., 2016) in a\nway that would fool many state-of-the-art systems, but not humans. This work prompted a wave of much-needed analysis and a corresponding lowering of expectations about the effectiveness of neural network methods.\nHowever, the results in Jia and Liang predate the development of modern pretraining methods in NLP (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), and the best systems studied in this work have more than twice the error rate of the current state of the art. While I am not aware of any results from current state-of-the-art systems on this data, results from 2019 systems suggest that we are making substantial progress (Table 1). We have no reason to expect, then, that the failures documented in this work are quantitatively or qualitatively similar to the failures of current systems.\nHowever, papers that cite these results often present them with no discussion of the model under study, yielding misleading implications. For example, the award-winning work of Linzen (2020) uses the Jia and Liang result to justify this claim:\n[F]or current deep learning systems: when tested on cases sampled from a distribution that differs from the one they were trained on, their behavior is unpredictable and inconsistent with that of humans\nThe chief concern in this context is the claim that this failure applies to current deep learning systems in general, and the corresponding unjustified implication that these failures are a fundamental or defining feature of neural network language models. Looking only to highly-cited works from the last two years that cite Jia and Liang, similar statements can be found in Xu et al. (2020), Zhang et al. (2020), and others.\nThe Long Shadow of BERT While the case of Jia and Liang is especially striking since it deals with models that predate pretraining entirely, a similar effect is much more common in a subtler form: Most analysis papers that identify limitations of a system come out well after the system description paper that claims the initial (typically positive) results. BERT, first released in Fall 2018, has been a major locus for this kind of analysis work, and remains one long after its release. Looking to a random sample of ten papers from the NAACL 2021 analysis track that study pretrained models,1 none of them analyze models that have come out since summer 2019, and five only study BERT, representing a median lag of nearly three years from the release of a model to the publication of the relevant analysis.\nThis trend has consequences for the conclusions that one would draw from a broad review of the recent literature on some problem: A review of that literature will contrast the successes of the best current systems against the weaknesses of the best systems from an earlier period. In many cases, these weaknesses will be so severe as to challenge the credibility of these successes if they are not properly recognized as belonging to different model generations.\nThis analysis work is often valuable and these long timelines can be justifiable: Good analysis work takes time, and researchers doing analysis work often have an incentive to focus on older models to ensure that they can reproduce previously observed effects. The BERT-only results, though, represent a clear missed opportunity: There exist newer models like RoBERTa and DeBERTa (Liu et al., 2019; He et al., 2020) which follow nearly identical APIs and architectures to BERT, such that it should generally be possible to reuse any BERToriented analysis method on these newer models without modification. In many cases, these newer models are different enough in their performance that we should expect analyzing them to yield different conclusions: For example, BERT performs slightly worse than chance on the few-shot Winograd Schema commonsense reasoning test set in SuperGLUE (Levesque et al., 2011; Wang et al.,\n1Papers studying only BERT: White et al. (2021); Slobodkin et al. (2021); Bian et al. (2021); Cao et al. (2021); Pezeshkpour et al. (2021). Papers studying other models predating Fall 2019: Wallace et al. (2021); Hall Maudslay and Cotterell (2021); Hollenstein et al. (2021); Bitton et al. (2021); Du et al. (2021)\n2019), while DeBERTa reaches a near-perfect 96% accuracy. How much better would our understanding of current technology be if a few of these works had additionally reported results with DeBERTa?\n2.2 Strong Claims about Understanding The influential work of Bender and Koller (2020) is centered on the claim that:\n[T]he language modeling task, because it only uses form as training data, cannot in principle lead to learning of meaning.\nThe proof of this claim is straightforward and convincing under some (but not all) mainstream definitions of the word meaning in the context of NLP: If meaning deals with the relationship between language and some external nonlinguistic reality, then a system that can only interact with the world through language cannot manipulate meaning.\nThis argument does not, on its own, make any prediction about the behavior of these models on tasks that take place entirely through the medium of language.2 Under this definition, a translation system is acting without reference to meaning even if it has a rich, structured internal model of the world, and even it interprets sentences with reference to that model when translating: As long as that model of the world is developed solely using language, no meaning is involved.\nIn addition, this argument does not justify any strong prediction about the behavior of models which are trained primarily, but not exclusively, on a language modeling objective, as with models that are fine-tuned to produce non-textual outputs like labels, or models which are trained in a multimodal language-and-vision regime.\nWhile this core claim is sound and important, public discussion of the paper has often repeated the claim in ways that imply stronger conclusions about model behavior. Utama et al. (2020), for example, write that:\nResearchers have recently studied more closely the success of large fine-tuned LMs in many NLU tasks and found that models are simply better in leveraging biased patterns instead of capturing a better notion of language understanding for the intended task (Bender and Koller, 2020).\n2See Merrill et al. (2021) for some limits on how closely such a model can correspond to the real world and Bommasani et al. (2021, §2.6.3) for further discussion of the implications of Bender and Koller’s arguments for NLP.\nIn another vein, Jang and Lukasiewicz (2021) make the straightforward claim that\nBender and Koller (2020) show that it is impossible to learn the meaning of language by only leveraging the form of sentences.\nbut they then use that claim to motivate a new regularization technique for language models, which does nothing to change the fact that they are trained on form alone. In this context, it is hard to avoid the incorrect inference that Bender and Koller show a specific and contingent problem with recent language models—a problem which could be mitigated by better regularization.\nSimilar claims can be found in many other citing works (Utama et al., 2020; van Noord et al., 2020; Hovy and Yang, 2021; Sayers et al., 2021; PetiStantić et al., 2021; Jang and Lukasiewicz, 2021). While Bender and Koller raise important points for discussion, these strong implications in citing works are misleading and potentially harmful."
    }, {
      "heading" : "2.3 Adversarially Collected Data",
      "text" : "Adversarially collected test sets (Bartolo et al., 2020; Nie et al., 2020; Kiela et al., 2021)—or test sets composed of examples that some target system gets wrong—have recently become a popular tool in the evaluation of NLP systems. Datasets of this kind are crowdsourced in a setting where an example-writer can interact with a model (or ensemble) in real time and is asked to come up with examples on which the model fails. Writers are generally incentivized to find these failure cases, and the test section(s) of the resulting dataset will generally consist exclusively of such cases.\nThis process produces difficult test sets and it can be a useful tool in understanding the limits of existing training sets or models (Williams et al., 2020). However, the constraint that a specified system must fail on the test examples makes it difficult to infer much from absolute measures of test-set performance. In particular, as long as a model makes any errors at all on any possible inputs, then we expect it to be possible to construct an adversarial test set against the model, and we expect the model to achieve zero test accuracy on that test set. We can further infer that any models that are sufficiently similar to the adversary should also perform very poorly on this test set, regardless of their ability. Neither of these observations would\ntell us anything non-trivial about the actual abilities of the models.\nWhat’s more, in many NLU data collection efforts, a large share of annotator disagreements represent subjective judgments rather than clear-cut errors (Pavlick and Kwiatkowski, 2019). This means that even a perfectly careful and perfectly wellqualified human annotator should be expected to disagree with the majority judgment on some examples, and will thereby be coded as having made errors. It is, therefore, possible to create an adversarial test set for which a careful human annotator would reliably achieve 0% accuracy.\nAdversarially collected datasets are increasingly commonly used as test sets in standard experimental paradigms, and these caveats about the interpretation of results are not always clear when numbers are presented. Sampling papers that cite Nie et al. (2020), for example, it is easy to find references that do not mention the adversarial design of the data and that therefore make claims that are hard to justify:3 Talmor et al. (2020) use the results from Nie et al. to claim that “LMs do not take into account the presence of negation in sentences”, and Hidey et al. (2020) use them to justify the claim that “examples for numerical reasoning and lexical inference have been shown to be difficult.” Liu et al. (2020) similarly use absolute results on the adversary models to back up the trivial but easilymisread claim that BERT-style models “may still suffer catastrophic failures in adversarial scenarios.”"
    }, {
      "heading" : "3 A Word on Hype",
      "text" : "The previous section has laid out some ways in which the mainstream NLP research community often makes unjustifiable claims about the limitations of state-of-the-art methods. These claims do not make the opposite phenomenon, hype, any less real or any less harmful. While hype is likely most severe in industry PR and in the media,it is nonetheless still prevalent in the research literature. In one especially clear example, a prominent paper claiming of human parity in machine translation performance (Hassan et al., 2018) severely overstates what has been accomplished relative to commonsense intuitions about what a human-level\n3I focus here about claims about the absolute performance level of models. Whether adversarially collected test sets are appropriate for comparing the relative effectiveness of models is a largely orthogonal issue (Bowman and Dahl, 2021; Kaushik et al., 2021).\ntranslation system would do (Toral et al., 2018; Läubli et al., 2018).\nI do not aim to argue that overclaiming or hype is acceptable or safe. Combating hype should be fully compatible with the goals laid out in this paper, and broad-based efforts to improve our practices in evaluation, analysis, writing, and forecasting should help reduce both underclaiming and hype."
    }, {
      "heading" : "4 Why Underclaiming is Harmful",
      "text" : "Research papers are generally most useful when they’re true and informative. A research field that allows misleading claims to go unchallenged is likely to waste its time solving problems that it doesn’t actually have, and is likely to lose credibility with serious funders, reporters, and industry stakeholders. This is the most obvious reason that we should be concerned about underclaiming, but it is not the whole story. This loss of insight and credibility can seriously challenge our ability to anticipate, understand, and manage the impacts of deploying NLP systems. This is especially true of impacts that are contingent on NLP technologies actually working well, which we should expect will become more substantial as time goes on."
    }, {
      "heading" : "4.1 Present-Day Impact Mitigation",
      "text" : "The deployment of modern NLP systems has had significant positive and negative impacts on the world. Researchers in NLP have an ethical obligation to inform (and if necessary, pressure) stakeholders about how to avoid or mitigate the negative impacts while realizing the positive ones. Most prominently, most applied NLP models show serious biases with respect to legally protected attributes like race and gender (Bolukbasi et al., 2016; Rudinger et al., 2018). We have no reliable mechanisms to mitigate these biases and no reason to believe that they will be satisfactorily resolved with larger scale. We thus are standing on shaky moral grounds when we deploy present systems in highimpact settings, but they are being widely deployed anyway (e.g. Dastin, 2018; Nayak, 2019; Dansby et al., 2020). Beyond bias, similar present-day concerns can be seen around issues involving minority languages and dialects, deceptive design, and the concentration of power (Joshi et al., 2020; Bender et al., 2021; Kenton et al., 2021, §3.3).\nPersuading the operators of deployed systems to take these issues seriously, and to mitigate harms or scale back deployments when necessary, will be\ndifficult. Intuitively, researchers concerned about these harms may find it appealing to emphasize the limitations of models in the hope that this will discourage the deployment of harmful systems. This is appropriate when done carefully, and could be a way in which the field’s increasing focus on model analysis could reduce some of these harms.\nHowever, this kind of strategic underclaiming can backfire if done sloppily: Models are often both useful and harmful, especially when the operator of the system is not the one being harmed. If the operator of some deployed system sees firsthand that the system is effective for their purposes, they have little reason to trust researchers who argue that that same system does not understand language, or who argue something similarly broad and negative. They will then be unlikely to listen to those researchers’ further claims that such a system is harmful, even if those further claims are accurate."
    }, {
      "heading" : "4.2 Longer-Term Risks",
      "text" : "We can reasonably expect NLP systems to improve over the coming decades. Even if intellectual progress from research were to slow, the dropping price of compute should allow us to continue to reap the benefits of larger-scale training (Kaplan et al., 2020; Brown et al., 2020). This improvement in capabilities is likely to amplify both the harms and benefits of language technology.\nWe have good reason to expect that this further progress in NLP, over many years or decades, will lead to upheavals in areas like education, medicine, law, and the service sector more broadly, as well as making mass surveillance and misinformation campaigns far more effective, and opening up additional new use cases that will be hard for us to foresee (Brundage et al., 2018; Tamkin et al., 2021; Bommasani et al., 2021). One can reasonably expect that the positive and negative impacts of these upheavals will far exceed the impacts that our technologies have produced to date. In turn, NLP researchers who want to ensure that their career has a net-positive impact on the world should be concerned with these possibilities.\nIt will be difficult to do the necessary technical, social, and governance work to prepare for these advances if we do not have a clear picture of our current capabilities, and it will be difficult to convince outside stakeholders to act appropriately to mitigate these risks if we don’t acknowledge that we have made, and are making, real progress to-\nward effective language technology. Looking somewhat further into the future, a substantial community of philosophers, economists, and general ML researchers are concerned that highly-capable AI systems—of the kind that could plausibly be developed through existing ML research paradigms—are extremely dangerous by default (Bostrom, 2012; Critch and Krueger, 2020; Christian, 2020; Ord, 2020; Russell and Norvig, 2020). Expert forecasts suggest that this could take place within a few decades (Grace et al., 2018). If these hypotheses hold, and if we are poorly prepared for these developments, the worst-case outcomes could be catastrophic, even threatening the existence of human civilization on some views.\nInvestments in research into these potential catastrophic risks from advanced machine learning have become substantial: Funding from one foundation alone has totaled over $75M USD.4 Concerns about risks from AI have also been the stated motivation for a significant fraction of the work from DeepMind and OpenAI, which both have access to even greater amounts of funding.\nSpurred on in particular by the shift in emergent capabilities from GPT-2 to GPT-3, the attention of these AI risk researchers has also been increasingly centered on language models and similar selfsupervised multimodal models (Irving et al., 2018; Stiennon et al., 2020; Hendrycks et al., 2020; Kenton et al., 2021; Wu et al., 2021; Bommasani et al., 2021, §4.9). Despite the scale of this research, and its recent shift of focus toward language models, there has been little interaction between the AI risk research community and mainstream academic NLP to date.\nThe facts that AI risk research is growing in influence and that it is increasingly focused on language models put NLP in an exceptionally strange and troubling situation as a field. To the extent that these concerns are valid, they represent an urgent call for reprioritization within NLP research to favor safety-relevant areas like interpretability, control, and evaluation over scaling, and to push for better oversight and regulation of large-scale research (Dafoe, 2018): Even a small risk of a globally significant catastrophe warrants a dramatic response. On the other hand, to the extent that these concerns are unfounded or are built on misunderstandings about the possible trajectories of\n4https://www.openphilanthropy.org/ giving/grants\nML research, it would be quite valuable to correct this misunderstanding. Correcting the record could redirect these resources and, more significantly, reduce the risk that popular or regulatory pressure will snuff out the positive potential of NLP technologies."
    }, {
      "heading" : "5 Why Worry?",
      "text" : "Because these more speculative concerns around advanced artificial intelligence are rarely discussed in the NLP literature, I will here offer a brief overview of that work. Recent writing tends to focus on four clusters of hypotheses:\nUnaccountable Organizations Highly-capable AI is likely to lead to highly-profitable applications. Highly-capable AI should also be able to displace human labor in technical fields to a large extent, increasing the relative value of capital over labor, and making it easier for the leaders of profitable organizations to take unpopular actions unilaterally. In the longer term, highly-capable AI may also contribute to the effectiveness of persuasion campaigns, further insulating these organizations from outside pressure. These forces could conspire to make the companies or governments that first produce highly-capable AI almost entirely unaccountable, and allowing their decisions to play a major role in the trajectory of humanity as a whole (Ord, 2020).\nAlignment and Robustness Failures Even if a system is deployed by an actor with good intentions and substantial oversight, good outcomes are not guaranteed. As AI systems become more capable, it becomes more likely that they will be trusted to make high-stakes decisions autonomously. In these cases, it becomes crucial that they behave in ways that we would endorse, even when they are pushed into unfamiliar new situations. This requires both that the systems be optimized for the right objectives and that the systems actually internalize and generalize those objectives correctly.\nSpecifying and using safe objectives, such that aggressively optimizing them does not produce catastrophic outcomes, is difficult (Critch and Krueger, 2020). Human preferences are complex, making the problem of specifying an objective that rules out unintended bad behavior non-trivial. Goodhart’s law5 means that many objectives that\n5in the formulation of Strathern (1997): “When a measure becomes a target, it ceases to be a good measure.”\nserve as good proxies for what we want in in familiar situations can break down in new situations.\nFurther, training large models with high precision is difficult. A small flaw in a highly-capable system’s learned understanding of its objective can cause catastrophic failures, even if the true intended objective would have safe (Hubinger et al., 2019).\nInstrumentally-Convergent Subgoals The instrumental convergence hypothesis holds that systems that are optimizing for benign objectives, once they become sufficiently capable, have a predictable reason to take on dangerous subgoals— like accumulating large amounts of computational, economic, or political power—to maximize the odds that their primary objectives are achieved (Bostrom, 2003; Omohundro, 2008; Bostrom, 2012).6 Systems that interact with humans only through text, or systems whose goals are circumscribed to a well-defined task like question answering, are not exempt from this concern (Armstrong et al., 2012).\nRisks Will Be Difficult to Spot Human-level capabilities are likely to emerge first from large machine learning models that, like modern neural networks, are not directly interpretable. This means that it may be difficult to spot ways in which a model is unsafe or to forecast ways in which its behavior might change in novel settings (Critch and Krueger, 2020).\nFurther, we should expect highly-capable AI systems to be useful in the short term, giving potential users a strong incentive to deploy them as soon as they are affordable, even if their safety is not guaranteed. This means that it is not enough that it simply be possible for us to develop safe systems, it is additionally necessary that it be nearly as easy and nearly as affordable as developing unsafe systems (Irving et al., 2018).\nSo What? None of these hypotheses is simple to prove, but as far as I am aware, all have resisted straightforward attempts at falsification. All four are potentially applicable to neural networkbased models and to models which operate pri-\n6This is exemplified by the thought experiment of the paperclip maximizer, which points out that a machine which tasked with manufacturing as many paperclips as possible, if sufficiently capable, can be expected to turn nearly all matter on earth into paperclips. While this vision of a single system acting alone on such a trivial objective is unrealistic, it demonstrates the key hypothesis that almost any reasonable-sounding goal starts to conflict with basic human needs if a sufficiently capable system pursues it single-mindedly.\nmarily through language. While the nascent field of AI alignment has proposed some mechanisms by which we might mitigate these risks, work in this area is still largely exploratory, with no clear research agenda in place to ensure that powerful models will be safe. If these hypotheses hold, significant further work is needed to ensure a safe long-term outcome. This will be difficult to achieve without a clear accounting of the abilities and limitations of current and plausible near-future systems."
    }, {
      "heading" : "6 Ways to Do Better",
      "text" : "The core issue in this paper is one of sloppy communication about results. The most straightforward step that we can take to remedy underclaiming is to simply use the same practices that we already use to avoid overclaiming: The peer-review process already polices overclaiming to a significant extent, and most researchers have learned to be careful about overclaiming in their writing. We should apply high standards of evidence to our own empirical claims and those of others, both in peer-reviewed venues and in more informal scientific communication, even when those claims are negative and cloaked in a frame of individual or field-level modesty.\nBeyond this, there are specific best practices or research directions that can help make these mistakes harder to make:\nA Rule of Thumb In light of the issues with negative results on older models discussed in Section 2.1, it could be productive to introduce a new heuristic when reviewing or evaluating papers that discuss model failures.7 In the spirit of the Bender Rule (Bender, 2019), I propose:\nWhen describing the failure of a machine learning model on some empirical evaluation, make it clear\ni. what kind of model has failed, ii. whether the model is significantly\nless capable than the current state of the art in the domain, and\niii. whether the evaluation was deliberately set up to trick that model or another model like it.\n7While a corresponding rule could be helpful in the context of results describing the success of a machine learning system on some evaluation, the asymmetry here is intentional: Successes are likely to be deliberately replicated from one generation of models to the next, while the opposite is true of failures.\nBetter Evaluation The pervasiveness of underclaiming can likely be attributed, at least in large part, to the ineffectiveness of current evaluation practices in many areas of NLP. If encouraging numbers on widely-used benchmarks are usually followed by disappointment, suggesting that good evaluation numbers don’t translate to effective systems, it is rational to treat new encouraging results with extreme skepticism.\nBetter benchmarks and evaluation practices could help mitigate this by providing a firmer ground on which to make positive claims about system capacities.8 In practice, research into more effective crowdsourcing and benchmark design and research into better statistical reporting and publication norms seem especially high-impact under this lens.\nBetter Analysis We can help address the timelag issue discussed in Section 2.1 by building tooling to make it easier to adapt existing analysis techniques to new models seamlessly. Leaderboards that integrate conventional benchmarking with analysis can be especially helpful by making this largely automatic (Wang et al., 2018; Dua et al., 2019; Gehrmann et al., 2021).\nBetter Forecasting Scaling laws results in NLP (Hestness et al., 2017; Kaplan et al., 2020; Brown et al., 2020) offer the promise that we can predict the performance of future larger-scale machine learning models on at least some metrics. This line of work is still nascent, and successes to date have largely focused on loss values rather than more interpretable measures of capability. Further developing these methods, as well as others that allow us to better forecast near future progress, should be helpful. Better forecasting will provide a useful way to sanity-check future claims (DellaVigna et al., 2019) and will help improve the responsiveness of model analysis by enabling us to prepare analysis methods and datasets that anticipate future capabilities."
    }, {
      "heading" : "7 Additional Related Work",
      "text" : "While much of this paper discusses the state of the NLP literature, a few related works that warrant emphasis as starting points for further reading:\nBender and Koller (2020), Bender et al. (2021), and Raji et al. (2021) discuss the role of hype in\n8Though Raji et al. (2021) point out ways in which better benchmarking alone is unlikely to be fully satisfactory.\ndriving bad outcomes from the development of language technology. Jin et al. (2021) and Rogers (2021) offer broader discussion of how to ensure that the net impact of near-future NLP deployments on the world is positive. Morris et al. (2020) and Hauser et al. (2021) highlight overly strong negative claims in papers analyzing models’ robustness to synonym substitution. Looking to the longer term, Bommasani et al. (2021, §4.9) provides an introduction to the AI risk and AI alignment literature from a perspective that emphasizes NLP and language. Welty et al. (2019), Linzen (2020), Ribeiro et al. (2020), Raji et al. (2021), Bowman and Dahl (2021), and Dehghani et al. (2021), among many others, discuss the challenges involved in designing evaluations that yield trustworthy and accurate depictions of the capabilities of ML models."
    }, {
      "heading" : "8 Conclusion",
      "text" : "Like many research fields that have a tight connection to technological practice, NLP has long struggled to avoid inflated expectations about the capabilities of state-of-the-art tools. This remains a serious issue. However, this paper argues that our attempts to avoid hype often overshoot: Instead of merely correcting overly optimistic claims about our capabilities, we replace them with overly pessimistic claims.\nMaking misleading claims is generally a bad sign for the health and credibility of a scientific field, and the stakes are high: NLP technologies are implicated in a range of serious real-world harms, and plausible future elaborations of these technologies are potentially much more dangerous still. Our ability to mitigate existing harms will depend on our ability to make reliably credible claims about the limitations of our systems. Our ability to mitigate future harms will depend on our ability to accurately anticipate, recognize, agree upon, and report upon emerging capabilities. Both of these goals are seriously hampered by claims that current technologies are less capable than they in fact are.\nBetter evaluation, better tooling for model analysis, and better mechanisms for technical forecasting should all contribute to making these pessimistic claims easier to avoid or debunk. However, this problem is ultimately one of scientific communication, and to solve it fully, we will need to use the tools and norms of science to better police false or misleading claims."
    } ],
    "references" : [ {
      "title" : "Thinking inside the box: Controlling and using an oracle AI",
      "author" : [ "Stuart Armstrong", "Anders Sandberg", "Nick Bostrom." ],
      "venue" : "Minds and Machines, 22(4):299–324.",
      "citeRegEx" : "Armstrong et al\\.,? 2012",
      "shortCiteRegEx" : "Armstrong et al\\.",
      "year" : 2012
    }, {
      "title" : "Beat the AI: Investigating adversarial human annotation for reading comprehension",
      "author" : [ "Max Bartolo", "Alastair Roberts", "Johannes Welbl", "Sebastian Riedel", "Pontus Stenetorp." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:662–678.",
      "citeRegEx" : "Bartolo et al\\.,? 2020",
      "shortCiteRegEx" : "Bartolo et al\\.",
      "year" : 2020
    }, {
      "title" : "The #benderrule: On naming the languages we study and why it matters",
      "author" : [ "Emily Bender." ],
      "venue" : "The Gradient.",
      "citeRegEx" : "Bender.,? 2019",
      "shortCiteRegEx" : "Bender.",
      "year" : 2019
    }, {
      "title" : "On the dangers of stochastic parrots: Can language models be too big? [parrot emoji",
      "author" : [ "Emily M. Bender", "Timnit Gebru", "Angelina McMillanMajor", "Shmargaret Shmitchell." ],
      "venue" : "Proceedings of the 2021 ACM Conference on Fairness, Accountability,",
      "citeRegEx" : "Bender et al\\.,? 2021",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "Climbing towards NLU: On meaning, form, and understanding in the age of data",
      "author" : [ "Emily M. Bender", "Alexander Koller." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online. As-",
      "citeRegEx" : "Bender and Koller.,? 2020",
      "shortCiteRegEx" : "Bender and Koller.",
      "year" : 2020
    }, {
      "title" : "On attention redundancy: A comprehensive study",
      "author" : [ "Yuchen Bian", "Jiaji Huang", "Xingyu Cai", "Jiahong Yuan", "Kenneth Church." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Bian et al\\.,? 2021",
      "shortCiteRegEx" : "Bian et al\\.",
      "year" : 2021
    }, {
      "title" : "Automatic generation of contrast sets from scene graphs: Probing the compositional consistency of GQA",
      "author" : [ "Yonatan Bitton", "Gabriel Stanovsky", "Roy Schwartz", "Michael Elhadad." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of",
      "citeRegEx" : "Bitton et al\\.,? 2021",
      "shortCiteRegEx" : "Bitton et al\\.",
      "year" : 2021
    }, {
      "title" : "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "author" : [ "Tolga Bolukbasi", "Kai-Wei Chang", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai." ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Bolukbasi et al\\.,? 2016",
      "shortCiteRegEx" : "Bolukbasi et al\\.",
      "year" : 2016
    }, {
      "title" : "On the opportunities and risks of foundation models. arXiv preprint 2108.07258",
      "author" : [ "Rishi Bommasani", "Drew A Hudson", "Ehsan Adeli", "Russ Altman", "Simran Arora", "Sydney von Arx", "Michael S Bernstein", "Jeannette Bohg", "Antoine Bosselut", "Emma Brunskill" ],
      "venue" : null,
      "citeRegEx" : "Bommasani et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bommasani et al\\.",
      "year" : 2021
    }, {
      "title" : "Ethical issues in advanced artificial intelligence",
      "author" : [ "Nick Bostrom." ],
      "venue" : "Science fiction and philosophy: from time travel to superintelligence, pages 277– 284.",
      "citeRegEx" : "Bostrom.,? 2003",
      "shortCiteRegEx" : "Bostrom.",
      "year" : 2003
    }, {
      "title" : "The superintelligent will: Motivation and instrumental rationality in advanced artificial agents",
      "author" : [ "Nick Bostrom." ],
      "venue" : "Minds and Machines, 22(2):71–85.",
      "citeRegEx" : "Bostrom.,? 2012",
      "shortCiteRegEx" : "Bostrom.",
      "year" : 2012
    }, {
      "title" : "What will it take to fix benchmarking in natural language understanding",
      "author" : [ "Samuel R. Bowman", "George Dahl" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Bowman and Dahl.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bowman and Dahl.",
      "year" : 2021
    }, {
      "title" : "The malicious use of artificial intelligence: Forecasting, prevention, and mitigation",
      "author" : [ "Miles Brundage", "Shahar Avin", "Jack Clark", "Helen Toner", "Peter Eckersley", "Ben Garfinkel", "Allan Dafoe", "Paul Scharre", "Thomas Zeitzoff", "Bobby Filar" ],
      "venue" : null,
      "citeRegEx" : "Brundage et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Brundage et al\\.",
      "year" : 2018
    }, {
      "title" : "Low-complexity probing via finding subnetworks",
      "author" : [ "Steven Cao", "Victor Sanh", "Alexander Rush." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Cao et al\\.,? 2021",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2021
    }, {
      "title" : "The Alignment Problem: Machine Learning and Human Values",
      "author" : [ "Brian Christian." ],
      "venue" : "WW Norton & Company.",
      "citeRegEx" : "Christian.,? 2020",
      "shortCiteRegEx" : "Christian.",
      "year" : 2020
    }, {
      "title" : "AI research considerations for human existential safety (ARCHES)",
      "author" : [ "Andrew Critch", "David Krueger." ],
      "venue" : "arXiv preprint 2006.04948.",
      "citeRegEx" : "Critch and Krueger.,? 2020",
      "shortCiteRegEx" : "Critch and Krueger.",
      "year" : 2020
    }, {
      "title" : "AI governance: a research agenda",
      "author" : [ "Allan Dafoe." ],
      "venue" : "Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK, 1442:1443.",
      "citeRegEx" : "Dafoe.,? 2018",
      "shortCiteRegEx" : "Dafoe.",
      "year" : 2018
    }, {
      "title" : "AI advances to better detect hate speech",
      "author" : [ "Ryan Dansby", "Han Fang", "Hao Ma", "Chris Moghbel", "Umut Ozertem", "Xiaochang Peng", "Ves Stoyanov", "Sinong Wang", "Fan Yang", "Kevin Zhang." ],
      "venue" : "Facebook AI Blog.",
      "citeRegEx" : "Dansby et al\\.,? 2020",
      "shortCiteRegEx" : "Dansby et al\\.",
      "year" : 2020
    }, {
      "title" : "Amazon scraps secret AI recruiting tool that showed bias against women",
      "author" : [ "Jeffrey Dastin." ],
      "venue" : "Reuters. 9",
      "citeRegEx" : "Dastin.,? 2018",
      "shortCiteRegEx" : "Dastin.",
      "year" : 2018
    }, {
      "title" : "The benchmark lottery",
      "author" : [ "Mostafa Dehghani", "Yi Tay", "Alexey A Gritsenko", "Zhe Zhao", "Neil Houlsby", "Fernando Diaz", "Donald Metzler", "Oriol Vinyals." ],
      "venue" : "arXiv preprint 2107.07002.",
      "citeRegEx" : "Dehghani et al\\.,? 2021",
      "shortCiteRegEx" : "Dehghani et al\\.",
      "year" : 2021
    }, {
      "title" : "Predict science to improve science",
      "author" : [ "Stefano DellaVigna", "Devin Pope", "Eva Vivalt." ],
      "venue" : "Science, 366(6464):428–429.",
      "citeRegEx" : "DellaVigna et al\\.,? 2019",
      "shortCiteRegEx" : "DellaVigna et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards interpreting and mitigating shortcut learning behavior of NLU models",
      "author" : [ "Mengnan Du", "Varun Manjunatha", "Rajiv Jain", "Ruchi Deshpande", "Franck Dernoncourt", "Jiuxiang Gu", "Tong Sun", "Xia Hu." ],
      "venue" : "Proceedings of the 2021 Conference of the",
      "citeRegEx" : "Du et al\\.,? 2021",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2021
    }, {
      "title" : "Orb: An open reading benchmark for comprehensive evaluation of machine reading comprehension",
      "author" : [ "Dheeru Dua", "Ananth Gottumukkala", "Alon Talmor", "Sameer Singh", "Matt Gardner." ],
      "venue" : "EMNLP 2019 MRQA Workshop, page 147.",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "F. Liu", "Phoebe Mulcaire", "Qiang Ning", "Sameer Singh", "Noah A. Smith", "Sanjay Subramanian", "Reut Tsarfaty", "Eric Wallace", "Ally Zhang", "Ben Zhou" ],
      "venue" : "In Findings of the Association",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "When will ai exceed human performance? evidence from ai experts",
      "author" : [ "Katja Grace", "John Salvatier", "Allan Dafoe", "Baobao Zhang", "Owain Evans." ],
      "venue" : "Journal of Artificial Intelligence Research, 62:729– 754.",
      "citeRegEx" : "Grace et al\\.,? 2018",
      "shortCiteRegEx" : "Grace et al\\.",
      "year" : 2018
    }, {
      "title" : "Do syntactic probes probe syntax? experiments",
      "author" : [ "Rowan Hall Maudslay", "Ryan Cotterell" ],
      "venue" : null,
      "citeRegEx" : "Maudslay and Cotterell.,? \\Q2021\\E",
      "shortCiteRegEx" : "Maudslay and Cotterell.",
      "year" : 2021
    }, {
      "title" : "Achieving human parity on automatic chinese to english news",
      "author" : [ "Hany Hassan", "Anthony Aue", "Chang Chen", "Vishal Chowdhary", "Jonathan Clark", "Christian Federmann", "Xuedong Huang", "Marcin Junczys-Dowmunt", "William Lewis", "Mu Li" ],
      "venue" : null,
      "citeRegEx" : "Hassan et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert is robust! a case against synonym-based adversarial examples in text classification",
      "author" : [ "Jens Hauser", "Zhao Meng", "Damián Pascual", "Roger Wattenhofer." ],
      "venue" : "arXiv preprint 2109.07403.",
      "citeRegEx" : "Hauser et al\\.,? 2021",
      "shortCiteRegEx" : "Hauser et al\\.",
      "year" : 2021
    }, {
      "title" : "DeBERTa: Decodingenhanced BERT with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "arXiv preprint 2006.03654.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning AI with shared human values",
      "author" : [ "Dan Hendrycks", "Collin Burns", "Steven Basart", "Andrew Critch", "Jerry Li", "Dawn Song", "Jacob Steinhardt." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Hendrycks et al\\.,? 2020",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep learning scaling is predictable, empirically",
      "author" : [ "Joel Hestness", "Sharan Narang", "Newsha Ardalani", "Gregory Diamos", "Heewoo Jun", "Hassan Kianinejad", "Md Patwary", "Mostofa Ali", "Yang Yang", "Yanqi Zhou." ],
      "venue" : "arXiv preprint 1712.00409.",
      "citeRegEx" : "Hestness et al\\.,? 2017",
      "shortCiteRegEx" : "Hestness et al\\.",
      "year" : 2017
    }, {
      "title" : "DeSePtion: Dual sequence prediction and adversarial examples for improved fact-checking",
      "author" : [ "Christopher Hidey", "Tuhin Chakrabarty", "Tariq Alhindi", "Siddharth Varia", "Kriste Krstovski", "Mona Diab", "Smaranda Muresan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Hidey et al\\.,? 2020",
      "shortCiteRegEx" : "Hidey et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual language models predict human reading behavior",
      "author" : [ "Nora Hollenstein", "Federico Pirovano", "Ce Zhang", "Lena Jäger", "Lisa Beinborn." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Hollenstein et al\\.,? 2021",
      "shortCiteRegEx" : "Hollenstein et al\\.",
      "year" : 2021
    }, {
      "title" : "The importance of modeling social factors of language: Theory and practice",
      "author" : [ "Dirk Hovy", "Diyi Yang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hovy and Yang.,? 2021",
      "shortCiteRegEx" : "Hovy and Yang.",
      "year" : 2021
    }, {
      "title" : "Risks from learned optimization in advanced machine learning systems",
      "author" : [ "Evan Hubinger", "Chris van Merwijk", "Vladimir Mikulik", "Joar Skalse", "Scott Garrabrant." ],
      "venue" : "arXiv preprint 1906.01820. 10",
      "citeRegEx" : "Hubinger et al\\.,? 2019",
      "shortCiteRegEx" : "Hubinger et al\\.",
      "year" : 2019
    }, {
      "title" : "AI safety via debate",
      "author" : [ "Geoffrey Irving", "Paul Christiano", "Dario Amodei." ],
      "venue" : "arXiv preprint 1805.00899.",
      "citeRegEx" : "Irving et al\\.,? 2018",
      "shortCiteRegEx" : "Irving et al\\.",
      "year" : 2018
    }, {
      "title" : "NoiER: An approach for training more reliable finetuned downstream task models",
      "author" : [ "Myeongjun Jang", "Thomas Lukasiewicz." ],
      "venue" : "arXiv preprint 2110.02054.",
      "citeRegEx" : "Jang and Lukasiewicz.,? 2021",
      "shortCiteRegEx" : "Jang and Lukasiewicz.",
      "year" : 2021
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "How good is NLP? a sober look at NLP tasks through the lens of social impact",
      "author" : [ "Zhijing Jin", "Geeticka Chauhan", "Brian Tse", "Mrinmaya Sachan", "Rada Mihalcea." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
      "citeRegEx" : "Jin et al\\.,? 2021",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2021
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Scaling laws for neural language models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei." ],
      "venue" : "arXiv preprint 2001.08361.",
      "citeRegEx" : "Kaplan et al\\.,? 2020",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "On the efficacy of adversarial data collection for question answering: Results from a large-scale randomized study",
      "author" : [ "Divyansh Kaushik", "Douwe Kiela", "Zachary C. Lipton", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association",
      "citeRegEx" : "Kaushik et al\\.,? 2021",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2021
    }, {
      "title" : "Alignment of language agents",
      "author" : [ "Zachary Kenton", "Tom Everitt", "Laura Weidinger", "Iason Gabriel", "Vladimir Mikulik", "Geoffrey Irving." ],
      "venue" : "arXiv preprint 2103.14659.",
      "citeRegEx" : "Kenton et al\\.,? 2021",
      "shortCiteRegEx" : "Kenton et al\\.",
      "year" : 2021
    }, {
      "title" : "Dynabench: Rethinking benchmarking in NLP",
      "author" : [ "hit Bansal", "Christopher Potts", "Adina Williams" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Bansal et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2021
    }, {
      "title" : "Has machine translation achieved human parity? a case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796,",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "The Winograd schema challenge",
      "author" : [ "Hector J Levesque", "Ernest Davis", "Leora Morgenstern." ],
      "venue" : "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, page 47.",
      "citeRegEx" : "Levesque et al\\.,? 2011",
      "shortCiteRegEx" : "Levesque et al\\.",
      "year" : 2011
    }, {
      "title" : "How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5210– 5217, Online",
      "author" : [ "Tal Linzen." ],
      "venue" : "Association for Computational Lin-",
      "citeRegEx" : "Linzen.,? 2020",
      "shortCiteRegEx" : "Linzen.",
      "year" : 2020
    }, {
      "title" : "Adversarial training for large neural language models",
      "author" : [ "Xiaodong Liu", "Hao Cheng", "Pengcheng He", "Weizhu Chen", "Yu Wang", "Hoifung Poon", "Jianfeng Gao." ],
      "venue" : "arXiv preprint 2004.08994.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint 1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand? Transactions of the Association for Computational Lin",
      "author" : [ "William Merrill", "Yoav Goldberg", "Roy Schwartz", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Merrill et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Merrill et al\\.",
      "year" : 2021
    }, {
      "title" : "Reevaluating adversarial examples in natural language",
      "author" : [ "John Morris", "Eli Lifland", "Jack Lanchantin", "Yangfeng Ji", "Yanjun Qi." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3829–3839, Online. Association for",
      "citeRegEx" : "Morris et al\\.,? 2020",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding searches better than ever before",
      "author" : [ "Pandu Nayak." ],
      "venue" : "The Keyword blog.",
      "citeRegEx" : "Nayak.,? 2019",
      "shortCiteRegEx" : "Nayak.",
      "year" : 2019
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing neural network comprehension of natural language arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658–4664, Florence, Italy. Association",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "The basic AI drives",
      "author" : [ "Stephen M Omohundro." ],
      "venue" : "Artificial General Intelligence 2008, pages 483–492. IOS Press.",
      "citeRegEx" : "Omohundro.,? 2008",
      "shortCiteRegEx" : "Omohundro.",
      "year" : 2008
    }, {
      "title" : "The precipice: existential risk and the future of humanity",
      "author" : [ "Toby Ord." ],
      "venue" : "Hachette Books.",
      "citeRegEx" : "Ord.,? 2020",
      "shortCiteRegEx" : "Ord.",
      "year" : 2020
    }, {
      "title" : "Inherent disagreements in human textual inferences",
      "author" : [ "Ellie Pavlick", "Tom Kwiatkowski." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:677–694.",
      "citeRegEx" : "Pavlick and Kwiatkowski.,? 2019",
      "shortCiteRegEx" : "Pavlick and Kwiatkowski.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "The croatian psycholinguistic database: estimates for 6000 nouns",
      "author" : [ "Anita Peti-Stantić", "Maja And̄el", "Vedrana Gnjidić", "Gordana Keresteš", "Nikola Ljubešić", "Irina Masnikosa", "Mirjana Tonković", "Jelena Tušek", "Jana Willer-Gold", "Mateusz-Milan Stanojević" ],
      "venue" : null,
      "citeRegEx" : "Peti.Stantić et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Peti.Stantić et al\\.",
      "year" : 2021
    }, {
      "title" : "An empirical comparison of instance attribution methods for NLP",
      "author" : [ "Pouya Pezeshkpour", "Sarthak Jain", "Byron Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Pezeshkpour et al\\.,? 2021",
      "shortCiteRegEx" : "Pezeshkpour et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "Unpublished ms. available through a link at https://blog. openai.com/language-unsupervised/.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Ai and the everything in the whole wide world benchmark",
      "author" : [ "Inioluwa Deborah Raji", "Emily Denton", "Emily M Bender", "Alex Hanna", "Amandalynne Paullada." ],
      "venue" : "Proceedings of The ML-Retrospectives, Surveys & Meta-Analyses @",
      "citeRegEx" : "Raji et al\\.,? 2021",
      "shortCiteRegEx" : "Raji et al\\.",
      "year" : 2021
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Changing the world by changing the data",
      "author" : [ "Anna Rogers." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
      "citeRegEx" : "Rogers.,? 2021",
      "shortCiteRegEx" : "Rogers.",
      "year" : 2021
    }, {
      "title" : "Gender bias in coreference resolution",
      "author" : [ "Rachel Rudinger", "Jason Naradowsky", "Brian Leonard", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Rudinger et al\\.,? 2018",
      "shortCiteRegEx" : "Rudinger et al\\.",
      "year" : 2018
    }, {
      "title" : "Artificial Intelligence: A Modern Approach (Fourth Edition)",
      "author" : [ "Stuart J. Russell", "Peter Norvig." ],
      "venue" : "Pearson.",
      "citeRegEx" : "Russell and Norvig.,? 2020",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2020
    }, {
      "title" : "The Dawn of the Human-Machine Era: A forecast of new and emerging language technologies",
      "author" : [ "Turchi", "Sule Yildirim Yayilgan." ],
      "venue" : "HAL preprint 03230287.",
      "citeRegEx" : "Turchi and Yayilgan.,? 2021",
      "shortCiteRegEx" : "Turchi and Yayilgan.",
      "year" : 2021
    }, {
      "title" : "Reasonet: Learning to stop reading in machine comprehension",
      "author" : [ "Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Mediators in determining what processing BERT performs first",
      "author" : [ "Aviv Slobodkin", "Leshem Choshen", "Omri Abend." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Slobodkin et al\\.,? 2021",
      "shortCiteRegEx" : "Slobodkin et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to summarize with human feedback",
      "author" : [ "Nisan Stiennon", "Long Ouyang", "Jeffrey Wu", "Daniel Ziegler", "Ryan Lowe", "Chelsea Voss", "Alec Radford", "Dario Amodei", "Paul F Christiano." ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Stiennon et al\\.,? 2020",
      "shortCiteRegEx" : "Stiennon et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving ratings’: audit in the British University system",
      "author" : [ "Marilyn Strathern." ],
      "venue" : "European review, 5(3):305–321.",
      "citeRegEx" : "Strathern.,? 1997",
      "shortCiteRegEx" : "Strathern.",
      "year" : 1997
    }, {
      "title" : "oLMpics-on what language model pre-training captures",
      "author" : [ "Alon Talmor", "Yanai Elazar", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:743–758.",
      "citeRegEx" : "Talmor et al\\.,? 2020",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding the capabilities, limitations, and societal impact of large language models",
      "author" : [ "Alex Tamkin", "Miles Brundage", "Jack Clark", "Deep Ganguli." ],
      "venue" : "arXiv preprint 2102.02503.",
      "citeRegEx" : "Tamkin et al\\.,? 2021",
      "shortCiteRegEx" : "Tamkin et al\\.",
      "year" : 2021
    }, {
      "title" : "Attaining the unattainable? Reassessing claims of human parity in neural machine translation",
      "author" : [ "Antonio Toral", "Sheila Castilho", "Ke Hu", "Andy Way." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 113–123, Brus-",
      "citeRegEx" : "Toral et al\\.,? 2018",
      "shortCiteRegEx" : "Toral et al\\.",
      "year" : 2018
    }, {
      "title" : "Mind the trade-off: Debiasing NLU models without degrading the in-distribution performance",
      "author" : [ "Prasetya Ajie Utama", "Nafise Sadat Moosavi", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Utama et al\\.,? 2020",
      "shortCiteRegEx" : "Utama et al\\.",
      "year" : 2020
    }, {
      "title" : "Character-level representations improve DRS-based semantic parsing even in the age of BERT",
      "author" : [ "Rik van Noord", "Antonio Toral", "Johan Bos." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Noord et al\\.,? 2020",
      "shortCiteRegEx" : "Noord et al\\.",
      "year" : 2020
    }, {
      "title" : "Concealed data poisoning attacks on NLP models",
      "author" : [ "Eric Wallace", "Tony Zhao", "Shi Feng", "Sameer Singh." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Wallace et al\\.,? 2021",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2021
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Metrology for AI: From benchmarks to instruments",
      "author" : [ "Chris Welty", "Praveen Paritosh", "Lora Aroyo." ],
      "venue" : "arXiv preprint 1911.01875.",
      "citeRegEx" : "Welty et al\\.,? 2019",
      "shortCiteRegEx" : "Welty et al\\.",
      "year" : 2019
    }, {
      "title" : "A non-linear structural probe",
      "author" : [ "Jennifer C. White", "Tiago Pimentel", "Naomi Saphra", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "White et al\\.,? 2021",
      "shortCiteRegEx" : "White et al\\.",
      "year" : 2021
    }, {
      "title" : "ANLIzing the adversarial natural language inference dataset",
      "author" : [ "Adina Williams", "Tristan Thrush", "Douwe Kiela." ],
      "venue" : "arXiv preprint 2010.12729.",
      "citeRegEx" : "Williams et al\\.,? 2020",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursively summarizing books with human feedback",
      "author" : [ "Jeff Wu", "Long Ouyang", "Daniel M Ziegler", "Nissan Stiennon", "Ryan Lowe", "Jan Leike", "Paul Christiano." ],
      "venue" : "arXiv preprint 2109.10862.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Adversarial attacks and defenses in images, graphs and text: A review",
      "author" : [ "Han Xu", "Yao Ma", "Hao-Chen Liu", "Debayan Deb", "Hui Liu", "Ji-Liang Tang", "Anil K Jain." ],
      "venue" : "International Journal of Automation and Computing, 17(2):151–178.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantics-aware BERT for language understanding",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Hai Zhao", "Zuchao Li", "Shuailiang Zhang", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(5):9628–9635.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "The curse of performance instability in analysis datasets: Consequences, source, and suggestions",
      "author" : [ "Xiang Zhou", "Yixin Nie", "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 38,
      "context" : "Over the last few years, natural language processing has seen a wave of surprising negative results overturning previously-reported success stories about what our models can do, and showing that widely-used models are surprisingly brittle (Jia and Liang, 2017; Niven and Kao, 2019; McCoy et al., 2019).",
      "startOffset" : 239,
      "endOffset" : 301
    }, {
      "referenceID" : 55,
      "context" : "Over the last few years, natural language processing has seen a wave of surprising negative results overturning previously-reported success stories about what our models can do, and showing that widely-used models are surprisingly brittle (Jia and Liang, 2017; Niven and Kao, 2019; McCoy et al., 2019).",
      "startOffset" : 239,
      "endOffset" : 301
    }, {
      "referenceID" : 50,
      "context" : "Over the last few years, natural language processing has seen a wave of surprising negative results overturning previously-reported success stories about what our models can do, and showing that widely-used models are surprisingly brittle (Jia and Liang, 2017; Niven and Kao, 2019; McCoy et al., 2019).",
      "startOffset" : 239,
      "endOffset" : 301
    }, {
      "referenceID" : 64,
      "context" : "Adversarial Examples for SQuAD Jia and Liang (2017) published one of the first prominent demonstrations of adversarial failures in neuralnetwork-based NLP, showing that a simple algorithm could automatically augment examples from the SQuAD benchmark (Rajpurkar et al., 2016) in a Model Year SQuAD AS AOS",
      "startOffset" : 250,
      "endOffset" : 274
    }, {
      "referenceID" : 70,
      "context" : "Results cover the best-performing SQuAD model studied by Jia and Liang—ReasoNet (Shen et al., 2017)—and the newer BERT and XLNet models (Devlin et al.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : ", 2017)—and the newer BERT and XLNet models (Devlin et al., 2019; Yang et al., 2019), as tested by Zhou et al.",
      "startOffset" : 44,
      "endOffset" : 84
    }, {
      "referenceID" : 86,
      "context" : ", 2017)—and the newer BERT and XLNet models (Devlin et al., 2019; Yang et al., 2019), as tested by Zhou et al.",
      "startOffset" : 44,
      "endOffset" : 84
    }, {
      "referenceID" : 49,
      "context" : "The BERT-only results, though, represent a clear missed opportunity: There exist newer models like RoBERTa and DeBERTa (Liu et al., 2019; He et al., 2020) which follow nearly identical APIs and architectures to BERT, such that it should generally be possible to reuse any BERToriented analysis method on these newer models without modification.",
      "startOffset" : 119,
      "endOffset" : 154
    }, {
      "referenceID" : 29,
      "context" : "The BERT-only results, though, represent a clear missed opportunity: There exist newer models like RoBERTa and DeBERTa (Liu et al., 2019; He et al., 2020) which follow nearly identical APIs and architectures to BERT, such that it should generally be possible to reuse any BERToriented analysis method on these newer models without modification.",
      "startOffset" : 119,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "Researchers have recently studied more closely the success of large fine-tuned LMs in many NLU tasks and found that models are simply better in leveraging biased patterns instead of capturing a better notion of language understanding for the intended task (Bender and Koller, 2020).",
      "startOffset" : 256,
      "endOffset" : 281
    }, {
      "referenceID" : 1,
      "context" : "Adversarially collected test sets (Bartolo et al., 2020; Nie et al., 2020; Kiela et al., 2021)—or test sets composed of examples that some target sys-",
      "startOffset" : 34,
      "endOffset" : 94
    }, {
      "referenceID" : 54,
      "context" : "Adversarially collected test sets (Bartolo et al., 2020; Nie et al., 2020; Kiela et al., 2021)—or test sets composed of examples that some target sys-",
      "startOffset" : 34,
      "endOffset" : 94
    }, {
      "referenceID" : 83,
      "context" : "This process produces difficult test sets and it can be a useful tool in understanding the limits of existing training sets or models (Williams et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 157
    }, {
      "referenceID" : 58,
      "context" : "resent subjective judgments rather than clear-cut errors (Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 57,
      "endOffset" : 88
    }, {
      "referenceID" : 27,
      "context" : "In one especially clear example, a prominent paper claiming of human parity in machine translation performance (Hassan et al., 2018) severely overstates what has been accomplished relative to commonsense intuitions about what a human-level",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "Whether adversarially collected test sets are appropriate for comparing the relative effectiveness of models is a largely orthogonal issue (Bowman and Dahl, 2021; Kaushik et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 184
    }, {
      "referenceID" : 42,
      "context" : "Whether adversarially collected test sets are appropriate for comparing the relative effectiveness of models is a largely orthogonal issue (Bowman and Dahl, 2021; Kaushik et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "Most prominently, most applied NLP models show serious biases with respect to legally protected attributes like race and gender (Bolukbasi et al., 2016; Rudinger et al., 2018).",
      "startOffset" : 128,
      "endOffset" : 175
    }, {
      "referenceID" : 67,
      "context" : "Most prominently, most applied NLP models show serious biases with respect to legally protected attributes like race and gender (Bolukbasi et al., 2016; Rudinger et al., 2018).",
      "startOffset" : 128,
      "endOffset" : 175
    }, {
      "referenceID" : 53,
      "context" : "We thus are standing on shaky moral grounds when we deploy present systems in highimpact settings, but they are being widely deployed anyway (e.g. Dastin, 2018; Nayak, 2019; Dansby et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 194
    }, {
      "referenceID" : 17,
      "context" : "We thus are standing on shaky moral grounds when we deploy present systems in highimpact settings, but they are being widely deployed anyway (e.g. Dastin, 2018; Nayak, 2019; Dansby et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "progress in NLP, over many years or decades, will lead to upheavals in areas like education, medicine, law, and the service sector more broadly, as well as making mass surveillance and misinformation campaigns far more effective, and opening up additional new use cases that will be hard for us to foresee (Brundage et al., 2018; Tamkin et al., 2021; Bommasani et al., 2021).",
      "startOffset" : 306,
      "endOffset" : 374
    }, {
      "referenceID" : 75,
      "context" : "progress in NLP, over many years or decades, will lead to upheavals in areas like education, medicine, law, and the service sector more broadly, as well as making mass surveillance and misinformation campaigns far more effective, and opening up additional new use cases that will be hard for us to foresee (Brundage et al., 2018; Tamkin et al., 2021; Bommasani et al., 2021).",
      "startOffset" : 306,
      "endOffset" : 374
    }, {
      "referenceID" : 8,
      "context" : "progress in NLP, over many years or decades, will lead to upheavals in areas like education, medicine, law, and the service sector more broadly, as well as making mass surveillance and misinformation campaigns far more effective, and opening up additional new use cases that will be hard for us to foresee (Brundage et al., 2018; Tamkin et al., 2021; Bommasani et al., 2021).",
      "startOffset" : 306,
      "endOffset" : 374
    }, {
      "referenceID" : 25,
      "context" : "Expert forecasts suggest that this could take place within a few decades (Grace et al., 2018).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "To the extent that these concerns are valid, they represent an urgent call for reprioritization within NLP research to favor safety-relevant areas like interpretability, control, and evaluation over scaling, and to push for better oversight and regulation of large-scale research (Dafoe, 2018): Even a small risk of a globally significant catastrophe warrants a dramatic",
      "startOffset" : 280,
      "endOffset" : 293
    }, {
      "referenceID" : 57,
      "context" : "produce highly-capable AI almost entirely unaccountable, and allowing their decisions to play a major role in the trajectory of humanity as a whole (Ord, 2020).",
      "startOffset" : 148,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "Specifying and using safe objectives, such that aggressively optimizing them does not produce catastrophic outcomes, is difficult (Critch and Krueger, 2020).",
      "startOffset" : 130,
      "endOffset" : 156
    }, {
      "referenceID" : 35,
      "context" : "A small flaw in a highly-capable system’s learned understanding of its objective can cause catastrophic failures, even if the true intended objective would have safe (Hubinger et al., 2019).",
      "startOffset" : 166,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "economic, or political power—to maximize the odds that their primary objectives are achieved (Bostrom, 2003; Omohundro, 2008; Bostrom, 2012).",
      "startOffset" : 93,
      "endOffset" : 140
    }, {
      "referenceID" : 56,
      "context" : "economic, or political power—to maximize the odds that their primary objectives are achieved (Bostrom, 2003; Omohundro, 2008; Bostrom, 2012).",
      "startOffset" : 93,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "economic, or political power—to maximize the odds that their primary objectives are achieved (Bostrom, 2003; Omohundro, 2008; Bostrom, 2012).",
      "startOffset" : 93,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "scribed to a well-defined task like question answering, are not exempt from this concern (Armstrong et al., 2012).",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "that it may be difficult to spot ways in which a model is unsafe or to forecast ways in which its behavior might change in novel settings (Critch and Krueger, 2020).",
      "startOffset" : 138,
      "endOffset" : 164
    }, {
      "referenceID" : 36,
      "context" : "This means that it is not enough that it simply be possible for us to develop safe systems, it is additionally necessary that it be nearly as easy and nearly as affordable as developing unsafe systems (Irving et al., 2018).",
      "startOffset" : 201,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : "7 In the spirit of the Bender Rule (Bender, 2019), I propose:",
      "startOffset" : 35,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "Leaderboards that integrate conventional benchmarking with analysis can be especially helpful by making this largely automatic (Wang et al., 2018; Dua et al., 2019; Gehrmann et al., 2021).",
      "startOffset" : 127,
      "endOffset" : 187
    }, {
      "referenceID" : 31,
      "context" : "Better Forecasting Scaling laws results in NLP (Hestness et al., 2017; Kaplan et al., 2020; Brown et al., 2020) offer the promise that we can predict the performance of future larger-scale machine",
      "startOffset" : 47,
      "endOffset" : 111
    }, {
      "referenceID" : 41,
      "context" : "Better Forecasting Scaling laws results in NLP (Hestness et al., 2017; Kaplan et al., 2020; Brown et al., 2020) offer the promise that we can predict the performance of future larger-scale machine",
      "startOffset" : 47,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "Better forecasting will provide a useful way to sanity-check future claims (DellaVigna et al., 2019) and will help improve the responsiveness of model analysis by enabling us to prepare analysis methods and datasets that anticipate future capabilities.",
      "startOffset" : 75,
      "endOffset" : 100
    } ],
    "year" : 0,
    "abstractText" : "Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field’s successes, at least in part in an effort to combat the field’s widespread hype. Though well-meaning, this often yields misleading or even false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_141_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The ability to identify similarity across documents in large scientific corpora is fundamental for many applications, including recommendation (Bhagavatula et al., 2018), exploratory or analogical search (Hope et al., 2017, 2021b; Lissandrini et al., 2019), paper-reviewer matching (Mimno and McCallum, 2007; Berger et al., 2020) and many more uses.\nScientific papers often describe multifaceted arguments and ideas (Hope et al., 2021a; Lahav et al., 2021), suggesting that models capable of matching specific aspects can better capture overall document relatedness, too. For example, sentences in research abstracts can often be categorized as descriptions of objectives, methods, or findings (Kim\n1Code available at: https://anonymous.4open. science/r/aspire-F570\net al., 2011; Chan et al., 2018), centrally important discourse structures of scientific texts.\nIn this paper, we propose a new model for document similarity that makes aspect-level matches across papers and aggregates them into a documentlevel similarity. We focus on sentence-level aspects of paper abstracts, and train multi-vector representations of papers in terms of their contextualized sentence embeddings. To train our models, we leverage a readily available data source: sentences that co-cite multiple papers. Unlike recent work that used citation links for learning scientific document similarity (Cohan et al., 2020), we observe that papers cited in close proximity provide a more precise indication of relatedness. Furthermore, the citing sentences typically describe how the co-cited papers are related, in terms of shared aspects (e.g., similar methods or findings, related challenges or directions, etc.). Building on this observation, we leverage these textual descriptions as a novel source of textual supervision, using them to guide our model to learn which sentence-aspects match without any direct sentence-level supervision. Guidance for the document similarity model is obtained via an auxiliary sentence encoder model that is used for aligning abstract sentences by finding pairs most similar to the citing sentence text.\nOur document similarity objective is modeled as a function of similarity between sentence-level matches. We explore two strategies to aggregate over sentence-level distances between documents. First, a single-match method with minimum L2 distances between document aspect vectors. This approach readily supports approximate nearest neighbor search methods for large-scale retrieval. Second, a multi-match method that computes an Earth Mover’s Distance between documents’ aspect vectors by solving an Optimal Transport problem. This yields a soft sparse matching of aspect vectors, which when combined with their L2 distances gives a document-level distance.\nFinally, as an additional benefit of our representation, our models also support a finer aspectconditional retrieval task (Hope et al., 2017, 2021a; Chan et al., 2018; Mysore et al., 2021) where aspects can be specified by selecting abstract sentences — for example, selecting sentences describing methods and retrieving papers using similar methods. As we show, naively encoding sentences without their context leads to subpar results in this task, and our representation that does take context into account dramatically improves results.\nExtensive empirical evaluation on four English scientific text datasets and seven similarity tasks at the level of documents and sentences demonstrates the effectiveness of our models. These include biomedical document retrieval tasks and a recent faceted query-by-example corpus of computer science papers (Mysore et al., 2021). This latter dataset is used for evaluating retrieval conditioned on specific aspects in context (e.g., for finding papers with similar methods to a query document), demonstrating that our model can be used in this challenging and important setting. In summary, we make the following main contributions:\n1. Multi-Vector Document Similarity Model: We present ASPIRE2, a multi-vector document similarity model that flexibly aggregates over fine-grained sentence-level aspect matches. 2. Co-Citation Context Supervision: We exploit widely-available co-citation sentences as a new source of training data for document similarity and provide a method using a novel form of textual supervision to guide representation learning for aspect matching. 3. State of the Art Results: Our ASPIRE models outperforms strong baseline methods across four datasets for the abstract and aspectconditional similarity tasks."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "Given query document Q and a candidate amongst a set of documents C ∈ C, where documents consists of N sentences 〈S1, S2, . . . SN 〉 we aim to leverage fine-grained document similarity in two problem settings. An abstract level retrieval task (Brown et al., 2019; Cohan et al., 2020) and an aspect-level retrieval task (Mysore et al., 2021):\nDef 1. Retrieval by abstracts: Given query and candidate documents – Q and C a system must output the ranking over C.\n2ASPIRE: Aspectual Scientific Paper Relations.\nDef 2. Aspect-level retrieval by sentences: Given query and candidate documents – Q and C, and a subset of sentences SQ ⊆ Q conditional on which to retrieve documents, a system must output the ranking over C.\nModeling Desiderata: Next, we also outline key desired properties we require from models developed for task definitions 1 and 2. We follow these desiderata when building our methods (§3.1).\n1. Allowing specification of optional fine-grained aspects: We would like models to allow the ability to specify fine-grained query aspects in a query document based on which retrievals should be made. These may be obtained automatically (e.g., with a discourse tagging method) or via user specification. 2. Scalable to large corpora and efficient inference: State of the art retrieval systems often rely on expensive cross-attention mechanisms on query-document pairs making training and inference expensive (Zamani et al., 2018; Lin et al., 2021). This is exacerbated for longer scientific documents requiring specific transformer models (Caciularu et al., 2021). We require our methods to leverage large training corpora and allow efficient inference at scale."
    }, {
      "heading" : "3 Proposed Approach: ASPIRE",
      "text" : "In this section we describe our approach to document similarity – ASPIRE. We model finer-grained matches between documents at the level of sentences via contextual representations and aggregating over matches to obtain similarities between whole documents. We leverage co-citation sentences as a source of document similarity and also as implicit textual supervision describing related aspects of co-cited documents. We formulate our multi-vector models (Luan et al., 2021; Humeau et al., 2020) that can support scalable inference as novel multiple-instance learning (MIL) models."
    }, {
      "heading" : "3.1 Fine-grained Document Similarity",
      "text" : "We assume to be given a training set consisting of sets of documents P which are weakly-labeled for similarity. We leverage widely-available sets of papers co-cited together in the same sentence as similar (see Figure 1). This builds on the observation that co-citations in close proximity (e.g., in the same sentence) are strong indicators for paper relatedness (Gipp and Beel, 2009).\nWe follow the contrastive learning framework, commonly used for learning semantic similarity (Reimers and Gurevych, 2019; Cohan et al., 2020).\nsentences shown are each individually aligned to co-citation context b as per embeddings from BERTE (§3.2.2). Consequently these sentences in a and c are treated as sharing aspects between the co-cited papers to supervise our fine-grained similarity model for single matching.\nWe train models on triples of the form (p, p′, n) where p, p′ ∈ P and n /∈ P is a randomly selected negative, using the triple margin ranking loss Lf (p, p′, n) = max[f(p, p′)−f(p, n)+m, 0], where f(·, ·) is a distance between documents. All pairwise-combinations p, p′ ∈ P are treated as positive pairs in-turn. In this work, we parameterize f based on the distances between finer-grained document aspects A. Given documents p and p′, we focus on a family of functions f of the form:\nf(p, p′) = ∑\n(i,i′)∈Ap×Ap′\nwi,i′ · di,i′ . (1)\nHere, Ap ×A′p represents the space of alignments between aspects of document p and p′, di,i′ denotes a distance between two aspects i, i′, and wi,i′ represents a weight indicating the contribution of the aspect similarity to the overall document similarity. Unlike previous work (Neves et al., 2019; Jain et al., 2018; Hope et al., 2017), we make no assumption on specific aspect semantics in deriving a model architecture, and focus on aspects in the form of general subsets of document sentences.\nFor learning, we only assume to be given document-level supervision (sets of documents P), and no supervision on aspect-level similarity. Our task thus consists of learning wi,i′ and di,i′ via indirect supervision. We cast this problem setting as a novel type of multi-instance learning (MIL) (Ilse et al., 2018) problem. Prior work in MIL broadly aims to learn instance level classifiers given labels for a bag of instances, this bears resemblance to our setting, where instances are aspects A. However, unlike prior MIL work we focus on learning similarity rather than classification. We formulate two variants of f in Equation 1:\n(1) A single match model (§3.2.2) which considers documents similar based on the single most similar alignment îp, îp′ ∈ Ap×A ′ p. This assumes w = 1 for the best alignment and w = 0 elsewhere. (2) A multi match model (§3.2.3) which makes multiple alignments between documents. We find aspect importance weights wi,i′ , by solving an Optimal Transport (OT) problem (Peyré et al., 2019).\nIn both variants, during training we learn contextualized aspect embeddings that minimize the contrastive loss paramertized with f .\nCo-citation Contexts as Supervision: Finally, we present a method for incorporating implicit natural language supervision during training, presented by co-citation sentences which describe specific relations between co-cited documents. For example, Figure 1 shows a case explaining the similarity between the co-cited papers’ methods. We leverage this textual supervision to find a “best” alignment îp, îp′ in the single-alignment variant (1), and for guiding the optimal transport plan in variant (2). We describe the specific model components next."
    }, {
      "heading" : "3.2 Model Description",
      "text" : ""
    }, {
      "heading" : "3.2.1 Document Encoder",
      "text" : "We leverage a pre-trained BERT-based language model as a document encoder as the base of all our methods. Our encoder is mainly intended to output contextualized sentence representations. Given a document title and abstract, this is achieved as:\nS = BERTθ([CLS] Title [SEP] Abstract) (2)\nwhere S ∈ RN×d represents contextualized sentences s1 . . . sN stacked into a matrix. Here, each s is obtained by mean-pooling word-piece embeddings from the final layer of BERTθ for the sentence tokens. Pairwise distances between sentences\ndi,i′ in Eq 1 for p, p′, are represented as a matrix D ∈ RN×N ′ of L2 distances between Sp and Sp′ ."
    }, {
      "heading" : "3.2.2 Single Match & Textual Supervision",
      "text" : "Our single match model makes the assumption that document similarity is explained by a single best match, giving fTS(p, p′) = D[̂ip, îp′ ]. Here, we leverage weak supervision from co-citation contexts for training. This is done by using an auxiliary sentence encoder to compute a maximally aligned sentence îp in co-cited paper p to the co-citation context, similarly îp′ aligns a sentence in p′ to the co-cotation context. Then the two context aligned sentences are treated as aligned to each other, for training. In practice, the same papers P can be co-cited in multiple different papers (in ∼ 30% of co-cited papers) giving us a set of co-citation sentences, e ∈ E and training data of the form (E ,P). Alignments of the sentences in p and p′ to the co-citation contexts e ∈ E are computed as:\nîp, k̂p = argmax i=1...N,k=1...N ′\nRpR T E\nîp′ , k̂p′ = argmax i=1...N,k=1...N ′\nRp′R T E\n(3)\nHere Rp, Rp′ , and R T E are independent sentence representations for p, p ′\nand e, respectively, obtained from a auxiliary sentence encoder BERTE\n(details below), and îp, îp′ represent the single best alignment of sentences across p, p′ “anchored” on textual supervision sentences E . Importantly, this supervision is only used during training time to guide learning. This procedure is depicted in Figure 2b with Figure 1 showing an example.\nCo-citation Context Encoder The encoder BERTE represents a SCIBERT based sentence encoder pre-trained for scientific text similarity. We train BERTE on sets of co-citation contexts referencing the same set of papers (i.e. E) in a contrastive learning setup. This set, E , can be considered as paraphrases since co-citation sentences citing the same papers often describe similar relations between the papers This model is similar to SentenceBERT (Reimers and Gurevych, 2019) and we refer to it as CoSentBert. In training document encoder BERTθ, we keep BERTE frozen. Appendix C presents more detail on BERTE design."
    }, {
      "heading" : "3.2.3 Multiple Matches & Optimal Transport",
      "text" : "While a single best sentence alignment îp, îp′ may sufficiently explain document similarity for some documents and applications, documents often have a stronger and weaker alignments. So, in computing sentence alignments between documents we would like a sparse matching that aptly weights alignments while ignoring non-alignments — cor-\nresponding to learning weights wi,i′ in Eq 1. To model this intuition we leverage optimal transport.\nOptimal Transport The OT problem is constituted by two sets of points, Sp and Sp′ as in our case, and distributions xp and xp′ according to which the set of points is distributed. The OT problem involves computation of a transport plan P̂, which converts xp into xp′ by transporting probability mass while minimizing an aggregate cost computed from the pairwise costs D of aligning the points in Sp and Sp′ . P̂ is constrained such that its columns and rows marginalize respectively to xp and xp′ (so that all mass is accounted for). Specifically, the computation of P̂ takes the form of a constrained linear optimization problem:\nW = min P∈S 〈D,P〉 (4)\n= min P∈S N∑ i=1 N ′∑ j=1 D[i, j]P[i, j] (5)\nS = {P ∈RN×N ′\n+ |P1N′ = xp,P T1N′ = xp′} (6)\nwhereW refers to the Wasserstein or Earth Movers Distance and P̂ is the minimizer resulting from solving Eq 5. Of interest here is an established result which shows P̂ to be sparse withO(N+N ′) non-zero entries (Swanson et al., 2020). Therefore, P̂ represents a soft sparse alignment of sentences and can be used as weights wi,i′ in Eq 1, with document distances computed as fOT(p, p′) = 〈D, P̂〉. Fig 2c presents a schematic for this approach.\nNote that xp and xp′ allow control over importance of sentences in p and p′ in the form of relative probability mass. We compute these distributions using pairwise distances as x = softmax(−s/τ) where sp = mini D and sp′ = minj D, and τ is a softmax temperature hyper-parameter.\nFor our neural network models trained with automatic differentiation, we leverage an entropy regularized version of the Wasserstein distance in Eq 5 (Cuturi, 2013). Here computation of P̂, is achieved via Sinkhorn iterations, a set of iterative linear updates allowing training with autodiff libraries and leveraging GPU computation. Finally, Cuturi (2013) show that computingW with Sinkhorn iterations shows an empirical quadratic complexity, i.e. O(N2) — similar to that of attention as in a model for late interaction (Humeau et al., 2020).\nMulti-task model: To leverage training signals used in both the single and multi-match models, we train a multi match model supervised with textual supervision in a multi-task setup: LfTS + LfOT ."
    }, {
      "heading" : "3.3 Inference",
      "text" : "As outlined in §2, we are interested in a wholeabstract based retrieval (Def 1) and an aspect level retrieval (Def 2). In both setups given a query Q and candidate C documents we denote sentence representations from a trained model by SQ and SC . For both tasks, we compute distances for ranking while controlling the aspects AQ (i.e Ap) over which the weighted sum of Eq 1 is performed.\nWhole abstract retrieval: This corresponds to a setup where all aspects of the query document Ap are used in computing distances between documents. In the single-alignment models, candidates C are ranked based on their maximally aligned sentence with Q using distances from a trained model: îp, îp′ = argmini,j D. The multi match model ranks candidates using the distance 〈D, P̂〉, where P̂ is the solution to transport problem of Eq 5.\nAspect level retrieval: In aspect-level retrieval, a subset of sentences Aq ⊂ AQ is used for query document Q; for candidate documents C, we do not assume to be given specific aspects, and matching is done across all sentences in each C. In the single alignment models, we only consider a subset of the pairwise sentence distances to determine the maximally aligned sentences, giving DA = D[Aq, :]. This corresponds to finding the maximally aligned candidate sentence to the query sentences in Aq. Similarly, in the multiplealignment model we compute the plan P̂Aq based on the subset of sentences corresponding toAq and generate rankings by 〈DAq , P̂Aq〉. Note that SQ in Q is still contextualized, capturing document context of sentences not explicitly used in Aq.\nScaling Inference: Our multi-vector model for single matching performs retrievals via minimum L2 distance. Therefore, this method is amenable to approximate nearest neighbour (ANN) search methods for large-scale retrieval (Andoni et al., 2018; Luan et al., 2021). Retrieval with our singlematch model would involve |AQ| and |Aq| calls to an ANN structure for the whole abstract and aspect-level tasks respectively.\nOn the other hand, as stated earlier our multimatch model using Sinkhorn iterations involves a O(N2) computation (Cuturi, 2013), which is similar to late interaction methods. Humeau et al. (2020) show late interaction models to be significantly cheaper than cross-encoders while retaining most of their performance in ad-hoc search setups. While quadratic, OT computation in practice can\nbe time-consuming, however, recent work of Backurs et al. (2020) has seen development of fast ANN methods for Wasserstein distances with practical run-times significantly smaller than quadratic ones. This promises the use of ANN methods in largescale retrieval with our multi-match model\nIn our results we refer to our text supervised single match method as TSASPIRE, optimal transport multi match method as OTASPIRE, and the multitask trained multi aspect method as TS+OTASPIRE."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "Evaluation data: We evaluate the proposed methods on datasets for whole abstract document similarity and fine-grained document similarity. We overview these below. Appendix B provides detail.\n1. RELISH: An expert annotated dataset of biomedical abstract similarity (Brown et al., 2019).\n2. TRECCOVIDRF: The original TRECCOVID dataset is labelled for ad-hoc search by experts (Voorhees et al., 2021). We reformulate the dataset for abstract similarity, treating all abstracts relevant to one ad-hoc query as similar to each other and dissimilar from abstracts relevant to other queries. 3. SCIDOCS: A benchmark suite of tasks intended for evaluating abstract-level scientific document representations (Cohan et al., 2020). 4. CSFCUBE: Fine-grained retrieval is evaluated using the recent dataset of Mysore et al. (2021), an expert-annotated dataset of machine learning and NLP abstracts labelled against candidates for relevance to one of 3 broad aspects capturing the main components of methodological research: background/objective, method, result. Relevance is labelled for query sentences corresponding to those aspects, while considering the broader relevance of the sentences’ abstract context.\nBaselines: We compare the proposed approaches to three classes of methods. We overview these classes and associated models below, with Appendix D presenting further detail: 1. Sentence models: Sentence embedding models present reasonable baselines since we consider fine-grained matches at the sentence level. These are represented by MPNET-1B, a sentence model trained on over 1 billion text pairs3, Sentence-Bert (SENTBERT) (Reimers and Gurevych, 2019), SIMCSE (Gao et al., 2021), cosentbert of §3.2.2, and ICTSENTBERT (Lee et al., 2019).\n2. Abstract models: The abstract level model 3MPNET-1B: https://bit.ly/2Zbm2Iq\nSPECTER (Cohan et al., 2020), represents a SOTA model for scientific document similarity trained on cited abstract pairs. We also train a variant of this model on co-cited papers: SPECTER-COCITE.\n3. Sentence level models modified for whole abstract similarity: Here we combine the SOTA sentence encoder MPNET-1B with the optimal transport (§3.2.3) for aggregating sentence level matches giving OTMPNET-1B.\nSentence models use the same inference procedure as our single match method, abstract models rank using L2 distances between papers embeddings, and the modified sentence model uses the multi match inference procedure. All reported model hyper-parameters are tuned, trained on 1.3M co-citation triples, and initialized with SPECTER unless noted otherwise.4 Appendices A, E, and F detail training data, algorithms, and hyper-parameters. Next, we present our main results comparing proposed approaches to baselines."
    }, {
      "heading" : "4.1 Results",
      "text" : "Fine-grained similarity: Table 1 presents results on CSFCUBE. We report performance on the three facets background, method, and result annotated in the dataset, and aggregated across all facets. We first make some observations about baseline methods: 1. MPNET-1B outperforms all other sentence level models and a SOTA abstract representation, SPECTER, indicating the value of sentence-level information for capturing fine-grained similarities. With OTMPNET-1B indicating the value of modeling multiple matches. 2. SPECTER– COCITEScib, which is identical to SPECTER but trained on co-citations outperforms it, showing the value of co-citations for fine-grained similarity.\nNext, we examine performance of the proposed methods: 1. First we note that all of the proposed approaches consistently outperform performant prior work, OT/MPNET-1B and SPECTER, by about 5-6 points aggregated across queries. 2. Next, we note that the proposed approaches outperform SPECTER-COCITESpec, trained on co-citations by 2-3 points aggregated across queries. 3. Our single match model trained with textual supervision, TSASPIRE consistently outperforms baselines. 4. Finally, our multi-match model OTASPIRE, while outperforming baselines sees aggregate performance similar to single match methods. This is reasonable given the aspect-specific annotation of\n4Initialization indicated via subscript in tables.\nCSFCUBE where we expect gains from modeling single fine-grained (contextualized) matches rather than aggregating multiple matches.\nNow, we examine facet-specific performance: 1. Performance on background sees higher performance in general and the smallest gains for the proposed approaches. This may be attributed to background similarity being captured in coarse-grained topical similarity, a trait largely captured in existing baselines. 2. method similarity in CSFCUBE presents significant challenges (Mysore et al., 2021, Sec 6) since it relies upon procedural similarities across steps of a method and on domain knowledge based similarities - this is often captured in co-citation data (Fig 1 presents one such complex paraphrase example). We see strongest performance for TSASPIRE here. 3. Finally, given that paper results interpretations are often dependent on all aspects of a given paper, result similarity often depends on similarity across the whole abstract. This leads OTASPIRE which models multiple matches to see strong performance.\nWhole-abstract similarity: Table 2 presents results on TRECCOVIDRF and RELISH. At the outset, we note that while being annotated for wholeabstract relevance, these datasets present different characteristics. While TRECCOVIDRF presents queries centered on a very specific topic, RELISH presents a much more diverse set of queries. Further, TRECCOVIDRF pairs queries with pools of about 9000 candidates while RELISH has about 60\ncandidates per query. Next, we examine baselines. 1. In contrast to fine-grained similarity datasets the best sentence level model MPNET-1B, significantly underperforms an abstract level model, SPECTER, indicating the need for whole abstract representations for these datasets. Aggregating sentence matches as in OTMPNET-1B, drastically improves MPNET-1B. 2. Next, similar to results in Table 1, a model identical to SPECTER, but trained on co-citations, SPECTER-COCITESpec, outper-\nforms SPECTER indicating the value of co-citation signal for whole-abstract similarity too.\nIn examining performance of our proposed methods, we note the following: 1. Across datasets, our method for single matches, TSASPIRE, outperforms context-independent sentence baselines by several points indicating the value of contextualization. However, this method still underperforms abstract-level baselines. 2. However, methods modeling multiple matches, OTASPIRE and TS+OTASPIRE, substantially outperform TSASPIRE as well as baseline prior work SPECTER and OTMPNET-1B. This performance indicates the strength of OT based aggregation of fine-grained matches for abstract level similarity.\nWe present results demonstrating the value of the proposed approach on the SCIDOCS benchmark in Appendix G. Further, we also present a set of ablations in Appendix H. These ablations establish the value of textual supervision over the encoder (BERTE ) used for encoding the text, the value of optimal transport compared to attention alternatives, and alternative single-match models trained without co-citation contexts."
    }, {
      "heading" : "5 Related Work",
      "text" : "Aspect-based paper representations: A large body of work learns structured representations of scientific papers. Jain et al. (2018) present an approach which learns pre-defined aspect (PICO) encoders for biomedical papers. Similarly work of Neves et al. (2019), Chan et al. (2018), and Kobayashi et al. (2018) each label paper texts and then compute aspect-specific embeddings for document classification or ranking using existing methods. This line of work often relies on pre-defined aspects and building aspect-specific methods. Our work leverages co-citation contexts to supervise free-text aspects with a new model, that is also not tied to a specific schema of labels.\nFine-grained document representations: Another similar line of work is modeling fine-grained document-document similarity at the level of words or latent topics. Examples include early work ElArini and Guestrin (2011) presenting paper recommendation methods with unigram-level similarity between papers using authorship and citation links or using latent document topics (Gong et al., 2018; Yurochkin et al., 2019; Dieng et al., 2020).\nOur approach represents documents via sentences, a common and intuitive structure for reason-\ning about scientific document facets (Chan et al., 2018; Zhou et al., 2020). Ginzburg et al. (2021) present a self-supervised model for contextual sentence representations in long documents similar to our ICT baseline (Lee et al., 2019).\nAd-hoc Search: A range of recent work in information retrieval presents multi-vector models intended to capture different aspects of candidate documents with score aggregation relying on summations, max, or attention functions (Khattab and Zaharia, 2020; Luan et al., 2021; Humeau et al., 2020), these however focusing on short-text queries seen in search or question answering (QA). Mitra et al. (2017) explore an approach to model termlevel fine-grained similarities with neural networks, Liu et al. (2018) model fine-grained matches at the level of entity spans, and Akkalyoncu Yilmaz et al. (2019) model document relevance by aggregating sentence relevance. Similarly, recent work of Lee et al. (2021) models fine-grained matches for QA at the phrase level. Importantly, these methods rely on supervision from knowledge bases or QA datasets, limiting applicability to specific span definitions and areas with these resources, often not present in the scientific domain (Hope et al., 2021a).\nA range of modeling approaches in the context of other tasks resemble elements of our approach. We describe these in Appendix J."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We presented ASPIRE, a scientific document similarity model that is trained by leveraging co-citation contexts for learning fine-grained similarity. We use co-citation contexts as a novel form of textual supervision to guide the learning of multivector document representations. Our model outperformed strong baselines on seven document similarity tasks across four English scientific text datasets. Moreover, we showed that a fast singlematch method achieves competitive results, enabling fine-grained document similarity in largescale scientific corpora. A future direction is the interactive use of our methods, with a system allowing users to highlight specific aspects of papers and retrieve contextually-relevant matches. Another promising application is for finding analogies — structural matches between texts describing ideas, as in scientific papers, to boost discovery (Hope et al., 2017, 2021b; Chan et al., 2018)."
    }, {
      "heading" : "A Co-citation Data",
      "text" : "As noted in §3.1, we train the proposed methods on English co-cited papers. We build a dataset of co-cited papers from the S2ORC corpus5 (Lo et al., 2020). Since our evaluation datasets draw on text from different domains we build training sets with co-cited papers for each: biomedicine for RELISH and TRECCOVIDRF, computer science\n5Released under a CC BY-NC 2.0. license.\nfor CSFCUBE, and a 60/40 mix of biomedicine and CS for SCIDOCS. Each dataset contains 1.3M training triples.\nNext we describe construction of our co-citation data given 8.1 million English full text articles in the S2ORC corpus which have been parsed for citation mentions and linked to cited papers in the corpus using automatic tools (Lo et al., 2020):\n• Domain definition: We define our biomedical articles to be those tagged either “Medicine” or “Biology” in S2ORC. “Computer Science” tagged papers are treated as CS papers. • Co-citation contexts: To obtain co-cotation contexts - we first obtain sentence boundaries for co-citation contexts using the en_core_sci_sm pipeline included in spacy.6. • Filtering abstracts: In selecting abstracts for our dataset we retain those that have a minimum of 3 sentences, and a maximum of 20 sentences. Further, abstracts where all the sentences are too small (3 tokens) are excluded. Similarly, abstracts with sentences greater than 80 tokens are excluded. • Selecting training co-citated abstract data {P}: Given contexts with qualifying abstracts as described above, we only retain co-citation contexts with 2 or 3 co-cited papers. A manual examination revealed that larger co-cited sets tended to be more loosely related. • Selecting co-citation sentence training data for BERTE : Note that this represents a sentence encoder trained by treating co-citation contexts referencing the same paper as paraphrases. Here, we select co-citation contexts containing 2 or more co-cited papers as paraphrase sets E . Abstract level training triples for the biomedical and computer science sets are built by treating all unique pairs of papers as positives. 1.3 million triples were used for each domain - these were sampled from larger sets at random."
    }, {
      "heading" : "B Evaluation Dataset Details",
      "text" : "Here we provide further detail on the evaluation datasets overviewed in §4.\nRELISH: An annotated dataset of biomedical abstract queries labelled by experts (Brown et al., 2019). In a number of cases expert annotators are the authors of query papers. Per query candidate\n6https://allenai.github.io/scispacy/\npools are of size 60, with 1638 queries in development and test sets each. Dataset is released under a Creative Commons Attribution 4.0 International License.\nTRECCOVIDRF: While the original TRECCOVID dataset of Voorhees et al. (2021) is labelled for adhoc search by experts, we reformulate the dataset for abstract similarity, treating all documents relevant to one ad-hoc query as similar to each other. From each original query and its respective relevance-labeled documents, we sample an abstract from relevant documents (relevance of 2) and use that as our query document. We treat all other relevant documents as positive examples for the query. Documents relevant for other queries are treated as irrelevant for the sampled query. This results in about 9000 candidates per query abstract in TRECCOVIDRF. TRECCOVIDRF consists of about 1200 queries in the development and test splits each. This dataset builds on the CORD-19 dataset (Wang et al., 2020) released under a Apache License 2.0, the license of TRECCOVID however isn’t clear from the dataset release.\nSCIDOCS: A benchmark suite of tasks intended for abstract-level scientific document representations (Cohan et al., 2020). We evaluate our methods on the tasks of predicting: citations, co-citations, co-views, and co-reads. Per query candidate pools are of size 30 about 1000 queries per task and development and test split. We exclude classification and recommendation sub-tasks relying on additional inference components. Dataset is released under a GNU General Public License v3.0 license.\nCSFCUBE: The dataset consists of 50 queries labelled for relevance against about 120 candidates per query. Dataset is released under a Creative Commons Attribution-NonCommercial 4.0 International license."
    }, {
      "heading" : "C Co-citation Context Encoder",
      "text" : "Here we present details of alternative design choices for our co-citation context encoder. In the use of BERTE , we note in §3.2.2 that this encoder is kept frozen during the course of training BERTθ. Fine-tuning BERTE via a straight-through estimator (Bengio et al., 2013) under-performed freezing it. Using other encoders for scientific text such as SPECTER as BERTE under-performed CoSentBert. A recent strong model for sentence representation MPNet-1B7 lead to similar\n7MPNet-1B: https://bit.ly/2Zbm2Iq\nperformance on abstract and aspect-conditional tasks as CoSentBert, indicating that a minimum requisite sentence encoder is all that is needed for BERTE ."
    }, {
      "heading" : "D Baselines",
      "text" : "Here we provide further detail on the baselines overviewed in §4. MPNET-1B & OTMPNET-1B: A sentence level\nbaseline of a MPNet (Song et al., 2020) base model, fine-tuned on 1.17 billion similar text pairs in a contrastive learning setup.8 This training data broadly represents web and scientific texts. Further we combine MPNET-1B with an OT based aggregation scheme similar to our multi-match model to yield, OTMPNET1B a baseline using optimal transport with a performant sentence encoder. SimCSE: A recent sentence representation model (Gao et al., 2021). We compare to two model variants: an unsupervised model UNSIMCSEBERT, and a variant supervised with NLI data, SUSIMCSE-BERT. Sentence-Bert: A sentence level transformer model fine-tuned on similar sentence pairs (Reimers and Gurevych, 2019). We compare performance to two variants, SENTBERTPP and SENTBERT-NLI, fine-tuned on paraphrases and natural language inference (NLI) data respectively. CoSentBert: The sentence-level model we describe in §3.2.2: A SCIBERT model finetuned on co-citation sentence contexts referencing the same set of co-cited papers. ICTSENTBERT: A SCIBERT sentence model trained using the self-supervised inverse close task (Lee et al., 2019). Here we train abstract sentence representations to capture the semantics of their paragraph contexts. SPECTER: A state of the art abstract level representation (Cohan et al., 2020). Here a SCIBERT model is fine-tuned to maximize similarity between representations of cited papers. We also train a variant of this model on cocited papers: SPECTER-COCITE.\nFor the baselines described above specific model names from the Hugging Face9 and Sentence Transformers10 libraries are as follows:\n8MPNet-1B: https://bit.ly/2Zbm2Iq 9https://huggingface.co/\n10https://www.sbert.net/docs/ pretrained_models.html\nMPNet-1B: HF; sentence-transformers/all-mpnetbase-v2. SimCSE: HF; princeton-nlp/sup-simcse-bert-baseuncased, princeton-nlp/unsup-simcse-bertbase-uncased. Sentence-Bert: ST; Paraphrases: paraphraseTinyBERT-L6-v2. NLI: nli-roberta-base-v2 from the Sentence-Transformers library."
    }, {
      "heading" : "E Training",
      "text" : "All our approaches are trained using the Adam optimizer with an initial linear warm-up for 2000 steps followed by a linear decay using gradient accumilation for a batch size of 30. The margin m in the triplet loss is set to 1. We implement all methods using PyTorch, HuggingFace, and GeomLoss libraries. Training convergence is established based on the loss on a held out set of co-citation data ensuring that training does not rely on a labelled dataset for convergence checks.\nAll experiments were run with data parallelism over servers nodes with the following GPU configurations: 8×12GB NVIDIA GeForce GTX 1080 Ti GPUs, 4×24GB NVIDIA Tesla M40 GPUs, or 2×48GB NVIDIA Quadro RTX 8000 GPUs. Servers had 12-24 CPUs per node and 256-385GB RAM. The training time per experiment varied from 5-20 hours, and the experiments in this paper represent about 4746 GPU hours of training."
    }, {
      "heading" : "F Model Hyper-Parameters",
      "text" : "Here we report the best performing model hyperparameters. This is done per training dataset. For computer science trained models evaluated on CSFCUBE:\n• Specter-CoCiteScib: LR 2e-5. • Specter-CoCiteSpec: LR 2e-5. • TSASPIRESpec: LR 2e-5. • OTASPIRESpec: LR 2e-5. τ 0.5. • TS+OTASPIRESpec: LR 1e-5. τ 0.5.\nFor biomedical trained models evaluated on TRECCOVID and RELISH:\n• Specter-CoCiteScib: LR 2e-5. • Specter-CoCiteSpec: LR 2e-5. • TSASPIRESpec: LR 2e-5. • OTASPIRESpec: LR 2e-5. τ 5000. • TS+OTASPIRESpec: LR 2e-5. τ 5000.\nFor biomedical+computer science trained models evaluated on TRECCOVID and RELISH:\n• Specter-CoCiteScib: LR 2e-5. • Specter-CoCiteSpec: LR 2e-5. • TSASPIRESpec: LR 1e-5. • OTASPIRESpec: LR 1e-5. τ 5000. • TS+OTASPIRESpec: LR 1e-5. τ 5000.\nWe found it beneficial to use a low temperature τ in computing distributions x for OT computation for CSFCUBE - a fine-grained similarity dataset. On the other hand we found it beneficial to use a high temperature τ in computing distributions x, causing it to be effectively uniform, for OT computation in whole-abstract datasets SCIDOCS, RELISH, and TRECCOVIDRF. This is reasonable given the nature of similarity captured in these datasets. Hyper-parameters of the underlying encoders were not changed from their default values – other hyperparameters are common to methods and desribed in §4.\nFinally, in computing OT transport plans, we optimize a entropy regularized objective: min P∈S 〈D,P〉 − 1λh(P). Our experiments use a fixed value of λ = 20. Hyper-parameter tuning: We tune the hyperparameters of all the ablated and proposed methods across the different datasets on development set performance. For CSFCUBE the Aggregated dev set performance was used for computer science training data models, TRECCOVIDRF and RELISH dev sets were used for biomedical data models with ties between the two broken by the more challenging TRECCOVIDRF performance, and computer science +biomedical data models were tuned on average task performance of SCIDOCS tasks. Given the expense of training models (about 20h for the pro-\nposed models) we first tune softmax temperatures then tuned learning rates. Large changes across learning rates weren’t observed for the models. All learning rates are tuned over the range {1e-5, 2e5, 3e-5}, OT sentence softmax temperatures τ are tuned over {0.5, 1, 5, 5000}, and softmax temperatures for ablation A3 was tuned over {0.5, 1, 5}."
    }, {
      "heading" : "G SCIDOCS Benchmark Result",
      "text" : "SciDocs Benchmark: Table 3 indicates performance on the abstract level document similarity benchmark SCIDOCS of Cohan et al. (2020). First we note that the strong performance of SPECTER indicates a smaller gap to be closed. Here, although our proposed methods see similar performance to each other they consistently outperform SPECTER on 3 of 4 tasks establishing state of the art performance. Given SPECTER’s citation training signal and our co-citation signal, we see better performance on the Citations and Co-Citation tasks respectively. Finally, note that our co-citation trained approaches broadly see better performance (1-1.5 points) on extrinsic tasks of Co-Reads and Co-Views indicating the value of this signal."
    }, {
      "heading" : "H Ablations",
      "text" : "Here we ablate a range of model components in establishing factors which contribute performance. In ablations we only report performance on CSFCUBE, TRECCOVIDRF, and RELISH.\nA1. Does TSASPIRE gain from textual supervision over the encoder used to compute alignment? TSASPIRE relies upon a sentence alignment encoder, BERTE in §3.2.2, to compute alignments,\nîp, îp′ , from the co-citation context to the co-cited abstracts. Here we investigate if improvements in TSASPIRE are attributable to BERTE or to the cocitation contexts themselves. We investigate this by comparing the performance of TSASPIRE to a model trained to maximize the alignment between abstract sentences directly computed using BERTE , we refer to this as ABSASPIRE. This may be viewed as a form of knowledge distillation where alignments from a more local sentence encoder model, BERTE , are distilled into the contextual sentence encoder of TSASPIRE. As Table 4 shows, TSASPIRE consistently outperforms ABSASPIRE, indicating the value added by natural language supervision from the co-citation contexts.\nA2. Can multi-aspect matching use attention aggregation instead of optimal transport? Since our multi-aspect match model uses a soft sparse\nmatching with optimal transport we examine contributions of this component by comparing performance of a model (ATTASPIRE) trained with softalignment using an attention mask, A – attention is also a popular choice in prior work Humeau et al. (2020); Zhou et al. (2020). Here, fAtt(p, p′) = 〈D,A〉 with, A = softmax(−D/τ). Note that OT imposes specific inductive bias via the structure of the trasport plan in ensuring it to be a permutation matrix - a desirable property in computing multiple alignments between a set of points. Table 5 examines performance of these model variants. Broadly, ATTASPIRE sees performance comparable or worse than OTASPIRE. While ATTASPIRE sees improved performance in CSFCUBE it sees much larger variation across runs. In our abstract retrieval datasets, where we expect gains from modeling multiple matches, we see better or similar performance from OTASPIRE over ATTASPIRE.\nA3. Can single-match models be learned without co-citation contexts? While our model for single matches leverages weak textual supervision from co-citation contexts, we ask if these models can be learned in the absence of this supervision. We answer this by training a simpler model, MAXASPIRE, which finds the maximally aligned aspects between documents using the representations from BERTθ alone, giving us fMax(p, p ′) = maxi,jD. To examine the role of\nBERTθ we compare performance with different initializations, with SPECTER presenting a initial model fine-tuned for similarity vs SCIBERT which isnt fine-tuned for text similarity.\nWe note the following from the results in Table 6: MAXASPIRE sees a dependence on the underlying encoder, a SCIBERT initialization nearly always sees poorer performance – only seeing performance competitive with TSASPIRE when initialized with SPECTER. This is reasonable given that this model must bootstrap fine-grained similarity while only relying on the encoder induced similarity. In cases where MAXASPIRE matches performance of TSASPIRE it sees larger performance differences across runs which may also be explained by the dependence on the initialization. Finally, TSASPIRE consistently sees similar or better performance with varying initialization, indicating the value of our text supervised method."
    }, {
      "heading" : "I Extended Results",
      "text" : "Tables 1, 2 in §4.1 omit presentation of standard deviations across runs for the proposed approaches for brevity. We include these in Tables 7 and 8."
    }, {
      "heading" : "J Extended Related Work",
      "text" : "A range of modeling approaches in multi-instance learning, models leveraging textual supervision, and optimal transport resemble elements of our approach. We describe these next.\nMulti-instance Learning: Our work applies MIL for learning fine-grained similarity, while prior work has most often been applied to classification or regression tasks (Hope and Shahaf, 2016, 2018; Ilse et al., 2018; Angelidis and Lapata, 2018). Our work bears resemblance to an application of MIL in content based image retrieval (Song and Soleymani, 2019), where MIL is applied to learn alignments between image and text aspects.\nTextual Supervision: Our use of co-citation text as a source of textual supervision draws on other work leveraging textual justifications of labels as a source of supervision for classification tasks (Hancock et al., 2018; Murty et al., 2020) - co-citation contexts may be considered justifications for similarity of co-cited papers. Nie et al. (2020) presents work in a biomedical literature recommendation task, where human justifications of a relevance label are used to identify unigram features indicative of the label and train a recommendation model.\nOptimal Transport: Our use of optimal transport\ndraws on other recent work in learning sparse alignments between texts (Swanson et al., 2020; Tam et al., 2019). Work of Swanson et al. (2020) learns sparse binary alignments for sentence and document similarity tasks to rationalize decisions and Tam et al. (2019) leverage sparse soft alignments between characters for string similarity. Kusner et al. (2015) uses alignment based on word embeddings for document classification tasks using a K-nearest neighbors method. However, applying OT at the word level in scientific documents would lead to a large increase in computational complexity."
    } ],
    "references" : [ {
      "title" : "Cross-domain modeling of sentence-level evidence for document retrieval",
      "author" : [ "References Zeynep Akkalyoncu Yilmaz", "Wei Yang", "Haotian Zhang", "Jimmy Lin." ],
      "venue" : "Proceedings of the 2019 Conference on",
      "citeRegEx" : "Yilmaz et al\\.,? 2019",
      "shortCiteRegEx" : "Yilmaz et al\\.",
      "year" : 2019
    }, {
      "title" : "Approximate nearest neighbor search in high dimensions",
      "author" : [ "Alexandr Andoni", "Piotr Indyk", "Ilya Razenshteyn." ],
      "venue" : "Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018, pages 3287–3318.",
      "citeRegEx" : "Andoni et al\\.,? 2018",
      "shortCiteRegEx" : "Andoni et al\\.",
      "year" : 2018
    }, {
      "title" : "Multiple instance learning networks for fine-grained sentiment analysis",
      "author" : [ "Stefanos Angelidis", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:17–31.",
      "citeRegEx" : "Angelidis and Lapata.,? 2018",
      "shortCiteRegEx" : "Angelidis and Lapata.",
      "year" : 2018
    }, {
      "title" : "Scalable nearest neighbor search for optimal transport",
      "author" : [ "Arturs Backurs", "Yihe Dong", "Piotr Indyk", "Ilya Razenshteyn", "Tal Wagner." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Ma-",
      "citeRegEx" : "Backurs et al\\.,? 2020",
      "shortCiteRegEx" : "Backurs et al\\.",
      "year" : 2020
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Yoshua Bengio", "Nicholas Léonard", "Aaron Courville" ],
      "venue" : null,
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Effective distributed representations for academic expert search",
      "author" : [ "Mark Berger", "Jakub Zavrel", "Paul Groth." ],
      "venue" : "Proceedings of the First Workshop on Scholarly Document Processing, pages 56–71, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Berger et al\\.,? 2020",
      "shortCiteRegEx" : "Berger et al\\.",
      "year" : 2020
    }, {
      "title" : "Content-based citation recommendation",
      "author" : [ "Chandra Bhagavatula", "Sergey Feldman", "Russell Power", "Waleed Ammar." ],
      "venue" : "arXiv preprint arXiv:1802.08301.",
      "citeRegEx" : "Bhagavatula et al\\.,? 2018",
      "shortCiteRegEx" : "Bhagavatula et al\\.",
      "year" : 2018
    }, {
      "title" : "Large expert-curated database for benchmarking document similarity detection in biomedical literature search",
      "author" : [ "Peter Brown", "RELISH Consortium", "Yaoqi Zhou." ],
      "venue" : "Database, 2019. Baz085.",
      "citeRegEx" : "Brown et al\\.,? 2019",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2019
    }, {
      "title" : "CDLM: Cross-document language modeling",
      "author" : [ "Avi Caciularu", "Arman Cohan", "Iz Beltagy", "Matthew Peters", "Arie Cattan", "Ido Dagan." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2648–2662, Punta Cana, Do-",
      "citeRegEx" : "Caciularu et al\\.,? 2021",
      "shortCiteRegEx" : "Caciularu et al\\.",
      "year" : 2021
    }, {
      "title" : "Solvent: A mixed initiative system for finding analogies between research papers",
      "author" : [ "Joel Chan", "Joseph Chee Chang", "Tom Hope", "Dafna Shahaf", "Aniket Kittur." ],
      "venue" : "Proceedings of the ACM on HumanComputer Interaction, 2(CSCW):1–21.",
      "citeRegEx" : "Chan et al\\.,? 2018",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2018
    }, {
      "title" : "Sinkhorn distances: Lightspeed computation of optimal transport",
      "author" : [ "Marco Cuturi." ],
      "venue" : "Advances in neural information processing systems, 26:2292–2300.",
      "citeRegEx" : "Cuturi.,? 2013",
      "shortCiteRegEx" : "Cuturi.",
      "year" : 2013
    }, {
      "title" : "Topic modeling in embedding spaces",
      "author" : [ "Adji B. Dieng", "Francisco J.R. Ruiz", "David M. Blei." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:439–453.",
      "citeRegEx" : "Dieng et al\\.,? 2020",
      "shortCiteRegEx" : "Dieng et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond keyword search: Discovering relevant scientific literature",
      "author" : [ "Khalid El-Arini", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, page 439–447, New",
      "citeRegEx" : "El.Arini and Guestrin.,? 2011",
      "shortCiteRegEx" : "El.Arini and Guestrin.",
      "year" : 2011
    }, {
      "title" : "SimCSE: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "ACL.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Self-supervised document similarity ranking via contextualized language models and hierarchical inference",
      "author" : [ "Dvir Ginzburg", "Itzik Malkiel", "Oren Barkan", "Avi Caciularu", "Noam Koenigstein." ],
      "venue" : "Findings of the Association for Computational Linguis-",
      "citeRegEx" : "Ginzburg et al\\.,? 2021",
      "shortCiteRegEx" : "Ginzburg et al\\.",
      "year" : 2021
    }, {
      "title" : "Citation proximity analysis (cpa): A new approach for identifying related work based on co-citation analysis",
      "author" : [ "Bela Gipp", "Jöran Beel." ],
      "venue" : "ISSI’09: 12th international conference on scientometrics and informetrics, pages 571–575.",
      "citeRegEx" : "Gipp and Beel.,? 2009",
      "shortCiteRegEx" : "Gipp and Beel.",
      "year" : 2009
    }, {
      "title" : "Document similarity for texts of varying lengths via hidden topics",
      "author" : [ "Hongyu Gong", "Tarek Sakakini", "Suma Bhat", "JinJun Xiong." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Gong et al\\.,? 2018",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2018
    }, {
      "title" : "Training classifiers with natural language explanations",
      "author" : [ "Braden Hancock", "Paroma Varma", "Stephanie Wang", "Martin Bringmann", "Percy Liang", "Christopher Ré." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Hancock et al\\.,? 2018",
      "shortCiteRegEx" : "Hancock et al\\.",
      "year" : 2018
    }, {
      "title" : "Extracting a knowledge base of mechanisms from COVID-19 papers",
      "author" : [ "Tom Hope", "Aida Amini", "David Wadden", "Madeleine van Zuylen", "Sravanthi Parasa", "Eric Horvitz", "Daniel Weld", "Roy Schwartz", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2021 Con-",
      "citeRegEx" : "Hope et al\\.,? 2021a",
      "shortCiteRegEx" : "Hope et al\\.",
      "year" : 2021
    }, {
      "title" : "Accelerating innovation through analogy mining",
      "author" : [ "Tom Hope", "Joel Chan", "Aniket Kittur", "Dafna Shahaf." ],
      "venue" : "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’17, page 235–243, New York,",
      "citeRegEx" : "Hope et al\\.,? 2017",
      "shortCiteRegEx" : "Hope et al\\.",
      "year" : 2017
    }, {
      "title" : "Ballpark learning: Estimating labels from rough group comparisons",
      "author" : [ "Tom Hope", "Dafna Shahaf." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 299–314. Springer.",
      "citeRegEx" : "Hope and Shahaf.,? 2016",
      "shortCiteRegEx" : "Hope and Shahaf.",
      "year" : 2016
    }, {
      "title" : "Ballpark crowdsourcing: The wisdom of rough group comparisons",
      "author" : [ "Tom Hope", "Dafna Shahaf." ],
      "venue" : "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 234–242.",
      "citeRegEx" : "Hope and Shahaf.,? 2018",
      "shortCiteRegEx" : "Hope and Shahaf.",
      "year" : 2018
    }, {
      "title" : "Scaling creative inspiration with finegrained functional facets of product ideas",
      "author" : [ "Tom Hope", "Ronen Tamari", "Hyeonsu Kang", "Daniel Hershcovich", "Joel Chan", "Aniket Kittur", "Dafna Shahaf" ],
      "venue" : null,
      "citeRegEx" : "Hope et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hope et al\\.",
      "year" : 2021
    }, {
      "title" : "Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring",
      "author" : [ "Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Humeau et al\\.,? 2020",
      "shortCiteRegEx" : "Humeau et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention-based deep multiple instance learning",
      "author" : [ "Maximilian Ilse", "Jakub Tomczak", "Max Welling." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2127–",
      "citeRegEx" : "Ilse et al\\.,? 2018",
      "shortCiteRegEx" : "Ilse et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning disentangled representations of texts with application to biomedical abstracts",
      "author" : [ "Sarthak Jain", "Edward Banner", "Jan-Willem van de Meent", "Iain J Marshall", "Byron C Wallace." ],
      "venue" : "EMNLP, volume 2018, page 4683.",
      "citeRegEx" : "Jain et al\\.,? 2018",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2018
    }, {
      "title" : "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
      "author" : [ "Omar Khattab", "Matei Zaharia." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,",
      "citeRegEx" : "Khattab and Zaharia.,? 2020",
      "shortCiteRegEx" : "Khattab and Zaharia.",
      "year" : 2020
    }, {
      "title" : "Automatic classification of sentences to support evidence based medicine",
      "author" : [ "Su Nam Kim", "David Martinez", "Lawrence Cavedon", "Lars Yencken." ],
      "venue" : "BMC bioinformatics, volume 12, pages 1–10. BioMed Central.",
      "citeRegEx" : "Kim et al\\.,? 2011",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2011
    }, {
      "title" : "Citation recommendation using distributed representation of discourse facets in scientific articles",
      "author" : [ "Yuta Kobayashi", "Masashi Shimbo", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries, JCDL ’18,",
      "citeRegEx" : "Kobayashi et al\\.,? 2018",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2018
    }, {
      "title" : "From word embeddings to document distances",
      "author" : [ "Matt Kusner", "Yu Sun", "Nicholas Kolkin", "Kilian Weinberger." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Kusner et al\\.,? 2015",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2015
    }, {
      "title" : "A search engine for discovery of scientific challenges and directions",
      "author" : [ "Dan Lahav", "Jon Saad Falcon", "Bailey Kuehl", "Sophie Johnson", "Sravanthi Parasa", "Noam Shomron", "Duen Horng Chau", "Diyi Yang", "Eric Horvitz", "Daniel S Weld" ],
      "venue" : null,
      "citeRegEx" : "Lahav et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lahav et al\\.",
      "year" : 2021
    }, {
      "title" : "Phrase retrieval learns passage retrieval, too",
      "author" : [ "Jinhyuk Lee", "Alexander Wettig", "Danqi Chen." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3661– 3672, Online and Punta Cana, Dominican Republic.",
      "citeRegEx" : "Lee et al\\.,? 2021",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretrained transformers for text ranking: Bert and beyond",
      "author" : [ "Jimmy Lin", "Rodrigo Nogueira", "Andrew Yates." ],
      "venue" : "Synthesis Lectures on Human Language Technologies, 14(4):1–325.",
      "citeRegEx" : "Lin et al\\.,? 2021",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2021
    }, {
      "title" : "Example-based search: a new frontier for exploratory search",
      "author" : [ "Matteo Lissandrini", "Davide Mottin", "Themis Palpanas", "Yannis Velegrakis." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Infor-",
      "citeRegEx" : "Lissandrini et al\\.,? 2019",
      "shortCiteRegEx" : "Lissandrini et al\\.",
      "year" : 2019
    }, {
      "title" : "Entity-duet neural ranking: Understanding the role of knowledge graph semantics in neural information retrieval",
      "author" : [ "Zhenghao Liu", "Chenyan Xiong", "Maosong Sun", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "S2ORC: The semantic scholar open research corpus",
      "author" : [ "Kyle Lo", "Lucy Lu Wang", "Mark Neumann", "Rodney Kinney", "Daniel Weld." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online. As-",
      "citeRegEx" : "Lo et al\\.,? 2020",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 2020
    }, {
      "title" : "Sparse, dense, and attentional representations for text retrieval",
      "author" : [ "Yi Luan", "Jacob Eisenstein", "Kristina Toutanova", "Michael Collins." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:329–345.",
      "citeRegEx" : "Luan et al\\.,? 2021",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2021
    }, {
      "title" : "Expertise modeling for matching papers with reviewers",
      "author" : [ "David Mimno", "Andrew McCallum." ],
      "venue" : "10",
      "citeRegEx" : "Mimno and McCallum.,? 2007",
      "shortCiteRegEx" : "Mimno and McCallum.",
      "year" : 2007
    }, {
      "title" : "Learning to match using local and distributed representations of text for web search",
      "author" : [ "Bhaskar Mitra", "Fernando Diaz", "Nick Craswell." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, WWW ’17, page 1291–1299, Republic",
      "citeRegEx" : "Mitra et al\\.,? 2017",
      "shortCiteRegEx" : "Mitra et al\\.",
      "year" : 2017
    }, {
      "title" : "ExpBERT: Representation engineering with natural language explanations",
      "author" : [ "Shikhar Murty", "Pang Wei Koh", "Percy Liang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2106–2113, Online. Asso-",
      "citeRegEx" : "Murty et al\\.,? 2020",
      "shortCiteRegEx" : "Murty et al\\.",
      "year" : 2020
    }, {
      "title" : "CSFCube - a test collection of computer science research articles for faceted query by example",
      "author" : [ "Sheshera Mysore", "Tim O’Gorman", "Andrew McCallum", "Hamed Zamani" ],
      "venue" : "In Thirty-fifth Conference on Neural Information Processing Systems",
      "citeRegEx" : "Mysore et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Mysore et al\\.",
      "year" : 2021
    }, {
      "title" : "Evaluation of scientific elements for text similarity in biomedical publications",
      "author" : [ "Mariana Neves", "Daniel Butzke", "Barbara Grune." ],
      "venue" : "Proceedings of the 6th Workshop on Argument Mining, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Neves et al\\.,? 2019",
      "shortCiteRegEx" : "Neves et al\\.",
      "year" : 2019
    }, {
      "title" : "LitGen: Genetic Literature Recommendation Guided by Human Explanations, pages 67–78",
      "author" : [ "Allen Nie", "Arturo L. Pineda", "Matt W. Wright", "Hannah Wand", "Bryan Wulf", "Helio A. Costa", "Ronak Y. Patel", "Carlos D. Bustamante", "James Zou" ],
      "venue" : null,
      "citeRegEx" : "Nie et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Computational optimal transport: With applications to data science",
      "author" : [ "Gabriel Peyré", "Marco Cuturi" ],
      "venue" : "Foundations and Trends® in Machine Learning,",
      "citeRegEx" : "Peyré and Cuturi,? \\Q2019\\E",
      "shortCiteRegEx" : "Peyré and Cuturi",
      "year" : 2019
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Mpnet: Masked and permuted pretraining for language understanding",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "NeurIPS 2020. ACM.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Polysemous visual-semantic embedding for cross-modal retrieval",
      "author" : [ "Yale Song", "Mohammad Soleymani." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Song and Soleymani.,? 2019",
      "shortCiteRegEx" : "Song and Soleymani.",
      "year" : 2019
    }, {
      "title" : "Rationalizing text matching: Learning sparse alignments via optimal transport",
      "author" : [ "Kyle Swanson", "Lili Yu", "Tao Lei." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5609–5626, Online. Association",
      "citeRegEx" : "Swanson et al\\.,? 2020",
      "shortCiteRegEx" : "Swanson et al\\.",
      "year" : 2020
    }, {
      "title" : "Optimal transport-based alignment of learned character representations for string similarity",
      "author" : [ "Derek Tam", "Nicholas Monath", "Ari Kobren", "Aaron Traylor", "Rajarshi Das", "Andrew McCallum." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Tam et al\\.,? 2019",
      "shortCiteRegEx" : "Tam et al\\.",
      "year" : 2019
    }, {
      "title" : "Trec-covid: Constructing a pandemic information retrieval test collection",
      "author" : [ "Ellen Voorhees", "Tasmeer Alam", "Steven Bedrick", "Dina Demner-Fushman", "William R. Hersh", "Kyle Lo", "Kirk Roberts", "Ian Soboroff", "Lucy Lu Wang." ],
      "venue" : "SIGIR Forum.",
      "citeRegEx" : "Voorhees et al\\.,? 2021",
      "shortCiteRegEx" : "Voorhees et al\\.",
      "year" : 2021
    }, {
      "title" : "Hierarchical optimal transport for document representation",
      "author" : [ "Mikhail Yurochkin", "Sebastian Claici", "Edward Chien", "Farzaneh Mirzazadeh", "Justin M Solomon." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
      "citeRegEx" : "Yurochkin et al\\.,? 2019",
      "shortCiteRegEx" : "Yurochkin et al\\.",
      "year" : 2019
    }, {
      "title" : "From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing",
      "author" : [ "Hamed Zamani", "Mostafa Dehghani", "W. Bruce Croft", "Erik Learned-Miller", "Jaap Kamps." ],
      "venue" : "Proceedings of the 27th ACM International Conference",
      "citeRegEx" : "Zamani et al\\.,? 2018",
      "shortCiteRegEx" : "Zamani et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilevel text alignment with crossdocument attention",
      "author" : [ "Xuhui Zhou", "Nikolaos Pappas", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5012–5025, On-",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence-Bert: A sentence level transformer model fine-tuned on similar sentence pairs (Reimers and Gurevych, 2019)",
      "author" : [ "data", "SUSIMCSE-BERT" ],
      "venue" : null,
      "citeRegEx" : "data and SUSIMCSE.BERT.,? \\Q2019\\E",
      "shortCiteRegEx" : "data and SUSIMCSE.BERT.",
      "year" : 2019
    }, {
      "title" : "First we note that the strong performance of SPECTER indicates a smaller gap to be closed",
      "author" : [ "benchmark SCIDOCS of Cohan" ],
      "venue" : null,
      "citeRegEx" : "Cohan,? \\Q2020\\E",
      "shortCiteRegEx" : "Cohan",
      "year" : 2020
    }, {
      "title" : "alignment using an attention mask, A – attention is also a popular choice in prior work Humeau et al",
      "author" : [ "Zhou" ],
      "venue" : null,
      "citeRegEx" : "Zhou,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhou",
      "year" : 2020
    }, {
      "title" : "2019) leverage sparse soft alignments between characters for string similarity. Kusner et al. (2015) uses alignment based on word embeddings for document classification tasks using a K-nearest neighbors method",
      "author" : [ "Tam" ],
      "venue" : null,
      "citeRegEx" : "Tam,? \\Q2015\\E",
      "shortCiteRegEx" : "Tam",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "The ability to identify similarity across documents in large scientific corpora is fundamental for many applications, including recommendation (Bhagavatula et al., 2018), exploratory or analogical search (Hope et al.",
      "startOffset" : 143,
      "endOffset" : 169
    }, {
      "referenceID" : 34,
      "context" : ", 2018), exploratory or analogical search (Hope et al., 2017, 2021b; Lissandrini et al., 2019), paper-reviewer matching (Mimno and McCallum, 2007; Berger et al.",
      "startOffset" : 42,
      "endOffset" : 94
    }, {
      "referenceID" : 38,
      "context" : ", 2019), paper-reviewer matching (Mimno and McCallum, 2007; Berger et al., 2020) and many more uses.",
      "startOffset" : 33,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : ", 2019), paper-reviewer matching (Mimno and McCallum, 2007; Berger et al., 2020) and many more uses.",
      "startOffset" : 33,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "Scientific papers often describe multifaceted arguments and ideas (Hope et al., 2021a; Lahav et al., 2021), suggesting that models capable of matching specific aspects can better capture overall document relatedness, too.",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : "Scientific papers often describe multifaceted arguments and ideas (Hope et al., 2021a; Lahav et al., 2021), suggesting that models capable of matching specific aspects can better capture overall document relatedness, too.",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "conditional retrieval task (Hope et al., 2017, 2021a; Chan et al., 2018; Mysore et al., 2021) where aspects can be specified by selecting abstract sentences — for example, selecting sentences describing methods and retrieving papers using similar",
      "startOffset" : 27,
      "endOffset" : 93
    }, {
      "referenceID" : 41,
      "context" : "conditional retrieval task (Hope et al., 2017, 2021a; Chan et al., 2018; Mysore et al., 2021) where aspects can be specified by selecting abstract sentences — for example, selecting sentences describing methods and retrieving papers using similar",
      "startOffset" : 27,
      "endOffset" : 93
    }, {
      "referenceID" : 41,
      "context" : "These include biomedical document retrieval tasks and a recent faceted query-by-example corpus of computer science papers (Mysore et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "An abstract level retrieval task (Brown et al., 2019; Cohan et al., 2020) and an aspect-level retrieval task (Mysore et al.",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 41,
      "context" : ", 2020) and an aspect-level retrieval task (Mysore et al., 2021):",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 52,
      "context" : "ment pairs making training and inference expensive (Zamani et al., 2018; Lin et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 90
    }, {
      "referenceID" : 33,
      "context" : "ment pairs making training and inference expensive (Zamani et al., 2018; Lin et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "This is exacerbated for longer scientific documents requiring specific transformer models (Caciularu et al., 2021).",
      "startOffset" : 90,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : ", in the same sentence) are strong indicators for paper relatedness (Gipp and Beel, 2009).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 45,
      "context" : "We follow the contrastive learning framework, commonly used for learning semantic similarity (Reimers and Gurevych, 2019; Cohan et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 141
    }, {
      "referenceID" : 42,
      "context" : "Unlike previous work (Neves et al., 2019; Jain et al., 2018; Hope et al., 2017), we make no assumption on specific aspect semantics in deriving a model architecture, and focus on aspects in the form of general subsets of document sentences.",
      "startOffset" : 21,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : "Unlike previous work (Neves et al., 2019; Jain et al., 2018; Hope et al., 2017), we make no assumption on specific aspect semantics in deriving a model architecture, and focus on aspects in the form of general subsets of document sentences.",
      "startOffset" : 21,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "Unlike previous work (Neves et al., 2019; Jain et al., 2018; Hope et al., 2017), we make no assumption on specific aspect semantics in deriving a model architecture, and focus on aspects in the form of general subsets of document sentences.",
      "startOffset" : 21,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "We cast this problem setting as a novel type of multi-instance learning (MIL) (Ilse et al., 2018) problem.",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 45,
      "context" : "This set, E , can be considered as paraphrases since co-citation sentences citing the same papers often describe similar relations between the papers This model is similar to SentenceBERT (Reimers and Gurevych, 2019) and we refer to it as CoSentBert.",
      "startOffset" : 188,
      "endOffset" : 216
    }, {
      "referenceID" : 48,
      "context" : "Of interest here is an established result which shows P̂ to be sparse withO(N+N ′) non-zero entries (Swanson et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "For our neural network models trained with automatic differentiation, we leverage an entropy regularized version of the Wasserstein distance in Eq 5 (Cuturi, 2013).",
      "startOffset" : 149,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "O(N2) — similar to that of attention as in a model for late interaction (Humeau et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "Therefore, this method is amenable to approximate nearest neighbour (ANN) search methods for large-scale retrieval (Andoni et al., 2018; Luan et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 155
    }, {
      "referenceID" : 37,
      "context" : "Therefore, this method is amenable to approximate nearest neighbour (ANN) search methods for large-scale retrieval (Andoni et al., 2018; Luan et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, as stated earlier our multimatch model using Sinkhorn iterations involves a O(N2) computation (Cuturi, 2013), which is similar to late interaction methods.",
      "startOffset" : 113,
      "endOffset" : 127
    }, {
      "referenceID" : 50,
      "context" : "TRECCOVIDRF: The original TRECCOVID dataset is labelled for ad-hoc search by experts (Voorhees et al., 2021).",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 45,
      "context" : "These are represented by MPNET-1B, a sentence model trained on over 1 billion text pairs3, Sentence-Bert (SENTBERT) (Reimers and Gurevych, 2019), SIMCSE (Gao et al.",
      "startOffset" : 116,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "These are represented by MPNET-1B, a sentence model trained on over 1 billion text pairs3, Sentence-Bert (SENTBERT) (Reimers and Gurevych, 2019), SIMCSE (Gao et al., 2021), cosentbert of §3.",
      "startOffset" : 153,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "mendation methods with unigram-level similarity between papers using authorship and citation links or using latent document topics (Gong et al., 2018; Yurochkin et al., 2019; Dieng et al., 2020).",
      "startOffset" : 131,
      "endOffset" : 194
    }, {
      "referenceID" : 51,
      "context" : "mendation methods with unigram-level similarity between papers using authorship and citation links or using latent document topics (Gong et al., 2018; Yurochkin et al., 2019; Dieng et al., 2020).",
      "startOffset" : 131,
      "endOffset" : 194
    }, {
      "referenceID" : 11,
      "context" : "mendation methods with unigram-level similarity between papers using authorship and citation links or using latent document topics (Gong et al., 2018; Yurochkin et al., 2019; Dieng et al., 2020).",
      "startOffset" : 131,
      "endOffset" : 194
    }, {
      "referenceID" : 9,
      "context" : "Our approach represents documents via sentences, a common and intuitive structure for reasoning about scientific document facets (Chan et al., 2018; Zhou et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 53,
      "context" : "Our approach represents documents via sentences, a common and intuitive structure for reasoning about scientific document facets (Chan et al., 2018; Zhou et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 32,
      "context" : "present a self-supervised model for contextual sentence representations in long documents similar to our ICT baseline (Lee et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : "mations, max, or attention functions (Khattab and Zaharia, 2020; Luan et al., 2021; Humeau et al., 2020), these however focusing on short-text queries seen in search or question answering (QA).",
      "startOffset" : 37,
      "endOffset" : 104
    }, {
      "referenceID" : 37,
      "context" : "mations, max, or attention functions (Khattab and Zaharia, 2020; Luan et al., 2021; Humeau et al., 2020), these however focusing on short-text queries seen in search or question answering (QA).",
      "startOffset" : 37,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "mations, max, or attention functions (Khattab and Zaharia, 2020; Luan et al., 2021; Humeau et al., 2020), these however focusing on short-text queries seen in search or question answering (QA).",
      "startOffset" : 37,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "Another promising application is for finding analogies — structural matches between texts describing ideas, as in scientific papers, to boost discovery (Hope et al., 2017, 2021b; Chan et al., 2018).",
      "startOffset" : 152,
      "endOffset" : 197
    } ],
    "year" : 0,
    "abstractText" : "We present a new scientific document similarity model based on matching fine-grained aspects of texts. To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations). Such cocitations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related. This novel form of textual supervision is used for learning to match aspects across papers. We develop multi-vector representations where vectors correspond to sentence-level aspects of documents, and present two methods for aspect matching: (1) A fast method that only matches single aspects, and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover’s Distance between aspects. Our approach improves performance on document similarity tasks in four datasets. Further, our fast single-match method achieves competitive results, paving the way for applying finegrained similarity to large scientific corpora.1",
    "creator" : null
  }
}
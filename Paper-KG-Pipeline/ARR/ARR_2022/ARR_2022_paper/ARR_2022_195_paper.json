{
  "name" : "ARR_2022_195_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Grammatical Error Correction (GEC) task has a purpose to correct grammatical errors in natural texts. It includes correcting errors in spelling, punctuation, grammar, morphology, word choice, and others. Intelligent GEC system receives text containing mistakes and produces its corrected version. GEC task is complicated and challenging: the accuracy of edits, inference speed, and memory limitations are the topics of intensive research.\nCurrently, Machine Translation (MT) is the mainstream approach for GEC. In this setting, errorful sentences correspond to the source language, and error-free sentences correspond to the target language. Early GEC-MT methods leveraged phrase-based statistical machine translation (PBSMT) (Yuan and Felice, 2013). Then they\n1To the best of our knowledge, our best single model gives way only to much heavier T5 model (Rothe et al., 2021).\n2http://github.com/ to-appear-after-publication\nrapidly evolved to sequence-to-sequence Neural Machine Translation (NMT) based on gated recurrent neural networks (Yuan and Briscoe, 2016) and recent powerful Transformer-based Seq2Seq models. They autoregressively capture full dependency among output tokens; however, it might be slow due to sequential decoding. (Grundkiewicz et al., 2019) leveraged Transformer model (Vaswani et al., 2017) which was pre-trained on synthetic GEC data and right-to-left re-ranking for ensemble. (Kaneko et al., 2020) adopted several strategies of BERT (Devlin et al., 2018) usage for GEC. Recently, (Rothe et al., 2021) built their system on top of T5 (Xue et al., 2021), a xxl version of T5 Transformer encoder-decoder model and reached new state-of-the-art results (11B parameters).\nThe sequence tagging approach that generates a sequence of text edit operations encoded by tags for errorful input text is becoming more common now. LaserTagger (Malmi et al., 2019) is a sequence tagging model that casts text generation as a text editing task. Corrected texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. LaserTagger combines a BERT encoder with an autoregressive Transformer decoder, which predicts edit operations. Parallel Iterative Edit (PIE) model (Awasthi et al., 2019) does parallel decoding, achieving quality that is competitive with the Seq2Seq models3. It predicts edits instead of tokens and iteratively refines predictions to capture dependencies. A similar approach is presented in (Omelianchuk et al., 2020). GECToR system uses various Transformers as an encoder, linear layers with softmax for tag prediction and error detection instead of a decoder. It also managed to achieve competitive results being potentially several times faster than Seq2Seq because of replacing autoregressive decoder with linear output layers.\n3http://nlpprogress.com/english/ grammatical_error_correction\nAlso, nowadays generation of synthetic data is becoming significant for most GEC models. Natural languages are rich, and their Grammars contain many rules and exceptions; therefore, professional linguists usually need to annotate highquality corpora for further training of ML-based systems mostly in a supervised manner (Dahlmeier et al., 2013), (Bryant et al., 2019). At the same time, human annotation is expensive, so researchers are working on methods for augmentation of training data, synthetic data generation, and strategies for its efficient usage (Lichtarge et al., 2019), (Kiyono et al., 2019), (Stahlberg and Kumar, 2021). Most of the latest works use synthetic data to pre-train Transformer-based components of their models.\nIn this work, we are focusing on exploring sequence tagging models and their ensembles. Although most of our developments might eventually be applied to other languages, we work with English only in this study. Being a rich-resource language, English provides a highly competitive area for GEC task3. We leave dealing with other languages for future work."
    }, {
      "heading" : "2 Base System Overview",
      "text" : ""
    }, {
      "heading" : "2.1 GECToR architecture",
      "text" : "Our tagging models are inherited from the GECToR (Omelianchuk et al., 2020). To date, GECToR shows near-SOTA results on CoNLL-2014 and BEA-2019 benchmarks3. It is based on AllenNLP (Gardner et al., 2017) and HuggingFace’s Transformers (Wolf et al., 2019) libraries, and its source code is freely available4.\nGECToR is a sequence tagging model which contains a Transformer-based encoder stacked with two output linear layers that are responsible for error detection and error correction. They are trained with a cross-entropy loss function to produce tags that encode token-level edits. Then iterative postprocessing is performed. GECToR predicts the tag-encoded transformations for each token in the input sequence; it can then apply these transformations to get the modified output sequence.\nSince some corrections in a sentence may depend on others, applying the GEC sequence-tagger only once may not be enough to correct the sentence entirely. Therefore, they use an iterative correction approach: it modifies the sentence by running the tagger on it again and repeat - up to four times (Fig. 1).\n4https://github.com/grammarly/gector"
    }, {
      "heading" : "2.2 Tag-encoded edit operations",
      "text" : "The primary edit operations are encoded by the following tags: \"KEEP\" - leave the current token unchanged, \"DELETE\" - delete the current token, \"APPEND TOKEN\" - append the token \"TOKEN\" after the current token, \"REPLACE TOKEN\" - replace the current token with the token \"TOKEN\". Also, GECToR has special edit operations, such as transforming to uppercase or lowercase, transforming irregular verbs to their third forms, adding \"s\" word ending, etc. We refer to (Omelianchuk et al., 2020) for details of edit transforms."
    }, {
      "heading" : "2.3 Our contributions",
      "text" : "We claim the following contributions:\n1. We empirically investigate and improve the GECToR sequence tagging system (Omelianchuk et al., 2020) by upgrading Transformer encoders to Large configurations, leveraging advanced tokenizer, additional filtering of edits-free sentences, and increasing vocabulary size.\n2. We show that ensembling of sequence taggers by majority votes on output edit spans provides better performance compared to ensembling by averaging of output tag probabilities while staying tolerant to models’ architecture and vocabulary sizes.\n3. We apply the knowledge distillation method to produce annotated data by the ensemble of sequence taggers. Being trained on the distilled data, single GEC tagging models show competitive performance.\n4. We make the code, datasets, and trained models publicly available."
    }, {
      "heading" : "3 Datasets",
      "text" : ""
    }, {
      "heading" : "3.1 Annotated data",
      "text" : "For training single models and ensembles, we use parallel annotated data from Lang-8 Cor-\npus of Learner English (Lang-8)5 (Tajiri et al., 2012), National University of Singapore Corpus of Learner English (NUCLE)6 (Dahlmeier et al., 2013), First Certificate in English dataset (FCE)7 (Yannakoudakis et al., 2011), and Write & Improve (W&I) Corpus (Bryant et al., 2019)8. Please, see Table 1 for details."
    }, {
      "heading" : "3.2 Monolingual data, distilled data",
      "text" : "For knowledge distillation from the ensemble, we use parts of two monolingual datasets: One Billion Word Benchmark (1BW)9 (Chelba et al., 2013) and The Blog Authorship Corpus (Blogs)10 (Schler et al., 2005). Corresponding distilled datasets have prefixes \"Troy-\"; see more details about their generation in Section V."
    }, {
      "heading" : "3.3 Synthetic data",
      "text" : "After knowledge distillation for the final training of the student model we also use parallel sentences with synthetically generated grammatical errors from the PIE dataset11 (Awasthi et al., 2019).\n5https://sites.google.com/site/ naistlang8corpora\n6https://www.comp.nus.edu.sg/~nlp/ corpora.html\n7https://ilexir.co.uk/datasets/index. html\n8https://www.cl.cam.ac.uk/research/nl/ bea2019st/data/wi+locness_v2.1.bea19.tar. gz\n9http://statmt.org/wmt11/ training-monolingual.tgz\n10https://www.kaggle.com/rtatman/ blog-authorship-corpus\n11https://github.com/awasthiabhijeet/ PIE/tree/master/errorify"
    }, {
      "heading" : "3.4 Evaluation",
      "text" : "We report F0.5, Precision, and Recall metrics computed by ERRANT scorer (Bryant et al., 2017) on dev, and test datasets from W&I + LOCNESS Corpus from BEA-2019 GEC Shared Task (Bryant et al., 2019)."
    }, {
      "heading" : "4 Our System’s Design",
      "text" : ""
    }, {
      "heading" : "4.1 Tokenization",
      "text" : "In the original GECToR code, the custom implementation12 of the Byte-Pair Encoding (BPE) tokenizer (Sennrich et al., 2016) is used. It was chosen because out-from-the-box AllenNLP’s tokenizer was too slow, and HuggingFace’s Transformers’ tokenizers did not provide BPE to words mapping. Our work is fully implemented with Transformers from the HuggingFace Transformers library. In particular, we moved to the recently released fast tokenizers from it. Now encoders have the same tokenizers for fine-tuning as they had for initial pre-training that leads to better quality after finetuning."
    }, {
      "heading" : "4.2 Initialization and training setup",
      "text" : "Encoder is loaded with its default pretrained weights; the linear layers’ weights are initialized with random numbers. Our models are trained by Adam optimizer (Kingma and Ba, 2015) with default hyperparameters. The loss function is a multi-class categorical entropy. The early stopping technique is used: stopping criteria is 3 epochs without improving the loss function on the dev set, which is random 2% from the same source as training data and is different for each stage."
    }, {
      "heading" : "4.3 Training stages",
      "text" : "Model’s training is performed during several stages. On Stage I, model is pretrained on synthetic datasets; this stage is optional. Then, on Stage II, we carry out warming training on the Joint Train Dataset, which contains Lang-8, NUCLE, FCE, W&I Train datasets (Table 1). Thus we perform coarse fine-tuning on a large amount of diverse GEC data. Datasets are used sequentially; no shuffling is made. Also, in order not to ruin out-of-thebox pretrained weights of the encoder, during the first two epochs, we train only linear layers (socalled \"cold epochs\"); later, we make all model’s weights trainable.\n12https://github.com/google/ sentencepiece\nOn Stage III, we continue fine-tuning on the W&I Train dataset, which contains only the highest quality data. Another difference between Stages II and III is the share of edit-free sentences in the training data. We observed that too many sentences in training data without edits lead to reducing the appearance rate of the tagger and deteriorating the overall quality. Therefore, we filter out edit-free sentences from the Joint Train Dataset, which is used in Stage II. On Stage III, we fine-tune the model on the unfiltered version of the W&I Train dataset.\nThe final stage is \"inference tweaks\" (Omelianchuk et al., 2020) for balancing between the model’s precision and recall. It is performed by introducing additional hyperparameters: additional confidence (AC) to the probability for the KEEP tag and minimum error probability (MEP) for corrections tags. These hyperparameters are found through a random search on the BEA-2019 dev set."
    }, {
      "heading" : "4.4 Upgrading to Large encoders",
      "text" : "In the GECToR paper (Omelianchuk et al., 2020), authors investigated encoders from ALBERT (Lan et al., 2020), BERT (Devlin et al., 2018), GPT2 (Radford et al., 2018), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019) Transformers in their Base configurations. Most likely, Base configurations were chosen due to the better inference speed/quality ratio. They found that XLNet, RoBERTa, and BERT show the best quality.\nWe reproduce experiments for these encoders, but now we explore Large configurations as well. We additionally explore encoder from DeBERTa (He et al., 2020) (Table 3).\nWe observe that all models which are equipped with Large encoders have higher Precision, Recall, and F0.5 values than those equipped with their Base versions. The price for it is 2.3 - 2.5 times slower inference for Large configurations (Table 4).\nThe single model with RoBERTa encoder shows the best performance among Large configurations, whereas DeBERTa slightly outperforms RoBERTa and is the best one among Base configurations. At the same time, RoBERTa remains the fastest one in both configurations."
    }, {
      "heading" : "4.5 Exploring tag vocabulary sizes",
      "text" : "Most of the tag-encoded edits are token-specific, e.g., \"APPEND Amelia\", \"REPLACE Brandon\", and so on. Thus, the tag vocabulary size matters, and it should be a compromise between the covering of the natural language dictionary and the model’s generalization abilities.\nWe create the tag vocabulary by taking the most frequent edit tags which were generated from the Joint Train dataset (Table 1). To find the optimal tag vocabulary sizes, we experiment with {5k, 10k} vocabulary sizes (Table 5).\nWe observe that increasing the vocabulary size to 10k for Large encoders may improve the quality,\nas it happened for models with RoBERTa and DeBERTa. Nevertheless, also we see an example of quality deterioration for the model with XLNet."
    }, {
      "heading" : "5 Ensembling the GEC taggers",
      "text" : "Ensembling is a proven quality boosting method for the models’ sets that have diverse outputs. Most of the recent GEC solutions got their best results by ensembling single models (Stahlberg and Kumar, 2021), (Omelianchuk et al., 2020), (Awasthi et al., 2019). In this section we consider two ensembling methods for our GEC tagging models: averaging of output tag probabilities and majority votes on output edit spans (Fig. 2)."
    }, {
      "heading" : "5.1 Exploring averaging of output tag probabilities (” + ” operation)",
      "text" : "First, we reproduce the ensembling approach from (Omelianchuk et al., 2020). We add DeBERTa and carry out experiments with varying Base and Large configurations of encoders (Table 6).\nWe observe that ensembling by averaging of output tag probabilities improves the quality of corrections; the more models we combine, the better results we obtain. More surprisingly, combining the same encoders’ architectures in Base and Large configurations may provide slightly better results than we get for the Base and Large models separately, see RoBERTa(B) + RoBERTa(L) in Table 6.\nAlthough the ensemble RoBERTa(L) + BERT(L) + DeBERTa(L) + XLNet(L) shows the best performance, we select ensemble the RoBERTa(L) + DeBERTa(L) + XLNet(L) for further experiments. It has higher Recall that makes it possible to trade Recall for Precision later during inference tweaks."
    }, {
      "heading" : "5.2 Exploring majority votes on output edit spans (⊕ operation)",
      "text" : "This aggregation method combines single models’ outputs on the post-processing step (Fig. 2). We take span-level edits and leave only those which have most of the votes from the ensemble. A similar approach is used in (Liang et al., 2020), where the authors combined sequence tagging and sequence-to-sequence models for the Chinese language. The advantage of this ensembling method is that we can combine the results of models with different output dimensions and even different architectures. In our work, it allows us to combine models with different tag vocabulary sizes. We leave ensembling with Seq2Seq GEC systems as a part of our future work.\nFirst, we compare ensembling by averaging of output tag probabilities ”+” and by majority votes on output edit spans ⊕ for the selected ensemble after training on Joint Train Dataset (\"Stage II\"), finetuning on W&I dataset (\"Stage III\") and optimization of hyperparameters (\"inference tweaks\") (Table 7). We observe that ensembles based on majority votes on output edit spans show better results because of better Precision. However, F0.5 scores of both ensembling types are close to each other after inference tweaks.\nTo additionally improve the precision of ensembling by majority votes we introduce hyperparameter Nmin, \"majority quorum\". Majority quorum Nmin denotes minumum number of votes for triggering the edit, here 1 ≤ Nmin ≤ Nsingle_models. Increasing Nmin boosts the Precision by the cost of Recall because it filters out more edits where single\nmodels disagree (Table 8). Setting Nmin = 1 is a poor strategy because we can’t rely on the majority when resolving conflicting edits, so the resulting text might contain controversial and incoherent edits.\nIncreasing number of systems leads to higher quality, but requires adapting the Nmin parameter (Table 8). Based on this limited analysis we observe that Nmin = Nsingle_models − 1 works the best. For our pool of models there is no gain over using more than 4 models, but we want to explore adding more diverse models based on Seq2Seq approach to such an ensemble in future works.\nNext, since the majority votes on output edit spans is capable of combining any models, we test the ensemble of the best models that we already have trained (Table 9).\nFinally, we evaluate our best ensemble DeBERTa(L)10k ⊕ RoBERTa (L) 10k ⊕ XLNet (L) 5k on the BEA-2019 (test) dataset and achieve 76.05 of F0.5 score. This is a significant improvement over F0.5 = 73.70 for the best ensemble from (Omelianchuk et al., 2020) and to the best of our knowledge is a new state-of-the-art (SOTA) result for ensembles on BEA-2019 (test) benchmark. It is worth noting that the solution is obtained without pre-training on synthetic data."
    }, {
      "heading" : "6 Knowledge distillation",
      "text" : "Knowledge distillation is the method for transferring knowledge from a large model (\"teacher\") to a smaller one (\"student\") (Hinton et al., 2015), (Kim and Rush, 2016). It has strong practical applications because large models usually have expensive inference costs and are inconvenient for deployment.\nIn our case, the teacher model is an ensemble\nof trained sequence taggers, whereas the student model is a single sequence tagger. The ensemble receives errorful texts and generates their corrected versions. Later these input-output pairs of sentences are used for training single models. Of course, like any synthetic annotation method, knowledge distilled data contains a certain share of systematic errors that deteriorates the student model’s quality."
    }, {
      "heading" : "6.1 Distilling the data.",
      "text" : "In this work, we use two monolingual corpora to generate our distilled datasets: One Billion Words Benchmark (\"1BW\"), which mostly contains news, and The Blog Authorship Corpus (\"Blogs\"), which contains blog texts on various topics (Table 1). Being real-world natural texts, these datasets contain a certain share of grammatical errors, which are corrected by our system. For text pre-processing, we use the tokenizer from Spacy13.\nAs a teacher, we use the ensemble of the sequence taggers containing Large encoders with 5k vocabulary: DeBERTa(L)5k + RoBERTa (L) 5k + XLNet(L)5k (Table 7). It corrects 5% of processed sentences in 1BW and 28% of sentences in Blogs datasets. Distilled versions of the datasets have the prefix \"Troy-\" in their names (Table 1). Considering our past experience, we fill our distilled datasets only with edited sentence pairs, and we limit their number to 1.2M. We also limit the synthetic PIE dataset from (Awasthi et al., 2019) to 1.2M sentence pairs for better comparability in the experiments. We leave exploring other ensembles in the role of a teacher model for future research."
    }, {
      "heading" : "6.2 Pre-training on synthetic and distilled datasets (\"multi-stage training\")",
      "text" : "First, we reproduce the training scheme from (Omelianchuk et al., 2020) for a single model, RoBERTa(L)5k where PIE synthetic data is used for pre-training (Stage I), then the model is trained on Joint Train Dataset (Stage II), after that it is trained on the high-quality W&I dataset (Stage III), and finally, a hyperparameter search of additional confidence probability and the minimum error probability is performed (Inf. tweaks). We observe that sequence tagger with RoBERTa-Large encoder shows slightly better performance than RoBERTaBase from (Omelianchuk et al., 2020) where the\n13https://spacy.io/\nlast one had an 8x larger training dataset on Stage I (Fig. 3).\nNext, we replace the synthetic PIE dataset with our distilled datasets, Troy-1BW and Troy-Blogs. We observe that on Stage I, there is a difference with training on purely synthetic data that leads to the dramatic rise of Recall. However, when we start training on Stage II, a sharp deterioration in both Precision and Recall appears. It seems that the student model does not receive new information compared to Stage I. This is more noticeable for models trained on the Troy-Blogs dataset, which significantly drops Recall after training. At the same time, on Stage II, the F0.5 is better for models pretrained on distilled Troy- datasets.\nFinally, after training on Stage III and performing inference tweaks, single models pretrained on both datasets show very similar performance, but the model with RoBERTa(L)5k trained on Troy-1BW was slightly better. This single model reaches F0.5 = 73.21 on BEA-2019 (test), that significantly improves the results from (Omelianchuk et al., 2020) for single models where they have F0.5 = 71.5 for RoBERTa (B) 5k , and F0.5 = 72.4 for the XLNet(B)5k encoders."
    }, {
      "heading" : "6.3 One-stage training on distilled + annotated dataset",
      "text" : "We observed that models which were pretrained on the Troy-Blogs dataset show good results on Stage I, but loose their advantage after training on Stage II. Thus, we trained one more model with RoBERTa(L)5k encoder.\nWe performed one-stage training where the TroyBlogs dataset was concatenated with the most accurate W&I dataset that we usually use for Stage III. As a result, we got F0.5 = 55.81 on BEA-2019 (dev) and F0.5 = 72.69 on BEA-2019 (test) (Table 11). These results are obtained much easier than our best single model: just one-stage training for out-of-the-box RoBERTa, no pre-training on synthetic GEC data or multi-stage training."
    }, {
      "heading" : "7 Conclusions",
      "text" : "Our best ensemble achieves a new SOTA result the F0.5 = 76.05 on BEA-2019 (test). Ensembling sequence taggers by majority votes on output edit spans provides better performance than averaging output tag probabilities while staying tolerant to models’ architecture and vocabulary sizes. Single models in the ensemble were not pre-trained on synthetic GEC datasets that gives a room for improvement in future work.\nWe apply the knowledge distillation method to ensemble of sequence taggers for producing annotated Troy-Blogs and Troy-1BW datasets. After training on these datasets single GEC sequence tagging models show competitive results, F0.5 = 73.21/72.69 on BEA-2019 (test) for multistage/one-stage training. Replacing Base encoders in GECToR (Omelianchuk et al., 2020) with their Large configurations does improves the quality having up to x3 bigger size. However, in accuracy our best single model still gives way only to a much heavier T5 xxl model with 11B params (Rothe et al., 2021) having x30 less own size.\nWe make the code, datasets, and trained models publicly available14.\n14http://github.com/to-be-published"
    }, {
      "heading" : "A Appendix",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Parallel iterative edit models for local sequence transduction",
      "author" : [ "Abhijeet Awasthi", "Sunita Sarawagi", "Rasna Goyal", "Sabyasachi Ghosh", "Vihari Piratla." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th",
      "citeRegEx" : "Awasthi et al\\.,? 2019",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel iterative edit models for local sequence transduction",
      "author" : [ "Abhijeet Awasthi", "Sunita Sarawagi", "Rasna Goyal", "Sabyasachi Ghosh", "Vihari Piratla." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Awasthi et al\\.,? 2019",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2019
    }, {
      "title" : "The BEA-2019 shared task on grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Øistein E. Andersen", "Ted Briscoe." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–75,",
      "citeRegEx" : "Bryant et al\\.,? 2019",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic annotation and evaluation of error types for grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Ted Briscoe." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Bryant et al\\.,? 2017",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2017
    }, {
      "title" : "The bea-2019 shared task on grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Øistein E. Andersen", "Ted Briscoe." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages",
      "citeRegEx" : "Bryant et al\\.,? 2019",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2019
    }, {
      "title" : "One billion word benchmark for measuring progress in statistical language modeling",
      "author" : [ "Ciprian Chelba", "Tomás Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn." ],
      "venue" : "CoRR, abs/1312.3005.",
      "citeRegEx" : "Chelba et al\\.,? 2013",
      "shortCiteRegEx" : "Chelba et al\\.",
      "year" : 2013
    }, {
      "title" : "Building a large annotated corpus of learner english: The nus corpus of learner english",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu." ],
      "venue" : "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages",
      "citeRegEx" : "Dahlmeier et al\\.,? 2013",
      "shortCiteRegEx" : "Dahlmeier et al\\.",
      "year" : 2013
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina N. Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural grammatical error correction systems with unsupervised pre-training on synthetic data",
      "author" : [ "Roman Grundkiewicz", "Marcin Junczys-Dowmunt", "Kenneth Heafield." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building",
      "citeRegEx" : "Grundkiewicz et al\\.,? 2019",
      "shortCiteRegEx" : "Grundkiewicz et al\\.",
      "year" : 2019
    }, {
      "title" : "Deberta: Decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:2006.03654.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction",
      "author" : [ "Masahiro Kaneko", "Masato Mita", "Shun Kiyono", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Kaneko et al\\.,? 2020",
      "shortCiteRegEx" : "Kaneko et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Adam (2014), a method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations (ICLR), arXiv preprint arXiv, volume 1412.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "An empirical study of incorporating pseudo data into grammatical error correction",
      "author" : [ "Shun Kiyono", "Jun Suzuki", "Masato Mita", "Tomoya Mizumoto", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Kiyono et al\\.,? 2019",
      "shortCiteRegEx" : "Kiyono et al\\.",
      "year" : 2019
    }, {
      "title" : "An empirical study of incorporating pseudo data into grammatical error correction",
      "author" : [ "Shun Kiyono", "Jun Suzuki", "Masato Mita", "Tomoya Mizumoto", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Kiyono et al\\.,? 2019",
      "shortCiteRegEx" : "Kiyono et al\\.",
      "year" : 2019
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ICLR 2020 : Eighth International Conference on Learning Representa-",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert enhanced neural machine translation and sequence tagging model for chinese grammatical error diagnosis",
      "author" : [ "Deng Liang", "Chen Zheng", "Lei Guo", "Xin Cui", "Xiuzhang Xiong", "Hengqiao Rong", "Jinpeng Dong." ],
      "venue" : "Proceedings of the 6th Workshop on Natu-",
      "citeRegEx" : "Liang et al\\.,? 2020",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Corpora generation for grammatical error correction",
      "author" : [ "Jared Lichtarge", "Christopher Alberti", "Shankar Kumar", "Noam Shazeer", "Niki Parmar", "Simon Tong." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lichtarge et al\\.,? 2019",
      "shortCiteRegEx" : "Lichtarge et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Encode, tag, realize: High-precision text editing",
      "author" : [ "Eric Malmi", "Sebastian Krause", "Sascha Rothe", "Daniil Mirylenka", "Aliaksei Severyn." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Malmi et al\\.,? 2019",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2019
    }, {
      "title" : "Gector – grammatical error correction: Tag, not rewrite",
      "author" : [ "Kostiantyn Omelianchuk", "Vitaliy Atrasevych", "Artem N. Chernodub", "Oleksandr Skurzhanskyi." ],
      "venue" : "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applica-",
      "citeRegEx" : "Omelianchuk et al\\.,? 2020",
      "shortCiteRegEx" : "Omelianchuk et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "A Simple Recipe for Multilingual Grammatical Error Correction",
      "author" : [ "Sascha Rothe", "Jonathan Mallinson", "Eric Malmi", "Sebastian Krause", "Aliaksei Severyn." ],
      "venue" : "Proc. of ACL-IJCNLP.",
      "citeRegEx" : "Rothe et al\\.,? 2021",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2021
    }, {
      "title" : "Effects of age and gender on blogging",
      "author" : [ "Jonathan Schler", "Moshe Koppel", "Shlomo Argamon", "James W. Pennebaker." ],
      "venue" : "AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, pages 199–205.",
      "citeRegEx" : "Schler et al\\.,? 2005",
      "shortCiteRegEx" : "Schler et al\\.",
      "year" : 2005
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725,",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Synthetic data generation for grammatical error correction with tagged corruption models",
      "author" : [ "Felix Stahlberg", "Shankar Kumar." ],
      "venue" : "Proceedings of the 9",
      "citeRegEx" : "Stahlberg and Kumar.,? 2021",
      "shortCiteRegEx" : "Stahlberg and Kumar.",
      "year" : 2021
    }, {
      "title" : "Tense and aspect error correction for esl learners using global context",
      "author" : [ "Toshikazu Tajiri", "Mamoru Komachi", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
      "citeRegEx" : "Tajiri et al\\.,? 2012",
      "shortCiteRegEx" : "Tajiri et al\\.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Huggingface’s transformers: State-ofthe-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chap-",
      "citeRegEx" : "Xue et al\\.,? 2021",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 5753–",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "A new dataset and method for automatically grading esol texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    }, {
      "title" : "Grammatical error correction using neural machine translation",
      "author" : [ "Zheng Yuan", "Ted Briscoe." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Yuan and Briscoe.,? 2016",
      "shortCiteRegEx" : "Yuan and Briscoe.",
      "year" : 2016
    }, {
      "title" : "Constrained grammatical error correction using statistical machine translation",
      "author" : [ "Zheng Yuan", "Mariano Felice." ],
      "venue" : "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 52–61, Sofia, Bulgaria.",
      "citeRegEx" : "Yuan and Felice.,? 2013",
      "shortCiteRegEx" : "Yuan and Felice.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Early GEC-MT methods leveraged phrase-based statistical machine translation (PBSMT) (Yuan and Felice, 2013).",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "To the best of our knowledge, our best single model gives way only to much heavier T5 model (Rothe et al., 2021).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 33,
      "context" : "to-appear-after-publication rapidly evolved to sequence-to-sequence Neural Machine Translation (NMT) based on gated recurrent neural networks (Yuan and Briscoe, 2016) and recent powerful Transformer-based Seq2Seq models.",
      "startOffset" : 142,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : "(Grundkiewicz et al., 2019) leveraged Transformer model (Vaswani et al.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 28,
      "context" : ", 2019) leveraged Transformer model (Vaswani et al., 2017) which was pre-trained on synthetic GEC data and right-to-left re-ranking for ensemble.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : ", 2020) adopted several strategies of BERT (Devlin et al., 2018) usage for GEC.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 23,
      "context" : "Recently, (Rothe et al., 2021) built their system on top of T5 (Xue et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 30,
      "context" : ", 2021) built their system on top of T5 (Xue et al., 2021), a xxl version of T5 Transformer encoder-decoder model and reached new",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "LaserTagger (Malmi et al., 2019) is a sequence tagging model that casts text generation as a text edit-",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Parallel Iterative Edit (PIE) model (Awasthi et al., 2019) does parallel decoding, achieving quality that is competitive with the Seq2Seq models3.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "A similar approach is presented in (Omelianchuk et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "At the same time, human annotation is expensive, so researchers are working on methods for augmentation of training data, synthetic data generation, and strategies for its efficient usage (Lichtarge et al., 2019), (Kiyono et al.",
      "startOffset" : 188,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : ", 2019), (Kiyono et al., 2019), (Stahlberg and Kumar, 2021).",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 29,
      "context" : ", 2017) and HuggingFace’s Transformers (Wolf et al., 2019) libraries, and its source code is freely available4.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "We refer to (Omelianchuk et al., 2020) for details of edit transforms.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "We empirically investigate and improve the GECToR sequence tagging system (Omelianchuk et al., 2020) by upgrading Transformer encoders",
      "startOffset" : 74,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "pus of Learner English (Lang-8)5 (Tajiri et al., 2012), National University of Singapore Corpus of Learner English (NUCLE)6 (Dahlmeier et al.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : ", 2012), National University of Singapore Corpus of Learner English (NUCLE)6 (Dahlmeier et al., 2013), First Certificate in English dataset (FCE)7 (Yannakoudakis et al.",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 32,
      "context" : ", 2013), First Certificate in English dataset (FCE)7 (Yannakoudakis et al., 2011), and Write & Improve (W&I) Corpus (Bryant et al.",
      "startOffset" : 53,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : ", 2011), and Write & Improve (W&I) Corpus (Bryant et al., 2019)8.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "For knowledge distillation from the ensemble, we use parts of two monolingual datasets: One Billion Word Benchmark (1BW)9 (Chelba et al., 2013) and The Blog Authorship Corpus (Blogs)10 (Schler et al.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 24,
      "context" : ", 2013) and The Blog Authorship Corpus (Blogs)10 (Schler et al., 2005).",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "After knowledge distillation for the final training of the student model we also use parallel sentences with synthetically generated grammatical errors from the PIE dataset11 (Awasthi et al., 2019).",
      "startOffset" : 175,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "5, Precision, and Recall metrics computed by ERRANT scorer (Bryant et al., 2017) on dev, and test datasets from W&I + LOCNESS Corpus from BEA-2019 GEC Shared Task (Bryant et al.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : ", 2017) on dev, and test datasets from W&I + LOCNESS Corpus from BEA-2019 GEC Shared Task (Bryant et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 25,
      "context" : "In the original GECToR code, the custom implementation12 of the Byte-Pair Encoding (BPE) tokenizer (Sennrich et al., 2016) is used.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "Our models are trained by Adam optimizer (Kingma and Ba, 2015) with default hyperparameters.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "Pre-training on synthetic data (Stage I) as was done in (Omelianchuk et al., 2020) is not performed.",
      "startOffset" : 56,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "The final stage is \"inference tweaks\" (Omelianchuk et al., 2020) for balancing be-",
      "startOffset" : 38,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : "In the GECToR paper (Omelianchuk et al., 2020), authors investigated encoders from ALBERT (Lan et al.",
      "startOffset" : 20,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : ", 2020), authors investigated encoders from ALBERT (Lan et al., 2020), BERT (Devlin et al.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : ", 2020), BERT (Devlin et al., 2018), GPT2 (Radford et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : ", 2018), GPT2 (Radford et al., 2018), RoBERTa (Liu et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : ", 2018), RoBERTa (Liu et al., 2019), and XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 31,
      "context" : ", 2019), and XLNet (Yang et al., 2019) Transformers in their Base configurations.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "We additionally explore encoder from DeBERTa (He et al., 2020) (Table 3).",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "Most of the recent GEC solutions got their best results by ensembling single models (Stahlberg and Kumar, 2021), (Omelianchuk et al.",
      "startOffset" : 84,
      "endOffset" : 111
    }, {
      "referenceID" : 21,
      "context" : "Most of the recent GEC solutions got their best results by ensembling single models (Stahlberg and Kumar, 2021), (Omelianchuk et al., 2020), (Awasthi et al.",
      "startOffset" : 113,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "First, we reproduce the ensembling approach from (Omelianchuk et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "similar approach is used in (Liang et al., 2020), where the authors combined sequence tagging and sequence-to-sequence models for the Chinese language.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "70 for the best ensemble from (Omelianchuk et al., 2020) and to the best of our knowledge is a new state-of-the-art (SOTA) result for ensembles on BEA-2019 (test) benchmark.",
      "startOffset" : 30,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "Knowledge distillation is the method for transferring knowledge from a large model (\"teacher\") to a smaller one (\"student\") (Hinton et al., 2015), (Kim and Rush, 2016).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "We also limit the synthetic PIE dataset from (Awasthi et al., 2019) to 1.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "First, we reproduce the training scheme from (Omelianchuk et al., 2020) for a single model, RoBERTa 5k where PIE synthetic data is used for pre-training (Stage I), then the model is trained on Joint Train Dataset (Stage II), after that it is trained on the high-quality W&I dataset (Stage III), and finally, a hyperparameter search of additional confidence probability and the minimum error probability is performed (Inf.",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "We observe that sequence tagger with RoBERTa-Large encoder shows slightly better performance than RoBERTaBase from (Omelianchuk et al., 2020) where the",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "21 on BEA-2019 (test), that significantly improves the results from (Omelianchuk et al., 2020) for single models where they have F0.",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "Replacing Base encoders in GECToR (Omelianchuk et al., 2020) with their Large configurations does improves the quality having up to x3 bigger size.",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : "However, in accuracy our best single model still gives way only to a much heavier T5 xxl model with 11B params (Rothe et al., 2021) having x30 less own size.",
      "startOffset" : 111,
      "endOffset" : 131
    } ],
    "year" : 0,
    "abstractText" : "In this paper, we investigate GEC sequence tagging architecture with focusing on ensembling of the recent cutting-edge Transformers’ encoders in their Large configurations. We encourage ensembling models by majority votes on span-level edits because it’s tolerant to the model architecture and vocabulary size. Our best ensemble achieves a new SOTA result, the F0.5 score of 76.05 on BEA-2019 (test), even without pre-training on synthetic datasets. Also, we perform model distillation of a trained ensemble to generate new training synthetic datasets, \"Troy-Blogs\" and \"Troy-1BW\". Our best single sequence tagging model that is pretrained on generated Troydatasets in combination with publicly available synthetic PIE dataset achieves a near-SOTA1 result of the F0.5 score of 73.21 on BEA-2019 (test). The code, datasets, and trained models are publicly available2.",
    "creator" : null
  }
}
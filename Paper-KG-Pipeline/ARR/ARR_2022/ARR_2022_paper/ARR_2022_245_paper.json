{
  "name" : "ARR_2022_245_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "End-to-end Spoken Conversational Question Answering: Task, Dataset and Model",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Conversational question answering (CQA) has been studied extensively over the past few years\n1Codes and datasets will be made publicly available.\nwithin the natural language processing (NLP) communities (Zhu et al., 2018; Liu et al., 2019; Yang et al., 2019). Different from traditional question answering (QA) tasks, CQA aims to enable models to learn the representation of the context paragraph and multi-turn dialogues. Existing CQA methods (Huang et al., 2018a; Devlin et al., 2018; Xu et al., 2019; Gong et al., 2020) have achieved superior performances on several benchmark datasets, such as QuAC (Choi et al., 2018) and CoQA (Elgohary et al., 2018).\nCurrent CQA research mainly focuses on leveraging written text sources in which the answer can be extracted from a large document collection. However, humans communicate with each other via spontaneous speech (e.g., meetings, lectures, online conversations), which convey rich information. Consider our multimodal experience, fine-grained representations of both audio recordings and text documents are considered to be of paramount importance. Thus, we learn to draw useful relations between modalities (speech and language), which enables us to form fine-grained multimodal representations for end-to-end speech-and-language learning problems in many real-world applications, such as voice assistant and chat robot.\nIn this paper, we propose a novel and challenging spoken conversational question answering task - SCQA. An overview pipeline of this task is shown in Figure 1. Collecting such a SCQA dataset is a non-trivial task, as in contrast to current CQA tasks, we build our SCQA with two main goals as follows: (1) SCQA is a multi-turn conversational spoken question answering task, which is more challenging than only text-based task; (2) existing CQA methods rely on a single modality (text) as the context source. However, plainly leveraging uni-modality information is naturally undesirable for end-to-end speech-and-language learning problems since the useful connections between speech and text are elusive. Thus, employing data from\nthe context of another modality (speech) can allow us to form fine-grained multimodal representations for the downstream speech-and-language tasks; and (3) considering the speech features are based on regions and are not corresponding to the actual words, this indicates that the semantic inconsistencies between the two domains can be considered as the semantic gap, which requires to be resolved by the downstream systems themselves.\nIn order to provide a strong baseline for this challenging multi-modal spoken conversational question answering task, we first present a novel knowledge distillation (KD) method for the proposed SCQA task. Our intuition is that speech utterances and text contents share the dual nature property, and we can take advantage of this property to learn the correspondences between these two forms. Specifically, we enroll multi-modal knowledge into the teacher model, and then guide the student (only trained on noisy speech documents) to boost network performance. Moreover, considering that the\nsemantics of the speech features and the textual representations are usually inconsistent, we introduce a novel mechanism, termed Dual Attention, to encourage fine-grained alignments between audio and text to close the cross-modal semantic gap between speech and language. One example of cross-modal gap is shown in Table 1. The experimental results show that our proposed DDNET achieves remarkable performance gains in the SCQA task. To the best of our knowledge, we are the first work in spoken conversational question answering task.\nOur main contributions are as follows:\n• We propose Spoken Conversational Question Answering task (SCQA), and comprise Spoken-CoQA dataset for machine comprehension of spoken question-answering style conversations. To the best of our knowledge, our Spoken-CoQA is the first spoken conversational question answering dataset.\n• We develop a novel end-to-end method based\non data distillation to learn both from speech and language domain. Specifically, we use the model trained on clear texts as well as recordings to guide the model trained on noisy speech transcriptions. Moreover, we propose a novel Dual Attention mechanism to align the speech features and textual representations in each domain.\n• We demonstrate that, by applying our proposed DDNET on several previous baselines, we can obtain considerable performance gains on our proposed Spoken-CoQA dataset."
    }, {
      "heading" : "2 Related Work",
      "text" : "Text Question Answering. In recent years, the natural language processing research community has devoted substantial efforts to text question answering tasks (Huang et al., 2018a; Zhu et al., 2018; Xu et al., 2019; Zhang et al., 2020; Gong et al., 2020). Within the growing body of work on machine reading comprehension, an important sub-task of text question answering, two signature attributes have emerged: the availability of large benchmark datasets (Choi et al., 2018; Elgohary et al., 2018; Reddy et al., 2019) and pre-trained language models (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2020). However, these existing works typically focus on modeling the complicated context dependency in text form. In contrast, we focus on enabling the machine to build the capability of language recognition and dialogue modeling in both speech and text domains. Spoken Question Answering. In parallel to the recent works in natural language processing (Huang et al., 2018a; Zhu et al., 2018), these trends have\nalso been pronounced in the speech field (Chen et al., 2018; Haghani et al., 2018; Lugosch et al., 2019; Palogiannidi et al., 2020; You et al., 2021), where spoken question answering (SQA), an extended form of QA, has explored the prospect of machine comprehension in spoken form. Previous work on SQA typically includes two separate modules: automatic speech recognition (ASR) and text question answering. It involves transferring spoken content to ASR transcriptions, and then employs NLP techniques to handle speech tasks.\nExisting methods (Tseng et al., 2016; Serdyuk et al., 2018; Su and Fung, 2020) focus on optimizing each module in a two-stage manner, where errors in the ASR module would result in severe performance loss. Lee et al. (Lee et al., 2019) proved that utilizing clean texts can help model trained on the ASR transcriptions to boost the performance via domain adaptation. SpeechBERT (Chuang et al., 2019) cascaded the BERT-based models as a unified model, and then trained it in a joint manner of audio and text. However, the existing SQA methods aimed at solving a single question given the related passage, without building and maintaining the connections of different questions in the human conversations. In addition, we compare our SpokenCoQA with existing SQA datasets (See Table 2). Unlike existing SQA datasets, Spoken-CoQA is a multi-turn conversational SQA dataset, which is more challenging than single-turn benchmarks.\nKnowledge Distillation. Hinton et al. (Hinton et al., 2015) introduced the idea of Knowledge Distillation (KD) in a teacher-student scenario. In other words, we can distill the knowledge from one model (massive or teacher model) to another\n(small or student model). Previous work has shown that KD can significantly boost prediction accuracy in natural language processing and speech processing (Kim and Rush, 2016; Hu et al., 2018; Huang et al., 2018b; Hahn and Choi, 2019), while adopting KD-based methods for SQA tasks has been less explored. In this work, our goal is to handle the SCQA tasks. More importantly, we focus the core nature property in speech and text: Can spoken conversational dialogues further assist the model to boost the performance? Finally, we incorporate the knowledge distillation framework to distill reliable dialogue flow from the spoken contexts, and utilize the learned predictions to guide the student model to train well on the noisy input data."
    }, {
      "heading" : "3 Task Definition",
      "text" : "In this section, we propose the novel SCQA task and collect a Spoken-CoQA (S-CoQA) dataset, which uses the spoken form of multi-turn dialogues and spoken documents to answer questions in multiturn conversations.\nGiven a spoken document Ds, we use Dt to denote the clean original text and Da to denote the ASR transcribed document. We also have Qa1:L={q a 1 , q a 2 , ..., q a L}, which is a collection of L-turn ASR transcribed spoken questions Qs1:L, as well as A t 1:L= {a t 1, a t 2, ..., a t L} which are the corresponding answers to the questions in clean texts. The objective of SCQA task is then to generate the answer atL for question q a L, given document Da, multi-turn history questions Qa1:L−1={q a 1 , q a 2 , ..., q a L−1}, and reference answers At1:L−1= {a t 1, a t 2, ..., a t L−1}. In other words, our task in the testing phase can be formulated as\n{Ds, Qs1:L} ASR−−→ {qaL, Da, Qa1:L−1, at1:L−1} → atL\n(1) Please note that in order to improve the performance, in the training phase, we make use of auxiliary information which are the clean texts of document Dt and dialogue questions Qt={qt1, q t 2, . . . , q t L}, to guide the training of stu-\ndent model. As a result, the training process could be formulated as below:\nstudent: {Ds, Qs1:L} ASR−−→ {qaL, Da, Qa1:L−1, at1:L−1}\nteacher: {Dt, Qt1:L}\n} → atL\nHowever, in the inference stage, these additional information of Dt and Qt1:L are not needed."
    }, {
      "heading" : "4 DDNet",
      "text" : "In this section, we propose DDNET to deal with the SCQA task, which is illustrated in Figure 2. We first describe the embedding generation process for both audio and text data. Next, we propose Dual Attention to fuse the speech and textual modalities. After that, we present the major components of the DDNET module. Finally we describe a simple yet effective distillation strategy in the proposed DDNET to learn enriched representations in the speech-text domain comprehensively."
    }, {
      "heading" : "4.1 Embedding",
      "text" : "Given spoken words S = {s1, s2, ..., sm} and corresponding clean text words T = {t1, t2, ..., tn}, we utilize Speech-BERT and Text-BERT to generate speech feature embedding Es={Es1,Es2, ...,Esm} and context word embedding Et={Et1,Et2, ...,Etn}2, respectively. Concretely, for speech input, we first use vqwav2vec (Baevski et al., 2019) to transfer speech signals into a series of tokens, which is the standard tokenization procedure in speech related tasks. Next, use Speech-BERT (Chuang et al., 2019), a variant of BERT-based models retrained on our Spoken-CoQA dataset, to process the speech sequences for training. The text contents are embbed into a sequence of vectors via our text encoder - Text-BERT, with the same architecture of BERT-base (Devlin et al., 2018)."
    }, {
      "heading" : "4.2 Dual Attention",
      "text" : "Dual Attention (DA) is proposed to optimize the alignment between speech and language domains by capturing useful information from the two domains. In particular, we first use cross attention to align speech and text representations in the initial stage. After that, we utilize contextualized attention to further align the cross-modal representations in the contextualized word-level. Finally, we employ the self-attention mechanism to form fine-grained audio-text representations.\n2In our implement, the padding strategy is used to keep m and n to be the same as the max sequence length.\nCross Attention. Inspired by ViLBERT (Lu et al., 2019), we apply the co-attention transformer layer, a variant of Self-Attention (Vaswani et al., 2017), as the Cross Attention module for the fusing of speech and text embeddings. The Cross Attention is implemented by the standard Attention module involving Multi-Head Attention (MHA) and FeedForward Network (FFN) (Vaswani et al., 2017) as below:\nAttention(Q,K, V ) = FFN(MHA(Q,K, V ))\nCrossAttention(F1, F2) = Attention(F1, F2, F2) (2)\nwhere Q, K, V denote query, key, and value matrices, and F1, F2 denote features from difference modalities, respectively. The co-attention module then use the Cross Attention function to compute the cross attention-pooled features, by querying one modality using the query vector of another modality.\nÊcrosss = CrossAttention(Es,Et) = Attention(Es,Et,Et),\nÊcrosst = CrossAttention(Et,Es) = Attention(Et,Es,Es),\n(3)\nwhere Êcrosss ∈ Rn×d, Êcrosst ∈ Rn×d and d is the dimension of feature vectors. Contextualized Attention (CA). After obtaining speech-aware representation Êcrosss and text-aware representation Êcrosst , our next goal is to construct more robust contextualized cross-modal representations by integrating features from both modalities. The features with fused modalities are computed as follows:\nHCA=ReLU(Êcrosss W T 1 )ReLU(Ê cross x W T 1 )W T 2 , (4) where W1,W2 ∈ Rn×d are trainable weights. Self-Attention. To build a robust SCQA system, special attention needs to be paid on the sequential order of the dialogue, since the changes in utterances order may cause severely low-quality and in-coherent corpora. As a result, to capture the longrange dependencies such as co-references for the downstream speech-and-language tasks, similar to (Li et al., 2016; Zhu et al., 2018), we introduce a self-attention layer to obtain the final Dual Attention (DA) representations.\nEDA = SelfAttention(HCA) = Attention(HCA,HCA,HCA).\n(5)"
    }, {
      "heading" : "4.3 Key Components",
      "text" : "The framework of our SCQA module is similar to recent works (Zhu et al., 2018; Huang et al., 2017), which is divided into three key components: Encoding Layer, Attention Layer and Output Layer. Encoding Layer. Then documents and conversations (questions and answers) are first converted into the corresponding feature embeddings (i.e., character embeddings, word embeddings, and contextual embedding). The output contextual embeddings are then concatenated by the aligned crossmodal embedding EDA to form the encoded input features:\nEenc = [Et;EDA]. (6)\nAttention Layer. We compute the attention on the context representations of the documents and questions, and extensively exploit correlations between them. Note that we adopt the default attention layers in four baseline models. Output Layer. After obtaining attention-pooled representations, the Output Layer computes the probability distributions of the start and end index within the entire documents and predicts an answer to current question:\nL = −logP(st = aL,st|X)−logP(ed = aL,ed|X) (7) where X denotes the input document D and QL, and “st”, “ed” denote the start and end positions."
    }, {
      "heading" : "4.4 Knowledge Distillation",
      "text" : "In previous speech-language models, the only guidance is the standard training objective to measure the difference between the prediction and the reference answer. However, for noisy ASR transcriptions, such criteria may not be suitable enough. To overcome such problem, we distill the knowledge from our teacher model, and use them to guide the student model to learn contextual features in our SCQA task. Concretely, we set the model trained on the speech document and the clean text corpus as the teacher model and trained on the ASR transcripts as the student model, respectively. Thus, the student trained on low-quality data learns to absorb the knowledge that the teacher has discovered. Given the zS and zT as the prediction vectors by the student and teacher models, the objective is defined as:\nLSCQA= ∑ x∈X (ατ2KL(pτ (zS), pτ (zT ))+(1−α)L),\n(8)\nwhere KL(·) denotes the Kullback-Leibler divergence. pτ (·) is the softmax function with temperature τ , and α is a balancing factor."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : "In this section, we first describe the collection and filtering process of our proposed Spoken-CoQA dataset in detail. Next, we introduce several stateof-the-art language models as our baselines, and then evaluate the robustness of these models on our proposed Spoken-CoQA dataset. Finally, we provide a thorough analysis of different components of our method. Note that we use the default settings in all evaluated methods. Data Collection. We detail the procedures to build Spoken-CoQA as follows. First, we select the conversational question-answering dataset CoQA (Reddy et al., 2019)3 as our basis data since it is one of the largest public CQA datasets. CoQA contains around 8k stories (documents) and over 120k questions with answers. The average dialogue length of CoQA is about 15 turns, and the answers areis in free-form texts. In CoQA, the training set and the development set contain 7,199 and 500 conversations over the given stories, respectively. Therefore, we use the CoQA training set as our reference text of the training set and the CoQA development set as the test set in Spoken-CoQA.\nNext, we employ the Google text-to-speech system to transform the questions and documents in CoQA into the spoken form, and adopt CMU Sphinx to transcribe the processed spoken contents into ASR transcriptions. In doing so, we collect more than 40G audio data, and the data duration is around 300 hours. The ASR transcription has a kappa score of 0.738 and Word Error Rates (WER) of 15.9%, which can be considered sufficiently good since it is below the accuracy threshold of 30% WER (Gaur et al., 2016). For the test set, we invite 5 human native English speakers to read the sentences of the documents and questions. The sen-\n3Considering that the test set of CoQA (Reddy et al., 2019) idoes not publicly availablesh the test set, we follow the widely used setting in the spoken question answering task (Li et al., 2018), where we divide Spoken-CoQA dataset into train and test set.\ntences of one single document are assigned to a single speaker to keep consistency, while the questions in one example may have different speakers. All speech files are sampled at 16kHz, following the common approach in the speech community. We provide an example of our Spoken-CoQA dataset in Table 1 and Fig. 5.\nData Filtering In our SCQA task, the model predicts the start and end positions of answers in the ASR transcriptions. As a result, during data construction, it is necessary for us to perform data filtering by eliminating question-answer pairs if the answer spans to questions do not exist in the noisy ASR transcriptions. We follow the conventional settings in (Lee et al., 2018)4. In our approach, an ASR question will be removed if the groundtruth answers do not exist in ASR passages. However, when coreference resolution and inference occurs, the contextual questions related to the previous ones are required to be discarded too. For the case of coreference resolution, we change the corresponding coreference. For the case of coreference inference, if the question has strong dependence on the previous one that has already been discarded, it will also be removed. After data filtering, we get a total number of our Spoken-CoQA dataset, we collect 4k conversations in the training set, and 380 conversations in the test set in our SpokenCoQA dataset, respectively. Our dataset includes 5 domains, and we show the domain distributions in Table 3.\nBaselines. For SCQA tasks, our DDNET is able to utilize a variety of backbone networks for SCQA tasks. We choose several state-of-the-art language models (FlowQA (Huang et al., 2018a), SDNet (Zhu et al., 2018), BERT-base (Devlin et al., 2018), ALBERT (Lan et al., 2020)) as our backbone network baselines. We also compare our proposed DDNET with several state-of-the-art SQA methods (Lee et al., 2018; Serdyuk et al., 2018; Lee et al., 2019; Kuo et al., 2020). To use the teacherstudent architecture in our models, we first train baselines on the CoQA training set as teacher and then evaluate the performances of testing baselines on CoQA dev set and Spoken-CoQA dev set. Finally, we train the baselines on the Spoken-CoQA training set as student and evaluate the baselines on the CoQA dev set and Spoken-CoQA test set.\n4We compare different Speech APIs, e.g., Google and CMU. Considering the quality of generated speech transcripts, we choose Google TTS for TTS and CMU Sphinx for ASR.\nWe provide quantitative results in Table 4.\nExperiment Settings. We use the official BERT (Devlin et al., 2018) and ALBERT (Lan et al., 2020) as our textual embedding modules. We use BERTbase (Devlin et al., 2018) and ALBERT-base (Lan et al., 2020), which both include 12 transformer encoders, and the hidden size of each word vector is 768. BERT and ALBERT both utilize BPE as the tokenizer, but FlowQA and SDNet use SpaCy (Honnibal and Montani, 2017) for tokenization. Under the circumstance when tokens in spaCy (Honnibal and Montani, 2017) correspond to more than one BPE sub-tokens, we average the BERT embeddings of these BPE sub-tokens as the final embeddings for each token. For fair comparisons, we use standard implementations and hyper-parameters of four baselines for training. The balancing factor α is set to 0.9, and the temperature τ is set to 2. We train all models on 4 24GB RTX GPUs, with a batch size of 8 on each GPU. For evaluation, we use three metrics: Exact Match (EM), F1 score and Audio Overlapping Score (AOS) (Li et al., 2018) to compare the model performance comprehensively. Please note that the metric numbers of baseline may be different from that in the CoQA leader board as we use our own implementations, Note that, we only utilize the student network for inference.\nResults. We compare several teacher-student pairs on CoQA and Spoken-CoQA dataset and the quantitative results are shown in Table 4. We can observe that the average F1 scores is 77.6% when training on CoQA (text document) and testing on CoQA dev set. However, when training the models on Spoken-CoQA (ASR transcriptions) and testing on Spoken-CoQA test set, the average F1 scores drops significantly to 49.3%. For FlowQA, the performance even drops by 40.4 pts in terms of F1 score. This corroborates the importance of mitigating ASR errors.\nTable 5 compares our approach DDNET to all the previous results. As shown in the table, our distillation models achieve strong performance, and\nincorporating DA mechanism further improves the results considerably. Our DDNET using BERTbase models as backbone achieves similar or better results compared to all the state-of-the-art methods, and we observe that using a larger encoder ALBERT-base will give further bring large gains on performance.\nAs seen from Table 6, we find that our best model ALBERT-base only trained with KD achieve an absolute EM/F1 improvement of +1.7pts/+1.7pts, +2.5pts/+2.5pts, on CoQA and S-CoQA, respectively. This shows that cross-modal information is useful for the model, hence demonstrating that such information is able to build more robust contextualized cross-modal representations for the network performance improvements. As shown in Table 6, we also observe that our approach ALBERTbase only trained with DA outperforms the original method by an absolute EM/F1 of +1.4pts/+1.2pts, +1.8pts/+2.0pts, on CoQA and S-CoQA, respectively. This indicates that the fine-grained alignment between audio and text learned through DA during training benefits the downstream speechand-language tasks. Overall, our results suggest that such a network notably improves prediction performance for spoken conversational question answering tasks. Such significant improvements demonstrate the effectiveness of DDNET."
    }, {
      "heading" : "6 Ablation Study",
      "text" : "We conduct ablation studies to show the effectiveness of several components in DDNet in this section and appendix.\nMulti-Modality Fusion Mechanism. To study the effect of different modality fusion mechanisms, we introduce a novel fusion mechanism Con Fusion: first, we directly concatenate two output embedding from speech-BERT and text-BERT models, and then pass it to the encoding layer in the following SCQA module. In Table 8, we observe that Dual Attention mechanism outperform four baselines with Con Fusion in terms of EM and\nF1 scores. We further investigate the effect of unimodel input. Table 8 shows that text-only performs better than speech-only. One possible reason for this performance is that only using speech features can bring additional noise. Note that speech-only (text-only) means that we only feed the speech (text) embedding for speech-BERT (text-BERT) to\nthe encoding layer in the SCQA module."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we have presented SCQA, a new spoken conversational question answering task, for enabling human-machine communication. We make our effort to collect a challenging dataset - SpokenCoQA, including multi-turn conversations and passages in both text and speech form. We show that the performance of existing state-of-the-art models significantly degrade on our collected dataset, hence demonstrating the necessity of exploiting cross-modal information in achieving strong results. We provide some initial solutions via knowledge distillation and the proposed dual attention mechanism, and have achieved some good results on Spoken-CoQA. Experimental results show that DDNET achieves substantial performance improvements in accuracy. In future, we will further investigate the different mechanisms of integrating speech and text content, and our method also opens up the possibility for downstream spoken language tasks."
    }, {
      "heading" : "A Temperature τ",
      "text" : "To study the effect of temperature τ , we conduct the additional experiments of four baselines with the standard choice of the temperature τ ∈ {1, 2, 4, 6, 8, 10}. All models are trained on Spoken-CoQA dataset, and validated on the Spoken-CoQA test set. We present the results in Figure 4. When τ is set to 2, four baselines all achieve their best performance in term of F1 and EM metrics."
    }, {
      "heading" : "B Effects of Different Word Error Rates",
      "text" : "We study how the network performances change when trained with different word error rates (WER) in Figure 3. Specifically, we first split SpokenSQuAD and Spoken-CoQA into smaller groups with different WERs. Then we utilize Frame-level F1 score (Chuang et al., 2019) to validate the effectiveness of our proposed method on Spoken-CoQA. In Figure 3, we find that all evaluated networks for two tasks are remarkably similar: all evaluated models suffer larger degradation in performance at higher WER, and adopting knowledge distillation strategy is capable of alleviating such issues. Such phenomenon further demonstrates the importance of knowledge distillation in the case of high WER."
    }, {
      "heading" : "C Results on Human Recorded Speech",
      "text" : "The results using BERT-base as the baseline are shown in Table 7. We train the model in the Spoken-CoQA training dataset and evaluate the model in both machine synthesized and human recorded speech. As shown in Table 7, the average EM/F1/AOS scores using BERT fell from 40.6/54.1/48.0 to 39.4/53.0/46.8, respectively. In addition, the similar trends can be observed on our proposed method. We hypothesise that the human recorded speech introduces additional noise during training, which leads to the performance degradation."
    }, {
      "heading" : "D More Information about Spoken-CoQA",
      "text" : "To perform qualitative analysis of speech features, we visualize the log-mel spectrogram features and the mel-frequency cepstral coefficients (MFCC) feature embedding learned by DDNet in Figure 5. We can observe how the spectrogram features respond to different sentence examples. In this example, we observe that given the text document (ASRdocument), the conversation starts with the question Q1 (ASR-Q1), and then the system requires to answer Q1 (ASR-Q1) with A1 based on a contiguous text span R1. Compared to the existing benchmark datasets, ASR transcripts (both the document and questions) are much more difficult for the machine to comprehend questions, reason among the passages, and even predict the correct answer."
    }, {
      "heading" : "E More Comparisons on Spoken-SQuAD",
      "text" : "To verify that our proposed DDNET is not biased towards specific settings, we conduct a series of experiments on Spoken-SQuAD (Li et al., 2018) by training the teacher model on textual documents, and the student model on the ASR transcripts. From the Table 5, compared with the performances on Spoken-CoQA, all baselines performances improve by a large margin, indicating our proposed dataset is a more challenging task for current models. We verify that, in the setting (KD+DA), the model consistently achieves significant performance boosts on all\nbaselines. Specifically, for FlowQA, our method achieves 55.6%/68.8% (vs.51.9%/65.7%), and 52.8%/68.0% (vs.49.1%/63.9%) in terms of EM/F1 score over the text documents and ASR transcriptions, respectively. For SDNet, our method outperforms the baseline without distillation, achieving 60.1%/73.7% (vs.56.1%/70.5%) and 60.9%/75.7% (vs.57.8%/71.8%) in terms of EM/F1 score. As for two BERT-based models (BRET-large and ALBERT-large), our methods with KD consistently improve EM/F1 scores to 62.1%/74.6% (vs.58.3%/70.2%) and 63.3%/76.0% (vs.58.6%/71.1%); 62.6%/75.7% (vs.59.1%/71.9%) and 64.1%/77.1% (vs.59.4%/72.2%), respectively. These results confirm the importance of knowledge distillation strategy and dual attention mechanism."
    }, {
      "heading" : "F Broader Impact",
      "text" : "In this section, we acknowledge that our work will not bring potential risks to society considering the data is from open source with no private or sensitive information. We also discuss some limitations of our work. First, we admit that using Google TTS for TTS and CMU Sphinx for ASR may affect the distribution of errors compared with the human recorded speech. Second, we currently cover only English language but it would be interesting to see that contributions for other languages would follow. Finally, as our collection comes with reliable data, it should trigger future analysis works on analyzing\nspoken conversational question answering biases."
    } ],
    "references" : [ {
      "title" : "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "author" : [ "Alexei Baevski", "Steffen Schneider", "Michael Auli." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Baevski et al\\.,? 2019",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2019
    }, {
      "title" : "Spoken language understanding without speech recognition",
      "author" : [ "Yuan-Ping Chen", "Ryan Price", "Srinivas Bangalore." ],
      "venue" : "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Quac: Question answering in context",
      "author" : [ "Eunsol Choi", "He He", "Mohit Iyyer", "Mark Yatskar", "Wentau Yih", "Yejin Choi", "Percy Liang", "Luke Zettlemoyer." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "SpeechBERT: Cross-modal pre-trained language model for end-to-end spoken question answering",
      "author" : [ "Yung-Sung Chuang", "Chi-Liang Liu", "Hung-Yi Lee." ],
      "venue" : "arXiv preprint arXiv:1910.11559.",
      "citeRegEx" : "Chuang et al\\.,? 2019",
      "shortCiteRegEx" : "Chuang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Dataset and baselines for sequential opendomain question answering",
      "author" : [ "Ahmed Elgohary", "Chen Zhao", "Jordan Boyd-Graber." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Elgohary et al\\.,? 2018",
      "shortCiteRegEx" : "Elgohary et al\\.",
      "year" : 2018
    }, {
      "title" : "The effects of automatic speech recognition quality on human transcription latency",
      "author" : [ "Yashesh Gaur", "Walter S Lasecki", "Florian Metze", "Jeffrey P Bigham." ],
      "venue" : "Proceedings of the 13th Web for All Conference, pages 1–8.",
      "citeRegEx" : "Gaur et al\\.,? 2016",
      "shortCiteRegEx" : "Gaur et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent chunking mechanisms for long-text machine reading comprehensio",
      "author" : [ "Hongyu Gong", "Yelong Shen", "Dian Yu", "Jianshu Chen", "Dong Yu." ],
      "venue" : "arXiv preprint arXiv:2005.08056.",
      "citeRegEx" : "Gong et al\\.,? 2020",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2020
    }, {
      "title" : "From audio to semantics: Approaches to end-to-end spoken language understanding",
      "author" : [ "Parisa Haghani", "Arun Narayanan", "Michiel Bacchiani", "Galen Chuang", "Neeraj Gaur", "Pedro Moreno", "Rohit Prabhavalkar", "Zhongdi Qu", "Austin Waters." ],
      "venue" : "Spoken Language",
      "citeRegEx" : "Haghani et al\\.,? 2018",
      "shortCiteRegEx" : "Haghani et al\\.",
      "year" : 2018
    }, {
      "title" : "Selfknowledge distillation in natural language processing",
      "author" : [ "Sangchul Hahn", "Heeyoul Choi." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP), pages 423–430.",
      "citeRegEx" : "Hahn and Choi.,? 2019",
      "shortCiteRegEx" : "Hahn and Choi.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani" ],
      "venue" : null,
      "citeRegEx" : "Honnibal and Montani.,? \\Q2017\\E",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Attention-guided answer distillation for machine reading comprehension",
      "author" : [ "Minghao Hu", "Yuxing Peng", "Furu Wei", "Zhen Huang", "Dongsheng Li", "Nan Yang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1808.07644.",
      "citeRegEx" : "Hu et al\\.,? 2018",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2018
    }, {
      "title" : "FlowQA: Grasping flow in history for conversational machine comprehension",
      "author" : [ "Hsin-Yuan Huang", "Eunsol Choi", "Wen-tau Yih." ],
      "venue" : "arXiv preprint arXiv:1810.06683.",
      "citeRegEx" : "Huang et al\\.,? 2018a",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Fusionnet: Fusing via fullyaware attention with application to machine comprehension",
      "author" : [ "Hsin-Yuan Huang", "Chenguang Zhu", "Yelong Shen", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:1711.07341.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Knowledge distillation for sequence model",
      "author" : [ "Mingkun Huang", "Yongbin You", "Zhehuai Chen", "Yanmin Qian", "Kai Yu." ],
      "venue" : "Interspeech, pages 3703–3707.",
      "citeRegEx" : "Huang et al\\.,? 2018b",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M Rush." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1317–1327.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "An audio-enriched bert-based framework for spoken multiple-choice question answering",
      "author" : [ "Chia-Chih Kuo", "Shang-Bao Luo", "Kuan-Yu Chen." ],
      "venue" : "arXiv preprint arXiv:2005.12142.",
      "citeRegEx" : "Kuo et al\\.,? 2020",
      "shortCiteRegEx" : "Kuo et al\\.",
      "year" : 2020
    }, {
      "title" : "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Mitigating the impact of speech recognition errors on spoken question answering by adversarial domain adaptation",
      "author" : [ "Chia-Hsuan Lee", "Yun-Nung Chen", "Hung-Yi Lee." ],
      "venue" : "International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "ODSQA: Opendomain spoken question answering dataset",
      "author" : [ "Chia-Hsuan Lee", "Shang-Ming Wang", "Huan-Cheng Chang", "Hung-Yi Lee." ],
      "venue" : "Spoken Language Technology Workshop (SLT), pages 949–956. IEEE.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Spoken SQuAD: A study of mitigating the impact of speech recognition errors on listening comprehension",
      "author" : [ "Chia-Hsuan Li", "Szu-Lin Wu", "Chi-Liang Liu", "Hungyi Lee." ],
      "venue" : "arXiv preprint arXiv:1804.00320.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "William B Dolan." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Speech model pre-training for end-to-end spoken language understanding",
      "author" : [ "Loren Lugosch", "Mirco Ravanelli", "Patrick Ignoto", "Vikrant Singh Tomar", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1904.03670.",
      "citeRegEx" : "Lugosch et al\\.,? 2019",
      "shortCiteRegEx" : "Lugosch et al\\.",
      "year" : 2019
    }, {
      "title" : "Endto-end architectures for asr-free spoken language understanding",
      "author" : [ "Elisavet Palogiannidi", "Ioannis Gkinis", "George Mastrapas", "Petr Mizera", "Themos Stafylakis." ],
      "venue" : "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.",
      "citeRegEx" : "Palogiannidi et al\\.,? 2020",
      "shortCiteRegEx" : "Palogiannidi et al\\.",
      "year" : 2020
    }, {
      "title" : "CoQA: A Conversational Question Answering Challenge",
      "author" : [ "Siva Reddy", "Danqi Chen", "Christopher D Manning." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:249–266.",
      "citeRegEx" : "Reddy et al\\.,? 2019",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards end-to-end spoken language understanding",
      "author" : [ "Dmitriy Serdyuk", "Yongqiang Wang", "Christian Fuegen", "Anuj Kumar", "Baiyang Liu", "Yoshua Bengio." ],
      "venue" : "International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5754–5758.",
      "citeRegEx" : "Serdyuk et al\\.,? 2018",
      "shortCiteRegEx" : "Serdyuk et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving spoken question answering using contextualized word representation",
      "author" : [ "Dan Su", "Pascale Fung." ],
      "venue" : "International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8004– 8008. IEEE.",
      "citeRegEx" : "Su and Fung.,? 2020",
      "shortCiteRegEx" : "Su and Fung.",
      "year" : 2020
    }, {
      "title" : "Towards machine comprehension of spoken content: Initial TOEFL listening comprehension test by machine",
      "author" : [ "Bo-Hsiang Tseng", "Sheng-Syun Shen", "Hung-Yi Lee", "Lin-Shan Lee." ],
      "venue" : "arXiv preprint arXiv:1608.06378.",
      "citeRegEx" : "Tseng et al\\.,? 2016",
      "shortCiteRegEx" : "Tseng et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems (NeurIPS), pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Review conversational reading comprehension",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip S Yu." ],
      "venue" : "arXiv preprint arXiv:1902.00821.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems (NeurIPS), pages 5753–",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge distillation for improved accuracy in spoken question answering",
      "author" : [ "Chenyu You", "Nuo Chen", "Yuexian Zou." ],
      "venue" : "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.",
      "citeRegEx" : "You et al\\.,? 2021",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2021
    }, {
      "title" : "SG-Net: Syntax-guided machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Junru Zhou", "Sufeng Duan", "Hai Zhao", "Rui Wang." ],
      "venue" : "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "SDNet: Contextualized attention-based deep network for conversational question answering",
      "author" : [ "Chenguang Zhu", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "arXiv preprint arXiv:1812.03593.",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "within the natural language processing (NLP) communities (Zhu et al., 2018; Liu et al., 2019; Yang et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 112
    }, {
      "referenceID" : 32,
      "context" : "within the natural language processing (NLP) communities (Zhu et al., 2018; Liu et al., 2019; Yang et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "Existing CQA methods (Huang et al., 2018a; Devlin et al., 2018; Xu et al., 2019; Gong et al., 2020) have achieved superior performances on several benchmark datasets, such as QuAC (Choi et al.",
      "startOffset" : 21,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "Existing CQA methods (Huang et al., 2018a; Devlin et al., 2018; Xu et al., 2019; Gong et al., 2020) have achieved superior performances on several benchmark datasets, such as QuAC (Choi et al.",
      "startOffset" : 21,
      "endOffset" : 99
    }, {
      "referenceID" : 31,
      "context" : "Existing CQA methods (Huang et al., 2018a; Devlin et al., 2018; Xu et al., 2019; Gong et al., 2020) have achieved superior performances on several benchmark datasets, such as QuAC (Choi et al.",
      "startOffset" : 21,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "Existing CQA methods (Huang et al., 2018a; Devlin et al., 2018; Xu et al., 2019; Gong et al., 2020) have achieved superior performances on several benchmark datasets, such as QuAC (Choi et al.",
      "startOffset" : 21,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : ", 2020) have achieved superior performances on several benchmark datasets, such as QuAC (Choi et al., 2018) and CoQA (Elgohary et al.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "In recent years, the natural language processing research community has devoted substantial efforts to text question answering tasks (Huang et al., 2018a; Zhu et al., 2018; Xu et al., 2019; Zhang et al., 2020; Gong et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 228
    }, {
      "referenceID" : 35,
      "context" : "In recent years, the natural language processing research community has devoted substantial efforts to text question answering tasks (Huang et al., 2018a; Zhu et al., 2018; Xu et al., 2019; Zhang et al., 2020; Gong et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 228
    }, {
      "referenceID" : 31,
      "context" : "In recent years, the natural language processing research community has devoted substantial efforts to text question answering tasks (Huang et al., 2018a; Zhu et al., 2018; Xu et al., 2019; Zhang et al., 2020; Gong et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 228
    }, {
      "referenceID" : 34,
      "context" : "In recent years, the natural language processing research community has devoted substantial efforts to text question answering tasks (Huang et al., 2018a; Zhu et al., 2018; Xu et al., 2019; Zhang et al., 2020; Gong et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 228
    }, {
      "referenceID" : 7,
      "context" : "In recent years, the natural language processing research community has devoted substantial efforts to text question answering tasks (Huang et al., 2018a; Zhu et al., 2018; Xu et al., 2019; Zhang et al., 2020; Gong et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 228
    }, {
      "referenceID" : 2,
      "context" : "Within the growing body of work on machine reading comprehension, an important sub-task of text question answering, two signature attributes have emerged: the availability of large benchmark datasets (Choi et al., 2018; Elgohary et al., 2018; Reddy et al., 2019) and pre-trained language models (Devlin et al.",
      "startOffset" : 200,
      "endOffset" : 262
    }, {
      "referenceID" : 5,
      "context" : "Within the growing body of work on machine reading comprehension, an important sub-task of text question answering, two signature attributes have emerged: the availability of large benchmark datasets (Choi et al., 2018; Elgohary et al., 2018; Reddy et al., 2019) and pre-trained language models (Devlin et al.",
      "startOffset" : 200,
      "endOffset" : 262
    }, {
      "referenceID" : 26,
      "context" : "Within the growing body of work on machine reading comprehension, an important sub-task of text question answering, two signature attributes have emerged: the availability of large benchmark datasets (Choi et al., 2018; Elgohary et al., 2018; Reddy et al., 2019) and pre-trained language models (Devlin et al.",
      "startOffset" : 200,
      "endOffset" : 262
    }, {
      "referenceID" : 4,
      "context" : ", 2019) and pre-trained language models (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : ", 2019) and pre-trained language models (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "In parallel to the recent works in natural language processing (Huang et al., 2018a; Zhu et al., 2018), these trends have also been pronounced in the speech field (Chen et al.",
      "startOffset" : 63,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "In parallel to the recent works in natural language processing (Huang et al., 2018a; Zhu et al., 2018), these trends have also been pronounced in the speech field (Chen et al.",
      "startOffset" : 63,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : ", 2018), these trends have also been pronounced in the speech field (Chen et al., 2018; Haghani et al., 2018; Lugosch et al., 2019; Palogiannidi et al., 2020; You et al., 2021), where spoken question answering (SQA), an extended form of QA, has explored the prospect of",
      "startOffset" : 68,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : ", 2018), these trends have also been pronounced in the speech field (Chen et al., 2018; Haghani et al., 2018; Lugosch et al., 2019; Palogiannidi et al., 2020; You et al., 2021), where spoken question answering (SQA), an extended form of QA, has explored the prospect of",
      "startOffset" : 68,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : ", 2018), these trends have also been pronounced in the speech field (Chen et al., 2018; Haghani et al., 2018; Lugosch et al., 2019; Palogiannidi et al., 2020; You et al., 2021), where spoken question answering (SQA), an extended form of QA, has explored the prospect of",
      "startOffset" : 68,
      "endOffset" : 176
    }, {
      "referenceID" : 25,
      "context" : ", 2018), these trends have also been pronounced in the speech field (Chen et al., 2018; Haghani et al., 2018; Lugosch et al., 2019; Palogiannidi et al., 2020; You et al., 2021), where spoken question answering (SQA), an extended form of QA, has explored the prospect of",
      "startOffset" : 68,
      "endOffset" : 176
    }, {
      "referenceID" : 33,
      "context" : ", 2018), these trends have also been pronounced in the speech field (Chen et al., 2018; Haghani et al., 2018; Lugosch et al., 2019; Palogiannidi et al., 2020; You et al., 2021), where spoken question answering (SQA), an extended form of QA, has explored the prospect of",
      "startOffset" : 68,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : "Existing methods (Tseng et al., 2016; Serdyuk et al., 2018; Su and Fung, 2020) focus on optimizing each module in a two-stage manner, where errors in the ASR module would result in severe performance loss.",
      "startOffset" : 17,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "Existing methods (Tseng et al., 2016; Serdyuk et al., 2018; Su and Fung, 2020) focus on optimizing each module in a two-stage manner, where errors in the ASR module would result in severe performance loss.",
      "startOffset" : 17,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : "Existing methods (Tseng et al., 2016; Serdyuk et al., 2018; Su and Fung, 2020) focus on optimizing each module in a two-stage manner, where errors in the ASR module would result in severe performance loss.",
      "startOffset" : 17,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "(Lee et al., 2019) proved that utilizing clean texts can help model trained on the ASR transcriptions to boost the performance via domain adaptation.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "SpeechBERT (Chuang et al., 2019) cascaded the BERT-based models as a unified model, and then trained it in a joint manner of audio and text.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "(Hinton et al., 2015) introduced the idea of Knowledge Distillation (KD) in a teacher-student scenario.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 29,
      "context" : "TOEFL (Tseng et al., 2016) × √ Multi-choice S-SQuAD (Li et al.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : ", 2016) × √ Multi-choice S-SQuAD (Li et al., 2018) × √ Spans ODSQA (Lee et al.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "Previous work has shown that KD can significantly boost prediction accuracy in natural language processing and speech processing (Kim and Rush, 2016; Hu et al., 2018; Huang et al., 2018b; Hahn and Choi, 2019), while adopting KD-based methods for SQA tasks has been less explored.",
      "startOffset" : 129,
      "endOffset" : 208
    }, {
      "referenceID" : 12,
      "context" : "Previous work has shown that KD can significantly boost prediction accuracy in natural language processing and speech processing (Kim and Rush, 2016; Hu et al., 2018; Huang et al., 2018b; Hahn and Choi, 2019), while adopting KD-based methods for SQA tasks has been less explored.",
      "startOffset" : 129,
      "endOffset" : 208
    }, {
      "referenceID" : 15,
      "context" : "Previous work has shown that KD can significantly boost prediction accuracy in natural language processing and speech processing (Kim and Rush, 2016; Hu et al., 2018; Huang et al., 2018b; Hahn and Choi, 2019), while adopting KD-based methods for SQA tasks has been less explored.",
      "startOffset" : 129,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "Previous work has shown that KD can significantly boost prediction accuracy in natural language processing and speech processing (Kim and Rush, 2016; Hu et al., 2018; Huang et al., 2018b; Hahn and Choi, 2019), while adopting KD-based methods for SQA tasks has been less explored.",
      "startOffset" : 129,
      "endOffset" : 208
    }, {
      "referenceID" : 0,
      "context" : "Concretely, for speech input, we first use vqwav2vec (Baevski et al., 2019) to transfer speech signals into a series of tokens, which is the standard tokenization procedure in speech related tasks.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Next, use Speech-BERT (Chuang et al., 2019), a variant of BERT-based models retrained on our Spoken-CoQA dataset, to process the speech sequences for training.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "The text contents are embbed into a sequence of vectors via our text encoder - Text-BERT, with the same architecture of BERT-base (Devlin et al., 2018).",
      "startOffset" : 130,
      "endOffset" : 151
    }, {
      "referenceID" : 23,
      "context" : "Inspired by ViLBERT (Lu et al., 2019), we apply the co-attention transformer layer, a variant of Self-Attention (Vaswani et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 30,
      "context" : ", 2019), we apply the co-attention transformer layer, a variant of Self-Attention (Vaswani et al., 2017), as the Cross Attention module for the fusing of speech and text embeddings.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "As a result, to capture the longrange dependencies such as co-references for the downstream speech-and-language tasks, similar to (Li et al., 2016; Zhu et al., 2018), we introduce a self-attention layer to obtain the final Dual Attention (DA) representations.",
      "startOffset" : 130,
      "endOffset" : 165
    }, {
      "referenceID" : 35,
      "context" : "As a result, to capture the longrange dependencies such as co-references for the downstream speech-and-language tasks, similar to (Li et al., 2016; Zhu et al., 2018), we introduce a self-attention layer to obtain the final Dual Attention (DA) representations.",
      "startOffset" : 130,
      "endOffset" : 165
    }, {
      "referenceID" : 35,
      "context" : "The framework of our SCQA module is similar to recent works (Zhu et al., 2018; Huang et al., 2017), which is divided into three key components: Encoding Layer, Attention Layer and Output Layer.",
      "startOffset" : 60,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "The framework of our SCQA module is similar to recent works (Zhu et al., 2018; Huang et al., 2017), which is divided into three key components: Encoding Layer, Attention Layer and Output Layer.",
      "startOffset" : 60,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "First, we select the conversational question-answering dataset CoQA (Reddy et al., 2019)3 as our basis data since it is one of the largest public CQA datasets.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "9%, which can be considered sufficiently good since it is below the accuracy threshold of 30% WER (Gaur et al., 2016).",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : "Considering that the test set of CoQA (Reddy et al., 2019) idoes not publicly availablesh the test set, we follow the widely used setting in the spoken question answering task (Li et al.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : ", 2019) idoes not publicly availablesh the test set, we follow the widely used setting in the spoken question answering task (Li et al., 2018), where we divide Spoken-CoQA dataset into train and test set.",
      "startOffset" : 125,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "We follow the conventional settings in (Lee et al., 2018)4.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "We choose several state-of-the-art language models (FlowQA (Huang et al., 2018a), SDNet (Zhu et al.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 35,
      "context" : ", 2018a), SDNet (Zhu et al., 2018), BERT-base (Devlin et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : ", 2018), BERT-base (Devlin et al., 2018), ALBERT (Lan et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : ", 2018), ALBERT (Lan et al., 2020)) as our backbone network baselines.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "We also compare our proposed DDNET with several state-of-the-art SQA methods (Lee et al., 2018; Serdyuk et al., 2018; Lee et al., 2019; Kuo et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 153
    }, {
      "referenceID" : 27,
      "context" : "We also compare our proposed DDNET with several state-of-the-art SQA methods (Lee et al., 2018; Serdyuk et al., 2018; Lee et al., 2019; Kuo et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "We also compare our proposed DDNET with several state-of-the-art SQA methods (Lee et al., 2018; Serdyuk et al., 2018; Lee et al., 2019; Kuo et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "We also compare our proposed DDNET with several state-of-the-art SQA methods (Lee et al., 2018; Serdyuk et al., 2018; Lee et al., 2019; Kuo et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : "We use the official BERT (Devlin et al., 2018) and ALBERT (Lan et al.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : ", 2018) and ALBERT (Lan et al., 2020) as our textual embedding modules.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "We use BERTbase (Devlin et al., 2018) and ALBERT-base (Lan et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 18,
      "context" : ", 2018) and ALBERT-base (Lan et al., 2020), which both include 12 transformer encoders, and the hidden size of each word vector is 768.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "BERT and ALBERT both utilize BPE as the tokenizer, but FlowQA and SDNet use SpaCy (Honnibal and Montani, 2017) for tokenization.",
      "startOffset" : 82,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "Under the circumstance when tokens in spaCy (Honnibal and Montani, 2017) correspond to more than one BPE sub-tokens, we average the BERT embeddings",
      "startOffset" : 44,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "For evaluation, we use three metrics: Exact Match (EM), F1 score and Audio Overlapping Score (AOS) (Li et al., 2018) to compare the model performance comprehensively.",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "1 BERT-base + back-translation (Lee et al., 2018) 42.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "6 BERT-base + domain adaptation (Lee et al., 2019) 43.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 20,
      "context" : "7 ALBERT-base + back-translation (Lee et al., 2018) 43.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "0 ALBERT-base + domain adaptation (Lee et al., 2019) 43.",
      "startOffset" : 34,
      "endOffset" : 52
    } ],
    "year" : 0,
    "abstractText" : "In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogue flows given the speech documents. In this task, our main objective is to build the system to deal with conversational questions based on the audio recordings, and to explore the plausibility of providing more cues from different modalities with systems in information gathering. To this end, instead of directly adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNET, which effectively ingests cross-modal information to achieve finegrained representations of the speech and language modalities. Moreover, we propose a simple and novel mechanism, termed Dual Attention, by encouraging better alignments between audio and text to ease the process of knowledge transfer. To evaluate the capacity of SCQA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 40k question-answer pairs from 4k conversations. The performance of the existing state-of-the-art methods significantly degrade on our dataset, hence demonstrating the necessity of cross-modal information integration. Our experimental results demonstrate that our proposed method achieves superior performance in spoken conversational question answering tasks.1",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_200_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Language Model Augmented Monotonic Attention for Simultaneous Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A typical application of simultaneous neural machine translation (SNMT) is conversational speech or live video caption translation. In order to achieve live translation, an SNMT model alternates between performing read from source sequence and write to target sequence. For a model to decide whether to read or write at certain moment, either a fixed or an adaptive read/write policy can be used.\nEarlier approaches in simultaneous translation such as Ma et al. (2019a) and Dalvi et al. (2018) employ a fixed policy that alternate between read and write after the waiting period of k tokens. To alleviate possible long delay of fixed polices, recent works such as monotonic infinite lookback attention (MILk) (Arivazhagan et al., 2019), and monotonic multihead attention (MMA) (Ma et al., 2019c) developed flexible policies using monotonic attention (Raffel et al., 2017).\nWhile these monotonic attention anticipates target words using only available prefix source and\ntarget sequence, human translators anticipate the target words using their language expertise (linguistic anticipation) as well as contextual information (extra-linguistic anticipation) (Vandepitte, 2001). Inspired by human translation experts, we aim to augment monotonic attention with future information using language models (LM) (Devlin et al., 2019; Conneau et al., 2019).\nIntegrating the external information effectively into text-to-text machine translation (MT) systems has been explored by several works (Khandelwal et al., 2020; Gulcehre et al., 2015, 2017; Stahlberg et al., 2018). Also, integrating future information implicitly into SNMT system during training is explored in Wu et al. (2020) by simultaneously training different wait-k SNMT systems. However, no previous works make use of explicit future information both during training and inference. To utilize explicit future information, we explored to integrate future information from LM directly into the output layer of the MMA model. However, it did not provide any improvements (refer to Appendix A), thus motivating us to explore a tighter integration of the LM information into SNMT model.\nIn this work, we explicitly use plausible future information from LM during training by transforming the monotonic attention mechanism. As shown\nin Figure 1, at each step, the LM takes the prefix target (and source, for cross-lingual LM) sequence and predicts the probable future information. We hypothesize that aiding the monotonic attention with this future information can improve MMA model’s read/write policy, eventually leading to better translation with less delay. Several experiments on MuST-C (Di Gangi et al., 2019) EnglishGerman and English-French speech-to-text translation tasks with our proposed approach show clear improvements of latency-quality trade-offs over the state-of-the-art MMA models."
    }, {
      "heading" : "2 Monotonic Attention with Future Information Model",
      "text" : ""
    }, {
      "heading" : "2.1 Monotonic Attention",
      "text" : "In simultaneous machine translation (SNMT) models, the probability of predicting the target token yi ∈ y depends on the partial source and target sequences (x≤j ∈ x, y<i ∈ y). In sequence-tosequence based SNMT model, each target token yi is generated as follows:\nhj = E(x≤j) (1) si = D(y<i, ci = A(si−1, h≤j)) (2)\nyi = Output(si) (3)\nwhere E(.) and D(.) are the encoder and decoder layers, and ci is a context vector. In monotonic attention based SNMT, the context vector is computed as follows:\nei,j = MonotonicEnergy(si−1, hj) (4)\npi,j = Sigmoid(ei,j) (5) zi,j ∼ Bernoulli(pi,j) (6)\nWhen generating a target token yi, the decoder chooses whether to read/write based on Bernoulli selection probability pi,j . When zi,j = 1 (write),\nmodel sets ti = j, ci = hj and generates the target token yi. For zi,j = 0 (read), it sets ti = j + 1 and repeats Eq. 4 to 6. Here ti refers to the index of the encoder when decoder needs to produce the ith target token. Instead of hard alignment of ci = hj , Raffel et al. (2017) compute an expected alignment in a recurrent manner and propose a closed-form parallel solution. Arivazhagan et al. (2019) adopt monotonic attention into SNMT and later, Ma et al. (2019c) extend it to MMA to integrate it into the Transformer model (Vaswani et al., 2017)."
    }, {
      "heading" : "2.2 Monotonic Attention with Future Information",
      "text" : "The monotonic attention described in Section 2.1 performs anticipation based only on the currently available source and target information. To augment this anticipation process using future information extracted using LMs, we propose the following modifications to the monotonic attention.\nFuture Representation Layer: At every decoding step i, the previous target token yi−1 is equipped with a plausible future token ŷi as shown in the Figure 2. Since the token ŷi comes from an LM possibly with a different tokenizer and vocabulary set, applying the model’s tokenizer and vocabulary might split the token ŷi further into multiple sub-tokens {ŷ1i , ŷ2i , · · · , ŷmi }. To get a single future token representation s̃i ∈ Rd from all the sub-tokens, we apply a sub-token summary layer:\ns̃i = Γ({ŷ1i , ŷ2i , · · · , ŷmi }) (7)\nThe Γ represents a general sequence representation layer such as a Transformer encoder layer or a simple normalized sum of sub-token representations.\nWe enrich s̃i at every layer l of the decoder block by applying a residual feed-forward network.\ns̃li = FFN(ỹ l−1 i ) (8)\nMonotonic Energy Layer with Future Information: Despite the fact that we can add the plausible future information to the output layer (Appendix A) or append it to the target token representation yi−1, the MMA read/write decisions happen in Eq. 4. Therefore, we integrate s̃i into the Eq. 4 instead.\nThe integration is carried out by modifying Eq. 4 - Eq. 5. We compute the monotonic energy for future information using the enriched future token representation s̃i available at each layer:\nẽi,j = MonotonicEnergy(s̃i, hj) (9)\nWe integrate the future monotonic energy function into Eq. 5 as follows:\np̃i,j = Sigmoid(ei,j + ẽi,j) (10)\nAfter computing p̃i,j , we compute ci similar to MMA model.\nThis way of integration of future information allows the model to condition the LM output usage on the input sequence. The model can control the relative weightage given to the LM output by varying the ẽi,j . In case of insufficient source information in the low latency regime, we expect the model’s decision policy to rely more on ẽi,j .\nInference: During inference, the start token does not contain any plausible information. After predicting the first target token, for every subsequent prediction of target token yi, we invoke the LM to predict the next plausible future token and integrate this new information into Eq. 10."
    }, {
      "heading" : "3 Experiments and Results",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Settings",
      "text" : "Datasets and Metrics: We conduct our experiments on the MuST-C English(En)-German(De) and English(En)-French(Fr) speech-to-text (ST) translation task. The speech sequence is represented using 80-dimensional log-mel filter bank features. The target sequence is represented as subwords using a SentencePiece (Kudo and Richardson, 2018) model with a unigram vocabulary of size 10,000. We evaluate the performance of the models on both the latency and quality aspects. We use Average Lagging(AL) as our latency metric and case-sensitive detokenized SacreBLEU (Post, 2018) to measure the translation quality, similar to (Ma et al., 2020). The best models are chosen based on the dev set results and reported results are from the MuST-C test (tst-COMMON) sets.\nLanguage Models We use two language models to train our proposed modified MMA model. Firstly, we use the pretrained XLM-RoBERTa (Conneau et al., 2019) model from Huggingface Transformers1 model repository. Since the LM output can be very open-ended and might not directly suit/cater to our task and dataset, we finetune the head of the model using the MuST-C target text data for each task.\nWe also train a smaller language model (SLM), which contains 6 Transformer decoder layers, 512 hidden-states and 24M parameters. We use the MuST-C data along with additional data augmentation to reduce overfitting. The SLM helps to remove the issues related to vocabulary mismatch as discussed in the Section 2.2.\nImplementation Details: Our base model is adopted from Ma et al. (2020). We use a predecision ratio of 7 , which means that the simultaneous read/write decisions are made after every seven encoder states. We use λ or λlatency to refer to the hyperparameter corresponding to the weighted average(λavg) in MMA. The values of this hyperparameter λ are chosen from the set {0.01, 0.05, 0.1}. The Γ layer in Eq. 7 computes the normalized sum of the sub-token representations. For SLM, it simply finds the embedding since it shares the same vocabulary set. All the models are trained on a NVIDIA v100 GPU with update_freq set to 8.\nSimultaneous Translation Models: Even though future information can be integrated explicitly into the fixed policy approaches such as Wait-K (Ma et al., 2019b), we choose monotonic attention as our baseline due to its superior performance (Arivazhagan et al., 2019; Ma et al.,\n1https://huggingface.co/transformers/\n2019c). We train a baseline based on Ma et al. (2020) work, called as MMA model. The MMA model encoder and decoder embedding dimensions are set to 392, whereas our proposed model’s encoder and decoder embeddings are set to 256 to have similar parameters (≈ 39M ) for a fair comparison. We train two models using the modified MMA based on two LMs (XLM, SLM), referred as MMA-XLM and MMA-SLM."
    }, {
      "heading" : "3.2 Results",
      "text" : "We first analyze how the LM predictions are being utilized by the our model. In order to measure the relative weight given to model’s internal states versus the predictions from the LM, we compare the norm of the monotonic energies corresponding to the LM predictions epred (Eq. 9) and the previous output tokens eoutput (Eq. 4). Let us define LM prediction weight as follows:\nLMpw = ( ∥epred∥ ∥eoutput∥ ) (11)\nIn Figure 3, we plot the variation of LMpw (averaged) vs. λ. We use two additional values of λ ∈ {0.005, 0.001} to obtain this plot. We can observe that as the latency requirements become more and more strict, the model starts to give more weightage to the predictions coming from the LM. This shows that the model learns to utilize the information coming from LM predictions based on latency requirements.\nNext, we discuss the performance improvements obtained from our proposed approach. By varying the λ, we train separate models for different latency regimes. Moreover, the quality and latency for a particular model can also be varied by controlling the speech segment size during the inference. Speech segment size or step size refers to the duration of speech (in ms) processed corresponding to each read decision. We vary these hyperparameters for all the three models, namely MMA, MMAXLM and MMA-SLM.\nThe BLEU-AL curves for all the models have been provided in Figure 4 and BLEU-AL numbers for all models are included in Appendix F for reference. We vary the step sizes in intervals of 80ms from 120 ms to 520 ms in order to get performances corresponding to different latency regimes. We can observe that the LM-based models using both XLM and SLM provide a significant performance improvement over the baseline MMA model. We observe improvements in the range of 1-2 BLEU scores consistently across all the latency regimes (λ = 0.1, 0.05, 0.01). The MMA using SLM language model performs slightly better than MMA using XLM language model. This is due to SLM’s higher accuracy on the next token prediction task as compared to XLM, 30.15% vs. 21.5% for German & 31.65% vs. 18.45% for French. The high accuracy of SLM is attributed to its training on in-domain data."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this work, we provide a generic framework to integrate the linguistic and extra-linguistic information into simultaneous models. We rely on language models to extract this plausible future information and propose a new monotonic attention mechanism to infuse this information. Several experiments on speech-to-text translation tasks show the effectiveness of proposed approach on obtaining superior quality-latency trade-offs, compared to the state-of-the-art monotonic multihead attention."
    }, {
      "heading" : "A LM at MMA Output Layer",
      "text" : "We explored a naive approach of integrating LM information into the MMA. In this approach, we integrate the future information obtained from the LM directly into the output layer of the MMA model. We refer to this experiment as ‘LM Rescoring(LMR)’, and the corresponding model is called MMA-LMR.\nAs observed in Figure 5, MMA-LMR has inferior performance compared to the MMA model. Since the LM information integration is only done at the output layer of the model, the MMA model cannot easily discard the incorrect information from LM. This motivates us to tightly integrate the LM information into the simultaneous model."
    }, {
      "heading" : "B Language Models",
      "text" : "As mentioned earlier, we train two different language models (LMs) and use them to improve the anticipation in monotonic attention based Simultaneous models.\nB.1 XLM-Roberta(XLM-R)\nXLM-R Large model 2 was trained on the 100 languages CommonCrawl corpora total size of 2.5TB with 550M parameters from 24 layers, 1024 hidden states, 4096 feed-forward hidden-states, and 16 heads. Total number of parameters is 558M. We finetune the head of the XLM-R LM model using the Masked Language Modeling objective which accounts for 0.23% of the total model parameters, i.e., 1.3M parameters.\nB.2 Smaller Language Model\nSince the LM predictions are computed serially during inference, the time taken to compute the LM token serves as a bottleneck to the latency requirements. To reduce the LM computation time, we train a smaller Language Model (SLM) from scratch using the Causal Language Modeling objective. SLM is composed of 6 Transformer decoder blocks, 512 hidden-states, 2048 feed-forward\n2https://huggingface.co/xlm-roberta-large\nhidden-states & 8 attention heads. It alleviates the need for the sub-token summary layer since it shares the vocabulary and tokenization with the MMA models. The train examples are at the sentence level, rather than forming a block out of multiple sentences(which is the usual case for Language Models).\nSince the target texts contain lesser than 250k examples, we use additional data augmentation techniques to upsample the target data. We also use additional data to avoid overfitting on the MuST-C target text. Details have been provided in B.2.1.\nB.2.1 Data Augmentation Up-Sampling: To boost the LM performance and mitigate overfitting, we use contextual data augmentation (Kobayashi, 2018) to upsample the MuST-C target text data by substituting and inserting words based on LM predictions. We use the NLPAUG 3 package to get similar words based on contextual embeddings. From the Hugging Face Repository, we use two different pretrained BERT (Devlin et al., 2019) models for German bert-basegerman-dbmdz-cased & bert-base-german-dbmdzuncased and bert-base-fr-cased for French. We upsample German to 1.13M examples and French to 1.38M examples.\nAdditional Data: We also use additional data to avoid overfitting. For German we use the Newscrawl(WMT 19) data which includes 58M examples. For French, we use Common Crawl and Europarl to augment 4M extra training examples.\nWe observe that both upsampling and data augmentation help us to reduce the overfitting on the MuST-C dev set.\nB.3 Token Prediction\nFor each output token, the LM prediction is obtained by feeding the prefix upto that token to the LM model. These predictions are pre-computed for training and validation sets. This ensures parallelization and avoids the overhead to run the LM simultaneously during the training process. During inference, the LM model is called every time a new output token is written."
    }, {
      "heading" : "C Dataset",
      "text" : "The MuST-C dataset comprises of English TED talks, the translations and transcriptions have been\n3https://pypi.org/project/nlpaug/\naligned with the speech at sentence level. Dataset statistics have been provided in the Table 1."
    }, {
      "heading" : "D Effect of LM Size on Latency-Quality",
      "text" : "We train several SLM models with varying sizes in our experiments and choose the best model based on the top-1 accuracy. As we increase the number of layers in the LM model from 2 to 4 to 6 layers, the SLM and the proposed MMA with future information models have shown performance improvements. However, increasing the number of layers greater than 6 does not yield any performance improvements. We also notice this degradation of performance with the XLM model while varying the number of hidden layers in the LM head."
    }, {
      "heading" : "E Training Details",
      "text" : "We follow the training process similar to Ma et al. (2020) training process. We train an English ASR model using the source speech data. Next, we train a simultaneous model without the latency loss (setting λlatency = 0) after initializing the encoder from the English ASR model. After this step, we finetune the simultaneous model for different λs. This training process is repeated for all the reported models and for each task. The details regarding the hyperparameters for the model have been provided in Table 2."
    }, {
      "heading" : "F BLEU-AL Numbers",
      "text" : "As mentioned in the results section of the main paper, we vary the latency weight hyperparameter (λ) to train different models to obtain different latency regimes. We also vary the step-size/speech segment size during inference. In total, we obtain 18 different data points corresponding to each model. In Table 3, we compare the results obtained using MMA, MMA-XLM and MMA-SLM under similar hyperparameter settings. It will help the reader to quantify the benefits obtained from our proposed approach."
    } ],
    "references" : [ {
      "title" : "Monotonic infinite lookback attention for simultaneous machine translation",
      "author" : [ "Naveen Arivazhagan", "Colin Cherry", "Wolfgang Macherey", "Chung-Cheng Chiu", "Semih Yavuz", "Ruoming Pang", "Wei Li", "Colin Raffel." ],
      "venue" : "Proceedings of the 57th Annual",
      "citeRegEx" : "Arivazhagan et al\\.,? 2019",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv",
      "citeRegEx" : "Conneau et al\\.,? 2019",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2019
    }, {
      "title" : "Incremental decoding and training methods for simultaneous translation in neural machine translation",
      "author" : [ "Fahim Dalvi", "Nadir Durrani", "Hassan Sajjad", "Stephan Vogel." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Dalvi et al\\.,? 2018",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "MuST-C: a Multilingual Speech Translation Corpus",
      "author" : [ "Mattia A. Di Gangi", "Roldano Cattoni", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "On using monolingual corpora in neural machine translation",
      "author" : [ "Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2015
    }, {
      "title" : "On integrating a language model into neural machine translation",
      "author" : [ "Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Computer Speech and Language, 45:137–148.",
      "citeRegEx" : "Gulcehre et al\\.,? 2017",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2017
    }, {
      "title" : "Nearest neighbor machine translation",
      "author" : [ "Urvashi Khandelwal", "Angela Fan", "Dan Jurafsky", "Luke Zettlemoyer", "Mike Lewis." ],
      "venue" : "arXiv preprint arXiv:2010.00710.",
      "citeRegEx" : "Khandelwal et al\\.,? 2020",
      "shortCiteRegEx" : "Khandelwal et al\\.",
      "year" : 2020
    }, {
      "title" : "Contextual augmentation: Data augmentation by words with paradigmatic relations",
      "author" : [ "Sosuke Kobayashi." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Kobayashi.,? 2018",
      "shortCiteRegEx" : "Kobayashi.",
      "year" : 2018
    }, {
      "title" : "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "arXiv preprint arXiv:1808.06226.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "2019a. Stacl: Simultaneous translation with implicit anticipation and controllable la",
      "author" : [ "Mingbo Ma", "Liang Huang", "Hao Xiong", "Renjie Zheng", "Kaibo Liu", "Baigong Zheng", "Chuanqiang Zhang", "Zhongjun He", "Hairong Liu", "Xing Li", "Hua Wu", "Haifeng Wang" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. STACL: Simultaneous translation with implicit anticipation and controllable la",
      "author" : [ "Mingbo Ma", "Liang Huang", "Hao Xiong", "Renjie Zheng", "Kaibo Liu", "Baigong Zheng", "Chuanqiang Zhang", "Zhongjun He", "Hairong Liu", "Xing Li", "Hua Wu", "Haifeng Wang" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Monotonic multihead attention",
      "author" : [ "Xutai Ma", "Juan Pino", "James Cross", "Liezl Puzon", "Jiatao Gu" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Simulmt to simulst: Adapting simultaneous text translation to end-to-end simultaneous speech translation",
      "author" : [ "Xutai Ma", "Juan Pino", "Philipp Koehn." ],
      "venue" : "arXiv preprint arXiv:2011.02048.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting bleu scores",
      "author" : [ "Matt Post." ],
      "venue" : "arXiv preprint arXiv:1804.08771.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Online and lineartime attention by enforcing monotonic alignments",
      "author" : [ "Colin Raffel", "Minh-Thang Luong", "Peter J. Liu", "Ron J. Weiss", "Douglas Eck." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings",
      "citeRegEx" : "Raffel et al\\.,? 2017",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple fusion: Return of the language model",
      "author" : [ "Felix Stahlberg", "James Cross", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 204–211, Brussels, Belgium. Association for Computational Lin-",
      "citeRegEx" : "Stahlberg et al\\.,? 2018",
      "shortCiteRegEx" : "Stahlberg et al\\.",
      "year" : 2018
    }, {
      "title" : "Anticipation in conference interpreting: a cognitive process",
      "author" : [ "Sonia Vandepitte." ],
      "venue" : "Alicante Journal of English Studies / Revista Alicantina de Estudios Ingleses, 0(14):323–335.",
      "citeRegEx" : "Vandepitte.,? 2001",
      "shortCiteRegEx" : "Vandepitte.",
      "year" : 2001
    }, {
      "title" : "Attention is all",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin" ],
      "venue" : null,
      "citeRegEx" : "Vaswani et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To alleviate possible long delay of fixed polices, recent works such as monotonic infinite lookback attention (MILk) (Arivazhagan et al., 2019), and monotonic multihead attention (MMA) (Ma et al.",
      "startOffset" : 117,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : ", 2019c) developed flexible policies using monotonic attention (Raffel et al., 2017).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "target sequence, human translators anticipate the target words using their language expertise (linguistic anticipation) as well as contextual information (extra-linguistic anticipation) (Vandepitte, 2001).",
      "startOffset" : 186,
      "endOffset" : 204
    }, {
      "referenceID" : 3,
      "context" : "Inspired by human translation experts, we aim to augment monotonic attention with future information using language models (LM) (Devlin et al., 2019; Conneau et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "Inspired by human translation experts, we aim to augment monotonic attention with future information using language models (LM) (Devlin et al., 2019; Conneau et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "Integrating the external information effectively into text-to-text machine translation (MT) systems has been explored by several works (Khandelwal et al., 2020; Gulcehre et al., 2015, 2017; Stahlberg et al., 2018).",
      "startOffset" : 135,
      "endOffset" : 213
    }, {
      "referenceID" : 16,
      "context" : "Integrating the external information effectively into text-to-text machine translation (MT) systems has been explored by several works (Khandelwal et al., 2020; Gulcehre et al., 2015, 2017; Stahlberg et al., 2018).",
      "startOffset" : 135,
      "endOffset" : 213
    }, {
      "referenceID" : 18,
      "context" : "(2019c) extend it to MMA to integrate it into the Transformer model (Vaswani et al., 2017).",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "The target sequence is represented as subwords using a SentencePiece (Kudo and Richardson, 2018) model with a unigram vocabulary of size 10,000.",
      "startOffset" : 69,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "We use Average Lagging(AL) as our latency metric and case-sensitive detokenized SacreBLEU (Post, 2018) to measure the translation quality, similar to (Ma et al.",
      "startOffset" : 90,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "We use Average Lagging(AL) as our latency metric and case-sensitive detokenized SacreBLEU (Post, 2018) to measure the translation quality, similar to (Ma et al., 2020).",
      "startOffset" : 150,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "Firstly, we use the pretrained XLM-RoBERTa (Conneau et al., 2019) model from Huggingface Transformers1 model repository.",
      "startOffset" : 43,
      "endOffset" : 65
    } ],
    "year" : 0,
    "abstractText" : "The state-of-the-art adaptive policies for simultaneous neural machine translation (SNMT) use monotonic attention to perform read/write decisions based on the partial source and target sequences. The lack of sufficient information might cause the monotonic attention to take poor read/write decisions, which in turn negatively affects the performance of the SNMT model. On the other hand, human translators make better read/write decisions since they can anticipate the immediate future words using linguistic information and domain knowledge. In this work, we propose a framework to aid monotonic attention with an external language model to improve its decisions. Experiments on MuST-C English-German and English-French speech-to-text translation tasks show the future information from language model improves the state-of-the-art monotonic multi-head attention model further.",
    "creator" : null
  }
}
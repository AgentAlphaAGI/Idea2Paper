{
  "name" : "ARR_2022_51_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "When do Contrastive Word Alignments Improve Many-to-many Neural Machine Translation?",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many-to-many neural machine translation (NMT) (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Sen et al., 2019; Arivazhagan et al., 2019; Lin et al., 2020; Pan et al., 2021b) jointly trains a translation system for multiple language pairs and obtain significant gains consistently across many translation directions. Previous work (Lin et al., 2020) shows that word alignment information helps improve pre-training for many-to-many NMT. However, cleaned high-quality ground-truth bilingual dictionaries are used to pre-edit the source sentences, which are unavailable for most language pairs.\nRecently, contrastive objectives (Clark et al., 2020; Gunel et al., 2021; Giorgi et al., 2021; Wei et al., 2021) have been shown to be superior at leveraging alignment knowledge in various NLP tasks by contrasting the representations of positive and negative samples in a discriminative manner. This objective, which implicitly utilizes word alignment\nlearned by any toolkit refraining the constraints of using manually constructed dictionaries, has not been explored in the context of leveraging word alignment for many-to-many NMT.\nAn existing contrastive method (Pan et al., 2021b) (mRASP2) for many-to-many NMT relies on sentence-level alignments. Given that the incorporation of word alignments has led to improvements in previous work, we believe that finegrained contrastive objectives focusing on word alignments should help improve translation. Therefore, this paper proposes word-level contrastive learning for many-to-many NMT using the word alignment extracted by automatic aligners. We conduct experiments on three many-to-many NMT systems covering general and spoken language domains. Results show that our proposed method achieves significant BLEU gains in the general domain compared to previous word alignment based methods and the sentence-level contrastive method.\nWe then analyze how the word-level contrastive objective affects NMT training. Inspired by previous work (Artetxe and Schwenk, 2019) training sentence retrieval model using many-to-many NMT, we speculate that our contrastive objectives affect the sentence retrieval performance and subsequently impact the translation quality. Further investigation reveals that in many-to-many NMT, the sentence retrieval precision of the multilingual encoder for a language pair strongly correlates with its translation quality (BLEU), which provides insight about when contrastive alignment improves translation. This revelation emphasizes the importance of improving the retrieval performance of the encoder for many-to-many NMT."
    }, {
      "heading" : "2 Word-level Contrastive Learning for Many-to-many NMT",
      "text" : "Inspired by the contrastive learning framework (Chen et al., 2020) and the sentence-level contrastive learning objective of mRASP2, we pro-\npose a word-level contrastive learning objective to explicitly guide the training of the multilingual encoder to obtain well-aligned cross-lingual representations. Specifically, we use word alignments, obtained using automatic word aligners, to supervise the training of the multilingual encoder by a contrastive objective alongside the NMT objective. Alignment Extraction Two main approaches for automatically extracting aligned words from a sentence pair are: using a bilingual dictionary and using unsupervised word aligners. The former extracts fewer but precise alignments, whereas the latter extracts more but noisy alignments. We extract word-level alignments by both methods and explore how they impact NMT training. For the former approach, we use word2word (Choe et al., 2020) to construct bilingual lexicons and then extract word pairs from parallel sentences. The extracted word pairs are combined to form a phrase if words are consecutive in the source and target sentence. For the latter approach, we use FastAlign (Dyer et al., 2013) and use only 1-to-1 mappings for training. Word-level Contrastive Learning With the extracted alignments, we propose a word-level contrastive learning objective for the multilingual encoder by the motivation that the aligned words within a sentence pair should have a similar contextual representation. We expect the supervision of the contrastive objective on the corresponding contextual word representation leads to a robust multilingual encoder. Assume that the tokenized source and target parallel sentences in i− th batch areDi = {srcij , tgtij}Bj=1, and the extracted alignments from all the sentence pairs in each batch are Ai = {sik, tik}Nk=1, where B and N denote the batch-size and the number of alignments, respectively. Note that sik and tik may contain several tokens after the word combination for word2word or subword tokenization for NMT. Then the wordlevel contrastive loss in a batch is:\nL(i)align = − N∑ k=1 (log exp (sim(sik, tik)/T )∑N m=1 exp (sim(sik, tim)/T )\n+ log exp (sim(sik, tik)/T )∑N\nm=1 exp (sim(sim, tik)/T ) )\n(1) where T denotes a similarity scaling temperature. The similarity between two words is measured by:\nsim(wordx, wordy) = cos(g(x̄), g(ȳ)) (2)\nwhere g(x) = W2σ(W1x) and x̄ denotes the average of contextual hidden states of the correspond-\ning subword positions on top of the multilingual encoder. Following (Chen et al., 2020), we use an MLP between contrastive loss and the contextual representation for NMT loss. ReLU activation is used for σ, W1 is d× d and W2 is d× d′, where d is the encoder’s hidden dimension and d′ < d .\nFinally, to jointly train with the NMT loss, we use the following equation to combine our proposed word-level contrastive loss for a batch:\nL(i) = 1 B (L(i)NMT + w NT 2N L(i)align) (3)\nwhere NT is the number of the tokens within a batch, NT2N is a multiplier that scales the contrastive loss to be consistent with NMT loss, and w is a weight to balance the joint training."
    }, {
      "heading" : "3 Experimental Settings",
      "text" : "Datasets and Preprocessing We selected ten languages, including English (en), Estonian (et), Italian (it), Japanese (ja), Kazakh (kk), Burmese (my), Dutch (nl), Romanian (ro), Turkish (tr), Vietnamese (vi) from different language families to train the NMT systems. We used the parallel datasets from different domains for the selected nine language pairs, including IWSLT, WMT, and ALT. We followed mBART (Liu et al., 2020) for tokenization. Details are given in Appendix A. For each parallel dataset, we implemented two approaches as stated in Section 2 to extract word pairs for the contrastive training objective. Data source and the number of the extracted word pairs are shown in Table 1. To ensure high alignment quality, we used large-scale out-of-domain (see Appendix B) parallel corpora with FastAlign.\nMany-to-many NMT systems We established three many-to-many NMT systems as follows:\n222_en-ja: Bidirectional en-ja NMT model using en-ja parallel corpus.\n626_en-it-ja-nl-tr-vi: 6-to-6 multilingual NMT model using spoken language domain corpora for en-it, en-ja, en-nl, en-tr and en-vi.\n626_en-tr-ro-et-my-kk: 6-to-6 multilingual NMT model using general domain corpora for en-tr, en-ro, en-et, en-my and en-kk. Baselines and Ours For each language group setting above, we conducted NMT experiments on both the multilingual training from scratch (MLSC) (Johnson et al., 2017; Aharoni et al., 2019) and the mBART multilingual fine-tuning (mBART FT) (Tang et al., 2020) as baselines. We applied our proposed word-level contrastive learning in both MLSC and mBART FT, and compared with another strong baseline, word alignment based joint NMT training (+align) (Garg et al., 2019). For applying our method, we investigated the performance of joint training with word pairs extracted by both word2word (+w2w) and FastAlign (+FA). Implementation We used mBART-large for mBART FT and transformer-base (Vaswani et al., 2017) for MLSC. See Appendix C for details."
    }, {
      "heading" : "4 Results and Analyses",
      "text" : "BLEU Results We report case-sensitive tokenized BLEU (Papineni et al., 2002) results in Table 2 and 3. In Table 2, we observe that with our proposed training objectives, BLEU scores are comparable in 222_en-ja and 626_en-it-ja-nl-tr-vi while they are slightly improved in 626_en-tr-ro-et-mykk. However, “+align” performs comparable or even worse compared with the baseline. Referring\nto Table 3 for specific BLEUs on each language pair, we find that with our methods, translation performances are significantly improved for mBART FT while nontrivial improvements can merely be observed on en-ro and en-kk direction for MLSC. Latent Encoder Alignment Property We now inspect which aspect of alignment-based methods impacts the translation performance. Previous work (Artetxe and Schwenk, 2019) show that the encoder of a strong multilingual NMT system is an ideal model for the bilingual sentence retrieval task. Inspired by this, we speculate that alignment-based objectives affect sentence retrieval performance, which further impacts the translation quality. We train MLSC and mBART FT and report the sentence retrieval precision and NMT loss during the training. Results are reported in Figure 2. We observe that the validation retrieval precision show the similar trend as the NMT loss. This indicates that during the normal many-to-many NMT training, encoder-side sentence-level retrieval precision is optimized along with the NMT loss. Sentence Retrieval P@1 Correlates with BLEU According to the investigation of the encoder alignment property above, we verify the relationship between BLEU score and sentence retrieval precision on the validation set for each language pair. Results are shown in Figure 1. Cross-referencing\nthe BLEU score in Table 3, we found that BLEU scores are improved when the encoder achieves gains on the sentence retrieval precision.1 For example, we see increases of the retrieval P@1 on enro, en-et, and en-my on mBART FT (the middle of Figure 1) while BLEU scores are significantly improved on these three language pairs (Table 3). We further calculate the Pearson correlation coefficient between the BLEU changes and sentence retrieval P@1 changes for mBART+align, mBART+w2w, and mBART+FA in the 626_en-tr-ro-et-my-kk setting. Results are 0.79, 0.93, 0.90, respectively, demonstrating a strong correlation between translation quality and sentence retrieval precision. Word-level Contrastive Objective and Sentence Retrieval P@1 With the word-level contrastive objective, we observed significant BLEU score improvements on language pairs such as en-ro, en-et and en-my as presented in Table 3. However, due\n1222_en-ja MLSC setting can hardly learn a well-aligned encoder while our methods improve the encoder sentencelevel alignment quality without sacrificing BLEU scores.\nto the noises of extracted word pairs (Pan et al., 2021a) from word alignment toolkits that leads to insufficient supervision for improving sentence retrieval P@1, some language pairs such as en-kk do not show BLEU improvements. We found that for en-kk, numbers of extracted word pairs per sentence by word2word and FastAlign are 1.0 and 2.2, respectively. In contrast, the numbers are 4.2 and 20.7 for improved language pairs, calculated from Table 1. We expect this finding to provide new perspectives for improving many-to-many NMT. Sentence-level Contrastive Objective We conducted the experiments for sentence-level contrastive objective (mRASP2) (Pan et al., 2021b) on 626_en-tr-ro-et-my-kk mBART FT. The average BLEU score of our +w2w is 20.89, which significantly outperforms mRASP’s 20.47 (last line in Table 8). Our word-level method outperforms the sentence-level method, indicating the sentencelevel objective’s limitation. Moreover, we checked the sentence retrieval P@1 for mRASP2 (last line in Table 10) and found that it correlates with BLEU changes, indicating that sentence-level contrastive objective is suboptimal for language pairs with decreased retrieval precision.2"
    }, {
      "heading" : "5 Conclusion",
      "text" : "We proposed a word-level contrastive learning objective for many-to-many NMT. Experimental results showed that our proposed method leads to significantly better translation for several language pairs, which is then explained by analyses showing the relationship between BLEU scores and sentence retrieval performance of the NMT encoder. Future work can focus on: (1) further improving the encoder’s retrieval ability in many-to-many NMT; (2) contrastive objective’s feasibility in a massively multilingual scenario.\n2Note that the sentence-level contrastive objective incorporates sentences in multiple languages for contrastive loss. It does not necessarily improve the pair-wise retrieval precision."
    }, {
      "heading" : "A Tokenization Settings",
      "text" : "For Japanese, we use Jumanpp (Morita et al., 2015; Tolmachev et al., 2018) for segmentation and we following the setting in mBART (Liu et al., 2020) for other languages: myseg.py (Ding et al., 2020) is used for Burmese, Moses tokenization and special normalization is used for Romanian following (Sennrich et al., 2016),3 and Moses tokenization for other languages.4"
    }, {
      "heading" : "B Datasets and Alignment Extraction",
      "text" : "The datasets used for NMT training, validation and test are shown in Table 4. For the word alignment extraction using FastAlign, we also use outof-domain parallel corpora to train the FastAlign jointly, aiming to obtain word alignments with less noise. The out-of-domain corpora for all the\n3https://github.com/rsennrich/ wmt16-scripts\n4https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl\nlanguage pairs contain Tatoeba, Europarl, GlobalVoices, NewsCommentary, OpenSubtitles, TED, WikiMatrix, QED, GNOME, bible-uedin, and ASPEC (Nakazawa et al., 2016). We collect them from the OPUS project (Christodoulopoulos and Steedman, 2015) and WAT.5 The number of the out-of-domain parallel sentences for each language pair is shown in Table 4.\nC Implementation Details\nFollowing Tang et al. (2020), we set the oversampling temperature of 1.5 for all the settings. For MLSC, we set the dropout of 0.3 to avoid overfitting on small-scale training data. We used the batch size of 1,024 tokens for all the settings. For our word-level contrastive learning, we set the weight of 0.1, the temperature of 0.2, d′ of 128, and a smaller dropout of 0.2 because our proposed objective serves as a regularization part. We followed the hyperparameter setting of Garg et al. (2019) for word alignment-based joint NMT training. We used 8 NVIDIA A100 for mBART FT and 8 TITAN Xp for MLSC model training. The model is validated every 1000 steps for 222_en-ja and 2000 steps for both two 626 settings. We do the early stopping if no improvement of the validation loss is observed for 8 checkpoints. The model with the best validation loss was used for evaluation."
    }, {
      "heading" : "D BLEU Scores",
      "text" : "We report all the BLEU results of 222_en-ja, 626_en-it-ja-nl-tr-vi, and 626_en-tr-ro-et-my-kk in\n5https://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2021/index.html\nTable 5, 7 and 8, respectively."
    }, {
      "heading" : "E Sentence Retrieval Precision",
      "text" : "We report the sentence retrieval precisions for all the systems in Table 6, 9 and 10."
    } ],
    "references" : [ {
      "title" : "Massively multilingual neural machine translation",
      "author" : [ "Roee Aharoni", "Melvin Johnson", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Aharoni et al\\.,? 2019",
      "shortCiteRegEx" : "Aharoni et al\\.",
      "year" : 2019
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey E. Hinton." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "word2word: A collection of bilingual lexicons for 3,564 language pairs",
      "author" : [ "Yo Joong Choe", "Kyubyong Park", "Dongwoo Kim." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 3036–3045, Marseille, France. European",
      "citeRegEx" : "Choe et al\\.,? 2020",
      "shortCiteRegEx" : "Choe et al\\.",
      "year" : 2020
    }, {
      "title" : "A massively parallel corpus: the bible in 100 languages",
      "author" : [ "Christos Christodoulopoulos", "Mark Steedman." ],
      "venue" : "Lang. Resour. Evaluation, 49(2):375–395.",
      "citeRegEx" : "Christodoulopoulos and Steedman.,? 2015",
      "shortCiteRegEx" : "Christodoulopoulos and Steedman.",
      "year" : 2015
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards burmese (myanmar) morphological analysis: Syllable-based tokenization and part-of-speech tagging",
      "author" : [ "Chenchen Ding", "Hnin Thu Zar Aye", "Win Pa Pa", "Khin Thandar Nwet", "Khin Mar Soe", "Masao Utiyama", "Eiichiro Sumita." ],
      "venue" : "ACM Trans.",
      "citeRegEx" : "Ding et al\\.,? 2020",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple, fast, and effective reparameterization of IBM model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "Jointly learning to align and translate with transformer models",
      "author" : [ "Sarthak Garg", "Stephan Peitz", "Udhyakumar Nallasamy", "Matthias Paulik." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Garg et al\\.,? 2019",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2019
    }, {
      "title" : "Declutr: Deep contrastive learning for unsupervised textual representations",
      "author" : [ "John M. Giorgi", "Osvald Nitski", "Bo Wang", "Gary D. Bader." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Giorgi et al\\.,? 2021",
      "shortCiteRegEx" : "Giorgi et al\\.",
      "year" : 2021
    }, {
      "title" : "Supervised contrastive learning for pre-trained language model fine-tuning",
      "author" : [ "Beliz Gunel", "Jingfei Du", "Alexis Conneau", "Veselin Stoyanov." ],
      "venue" : "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
      "citeRegEx" : "Gunel et al\\.,? 2021",
      "shortCiteRegEx" : "Gunel et al\\.",
      "year" : 2021
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Pretraining multilingual neural machine translation by leveraging alignment information",
      "author" : [ "Zehui Lin", "Xiao Pan", "Mingxuan Wang", "Xipeng Qiu", "Jiangtao Feng", "Hao Zhou", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pretraining for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Morphological analysis for unsegmented languages using recurrent neural network language model",
      "author" : [ "Hajime Morita", "Daisuke Kawahara", "Sadao Kurohashi." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Morita et al\\.,? 2015",
      "shortCiteRegEx" : "Morita et al\\.",
      "year" : 2015
    }, {
      "title" : "ASPEC: Asian scientific paper excerpt corpus",
      "author" : [ "Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara." ],
      "venue" : "Proceedings of the Tenth International Conference on Language",
      "citeRegEx" : "Nakazawa et al\\.,? 2016",
      "shortCiteRegEx" : "Nakazawa et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual BERT post-pretraining alignment",
      "author" : [ "Lin Pan", "Chung-Wei Hang", "Haode Qi", "Abhishek Shah", "Saloni Potdar", "Mo Yu." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Pan et al\\.,? 2021a",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2021
    }, {
      "title" : "Contrastive learning for many-to-many multilingual neural machine translation",
      "author" : [ "Xiao Pan", "Mingxuan Wang", "Liwei Wu", "Lei Li." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Pan et al\\.,? 2021b",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2021
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Multilingual unsupervised NMT using shared encoder and languagespecific decoders",
      "author" : [ "Sukanta Sen", "Kamal Kumar Gupta", "Asif Ekbal", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Sen et al\\.,? 2019",
      "shortCiteRegEx" : "Sen et al\\.",
      "year" : 2019
    }, {
      "title" : "Edinburgh neural machine translation systems for WMT 16",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 371–376, Berlin, Germany. Association",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual translation with extensible multilingual pretraining and finetuning",
      "author" : [ "Yuqing Tang", "Chau Tran", "Xian Li", "Peng-Jen Chen", "Naman Goyal", "Vishrav Chaudhary", "Jiatao Gu", "Angela Fan." ],
      "venue" : "CoRR, abs/2008.00401.",
      "citeRegEx" : "Tang et al\\.,? 2020",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Juman++: A morphological analysis toolkit for scriptio continua",
      "author" : [ "Arseny Tolmachev", "Daisuke Kawahara", "Sadao Kurohashi." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Tolmachev et al\\.,? 2018",
      "shortCiteRegEx" : "Tolmachev et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "On learning universal representations across languages",
      "author" : [ "Xiangpeng Wei", "Rongxiang Weng", "Yue Hu", "Luxi Xing", "Heng Yu", "Weihua Luo." ],
      "venue" : "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
      "citeRegEx" : "Wei et al\\.,? 2021",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    }, {
      "title" : "2020), we set the oversampling temperature of 1.5 for all the settings. For MLSC, we set the dropout of 0.3 to avoid overfitting on small-scale training data. We used the batch size of 1,024 tokens for all the settings",
      "author" : [ "Following Tang" ],
      "venue" : null,
      "citeRegEx" : "Tang,? \\Q2020\\E",
      "shortCiteRegEx" : "Tang",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Many-to-many neural machine translation (NMT) (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Sen et al., 2019; Arivazhagan et al., 2019; Lin et al., 2020; Pan et al., 2021b) jointly trains a translation system for multiple language pairs and obtain significant gains consistently across many translation directions.",
      "startOffset" : 46,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "Many-to-many neural machine translation (NMT) (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Sen et al., 2019; Arivazhagan et al., 2019; Lin et al., 2020; Pan et al., 2021b) jointly trains a translation system for multiple language pairs and obtain significant gains consistently across many translation directions.",
      "startOffset" : 46,
      "endOffset" : 191
    }, {
      "referenceID" : 20,
      "context" : "Many-to-many neural machine translation (NMT) (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Sen et al., 2019; Arivazhagan et al., 2019; Lin et al., 2020; Pan et al., 2021b) jointly trains a translation system for multiple language pairs and obtain significant gains consistently across many translation directions.",
      "startOffset" : 46,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "Many-to-many neural machine translation (NMT) (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Sen et al., 2019; Arivazhagan et al., 2019; Lin et al., 2020; Pan et al., 2021b) jointly trains a translation system for multiple language pairs and obtain significant gains consistently across many translation directions.",
      "startOffset" : 46,
      "endOffset" : 191
    }, {
      "referenceID" : 18,
      "context" : "Many-to-many neural machine translation (NMT) (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Sen et al., 2019; Arivazhagan et al., 2019; Lin et al., 2020; Pan et al., 2021b) jointly trains a translation system for multiple language pairs and obtain significant gains consistently across many translation directions.",
      "startOffset" : 46,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "Previous work (Lin et al., 2020) shows that word alignment information helps improve pre-training for many-to-many NMT.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "Recently, contrastive objectives (Clark et al., 2020; Gunel et al., 2021; Giorgi et al., 2021; Wei et al., 2021) have been shown to be superior at leveraging alignment knowledge in various NLP tasks by contrasting the representations of positive and negative samples in a discriminative manner.",
      "startOffset" : 33,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "Recently, contrastive objectives (Clark et al., 2020; Gunel et al., 2021; Giorgi et al., 2021; Wei et al., 2021) have been shown to be superior at leveraging alignment knowledge in various NLP tasks by contrasting the representations of positive and negative samples in a discriminative manner.",
      "startOffset" : 33,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "Recently, contrastive objectives (Clark et al., 2020; Gunel et al., 2021; Giorgi et al., 2021; Wei et al., 2021) have been shown to be superior at leveraging alignment knowledge in various NLP tasks by contrasting the representations of positive and negative samples in a discriminative manner.",
      "startOffset" : 33,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "Recently, contrastive objectives (Clark et al., 2020; Gunel et al., 2021; Giorgi et al., 2021; Wei et al., 2021) have been shown to be superior at leveraging alignment knowledge in various NLP tasks by contrasting the representations of positive and negative samples in a discriminative manner.",
      "startOffset" : 33,
      "endOffset" : 112
    }, {
      "referenceID" : 18,
      "context" : "An existing contrastive method (Pan et al., 2021b) (mRASP2) for many-to-many NMT relies on sentence-level alignments.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "Inspired by previous work (Artetxe and Schwenk, 2019) training sentence retrieval model using many-to-many NMT, we speculate that our contrastive objectives affect the sentence retrieval performance and subsequently impact the translation quality.",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Inspired by the contrastive learning framework (Chen et al., 2020) and the sentence-level contrastive learning objective of mRASP2, we pro-",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "For the former approach, we use word2word (Choe et al., 2020) to construct bilingual lexicons and then extract word pairs from parallel sentences.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "For the latter approach, we use FastAlign (Dyer et al., 2013) and use only 1-to-1 mappings for training.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Following (Chen et al., 2020), we use an MLP between contrastive loss and the contextual representation for NMT loss.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "We followed mBART (Liu et al., 2020) for tokenization.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Baselines and Ours For each language group setting above, we conducted NMT experiments on both the multilingual training from scratch (MLSC) (Johnson et al., 2017; Aharoni et al., 2019) and the mBART multilingual fine-tuning (mBART FT) (Tang et al.",
      "startOffset" : 141,
      "endOffset" : 185
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and the mBART multilingual fine-tuning (mBART FT) (Tang et al., 2020) as baselines.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "We applied our proposed word-level contrastive learning in both MLSC and mBART FT, and compared with another strong baseline, word alignment based joint NMT training (+align) (Garg et al., 2019).",
      "startOffset" : 175,
      "endOffset" : 194
    }, {
      "referenceID" : 24,
      "context" : "Implementation We used mBART-large for mBART FT and transformer-base (Vaswani et al., 2017) for MLSC.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "BLEU Results We report case-sensitive tokenized BLEU (Papineni et al., 2002) results in Table 2 and 3.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "Significantly better scores (Koehn, 2004) are in cyan and marginal improvements are in lightcyan.",
      "startOffset" : 28,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Previous work (Artetxe and Schwenk, 2019) show that the encoder of a strong multilingual NMT system is an ideal model for the bilingual sentence retrieval task.",
      "startOffset" : 14,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "to the noises of extracted word pairs (Pan et al., 2021a) from word alignment toolkits that leads to insufficient supervision for improving sentence re-",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Sentence-level Contrastive Objective We conducted the experiments for sentence-level contrastive objective (mRASP2) (Pan et al., 2021b) on 626_en-tr-ro-et-my-kk mBART FT.",
      "startOffset" : 116,
      "endOffset" : 135
    } ],
    "year" : 0,
    "abstractText" : "Word alignment has proven to benefit many-tomany neural machine translation (NMT). However, high-quality ground-truth bilingual dictionaries were used for pre-editing in previous methods, which are unavailable for most language pairs. Meanwhile, the contrastive objective can implicitly utilize automatically learned word alignment, which has not been explored in many-to-many NMT. This work proposes a word-level contrastive objective to leverage word alignments for many-to-many NMT. Empirical results show that this leads to 0.8 BLEU gains for several language pairs. Analyses reveal that in many-to-many NMT, the encoder’s retrieval performance highly correlates with the translation quality, which explains when the proposed method impacts translation. This motivates future exploration for many-to-many NMT focusing on improving the encoder retrieval performance.",
    "creator" : null
  }
}
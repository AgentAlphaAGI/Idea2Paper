{
  "name" : "ARR_2022_218_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Empirical Study on Explanations in Out-of-Domain Settings",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "An explanation or rationale2, typically consists of a subset of the input that contributes more to the prediction. Extracting faithful explanations is important for studying model behavior (Adebayo et al., 2020) and assisting in tasks requiring human decision making, such as clinical text classification (Chakrabarty et al., 2019) and automatic fact-checking (Popat et al., 2018). A faithful explanation is one which accurately represents the\n1Code is attached to the submission and will be publicly released.\n2We use these terms interchangeably throughout our work.\nreasoning behind a model’s prediction (Jacovi and Goldberg, 2020)\nTwo popular methods for extracting explanations are through feature attribution approaches (i.e. posthoc explanation methods) or via inherently faithful classifiers (i.e. select-then-predict models). The first computes the contribution of different parts of the input with respect to a model’s prediction (Sundararajan et al., 2017; Ribeiro et al., 2016; Shrikumar et al., 2017). The latter consists of using a rationale extractor to identify the most important parts of the input and a rationale classifier, a model trained using as input only the extractor’s rationales (Bastings et al., 2019; Jain et al., 2020; Guerreiro and Martins, 2021).3 Figure 1 illustrates the two approaches with an example.\nCurrently, these explanation methods have been mostly evaluated on in-domain settings (i.e. the train and test data come from the same distribution). However, when deploying models in real-world applications, inference might be performed on data from a different distribution, i.e. out-of-domain (Desai and Durrett, 2020; Ovadia et al., 2019). This can create implications when extracted explanations (either using post-hoc methods or through select-then-predict models) are used for assisting human decision making. Whilst we are aware of the limitations of current state-of-the-art models in out-of-domain predictive performance (Hendrycks et al., 2020), to the best of our knowledge, how faithful out-of-domain post-hoc explanations are has yet to be explored. Similarly, we are not aware how inherently faithful select-then-predict models generalize in out-of-domain settings.\nInspired by this, we conduct an extensive empirical study to examine the faithfulness of five feature attribution approaches and the generaliz-\n3We refer to the rationale generator (i.e. generating a rationale mask) from Bastings et al. (2019) and Jain et al. (2020) as a rationale extractor, to avoid any confusion between these approaches and free-text rationales (Wiegreffe et al., 2021).\nability of two select-then-predict models in out-ofdomain settings across six dataset pairs. We hypothesize that similar to model predictive performance, post-hoc explanation faithfulness reduces in out-ofdomain settings and that select-then-predict performance degrades. Our contributions are as follows:\n• To the best of our knowledge, we are the first to assess the faithfulness of post-hoc explanations and performance of select-then-predict models in out-of-domain settings.\n• We show that post-hoc explanation sufficiency and comprehensiveness show misleading increases in out-of-domain settings. We argue that they should be evaluated alongside a random baseline as yardstick out-of-domain.\n• We demonstrate that select-then-predict classifiers can be used in out-of-domain settings. They lead to comparable predictive performance to models trained on full-text, whilst offering inherent faithfulness."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Rationale Extraction",
      "text" : "Given a model M, we are interested in explaining why M predicted ŷ for a particular instance x ∈ X. An extracted rationale R, should therefore represent as accurately as possible the most important subset of the input (R ∈ x) which contributed mostly towards the model’s prediction ŷ.\nCurrently, there are two popular approaches for extracting rationales. The first consists of using feature attribution methods that attribute to the input tokens an importance score (i.e. how important an input token is to a model’s M prediction ŷ).\nWe can then form a rationale R, by selecting the K most important tokens (independent or contiguous) as indicated by the feature attribution method. The second select-then-predict approach focuses on training inherently faithful classifiers by jointly training two modules, a rationale extractor and a rationale classifier, trained only on rationales produced by the extractor (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Guerreiro and Martins, 2021). Recent studies have used feature attribution approaches as part of the rationale extractor (Jain et al., 2020; Treviso and Martins, 2020), showing improved classifier predictive performance."
    }, {
      "heading" : "2.2 Evaluating Rationale Faithfulness",
      "text" : "Having extracted R, we need to evaluate the quality of the explanation (i.e. how faithful that explanation is for a model’s prediction). Typically, post-hoc explanations from feature attribution approaches are evaluated using input erasure (Serrano and Smith, 2019; Atanasova et al., 2020; Madsen et al., 2021). This approach masks segments of the input to observe if the model’s prediction changed. DeYoung et al. (2020) proposed measuring the comprehensiveness and sufficiency of rationales as faithfulness metrics. A comprehensive rationale is one which is influential to a model’s prediction, while a sufficient rationale that which is adequate for a model’s prediction (DeYoung et al., 2020). The term fidelity is also used for jointly referring to comprehensiveness and sufficiency (Carton et al., 2020). Carton et al. (2020) suggested normalizing these metrics using the predictions of the model with a baseline input (i.e. an all zero embedding vector), to account for baseline model behavior.\nSelect-then-predict models are inherently faithful, as their classification component is trained only on extracted rationales (Jain et al., 2020). A good measure for measuring rationale quality is by evaluating the predictive performance of the classifier trained only on the rationales (Jain et al., 2020; Treviso and Martins, 2020). A higher score entails that the extracted rationales are better when compared to those of a classifier with lower predictive performance."
    }, {
      "heading" : "2.3 Explainability in Out-of-Domain Settings",
      "text" : "Given model M trained on an end-task, we typically evaluate its out-of-domain predictive performance on a test-set that does not belong to the same distribution as the data it was trained on (Hendrycks et al., 2020). Similarly, the model can also extract explanations R for its out-of-domain predictions.\nCamburu et al. (2018) studied whether generating explanations for language inference match human annotations (i.e. plausible explanations). They showed that this is challenging in-domain and becomes more challenging in out-of-domain settings. In a similar direction, Rajani et al. (2019) and Kumar and Talukdar (2020) examined model generated explanations in out-of-domain settings and find that explanation plausibility degrades compared to in-domain. Kennedy et al. (2020) proposed a method for detecting model bias towards group identity terms using a post-hoc feature attribution approach. Then, they use them for regularizing models to improve out-of-domain predictive performance. Adebayo et al. (2020) have studied feature attribution approaches for identifying outof-distribution images. They find that importance allocation in out-of-domain settings is similar to that of an in-domain model and thus cannot be used to detect such images. Feder et al. (2021) finally argued that explanations can lead to errors in out-of-distribution settings, as they may latch onto spurious features from the training distribution.\nThese studies indicate that there is an increasing need for evaluating post-hoc explanation faithfulness and select-then-predict performance in out-ofdomain settings. To the best of our knowledge, we are the first to examine these."
    }, {
      "heading" : "3 Extracting Rationales",
      "text" : ""
    }, {
      "heading" : "3.1 Post-hoc Explanations",
      "text" : "We employ a pre-trained BERT-base and fine-tune it on in-domain training data. We then extract post-\nhoc rationales for both the in-domain test-set and two out-of-domain test-sets. We compute input importance using five feature scoring methods and a random baseline:\n• Random (RAND): Random allocation of token importance.\n• Attention (α): Token importance corresponding to normalized attention scores (Jain et al., 2020).\n• Scaled Attention (α∇α): Attention scores αi scaled by their corresponding gradients ∇αi = ∂ŷ∂αi (Serrano and Smith, 2019).\n• InputXGrad (x∇x): Attributes input importance by multiplying the input with its gradient computed with respect to the predicted class, where ∇xi = ∂ŷ∂xi (Kindermans et al., 2016; Atanasova et al., 2020).\n• Integrated Gradients (IG): Ranking words by computing the integral of the gradients taken along a straight path from a baseline input (zero embedding vector) to the original input (Sundararajan et al., 2017).\n• DeepLift: Ranking words according to the difference between the activation of each neuron and a reference activation (zero embedding vector) (Shrikumar et al., 2017)."
    }, {
      "heading" : "3.2 Select-then-Predict Models",
      "text" : "We use two select-then-predict models:\n• HardKuma: An end-to-end trained model, where the rationale extractor uses Hard Kumaraswamy variables to produce a rationale mask z, which the classifier uses to mask the input (Bastings et al., 2019). Model training takes advantage of reparameterized gradients compared to REINFORCE style training employed by Lei et al. (2016) and has shown improved performance (Guerreiro and Martins, 2021).\n• FRESH: We compute the predictive performance of a classifier trained on rationales extracted with feature attribution metrics (see §3.1) using FRESH, following a similar approach to Jain et al. (2020). We extract rationales from an extractor by (1) selecting the top-k most important tokens (TOPK) and (2) selecting the span of length k with the highest overall importance (CONTIGUOUS).\nWe use BERT-base for the extraction and classification components of FRESH similar to Jain et al. (2020). However, for HardKuma we opt using a bi-LSTM (Hochreiter and Schmidhuber, 1997) as it provides comparable or improved performance over BERT variants (Guerreiro and Martins, 2021), even after hyperparameter tuning.4"
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "For evaluating out-of-domain model explanation, we consider the following datasets (see Table 1 and Appendix A for details):\nSST: Stanford Sentiment Treebank (SST) consists of sentences tagged with sentiment on a 5- point-scale from negative to positive (Socher et al., 2013). We remove sentences with neutral sentiment and label the remaining sentences as negative or positive if they have a score lower or higher than 3 respectively (Jain and Wallace, 2019).\nIMDB: The Large Movie Reviews Corpus consists of movie reviews labeled either as positive or negative (Maas et al., 2011; Jain and Wallace, 2019).\nYelp: Yelp polarity review texts. Similar to Zhang et al. (2015) we construct a binary classification task to predict a polarity label by considering one and two stars as negative, and three and four stars as positive.\nAmazon Reviews: We form 3-way classification tasks by predicting the sentiment (negative, neutral, positive) of Amazon product reviews across 3 item categories: (1) Digital Music (AmazDigiMu); (2) Pantry (AmazPantry); and (3) Musical Instruments (AmazInstr) (Ni et al., 2019).\n4See model details and hyper-parameters in Appendix B"
    }, {
      "heading" : "4.2 Evaluating Out-of-Domain Explanations",
      "text" : "Post-hoc Explanations: We evaluate post-hoc explanations using:\n• Normalized Sufficiency (NormSuff) measures the degree to which the extracted rationales are adequate for a model to make a prediction (DeYoung et al., 2020). Following Carton et al. (2020), we bind sufficiency between 0 and 1 and use the reverse difference so that higher is better:\nSuff(x, ŷ,R) = 1−max(0, p(ŷ|x)− p(ŷ|R))\nNormSuff(x, ŷ,R) = Suff(x, ŷ,R)− Suff(x, ŷ, 0) 1− Suff(x, ŷ, 0)\n(1)\nwhere Suff(x, ŷ, 0) is the sufficiency of a baseline input (zeroed out sequence) and ŷ the model predicted class using the full text x as input.\n• Normalized Comprehensiveness (NormComp) measures the influence of a rationale to a prediction (DeYoung et al., 2020). For an explanation to be highly comprehensive, the model’s prediction after masking the rationale should have a large difference compared to the model’s prediction using the full text. Similarly to Carton et al. (2020), we bind this metric between 0 and 1 and normalize it:\nComp(x, ŷ,R) = max(0, p(ŷ|x)− p(ŷ|x\\R))\nNormComp(x, ŷ,R) = Comp(x, ŷ,R) 1− Suff(x, ŷ, 0) (2)\nTo measure sufficiency and comprehensiveness across different explanation lengths we compute the “Area Over the Perturbation Curve\" (AOPC) following DeYoung et al. (2020). We therefore compute and report the average normalized sufficiency and comprehensiveness scores when keeping (for sufficiency) or masking (for comprehensiveness) the top 2%, 10%, 20% and 50% of tokens extracted by an importance attribution function.5\nWe omit from our evaluation the Remove-andRetrain method (Madsen et al., 2021) as it requires model retraining. Whilst this could be applicable for in-domain experiments where retraining is important, in this work we evaluate explanation faithfulness in zero-shot out-of-domain settings.\n5We also present results for each of these rationale lengths in Appendix F.\nSelect-then-Predict Models: We first train select-then-predict models in-domain and then measure their predictive performance on the in-domain test set and on two out-of-domain test-sets (Jain et al., 2020; Guerreiro and Martins, 2021). Our out-of-domain evaluation is performed without retraining (zero-shot). Similar to full-text trained models, we expect that predictive performance deteriorates out-of-domain. However, we assume that explanations from a select-then-predict model should generalize better in out-of-domain settings when the predictive performance approaches that of the full-text trained model.\nWe do not conduct human experiments to evaluate explanation faithfulness, since that is only relevant to explanation plausibility (i.e. how intuitive to humans a rationale is (Jacovi and Goldberg, 2020)) and in practice faithfulness and plausibility do not correlate (Atanasova et al., 2020)."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Post-hoc Explanation Faithfulness",
      "text" : "Table 2 presents the normalized comprehensiveness and sufficiency scores for post-hoc explanations on in-domain and out-of-domain test-sets, using five feature attribution methods and a random baseline. For reference, we include the averaged F1 performance across 5 random seeds, of a BERTbase model finetuned on the full text and evaluated\nin- and out-of-domain (Full-text F1).6\nIn-domain results show that feature attribution performance varies largely across datasets. This is in line with the findings of Atanasova et al. (2020) and Madsen et al. (2021) when masking rationales (i.e. comprehensiveness). We find the only exception to be α∇α, which consistently achieves the highest comprehensiveness and sufficiency scores across all in-domain datasets. For example α∇α evaluated on in-domain AmazDigiMu, results in sufficiency of 0.56 compared to the second best of 0.39 with IG.\nContrary to our expectations, results show that post-hoc explanation sufficiency and comprehensiveness are in many cases higher in out-of-domain test-sets compared to in-domain. For example using DeepLift, comprehensiveness for the in-domain test-set in Yelp (0.16) is lower compared to the out-of-domain test-sets (0.21 for SST and 0.23 for IMDB). This is also observed when measuring sufficiency with α∇α, scoring 0.32 when tested indomain on Yelp and 0.45 for the out-of-domain SST test-set.\nApart from increased sufficiency and comprehensiveness scores in out-of-domain post-hoc explanations, we also observe increased scores obtained by our random baseline. In fact, the random baseline outperforms several feature attribution approaches in certain cases in out-of-domain settings . As an\n6We report predictive performance for all models and standard deviations in the Appendix.\nexample, consider the case where the model has been trained on AmazInstr and tested on AmazPantry. Our random baseline achieves a comprehensiveness score of 0.27 while α, DeepLift, x∇x perform similarly or lower (0.22, 0.25 and 0.27 respectively). Similarly, using a model trained on Yelp and tested on SST, the random baseline produces equally sufficient rationales to x∇x and IG, with all of them achieving 0.41 normalized sufficiency. A glaring exception to this pattern is α∇α, which consistently outperforms both the random baseline and all other feature attribution approaches in in- and out-of-domain settings, suggesting that it produces the more faithful explanations. For example with out-of-domain AmazPantry test data, using a model trained on AmazInstr results in sufficiency scores of 0.39 with α∇α. This is a 0.15 point increase compared to the second best (x∇x with 0.24).\nWe recommend considering a feature attribution for producing faithful explanations out-of-domain, if it only scores above a baseline random attribution. We suggest that the higher the deviation from the random baseline, the more faithful an explanation is."
    }, {
      "heading" : "5.2 Select-then-predict Model Performance",
      "text" : "HardKuma: Table 3 presents the F1-macro performance of HardKuma models (Bastings et al., 2019) and the average rationale lengths (the ratio of the selected tokens compared to the length of the entire sequence) selected by the model. For reference, we also include the predictive performance of a full-text trained bi-LSTM. Results are averaged across 5 runs including standard deviations in brackets.\nAs expected, predictive performance of HardKuma models degrades when evaluated on outof-domain data. Surprisingly, though, we find that their performance is not significantly different (t-test; p-value > 0.05) to that of the full-text LSTM in 9 out of the 12 out-of-domain dataset pairs. For example, by evaluating the out-ofdomain performance of a HardKuma model trained on AmazDigiMu on the AmazPantry test-set, we record on average a score of 54.3 F1 compared to 55.3 with an LSTM classifier trained on full text. We also observe that HardKuma models trained on SST and IMDB generalize comparably to models trained on full-text when evaluated on Yelp, however the opposite does not apply. Our assumption is that HardKuma models trained on Yelp, learn more domain-specific information due to the large training corpus (when compared to training on IMDB and SST) so they fail to generalize well out-ofdomain.\nResults also show, that the length of rationales selected by HardKuma models depend on the source domain, i.e. training HardKuma on a dataset which favors shorter rationales, leads to also selecting shorter rationales out-ofdomain. For example, in-domain test-set explanation lengths are on average 56.8% of the fulltext input length for SST. In comparison, training a model on Yelp and evaluating on SST results in rationale lengths of 14.1%. We observe that in certain cases, HardKuma models maintain the number of words, not the ratio to the sequence in out-of-domain settings. For example, in-domain Yelp test-set rationales are about 11 tokens long that is the similar to the length selected when evaluating on IMDB using a model trained on Yelp. This is also observed where in-domain AmazInstr testset rationales are on average 5 tokens long, which is the same rationale length when evaluating on AmazDigiMu using a model trained on AmazInstr.\nIn general, our findings show that in the majority\nof cases, using HardKuma in out-of-domain data results to comparable performance with their full-text model counterparts. This suggests that HardKuma models can be used in out-of-domain settings, without significant sacrifices in predictive performance whilst also offering faithful rationales.\nFRESH: Table 4 shows the averaged F1-macro performance across 5 random seeds for FRESH classifiers on in- and out-of-domain using TopK rationales.7. We also include the a priori defined rationale length in parentheses and the predictive performance of the Full-Text model for reference.8\nWe first observe that in-domain predictive performance varies across feature attribution approaches with attention-based metrics (α∇α, α) outperforming the gradient-based ones (x∇x, IG), largely agreeing with Jain et al. (2020). We also find that α∇α and DeepLift are the feature attribution approaches that lead to the highest predictive performance across all datasets.\nAs we initially hypothesized, performance of FRESH generally degrades when testing on out-ofdomain data similarly to the behavior of models trained using the full text. The only exceptions are when using x∇x and IG in IMDB. We argue that this is due to these feature attribution meth-\n7For clarity we include standard deviations and Contiguous results in Appendix D\n8When evaluating out-of-domain, we use the rationale length of the dataset we evaluate on. This makes FRESH experiments comparable with those of HardKuma.\nods not being able to identify the appropriate tokens relevant to the task using a rationale length 2% of the original input. Increasing the rationale length to 20% (SST) and 10% (Yelp) also increases the performance. Results also suggest that α∇α and DeepLift outperform the rest of the feature attributions, with α∇α being the best performing one in the majority of cases. In fact when using α∇α or DeepLift, the out-of-domain performance of FRESH is not significantly different to that of models trained on full text (t-test; p-value > 0.05) in 5 cases. For example, a FRESH model trained on AmazPantry and evaluated on AmazInstr records 63.6 F1 macro (using DeepLift) compared to 64.5 obtained by a full-text model. However, this does not apply to the other feature attribution methods (α; x∇x; IG).\nTo better understand this behavior, we conduct a correlation analysis between the importance rankings using any single feature attribution from (1) a model trained on the same domain with the evaluation data; and (2) a model trained on a different domain (out-of-domain trained model). High correlations suggest that if a feature attribution from an out-of-domain trained model produces similar importance distributions with that of an in-domain model, it will also lead to high predictive performance out-of-domain. Contrary to our initial assumption we found that the lower the correlation, the higher the predictive performance with FRESH. Results show low correlations when us-\ning α∇α and DeepLift (highest FRESH performance). Surprisingly, IG and x∇x (lowest FRESH performance) showed consistently strong correlations across all dataset pairs. Thus, we conclude that lower correlation scores indicate lower attachment to spurious correlations learned during training. We expand our discussion and show results for the correlation analysis in Appendix E.\nOur findings therefore suggest that using FRESH in out-of-domain settings, can result to comparable performance with a model trained on full-text. However this highly depends on the choice of the feature attribution method.\nHardKuma vs. FRESH: We observe that HardKuma models are not significantly different compared to models trained on the full text in out-ofdomain settings in more cases, when compared to FRESH (9 out of 12 and 5 out of 12 respectively). However, FRESH with α∇α or DeepLift records higher predictive performance compared to HardKuma models (both in- and out-of-domain) in all cases. We attribute this to the underlying model architectures, as FRESH uses BERT and HardKuma a bi-LSTM. As we discussed in §3.2, we attempted using BERT for HardKuma models in the extractor and classifier similar to Jain et al. (2020). However, the performance of HardKuma with BERT is at most comparable to when using a bi-LSTM similar to findings of Guerreiro and Martins (2021)."
    }, {
      "heading" : "5.3 Correlation between Post-hoc Explanation Faithfulness and FRESH Performance",
      "text" : "We hypothesize that a feature attribution with high scores for sufficiency and comprehensiveness, should extract rationales that result in high FRESH predictive performance. We expect that if our hypothesis is valid, faithfulness scores can serve as early indicators of FRESH performance, both on in-domain and out-of-domain settings.\nTable 5 shows the Spearman’s ranking correlation (ρ) between FRESH F1 performance (see Table 4) and comprehensiveness and sufficiency (see Table 2). Correlation is computed using all feature scoring methods for each dataset pair. Results show that only 4 cases achieve statistically significant correlations (p-value < 0.05) with only 3 out-of-domain and mostly between sufficiency and FRESH performance. We do not observe high correlations with comprehensiveness which is expected, as comprehensiveness evaluated the rationale’s influence towards a model’s prediction.\nOur findings refute our initial hypothesis and suggest that there is no clear correlation across all cases, between post-hoc explanation faithfulness and FRESH predictive performance. Therefore, sufficiency and comprehensiveness scores cannot be used as early indicators of FRESH predictive performance."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We conducted an extensive empirical study to assess the faithfulness of post-hoc explanations (i.e. using feature attribution approaches) and performance of select-then-predict (i.e. inherently faithful) models in out-of-domain settings. Our findings highlight, that using sufficiency and comprehensiveness to evaluate post-hoc explanation faithfulness out-of-domain can be misleading. To address this issue, we suggest comparing faithfulness of post-hoc explanations to a random attribution baseline for a more robust evaluation. We also show that select-then-predict models, which are inherently faithful, perform surprisingly well in out-ofdomain settings. Despite performance degradation, in many cases their performance is comparable to those of full-text trained models. In future work, we aim to explore methods for improving the evaluation of faithfulness for out-of-domain post-hoc explanations."
    }, {
      "heading" : "A Dataset Characteristics",
      "text" : "Table 6 presents extended data characteristics for all datasets. We present information across the three data splits, including: (1) The average sequence length; (2) The number of documents in each split and (3) the number of documents under each label.\nOur dataset selection was highly motivated for also examining the differences when we have gradual shifts in domain. For example for the triplet SST - IMDB - YELP, two datasets are closely associated (SST, IMDB) as they are movie reviews, whilst Yelp is a task for classifying restaurant reviews. Similarly, AmazDigiMu and AmazInstr share similar characteristics, as they are reviews about items related to music. On the contrary, AmazPantry consists of reviews about pantry items. This is also the primary reason why we focused on text classification tasks, as it is easier to control for the output and other parameters, whilst allowing for control over the task it-self."
    }, {
      "heading" : "B Models and Hyper-parameters",
      "text" : "For feature attributions: We use BERT-base with pre-trained weights from the Huggingface library (Wolf et al., 2020). We use the AdamW optimizer (Loshchilov and Hutter, 2017) with an initial learning rate of 1e − 5 for fine-tuning BERT and 1e− 4 for the fully-connected classification layer. We train our models for 3 epochs using a linear scheduler with 10% of the data in the first epoch as warm-up. We also use a grad-norm of 1 and select the model with the lowest loss on the development set. All models are trained across 5 random seeds and we report the average and standard deviation. We present their test-set performance in Table 7 and their development set performance in Table 8.\nFor FRESH: For the rationale extractor, we use the same model for extracting rationales from feature attributions. For the classifier (trained only on the extracted rationales), we also use BERT-base with the same optimizer configuration and scheduler warm-up steps. We also use a grad-norm of 1 and select the model with the lowest loss on the development set. We train across 5 random seeds for 5 epochs.\nIn Table 7 we present full-text BERT-base F1macro scores averaged across 5 random seeds with standard deviations included in the brackets. Additionally, we present the mean Expected Calibration Error (ECE) scores. Finally, in Table 8 we present the in-domain F1-macro performance and loss on the development set.\nFor HardKuma: We use the 300-dimensional pre-trained GloVe embeddings from the 840B release (Pennington et al., 2014) as word representations and keep them frozen during training. The rationale extractor (which generates the rationale mask z) is a 200-d bi-directional LSTM layer (biLSTM) (Hochreiter and Schmidhuber, 1997) similar to (Bastings et al., 2019; Guerreiro and Martins, 2021). We use the Adam optimizer (Kingma and Ba, 2014) for all models with a learning rate between 1e − 5 and 1e − 4 and a weight decay of 1e− 5. We also enforce a grad-norm of 5 and train for 20 epochs across 5 random seeds. Similar to Guerreiro and Martins (2021) we select the model with the highest F1-macro score on the development set and find that tuning the Lagrangian relaxation algorithm parameters beneficial to model predictive performance. We also attempted training HardKuma models with BERT-base, similar to Jain\net al. (2020), however we found performance to be at best comparable with our LSTM variant, as in Guerreiro and Martins (2021), even after hyperparameter tuning. All experiments are run on a single NVIDIA Tesla V100 GPU."
    }, {
      "heading" : "C HardKuma - Extended",
      "text" : "In Table 9 we present for reference the performance of a 200-dimensional bi-LSTM classifier trained on full-text. We train the full-text LSTM for 20 epochs across 5 random seeds and select the model with the highest F1-macro performance on the development set. We use the Adam optimizer with a learning rate of 1e − 3 and 1e − 5 weight decay. We report predictive performance and ECE scores on the test-set. In Table 10 we include HardKuma performance with standard deviations, and expected calibration error (ECE), across five runs."
    }, {
      "heading" : "D FRESH - Extended",
      "text" : "Tables 11 and 12 presents FRESH F1 macro performance and Expected Calibration Error (ECE) for classifiers trained on TopK and Contiguous rationales respectively, with standard deviation in brackets. We include the a priori defined rationale length in the brackets (.%) and for reference, the ID performance of the Full-Text model (as also seen in Table 7).\nComparing with FRESH performance with Contiguous rationales rather than TopK (see Table 11), we first observe that performance degrades for most feature attribution methods. These findings are largely in agreement with those of Jain et al. (2020). However, x∇x and IG, which perform poorly with TopK, record surprisingly better scores with Contiguous type rationales. For example, in-domain performance with IG becomes comparable with α∇α in in-domain IMDB (83.2 with α∇α and 82.5 with IG). This is in sharp contrast with TopK, where IG recorded an F1 score of only 59.7, compared to 87.9 of α∇α.\nThese findings also hold in out-of-domain settings, where α∇α, α and DeepLift result in poorer FRESH performance with Contiguous type rationales, compared to TopK. However, IG and in many cases x∇x improves. For example with TopK rationales, evaluating on Yelp using IG from a model trained on IMDB, results on an F1-score of 69.1.\nOn the contrary, with Contiguous rationales and the same set-up, IG results in FRESH performance of 87.0.\nOur findings lead us to assume that, the rationale type has a large impact on FRESH performance, both in-domain and on out-of-domain settings. Certain feature attribution methods benefit from one type of rationales (e.g. DeepLift with TopK), whilst others from another (e.g. IG with Contiguous)."
    }, {
      "heading" : "E Extended Analysis",
      "text" : "E.1 Correlation of Rankings\nWe examine why x∇x and IG, do not perform as well as DeepLift and α∇α when using FRESH. We therefore conduct a study to gain better understand this. We first fix the domain of the data we evaluate on and then compute the correlation between importance rankings using any single feature attribution from: (1) a model trained on the same domain with the evaluation data and (2) a model from trained\non a different distribution (out-of-domain trained model). High correlations suggest that a feature attribution from an out-of-domain trained model, produce similar importance distributions with that of an in-domain model (i.e. both attend to similar tokens to make a prediction). Therefore, we assume that this will lead to high predictive performance out-of-domain. In Figure 2 we show Spearman’s ranking correlation across dataset pairs, between a model trained on the same distribution as the evaluation data (ID) and an out-of-domain trained model (OOD), such that (ID <-> OOD).\nAs expected, the random baseline produced almost no correlation between models. An interesting observation is that two of the gradient-based\nmethods (x∇x and IG) produce strongly correlated rankings. This suggests that these two metrics produce generalizable rankings irrespective of the domain shift, when comparing to the remainder of the feature attribution approaches. Surprisingly, Deeplift importance rankings exhibit almost low to no correlation betweenen them, despite being also gradient-based. We hypothesize that this happens because DeepLift considers a baseline input to compute its importance distribution, which highly depends on the model and as such is de-facto normalized and perhaps generalizes better.\nα for out-of-domain detection?: An interesting case is that of α, where we observe moderate to\nstrong correlations across all test-cases. What is more evident, is that in the OOD tuples we considered, it appears that stronger correlations appear where the OOD task and the ID task are closer together. For example in the case of SST and IMDB (both sentiment analysis tasks for movie reviews), α produces a strong correlation (0.68). This contrasts the moderate correlation of 0.58 between SST and Yelp, which is for restaurant reviews. This is also evident in the case of AmazDigiMu and AmazInstr, where both tasks are for review classification, but for musical related purchases. They both score strong correlations between them and moderate correlations with reviews for pantry purchases (AmazPantry). This observation might suggest, that using these correlation metrics with α might be an indicator of the degree of task-domainshift. Our observation is also supported by the findings of Adebayo et al. (2020), who show that feature attributions are good indicators of detecting spurious correlation signals in computer vision tasks.Considering α∇α we observe a wide range of correlations, ranging from low in the AmazInstrAmazDigiMu pair to strong in the AmazPantryAmazInstr pair, which we cannot interpret as something meaningful. Correlation values and FRESH: We first observe that the lowest correlated feature attributions α∇α and DeepLift perform the better on FRESH, followed by α which displays moderate correlations and at the end of the spectrum the two gradient-based methods which display high correlations. Contrary to our initial assumption, this suggests that the attributions which generalize better (i.e. return rationales that result in higher FRESH performance) are those which exhibit low to no correlations. Agreement at different rationale lengths: As the correlation analysis considers the entire length of the sequence, we now examine a scenario where we have a priori defined rationale lengths. Similarly\nto the correlation analysis, we now compute the agreement in tokens between ID feature attribution rankings to those of an OOD trained model. In Tables 13, 14 and 15 we therefore show the token agreement between in-domain and out-of-domain post-hoc explanations (on the same data) for 2%, 10% and 20% rationale lengths.\nOur findings show that across all rationale lengths, results largely agree with the correlation analysis. The two gradient-based methods exhibit higher agreement than the remainder, with α∇α and DeepLift recording the lowest agreements. Surprisingly, the poorest performers on out-of-domain FRESH record the highest agreement in tokens with in-domain models. Whilst this suggests that they generalize better, we believe that the inhibiting factor to their performance is their limited indomain capabilities (i.e. they record the lowest in-domain FRESH performance with TopK)."
    }, {
      "heading" : "F Post-hoc Explanation Faithfulness - Extended",
      "text" : "In Tables 16, 17 and 18, we present post-hoc explanation sufficiency and comprehensiveness scores at 2%, 10% and 20% rationale lengths."
    } ],
    "references" : [ {
      "title" : "Debugging tests for model explanations",
      "author" : [ "Julius Adebayo", "Michael Muelly", "Ilaria Liccardi", "Been Kim." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 700–712. Curran Associates, Inc.",
      "citeRegEx" : "Adebayo et al\\.,? 2020",
      "shortCiteRegEx" : "Adebayo et al\\.",
      "year" : 2020
    }, {
      "title" : "A diagnostic study of explainability techniques for text classification",
      "author" : [ "Pepa Atanasova", "Jakob Grue Simonsen", "Christina Lioma", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Atanasova et al\\.,? 2020",
      "shortCiteRegEx" : "Atanasova et al\\.",
      "year" : 2020
    }, {
      "title" : "Interpretable neural predictions with differentiable binary variables",
      "author" : [ "Jasmijn Bastings", "Wilker Aziz", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2963–2977, Florence, Italy. Associa-",
      "citeRegEx" : "Bastings et al\\.,? 2019",
      "shortCiteRegEx" : "Bastings et al\\.",
      "year" : 2019
    }, {
      "title" : "e-snli: Natural language inference with natural language explanations",
      "author" : [ "Oana-Maria Camburu", "Tim Rocktäschel", "Thomas Lukasiewicz", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.",
      "citeRegEx" : "Camburu et al\\.,? 2018",
      "shortCiteRegEx" : "Camburu et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating and characterizing human rationales",
      "author" : [ "Samuel Carton", "Anirudh Rathore", "Chenhao Tan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9294–9307, Online. Association for",
      "citeRegEx" : "Carton et al\\.,? 2020",
      "shortCiteRegEx" : "Carton et al\\.",
      "year" : 2020
    }, {
      "title" : "Pay “attention” to your context when classifying abusive language",
      "author" : [ "Tuhin Chakrabarty", "Kilol Gupta", "Smaranda Muresan." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 70–79, Florence, Italy. Association for Computational Lin-",
      "citeRegEx" : "Chakrabarty et al\\.,? 2019",
      "shortCiteRegEx" : "Chakrabarty et al\\.",
      "year" : 2019
    }, {
      "title" : "Calibration of pre-trained transformers",
      "author" : [ "Shrey Desai", "Greg Durrett." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 295–302, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Desai and Durrett.,? 2020",
      "shortCiteRegEx" : "Desai and Durrett.",
      "year" : 2020
    }, {
      "title" : "ERASER: A benchmark to evaluate rationalized NLP models",
      "author" : [ "Jay DeYoung", "Sarthak Jain", "Nazneen Fatema Rajani", "Eric Lehman", "Caiming Xiong", "Richard Socher", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "DeYoung et al\\.,? 2020",
      "shortCiteRegEx" : "DeYoung et al\\.",
      "year" : 2020
    }, {
      "title" : "Causal inference in natural language processing: Estimation",
      "author" : [ "Amir Feder", "Katherine A Keith", "Emaad Manzoor", "Reid Pryzant", "Dhanya Sridhar", "Zach Wood-Doughty", "Jacob Eisenstein", "Justin Grimmer", "Roi Reichart", "Margaret E Roberts" ],
      "venue" : null,
      "citeRegEx" : "Feder et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Feder et al\\.",
      "year" : 2021
    }, {
      "title" : "Spectra: Sparse structured text rationalization",
      "author" : [ "Nuno Miguel Guerreiro", "André FT Martins." ],
      "venue" : "arXiv preprint arXiv:2109.04552.",
      "citeRegEx" : "Guerreiro and Martins.,? 2021",
      "shortCiteRegEx" : "Guerreiro and Martins.",
      "year" : 2021
    }, {
      "title" : "Pretrained transformers improve out-of-distribution robustness",
      "author" : [ "Dan Hendrycks", "Xiaoyuan Liu", "Eric Wallace", "Adam Dziedzic", "Rishabh Krishnan", "Dawn Song." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hendrycks et al\\.,? 2020",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2020
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness",
      "author" : [ "Alon Jacovi", "Yoav Goldberg" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jacovi and Goldberg.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jacovi and Goldberg.",
      "year" : 2020
    }, {
      "title" : "Attention is not Explanation",
      "author" : [ "Sarthak Jain", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "Learning to faithfully rationalize by construction",
      "author" : [ "Sarthak Jain", "Sarah Wiegreffe", "Yuval Pinter", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4459–4473, Online. Association",
      "citeRegEx" : "Jain et al\\.,? 2020",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2020
    }, {
      "title" : "Contextualizing hate speech classifiers with post-hoc explanation",
      "author" : [ "Brendan Kennedy", "Xisen Jin", "Aida Mostafazadeh Davani", "Morteza Dehghani", "Xiang Ren." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kennedy et al\\.,? 2020",
      "shortCiteRegEx" : "Kennedy et al\\.",
      "year" : 2020
    }, {
      "title" : "Investigating the influence of noise and distractors on the interpretation of neural networks",
      "author" : [ "Pieter-Jan Kindermans", "Kristof Schütt", "Klaus-Robert Müller", "Sven Dähne." ],
      "venue" : "arXiv preprint arXiv:1611.07270.",
      "citeRegEx" : "Kindermans et al\\.,? 2016",
      "shortCiteRegEx" : "Kindermans et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "NILE : Natural language inference with faithful natural language explanations",
      "author" : [ "Sawan Kumar", "Partha Talukdar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8730–8742, Online. Association for",
      "citeRegEx" : "Kumar and Talukdar.,? 2020",
      "shortCiteRegEx" : "Kumar and Talukdar.",
      "year" : 2020
    }, {
      "title" : "Rationalizing neural predictions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107–117, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Lei et al\\.,? 2016",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2016
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "arXiv preprint arXiv:1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Evaluating the faithfulness of importance measures in nlp by recursively masking allegedly important tokens and retraining",
      "author" : [ "Andreas Madsen", "Nicholas Meade", "Vaibhav Adlakha", "Siva Reddy" ],
      "venue" : null,
      "citeRegEx" : "Madsen et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Madsen et al\\.",
      "year" : 2021
    }, {
      "title" : "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
      "author" : [ "Jianmo Ni", "Jiacheng Li", "Julian McAuley." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Ni et al\\.,? 2019",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2019
    }, {
      "title" : "Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift",
      "author" : [ "Yaniv Ovadia", "Emily Fertig", "Jie Ren", "Zachary Nado", "D. Sculley", "Sebastian Nowozin", "Joshua Dillon", "Balaji Lakshminarayanan", "Jasper Snoek." ],
      "venue" : "Advances in",
      "citeRegEx" : "Ovadia et al\\.,? 2019",
      "shortCiteRegEx" : "Ovadia et al\\.",
      "year" : 2019
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "DeClarE: Debunking fake news and false claims using evidence-aware deep learning",
      "author" : [ "Kashyap Popat", "Subhabrata Mukherjee", "Andrew Yates", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Popat et al\\.,? 2018",
      "shortCiteRegEx" : "Popat et al\\.",
      "year" : 2018
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "why should I trust you?\": Explaining the predictions of any classifier",
      "author" : [ "Marco Túlio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Fran-",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931–2951, Florence, Italy",
      "author" : [ "Sofia Serrano", "Noah A. Smith." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Serrano and Smith.,? 2019",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Learning important features through propagating activation differences",
      "author" : [ "Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning",
      "citeRegEx" : "Shrikumar et al\\.,? 2017",
      "shortCiteRegEx" : "Shrikumar et al\\.",
      "year" : 2017
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on Empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "The explanation game: Towards prediction explainability through sparse communication",
      "author" : [ "Marcos Treviso", "André F.T. Martins." ],
      "venue" : "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 107–118,",
      "citeRegEx" : "Treviso and Martins.,? 2020",
      "shortCiteRegEx" : "Treviso and Martins.",
      "year" : 2020
    }, {
      "title" : "Measuring association between labels and free-text rationales",
      "author" : [ "Sarah Wiegreffe", "Ana Marasović", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10266–10284, Online and Punta",
      "citeRegEx" : "Wiegreffe et al\\.,? 2021",
      "shortCiteRegEx" : "Wiegreffe et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc. 10",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "2021) we select the model with the highest F1-macro score on the development set and find that tuning the Lagrangian relaxation algorithm parameters beneficial to model predictive performance",
      "author" : [ "Guerreiro", "Martins" ],
      "venue" : null,
      "citeRegEx" : "Guerreiro and Martins,? \\Q2021\\E",
      "shortCiteRegEx" : "Guerreiro and Martins",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Extracting faithful explanations is important for studying model behavior (Adebayo et al., 2020) and assisting in tasks requiring human decision making, such as clinical text classification (Chakrabarty et al.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : ", 2020) and assisting in tasks requiring human decision making, such as clinical text classification (Chakrabarty et al., 2019) and automatic fact-checking (Popat et al.",
      "startOffset" : 101,
      "endOffset" : 127
    }, {
      "referenceID" : 26,
      "context" : ", 2019) and automatic fact-checking (Popat et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "reasoning behind a model’s prediction (Jacovi and Goldberg, 2020) Two popular methods for extracting explanations are through feature attribution approaches (i.",
      "startOffset" : 38,
      "endOffset" : 65
    }, {
      "referenceID" : 32,
      "context" : "The first computes the contribution of different parts of the input with respect to a model’s prediction (Sundararajan et al., 2017; Ribeiro et al., 2016; Shrikumar et al., 2017).",
      "startOffset" : 105,
      "endOffset" : 178
    }, {
      "referenceID" : 28,
      "context" : "The first computes the contribution of different parts of the input with respect to a model’s prediction (Sundararajan et al., 2017; Ribeiro et al., 2016; Shrikumar et al., 2017).",
      "startOffset" : 105,
      "endOffset" : 178
    }, {
      "referenceID" : 30,
      "context" : "The first computes the contribution of different parts of the input with respect to a model’s prediction (Sundararajan et al., 2017; Ribeiro et al., 2016; Shrikumar et al., 2017).",
      "startOffset" : 105,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "The latter consists of using a rationale extractor to identify the most important parts of the input and a rationale classifier, a model trained using as input only the extractor’s rationales (Bastings et al., 2019; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 192,
      "endOffset" : 263
    }, {
      "referenceID" : 14,
      "context" : "The latter consists of using a rationale extractor to identify the most important parts of the input and a rationale classifier, a model trained using as input only the extractor’s rationales (Bastings et al., 2019; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 192,
      "endOffset" : 263
    }, {
      "referenceID" : 9,
      "context" : "The latter consists of using a rationale extractor to identify the most important parts of the input and a rationale classifier, a model trained using as input only the extractor’s rationales (Bastings et al., 2019; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 192,
      "endOffset" : 263
    }, {
      "referenceID" : 10,
      "context" : "Whilst we are aware of the limitations of current state-of-the-art models in out-of-domain predictive performance (Hendrycks et al., 2020), to the best of our knowledge, how faithful out-of-domain post-hoc explanations are has yet to be explored.",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "(2020) as a rationale extractor, to avoid any confusion between these approaches and free-text rationales (Wiegreffe et al., 2021).",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 19,
      "context" : "The second select-then-predict approach focuses on training inherently faithful classifiers by jointly training two modules, a rationale extractor and a rationale classifier, trained only on rationales produced by the extractor (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 228,
      "endOffset" : 344
    }, {
      "referenceID" : 2,
      "context" : "The second select-then-predict approach focuses on training inherently faithful classifiers by jointly training two modules, a rationale extractor and a rationale classifier, trained only on rationales produced by the extractor (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 228,
      "endOffset" : 344
    }, {
      "referenceID" : 33,
      "context" : "The second select-then-predict approach focuses on training inherently faithful classifiers by jointly training two modules, a rationale extractor and a rationale classifier, trained only on rationales produced by the extractor (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 228,
      "endOffset" : 344
    }, {
      "referenceID" : 14,
      "context" : "The second select-then-predict approach focuses on training inherently faithful classifiers by jointly training two modules, a rationale extractor and a rationale classifier, trained only on rationales produced by the extractor (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 228,
      "endOffset" : 344
    }, {
      "referenceID" : 9,
      "context" : "The second select-then-predict approach focuses on training inherently faithful classifiers by jointly training two modules, a rationale extractor and a rationale classifier, trained only on rationales produced by the extractor (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 228,
      "endOffset" : 344
    }, {
      "referenceID" : 14,
      "context" : "Recent studies have used feature attribution approaches as part of the rationale extractor (Jain et al., 2020; Treviso and Martins, 2020), showing improved classifier predictive performance.",
      "startOffset" : 91,
      "endOffset" : 137
    }, {
      "referenceID" : 33,
      "context" : "Recent studies have used feature attribution approaches as part of the rationale extractor (Jain et al., 2020; Treviso and Martins, 2020), showing improved classifier predictive performance.",
      "startOffset" : 91,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : "Typically, post-hoc explanations from feature attribution approaches are evaluated using input erasure (Serrano and Smith, 2019; Atanasova et al., 2020; Madsen et al., 2021).",
      "startOffset" : 103,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "Typically, post-hoc explanations from feature attribution approaches are evaluated using input erasure (Serrano and Smith, 2019; Atanasova et al., 2020; Madsen et al., 2021).",
      "startOffset" : 103,
      "endOffset" : 173
    }, {
      "referenceID" : 22,
      "context" : "Typically, post-hoc explanations from feature attribution approaches are evaluated using input erasure (Serrano and Smith, 2019; Atanasova et al., 2020; Madsen et al., 2021).",
      "startOffset" : 103,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "A comprehensive rationale is one which is influential to a model’s prediction, while a sufficient rationale that which is adequate for a model’s prediction (DeYoung et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : "The term fidelity is also used for jointly referring to comprehensiveness and sufficiency (Carton et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "Select-then-predict models are inherently faithful, as their classification component is trained only on extracted rationales (Jain et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "A good measure for measuring rationale quality is by evaluating the predictive performance of the classifier trained only on the rationales (Jain et al., 2020; Treviso and Martins, 2020).",
      "startOffset" : 140,
      "endOffset" : 186
    }, {
      "referenceID" : 33,
      "context" : "A good measure for measuring rationale quality is by evaluating the predictive performance of the classifier trained only on the rationales (Jain et al., 2020; Treviso and Martins, 2020).",
      "startOffset" : 140,
      "endOffset" : 186
    }, {
      "referenceID" : 10,
      "context" : "Given model M trained on an end-task, we typically evaluate its out-of-domain predictive performance on a test-set that does not belong to the same distribution as the data it was trained on (Hendrycks et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 215
    }, {
      "referenceID" : 14,
      "context" : "• Attention (α): Token importance corresponding to normalized attention scores (Jain et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : "• Scaled Attention (α∇α): Attention scores αi scaled by their corresponding gradients ∇αi = ∂ŷ ∂αi (Serrano and Smith, 2019).",
      "startOffset" : 99,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "• InputXGrad (x∇x): Attributes input importance by multiplying the input with its gradient computed with respect to the predicted class, where ∇xi = ∂ŷ ∂xi (Kindermans et al., 2016; Atanasova et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "• InputXGrad (x∇x): Attributes input importance by multiplying the input with its gradient computed with respect to the predicted class, where ∇xi = ∂ŷ ∂xi (Kindermans et al., 2016; Atanasova et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 205
    }, {
      "referenceID" : 32,
      "context" : "• Integrated Gradients (IG): Ranking words by computing the integral of the gradients taken along a straight path from a baseline input (zero embedding vector) to the original input (Sundararajan et al., 2017).",
      "startOffset" : 182,
      "endOffset" : 209
    }, {
      "referenceID" : 30,
      "context" : "• DeepLift: Ranking words according to the difference between the activation of each neuron and a reference activation (zero embedding vector) (Shrikumar et al., 2017).",
      "startOffset" : 143,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "• HardKuma: An end-to-end trained model, where the rationale extractor uses Hard Kumaraswamy variables to produce a rationale mask z, which the classifier uses to mask the input (Bastings et al., 2019).",
      "startOffset" : 178,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "(2016) and has shown improved performance (Guerreiro and Martins, 2021).",
      "startOffset" : 42,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "However, for HardKuma we opt using a bi-LSTM (Hochreiter and Schmidhuber, 1997) as it provides comparable or improved performance over BERT variants (Guerreiro and Martins, 2021), even after hyperparameter tuning.",
      "startOffset" : 45,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "However, for HardKuma we opt using a bi-LSTM (Hochreiter and Schmidhuber, 1997) as it provides comparable or improved performance over BERT variants (Guerreiro and Martins, 2021), even after hyperparameter tuning.",
      "startOffset" : 149,
      "endOffset" : 178
    }, {
      "referenceID" : 31,
      "context" : "SST: Stanford Sentiment Treebank (SST) consists of sentences tagged with sentiment on a 5point-scale from negative to positive (Socher et al., 2013).",
      "startOffset" : 127,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "We remove sentences with neutral sentiment and label the remaining sentences as negative or positive if they have a score lower or higher than 3 respectively (Jain and Wallace, 2019).",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 21,
      "context" : "IMDB: The Large Movie Reviews Corpus consists of movie reviews labeled either as positive or negative (Maas et al., 2011; Jain and Wallace, 2019).",
      "startOffset" : 102,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "IMDB: The Large Movie Reviews Corpus consists of movie reviews labeled either as positive or negative (Maas et al., 2011; Jain and Wallace, 2019).",
      "startOffset" : 102,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "Amazon Reviews: We form 3-way classification tasks by predicting the sentiment (negative, neutral, positive) of Amazon product reviews across 3 item categories: (1) Digital Music (AmazDigiMu); (2) Pantry (AmazPantry); and (3) Musical Instruments (AmazInstr) (Ni et al., 2019).",
      "startOffset" : 258,
      "endOffset" : 275
    }, {
      "referenceID" : 7,
      "context" : "• Normalized Sufficiency (NormSuff) measures the degree to which the extracted rationales are adequate for a model to make a prediction (DeYoung et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "• Normalized Comprehensiveness (NormComp) measures the influence of a rationale to a prediction (DeYoung et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "5 We omit from our evaluation the Remove-andRetrain method (Madsen et al., 2021) as it requires model retraining.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "Select-then-Predict Models: We first train select-then-predict models in-domain and then measure their predictive performance on the in-domain test set and on two out-of-domain test-sets (Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 187,
      "endOffset" : 235
    }, {
      "referenceID" : 9,
      "context" : "Select-then-Predict Models: We first train select-then-predict models in-domain and then measure their predictive performance on the in-domain test set and on two out-of-domain test-sets (Jain et al., 2020; Guerreiro and Martins, 2021).",
      "startOffset" : 187,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : "how intuitive to humans a rationale is (Jacovi and Goldberg, 2020)) and in practice faithfulness and plausibility do not correlate (Atanasova et al.",
      "startOffset" : 39,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "how intuitive to humans a rationale is (Jacovi and Goldberg, 2020)) and in practice faithfulness and plausibility do not correlate (Atanasova et al., 2020).",
      "startOffset" : 131,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "HardKuma: Table 3 presents the F1-macro performance of HardKuma models (Bastings et al., 2019) and the average rationale lengths (the ratio of the selected tokens compared to the length of the entire sequence) selected by the model.",
      "startOffset" : 71,
      "endOffset" : 94
    } ],
    "year" : 0,
    "abstractText" : "Recent work in Natural Language Processing has focused on developing approaches that extract faithful explanations, either via identifying the most important tokens in the input (i.e. post-hoc explanations) or by designing inherently faithful models that first select the most important tokens and then use them to predict the correct label (i.e. select-then-predict models). Currently, these approaches are largely evaluated on in-domain settings. Yet, little is known about how post-hoc explanations and inherently faithful models perform in out-ofdomain settings. In this paper, we conduct an extensive empirical study that examines: (1) the out-of-domain faithfulness of post-hoc explanations, generated by five feature attribution methods; and (2) the out-of-domain performance of two inherently faithful models over six datasets. Contrary to our expectations, results show that in many cases out-of-domain post-hoc explanation faithfulness measured by sufficiency and comprehensiveness is higher compared to in-domain. We find this misleading and suggest using a random baseline as a yardstick for evaluating post-hoc explanation faithfulness. Our findings also show that selectthen predict models demonstrate comparable predictive performance in out-of-domain settings to full-text trained models.1",
    "creator" : null
  }
}
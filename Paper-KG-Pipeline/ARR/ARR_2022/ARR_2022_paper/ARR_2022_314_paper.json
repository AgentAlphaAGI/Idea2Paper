{
  "name" : "ARR_2022_314_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The stellar performance of Transformers (Vaswani et al., 2017) has garnered a lot of attention to analyzing the reasons behind their effectiveness.The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019). But, there have been debates on whether raw attention weights are reliable anchors for explaining model’s behavior (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Jain and Wallace, 2019). Recently, it was shown that incorporating vector norms should be an indispensable part of any attention-based analysis (Kobayashi et al., 2020, 2021). However, these norm-based studies incorporate only the attention block into their analysis, whereas the encoder layer is composed of more components. We show that these components are essential for a more accurate analysis. Moreover, these studies are constrained to the analysis of single layer attributions.\nIn order to expand the analysis to the entire model, an aggregation technique has to be em-\nployed. Abnar and Zuidema (2020) proposed two aggregation methods, rollout and max-flow, which combine raw attention weights across layers. Despite reporting improvements on the attributions, the final results are still very low on fine-tuned models. Also, gradient-based alternatives have been argued to provide a more robust basis for such analysis (Brunner et al., 2020; Pascual et al., 2021), while being computationally intensive.\nIn this paper, we propose a new global token attribution analysis method (GlobEnc), which incorporates not only the attention block, but also the second layer normalization that produces the encoder layer’s output. Our results on BERT (Devlin et al., 2019) show high correlations with gradient based methods in both local and global settings.\nTo evaluate our approach, we compare the global attribution with the input token attributions obtained by gradient-based saliency scores. We show that: (i) norm-based methods achieve higher correlation than weight-based methods; (ii) incorporating residual connections plays an essential role in token attribution; (iii) layer normalizations can improve our analysis only if coupled together; and (iv) aggregation across layers is crucial for an ac-\ncurate whole-model attribution analysis. Based on these findings, we propose a global attribution method that provides faithful and plausible results (Figure 1). In summary, our main contributions are threefold:\n• We expand the scope of analysis from attention block in Transformers to the whole encoder.\n• Our method significantly improves over existing techniques for quantifying global token attribution in BERT.\n• We qualitatively demonstrate that the attributions obtained by our method are plausibly interpretable."
    }, {
      "heading" : "2 Background",
      "text" : "In encoder-based language models (such as BERT), a Transformer encoder layer is composed of several components (Figure 2). The core component of the encoder is the self-attention mechanism (Vaswani et al., 2017), which is responsible for the information mixture of a sequence of token representations (x1, ...,xn). Each self-attention head computes a set of attention weights Ah = {αhi,j |1 ≤ i, j ≤ n}, where αhi,j is the raw attention weight from the ith token to the jth token in head h ∈ {1, ...,H}. Therefore, the output representation (zi ∈ Rd) for the ith token of a multi-head (with H heads) selfattention module is computed by concatenating the heads’ outputs followed by a head-mixing WO projection:\nzi = CONCAT(z1i , ...,z H i )WO (1)\nWhere each head’s output vector is generated by performing a weighted sum over the transformed value vectors v(xj) ∈ Rdv :\nzhi = n∑\nj=1\nαhi,jv h(xj) (2)\nNorm-based attention. While one may interpret the attention mechanism using the attention weights A, Kobayashi et al. (2020) argued that doing so would ignore the norm of the transformed vectors multiplied by the weights, elucidating that the weights are insufficient for interpretation. Their solution enhanced the interpretability of attention weights by incorporating the value vectors v(xj) and the following projection WO. By reformulating Equation 1, we can consider zi as a summation\nover the attentions heads:\nzi = H∑\nh=1 n∑ j=1 αhi,j v h(xj)W h O︸ ︷︷ ︸\nfh(xj)\n(3)\nUsing this reformulation1, Kobayashi et al. proposed a norm-based token attribution analysis method, N := (||zi←j ||) ∈ Rn×n, to measure each token’s contribution in a self-attention module:\nzi←j = H∑ h=1 αhi,jf h(xj) (4)\nThey showed that incorporating the magnitude of the transformation function (fh(x)) is crucial in assessing the input tokens’ contribution to the selfattention output.\nResidual connections & Layer Normalizations. Kobayashi et al. (2021) added the attention block’s Layer Normalization (LN #1) and Residual connection (RES #1) to its prior norm-based analysis to assess the impact of residual connections and layer normalization inside an attention block. NRES := (||z+i←j ||) ∈ Rn×n is the analysis method which incorporates the attention block’s residual\n1W hO is a head-specific slice of the original WO projection. For more information about the reformulation process, see Appendix C in Kobayashi et al. (2021)\nconnection. The input vector x is added to the attribution of each token to itself to incorporate the influence of residual connection #1:\nz+i←j = H∑\nh=1\nαhi,jf h(xi) + 1[i = j]xi (5)\nThey proposed a method for decomposing LN2 into a summation of normalizations:\nLN(z+i ) = n∑\nj=1\ngz+i (z + i←j) + β\ngz+i (z + i←j) := z+i←j −m(z+i←j) s(z+i ) ⊙ γ\n(6)\nwhere m(.) and s(.) are the element-wise mean and standard deviation of the input vector (cf. §A.1). The decomposition can be applied to the contribution vectors:\nz̃i←j = gz+i ( H∑\nh=1\nαhi,jf h(xi) + 1[i = j]xi) (7)\nAccordingly, we can compute the magnitude NRESLN := (||z̃i←j ||) ∈ Rn×n, which represents the amount of influence of an encoder layer’s input token j on its output token i. Based on this formulation, a context-mixing ratio could be defined as:\nri = || ∑n\nj=1,j ̸=i z̃i←j || || ∑n j=1,j ̸=i z̃i←j ||+ ||z̃i←i|| (8)\nExperiments by Kobayashi et al. (2021) revealed considerably low r values which indicates the huge impact of the residual connections. In other words, the model tends to preserve token representations more than mixing them with each other."
    }, {
      "heading" : "3 Methodology",
      "text" : "Our method for input token attribution analysis has a holistic view and takes into account almost every component within the encoder layer. To this end, we first extend the norm-based analysis of Kobayashi et al. (2021) by incorporating the encoder’s output layer normalization #2. We then apply an aggregation technique to combine the information flow throughout all layers.\n2γ ∈ Rd and β ∈ Rd are the trainable weights of LN. Similar to Kobayashi et al. (2021) we ignore β.\nEncoder layer output ̸= Attention block output. While the residual connection #1 and the layer normalization #1 from the attention block are included in the analysis of Kobayashi et al. (2021), the subsequent FFN, residual connection #2, and output LN #2 are ignored (see Fig. 2). Hence, NRESLN might not be indicative of the entire encoder layer’s function. To address this issue, we additionally include the encoder layer components from the attention block outputs (z̃i) to the output representations (x̃i). The output of each encoder (x̃i) is computed as follows:\nz̃+i = FFN(z̃i) + z̃i x̃i = LN(z̃+i )\n(9)\nWe apply the LN decomposition rule in Eq. 7 to separate the impacts of residual and FFN output:\nx̃i = n∑ j=1 ( gz̃+i (FFN(z̃i←j)) + gz̃+i (z̃i←j) ) + β\n(10) Given that the activation function between the two fully connected layers in the FFN component is non-linear (Vaswani et al., 2017), a linear decomposition similar to Eq. 7 cannot be derived. As a result, we omit FFN’s influence on the contribution of each token and instead consider residual connection #2, approximating x̃i←j as gz̃+i (z̃i←j). Nevertheless, it should be noted that the FFN still preserves some influence on this new setting due to the presence of s(z̃+i ) in gz̃+i (z̃i←j). Similar to Eq. 7, we can introduce a more inclusive layerwise analysis method NENC := (||x̃i←j ||) ∈ Rn×n from input token j to output token i using:\nx̃i←j ≈ gz̃+i (z̃i←j) = z̃i←j −m(z̃i←j)\ns(z̃+i ) ⊙ γ\n(11)\nAggregating multi-layer attention. To create an aggregated attribution score, Abnar and Zuidema (2020) proposed describing the model’s attentions via modelling the information flow with a directed graph. They introduced attention rollout method, which linearly combines attention along all available paths in the pairwise attention graph. The attention rollout of layer ℓ w.r.t. the inputs is computed recursively as follows:\nÃℓ =\n{ ÂℓÃℓ−1 ℓ > 1\nÂℓ ℓ = 1 (12)\nÂℓ = 0.5Āℓ + 0.5I (13)\nĀℓ is the raw attention map averaged across all heads in layer ℓ. This method assumes equal contribution from the residual connection and multi-head attention (See Fig. 2). Hence, an identity matrix is summed and renormalized, giving Âℓ.\nFor aggregating the layerwise analysis methods, we use the rollout technique with minor modifications. As many of the methods already include residual connections, we only use Eq. 12 (replacing Âℓ with the desired method’s attribution matrix in layer ℓ) to calculate the rollout of a given method. However, for methods that do not assume the residual connection, we define a corresponding “FIXED” variation using Eq. 13 that incorporates a fixed value for the context mixing ratio (ri = 0.5)."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we introduce the datasets and the token attribution analysis methods used in our evaluations, followed by the experimental setup and results."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "All analysis methods are evaluated on three different classification tasks. To cover sentiment detection tasks we use SST2 (Socher et al., 2013), MNLI (Williams et al., 2018) for Natural Language Inference and Hatexplain (Mathew et al., 2021) in hate speech detection."
    }, {
      "heading" : "4.2 Analysis Methods",
      "text" : "We use two groups explainability approaches in our work: Weight-based and Norm-based.3 The Weight-based approaches employed in our experiments are as follows:\n• W : The raw attention maps averaged across all heads (See Aℓ in §2).\n• WFIXEDRES : Abnar and Zuidema’s assumption; add an identity matrix as a fixed residual to Aℓ (See Âℓ in Eq. 13).\n• WRES : To correct the W with only the accurate residuals, add the residual based on the context-mixing ratios of NENC:\n3Note that in our experiments, we use all these methods within the rollout aggregation method.\nr̂i = ∥∥∥∑nj=1,j ̸=i x̃i←j∥∥∥∥∥∥∑nj=1,j ̸=i x̃i←j∥∥∥+ ∥x̃i←i∥ A′ℓ =diag (r̂1, · · · , r̂n) Āℓ+\ndiag (1− r̂1, . . . , 1− r̂n) I\n(14)\nThe Norm-based analysis methods, namely N , NRES and NRESLN are discussed in detail in §2. Our proposed norm-based method NENC is discussed in §3. For our ablation study, we introduce NFIXEDRES which is N , corrected with a fixed residual similar to WFIXEDRES4.\nN̂ = ( ||zi←j ||∑ j ||zi←j || ) ∈ Rn×n\nNFIXEDRES := 0.5 N̂ + 0.5 I\n(15)\nWe refer to our proposed global method—aggregated NENC by the rollout method at the final layer—as GlobEnc."
    }, {
      "heading" : "4.3 Gradient-based Methods for Faithfulness Analysis",
      "text" : "Gradient-based methods are widely used as alternatives for attention-based counterparts for quantifying the importance of a specific input feature in making the right prediction (Li et al., 2016; Atanasova et al., 2020). In this section we discuss the specific gradient-based methods we use, namely saliency, HTA, and our adjusted HTA."
    }, {
      "heading" : "4.3.1 Saliency",
      "text" : "Gradient-based saliency is based on the gradient of the output (yc) w.r.t. the input embeddings (e0i ). One of its most accurate variations is the gradient×input method (Kindermans et al., 2016) where the input embeddings is multiplied by the gradients. Thus, the contribution score of input token i is determined by first computing the elementwise product of the input embeddings (e0i ) and the gradients of the true class output score (yc) w.r.t. the input embeddings. Then, the L2 norm of the scaled gradients is computed to derive the final score:\nSaliencyi = ∥∥∥∥ ∂yc∂e0i ⊙ e0i ∥∥∥∥ 2\n(16)\n4The only difference is that we need to normalize N before adding an identity matrix."
    }, {
      "heading" : "4.3.2 HTA x Inputs",
      "text" : "To determine an upper bound on the information mixing within each layer, we use a modified version of Hidden Token Attribution (Brunner et al., 2020, HTA). In the original version, HTA is the sensitivity between any two vectors in the model’s computational graph. However, inspired by the gradient×input method (Kindermans et al., 2016), which has shown more faithful results (Atanasova et al., 2020; Wu and Ong, 2021), we multiply the input vectors by the gradients and then apply a Frobenius norm. We compute the attribution from hidden embedding j (eℓ−1j ) to hidden embedding i (eℓi) in layer ℓ as:\ncℓi←j = ∥∥∥∥∥ ∂eℓi∂eℓ−1j ⊙ eℓ−1j ∥∥∥∥∥ F\n(17)\nComputing HTA-based attribution matrices is an extremely computationally intensive task (especially for long texts) due to the high dimensionality of the hidden embeddings. Hence, we only use this method for 256 examples from the SST-2 task’s validation set. It is worth noting that extracting the HTA-based contribution maps for the aforementioned data took approximately 2 hours, whereas computing the maps for the entire analysis methods stated in §4.2 took only 5 seconds.6\n5As mentioned in §4.2, this analysis method is based on the original experiment by Abnar and Zuidema (2020). Our experiments on SST2 differ from theirs in two aspects: (i) We opted for gradient×input saliencies, while they used the sum of gradients (sensitivity) (ii) Instead of BERT, they used a DistillBERT fine-tuned model (Sanh et al., 2019). However, it still yields lower results (Spearman Corr. = 0.13)\n6Conducted on a 3070 GPU machine."
    }, {
      "heading" : "4.4 Setup",
      "text" : "We employ HuggingFace’s transformers library7 (Wolf et al., 2020) and the BERT-base-uncased model. For fine-tuning BERT, epochs vary from 3 to 5, and the batch size and learning rate are 32 and 3e-5, respectively.8\nAfter rollout aggregation of each analysis method, we obtain an accumulated attribution matrix for every layer (ℓ) of BERT. These matrices indicate the overall contribution of each input token to all token representations in layer ℓ. Since the classifier in a fine-tuned model is attached to the final layer representation of the [CLS] token, we consider the first row (corresponding to [CLS] attributions) of the last layer attribution matrix. This vector represents the contribution of each input token to the model’s final decision. As a measure of faithfulness of the resulting vector with the saliency scores, we report the Spearman’s rank correlation between the two vectors."
    }, {
      "heading" : "4.5 Results",
      "text" : "Table 1 shows the Spearman correlation of saliency scores with the aggregated attribution scores from [CLS] to input tokens at the final layer. In order to determine the contribution of each component of encoder layer to the overall performance, we report results for multiple attribution analysis methods. Our results demonstrate that incorporating the vector norms, residual connection, and both layer normalizations yields the highest correlation (NENC). In what follows next, we discuss the im-\n7https://github.com/huggingface/transformers 8Recommended by Devlin et al. (2019).\npact of incorporating various parts in the analysis."
    }, {
      "heading" : "4.5.1 On the role of vector norms",
      "text" : "As also suggested by Kobayashi et al. (2020), vector norms play an important role in determining attention outputs. This is highlighted by the significant gap between weight-based and norm-based settings across all datasets in Table 1.\nWe also show the correlation of the aggregated attention for all layers in Figure 3. The norm-based settings (N and NRES) attain higher correlation than the weight-based counterparts (W and WRES) almost in all layers, confirming the importance of incorporating vector norms."
    }, {
      "heading" : "4.5.2 On the role of residual connections",
      "text" : "Kobayashi et al. (2021) showed that in the encoder layer, the output representations of each token is mainly determined by its own representation, and the contextualization from other tokens’ plays a marginal role. This is in contrary to the simplifying assumption made by Abnar and Zuidema (2020) who used a fixed context-mixing ratio of 0.5 (assuming that BERT equally preserves and mixes the representations). This setting is shown as weightbased with fixed residual (WFIXEDRES) in Table 1. We compare this setting against WRES (see §4.2). WRES is similar to WFIXEDRES (in that it does not take into account vector norms) but differs in that it considers a dynamic mixing ratio (the one from NENC). The huge performance gap between the two settings in Table 1 clearly highlights the importance of considering accurate context-mixing ratios. Therefore, it is crucial to consider the resid-\nual connection in the attention block for input token attribution analysis.\nTo further demonstrate the role of residual connections, we utilize the introduced method in §4.2, where we corrected the norm-based attentions with fixed residual (r = 0.5). The comparison of normbased without any residual (N ) and with a fixed residual (NFIXEDRES) shows a consistent improvement for the latter across all the datasets. This provides evidence on that having a fixed uniform context-mixing ratio is better than neglecting the residual connection altogether.\nFinally, when we aggregate the norm-based analysis with an accurate dynamic context-mixing ratio (NRES), we observe the highest correlation up to this point, without layer normalization."
    }, {
      "heading" : "4.5.3 On the role of layer normalization",
      "text" : "In Table 1 we see a sudden drop in correlations for NRESLN. Although this method considers vector norms and residuals, incorporating LN #1 in the encoder seems to have deteriorated the accuracy for token attribution analysis. To determine whether this deterioration of correlation in aggregated attributions is also present in individual single layers, we compare the HTA maps as a baseline with the attribution matrices extracted from different analysis methods. Figure 4 shows the correlation of HTA attribution maps with the maps obtained by NRES, NRESLN, and NENC methods. The results indicate that NRESLN exhibits a significantly lower association.\nThe question that arises here is that how incorporating an additional component of the encoder (LN #1) in NRESLN degrades the results (compared\nto NRES). To answer this question, we investigate the learned weights of layer norm #1 and #2. The outlier weights9 in specific dimensions of LNs are shown to be significantly influential on the model’s performance (Kovaleva et al., 2021; Luo et al., 2021). It is interesting to note that based on our observations, the outlier weights of the two layer norms seem to be the opposite of each other. Figure 5 demonstrates the exact weights in layer 11 and also the correlation of the outlier weights across layers. The large negative correlations confirm that the outlier weights work contrary to each other. We speculate that the effect of outliers in the two layer norms is partly cancelled out when both are considered.\nAs shown in Figure 2, the FFN and the second layer normalization are on top of the attention block. However, NRESLN does not incorporate the components outside of the attention block. As described in §3, in our local analysis method NENC we incorporate the second layer normalization in the transformer’s encoder (Figure 2), thus considering the whole encoder block (except FFN). Overall, our global method noted as GlobEnc yields the best results among all the methods evaluated in our experiments. In general, Table 1 suggests that incorporating each component of the encoder will increase the correlation; however, the two layer normalizations should be considered together."
    }, {
      "heading" : "4.5.4 On the role of aggregation",
      "text" : "We carried out an additional analysis to verify if incorporating vector norms, residual connection and layer normalizations in individual layers is ade-\n9We identify the dimensions where the weights are at least 3σ from the mean as outliers (Kovaleva et al., 2021).\nquate for achieving high correlations, or if it is also necessary to aggregate them via rollout. Table 2 shows the correlation results in different layers for raw attributions (without aggregation) and for the aggregated attributions using the rollout method. Applying rollout method on attribution maps up to each layer results in higher correlations with the saliency scores than the raw single layer attribution maps, especially in deeper layers. Therefore, attention aggregation is essential for global input token attribution analysis.\nAn interesting point in Figure 3, which shows the correlation of the aggregated methods throughout the layers, is that the correlation curves flatten out after only a few layers.10 This indicates that BERT identifies decisive tokens only after the first few layers. The final layers only make minor adjustments to this order. Nevertheless, it is worth noting that the order of attribution does not necessarily imply the model’s final decision and the final result may still change for the better or worse (Zhou et al., 2020)."
    }, {
      "heading" : "4.5.5 Qualitative analysis",
      "text" : "To qualitatively answer if the aggregated attribution maps provide plausible and meaningful interpretations, we take a closer look at the attribution maps generated by GlobEnc. Figure 1 shows the GlobEnc attribution of the model trained on SST-2. Each layer demonstrates the [CLS] token’s aggregated attribution to input tokens up to the corresponding layer. The example inputs are “people cinema at its finest.” and “big fat waste of time.”, both correctly classified by the model. In both cases, GlobEnc focuses on the relevant words for\n10WRES is the only exception with a constant increase; this method is gradually and artificially corrected by NENC context mixing ratios.\nsentiment classification, i.e., “finest” and “waste”. An interesting observation in Figure 1 is that in the first few layers, the [CLS] token mostly attends to itself while other tokens have marginal impact. As the representations get more contextualized in deeper layers, the attribution correctly shifts to the words which indicate the sentiment of the sentence.11 More examples are shown in Figure A.1. Our qualitative analysis suggests that GlobEnc can be useful for a reasonable interpretation of attention mechanism in BERT and possibly any other transformer-based model."
    }, {
      "heading" : "5 Related Work",
      "text" : "While numerous studies have used attention weights to analyze and interpret the self-attention mechanism (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Several solutions have been proposed to address this issue, usually through converting raw attention weights to scores that provide better explanations. Brunner et al. (2020) used the transformation function fh(xj) to introduce effective attentions—the orthogonal component of the attention matrix in fh(xj) null space—to explain the inner workings of each layer. However, this technique ignores other components in the encoder and is computationally expensive due to the SVD required to compute the effective attentions. Kobayashi et al. (2020) incorporated the modified vector and introduced a vector norms-based analysis. This was later extended by integrating residual connections and layer normalization components to enhance the accuracy of explanations (Kobayashi et al., 2021). But, as discussed in §4.5, relying solely on LN #1 does not produce accurate results.\nWhile these methods can be employed for singlelayer (local) analysis, multi-layer attributions are not necessarily correlated with single-layer attributions due to the significant degree of information combination through multi-layer language models (Pascual et al., 2021; Brunner et al., 2020). Various saliency methods exist for explaining the model’s decision based on the input (Li et al., 2016; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, 2021; Mohebbi et al., 2021).\n11Complete attention maps in Figure A.2 show that, similarly to [CLS], other tokens also focus on sentiment tokens.\nHowever, these approaches are not primarily designed for computing inter-token attributions. To fill this gap, Brunner et al. (2020) proposed HTA, which is based on the gradient of each hidden embedding in relation to the input embeddings. In §4.3.2, we extend HTA to incorporate the impact of the input vectors. However, HTA is extremely computationally intensive. Attention rollout (see §3) and attention flow—which involve solving a max-flow problem on the attention graph—are two aggregation approaches introduced by Abnar and Zuidema (2020), in which raw attention weights (with equally weighted residual weights) are aggregated within multiple layers. We showed that attention rollout does not perform well on a BERT model fine-tuned on downstream tasks and that this problem can be resolved by utilizing attribution norms."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this work, we proposed a novel method for single layer token attribution analysis which incorporates the whole encoder layer, i.e., the attention block and the output layer normalization. When aggregated across layers using the rollout method, our technique achieves quantitatively and qualitatively plausible results. Our evaluation of different analysis methods provided evidence on roles played by individual components of the encoder layer, i.e., the vector norms, the residual connections, and the layer normalizations. Furthermore, our in-depth analysis suggested that the two layer normalizations in the encoder layer counteract each other; hence, it is important to couple them for an accurate analysis.\nAdditionally, using a newly proposed and improved version of Hidden Token Attribution, we demonstrated that encoder-based attribution analysis is more accurate when compared to other partial solutions in a single layer (local-level). This is consistent with our global observations. Quantifying global input token attribution based on our work can provide a meaningful explanation of the whole model’s behavior. In future work, one can apply our global analysis method on various datasets and models, to provide valuable insights into model decisions and interpretability."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 LN Formulation m(a) := 1d ∑ k a (k),\ns(a) := √\n1 d ∑ k(m(a)− a(k) + ϵ)2\nwhere ϵ is a small constant\nA.2 More examples Aggregated attributions by different methods throughout layers is shown in Figure A.1. Our proposed method shows more plausible results.\nAggregated attribution map for layer 12 is shown in Figure A.2. In this figure, the effect of each token can be seen on all other tokens and not just the [CLS] token."
    } ],
    "references" : [ {
      "title" : "Quantifying attention flow in transformers",
      "author" : [ "Samira Abnar", "Willem Zuidema." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Abnar and Zuidema.,? 2020",
      "shortCiteRegEx" : "Abnar and Zuidema.",
      "year" : 2020
    }, {
      "title" : "The illustrated transformer [blog post",
      "author" : [ "Jay Alammar" ],
      "venue" : null,
      "citeRegEx" : "Alammar.,? \\Q2018\\E",
      "shortCiteRegEx" : "Alammar.",
      "year" : 2018
    }, {
      "title" : "A diagnostic study of explainability techniques for text classification",
      "author" : [ "Pepa Atanasova", "Jakob Grue Simonsen", "Christina Lioma", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Atanasova et al\\.,? 2020",
      "shortCiteRegEx" : "Atanasova et al\\.",
      "year" : 2020
    }, {
      "title" : "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods",
      "author" : [ "Jasmijn Bastings", "Katja Filippova" ],
      "venue" : "In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks",
      "citeRegEx" : "Bastings and Filippova.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bastings and Filippova.",
      "year" : 2020
    }, {
      "title" : "On identifiability in transformers",
      "author" : [ "Gino Brunner", "Yang Liu", "Damian Pascual", "Oliver Richter", "Massimiliano Ciaramita", "Roger Wattenhofer." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Brunner et al\\.,? 2020",
      "shortCiteRegEx" : "Brunner et al\\.",
      "year" : 2020
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Do attention heads in BERT track syntactic dependencies? CoRR, abs/1911.12246",
      "author" : [ "Phu Mon Htut", "Jason Phang", "Shikha Bordia", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Htut et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Htut et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is not Explanation",
      "author" : [ "Sarthak Jain", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "Investigating the influence of noise and distractors on the interpretation of neural networks",
      "author" : [ "Pieter-Jan Kindermans", "Kristof Schütt", "Klaus-Robert Müller", "Sven Dähne" ],
      "venue" : null,
      "citeRegEx" : "Kindermans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kindermans et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is not only a weight: Analyzing transformers with vector norms",
      "author" : [ "Goro Kobayashi", "Tatsuki Kuribayashi", "Sho Yokoi", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kobayashi et al\\.,? 2020",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating Residual and Normalization Layers into Analysis of Masked Language Models",
      "author" : [ "Goro Kobayashi", "Tatsuki Kuribayashi", "Sho Yokoi", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Kobayashi et al\\.,? 2021",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT busters: Outlier dimensions that disrupt transformers",
      "author" : [ "Olga Kovaleva", "Saurabh Kulshreshtha", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 3392–3405, Online. Association",
      "citeRegEx" : "Kovaleva et al\\.,? 2021",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2021
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing and understanding neural models in NLP",
      "author" : [ "Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Positional artefacts propagate through masked language model embeddings",
      "author" : [ "Ziyang Luo", "Artur Kulmizev", "Xiaoxi Mao." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
      "citeRegEx" : "Luo et al\\.,? 2021",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2021
    }, {
      "title" : "Hatexplain: A benchmark dataset for explainable hate speech detection",
      "author" : [ "Binny Mathew", "Punyajoy Saha", "Seid Muhie Yimam", "Chris Biemann", "Pawan Goyal", "Animesh Mukherjee." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Mathew et al\\.,? 2021",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the role of BERT token representations to explain sentence probing results",
      "author" : [ "Hosein Mohebbi", "Ali Modarressi", "Mohammad Taher Pilehvar." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Mohebbi et al\\.,? 2021",
      "shortCiteRegEx" : "Mohebbi et al\\.",
      "year" : 2021
    }, {
      "title" : "Telling BERT’s full story: from local attention",
      "author" : [ "Damian Pascual", "Gino Brunner", "Roger Wattenhofer" ],
      "venue" : null,
      "citeRegEx" : "Pascual et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Pascual et al\\.",
      "year" : 2021
    }, {
      "title" : "Visualizing and measuring the geometry of bert",
      "author" : [ "Emily Reif", "Ann Yuan", "Martin Wattenberg", "Fernanda B Viegas", "Andy Coenen", "Adam Pearce", "Been Kim." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 8594–8603.",
      "citeRegEx" : "Reif et al\\.,? 2019",
      "shortCiteRegEx" : "Reif et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "NeurIPS EMC2 Workshop.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Is attention interpretable",
      "author" : [ "Sofia Serrano", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Serrano and Smith.,? \\Q2019\\E",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on Empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is not not explanation",
      "author" : [ "Sarah Wiegreffe", "Yuval Pinter." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Wiegreffe and Pinter.,? 2019",
      "shortCiteRegEx" : "Wiegreffe and Pinter.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "On explaining your explanations of BERT: an empirical study with sequence classification",
      "author" : [ "Zhengxuan Wu", "Desmond C. Ong." ],
      "venue" : "CoRR, abs/2101.00196.",
      "citeRegEx" : "Wu and Ong.,? 2021",
      "shortCiteRegEx" : "Wu and Ong.",
      "year" : 2021
    }, {
      "title" : "Bert loses patience: Fast and robust inference with early exit",
      "author" : [ "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian McAuley", "Ke Xu", "Furu Wei." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 18330–18341. Curran Associates,",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The stellar performance of Transformers (Vaswani et al., 2017) has garnered a lot of attention to analyzing the reasons behind their effectiveness.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 150
    }, {
      "referenceID" : 13,
      "context" : "The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 150
    }, {
      "referenceID" : 19,
      "context" : "The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 150
    }, {
      "referenceID" : 24,
      "context" : "But, there have been debates on whether raw attention weights are reliable anchors for explaining model’s behavior (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Jain and Wallace, 2019).",
      "startOffset" : 115,
      "endOffset" : 192
    }, {
      "referenceID" : 21,
      "context" : "But, there have been debates on whether raw attention weights are reliable anchors for explaining model’s behavior (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Jain and Wallace, 2019).",
      "startOffset" : 115,
      "endOffset" : 192
    }, {
      "referenceID" : 8,
      "context" : "But, there have been debates on whether raw attention weights are reliable anchors for explaining model’s behavior (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Jain and Wallace, 2019).",
      "startOffset" : 115,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "Also, gradient-based alternatives have been argued to provide a more robust basis for such analysis (Brunner et al., 2020; Pascual et al., 2021), while being computationally intensive.",
      "startOffset" : 100,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "Also, gradient-based alternatives have been argued to provide a more robust basis for such analysis (Brunner et al., 2020; Pascual et al., 2021), while being computationally intensive.",
      "startOffset" : 100,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "Our results on BERT (Devlin et al., 2019) show high correlations with gradient based methods in both local and global settings.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : "The core component of the encoder is the self-attention mechanism (Vaswani et al., 2017), which is responsible for the information mixture of a sequence of token representations (x1, .",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "(10) Given that the activation function between the two fully connected layers in the FFN component is non-linear (Vaswani et al., 2017), a linear decomposition similar to Eq.",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 22,
      "context" : "To cover sentiment detection tasks we use SST2 (Socher et al., 2013), MNLI (Williams et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 25,
      "context" : ", 2013), MNLI (Williams et al., 2018) for Natural Language Inference and Hatexplain (Mathew et al.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : ", 2018) for Natural Language Inference and Hatexplain (Mathew et al., 2021) in hate speech detection.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "Gradient-based methods are widely used as alternatives for attention-based counterparts for quantifying the importance of a specific input feature in making the right prediction (Li et al., 2016; Atanasova et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 219
    }, {
      "referenceID" : 2,
      "context" : "Gradient-based methods are widely used as alternatives for attention-based counterparts for quantifying the importance of a specific input feature in making the right prediction (Li et al., 2016; Atanasova et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 219
    }, {
      "referenceID" : 9,
      "context" : "One of its most accurate variations is the gradient×input method (Kindermans et al., 2016) where the input embeddings is multiplied by the gradients.",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "However, inspired by the gradient×input method (Kindermans et al., 2016), which has shown more faithful results (Atanasova et al.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : ", 2016), which has shown more faithful results (Atanasova et al., 2020; Wu and Ong, 2021), we multiply the input vectors by the gradients and then apply a Frobenius norm.",
      "startOffset" : 47,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : ", 2016), which has shown more faithful results (Atanasova et al., 2020; Wu and Ong, 2021), we multiply the input vectors by the gradients and then apply a Frobenius norm.",
      "startOffset" : 47,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "Our experiments on SST2 differ from theirs in two aspects: (i) We opted for gradient×input saliencies, while they used the sum of gradients (sensitivity) (ii) Instead of BERT, they used a DistillBERT fine-tuned model (Sanh et al., 2019).",
      "startOffset" : 217,
      "endOffset" : 236
    }, {
      "referenceID" : 12,
      "context" : "The outlier weights9 in specific dimensions of LNs are shown to be significantly influential on the model’s performance (Kovaleva et al., 2021; Luo et al., 2021).",
      "startOffset" : 120,
      "endOffset" : 161
    }, {
      "referenceID" : 15,
      "context" : "The outlier weights9 in specific dimensions of LNs are shown to be significantly influential on the model’s performance (Kovaleva et al., 2021; Luo et al., 2021).",
      "startOffset" : 120,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "We identify the dimensions where the weights are at least 3σ from the mean as outliers (Kovaleva et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 28,
      "context" : "Nevertheless, it is worth noting that the order of attribution does not necessarily imply the model’s final decision and the final result may still change for the better or worse (Zhou et al., 2020).",
      "startOffset" : 179,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "While numerous studies have used attention weights to analyze and interpret the self-attention mechanism (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",
      "startOffset" : 105,
      "endOffset" : 186
    }, {
      "referenceID" : 13,
      "context" : "While numerous studies have used attention weights to analyze and interpret the self-attention mechanism (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",
      "startOffset" : 105,
      "endOffset" : 186
    }, {
      "referenceID" : 19,
      "context" : "While numerous studies have used attention weights to analyze and interpret the self-attention mechanism (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",
      "startOffset" : 105,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "While numerous studies have used attention weights to analyze and interpret the self-attention mechanism (Clark et al., 2019; Kovaleva et al., 2019; Reif et al., 2019; Htut et al., 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",
      "startOffset" : 105,
      "endOffset" : 186
    }, {
      "referenceID" : 21,
      "context" : ", 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",
      "startOffset" : 114,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : ", 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",
      "startOffset" : 114,
      "endOffset" : 191
    }, {
      "referenceID" : 24,
      "context" : ", 2019), the use of mere attention weights to explain a model’s inner workings has been an active topic of debate (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",
      "startOffset" : 114,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "This was later extended by integrating residual connections and layer normalization components to enhance the accuracy of explanations (Kobayashi et al., 2021).",
      "startOffset" : 135,
      "endOffset" : 159
    }, {
      "referenceID" : 18,
      "context" : "While these methods can be employed for singlelayer (local) analysis, multi-layer attributions are not necessarily correlated with single-layer attributions due to the significant degree of information combination through multi-layer language models (Pascual et al., 2021; Brunner et al., 2020).",
      "startOffset" : 250,
      "endOffset" : 294
    }, {
      "referenceID" : 4,
      "context" : "While these methods can be employed for singlelayer (local) analysis, multi-layer attributions are not necessarily correlated with single-layer attributions due to the significant degree of information combination through multi-layer language models (Pascual et al., 2021; Brunner et al., 2020).",
      "startOffset" : 250,
      "endOffset" : 294
    }, {
      "referenceID" : 14,
      "context" : "Various saliency methods exist for explaining the model’s decision based on the input (Li et al., 2016; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, 2021; Mohebbi et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "Various saliency methods exist for explaining the model’s decision based on the input (Li et al., 2016; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, 2021; Mohebbi et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : "Various saliency methods exist for explaining the model’s decision based on the input (Li et al., 2016; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, 2021; Mohebbi et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 197
    }, {
      "referenceID" : 27,
      "context" : "Various saliency methods exist for explaining the model’s decision based on the input (Li et al., 2016; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, 2021; Mohebbi et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 197
    }, {
      "referenceID" : 17,
      "context" : "Various saliency methods exist for explaining the model’s decision based on the input (Li et al., 2016; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, 2021; Mohebbi et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 197
    } ],
    "year" : 0,
    "abstractText" : "There has been a growing interest in interpreting the underlying dynamics of Transformers. While self-attention patterns were initially deemed as the primary choice, recent studies have shown that integrating other components can yield more accurate explanations. This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers. We quantitatively and qualitatively demonstrate that our method can yield faithful and meaningful global token attributions. Our extensive experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings. Our global attribution analysis surpasses previous methods by achieving significantly higher results in various datasets.",
    "creator" : null
  }
}
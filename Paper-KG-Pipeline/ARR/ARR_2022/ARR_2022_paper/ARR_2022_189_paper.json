{
  "name" : "ARR_2022_189_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Requirements and motivations of low-resource speech synthesis for language revitalization",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There are approximately 70 Indigenous languages spoken in Canada, from 10 distinct language families (Rice, 2008). As a consequence of the residential school system and other policies of cultural suppression, the majority of these languages now have fewer than 500 fluent speakers remaining, most of them elderly. Despite this, interest from students and parents in Indigenous language education continues to grow (Statistics Canada, 2016); we have heard from teachers that they are overwhelmed with interest from potential students, and the growing trend towards online education means many students who have not previously had access to language classes now do. Supporting these growing cohorts of students comes with unique challenges for languages with few fluent first-language speakers. A particular concern of teachers is to furnish their students with opportunities to hear the language outside of class. Text-to-speech synthesis technology (TTS) shows potential for supplementing text-based language learning tools with audio in the event that the domain is too large to be recorded directly, or as\nan interim solution pending recordings from firstlanguage speakers. Development of TTS systems in this context faces several challenges. Most notable is the usual assumption that neural speech synthesis models require tens of hours of audio recordings with corresponding text transcripts to be trained adequately. Such a data requirement is far beyondwhat is available for the languages we are concerned with, and is difficult to meet given the limited time of the relatively small number of speakers of these languages. The limited availability of Indigenous language speakers also hinders the subjective evaluation methods often used in TTS studies, where naturalness of synthetic speech samples is judged by speakers of the language in question. In this paper, we re-evaluate some of these challenges for applying TTS in the low-resource context of language revitalization. We build TTS systems for three Indigenous languages of Canada, with training data ranging from 25 minutes to 3.5 hours, and confirm that we can produce acceptable speech as judged by language teachers and learners. Outputs from these systems could be suitable for use in some classroom applications, for example a speaking verb conjugator."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Language Revitalization",
      "text" : "It is no secret that the majority of the world’s languages are in crisis, and in many cases this crisis is even more urgent than conservation biologists’ dire predictions for flora and fauna (Sutherland, 2003). However, the ‘doom and gloom’ rhetoric that often follows endangered languages over-represents vulnerability and under-represents the enduring strength of Indigenous communities who have refused to stop speaking their languages despite over a century of colonial policies against their use (Pine and Turin, 2017). Contin-\nuing to speak Indigenous languages is often seen as a political act of anti-colonial resistance. As such, the goals of any given language revitalization effort extend far beyond memorizing verb paradigms to broader goals of nationhood and self-determination (Pitawanakwat, 2009;McCarty, 2018). Language revitalization programs can also have immediate and important impacts on factors including community health andwellness (Whalen et al., 2016; Oster et al., 2014). There is a growing international consensus on the importance of linguistic diversity, from the Truth & Reconciliation Commission of Canada (TRC) report in 2015 which issued nine calls to action related to language, to 2019 being declared an International Year of Indigenous Languages by the UN, and 2022-2032 being declared an International Decade of Indigenous Languages. From 1996 to 2016, the number of speakers of Indigenous languages increased by 8% (Statistics Canada, 2016). These efforts have been successful despite a lack of support from digital technologies. While opportunities may exist for technology to assist and support language revitalization efforts, these technologies must be developed in a way that does not further marginalize communities (Brinklow et al., 2019; Bird, 2020)."
    }, {
      "heading" : "2.2 Why TTS for Language Revitalization?",
      "text" : "Our interest in speech synthesis for language revitalization was sparked during user evaluations of Kawennón:nis (lit. ‘it makes words‘), a Kanien’kéha verb conjugator (Kazantseva et al., 2018) developed in collaboration between the National Research Council Canada and the Onkwawenna Kentyohkwa adult immersion program in Six Nations of the Grand River in Ontario, Canada. Kawennón:nis models a pedagogicallyimportant subset of verb conjugations in XFST (Beesley and Karttunen, 2003), and currently produces 247,450 unique conjugations. The pronominal system is largely responsible for much of this productivity, since in transitive paradigms, agent/patient pairs are fused:\n(1) Senòn:wes you.to.it-like-habitual ‘You like it.’\n(2) Takenòn:wes you.to.me-like-habitual ‘You likeme.’ (Kazantseva et al., 2018)\nIn user evaluations of Kawennón:nis, students often asked whether it was possible to add audio to the tool, to model the pronunciation of unfamiliar words. Assuming a rate of 200 forms/hr for 4 hours per day, 5 days per week, this would take a teacher out of the classroom for approximately a year. Considering Kawennón:nis is anticipated to have over 1,000,000 unique forms by the time the grammar modelling work is finished, recording audio manually becomes infeasible. The research question that then emerged was ‘what is the smallest amount of data needed in order to generate audio for all verb forms in Kawennón:nis’. Beyond Kawennón:nis, we anticipate that there are many similar language revitalization projects that would want to add supplementary audio to other text-based pedagogical tools."
    }, {
      "heading" : "2.3 Speech Synthesis",
      "text" : "The last few years have shown an explosion in research into purely neural network-based approaches to speech synthesis (Tan et al., 2021). Similar to their HMM/GMM predecessors, neural pipelines typically consist of both a network predicting the acoustic properties of a sequence of text and a vocoder. The feature prediction network must be trained using parallel speech/text data where the input is typically a sequence of characters or phones that make up an utterance, and the output is a sequence of fixed-width frames of acoustic features. In most cases the predictions from the TTS model are log Mel-spectral features and a vocoder is used to generate the waveform from these acoustic features. Much of the previous work on low resource speech synthesis has focused on transfer learning; that is, ‘pre-training’ a network using data from a language that has more data, and then ‘fine-tuning’ using data from the low-resource language. One of the problems with this approach is that the input space differs between languages. As the inputs to these systems are sequences of characters or phones, and as these sequences are typically onehot encoded, it can be difficult to devise a principled method for transferring weights from the source language network to the target if there is a difference between the character or phone inventories of the two languages. Various strategies have emerged for normalizing the input space. For example, Demirsahin et al. (2018) propose a unified inventory for regional multilingual training of\nSouth Asian languages, while Tu et al. (2019) compare various methods to create mappings between source and target input spaces. Another proposal is to normalize the input space between source and target languages by replacing one-hot encodings of text with multi-hot phonological feature encodings (Gutkin et al., 2018; Wells and Richmond, 2021)."
    }, {
      "heading" : "2.4 Speech Synthesis for Indigenous Languages in Canada",
      "text" : "There is extremely little published work on speech synthesis for Indigenous languages in Canada (and North America generally). A statistical parametric speech synthesizer using Simple4All was recently developed for Plains Cree (Harrigan et al., 2019; Clark, 2014). Although it was unpublished, two highschool students created a statistical parametric speech synthesizer for Kanien’kéha by adapting eSpeak (Duddington and Dunn, 2007). We know of no other attempts to create speech synthesis systems for Indigenous languages in Canada. Elsewhere in North America, a Tacotron2 system has been built for Cherokee (Conrad, 2020), and some early work on concatenative systems for Navajo was discussed in a technical report (Whitman et al., 1997), as well as on Rarámuri (Urrea et al., 2009)."
    }, {
      "heading" : "3 Indigenous Language Data",
      "text" : "Although the term ‘low resource’ is used to describe a wide swath of languages, most Indigenous languages in Canada would be considered ‘lowresource’ in multiple senses of the word, having both a low amount of available data (annotated or unannotated), and a low number of speakers proportional to the population. Most Indigenous languages lack transcribed audio corpora; fewer still have such data recorded in a studio context. Due to the limited number of speakers, creating these resources is non-trivial: limited amounts of text from which a speaker could read, and there are few people available who are sufficiently literate in the languages to transcribe. Re-focusing speakers’ limited time to these tasks presents a significant opportunity cost; they are often already overworked and over-burdened in under-funded and under-resourced language teaching projects. As mentioned in §2.1, language technology projects that aim to assist language revitalization and reclamation efforts must be centered around the primary goals of those efforts and ensure that the means of developing the technology do not\ndistract or work against the broader sociopolitical goals. A primary stress point for many natural language processing projects involving Indigenous communities surrounds issues of data sovereignty. It is important that communities direct the development of these tools, and maintain control, ownership, and distribution rights for their data, as well as for the resulting speech synthesis models (Keegan, 2019; Brinklow, 2021). To test the feasibility of developing speech synthesis systems for Indigenous languages, we trained models for three unrelated Indigenous languages, Kanien’kéha (§3.1), Gitksan (§3.2), and SENĆOŦEN (§3.3)."
    }, {
      "heading" : "3.1 Kanien’kéha",
      "text" : "Kanien’kéha1 (a.k.a. Mohawk) is an Iroquoian language spoken by roughly 2,350 people in southern Ontario, Quebec, and northern New York state (Statistics Canada, 2016). In 1979 the first immersion school of any Indigenous language in Canada was opened for Kanien’kéha, and many other very successful programs have been started since, including the Onkwawenna Kentyohkwa adult immersion program in 1999 (Gomashie, 2019). In the late 1990s, a team of five Kanien’kéha translators worked with the Canadian Bible Society to translate and record parts of the Bible; one of the speakers on these recordings, Satewas, is still living. Translation runs in Satewas’s family, with his great-grandfather also working on Bible translations in the 19th century. Later, a team of four speakers and learners, including one of the authors, aligned the text and audio at the utterance level using Praat (Boersma and van Heuven, 2001) and ELAN (Brugman and Russel, 2004). While a total of 24 hours of audio were recorded, it was deemed inappropriate to use the voices of speakers who had passed away, leaving only recordings of Satewas’s voice. Using a GMMbased speaker ID system Kumar (2017), we removed utterances by these speakers, then removed utterances that were outliers in duration (less than 0.4s or greater than 11s) and speaking rate (less than 4 phones per second or greater than 15), recordings with an unknown phase effect present, and utterances containing non-Kanien’kéha characters (e.g. proper names like ‘Euphrades’). The\n1As there are different variations of spelling, we use the spelling used in the communities of Kahnawà:ke and Kahnesetà:ke throughout this paper\nresulting speech corpus comprised 3.46 hours of speech."
    }, {
      "heading" : "3.2 Gitksan",
      "text" : "Gitksan2 is one of four languages belonging to the Tsimshianic language family spoken along the Skeena river and its surrounding tributaries in the area colonially known as northern British Columbia. Traditional Gitksan territory spans some 33,000 square kilometers and is home to almost 10,000 people, with approximately 10% of the population continuing to speak the language fluently (First Peoples’ Cultural Council, 2018). As there were no studio-quality recordings of the Gitksan language publicly available, and as an intermediate speaker of the language, the first author decided to record a sample set himself. In total, he recorded 35.46 minutes of audio reading isolated sentences from published and unpublished stories (Forbes et al., 2017)."
    }, {
      "heading" : "3.3 SENĆOŦEN",
      "text" : "The SENĆOŦEN language is spoken by the W ¯ SÁNEĆ people on the southern part of the island colonially known as Vancouver Island. It belongs to the Coastal branch of the Salish language family. The W\n¯ SÁNEĆ community runs a world-\nfamous language revitalization program3, and uses an orthography developed by the late SENĆOŦEN speaker and W\n¯ SÁNEĆ elder PENÁĆ Dave Elliott.\nWhile the community of approximately 3,500 has fewer than 10 fluent speakers, there are hundreds of learners, many of whom have been enrolled in years of immersion education in the language (First Peoples’ Cultural Council, 2018). As there were no studio-quality recordings of the SENĆOŦEN language publicly available, we recorded 25.92 minutes of the language from PENÁĆ David Underwood reading two stories originally spoken by elder Chris Paul."
    }, {
      "heading" : "4 Research Questions",
      "text" : "Given the motivation and context for language revitalization-based speech synthesis, a number of research questions follow. Namely, how much data is required in order to build a system of reasonable pedagogical quality? How do we evalu-\n2We use Lonnie Hindle and Bruce Rigsby’s spelling of the language, which, with the use of ‘k’ and ‘a’ is a blend of upriver (gigeenix) and downriver (gyets) dialects\n3https://wsanecschoolboard.ca/sencotenlanguage/\nate such a system? And, how is the resulting system best integrated into the classroom? In §4.1, we discuss the difficulty of evaluating TTS systems in low-resource settings. We then discuss preliminary results for English and Indigenous language TTS which show that acceptable speech quality can be achieved with much less training data than usually considered for neural speech synthesis (§4.2). Finally, we suggest possible directions for pedagogical integration in section §4.4."
    }, {
      "heading" : "4.1 Low-resource Evaluation",
      "text" : "One of the most significant challenges in researching speech synthesis for languages with few speakers is evaluating the models. For some Indigenous languages in Canada, the total number of speakers of the language is less than the number typically required for statistical significance in a listening test. While the number of speakers in these conditions is sub-optimal for statistical analysis, the positive assessment of a few widely respected and community-engaged language speakers is practically sufficient to assess the pedagogical value of speech models in language revitalization contexts. For the experiments described in this paper, we ran listening tests for both Kanien’kéha and Gitksan with speakers, teachers, and learners, but were not able to run any such tests for SENĆOŦEN due to very few speakers with already busy schedules.\nWhile some objective metrics do exist, such as Mel cepstral distortion (MCD, Kubichek, 1993), we don’t believe they should be considered reliable proxies for listening tests. For example, MCDmay be sensitive to errors in acoustic feature prediction outside the perceptual range of listening test participants. Future research on speech synthesis for languages with few speakers should prioritize efficient and effective means of evaluating results.\nIn many cases, including in the experiment described in §4.2, artificial data constraints can be placed on a language with more data, like English, to simulate a low-resource scenario. While this technique can be insightful and it is tempting to draw universal conclusions, English is linguistically very different from many of the other languages spoken in the world. Accordingly, we should be cautious not to assume that results from these types of experiments will necessarily transfer or extend to genuinely low-resource languages."
    }, {
      "heading" : "4.2 How much data do you really need?",
      "text" : "The first question to answer is whether our Indigenous language corpora ranging from 25 minutes to 3.46 hours of speech are sufficient for building neural speech synthesizers. Due to the prominence of Tacotron2 (Shen et al., 2018), many people have assumed the data requirements for training a Tacotron2 model are synonymous with the data requirements for training any neural speech synthesizer of similar quality. As a result, some researchers still choose to implement either concatenative or HMM/GMM-based statistical parametric speech synthesis systems in low-resource situations based on the assumption that a “sufficiently large corpus [for neural TTS] is unavailable” (James et al., 2020, p. 298). We argue that attention-based models such as Tacotron2 should not be used as a benchmark for data requirements among all neural TTS methods, as they are notoriously difficult to train and unnecessarily inflate training data requirements."
    }, {
      "heading" : "4.2.1 Architecture choice",
      "text" : "Tacotron2 is an autoregressive model, meaning it predicts the speech parameters yt from both the input sequence of text x and the previous speech parameters y1, ..., yt−1. Typically, the model is trained with ‘teacher-forcing’, where the autoregressive frame yt−1 passed as input for predicting y is taken from the ground truth label and not the prediction network’s output from the previous frame ŷt−1. As discussed by Liu et al. (2019), such a system might learn to copy the teacher forcing input or disregard the text entirely, which could still optimize Tacotron2’s root mean square error function, but result in an untrained or degenerate attention network which is unable to properly generalize to new inputs at inference time when the teacher forcing input is unavailable. Attention failures represent a characteristic class of errors for models such as Tacotron2, for example skipping or repeating words from the input text (ValentiniBotinhao and King, 2021). There have been many proposals to improve training of the attention network, for example by guiding the attention or using a CTC loss function to respect the monotonic alignment between text inputs and speech outputs (Tachibana et al., 2018; Liu et al., 2019; Zheng et al., 2019; Gölge, 2020). As noted by Liu et al. (2019), increasing the socalled ‘reduction factor’ – which applies dropout\nto the autoregressive frames – can also help the model learn to rely more on the attention network than the teacher forcing inputs, but possibly at the risk of compromising synthesis quality. FastSpeech2 (Ren et al., 2021), and similar systems like FastPitch (Łańcucki, 2021), present an alternative to Tacotron2-type autoregressive systems with similar listening test results and without the characteristic errors related to attention. Instead of modelling duration using attention, they include an explicit duration prediction module trained on phone duration targets extracted from forced alignments over the training data. In the predecessor ‘FastSpeech’, attention weights from a Tacotron2 system were used to provide phone durations (Ren et al., 2019). However, there might not be sufficient of data to train an initial Tacotron2 model in the first place. In FastSpeech2, phone duration targets are instead extracted using the Montreal Forced Aligner (MFA, McAuliffe et al., 2017).We have found MFA can provide suitable alignments for our target languages, even with alignment models being trained on only limited data. Faster convergence of text-acoustic feature alignments has been found to speed up overall encoder-decoder TTS model training, as stable alignments provide a solid foundation for further training of the decoder. Badlani et al. (2021) show this by adding a jointly-learned alignment framework to a Tacotron2 architecture, reducing time to convergence. In contrast, they found that replacingMFA duration targets in FastSpeech2 training offers no benefit – forced alignment targets already provide enough information for more timeefficient training compared to an attention-based Tacotron2 system. Relieving the burden of learning an internal alignment model also opens the door to more data-efficient training. For example, Perez-Gonzalez-de-Martos et al. (2021) submitted a non-autoregressive model trained from forced alignments to the Blizzard Challenge 2021, where their system was found to be among the most natural and intelligible in subjective listening tests despite only using 5 hours of speech; all other submitted systems included often significant amounts of additional training data (up to 100 hours total)."
    }, {
      "heading" : "4.2.2 Experimental comparison of data requirements for neural TTS",
      "text" : "To investigate the effects of differing amounts of data on the attention network, and in preparation for training systems with our limited Indigenous\nlanguage data sets, we trained five Tacotron2 models on incremental partitions of the LJ Speech corpus of American English (Ito and Johnson, 2017). We used the NVIDIA implementation with default hyperparameters apart from a reduced batch size of 32 to fit the memory capacity of our GPU resources. We artificially constrained the training data such that the first model saw only the first hour of data from the shuffled corpus, the second model that same first hour plus another two hours (3 total) etc., so that the five models were trained on 1, 3, 5, 10 and 24 (full corpus) hours of speech. The models were trained for 100k steps and, as seen in Figure 1, using up to 5 hours of data the attention mechanism does not learn properly, resulting in degenerate outputs.\nFor comparison, we trained seven FastSpeech2 models on 15 and 30minute, 1, 3, 5, 10 and 24 hour incremental partitions of LJ Speech. Our model is based on an open-source implementation (Chien, 2021), which adds learnable speaker embeddings and a decoder postnet to the original model, as well as predicting pitch and energy values at the phone rather than frame level. We further modified the base architecture to match the LightSpeech model presented in Luo et al. (2021), removing the energy adaptor and substituting depthwise separable convolutions throughout the model. This significantly reduced the number of model parameters, speeding up training and inference; for additional discussion of the social and environmental ramifications of this change, see Appendix A. We also added learnable language embeddings for supplementary experiments in cross-lingual fine-tuning."
    }, {
      "heading" : "4.2.3 Results",
      "text" : "We conducted a short (10-15 minute) listening test to compare the two Tacotron2 models that trained properly (10h, full) against the seven FastSpeech2 models. We recruited 30 participants through Prolific, and presented each with fourMUSHRA-style questions where they were asked to rank the 9 voices along with a hidden natural speech reference (ITU-R, 2003). While it only took 30 minutes to recruit 30 participants using Prolific, the quality of responses was quite varied. We rejected two outright as they seemingly did not listen to the stimuli and left the same rankings for every voice. Even still, there was a lot of variation in responses from the remaining participants (Figure 2). We tested for significant differences between pairs of voices using Bonferroni-corrected Wilcoxon signed rank tests. Pairwise test results are summarized in the heat map of their p-values in Figure 3.\nIn the results from the pairwise analysis, we can see that natural speech is rated as significantly more natural than all synthetic speech samples. Naturalness ratings for the FastSpeech2 voices trained on 15m and 30m of data are significantly lower than all other voices, and significantly different from each other. The results for the remaining voices, while showing consistent improvements in naturalness ratings as more data is added (as shown in Figure 2), are not significantly different from each other. This is a relevant and important finding for low-resource speech synthesis\nbecause it shows that a FastSpeech2 voice built with 3 hours of data can achieve subjective naturalness ratings which are not significantly different from a Tacotron2 voice built with 24 hours of data. Similarly, the results of the listening test for our FastSpeech2 voice built with 1 hour of data are not significantly different from our Tacotron2 voice built with 10 hours of data. Additionally, while all the FastSpeech2 voices were intelligible, all Tacotron2 models trained with 5 hours or less data produced unintelligible speech."
    }, {
      "heading" : "4.3 Indigenous language experiments",
      "text" : "Despite the difficulty in evaluation (§4.1), we built and evaluated a number of TTS systems for the Indigenous languages described in §3. We built a baseline concatenative model using Festival and Multisyn (Taylor et al., 1998; Clark et al., 2007). Additionally, we trained cold-start FastSpeech2 models for each language, as well as models trained in a pre-training, fine-tuning pipeline where the multilingual, multispeaker FastSpeech2 donor model was trained on a combination of VCTK (Yamagishi et al., 2019), Kanien’kéha and Gitksan recordings. A rule-based mapping from orthography to pronunciation form was developed for each language using the ‘g2p’ Python library in order to perform alignment and synthesis at the phone-level instead of character-level (Pine and National Research Council Canada, 2021)."
    }, {
      "heading" : "4.3.1 Results",
      "text" : "We carried out listening test evaluations of Gitksan and Kanien’kéha models. Participants were recruited by contacting teachers, learners and linguists with at least some familiarity with the languages. For theKanien’kéha listening test, 6 participants were asked to answer 20 A/B questions comparing different synthesized utterances. Results showed that 72.22% of A/B responses from participants preferred our FastSpeech2model over our baseline concatenative model. In addition, 81.67% of A/B responses from participants preferred the cold-start to themodel fine-tuned on themulti-speaker, multilingual model, suggesting that the transfer learning approach discussed in §2.3 might not be necessary for models with explicit durations such as FastSpeech2. For the Gitksan listening test, 10 MOS-style questions were presented to 12 participants for both natural utterances and samples from our FastSpeech2 model. Results are shown in Figure 4. While both Kanien’kéha and Gitksan results seem to corroborate our belief that these models should be of reasonable quality despite limited training data, it is difficult to make any conclusive statement given the low number of eligible participants available for evaluation.\nAs the main goal of our efforts here is to eventually integrate our speech synthesis systems into a pedagogical setting, we also asked the question of whether they approved of the synthesis quality directly to the 18 people who participated across Kanien’kéha and Gitksan listening tests (see Fig. 5). Full responses are reported in Appendix B."
    }, {
      "heading" : "4.4 Integrating TTS in the Classroom",
      "text" : "Satisfying the goal of adding supplementary audio to a reference tool like Kawennón:nis, can be straightforwardly implemented by linking entries in the verb conjugator to pre-generated audio for the domain from a static server. This implementation also limits the potential of out of domain utterances that might be deemed inappropriate, which is an ethical concern in communities with low numbers of speakers where the identity of the ‘model’ speaker is easily determined. However, the ability to synthesize novel utterances could be pedagogically useful. Students often come into contact with words or sentences which do not have audio, and teachers often have to prepare new thematic word lists or vocabulary lessons that could benefit from a more general purpose speech synthesis solution. In those cases, with community and speaker input, we might consider what controls would be necessary for the users of this technology. One potential solution is the variance adaptor architecture present in FastSpeech2, allowing for phone-level control of duration, pitch and energy; an engaging demonstration of a graphical user interface for the corresponding controls in a FastPitchmodel is also available.4 We would like to focus further efforts on designing a user interface for speech synthesis systems that satisfies ethical concerns while prioritizing language pedagogy as the fundamental use case. In addition to fine-grained prosodic controls, we would like to explore the synthesis of hyperarticulated speech, as often used by language teachers when modelling pronunciation of unfamiliar words or sounds for students. This style of speech\n4https://fastpitch.github.io/\ntypically involves adjustment beyond the parameters of pitch, duration and energy, and is characterized by more careful enunciation of individual phones than is found in normal speech. This problem has parallels to the synthesis of Lombard speech (Hu et al., 2021), as used to improve intelligibility by speakers who find themselves in noisy environments."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we presented the first neural speech synthesis systems for Indigenous languages spoken in Canada. Subjective listening tests showed encouraging results for the naturalness and acceptability of voices for two languages, Kanien’kéha and Gitksan, despite limited training data availability (3.5 hours and 35 minutes, respectively). More extensive evaluation on English shows that the FastSpeech2 architecture can produce speech with similar quality to a Tacotron2 system using a fraction of the amount of speech usually considered for neural speech synthesis. Notably, a FastSpeech2 voice trained on 1 hour of English speech achieved subjective naturalness ratings not significantly different from a Tacotron2 voice using 10 hours of data, while a 3-hour FastSpeech2 system showed no significant difference from a 24-hour Tacotron2 voice. We attribute these results to the fact that FastSpeech2 learns input token durations from forced alignments, rather than jointly learning to align linguistic inputs to acoustic features alongside the acoustic feature prediction task as in attentionbased architectures such as Tacotron2. Given forced alignments of sufficient quality, which we found to be achievable even by training a Montreal Forced Aligner model only on our limited Indigenous language training data, this makes for more data-efficient training of neural TTS systems than has generally been explored in previous work. These findings show great promise for future work in low-resource TTS for language revitalization, especially as they come from systems trained from scratch on such limited data, rather than pre-training on a high-resource language and subsequent fine-tuning on limited target language data."
    }, {
      "heading" : "A Compute, Accessibility, &",
      "text" : "Environmental Impact\nFor reasons of environmental impact and accessibility, reducing the amount of computation required for both training and inference is important for any neural speech synthesis system, particularly so for Indigenous languages.\nA.1 Accessibility, Training & Inference Speed\nReducing the number of parameters in the model should translate to increased efficiency, and might make the model less prone to overfitting when training on limited amounts of data. Following Luo et al. (2021), we removed the energy variance adaptor and refactored the original convolutional layers in the encoder, decoder and remaining variance predictors to depthwise separable convolutional layers (Kaiser et al., 2018). We also changed the number of layers in the decoder from 6 to 45 and changed the size of the kernel in the encoder\n5This matches the original paper (Ren et al., 2019), but not the implementation by Chien (2021)\nand decoder convolutional layers to match the LightSpeech model described in Luo et al. (2021). These changes reduced the number of parameters in the model from 35,076,161 to 11,591,233 without noticeable change in voice quality, in addition to reducing the size of the stored model from 417.06MB to 135.24MB and significantly improving inference and train times as summarized in Table 1. In addition to describing the environmental cost of popular NLP models, Strubell et al. (2019) also make a compelling argument for equitable access to compute. Put another way, systems which require less compute, are more accessible. While language revitalization efforts are mostly encouraging about integrating new technologies into curriculum, there is a growing awareness of the potential harms. Beyond assessing the benefits and risks of introducing a new technology into language revitalization efforts, communities are concerned with the way the technology is researched and developed; as this process has the ability to empower or disempower language communities in equal measure (Alia, 2009; Brinklow et al., 2019). The current model for developing speech synthesis systems is not very equitable - models need to be run on GPUs by people with specialized training. For Indigenous communities to create speech synthesis tools for their languages, they should not need be required to hand over their language data to a large government or corporate organization. A pre-training, fine-tuning pipeline is attractive for this reason; communities could fine tune their own models on a laptop if a multilingual/multi-speaker model were pre-trained on GPUs at a larger institution. Reducing the computational requirements for training and inference of these models could help ensure language communities have greater control over the process of the development of these systems, less dependence on governmental organizations or corporations, and more sovereignty over their data (Keegan, 2019). In Table 1, we compare the training and inference time of the off-the-shelf FastSpeech2 system, as well as the adapted version described above. Results were timed by running themodel for 300 repetitions and taking themean. TheGPU (Tesla V100SXM2 16GB) was warmed up for 10 repetitions before timing started, and PyTorch’s built-in GPU synchronization method was used to synchronize\ntiming (which occurs on the CPU)with the training or inference running on the GPU. CPU tests were performed on an Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz with 4 cores and 16GB memory reserved.\nA.2 CO2 Consumption\nStrubell et al. (2019) argue that NLP researchers should have a responsibility to disclose the environmental footprint of their research, in order for the community to effectively evaluate any gains and to allow for a more equitable and reproducible field. The Canadian General Purpose Science Cluster (GPSC) in Dorval, Quebec is where all experiments requiring a GPU were performed. Experiments were all run on single Tesla V100-SXM2 16GB GPUs. Strubell et al. (2019) provide the following equation for estimating CO2 production:\npt = 1.58t(pc + pr + (g ∗ pg))\n1000 (1)\nwhere t is time, pt is total power for training, pc is average draw of CPU sockets, pr is average DRAM memory draw, g is the number of GPUs used in training and pg is the average draw from GPUs.\nIn our case, we estimate t to be equal to 1,541.986 after summing the time for experiments based on their log files, pc is 75 watts, pr is 6 watts, g is 1, and pg is 250 watts, and the equation for grams of CO2 consumption is CO2 = 34.5pt as the average carbon footprint of electricity distributed in Quebec is estimated at 34.5g CO2eq/kWh (Levasseur et al., 2021). This results in a total equivalent carbon consumption of 27,821.65 grams, roughly equivalent to driving\n6Note this estimate is based on the total number of hours spent running experiments from theM.Sc. dissertation this paper draws its experiments from. There were additional models trained for experiments that are not discussed in this paper. As such, this is a generous overestimation of t\na single passenger gas-powered vehicle for 110 kilometres according to the average rate of 404 grams/mile (EPA, 2019). This is a comparatively low C02 consumption for over 1500 GPU hours, largely due to the low CO2/kWh output of Quebec electricity when compared with the 2019 USA average of 400g CO2eq/kWh (EPA, 2019). However, CO2 equivalents are just a proxy for environmental impact and should not be understood to comprehensively account for social and environmental impact. Hydroelectric dam projects in Quebec, like the ones powering the GPSC have a sordid and complex history in the province. Innu Nation Grand Chief Mary Ann Nui spoke to this when she commented that “over the past 50 years, vast areas of our ancestral lands were destroyed by the Churchill Falls hydroelectric project, people lost their land, their livelihoods, their travel routes, and their personal belongings when the area where the project is located was flooded. Our ancestral burial sites are under water, our way of life was disrupted forever. Innu of Labrador weren’t informed or consulted about that projec” (Innu-Atikamekw-Anishnabeg Coalition, 2020)."
    }, {
      "heading" : "B Qualitative Results",
      "text" : "Question: “Would you be comfortable with any of the voices you heard being played online, say for a digital dictionary or verb conjugator if no other recording existed?”\nKanien’kéha responses:\n• Yes.\n• yes\n• Yes\n• Out of the two voices I hear, the first was clearer to understand\n• Yes, voices sounds really good!\n• yes\nGitksan responses:\n• yes\n• Yes, but the ones that have the most whistling or buzzing would be annoying.\n• maybe?? I think for a talking dictionary people do want to hear original pronunciations, but it could be a useful interim solution or a way to do short phrases!\n• Yes\n• Yes.\n• Assuming there is a single control for the last section of the survey/test, then some of the synthesised voices actually sound really good and I would be comfortable hearing those in an online dictionary where audio didn’t exist for a particular word or phrase.\n• yes\n• The ones with higher ratings for sure, some of the lower ratings were just about the sound quality because that hampered hearing the speech quality. So I may have confounded the results with that, but point remains that it is always good to try to avoid poor audio recordings for online dictionaries\n• Maybe/yes\n• only ones rated fair or above fair\n• Absolutely yes\n• yes, as long as they were identified as synthesized"
    } ],
    "references" : [ {
      "title" : "The NewMedia Nation: Indigenous Peoples and Global Communication, ned - new edi8",
      "author" : [ "Valerie Alia" ],
      "venue" : null,
      "citeRegEx" : "Alia.,? \\Q2009\\E",
      "shortCiteRegEx" : "Alia.",
      "year" : 2009
    }, {
      "title" : "One TTS Alignment To Rule Them All",
      "author" : [ "Rohan Badlani", "Adrian Łancucki", "Kevin J. Shih", "Rafael Valle", "Wei Ping", "Bryan Catanzaro." ],
      "venue" : "arXiv:2108.10447 [cs, eess].",
      "citeRegEx" : "Badlani et al\\.,? 2021",
      "shortCiteRegEx" : "Badlani et al\\.",
      "year" : 2021
    }, {
      "title" : "Finite State Morphology",
      "author" : [ "Kenneth R. Beesley", "Lauri Karttunen." ],
      "venue" : "CSLI Publications.",
      "citeRegEx" : "Beesley and Karttunen.,? 2003",
      "shortCiteRegEx" : "Beesley and Karttunen.",
      "year" : 2003
    }, {
      "title" : "Decolonising speech and language technology",
      "author" : [ "Steven Bird." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3504–3519.",
      "citeRegEx" : "Bird.,? 2020",
      "shortCiteRegEx" : "Bird.",
      "year" : 2020
    }, {
      "title" : "Speak and unSpeak with PRAAT",
      "author" : [ "Paul Boersma", "Vincent van Heuven." ],
      "venue" : "Glot International, 5(9/10):341–347.",
      "citeRegEx" : "Boersma and Heuven.,? 2001",
      "shortCiteRegEx" : "Boersma and Heuven.",
      "year" : 2001
    }, {
      "title" : "Indigenous language technologies: Anti-colonial oases in a colonizing (digital) world",
      "author" : [ "Nathan Thanyehténhas Brinklow." ],
      "venue" : "WINHEC: International Journal of Indigenous Education Scholarship, (1).",
      "citeRegEx" : "Brinklow.,? 2021",
      "shortCiteRegEx" : "Brinklow.",
      "year" : 2021
    }, {
      "title" : "Indigenous Language Technologies & Language Reclamation in Canada",
      "author" : [ "Nathan Thanyehténhas Brinklow", "Patrick Littell", "Delaney Lothian", "Aidan Pine", "Heather Souter." ],
      "venue" : "Proceedings of the 1st International Conference on Language Technolo-",
      "citeRegEx" : "Brinklow et al\\.,? 2019",
      "shortCiteRegEx" : "Brinklow et al\\.",
      "year" : 2019
    }, {
      "title" : "Annotating Multi-media/Multi-modal Resources with ELAN",
      "author" : [ "Hennie Brugman", "Albert Russel." ],
      "venue" : "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04), Lisbon, Portugal. European Language",
      "citeRegEx" : "Brugman and Russel.,? 2004",
      "shortCiteRegEx" : "Brugman and Russel.",
      "year" : 2004
    }, {
      "title" : "ming024/FastSpeech2",
      "author" : [ "Chung-Ming Chien." ],
      "venue" : "Original-date: 2020-06-25T13:57:53Z.",
      "citeRegEx" : "Chien.,? 2021",
      "shortCiteRegEx" : "Chien.",
      "year" : 2021
    }, {
      "title" : "Simple4all",
      "author" : [ "Robert AJ Clark." ],
      "venue" : "Proc. Interspeech 2014, pages 1502–1503.",
      "citeRegEx" : "Clark.,? 2014",
      "shortCiteRegEx" : "Clark.",
      "year" : 2014
    }, {
      "title" : "Multisyn: Open-domain unit selection for the festival speech synthesis system",
      "author" : [ "Robert AJ Clark", "Korin Richmond", "Simon King." ],
      "venue" : "Speech Communication, 49(4):317–330.",
      "citeRegEx" : "Clark et al\\.,? 2007",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2007
    }, {
      "title" : "Tacotron2 and Cherokee TTS",
      "author" : [ "Michael Conrad" ],
      "venue" : null,
      "citeRegEx" : "Conrad.,? \\Q2020\\E",
      "shortCiteRegEx" : "Conrad.",
      "year" : 2020
    }, {
      "title" : "A unified phonological representation of South Asian languages for multilingual text-tospeech",
      "author" : [ "Isin Demirsahin", "Martin Jansche", "Alexander Gutkin." ],
      "venue" : "Proc. The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Lan-",
      "citeRegEx" : "Demirsahin et al\\.,? 2018",
      "shortCiteRegEx" : "Demirsahin et al\\.",
      "year" : 2018
    }, {
      "title" : "eSpeak: Speech Synthesizer",
      "author" : [ "Jonathan Duddington", "Reece Dunn" ],
      "venue" : null,
      "citeRegEx" : "Duddington and Dunn.,? \\Q2007\\E",
      "shortCiteRegEx" : "Duddington and Dunn.",
      "year" : 2007
    }, {
      "title" : "Three Gitksan Texts",
      "author" : [ "Clarissa Forbes", "Henry Davis", "Michael Schwan", "Gitksan Research Lab." ],
      "venue" : "45:47–89.",
      "citeRegEx" : "Forbes et al\\.,? 2017",
      "shortCiteRegEx" : "Forbes et al\\.",
      "year" : 2017
    }, {
      "title" : "Solving Attention Problems of TTS models with Double Decoder Consistency",
      "author" : [ "Eren Gölge" ],
      "venue" : null,
      "citeRegEx" : "Gölge.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gölge.",
      "year" : 2020
    }, {
      "title" : "Kanien’keha / Mohawk Indigenous language revitalisation efforts in Canada",
      "author" : [ "Grace A. Gomashie." ],
      "venue" : "McGill Journal of Education / Revue des sciences de l’éducation de McGill, 54(1).",
      "citeRegEx" : "Gomashie.,? 2019",
      "shortCiteRegEx" : "Gomashie.",
      "year" : 2019
    }, {
      "title" : "FonBund: A Library for Combining Cross-lingual Phonological Segment Data",
      "author" : [ "Alexander Gutkin", "Martin Jansche", "Tatiana Merkulova." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evalua-",
      "citeRegEx" : "Gutkin et al\\.,? 2018",
      "shortCiteRegEx" : "Gutkin et al\\.",
      "year" : 2018
    }, {
      "title" : "A Preliminary Plains Cree Speech Synthesizer",
      "author" : [ "Atticus Harrigan", "Antti Arppe", "Timothy Mills." ],
      "venue" : "Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers), pages 64–",
      "citeRegEx" : "Harrigan et al\\.,? 2019",
      "shortCiteRegEx" : "Harrigan et al\\.",
      "year" : 2019
    }, {
      "title" : "Whispered and Lombard Neural Speech Synthesis",
      "author" : [ "Qiong Hu", "Tobias Bleisch", "Petko Petkov", "Tuomo Raitio", "Erik Marchi", "Varun Lakshminarasimhan." ],
      "venue" : "2021 IEEE Spoken Language Technology Workshop (SLT), pages 454–461.",
      "citeRegEx" : "Hu et al\\.,? 2021",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2021
    }, {
      "title" : "The LJ speech dataset",
      "author" : [ "Keith Ito", "Linda Johnson." ],
      "venue" : "https://keithito.com/LJ-SpeechDataset/.",
      "citeRegEx" : "Ito and Johnson.,? 2017",
      "shortCiteRegEx" : "Ito and Johnson.",
      "year" : 2017
    }, {
      "title" : "Developing resources for te reo Māori text to speech synthesis system",
      "author" : [ "Jesin James", "Isabella Shields", "Rebekah Berriman", "Peter Keegan", "Catherine Watson." ],
      "venue" : "P. Sojka, I. Kopeček, K. Pala, and A. Horák, editors, Text, Speech, and Dialogue, pages",
      "citeRegEx" : "James et al\\.,? 2020",
      "shortCiteRegEx" : "James et al\\.",
      "year" : 2020
    }, {
      "title" : "Depthwise Separable Convolutions for Neural Machine Translation",
      "author" : [ "Lukasz Kaiser", "Aidan N. Gomez", "Francois Chollet" ],
      "venue" : null,
      "citeRegEx" : "Kaiser et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2018
    }, {
      "title" : "Kawennón:nis: the wordmaker for Kanyen’kéha",
      "author" : [ "Anna Kazantseva", "Owennatekha Brian Maracle", "Ronkwe’tiyóhstha Josiah Maracle", "Aidan Pine" ],
      "venue" : "In Proceedings of the Workshop",
      "citeRegEx" : "Kazantseva et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kazantseva et al\\.",
      "year" : 2018
    }, {
      "title" : "Issues with Māori sovereignty over Māori language data",
      "author" : [ "Te Taka Keegan" ],
      "venue" : null,
      "citeRegEx" : "Keegan.,? \\Q2019\\E",
      "shortCiteRegEx" : "Keegan.",
      "year" : 2019
    }, {
      "title" : "Mel-cepstral distance measure for objective speech quality assessment",
      "author" : [ "R. Kubichek." ],
      "venue" : "Proceedings of IEEE Pacific RimConference on Communications Computers and Signal Processing, volume 1, pages 125–128 vol.1.",
      "citeRegEx" : "Kubichek.,? 1993",
      "shortCiteRegEx" : "Kubichek.",
      "year" : 1993
    }, {
      "title" : "Spoken Speaker Identification based on Gaussian Mixture Models : Python Implementation",
      "author" : [ "Abhijeet Kumar" ],
      "venue" : null,
      "citeRegEx" : "Kumar.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kumar.",
      "year" : 2017
    }, {
      "title" : "Fastpitch: Parallel Text-toSpeech with Pitch Prediction",
      "author" : [ "Adrian Łańcucki." ],
      "venue" : "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6588–6592.",
      "citeRegEx" : "Łańcucki.,? 2021",
      "shortCiteRegEx" : "Łańcucki.",
      "year" : 2021
    }, {
      "title" : "Improving the accuracy of electricity carbon footprint: Estimation of hydroelectric reservoir greenhouse gas emissions",
      "author" : [ "A. Levasseur", "S. Mercier-Blais", "Y.T. Prairie", "A. Tremblay", "C. Turpin." ],
      "venue" : "Renewable and Sustainable Energy Reviews, 136:110433.",
      "citeRegEx" : "Levasseur et al\\.,? 2021",
      "shortCiteRegEx" : "Levasseur et al\\.",
      "year" : 2021
    }, {
      "title" : "Maximizing Mutual Information for Tacotron",
      "author" : [ "Peng Liu", "Xixin Wu", "Shiyin Kang", "Guangzhi Li", "Dan Su", "Dong Yu." ],
      "venue" : "arXiv:1909.01145 [cs, eess]. ArXiv: 1909.01145.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "LightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search",
      "author" : [ "Renqian Luo", "Xu Tan", "Rui Wang", "Tao Qin", "Jinzhu Li", "Sheng Zhao", "Enhong Chen", "Tie-Yan Liu" ],
      "venue" : null,
      "citeRegEx" : "Luo et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2021
    }, {
      "title" : "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "author" : [ "Michael McAuliffe", "Michaela Socolof", "Sarah Mihuc", "Michael Wagner", "Morgan Sonderegger." ],
      "venue" : "Interspeech 2017, pages 498–502. ISCA.",
      "citeRegEx" : "McAuliffe et al\\.,? 2017",
      "shortCiteRegEx" : "McAuliffe et al\\.",
      "year" : 2017
    }, {
      "title" : "Community-based language planning: Perspectives from Indigenous language revitalization",
      "author" : [ "Teresa L McCarty." ],
      "venue" : "The Routledge handbook of language revitalization, pages 22–35. Routledge.",
      "citeRegEx" : "McCarty.,? 2018",
      "shortCiteRegEx" : "McCarty.",
      "year" : 2018
    }, {
      "title" : "Cultural continuity, traditional Indigenous language, and diabetes in Alberta first nations: a mixed methods study",
      "author" : [ "Richard Oster", "Angela Grier", "Rick Lightning", "Maria Mayan", "Ellen Toth." ],
      "venue" : "International journal for equity in health, 13:92.",
      "citeRegEx" : "Oster et al\\.,? 2014",
      "shortCiteRegEx" : "Oster et al\\.",
      "year" : 2014
    }, {
      "title" : "VRAIN-UPV MLLP’s system for the Blizzard Challenge 2021",
      "author" : [ "Alejandro Perez-Gonzalez-de-Martos", "Albert Sanchis", "Alfons Juan." ],
      "venue" : "Blizzard Challenge 2021 Workshop, page 5.",
      "citeRegEx" : "Perez.Gonzalez.de.Martos et al\\.,? 2021",
      "shortCiteRegEx" : "Perez.Gonzalez.de.Martos et al\\.",
      "year" : 2021
    }, {
      "title" : "Language Revitalization",
      "author" : [ "Aidan Pine", "Mark Turin." ],
      "venue" : "ISBN: 9780199384655.",
      "citeRegEx" : "Pine and Turin.,? 2017",
      "shortCiteRegEx" : "Pine and Turin.",
      "year" : 2017
    }, {
      "title" : "Anishinaabemodaa Pane Oodenang: a qualitative study of Anishinaabe language revitalization as selfdetermination in Manitoba and Ontario",
      "author" : [ "Brock Thorbjorn Pitawanakwat." ],
      "venue" : "Ph.D. thesis.",
      "citeRegEx" : "Pitawanakwat.,? 2009",
      "shortCiteRegEx" : "Pitawanakwat.",
      "year" : 2009
    }, {
      "title" : "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
      "author" : [ "Yi Ren", "Chenxu Hu", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu" ],
      "venue" : null,
      "citeRegEx" : "Ren et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2021
    }, {
      "title" : "FastSpeech: Fast, Robust and Controllable Text to Speech",
      "author" : [ "Yi Ren", "Yangjun Ruan", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32.",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Indigenous languages in Canada",
      "author" : [ "Keren Rice." ],
      "venue" : "The Canadian Encyclopedia.",
      "citeRegEx" : "Rice.,? 2008",
      "shortCiteRegEx" : "Rice.",
      "year" : 2008
    }, {
      "title" : "Energy and Policy Considerations for Deep Learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel extinction risk and global distribution of languages and species",
      "author" : [ "William J. Sutherland." ],
      "venue" : "Nature, 423(6937):276–279. Bandiera_abtest: a Cg_type: Nature Research Journals Number: 6937 Primary_atype: Research Publisher: Nature Publish-",
      "citeRegEx" : "Sutherland.,? 2003",
      "shortCiteRegEx" : "Sutherland.",
      "year" : 2003
    }, {
      "title" : "Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention",
      "author" : [ "Hideyuki Tachibana", "Katsuya Uenoyama", "Shunsuke Aihara." ],
      "venue" : "2018 IEEE International Conference on Acoustics, Speech and Signal",
      "citeRegEx" : "Tachibana et al\\.,? 2018",
      "shortCiteRegEx" : "Tachibana et al\\.",
      "year" : 2018
    }, {
      "title" : "The architecture of the Festival speech synthesis system",
      "author" : [ "Paul Taylor", "Alan W Black", "Richard Caley." ],
      "venue" : "The Third ESCA/COCOSDA Workshop (ETRW) on Speech Synthesis.",
      "citeRegEx" : "Taylor et al\\.,? 1998",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 1998
    }, {
      "title" : "End-to-end Text-to-speech for Low-resource Languages by Cross-Lingual Transfer Learning",
      "author" : [ "Tao Tu", "Yuan-Jui Chen", "Cheng-chieh Yeh", "Hungyi Lee" ],
      "venue" : null,
      "citeRegEx" : "Tu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards the Speech Synthesis of Raramuri: A Unit Selection Approach based on Unsupervised Extraction of Suffix Sequences",
      "author" : [ "A.M. Urrea", "José Abel Herrera Camacho", "Maribel Alvarado Garćıa" ],
      "venue" : null,
      "citeRegEx" : "Urrea et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Urrea et al\\.",
      "year" : 2009
    }, {
      "title" : "Detection andAnalysis of Attention Errors in Sequenceto-Sequence Text-to-Speech",
      "author" : [ "Cassia Valentini-Botinhao", "Simon King." ],
      "venue" : "Interspeech 2021, pages 2746–2750. ISCA.",
      "citeRegEx" : "Valentini.Botinhao and King.,? 2021",
      "shortCiteRegEx" : "Valentini.Botinhao and King.",
      "year" : 2021
    }, {
      "title" : "Cross-lingual Transfer of Phonological Features for Low-resource Speech Synthesis",
      "author" : [ "Dan Wells", "Korin Richmond." ],
      "venue" : "Proc. 11th ISCA Speech Synthesis Workshop.",
      "citeRegEx" : "Wells and Richmond.,? 2021",
      "shortCiteRegEx" : "Wells and Richmond.",
      "year" : 2021
    }, {
      "title" : "Healing through language: Positive physical health effects of indigenous language use",
      "author" : [ "D. Whalen", "Margaret Moss", "Daryl Baldwin." ],
      "venue" : "F1000Research, 5:852.",
      "citeRegEx" : "Whalen et al\\.,? 2016",
      "shortCiteRegEx" : "Whalen et al\\.",
      "year" : 2016
    }, {
      "title" : "A Navajo Language Text-to-Speech Synthesizer",
      "author" : [ "Robert Whitman", "Richard Sproat", "Chilin Shih." ],
      "venue" : "AT&T Bell Laboratories.",
      "citeRegEx" : "Whitman et al\\.,? 1997",
      "shortCiteRegEx" : "Whitman et al\\.",
      "year" : 1997
    }, {
      "title" : "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92). The Rainbow Passage which the speakers",
      "author" : [ "Junichi Yamagishi", "Christophe Veaux", "Kirsten MacDonald" ],
      "venue" : null,
      "citeRegEx" : "Yamagishi et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yamagishi et al\\.",
      "year" : 2019
    }, {
      "title" : "Forward-Backward Decoding for Regularizing Endto-End TTS",
      "author" : [ "Yibin Zheng", "Xi Wang", "Lei He", "Shifeng Pan", "Frank K. Soong", "Zhengqi Wen", "Jianhua Tao" ],
      "venue" : null,
      "citeRegEx" : "Zheng et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    }, {
      "title" : "2021), we removed the energy variance",
      "author" : [ "Luo" ],
      "venue" : null,
      "citeRegEx" : "Luo,? \\Q2021\\E",
      "shortCiteRegEx" : "Luo",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "There are approximately 70 Indigenous languages spoken in Canada, from 10 distinct language families (Rice, 2008).",
      "startOffset" : 101,
      "endOffset" : 113
    }, {
      "referenceID" : 41,
      "context" : "It is no secret that the majority of the world’s languages are in crisis, and in many cases this crisis is even more urgent than conservation biologists’ dire predictions for flora and fauna (Sutherland, 2003).",
      "startOffset" : 191,
      "endOffset" : 209
    }, {
      "referenceID" : 35,
      "context" : "However, the ‘doom and gloom’ rhetoric that often follows endangered languages over-represents vulnerability and under-represents the enduring strength of Indigenous communities who have refused to stop speaking their languages despite over a century of colonial policies against their use (Pine and Turin, 2017).",
      "startOffset" : 290,
      "endOffset" : 312
    }, {
      "referenceID" : 36,
      "context" : "tion effort extend far beyond memorizing verb paradigms to broader goals of nationhood and self-determination (Pitawanakwat, 2009;McCarty, 2018).",
      "startOffset" : 110,
      "endOffset" : 144
    }, {
      "referenceID" : 32,
      "context" : "tion effort extend far beyond memorizing verb paradigms to broader goals of nationhood and self-determination (Pitawanakwat, 2009;McCarty, 2018).",
      "startOffset" : 110,
      "endOffset" : 144
    }, {
      "referenceID" : 48,
      "context" : "Language revitalization programs can also have immediate and important impacts on factors including community health andwellness (Whalen et al., 2016; Oster et al., 2014).",
      "startOffset" : 129,
      "endOffset" : 170
    }, {
      "referenceID" : 33,
      "context" : "Language revitalization programs can also have immediate and important impacts on factors including community health andwellness (Whalen et al., 2016; Oster et al., 2014).",
      "startOffset" : 129,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "ogy to assist and support language revitalization efforts, these technologies must be developed in a way that does not further marginalize communities (Brinklow et al., 2019; Bird, 2020).",
      "startOffset" : 151,
      "endOffset" : 186
    }, {
      "referenceID" : 3,
      "context" : "ogy to assist and support language revitalization efforts, these technologies must be developed in a way that does not further marginalize communities (Brinklow et al., 2019; Bird, 2020).",
      "startOffset" : 151,
      "endOffset" : 186
    }, {
      "referenceID" : 23,
      "context" : "‘it makes words‘), a Kanien’kéha verb conjugator (Kazantseva et al., 2018) developed in collaboration between",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Kawennón:nis models a pedagogicallyimportant subset of verb conjugations in XFST (Beesley and Karttunen, 2003), and currently produces 247,450 unique conjugations.",
      "startOffset" : 81,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "’ (Kazantseva et al., 2018) In user evaluations of Kawennón:nis, students often asked whether it was possible to add audio to the tool, to model the pronunciation of unfamiliar words.",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "is to normalize the input space between source and target languages by replacing one-hot encodings of text with multi-hot phonological feature encodings (Gutkin et al., 2018; Wells and Richmond, 2021).",
      "startOffset" : 153,
      "endOffset" : 200
    }, {
      "referenceID" : 47,
      "context" : "is to normalize the input space between source and target languages by replacing one-hot encodings of text with multi-hot phonological feature encodings (Gutkin et al., 2018; Wells and Richmond, 2021).",
      "startOffset" : 153,
      "endOffset" : 200
    }, {
      "referenceID" : 18,
      "context" : "speech synthesizer using Simple4All was recently developed for Plains Cree (Harrigan et al., 2019; Clark, 2014).",
      "startOffset" : 75,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "speech synthesizer using Simple4All was recently developed for Plains Cree (Harrigan et al., 2019; Clark, 2014).",
      "startOffset" : 75,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "Although it was unpublished, two highschool students created a statistical parametric speech synthesizer for Kanien’kéha by adapting eSpeak (Duddington and Dunn, 2007).",
      "startOffset" : 140,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "Elsewhere in North America, a Tacotron2 system has been built for Cherokee (Conrad, 2020), and some early work on concatenative systems for Navajo was discussed in a technical report (Whitman et al.",
      "startOffset" : 75,
      "endOffset" : 89
    }, {
      "referenceID" : 49,
      "context" : "Elsewhere in North America, a Tacotron2 system has been built for Cherokee (Conrad, 2020), and some early work on concatenative systems for Navajo was discussed in a technical report (Whitman et al., 1997), as well as on Rarámuri (Urrea et al.",
      "startOffset" : 183,
      "endOffset" : 205
    }, {
      "referenceID" : 24,
      "context" : "It is important that communities direct the development of these tools, and maintain control, ownership, and distribution rights for their data, as well as for the resulting speech synthesis models (Keegan, 2019; Brinklow, 2021).",
      "startOffset" : 198,
      "endOffset" : 228
    }, {
      "referenceID" : 5,
      "context" : "It is important that communities direct the development of these tools, and maintain control, ownership, and distribution rights for their data, as well as for the resulting speech synthesis models (Keegan, 2019; Brinklow, 2021).",
      "startOffset" : 198,
      "endOffset" : 228
    }, {
      "referenceID" : 16,
      "context" : "In 1979 the first immersion school of any Indigenous language in Canada was opened for Kanien’kéha, and many other very successful programs have been started since, including the Onkwawenna Kentyohkwa adult immersion program in 1999 (Gomashie, 2019).",
      "startOffset" : 233,
      "endOffset" : 249
    }, {
      "referenceID" : 7,
      "context" : "authors, aligned the text and audio at the utterance level using Praat (Boersma and van Heuven, 2001) and ELAN (Brugman and Russel, 2004).",
      "startOffset" : 111,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "46 minutes of audio reading isolated sentences from published and unpublished stories (Forbes et al., 2017).",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 42,
      "context" : "There have been many proposals to improve training of the attention network, for example by guiding the attention or using a CTC loss function to respect the monotonic alignment between text inputs and speech outputs (Tachibana et al., 2018; Liu et al., 2019; Zheng et al., 2019; Gölge, 2020).",
      "startOffset" : 217,
      "endOffset" : 292
    }, {
      "referenceID" : 29,
      "context" : "There have been many proposals to improve training of the attention network, for example by guiding the attention or using a CTC loss function to respect the monotonic alignment between text inputs and speech outputs (Tachibana et al., 2018; Liu et al., 2019; Zheng et al., 2019; Gölge, 2020).",
      "startOffset" : 217,
      "endOffset" : 292
    }, {
      "referenceID" : 51,
      "context" : "There have been many proposals to improve training of the attention network, for example by guiding the attention or using a CTC loss function to respect the monotonic alignment between text inputs and speech outputs (Tachibana et al., 2018; Liu et al., 2019; Zheng et al., 2019; Gölge, 2020).",
      "startOffset" : 217,
      "endOffset" : 292
    }, {
      "referenceID" : 15,
      "context" : "There have been many proposals to improve training of the attention network, for example by guiding the attention or using a CTC loss function to respect the monotonic alignment between text inputs and speech outputs (Tachibana et al., 2018; Liu et al., 2019; Zheng et al., 2019; Gölge, 2020).",
      "startOffset" : 217,
      "endOffset" : 292
    }, {
      "referenceID" : 37,
      "context" : "FastSpeech2 (Ren et al., 2021), and similar systems like FastPitch (Łańcucki, 2021), present an alternative to Tacotron2-type autoregressive systems with similar listening test results and without the characteristic errors related to attention.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 27,
      "context" : ", 2021), and similar systems like FastPitch (Łańcucki, 2021), present an alternative to Tacotron2-type autoregressive systems with similar listening test results and without the characteristic errors related to attention.",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 38,
      "context" : "In the predecessor ‘FastSpeech’, attention weights from a Tacotron2 system were used to provide phone durations (Ren et al., 2019).",
      "startOffset" : 112,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "language data sets, we trained five Tacotron2 models on incremental partitions of the LJ Speech corpus of American English (Ito and Johnson, 2017).",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "Our model is based on an open-source implementation (Chien, 2021), which adds learnable speaker embeddings and a decoder postnet to the original model, as well as predicting pitch and energy values at the phone rather than frame level.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 43,
      "context" : "We built a baseline concatenative model using Festival and Multisyn (Taylor et al., 1998; Clark et al., 2007).",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "We built a baseline concatenative model using Festival and Multisyn (Taylor et al., 1998; Clark et al., 2007).",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 50,
      "context" : "Additionally, we trained cold-start FastSpeech2 models for each language, as well as models trained in a pre-training, fine-tuning pipeline where the multilingual, multispeaker FastSpeech2 donor model was trained on a combination of VCTK (Yamagishi et al., 2019), Kanien’kéha and Gitksan recordings.",
      "startOffset" : 238,
      "endOffset" : 262
    }, {
      "referenceID" : 19,
      "context" : "This problem has parallels to the synthesis of Lombard speech (Hu et al., 2021), as used to improve intelligibility by speakers who find themselves in noisy environments.",
      "startOffset" : 62,
      "endOffset" : 79
    } ],
    "year" : 0,
    "abstractText" : "This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien’kéha, Gitksan & SENĆOŦEN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models. For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data. Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.",
    "creator" : null
  }
}
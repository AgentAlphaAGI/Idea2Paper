{
  "name" : "ARR_2022_317_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exposing the Limits of Video-Text Models through Contrast Sets",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Relating video and text modalities is one of the important goals in vision and language. Video is a complex signal where people and objects act and interact with each other through space and time. Thus correctly associating a textual description and a video requires understanding of entities, their actions and much more, making it a hard problem.\nOne of the popular ways of training and evaluating video-text models is via cross-modal matching. Often the task is formulated as a retrieval problem, where the goal is to select the correct match among many (e.g. thousand) candidates, and distractors are picked randomly (Yu et al., 2018). Another way is via multiple-choice prediction, where the goal is to pick the true match out of several (e.g. 5) candidates (Torabi et al., 2016). The latter allows for more controlled choice of negatives, which are typically selected from other videos. Commonly,\nthe retrieval setting is used during training to avoid capturing any specific multiple-choice patterns or biases, while both are used for evaluation.\nRecent methods that leverage the large-scale CLIP model (Radford et al., 2021) show significant improvement in cross-modal matching, specifically, in the retrieval setting (Fang et al., 2021; Luo et al., 2021). They outperform the prior state-of-the-art methods, often based on the Multimodal Transformer design (Miech et al., 2020; Gabeur et al., 2020; Lei et al., 2021). However, we know that often model performance is “over-estimated” due to the lack of challenging samples in evaluation. For instance, Gardner et al. (2020) show that model performance on several NLP tasks and one image-text task is much lower on contrast sets, which are test samples with small perturbation done by human experts in a way that changes the gold label.\nIn this work, we are investigating whether the video-text models also struggle in an evaluation framework that probes them with hard negatives. Instead of using human-designed contrast sets that are not easily scalable, we propose an automated pipeline that can generate contrast sets via verb and human entity manipulation. Our manipulations are carefully designed to preserve fluency but change\nsemantics of the textual descriptions, making them invalid for a given video. We focus on entities and verbs to evaluate if the model can truly understand “who did what\" in a video. Inspired by (Li et al., 2020; Morris et al., 2020), we leverage a generative T5 language model (Raffel et al., 2020) to manipulate the verb phrase and use heuristics to swap person entities. Note that our pipeline does not require a trained video-text model in the loop.\nWe apply our automatic manipulations to two popular video-text benchmarks, MSR-VTT (Xu et al., 2016) and LSMDC (Rohrbach et al., 2017). We additionally collect human generated contrast sets to compare with our automatic ones. To make sure that our automatic negatives are of high quality, we also confirm that humans can successfully select the correct description for a given video with our hard negatives. Finally, we benchmark several video-text models on our contrast sets. We find that all methods degrade in performance with the introduction of hard negatives in the multiplechoice setting (Figure 1). This includes the recent CLIP-based works that demonstrated large gains in the retrieval setting. This shows that all methods have difficulty discriminating between entities and verbs when the remaining context is unchanged. We observe that model performance drops especially on cases such as verb antonym swaps, where fine-grained action understanding is important."
    }, {
      "heading" : "2 Related Work",
      "text" : "Defending and generating adversarial examples (Jia et al., 2019; Jin et al., 2020) have been mostly explored in NLP since the reign of pre-trained language models (LMs) (Devlin et al., 2019). Li et al. (2020); Garg and Ramakrishnan (2020); Morris et al. (2020) show that substituting words in a sentence with masked LMs (Devlin et al., 2019; Liu et al., 2019) can successfully mislead the classification and entailment model predictions to be incorrect. Template-based (McCoy et al., 2019; Glockner et al., 2018) and manually crafted (Gardner et al., 2020) perturbations on evaluation datasets have also been studied for textual entailment.\nLanguage-based adversarial examples can be collected to study the robustness of vision-language models as well. Shekhar et al. (2017) introduces FOIL-COCO dataset to evaluate the visionlanguage model’s decision when associating images with both correct and \"foil\" captions. Hendricks and Nematzadeh (2021) show that vision-\nlanguage Transformers are worse at verb understanding than nouns. New versions of the VQA dataset (Antol et al., 2015) are proposed to study robustness of VQA models (Shah et al., 2019; Li et al., 2021). Our work is different in that we use pre-trained LMs to introduce perturbations and evaluate robustness of video-language models."
    }, {
      "heading" : "3 Designing Contrast Sets",
      "text" : "In this section we present our approach to automatically constructing text-based contrast sets for videolanguage tasks. Suppose we are given a video Vi and description si. Contrast sets Ĉi = {ŝ1, ..., ŝi} are designed such that ŝi is semantically inconsistent with Vi and yet models incorrectly select ŝi over si in a video-to-text multiple-choice setting. While there are different ways to create valid Ĉi, we investigate manipulating 1) person entities and 2) verb phrases in the original descriptions. Qualitative examples of Ĉi are shown in the Appendix."
    }, {
      "heading" : "3.1 Contrast sets for Person Entities",
      "text" : "First, we investigate swapping the name or identity of a person. The LSMDC dataset (Rohrbach et al., 2017; Park et al., 2020) includes movie descriptions with character identities (e.g. Harry Potter), and a list of characters present in each movie along with their gender. We replace each character’s identity with one from the same movie and with the same gender, to prevent the language statistics alone from detecting the swapped IDs.\nFor the MSR-VTT dataset (Xu et al., 2016) we do not have the identities, however 80% of videos have gender cues in the descriptions. Thus the contrast sets are created by swapping the gender of a person mentioned in a sentence and the corresponding pronouns (e.g., A woman is pushing her stroller → A man is pushing his stroller). This is done with a template that maps gender-sensitive words and pronouns to their counterparts (see Appendix)."
    }, {
      "heading" : "3.2 Contrast Sets for Verb Phrases using Language Models",
      "text" : "The above rule-based strategies cannot be directly translated to create contrast sets for verb phrases: 1) a substitute verb phrase is not guaranteed to be inconsistent with a video, and 2) the sentence may look unnatural and no longer be textually plausible. Based on their success in adversarial attack generation (Li et al., 2020; Garg and Ramakrishnan, 2020; Morris et al., 2020), we instead leverage pre-\ntrained language models (LMs) to automatically manipulate the verb phrases.\nWe identify verb phrases in a sentence using Spacy (Honnibal and Montani, 2017), replace them with a mask token [MASK], and select top K phrases that best fit the mask token using probability scores from a LM. Different from prior work (Li et al., 2020), we use T5-base model (Raffel et al., 2020) instead of masked language models (Devlin et al., 2019; Liu et al., 2019) to easily support generating multi-word candidates. We additionally finetune T5 to learn verb phrases in the downstream training data with unsupervised denoising objective (Raffel et al., 2020). This is done to mitigate the distribution shift between ground truth and generated descriptions.\nWe then filter the K sentence candidates with the following criteria: 1) There is no verb in the sentence. 2) Verbs are rare or unseen in training descriptions. 3) The sentence has a high perplexity obtained by GPT2-XL (Radford et al., 2019) to ensure grammaticality and plausibility (Morris et al., 2020). Lastly, we check that the semantics of a candidate is inconsistent with the original sentence. This is when a) the candidate verb is an antonym1 of original verb, or b) the word embedding (Mrkšić et al., 2016) of candidate and original verb and their sentence encodings (Reimers and Gurevych, 2019) both have low cosine similarity scores."
    }, {
      "heading" : "3.3 Human-Generated Verb Contrast Sets",
      "text" : "Are language models capable of generating contrast sets of good quality? To answer this question, we follow the original contrast sets work (Gardner et al., 2020), and create negatives manually to see if the performance on machine and human generated contrast sets is similar. We use the Amazon Mechanical Turk (AMT) platform and ask workers to modify a verb phrase such that a sentence becomes inconsistent with a video (see Appendix)."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Multiple Choice Design",
      "text" : "MSR-VTT (Xu et al., 2016) is composed of 10K YouTube videos each paired with 20 natural descriptions and is typically evaluated on retrieval performance with 1000 video text pairs as candidates in the test set. The multiple choice version (Yu et al., 2018) has 2,990 test videos as queries, and a positive caption with 4 random captions from\n1Extracted using VerbNet (Schuler, 2005).\nother videos as 5 answer options. We label this split as the Random MC. We design another MC problem by replacing one negative option with one from our contrast sets. In particular, Gender MC swaps gender in an original sentence; VerbLM MC and VerbH MC include verb-based negatives generated by our approach and by humans.\nLSMDC (Rohrbach et al., 2017) includes short movie clips and captions. Characters in these captions are labeled as SOMEONE and we cannot construct contrast sets for person-entities. We instead use captions in (Park et al., 2020) that include the character identities. We create a new training/test split using the same movies in training and test so that the test identities have been seen during training. We call this modified dataset LSMDC-IDs. Using this set, Random MC is newly defined with 4 negative captions drawn randomly from different clips of the same movie. ID MC swaps the character IDs (Section 3.1) as negatives, and Verb MC includes the verb contrast sets, as before."
    }, {
      "heading" : "4.2 Video-Text Models and Evaluation",
      "text" : "We benchmark Transformer (Vaswani et al., 2017) based video-language models in our experiments. Multi Modal Transformer (MMT) (Gabeur et al., 2020) learns the joint representation between text and multiple modalities in videos. CLIP-Straight (Portillo-Quintero et al., 2021) applies frozen CLIP features (Radford et al., 2021) for zero-shot prediction. Inspired by Dzabraev et al. (2021), we also extend MMT to take frozen CLIP features as input, which we denote as MMT-CLIP. CLIP4CLIP (Luo et al., 2021) and CLIP2Video (Fang et al., 2021) directly finetune CLIP with temporal pooler and are the state-of-the-art in retrieval tasks. ViT-B/32 model is used for CLIP experiemnts, see Appendix C for more implementation details. We train the above models with a contrastive loss to learn the joint video-text representation. In MC settings, we mark it as correct, if a ground truth sentence is scored the highest. We also report video-to-text (V → T) Recall@1 for retrieval evaluation."
    }, {
      "heading" : "4.3 Results",
      "text" : "Table 1 shows results on the MSR-VTT dataset. In video-to-text retrieval, we see a significant gap in performance between the CLIP-finetuned models and all other models; even CLIP-Straight outperforms MMT in this metric. Next, we see that Random MC is nearly solved by almost all models. However there is a significant drop in performance\nacross all models when evaluated on contrastset based MC. Interestingly, the performance gap between MMT and the finetuned CLIP models with high retrieval performance (CLIP4CLIP and CLIP2Video) is gone in this setting, meaning stronger retrieval performance does not guarantee robustness to word-level manipulations. We also observe that models with frozen CLIP features perform better on Gender MC than Verb MC, and finetuning the CLIP features on video-language task can make the model less sensitive to gender information. Finally, to verify that the automated verbbased contrast sets are valid, we note that: models on VerbLM MC perform on par with the human produced ones VerbH MC, and humans maintain accuracy greater than 90% on both contrast sets.2\nTable 2 presents results on the LSMDC-IDs dataset. Overall, it is more challenging than MSRVTT, as it often requires more fine-grained understanding. Similar to MSR-VTT, models obtain lower performance on contrast-set MC designs. While we see that models found VerbLM MC to be more difficult than VerbH MC, our automated contrast sets are valid as humans still perform above 90% for both cases. We also notice that the ID swaps are easier than the verb swaps, and CLIP features are helpful in distinguishing character IDs (MMT vs. MMT-CLIP). Table 6 in Appendix shows that model accuracy drops by at least 13.9% when the “negative” IDs appear more frequently in\n2We report majority vote over 3 human judges.\nthe training data than the original IDs, meaning the models struggle to identify IDs in the long-tail.\nDoes Semantic Proximity of Verb Contrast Sets Affect Model Accuracy? To answer this, we use off-the-shelf sentence embeddings to measure the semantic proximity b.w. original and hard negative sentences, and select the subsets of the data with the highest and lowest 15% according to these scores (see examples in the Appendix). In Table 3, we see that models can achieve accuracy greater than 93% on semantically different examples (Diff.) as measured by SentBERT, i.e., on par with humans. However for contrast sets with high semantic similarity (Sim.), model performance is much lower, while human performance is not affected (e.g. CLIP4CLIP drops to 71.8% and humans maintain 92.7% accuracy on SentBERT Sim.). We found that many contrast sets in this subset include antonyms of the original verbs (e.g. pulling vs. pushing).3 Distinguishing such antonyms requires fine-grained understanding of actions, which SOTA video-language models fail to demonstrate."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We present a pipeline to build automatic contrast sets for video and language tasks, focused on manipulating person entities and verb phrases. We show that models struggle on contrast sets compared to random negatives, and stronger retrieval models do not show better robustness to hard negatives. For verb contrast sets, we find that model performance is strongly correlated with semantic proximity, unlike humans. We leave it as future work to use automatic contrast sets in training to improve model robustness, and designing contrast sets for different concepts/parts of speech.\n3Recall from Section 3 that we do not apply similarity threshold for antonyms."
    }, {
      "heading" : "6 Ethical Considerations",
      "text" : "Our goal is to diagnose performance of videolanguage models on hard negative samples w.r.t. verbs and person entities. Overall, we envision positive impact from this work, as it aims to expose limitations of the existing models. Some of our entity swaps focus on apparent gender (as described by humans in the video-text datasets), but we do not predict biological sex or gender identity. We construct our verb-focused contrast sets automatically, using a large generative language model, thus potentially some biases present in such a model could propagate into our hard negative samples. Practitioners who wish to use our contrast sets should be mindful of such sources of bias."
    }, {
      "heading" : "A Contrast Set Construction",
      "text" : "Here, we provide more details on construction of each contrast set.\nA.1 Gender Contrast Sets\nTable 4 shows the mapping of gender-sensitive words. We use these rules to swap only a single word in the sentence. This is to guarantee that swapping gender leads to different semantics (e.g. man and woman walk together −→ woman and man walk together both apply to the same video if all words are swapped). If there are more than one possible mappings, we randomly sample one from a uniform distribution. Lastly, we swap all gendersensitive pronouns that have the same gender as original noun. These contrast sets are used for the MSR-VTT dataset (Xu et al., 2016).\nA.2 Person ID Contrast Sets\nThe first character ID in a sentence is replaced by a different character ID that appears in the same movie and has the same gender. Among all the candidates, the manipulated ID is sampled from a uniform distribution. The following character IDs in the same sentence have uniform chance of being kept or swapped using the same strategy. These contrast sets are used for the LSMDC-IDs dataset.\nA.3 Verb Contrast Sets\nAttack Selection We use Spacy to get the POS tags, and find verb phrases that match a list of predefined patterns (verb; verb + preposition).\nCandidate Generation We use T5 model and performed beam search (beam size = 50) to generate K = 50 multi-word candidates.\nCandidate Constraints We keep a candidate if the lemmatized verbs 4 in it appeared more than 30 times in the training set. For fluency, we calculate perplexity score of original and manipulated sentence using GPT2-XL (Radford et al., 2019), which we call pplo and pplm. We calculate the normalized difference of perplexity scores ppldiff = pplo−pplm pplo to remove a candidate that is less plausible than the original. Specifically, candidates are kept if ppldiff < 0.6, or ppldiff < 1.4∩pplm < 750. Lastly, the semantic inconsistency constraints are satisfied if the word embedding (Mrkšić et al., 2016) of the lemmatized verbs in the candidate and original sentence have cosine similarity score lower than 0.4, and the sentence embeddings (Reimers and Gurevych, 2019) have cosine similarity score lower than 0.8."
    }, {
      "heading" : "B Contrast Set Examples",
      "text" : "Random examples of automatically constructed contrast sets using descriptions from MSR-VTT and LSMDC-IDs datasets are shown in Table 5.\nWe also illustrate the top/bottom 10% (Sim./Diff.) according to SentBERT similarity, as discussed in the main paper. A few examples from each subset are shown in Figure 2.\nC Implementation Details\n• MMT (Gabeur et al., 2020): We use the following features extracted from video5: motion from S3D (Xie et al., 2018), audio from VGGish (Hershey et al., 2017), scene embeddings, face, OCR, Speech, and Appearance. We refer to Miech et al. (2018); Gabeur et al. (2020) for more details about the features.\nFor MSR-VTT, we use the released checkpoint from their code6, which is pre-trained on HowTo100M dataset (Miech et al., 2019) and further finetuned on MSR-VTT.\nFor LSMDC-IDs which needs re-training, we used their finetuning code for LSMDC dataset (Rohrbach et al., 2017). The model is trained with max margin ranking loss on 1 Nvidia RTX-6000 GPU for 12 hours. Hyperparameter search was done to find margin of 0.05, batch size of 32, and Adam opti-\n4https://www.nltk.org/_modules/nltk/ stem/wordnet.html\n5https://github.com/albanie/ collaborative-experts\n6https://github.com/gabeur/mmt\nDataset Original Person Entity Verb Phrase\nMSRVTT Two men are doing wrestling. Two women are doing wrestling. Two men are dancing. A man in black shirt is talking with his two friends. A woman in black shirt is talking with her two friends. A man in black shirt is running with his two friends. LSMDC-ID His gaze steely, Jenko lowers his gun. His gaze steely, Schmidt lowers his gun. His gaze steely, Jenko raises his gun.\nJenko and Schmidt sit in the rear pew. Zach and Schmidt sit in the rear pew. Jenko and Schmidt stand in the rear pew.\n• CLIP2Video (Fang et al., 2021): We used the released checkpoint on MSR-VTT using their code base8. This model is not used for LSMDC-IDs because finetuning code was not provided. CLIP model is initialized with ViTB-32 weights."
    }, {
      "heading" : "D Multiple Choice Details",
      "text" : "Here we provide more details about our evaluation data. Note, that we use 5 text candidates (1 positive and 4 negative) for all multiple choice (MC) settings.\nD.1 MSR-VTT\nWe use the standard train/val/test split in MSRVTT dataset (Xu et al., 2016).\n• Retrieval: 1,000 ground truth video-text pairs in the test set (Yu et al., 2018).\n• Random MC: 2,990 videos and all negative options are drawn randomly from other videos (Yu et al., 2018).\n• Gender MC: 2,477 video-text instances. Using the original descriptions from Random MC, a single negative is drawn from gender contrast sets to replace one of the options in Random MC (the remaining 3 are kept). Note, that not all videos involved people or contained gender-sensitive words in descriptions, hence some instances are filtered.\n• VerbLM MC: 2,554 video-text instances. Constructed using the same strategy as in Gender MC but a single negative is drawn from verb contrast sets generated by language models. Instances are filtered when there are no valid verb contrast sets satisfying constraints in Section A.3.\n• VerbH MC: 2,554 video-text instances. We use the instances in VerbLM MC, and a negative is drawn from human designed verb contrast sets.\nD.2 LSDMC-IDs\nWe define a new split using LSMDC descriptions with character IDs (proper names) (Park et al., 2020). Note, that Rohrbach et al. (2017); Park et al. (2020) use development and test sets where videos come from distinct movies than the training\n8https://github.com/CryhanFang/ CLIP2Video\ndata, meaning that IDs in test data are not seen in training. To overcome this issue, we split their training descriptions into 80%/10%/10%/ ratio to create new training/validation/test sets that share the same movies and identities across splits.\n• Retrieval: 7,010 ground truth video-text pairs.\n• Random MC: 7,010 videos, negative text options drawn randomly from different videos but the same movie.\n• ID MC: 7,010 video-text instances. We replace one negative in Random MC with the one from ID contrast sets.\n• VerbLM MC: 7,010 video-text instances. We replace one negative in Random MC with one from the language model generated verb contrast sets.\n• VerbH MC: 3,500 video-text instances. We replace one negative in Random MC with one from the human designed verb contrast sets (we only crowdsourced 3,500 instances)."
    }, {
      "heading" : "E Human Annotation Details",
      "text" : "We ran two different human annotations, one to evaluate our VerbLM MC and another to manually design verb contrast sets. Figures 3 and 4 show the respective HIT UIs. We use Amazon Mechanical Turk interface to get a pool of annotators from native Enlgish speaking countries and with high approval rate, and pay them $15 hour on average which is above a minimum wage."
    }, {
      "heading" : "F Dataset Details",
      "text" : "We include additional information on the MSRVTT (Xu et al., 2016) and LSMDC (Rohrbach et al., 2017) datasets. MSR-VTT contains diverse YouTube videos and corresponding crowdsourced\ndescriptions in English language. LSMDC contains movie clips and associated descriptions from scripts or Audio Description, also in English. Both datasets are distributed for research use. The license, personally identifiable information (PII), and consent details of each dataset are in the respective papers. Since LSMDC contains clips from movies, some may contain nudity or violence, etc."
    } ],
    "references" : [ {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "ArXiv, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Mdmmt: Multidomain multimodal transformer for video retrieval",
      "author" : [ "Maksim Dzabraev", "Maksim Kalashnikov", "Stepan Alekseevich Komkov", "Aleksandr Petiushko." ],
      "venue" : "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "citeRegEx" : "Dzabraev et al\\.,? 2021",
      "shortCiteRegEx" : "Dzabraev et al\\.",
      "year" : 2021
    }, {
      "title" : "Clip2video: Mastering video-text retrieval via image clip",
      "author" : [ "Han Fang", "Pengfei Xiong", "Luhui Xu", "Yu Chen." ],
      "venue" : "arXiv preprint arXiv:2106.11097.",
      "citeRegEx" : "Fang et al\\.,? 2021",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-modal transformer for video retrieval",
      "author" : [ "Valentin Gabeur", "Chen Sun", "Karteek Alahari", "Cordelia Schmid." ],
      "venue" : "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part IV 16, pages 214–229.",
      "citeRegEx" : "Gabeur et al\\.,? 2020",
      "shortCiteRegEx" : "Gabeur et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "Matt Gardner", "Yoav Artzi", "Victoria Basmova", "Jonathan Berant", "Ben Bogin", "Sihao Chen", "Pradeep Dasigi", "Dheeru Dua", "Yanai Elazar", "Ananth Gottumukkala" ],
      "venue" : "In Findings of the Association",
      "citeRegEx" : "Gardner et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2020
    }, {
      "title" : "Bae: Bert-based adversarial examples for text classification",
      "author" : [ "Siddhant Garg", "Goutham Ramakrishnan." ],
      "venue" : "ArXiv, abs/2004.01970.",
      "citeRegEx" : "Garg and Ramakrishnan.,? 2020",
      "shortCiteRegEx" : "Garg and Ramakrishnan.",
      "year" : 2020
    }, {
      "title" : "Breaking nli systems with sentences that require simple lexical inferences",
      "author" : [ "Max Glockner", "Vered Shwartz", "Yoav Goldberg." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Glockner et al\\.,? 2018",
      "shortCiteRegEx" : "Glockner et al\\.",
      "year" : 2018
    }, {
      "title" : "Probing image-language transformers for verb understanding",
      "author" : [ "Lisa Anne Hendricks", "Aida Nematzadeh." ],
      "venue" : "arXiv preprint arXiv:2106.09141.",
      "citeRegEx" : "Hendricks and Nematzadeh.,? 2021",
      "shortCiteRegEx" : "Hendricks and Nematzadeh.",
      "year" : 2021
    }, {
      "title" : "Cnn architectures for large",
      "author" : [ "Shawn Hershey", "Sourish Chaudhuri", "Daniel P.W. Ellis", "Jort F. Gemmeke", "Aren Jansen", "R. Channing Moore", "Manoj Plakal", "Devin Platt", "Rif A. Saurous", "Bryan Seybold", "Malcolm Slaney", "Ron J. Weiss", "Kevin W. Wilson" ],
      "venue" : null,
      "citeRegEx" : "Hershey et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hershey et al\\.",
      "year" : 2017
    }, {
      "title" : "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani." ],
      "venue" : "To appear.",
      "citeRegEx" : "Honnibal and Montani.,? 2017",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Certified robustness to adversarial word substitutions",
      "author" : [ "Robin Jia", "Aditi Raghunathan", "Kerem Göksel", "Percy Liang." ],
      "venue" : "EMNLP/IJCNLP (1).",
      "citeRegEx" : "Jia et al\\.,? 2019",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits" ],
      "venue" : null,
      "citeRegEx" : "Jin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Less is more: Clipbert for video-and-language learning via sparse sampling",
      "author" : [ "Jie Lei", "Linjie Li", "Luowei Zhou", "Zhe Gan", "Tamara L Berg", "Mohit Bansal", "Jingjing Liu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
      "citeRegEx" : "Lei et al\\.,? 2021",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2021
    }, {
      "title" : "Adversarial vqa: A new benchmark for evaluating the robustness of vqa models",
      "author" : [ "Linjie Li", "Jie Lei", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "ArXiv, abs/2106.00245.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Bert-attack: Adversarial attack against bert using bert",
      "author" : [ "Linyang Li", "Ruotian Ma", "Qipeng Guo", "Xiangyang Xue", "Xipeng Qiu." ],
      "venue" : "The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Clip4clip: An empirical study of clip for end to end video clip retrieval",
      "author" : [ "Huaishao Luo", "Lei Ji", "Ming Zhong", "Yang Chen", "Wen Lei", "Nan Duan", "Tianrui Li." ],
      "venue" : "arXiv preprint arXiv:2104.08860.",
      "citeRegEx" : "Luo et al\\.,? 2021",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2021
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end learning of visual representations from uncurated instructional videos",
      "author" : [ "Antoine Miech", "Jean-Baptiste Alayrac", "Lucas Smaira", "Ivan Laptev", "Josef Sivic", "Andrew Zisserman." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-",
      "citeRegEx" : "Miech et al\\.,? 2020",
      "shortCiteRegEx" : "Miech et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning a text-video embedding from incomplete and heterogeneous data",
      "author" : [ "Antoine Miech", "Ivan Laptev", "Josef Sivic." ],
      "venue" : "arXiv preprint arXiv:1804.02516.",
      "citeRegEx" : "Miech et al\\.,? 2018",
      "shortCiteRegEx" : "Miech et al\\.",
      "year" : 2018
    }, {
      "title" : "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
      "author" : [ "Antoine Miech", "Dimitri Zhukov", "Jean-Baptiste Alayrac", "Makarand Tapaswi", "Ivan Laptev", "Josef Sivic." ],
      "venue" : "2019 IEEE/CVF International Conference on",
      "citeRegEx" : "Miech et al\\.,? 2019",
      "shortCiteRegEx" : "Miech et al\\.",
      "year" : 2019
    }, {
      "title" : "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
      "author" : [ "John X. Morris", "Eli Lifland", "Jin Yong Yoo", "Jake Grigsby", "Di Jin", "Yanjun Qi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Morris et al\\.,? 2020",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2020
    }, {
      "title" : "Counter-fitting word vectors to linguistic constraints",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Blaise Thomson", "Milica Gašić", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young." ],
      "venue" : "Proceedings of HLT-NAACL.",
      "citeRegEx" : "Mrkšić et al\\.,? 2016",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2016
    }, {
      "title" : "Identity-aware multi-sentence video description",
      "author" : [ "Jae Sung Park", "Trevor Darrell", "Anna Rohrbach." ],
      "venue" : "European Conference on Computer Vision, pages 360–378. Springer.",
      "citeRegEx" : "Park et al\\.,? 2020",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2020
    }, {
      "title" : "A straightforward framework for video retrieval using clip",
      "author" : [ "Jesús Andrés Portillo-Quintero", "José Carlos OrtizBayliss", "Hugo Terashima-Marín" ],
      "venue" : null,
      "citeRegEx" : "Portillo.Quintero et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Portillo.Quintero et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning transferable visual models from natural language",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Movie description",
      "author" : [ "Anna Rohrbach", "Atousa Torabi", "Marcus Rohrbach", "Niket Tandon", "Chris Pal", "Hugo Larochelle", "Aaron Courville", "Bernt Schiele." ],
      "venue" : "International Journal of Computer Vision.",
      "citeRegEx" : "Rohrbach et al\\.,? 2017",
      "shortCiteRegEx" : "Rohrbach et al\\.",
      "year" : 2017
    }, {
      "title" : "VerbNet: A broadcoverage, comprehensive verb lexicon",
      "author" : [ "Karin Kipper Schuler." ],
      "venue" : "University of Pennsylvania.",
      "citeRegEx" : "Schuler.,? 2005",
      "shortCiteRegEx" : "Schuler.",
      "year" : 2005
    }, {
      "title" : "Cycle-consistency for robust visual question answering",
      "author" : [ "Meet Shah", "Xinlei Chen", "Marcus Rohrbach", "Devi Parikh." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6642–6651.",
      "citeRegEx" : "Shah et al\\.,? 2019",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2019
    }, {
      "title" : "Foil it! find one mismatch between image and language caption",
      "author" : [ "Ravi Shekhar", "Sandro Pezzelle", "Yauhen Klimovich", "Aurélie Herbelot", "Moin Nabi", "Enver Sangineto", "Raffaella Bernardi." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for",
      "citeRegEx" : "Shekhar et al\\.,? 2017",
      "shortCiteRegEx" : "Shekhar et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning language-visual embedding for movie understanding with natural-language",
      "author" : [ "Atousa Torabi", "Niket Tandon", "Leon Sigal." ],
      "venue" : "arXiv:1609.08124.",
      "citeRegEx" : "Torabi et al\\.,? 2016",
      "shortCiteRegEx" : "Torabi et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam M. Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification",
      "author" : [ "Saining Xie", "Chen Sun", "Jonathan Huang", "Zhuowen Tu", "Kevin Murphy" ],
      "venue" : null,
      "citeRegEx" : "Xie et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2018
    }, {
      "title" : "Msrvtt: A large video description dataset for bridging video and language",
      "author" : [ "Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288–5296.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "A joint sequence fusion model for video question answering and retrieval",
      "author" : [ "Youngjae Yu", "Jongseok Kim", "Gunhee Kim." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 471–487.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "thousand) candidates, and distractors are picked randomly (Yu et al., 2018).",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 38,
      "context" : "Figure 1: Samples of our video-to-text tasks on the MSR-VTT (Xu et al., 2016) and LSMDC dataset (Rohrbach et al.",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Two SOTA methods MMT (Gabeur et al., 2020) and CLIP4CLIP (Luo et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : ", 2020) and CLIP4CLIP (Luo et al., 2021) incorrectly choose the manipulated sentence (option B) in both these cases.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "Recent methods that leverage the large-scale CLIP model (Radford et al., 2021) show significant improvement in cross-modal matching, specifically, in the retrieval setting (Fang et al.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : ", 2021) show significant improvement in cross-modal matching, specifically, in the retrieval setting (Fang et al., 2021; Luo et al., 2021).",
      "startOffset" : 101,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : ", 2021) show significant improvement in cross-modal matching, specifically, in the retrieval setting (Fang et al., 2021; Luo et al., 2021).",
      "startOffset" : 101,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "They outperform the prior state-of-the-art methods, often based on the Multimodal Transformer design (Miech et al., 2020; Gabeur et al., 2020; Lei et al., 2021).",
      "startOffset" : 101,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "They outperform the prior state-of-the-art methods, often based on the Multimodal Transformer design (Miech et al., 2020; Gabeur et al., 2020; Lei et al., 2021).",
      "startOffset" : 101,
      "endOffset" : 160
    }, {
      "referenceID" : 14,
      "context" : "They outperform the prior state-of-the-art methods, often based on the Multimodal Transformer design (Miech et al., 2020; Gabeur et al., 2020; Lei et al., 2021).",
      "startOffset" : 101,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "Inspired by (Li et al., 2020; Morris et al., 2020), we leverage a generative T5 language model (Raffel et al.",
      "startOffset" : 12,
      "endOffset" : 50
    }, {
      "referenceID" : 23,
      "context" : "Inspired by (Li et al., 2020; Morris et al., 2020), we leverage a generative T5 language model (Raffel et al.",
      "startOffset" : 12,
      "endOffset" : 50
    }, {
      "referenceID" : 29,
      "context" : ", 2020), we leverage a generative T5 language model (Raffel et al., 2020) to manipulate the verb phrase and use heuristics to",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 38,
      "context" : "We apply our automatic manipulations to two popular video-text benchmarks, MSR-VTT (Xu et al., 2016) and LSMDC (Rohrbach et al.",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "Defending and generating adversarial examples (Jia et al., 2019; Jin et al., 2020) have been mostly explored in NLP since the reign of pre-trained language models (LMs) (Devlin et al.",
      "startOffset" : 46,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "Defending and generating adversarial examples (Jia et al., 2019; Jin et al., 2020) have been mostly explored in NLP since the reign of pre-trained language models (LMs) (Devlin et al.",
      "startOffset" : 46,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : ", 2020) have been mostly explored in NLP since the reign of pre-trained language models (LMs) (Devlin et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "(2020) show that substituting words in a sentence with masked LMs (Devlin et al., 2019; Liu et al., 2019) can successfully mislead the classification and entailment model predictions to be incorrect.",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "(2020) show that substituting words in a sentence with masked LMs (Devlin et al., 2019; Liu et al., 2019) can successfully mislead the classification and entailment model predictions to be incorrect.",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "Template-based (McCoy et al., 2019; Glockner et al., 2018) and manually crafted (Gardner et al.",
      "startOffset" : 15,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Template-based (McCoy et al., 2019; Glockner et al., 2018) and manually crafted (Gardner et al.",
      "startOffset" : 15,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : ", 2018) and manually crafted (Gardner et al., 2020) perturbations on evaluation datasets have also been studied for textual entailment.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "New versions of the VQA dataset (Antol et al., 2015) are proposed to study robustness of VQA models (Shah et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 33,
      "context" : ", 2015) are proposed to study robustness of VQA models (Shah et al., 2019; Li et al., 2021).",
      "startOffset" : 55,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : ", 2015) are proposed to study robustness of VQA models (Shah et al., 2019; Li et al., 2021).",
      "startOffset" : 55,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : "The LSMDC dataset (Rohrbach et al., 2017; Park et al., 2020) includes movie descriptions with character identities (e.",
      "startOffset" : 18,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "The LSMDC dataset (Rohrbach et al., 2017; Park et al., 2020) includes movie descriptions with character identities (e.",
      "startOffset" : 18,
      "endOffset" : 60
    }, {
      "referenceID" : 38,
      "context" : "For the MSR-VTT dataset (Xu et al., 2016) we do not have the identities, however 80% of videos have gender cues in the descriptions.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Based on their success in adversarial attack generation (Li et al., 2020; Garg and Ramakrishnan, 2020; Morris et al., 2020), we instead leverage pre-",
      "startOffset" : 56,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "Based on their success in adversarial attack generation (Li et al., 2020; Garg and Ramakrishnan, 2020; Morris et al., 2020), we instead leverage pre-",
      "startOffset" : 56,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "Based on their success in adversarial attack generation (Li et al., 2020; Garg and Ramakrishnan, 2020; Morris et al., 2020), we instead leverage pre-",
      "startOffset" : 56,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "We identify verb phrases in a sentence using Spacy (Honnibal and Montani, 2017), replace them with a mask token [MASK], and select top K phrases that best fit the mask token using probability scores from a LM.",
      "startOffset" : 51,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "Different from prior work (Li et al., 2020), we use T5-base model (Raffel et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 29,
      "context" : ", 2020), we use T5-base model (Raffel et al., 2020) instead of masked language models (Devlin et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : ", 2020) instead of masked language models (Devlin et al., 2019; Liu et al., 2019) to easily support generating multi-word candidates.",
      "startOffset" : 42,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : ", 2020) instead of masked language models (Devlin et al., 2019; Liu et al., 2019) to easily support generating multi-word candidates.",
      "startOffset" : 42,
      "endOffset" : 81
    }, {
      "referenceID" : 29,
      "context" : "We additionally finetune T5 to learn verb phrases in the downstream training data with unsupervised denoising objective (Raffel et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "3) The sentence has a high perplexity obtained by GPT2-XL (Radford et al., 2019) to ensure grammaticality and plausibility (Morris et al.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : ", 2019) to ensure grammaticality and plausibility (Morris et al., 2020).",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "This is when a) the candidate verb is an antonym1 of original verb, or b) the word embedding (Mrkšić et al., 2016) of candidate and original verb and their sentence encodings (Reimers and Gurevych, 2019) both have low cosine similarity scores.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 30,
      "context" : ", 2016) of candidate and original verb and their sentence encodings (Reimers and Gurevych, 2019) both have low cosine similarity scores.",
      "startOffset" : 68,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Are language models capable of generating contrast sets of good quality? To answer this question, we follow the original contrast sets work (Gardner et al., 2020), and create negatives manually to see if the performance on machine and human generated contrast sets is similar.",
      "startOffset" : 140,
      "endOffset" : 162
    }, {
      "referenceID" : 38,
      "context" : "MSR-VTT (Xu et al., 2016) is composed of 10K YouTube videos each paired with 20 natural descriptions and is typically evaluated on retrieval performance with 1000 video text pairs as candidates in the test set.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 39,
      "context" : "The multiple choice version (Yu et al., 2018) has 2,990 test videos as queries, and a positive caption with 4 random captions from",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 31,
      "context" : "LSMDC (Rohrbach et al., 2017) includes short movie clips and captions.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 25,
      "context" : "We instead use captions in (Park et al., 2020) that include the character identities.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 36,
      "context" : "We benchmark Transformer (Vaswani et al., 2017) based video-language models in our experiments.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "Multi Modal Transformer (MMT) (Gabeur et al., 2020) learns the joint representation between text and multiple modalities in videos.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : "CLIP-Straight (Portillo-Quintero et al., 2021) applies frozen CLIP features (Radford et al.",
      "startOffset" : 14,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : ", 2021) applies frozen CLIP features (Radford et al., 2021) for zero-shot prediction.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "CLIP4CLIP (Luo et al., 2021) and CLIP2Video (Fang et al.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : ", 2021) and CLIP2Video (Fang et al., 2021) directly finetune CLIP with temporal pooler and are the state-of-the-art in retrieval tasks.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 30,
      "context" : "Similarity scores are calculated using: SentBERT in (Reimers and Gurevych, 2019) and zero shot CLIP (Radford et al.",
      "startOffset" : 52,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : "Similarity scores are calculated using: SentBERT in (Reimers and Gurevych, 2019) and zero shot CLIP (Radford et al., 2021) text embedding.",
      "startOffset" : 100,
      "endOffset" : 122
    } ],
    "year" : 0,
    "abstractText" : "Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a pre-trained language model and a set of heuristics to create verb and person entity focused contrast sets. We apply these in the multiple choice video-totext classification setting. We test the robustness of recent methods on the proposed automatic contrast sets, and compare them to additionally collected human-generated counterparts, to assess their effectiveness. We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods.",
    "creator" : null
  }
}
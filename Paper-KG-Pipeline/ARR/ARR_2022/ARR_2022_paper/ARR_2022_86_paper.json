{
  "name" : "ARR_2022_86_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "International Classification of Diseases (ICD) is a classification and terminology that provides diagnostic codes with descriptions for diseases1. The task of ICD coding refers to assigning ICD codes to electronic medical records (EMRs) which is highly related to clinical tasks or systems including patient similarity learning (Suo et al., 2018), medical billing (Sonabend et al., 2020), and clinical decision support systems (Sutton et al., 2020). Traditionally, healthcare organizations have to employ specialized coders for this task, which is expensive, time-consuming, and error-prone. As a result, many methods have been proposed for automatic ICD coding since the 1990s (de Lima et al., 1998).\nDeep learning methods usually treat this task as a multi-label classification problem (Xie and Xing, 2018; Li and Yu, 2020; Zhou et al., 2021), which learn deep representations of EMRs with\n1who.int/standards/classifications/ classification-of-diseases\nan RNN or CNN encoder and then predict codes with a multi-label classifier. Recent state-of-the-art methods propose label attention that uses the code representations as attention queries to extract the code-related representations2 (Mullenbach et al., 2018). Following this idea, many works further propose using code hierarchical structures (Falis et al., 2019; Xie et al., 2019; Cao et al., 2020) and descriptions (Cao et al., 2020; Song et al., 2020) for better label representations.\nIn this work, we argue that the synonyms of codes can provide more comprehensive information. For example, the description of code 244.9 is “Unspecified hypothyroidism” in ICD. However, this code can be described in different forms in EMRs such as “low t4” and “subthyroidism”. Fortunately, these different expressions can be found in the Unified Medical Language System (Bodenreider, 2004), a repository of biomedical vocabularies that contains various synonyms for all ICD codes. Therefore, we propose to leverage synonyms of codes to help the label representation learning and further benefit its matching to the EMR texts.\nTo model the synonym and its matching to EMR text, we further propose a Multiple Synonyms Matching Network (MSMN). Specifically, we first apply a shared LSTM to encode EMR texts and each synonym. Then, we propose a novel multisynonyms attention mechanism inspired by the multi-head attention (Vaswani et al., 2017), which considers synonyms as attention queries to extract different code-related text snippets for codewise representations. Finally, we propose using a biaffine-based similarity of code-wise text representations and code representations for classification.\nWe conduct experiments on the MIMIC-III dataset with two settings: full codes and top-50 codes. Results show that our method performs better than previous state-of-the-art methods. We will release our codes for further research.\n2“Label” is equivalent to “code” in this paper."
    }, {
      "heading" : "2 Approach",
      "text" : "Consider free text S (usually discharge summaries) from EMR with words {wi}Ni=1. Let C be the ICD codes set, for each code l ∈ C with code description l1 from ICD, the task is to assign a binary label yl ∈ {0, 1} based on S. Figure 1 shows an overview of our method."
    }, {
      "heading" : "2.1 Code Synonyms",
      "text" : "We extend the code description l1 by synonyms from the medical knowledge graph (i.e., UMLS Metathesaurus). We first align the code to the Concept Unique Identifiers (CUIs) from UMLS. Then we select corresponding synonyms of English terms from UMLS with same CUIs and add additional synonyms by removing hyphens and the word “NOS” (Not Otherwise Specified). We denote the code synonyms as {l2, ..., lm} in which each code synonym lj is composed of words {lji } Nj i=1."
    }, {
      "heading" : "2.2 Encoding",
      "text" : "Previous works (Ji et al., 2021; Pascual et al., 2021) have shown that pretrained language models like BERT (Devlin et al., 2019) cannot help the ICD coding performance, hence we use an LSTM (Hochreiter and Schmidhuber, 1997) as our encoder. We use pre-trained word embeddings to map words wi to xi. A d-layer bi-directional LSTM layer with output size h is followed by word embeddings to obtain text hidden representations H.\nH = h1, ...,hN = Enc(x1, ...,xN ) (1)\nFor code synonym lj , we apply the same encoder with a max-pooling layer to obtain representation qj ∈ Rh.\nqj = MaxPool(Enc(xj1, ...,x j Nj )) (2)"
    }, {
      "heading" : "2.3 Multi-synonyms Attention",
      "text" : "To interact text with multiple synonyms, we propose a multi-synonyms attention inspired by the multi-head attention (Vaswani et al., 2017). We split H ∈ RN×h into m heads Hj ∈ RN× h m :\nH = H1, ...,Hm (3)\nThen, we use code synonyms qj to query Hj . We take the linear transformations of Hj and qj to calculate attention scores αjl ∈ R\nN . Text related to code synonym lj can be represented by Hαjl . We aggregate code-wise text representations vl ∈\nRh using max-pooling of Hαjl since the text only needs to match one of the synonyms.\nαjl = softmax(WQq j · tanh(WHHj)) (4) vl = MaxPool(Hα 1 l , ...,Hα m l ) (5)"
    }, {
      "heading" : "2.4 Classification",
      "text" : "We classify whether the text S contains code l based on the similarity between code-wise text representation vl and code representation. We aggregate code synonym representations {qj} to code representation ql ∈ Rh by max-pooling. We then propose using a biaffine transformation to measure the similarity for classification:\nql = MaxPool(q 1,q2, ...,qm) (6)\nŷl = σ(logitl) = σ(v T l Wql) (7)\nPrevious works (Mullenbach et al., 2018; Vu et al., 2020) classify codes via3:\nŷl = σ(logitl) = σ(v T l wl) (8)\nTheir work need to learn code-dependent parameters [wl]l∈C ∈ R∥C∥×h for classification, which suffers from training rare codes. On the contrary, our biaffine function that replaces Wql to wl only needs to learn code-independent parameters W ∈ Rh×h."
    }, {
      "heading" : "2.5 Training",
      "text" : "We optimize the model using binary cross-entropy between predicted probabilities ŷl and labels yl:\nL = ∑ l∈C −yl log(ŷl)− (1− yl) log(1− ŷl) (9)\n3We omit the biases in all equations for simplification."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset",
      "text" : "MIMIC-III dataset (Johnson et al., 2016) contains deidentified discharge summaries with humanlabeled ICD-9 codes. We use the same splits with previous works (Mullenbach et al., 2018; Vu et al., 2020) with two settings as full codes (MIMIC-III full) and top-50 frequent codes (MIMIC-III 50). We follow the preprocessing of Xie et al. (2019); Vu et al. (2020) to truncate discharge summaries at 4,000 words. We measure the results using macro AUC, micro AUC, macro F1, micro F1 and precision@k (k = 5 for MIMIC-III 50, 8 and 15 for MIMIC-III full). Detailed statistics of the MIMICIII dataset are listed in Appendix A."
    }, {
      "heading" : "3.2 Implementation Details",
      "text" : "We sample m = 4 and 8 synonyms per code for MIMIC-III full and MIMIC-III 50 respectively. We use the same word embeddings as Vu et al. (2020) which are pretrained on the MIMIC-III discharge summaries using CBOW (Mikolov et al., 2013) with hidden size 100. We apply R-Drop with α = 5 (Liang et al., 2021) to regularize the model to prevent over-fitting. We train MSMN with AdamW (Loshchilov and Hutter, 2019) with a linear learning rate decay. We optimize the threshold of classification using the development set."
    }, {
      "heading" : "3.3 Baselines",
      "text" : "CAML (Mullenbach et al., 2018) uses CNN to encode texts and proposes label attention for coding. MSATT-KG (Xie et al., 2019) applies multi-scale attention and GCN to capture codes relations. MultiResCNN (Li and Yu, 2020) encodes text using multi-filter residual CNN. HyperCore (Cao et al., 2020) embeds ICD codes into the hyperbolic space to utilize code hierarchy and uses GCN to leverage the code co-occurrence. LAAT & JointLAAT (Vu et al., 2020) propose a hierarchical joint learning mechanism to relieve the imbalanced labels, which is our main baseline since it is most similar to our work."
    }, {
      "heading" : "3.4 Main Results",
      "text" : "Table 1 and 2 show the main results under the MIMIC-III full and MIMIC-III 50 settings, respectively. Under the full setting, our MSMN achieves 95.0 (+2.0), 99.2 (+0.0), 10.3 (-0.4), 58.4 (+0.9), 75.2 (+1.4), and 59.9 (+0.8) in terms of macro-AUC, micro-AUC, macro-F1, micro-F1, P@8, and P@15 respectively (parentheses shows the differences against previous best results), which shows that MSMN obtains state-of-the-art results in most metrics. Under the top-50 codes setting, MSMN performs better than LAAT in all metrics and achieves state-of-the-art scores of 92.8 (+0.3), 94.7 (+0.1), 68.3 (+1.7), 72.5 (+0.9), 68.0 (+0.5) on macro-AUC, micro-AUC, macro-F1, micro-F1, and P@5, respectively. We notice that the macro F1 has large variance in MIMIC-III full setting because it is more sensitive in a long tail problem."
    }, {
      "heading" : "3.5 Discussion",
      "text" : "To explore the influence of leveraging different numbers of code synonyms, we search m among {1, 2, 4, 8, 16} on the MIMIC-III 50 dataset. Results are shown in Table 3. Compared with m = 1 that we only use ICD code descriptions itself, lever-\naging more synonyms from UMLS consistently improves the performance. Using m = 4, 8 achieves the best performances in AUC, and m = 8 achieves the best performances in terms of F1 and P@5. In addition, the median and mean count of UMLS synonyms are 5.0 and 5.4 respectively, which echoes why the results of m = 4 or 8 are better.\nTo evaluate the effectiveness of our proposed biaffine-based similarity function, we compare it with the baseline LAAT in Table 3. We also provide a simple function by removing W to vTl ql in Equation 7. Results show the biaffine-based similarity scoring performs best among others.\nTo better understand what MSMN learns from the multi-synonyms attention, we plot the synonym representations qj under MIMIC-III 50 setting via t-SNE (van der Maaten and Hinton, 2008) in Figure 2. We observe for some codes like 585.9 (“chronic kidney diseases”), all synonym representations cluster together, which indicates that synonyms extract similar text snippets. However, codes like 410.71 (“subendocardial infarction initial episode of care” or “subendo infarct, initial”) and 403.90 (“hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage i through stage iv” or “unspecified orhy kid w cr kid i iv”) with very different synonyms learn different representations, which benefits to match different text snippets. Furthermore, we observe it has similar representations for sibling codes 37.22 (“left heart cardiac catheterization”) and 37.23 (“rt/left heart card cath”), which indicates the model can also implicitly capture the code hierarchy."
    }, {
      "heading" : "4 Related Work",
      "text" : "Automatic ICD coding is an important task in the medical NLP community. Earlier works use ma-\nchine learning methods for coding (Larkey and Croft, 1996; Pestian et al., 2007; Perotte et al., 2014). With the development of neural networks, many recent works consider ICD coding as a multilabel text classification task. They usually apply RNN or CNN to encode texts and use the label attention mechanism to extract and match the most relevant parts for classification. The label attention relies on the label representations as attention queries. Li and Yu (2020); Vu et al. (2020) randomly initialize the label representations which ignore the code semantic information. Cao et al. (2020) use the average of word embeddings as label representations to leverage the code semantic information. Xie et al. (2019); Cao et al. (2020) use GCN to fuse hierarchical structures of ICD codes for label representations. Compared with previous works, we use synonyms instead of a single description to represent the code, which can provide more comprehensive expressions of codes."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we propose MSMN to leverage code synonyms from UMLS to improve the automatic ICD coding. Multi-synonyms attention is proposed for extracting different related text snippets for code-wise text representations. We also propose a biaffine transformation to calculate similarities among texts and codes for classification. Experiments show that MSMN outperforms previous methods with label attention and achieves state-ofthe-art results in the MIMIC-III dataset. Ablation studies show the effectiveness of multi-synonyms attention and biaffine-based similarity."
    }, {
      "heading" : "A MIMIC-III Dataset Statistics",
      "text" : "We list the document counts, average word counts per document, average codes counts per document, and total codes of the MIMIC-III dataset in Table 4."
    }, {
      "heading" : "B Training Details",
      "text" : "For the MIMIC-III 50 setting, we train with one 16GB NVIDIA-V100 GPU. For the MIMICIII full setting, we train with 8 32GB NVIDIAV100 GPUs. We list the detailed training hyperparameters in Table 5. We apply the dropout with a ratio of 0.2 after the word embedding layer and before the classification layer. For text encoding,\nTrain Dev Test MIMIC-III Full\n# Doc. 47,723 1,631 3,372 Avg # words per Doc. 1,434 1,724 1,731 Avg # codes per Doc. 15.7 18.0 17.4 Total # codes 8,692 3,012 4,085\nMIMIC-III 50\nwe add a linear layer upon the LSTM layer (the output dimension of the linear layer refers to LSTM output dim. in the Table 5)."
    } ],
    "references" : [ {
      "title" : "The unified medical language system (umls): integrating biomedical terminology",
      "author" : [ "Olivier Bodenreider." ],
      "venue" : "Nucleic acids research, 32(suppl_1):D267– D270.",
      "citeRegEx" : "Bodenreider.,? 2004",
      "shortCiteRegEx" : "Bodenreider.",
      "year" : 2004
    }, {
      "title" : "Hypercore: Hyperbolic and co-graph representation for automatic icd coding",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu", "Weifeng Chong." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "A hierarchical approach to the automatic categorization of medical documents",
      "author" : [ "Luciano RS de Lima", "Alberto HF Laender", "Berthier A Ribeiro-Neto." ],
      "venue" : "Proceedings of the seventh international conference on Information and knowledge",
      "citeRegEx" : "Lima et al\\.,? 1998",
      "shortCiteRegEx" : "Lima et al\\.",
      "year" : 1998
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Ontological attention ensembles for capturing semantic concepts in icd code prediction from clinical text",
      "author" : [ "Matus Falis", "Maciej Pajak", "Aneta Lisowska", "Patrick Schrempf", "Lucas Deckers", "Shadia Mikhael", "Sotirios Tsaftaris", "Alison O’Neil" ],
      "venue" : null,
      "citeRegEx" : "Falis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Falis et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Does the magic of bert apply to medical code assignment? a quantitative study",
      "author" : [ "Shaoxiong Ji", "Matti Hölttä", "Pekka Marttinen." ],
      "venue" : "Computers in Biology and Medicine, 139:104998.",
      "citeRegEx" : "Ji et al\\.,? 2021",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2021
    }, {
      "title" : "Mimiciii, a freely accessible critical care database",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "H Lehman Li-Wei", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark." ],
      "venue" : "Scien-",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Combining classifiers in text categorization",
      "author" : [ "Leah S Larkey", "W Bruce Croft." ],
      "venue" : "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 289–297.",
      "citeRegEx" : "Larkey and Croft.,? 1996",
      "shortCiteRegEx" : "Larkey and Croft.",
      "year" : 1996
    }, {
      "title" : "Icd coding from clinical text using multi-filter residual convolutional neural network",
      "author" : [ "Fei Li", "Hong Yu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8180–8187.",
      "citeRegEx" : "Li and Yu.,? 2020",
      "shortCiteRegEx" : "Li and Yu.",
      "year" : 2020
    }, {
      "title" : "R-drop: Regularized dropout for neural networks. In NeurIPS",
      "author" : [ "Xiaobo* Liang", "Lijun* Wu", "Juntao Li", "Yue Wang", "Qi Meng", "Tao Qin", "Wei Chen", "Min Zhang", "TieYan Liu" ],
      "venue" : null,
      "citeRegEx" : "Liang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2021
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomás Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Pro-",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Explainable prediction of medical codes from clinical text",
      "author" : [ "James Mullenbach", "Sarah Wiegreffe", "Jon Duke", "Jimeng Sun", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Mullenbach et al\\.,? 2018",
      "shortCiteRegEx" : "Mullenbach et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards BERT-based automatic ICD coding: Limitations and opportunities",
      "author" : [ "Damian Pascual", "Sandro Luck", "Roger Wattenhofer." ],
      "venue" : "Proceedings of the 20th Workshop on Biomedical Language Processing, pages 54–63, Online. Association for Computational",
      "citeRegEx" : "Pascual et al\\.,? 2021",
      "shortCiteRegEx" : "Pascual et al\\.",
      "year" : 2021
    }, {
      "title" : "Diagnosis code assignment: models and evaluation metrics",
      "author" : [ "Adler Perotte", "Rimma Pivovarov", "Karthik Natarajan", "Nicole Weiskopf", "Frank Wood", "Noémie Elhadad." ],
      "venue" : "Journal of the American Medical Informatics Association, 21(2):231–237.",
      "citeRegEx" : "Perotte et al\\.,? 2014",
      "shortCiteRegEx" : "Perotte et al\\.",
      "year" : 2014
    }, {
      "title" : "A shared task involving multi-label classification of clinical free text",
      "author" : [ "John P. Pestian", "Chris Brew", "Pawel Matykiewicz", "DJ Hovermale", "Neil Johnson", "K. Bretonnel Cohen", "Wlodzislaw Duch." ],
      "venue" : "Biological, translational, and clinical language pro-",
      "citeRegEx" : "Pestian et al\\.,? 2007",
      "shortCiteRegEx" : "Pestian et al\\.",
      "year" : 2007
    }, {
      "title" : "Automated icd coding via unsupervised knowledge integration (unite)",
      "author" : [ "Aaron Sonabend", "Winston Cai", "Yuri Ahuja", "Ashwin Ananthakrishnan", "Zongqi Xia", "Sheng Yu", "Chuan Hong." ],
      "venue" : "International journal of medical informatics, 139:104135.",
      "citeRegEx" : "Sonabend et al\\.,? 2020",
      "shortCiteRegEx" : "Sonabend et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalized zeroshot text classification for icd coding",
      "author" : [ "Congzheng Song", "Shanghang Zhang", "Najmeh Sadoughi", "Pengtao Xie", "Eric Xing." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 4018–",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep patient similarity learning for personalized",
      "author" : [ "Qiuling Suo", "Fenglong Ma", "Ye Yuan", "Mengdi Huai", "Weida Zhong", "Jing Gao", "Aidong Zhang" ],
      "venue" : null,
      "citeRegEx" : "Suo et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Suo et al\\.",
      "year" : 2018
    }, {
      "title" : "An overview of clinical decision support systems: benefits, risks, and strategies for success",
      "author" : [ "Reed T Sutton", "David Pincock", "Daniel C Baumgart", "Daniel C Sadowski", "Richard N Fedorak", "Karen I Kroeker." ],
      "venue" : "NPJ digital medicine, 3(1):1–10.",
      "citeRegEx" : "Sutton et al\\.,? 2020",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9(86):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A label attention model for icd coding from clinical text",
      "author" : [ "Thanh Vu", "Dat Quoc Nguyen", "Anthony Nguyen." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3335–3341. Main track.",
      "citeRegEx" : "Vu et al\\.,? 2020",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural architecture for automated icd coding",
      "author" : [ "Pengtao Xie", "Eric Xing." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1066– 1076.",
      "citeRegEx" : "Xie and Xing.,? 2018",
      "shortCiteRegEx" : "Xie and Xing.",
      "year" : 2018
    }, {
      "title" : "Ehr coding with multi-scale feature attention and structured knowledge graph propagation",
      "author" : [ "Xiancheng Xie", "Yun Xiong", "Philip S Yu", "Yangyong Zhu." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowledge Management,",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic icd coding via interactive shared representation networks with self-distillation mechanism",
      "author" : [ "Tong Zhou", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Kun Niu", "Weifeng Chong", "Shengping Liu." ],
      "venue" : "Proceedings of the 59th Annual Meet-",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "The task of ICD coding refers to assigning ICD codes to electronic medical records (EMRs) which is highly related to clinical tasks or systems including patient similarity learning (Suo et al., 2018), medical billing (Sonabend et al.",
      "startOffset" : 181,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : ", 2018), medical billing (Sonabend et al., 2020), and clinical decision support systems (Sutton et al.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : ", 2020), and clinical decision support systems (Sutton et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "Deep learning methods usually treat this task as a multi-label classification problem (Xie and Xing, 2018; Li and Yu, 2020; Zhou et al., 2021), which learn deep representations of EMRs with",
      "startOffset" : 86,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "Deep learning methods usually treat this task as a multi-label classification problem (Xie and Xing, 2018; Li and Yu, 2020; Zhou et al., 2021), which learn deep representations of EMRs with",
      "startOffset" : 86,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : "Deep learning methods usually treat this task as a multi-label classification problem (Xie and Xing, 2018; Li and Yu, 2020; Zhou et al., 2021), which learn deep representations of EMRs with",
      "startOffset" : 86,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Recent state-of-the-art methods propose label attention that uses the code representations as attention queries to extract the code-related representations2 (Mullenbach et al., 2018).",
      "startOffset" : 157,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : "Following this idea, many works further propose using code hierarchical structures (Falis et al., 2019; Xie et al., 2019; Cao et al., 2020) and descriptions (Cao et al.",
      "startOffset" : 83,
      "endOffset" : 139
    }, {
      "referenceID" : 25,
      "context" : "Following this idea, many works further propose using code hierarchical structures (Falis et al., 2019; Xie et al., 2019; Cao et al., 2020) and descriptions (Cao et al.",
      "startOffset" : 83,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "Following this idea, many works further propose using code hierarchical structures (Falis et al., 2019; Xie et al., 2019; Cao et al., 2020) and descriptions (Cao et al.",
      "startOffset" : 83,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : ", 2020) and descriptions (Cao et al., 2020; Song et al., 2020) for better label representations.",
      "startOffset" : 25,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : ", 2020) and descriptions (Cao et al., 2020; Song et al., 2020) for better label representations.",
      "startOffset" : 25,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Fortunately, these different expressions can be found in the Unified Medical Language System (Bodenreider, 2004), a repository of biomedical vocabularies that contains various synonyms for all ICD codes.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : "Then, we propose a novel multisynonyms attention mechanism inspired by the multi-head attention (Vaswani et al., 2017), which considers synonyms as attention queries to extract different code-related text snippets for codewise representations.",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "Previous works (Ji et al., 2021; Pascual et al., 2021) have shown that pretrained language models like BERT (Devlin et al.",
      "startOffset" : 15,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "Previous works (Ji et al., 2021; Pascual et al., 2021) have shown that pretrained language models like BERT (Devlin et al.",
      "startOffset" : 15,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : ", 2021) have shown that pretrained language models like BERT (Devlin et al., 2019) cannot help the ICD coding performance, hence we use an LSTM (Hochreiter and Schmidhuber, 1997) as our encoder.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : ", 2019) cannot help the ICD coding performance, hence we use an LSTM (Hochreiter and Schmidhuber, 1997) as our encoder.",
      "startOffset" : 69,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "To interact text with multiple synonyms, we propose a multi-synonyms attention inspired by the multi-head attention (Vaswani et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Previous works (Mullenbach et al., 2018; Vu et al., 2020) classify codes via3:",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : "Previous works (Mullenbach et al., 2018; Vu et al., 2020) classify codes via3:",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "MIMIC-III dataset (Johnson et al., 2016) contains deidentified discharge summaries with humanlabeled ICD-9 codes.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "We use the same splits with previous works (Mullenbach et al., 2018; Vu et al., 2020) with two settings as full codes (MIMIC-III full) and top-50 frequent codes (MIMIC-III 50).",
      "startOffset" : 43,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "We use the same splits with previous works (Mullenbach et al., 2018; Vu et al., 2020) with two settings as full codes (MIMIC-III full) and top-50 frequent codes (MIMIC-III 50).",
      "startOffset" : 43,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "(2020) which are pretrained on the MIMIC-III discharge summaries using CBOW (Mikolov et al., 2013) with hidden size 100.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "We apply R-Drop with α = 5 (Liang et al., 2021) to regularize the model to prevent over-fitting.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "We train MSMN with AdamW (Loshchilov and Hutter, 2019) with a linear learning rate decay.",
      "startOffset" : 25,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "CAML (Mullenbach et al., 2018) uses CNN to encode texts and proposes label attention for coding.",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "MSATT-KG (Xie et al., 2019) applies multi-scale attention and GCN to capture codes relations.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "MultiResCNN (Li and Yu, 2020) encodes text using multi-filter residual CNN.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "HyperCore (Cao et al., 2020) embeds ICD codes into the hyperbolic space to utilize code hierarchy and uses GCN to leverage the code co-occurrence.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 23,
      "context" : "LAAT & JointLAAT (Vu et al., 2020) propose a hierarchical joint learning mechanism to relieve the imbalanced labels, which is our main baseline since it is most similar to our work.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "chine learning methods for coding (Larkey and Croft, 1996; Pestian et al., 2007; Perotte et al., 2014).",
      "startOffset" : 34,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "chine learning methods for coding (Larkey and Croft, 1996; Pestian et al., 2007; Perotte et al., 2014).",
      "startOffset" : 34,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "chine learning methods for coding (Larkey and Croft, 1996; Pestian et al., 2007; Perotte et al., 2014).",
      "startOffset" : 34,
      "endOffset" : 102
    } ],
    "year" : 0,
    "abstractText" : "Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs). Existing methods apply label attention with code representations to match related text snippets for coding. Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code in ICD. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on two settings of the MIMIC-III dataset show that our proposed method outperforms previous stateof-the-art methods.",
    "creator" : null
  }
}
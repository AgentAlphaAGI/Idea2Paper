{
  "name" : "ARR_2022_158_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Brain encoding aims at constructing neural brain activity given an input stimulus. Since the discovery of the relationship between language stimuli and functions of brain networks using fMRI [for ex., (Constable et al., 2004)], researchers have been interested in understanding how the neural encoding models predict the fMRI brain activity. Several brain encoding models have been developed\nto (i) understand the ventral stream in biological vision (Yamins et al., 2014; Kietzmann et al., 2019; Bao et al., 2020), and (ii) to study the higher-level cognition like language processing (Gauthier and Levy, 2019; Schrimpf et al., 2021; Schwartz et al., 2019).\nSome recent studies (Nishida et al., 2015; Huth et al., 2016) have been able to identify brain ROIs that respond to words that have a similar meaning and have thus built a “semantic atlas” of how the human brain organizes language. Further, several studies (Oota et al., 2018; Jain and Huth, 2018; Hollenstein et al., 2019) have used a wide variety of word embeddings where words represented as vectors in an embedding space are mapped to brain activation for improved neural coding.\nRecently, Transformer (Vaswani et al., 2017) based models like BERT (Devlin et al., 2019) have been found to be very effective across a large number of natural language processing (NLP) tasks. These Transformer based models have been pretrained on millions of text instances in an unsupervised manner and further finetuned to specialize for various NLP tasks. Natural language understanding requires integrating several cognitive skills like syntactic parsing of the language structure, identifying the named entities, capturing the word meaning in the context, coreference resolution, etc. Learning from massive corpora enables these models to excel at cognitive skills required for language understanding. Interestingly, such Transformer-based neural representations have been found to be very effective for brain encoding as well (Schrimpf et al., 2021).\nDespite the recent advances in mapping between language transformers and the brain activity recorded with reading (Schrimpf et al., 2021), the Transformer features themselves are notoriously difficult to interpret. In recent works, Caucheteux et al. (2021a); Antonello et al. (2021) address this issue by disentangling the high-dimensional Trans-\nformer representations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations to explore which class is highly associated with language cortical ROIs. Representations do not exist in a vacuum but become meaningful only when they accomplish a task. Therefore, the next logical step is to see which of these Transformer representations most effectively drive the linear mapping between language models and the brain in the context of NLP tasks. Gauthier and Levy (2019) fine-tune a pretrained BERT model on multiple tasks to find tasks best correlated with high decoding performance. In this study, we investigate the correlation between brain activation and feature representations learned by different task-specific networks, and ask which tasks lead to improvements in brainencoding performance.\nRecently, a study using multiple computer vision tasks has shown that 3D vision task models predict better fMRI brain activity than 2D vision task models (Wang et al., 2019) for visual stimuli. Inspired by the success of correlations in the vision field (Wang et al., 2019), and brain encoding study of a variety of language Transformer models (Schrimpf et al., 2021; Caucheteux et al., 2021b,a), we build neural language taskonomy models for brain encoding and aim to find NLP tasks that are most explanatory of brain activations for reading and listening tasks.\nIn this paper, we uncover insights about the association between fMRI voxel activations and representations of diverse NLP tasks representations. The predictive power of task-specific representations with brain activation is ascertained by (1) using ridge regression on such representations and predicting activations and (2) computing popular metrics like 2V2 accuracy and Pearson correlation between actual and predicted activations.\nSpecifically, we make the following contributions in this paper. (1) Given Transformer models finetuned for various NLP tasks, we propose the problem of finding which of these are the most predictive of fMRI brain activity for reading and listening tasks. (2) Our language taskonomy results reveal that Coreference Resolution, Named Entity Recognition, and Shallow Syntax Parsing tasks have higher predictive performance while reading the text. On the other hand, paraphrase detection, summarization, and Natural Language Inference tasks display better correlation during\nlistening. (3) We also perform similarity analysis between task representations from transfer learning and neural taskonomy and derive interesting cognitive insights from brain maps."
    }, {
      "heading" : "2 Related Work",
      "text" : "Older methods for text-based stimulus representation include text corpus co-occurrence counts (Mitchell et al., 2008; Pereira et al., 2013; Huth et al., 2016), syntactic and discourse features (Wehbe et al., 2014). In recent times, both semantic and experiential attribute models have been explored for text-based stimuli. Semantic representation models include distributed word embeddings (Pereira et al., 2016; Anderson et al., 2017a; Pereira et al., 2018; Toneva and Wehbe, 2019; Hollenstein et al., 2019; Wang et al., 2020), sentence representation models (Sun et al., 2019; Toneva and Wehbe, 2019; Sun et al., 2020), recurrent neural networks (Jain and Huth, 2018; Oota et al., 2019), and Transformer-based language models (Gauthier and Levy, 2019; Toneva and Wehbe, 2019; Schwartz et al., 2019). Experiential attribute models represent words in terms of human ratings of their degree of association with different attributes of experience, typically on a scale of 0-6 (Anderson et al., 2019, 2020; Berezutskaya et al., 2020; Jat et al., 2020; Caucheteux et al., 2021a; Antonello et al., 2021) or binary (Handjaras et al., 2016; Wang et al., 2017). Fine-grained details such as lexical, compositional, syntactic, and semantic representations of narratives are factorized from transformerbased models and utilized for training encoding models. The resulting models are better able to disentangle the corresponding brain responses in fMRI (Caucheteux et al., 2021a).\nIn this paper, we focus on Transformer-based linguistic stimuli representations since they have been found to be most effective. Unlike previous studies which directly used existing task-agnostic pretrained models, we train task-specific Transformer models and aim to find which model leads to the best encoding accuracy given reading and listening language stimuli."
    }, {
      "heading" : "3 Brain Imaging Datasets",
      "text" : "We work with two datasets: Pereira and Narratives-Pieman. Results on Narratives-Lucy and Narratives-SlumLord show similar trends. Hence, we also show results on Narratives-Lucy and Narratives-SlumLord in the appendix.\nPereira Dataset (Reading Sentences from Passages) For the Pereira dataset, similar to earlier work (Sun et al., 2019, 2020), we combine the data from sentence-based experiments (experiments-2 and 3) from Pereira et al. (2018). Five subjects were presented a total of 627 sentences from 48 broad topics, spanning over 168 passages, where each passage consists of 3-4 sentences. As in (Pereira et al., 2018), we focused on nine brain ROIs (regions of interest) corresponding to four brain networks: (i) Default Mode Network (DMN) (linked to the functionality of semantic processing), (ii) Language Network (related to language processing, understanding, word meaning, and sentence comprehension), (iii) Task Positive Network (TP) (related to attention, salience information), and (iv) Visual Network (related to the processing of visual objects, object recognition). We briefly summarize the details of the dataset and the number of voxels corresponding to each ROI in Table 2 (refer to appendix). We use the AAL parcellation Atlas (116 × 116 brain ROIs) to present the brain map results, since Pereira dataset contains annotations tied to this atlas.\nNarratives-Pieman (Listening to Stories) The “Narratives” collection aggregates a variety of fMRI datasets collected while human subjects listened to naturalistic spoken stories. The Narratives dataset that includes 345 subjects, 891 functional scans, and 27 diverse stories of varying duration totaling ∼4.6 hours of unique stimuli (∼43,000 words) was proposed in (Nastase et al., 2021). Similar to earlier works (Caucheteux et al., 2021b), we analyze data from 82 subjects listening to the story titled ‘PieMan’ with 259 TRs (repetition time – fMRI recorded every 1.5 sec.). We list number of voxels per ROI in this dataset in Table 3 (refer to appendix). We use the multi-modal parcellation of the human cerebral cortex (Glassar Atlas: consists of 180 ROIs in each hemisphere) to display the brain maps (Glasser et al., 2016), since Narratives dataset contains annotations tied to this atlas. The data covers ten brain ROIs in the human brain, i.e., Left hemisphere (L), and Right hemisphere (R) for each of the following: (i) early auditory cortex (EAC: A1, LBelt, MBelt, PBelt, and R1) which plays a key role for sound perception since it represents one of the first cortical processing stations for sounds; (ii) auditory association cortex (AAC: A4, A5, STSdp, STSda, STSvp, STSva, STGa, and TA2) which is concerned with\nthe memory and classification of sounds; (iii) posterior medial cortex (PMC: POS1, POS2, v23ab, d23ab, 31pv, 31pd, 7m); (iv) the temporo parieto occipital junction (TPOJ: TPOJ1, TPOJ2, TPOJ3, STV, PSL) which is a complex brain territory heavily involved in several high-level neurological functions, such as language, visuo-spatial recognition, writing, reading, symbol processing, calculation, self-processing, working memory, musical memory, and face and object recognition; and (v) the dorsal frontal lobe (DFL: L_55b, SFL, L_44, L_45, IFJA, IFSP) which covers the aspects of pragmatic processing such as discourse management, integration of prosody, interpretation of nonliteral meanings, inference making, ambiguity resolution, and error repair."
    }, {
      "heading" : "4 Encoding Model",
      "text" : "To explore how and where contextual language features are represented in the brain when reading sentences and listening to stories, we extract different features spaces describing each stimulus sentence and use them in an encoding model to predict brain responses. Our reasoning is as follows. If a feature is a good predictor of a specific brain region, information about that feature is likely encoded in that region. In this paper, for both datasets, we train fMRI encoding models using Ridge regression on stimuli representations obtained using a variety of NLP tasks. The main goal of each fMRI encoder model is to predict brain responses associated with each brain region given a stimuli. In all cases, we train a model per subject separately. Following literature on brain encoding (Caucheteux et al., 2021b; Toneva et al., 2020), we choose to use a ridge regression model instead of more complicated models. We plan to explore more such models as part of future work. We follow K-fold (K=10) cross-validation. All the data samples from K-1 folds were used for training, and the model was tested on samples of the left-out fold. More hyper-parameter settings are discussed in Section .1 in the Appendix."
    }, {
      "heading" : "4.1 Feature Spaces",
      "text" : "To simultaneously test representations from multiple NLP tasks, we used the latent space features from each of the following ten popular NLP tasks: coreference resolution (CR), named entity recognition (NER), natural language inference (NLI), paraphrase detection (PD), question answering\n(QA), sentiment analysis (SA), semantic role labeling (SRL), shallow syntax parsing (SS), summarization (Sum) and word sense disambiguation (WSD). All of these are discriminative NLP tasks, and thus we use models obtained by task-specific finetuning of the same pretrained Transformer encoder model (BERT-base-cased with dimensionality=768). Given an input sentence, each task Transformer outputs token representations at the final layer. We use the #tokens × 768 dimension vector obtained from the last hidden layer to obtain latent features for the stimuli. We then build individual ridge regression models with the extracted latent features to predict brain responses and measure the correlation between the prediction and the true response. Pereira: Since individual sentences were presented to the subjects while modeling, sentences were passed one by one to the task Transformer model, and average-pooled representations were used to encode the sentence stimuli. Narratives-Pieman: Due to the constraint on input sequence length for BERT (512), we considered a window size of 10 sentences with the last two sentences of one window overlapping with the next to be given as input to the BERT model. We use the average-pooled representation from BERT to encode text stimuli. To get the representation for a TR, we pooled the representations of only those words of the sentences in that TR."
    }, {
      "heading" : "4.2 Task Descriptions",
      "text" : "Here we describe the functionality of each NLP task that we used for fMRI encoding. CR: involves finding all expressions that refer to the same entity in a text. PD: involves taking a passage – either spoken or written – and rewording it in shorter or own words. Summarization (Sum): involves selecting a few important sentences from a document or paragraph. NER: involves detection of the named entities such as person names, location names, company names from a given text. NLI: investigates the entailment relationship between two texts: premise and hypothesis. QA: aims to select an answer given a passage, a question, and a set of candidate answers. SA: involves determining whether a piece of text is positive, negative, or neutral. SRL: assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result. SS: provides an approximation of phrase-syntactic\nstructure of sentences. WSD: involves determining which sense (meaning) of a word is activated by the use of the word in a particular context.\nOf these, NER and SS are syntactic, while the others involve semantic reasoning. Our selection of these tasks was based on the following design principles: (1) We wanted to select a set of tasks covering diverse cognitive-linguistic skills. (2) We wanted to select tasks that are a part of popular NLP benchmarks like GLUE (Wang et al., 2018). (3) We selected tasks for which BERT-base-cased finetuned models were available. Note that we did not finetune any of these models ourselves but leveraged the state-of-the-art finetuned models available on Huggingface. Details of the specific finetuned model checkpoints are mentioned in Table 1 in the Appendix."
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "We evaluate our models using popular brain encoding evaluation metrics described in the following. Given a subject and a brain region, let N be the number of samples. Let {Yi}Ni=1 and {Ŷi}Ni=1 denote the actual and predicted voxel value vectors for the ith sample. Thus, Y ∈ RN×V and Ŷ ∈ RN×V where V is the number of voxels in that region. 2V2 Accuracy is computed as 2V2Acc= 1NC2 ∑N−1 i=1 ∑N j=i+1 I[cosD(Yi, Ŷi) + cosD(Yj , Ŷj) < cosD(Yi, Ŷj) + cosD(Yj , Ŷi)] where cosD is the cosine distance function. I[c] is an indicator function such that I[c] = 1 if c is true, else it is 0. The higher the 2V2 accuracy, the better. Pearson Correlation (PC) is computed as PC= 1N ∑n i=1 corr[Yi, Ŷi] where corr is the correlation function. Mean Absolute Error (MAE) is computed as MAE= 1N ∑n i=1 |[Yi − Ŷi]|. Statistical Significance: In order to estimate the statistical significance of the performance differences (across all results), we performed one-way ANOVA on the mean values for the subjects. In all such cases we report p-values corrected using Bonferroni correction."
    }, {
      "heading" : "4.4 Neural Language Tasks Similarity Computation",
      "text" : "To estimate the similarity between 10 language tasks, we took the prediction performance scores across all the voxels in Pereira (97,539) and Narratives-Pieman datasets (10,732). To analyze the relationship between tasks based on neural rep-\nresentations, we calculated the Pearson correlation between predicted voxels of each task with the remaining tasks. These Pearson correlation values were used to construct heatmaps and the task similarity trees(dendograms) using hierarchical clustering for Pereira and Narratives-Pieman datasets."
    }, {
      "heading" : "5 Results",
      "text" : "In order to assess the performance of the fMRI encoder models learned using the representations from a variety of NLP tasks, we computed the 2V2 accuracy and Pearson correlation coefficient between the predicted and true responses across various ROIs for both the reading (Pereira) dataset (Fig. 1) as well as the listening (Narratives-Pieman) dataset (Fig. 2)."
    }, {
      "heading" : "5.1 Encoding performance of Language Task models for reading vs listening tasks",
      "text" : "Reading Sentences (Pereira): From Fig. 1, we observe that tasks such as CR, NER, SRL, and SS appear to have a better correlation to the brain responses compared to the other tasks. In order to estimate the statistical significance of the performance differences, we performed one-way ANOVA on the mean correlation values for the subjects across the ten language tasks for the nine brain ROIs. The main effect of the ANOVA test was significant for all the ROIs with p≤ 10−2 with confidence 95% (see Appendix for detailed ANOVA results). Further, post hoc pairwise com-\nparisons (Ruxton and Beauchamp, 2008) confirmed the visual observations that on both 2V2 accuracy and Pearson correlation measures, tasks such as CR, NER, SRL, and SS performed significantly better compared to other tasks (see Appendix for pairwise comparison results). These results demonstrate that when reading a sentence, information processing operations related to recognizing named entities, labeling semantic roles to the constituents of a sentence, identifying the references from a sentence to the given topic (concept), and syntactic processing may be engaged.\nFurther, we observe that the ROI corresponding to language processing in the left hemisphere (Language_LH) has higher encoding performance than that of the right hemisphere (Language_RH). This is in line with the left hemisphere dominance for language processing (Binder et al., 2009). Also, lateral visual ROIs such as Vision_Object, Vision_Body, Vision_Face, and Vision ROIs display higher correlation with the language tasks associated with named entities (NER), relating the entities (CR), and syntax processing (SS). Higher correlations with all the visual brain regions point to the possible alignment of visual and language regions for semantic understanding (Popham et al., 2021) in a reading task. Finally, across all regions, pretrained BERT model has worse correlation compared to at least 5 other task models.\nListening Stories (Narratives-Pieman): From Fig. 2, we observe that the profiles of performance\nshow low scores in the early auditory cortex (EAC), auditory association cortex (AAC); average scores in TPOJ and DFL; and superior scores in PMC. This aligns with the known language hierarchy for spoken language understanding (Nastase et al., 2020). Tasks such as PD, Summarization, and NLI seem to yield better performance in predicting the brain responses than the other NLP tasks across all the ROIs. These Pearson correlation (τ ) results are comparatively much higher compared to those obtained using pretrained (task-agnostic) GPT2 model in (Caucheteux et al., 2021a) (τ ranging from 0.02 − 0.06). As shown in Fig. 2, our method obtains much higher correlations (τ ranging from 0.02 − 0.229). Similar to the Pereira dataset, we estimate the statistical significance of the performance differences using the one-way ANOVA test. The main effect of task was significant for all the ROIs with p≤ 10−3 with confidence 95% (see Appendix for detailed ANOVA results). Also, Post hoc pairwise comparisons (Ruxton and Beauchamp, 2008) revealed that on both 2V2 accuracy and Pearson correlation measures, tasks such as PD, Sum, and NLI performed significantly better compared to other tasks (see Appendix for pairwise comparison results).\nFurther, from Fig. 2, we see that the bilateral posterior medial cortex (PMC) associated with higher language function exhibits a higher correlation among all the brain ROIs. ROIs, including bilateral TPOJ and bilateral DFL, yield higher correlations with the five NLP tasks, which is in line\nwith the language processing hierarchy in the human brain. Finally, across all regions, pretrained BERT model has worse correlation compared to at least 5 other task models.\nIn summary, different and distinct language Taskonomy features seem to be related to the encoding performance in reading versus listening tasks. CR, NER, SRL, and SS perform better for reading. PD, Sum, and NLI perform better for listening. While listening the subject is cognitively more involved in the activity compared to reading (Buchweitz et al., 2009). Thus, it makes sense that shallow tasks like NER and SS are useful for reading while more complex NLP tasks like PD, Sum and NLI are effective for encoding listening stimuli."
    }, {
      "heading" : "5.2 Language Task Similarity Computation",
      "text" : "Pearson correlation values between predicted responses for each pair of tasks were used to construct the similarity matrix with heatmap for both Pereira and Narratives-Pieman datasets, as shown in Figs. 3 and 4. We observe that the following task pairs are highly correlated for the Pereira dataset:\n(NER and CR), (SS and CR) and (PD and Sum). Also these task pairs are highly correlated for the Narratives-Pieman dataset: (CR and NLI), (NLI and SA) and (PD and Sum). Similarities are relatively higher for Narratives-Pieman compared to the Pereira dataset. Surprisingly, the (NLI, SA) pair has lowest similarity for Pereira (reading) and close to highest in Narratives-Pieman (listening). We hypothesize that this is because sentiment is best conveyed while the subject is listening. Reading sentences (Pereira): The stimulus sentences from the Pereira dataset were fed as input to each of the 10 task Transformers. The similarity among the resulting representations was analyzed using hierarchical clustering, and the clusters are visualized as dendrograms in Fig. 5 (left). We observe that the tasks are clustered into three groups denoted using red, green, and blue colors. Next, we wished to check if similar task grouping is observed on brain activations predicted by ridge regression trained on task-specific representations. Hence, similar clustering analysis was conducted on the neural space representations, and the clusters are visualized as dendrograms in Fig. 5 (right) across all subjects. Interestingly, the tree derived from brain representation also shows a similar distribution of tasks across the three groups. Similar dendrograms for individual subjects are illustrated in Appendix-Fig. 11. Listening Stories (Narratives-Pieman): Fig. 6 compares the task similarity tree based on the patterns from the pretrained task Transformers, with the task similarity tree generated based on similar-\nity in brain response prediction performance averaged across all subjects. We observe that the tasks are clustered into three groups denoted using red, green, and blue colors. Again, the tree derived from brain representation also shows a similar distribution of tasks across the three groups. Dendrograms for individual subjects are in the Appendix-Fig. 12."
    }, {
      "heading" : "5.3 Brain maps for whole brain predictions",
      "text" : "The mean absolute error (MAE) between predictive and actual responses is obtained using individual task features from the taskonomy. MAE values are obtained for all the voxels in the brain for both the reading (Fig. 7) and listening datasets (Fig. 8).\nIn the reading task, we observe from Fig. 7 that CR has lower MAE compared to PD which in turn has lower MAE compared to the NLI task (brain maps for the other tasks are reported in Fig. 17 in the Appendix). Overall, for the reading stimuli, tasks such as NLI, QA, and SA display higher MAE values. To further investigate which sub ROIs (LPTG, LMTG, LATG, LFus, Lpar, Lang, LIFGorb, LIFG, LaMFG, LpMFG, and LmMFG) of the Language network are related to the predictive task features, we train encoding models for all the sub ROIs for the best encoding task, i.e., for the CR task (see Fig. 14 in Appendix). We notice that both LMTG (middle temporal gyrus) and LPTG (posterior temporal gyrus) are more accurately predicted than the other sub ROIs. On the other hand, LIFG-orb displays a lower Pearson correlation for the CR task. The presence of superior encoding information in the ROIs in the temporal gyrus as compared to those in the inferior frontal gyrus seems to mirror similar observations seen in decoder performance (Anderson et al., 2017b).\nOn the other hand, in the listening task, we observe from Fig. 8 that Paraphrase and WSD display lower MAE values compared to QA task (brain maps for the other tasks are reported in Fig. 18 in the Appendix). Taken together, for listening\nstimuli, tasks such as NER, QA, SA, CR, and SS display higher MAE values. From Fig. 8, we see that ROIs such as EAC and AAC have higher MAE compared to PMC and TPOJ brain ROIs.\nWe further demonstrate the prediction performance of the encoder model trained on sub ROIs for the paraphrase task in Fig. 15 in the Appendix. It can be observed that sub ROIs such as Pos1 and Pos2 have a higher Pearson correlation than other sub ROIs of the PMC region. Both sfl and l55b display a higher correlation among all the sub ROIs for the DFL ROI. However, all the sub ROIs in the TPOJ yield higher correlation, as shown in Fig. 15. The control and attention ROIs in the posterior cingulate cortex (for ex., POS1 in PMC), together with the superior frontal language region (sfl in DFL) and TPOJ, are part of the well-known language network associated with narrative comprehension (Nastase et al., 2020), and it is heartening to see that task features from PD task also relate to semantic analysis of the ongoing narrative."
    }, {
      "heading" : "5.4 Discussion",
      "text" : "(1) We used a ridge regression model instead of more complicated models for encoding. We believe that more complex models can lead to further\nexciting insights. (2) We experimented with 10 NLP tasks. Models can be pretrained models for more such tasks to check if other tasks are better predictive of voxel activations. (3) We leveraged models finetuned using datasets of different sizes across tasks. While a fair comparison of dataset sizes across tasks is impossible, we understand that this could have resulted in some bias in our results. (4) We used a different dataset for reading vs listening. While we believe that the differences in task-specific model performances across reading and listening are mainly due to the learned stimulus representations, but they could also arise from other factors such as experimental conditions, the text domain of the stimuli or number of voxels, etc."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we studied the effectiveness of task specific NLP models for brain encoding. We observe that building individual encoding models and exploiting existing relationships among models can provide a more in-depth understanding of the neural representation of language information. Our experiments on Pereira and Narrative datasets lead to interesting cognitive insights."
    }, {
      "heading" : "7 Ethical Statement",
      "text" : "We reused publicly available datasets for this work: Pereira and Narratives. We did not collect any new dataset.\nPereira dataset can be downloaded from https: //osf.io/crwz7/. Please read their terms of use1 for more details.\nNarratives dataset can be dowloaded from https://datasets.datalad.org/ ?dir=/labs/hasson/narratives. Please read their terms of use2 for more details.\nWe do not foresee any harmful uses of this technology."
    }, {
      "heading" : "A Details of the Finetuned Models",
      "text" : "We selected tasks for which BERT-base-cased finetuned models were available. Note that we did not finetune any of these models ourselves but leveraged the state-of-the-art finetuned models available on Huggingface. Details of the specific finetuned model checkpoints are mentioned in Table 1."
    }, {
      "heading" : "B Dataset Details",
      "text" : ""
    }, {
      "heading" : "C ANOVA test results",
      "text" : "C.1 Pereira dataset The main effect of model was significant for the ROIs with 95% confidence with these statistics:\n• Language_LH: [F(9, 40) = 3.95, p=0.0052]\n• Language_RH: [F(9, 40) = 4.53, p=0.0015]\n• Vision_Body: [F(9, 40) = 4.397, p=0.00227]\n• Vision_Face: [F(9, 40) = 3.46, p=0.0085]\n• Vision_Object: [F(9, 40) = 3.40, p=0.0121]\n• Vision_Scenes: [F(9, 40) = 4.917, p=0.0007]\n• Vision: [F(9, 40) = 3.945, p=0.00385]\n• DMN: [F(9, 40) = 6.28, p=0.00034]\n• TP: [F(9, 40) = 6.54, p=0.00042]\nC.2 Narratives-Pieman dataset The main effect of model was significant for the ROIs with 95% confidence with these statistics:\n• EAC_L [F(9,810)=3.88, p=.00009]\n• EAC_R [F(9,810)=3.34, p=.00055]\n• AAC_L [F(9,810)=5.37, p=.0000007]\n• AAC_R [F(9,810)=6.955, p=.00000]\n• PMC_L [F(9,810)=37.21, p=.00000]\n• PMC_R [F(9,810)=31.62, p=.00000]\n• TPOJ_L [F(9,810)=9.166, p=.00000]\n• TPOJ_R [F(9,810)=7.797, p=.00000]\n• DFL_L [F(9,810)=12.445, p=.00000]\n• DFL_R [F(9,810)=12.27, p=.00000]"
    } ],
    "references" : [ {
      "title" : "Visually grounded and textual semantic models differentially decode brain activity associated with concrete and abstract nouns",
      "author" : [ "Andrew J Anderson", "Douwe Kiela", "Stephen Clark", "Massimo Poesio." ],
      "venue" : "Transactions of the Association for Computational",
      "citeRegEx" : "Anderson et al\\.,? 2017a",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2017
    }, {
      "title" : "Predicting neural activity patterns associated with sentences using a neurobio",
      "author" : [ "Andrew James Anderson", "Jeffrey R Binder", "Leonardo Fernandino", "Colin J Humphries", "Lisa L Conant", "Mario Aguilar", "Xixi Wang", "Donias Doko", "Rajeev DS Raizada" ],
      "venue" : null,
      "citeRegEx" : "Anderson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2017
    }, {
      "title" : "An integrated neural decoder of linguistic and experiential meaning",
      "author" : [ "Andrew James Anderson", "Jeffrey R Binder", "Leonardo Fernandino", "Colin J Humphries", "Lisa L Conant", "Rajeev DS Raizada", "Feng Lin", "Edmund C Lalor." ],
      "venue" : "Journal of Neuroscience,",
      "citeRegEx" : "Anderson et al\\.,? 2019",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoding individual identity from brain activity elicited in imagining common experiences",
      "author" : [ "Andrew James Anderson", "Kelsey McDermott", "Brian Rooks", "Kathi L Heffner", "David Dodell-Feder", "Feng V Lin." ],
      "venue" : "Nature communications, 11(1):1–14.",
      "citeRegEx" : "Anderson et al\\.,? 2020",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2020
    }, {
      "title" : "Low-dimensional structure in the space of language representations is reflected in brain responses",
      "author" : [ "Richard Antonello", "Javier Turek", "Vy Vo", "Alexander Huth." ],
      "venue" : "arXiv preprint arXiv:2106.05426.",
      "citeRegEx" : "Antonello et al\\.,? 2021",
      "shortCiteRegEx" : "Antonello et al\\.",
      "year" : 2021
    }, {
      "title" : "A map of object space in primate inferotemporal cortex",
      "author" : [ "Pinglei Bao", "Liang She", "Mason McGill", "Doris Y Tsao." ],
      "venue" : "Nature, 583(7814):103–108.",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Cortical network responses map onto data-driven features that capture visual semantics of movie fragments",
      "author" : [ "Nick F Ramsey." ],
      "venue" : "Scientific reports, 10(1):1–21.",
      "citeRegEx" : "Ramsey.,? 2020",
      "shortCiteRegEx" : "Ramsey.",
      "year" : 2020
    }, {
      "title" : "Where is the semantic system? a critical review and meta-analysis of 120 functional neuroimaging studies",
      "author" : [ "Jeffrey R Binder", "Rutvik H Desai", "William W Graves", "Lisa L Conant." ],
      "venue" : "Cerebral cortex, 19(12):2767–2796.",
      "citeRegEx" : "Binder et al\\.,? 2009",
      "shortCiteRegEx" : "Binder et al\\.",
      "year" : 2009
    }, {
      "title" : "Brain activation for reading and listening comprehension: An fmri study of modality effects and individual differences in language comprehension",
      "author" : [ "Augusto Buchweitz", "Robert A Mason", "Lêda Tomitch", "Marcel Adam Just." ],
      "venue" : "Psychology & neuroscience,",
      "citeRegEx" : "Buchweitz et al\\.,? 2009",
      "shortCiteRegEx" : "Buchweitz et al\\.",
      "year" : 2009
    }, {
      "title" : "Disentangling syntax and semantics in the brain with deep networks",
      "author" : [ "Charlotte Caucheteux", "Alexandre Gramfort", "JeanRemi King." ],
      "venue" : "International Conference on Machine Learning, pages 1336–1348. PMLR.",
      "citeRegEx" : "Caucheteux et al\\.,? 2021a",
      "shortCiteRegEx" : "Caucheteux et al\\.",
      "year" : 2021
    }, {
      "title" : "Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects",
      "author" : [ "Charlotte Caucheteux", "Alexandre Gramfort", "JeanRémi King." ],
      "venue" : "arXiv preprint arXiv:2110.06078.",
      "citeRegEx" : "Caucheteux et al\\.,? 2021b",
      "shortCiteRegEx" : "Caucheteux et al\\.",
      "year" : 2021
    }, {
      "title" : "Sentence complexity and input modality effects in sentence comprehension: an fmri study",
      "author" : [ "R Todd Constable", "Kenneth R Pugh", "Ella Berroya", "W Einar Mencl", "Michael Westerveld", "Weijia Ni", "Donald Shankweiler." ],
      "venue" : "NeuroImage, 22(1):11–21.",
      "citeRegEx" : "Constable et al\\.,? 2004",
      "shortCiteRegEx" : "Constable et al\\.",
      "year" : 2004
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Linking artificial and human neural representations of language",
      "author" : [ "Jon Gauthier", "Roger Levy." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "Gauthier and Levy.,? 2019",
      "shortCiteRegEx" : "Gauthier and Levy.",
      "year" : 2019
    }, {
      "title" : "A multimodal parcellation of human cerebral cortex",
      "author" : [ "Matthew F Glasser", "Timothy S Coalson", "Emma C Robinson", "Carl D Hacker", "John Harwell", "Essa Yacoub", "Kamil Ugurbil", "Jesper Andersson", "Christian F Beckmann", "Mark Jenkinson" ],
      "venue" : null,
      "citeRegEx" : "Glasser et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Glasser et al\\.",
      "year" : 2016
    }, {
      "title" : "How concepts are encoded in the human brain: a modality independent, category-based cortical organization",
      "author" : [ "Giacomo Handjaras", "Emiliano Ricciardi", "Andrea Leo", "Alessandro Lenci", "Luca Cecchetti", "Mirco Cosottini", "Giovanna Marotta", "Pietro Pietrini" ],
      "venue" : null,
      "citeRegEx" : "Handjaras et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Handjaras et al\\.",
      "year" : 2016
    }, {
      "title" : "Cognival: A framework for cognitive word embedding evaluation",
      "author" : [ "Nora Hollenstein", "Antonio de la Torre", "Nicolas Langer", "Ce Zhang." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 538–549.",
      "citeRegEx" : "Hollenstein et al\\.,? 2019",
      "shortCiteRegEx" : "Hollenstein et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural speech reveals the semantic maps that tile human cerebral cortex",
      "author" : [ "Alexander G Huth", "Wendy A De Heer", "Thomas L Griffiths", "Frédéric E Theunissen", "Jack L Gallant." ],
      "venue" : "Nature, 532(7600):453– 458.",
      "citeRegEx" : "Huth et al\\.,? 2016",
      "shortCiteRegEx" : "Huth et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating context into language encoding models for fmri",
      "author" : [ "Shailee Jain", "Alexander G Huth." ],
      "venue" : "Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 6629–6638.",
      "citeRegEx" : "Jain and Huth.,? 2018",
      "shortCiteRegEx" : "Jain and Huth.",
      "year" : 2018
    }, {
      "title" : "Relating simple sentence representations in deep neural networks and the brain",
      "author" : [ "S Jat", "H Tang", "P Talukdar", "T Mitchel." ],
      "venue" : "ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages",
      "citeRegEx" : "Jat et al\\.,? 2020",
      "shortCiteRegEx" : "Jat et al\\.",
      "year" : 2020
    }, {
      "title" : "Recurrence is required to capture the representational dynamics of the human visual system",
      "author" : [ "Tim C Kietzmann", "Courtney J Spoerer", "Lynn KA Sörensen", "Radoslaw M Cichy", "Olaf Hauk", "Nikolaus Kriegeskorte." ],
      "venue" : "Proceedings of the National Academy",
      "citeRegEx" : "Kietzmann et al\\.,? 2019",
      "shortCiteRegEx" : "Kietzmann et al\\.",
      "year" : 2019
    }, {
      "title" : "Predicting human brain activity associated with the meanings of nouns",
      "author" : [ "Tom M Mitchell", "Svetlana V Shinkareva", "Andrew Carlson", "Kai-Min Chang", "Vicente L Malave", "Robert A Mason", "Marcel Adam Just." ],
      "venue" : "science, 320(5880):1191–1195.",
      "citeRegEx" : "Mitchell et al\\.,? 2008",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2008
    }, {
      "title" : "Leveraging shared connectivity to aggregate heterogeneous datasets into a common response space",
      "author" : [ "Samuel A Nastase", "Yun-Fei Liu", "Hanna Hillman", "Kenneth A Norman", "Uri Hasson." ],
      "venue" : "NeuroImage, 217:116865.",
      "citeRegEx" : "Nastase et al\\.,? 2020",
      "shortCiteRegEx" : "Nastase et al\\.",
      "year" : 2020
    }, {
      "title" : "Narratives: fmri data for evaluating models of naturalistic language comprehension",
      "author" : [ "Samuel A Nastase", "Yun-Fei Liu", "Hanna Hillman", "Asieh Zadbood", "Liat Hasenfratz", "Neggin Keshavarzian", "Janice Chen", "Christopher J Honey", "Yaara Yeshurun", "Mor Regev" ],
      "venue" : null,
      "citeRegEx" : "Nastase et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Nastase et al\\.",
      "year" : 2021
    }, {
      "title" : "Word statistics in large-scale texts explain the human cortical semantic representation of objects, actions, and impressions",
      "author" : [ "Satoshi Nishida", "Alexander G Huth", "Jack L Gallant", "Shinji Nishimoto." ],
      "venue" : "Society for Neuroscience Annual Meeting, volume 333.",
      "citeRegEx" : "Nishida et al\\.,? 2015",
      "shortCiteRegEx" : "Nishida et al\\.",
      "year" : 2015
    }, {
      "title" : "fmri semantic category decoding using linguistic encoding of word embeddings",
      "author" : [ "Subba Reddy Oota", "Naresh Manwani", "Raju S Bapi." ],
      "venue" : "International Conference on Neural Information Processing, pages 3–15. Springer.",
      "citeRegEx" : "Oota et al\\.,? 2018",
      "shortCiteRegEx" : "Oota et al\\.",
      "year" : 2018
    }, {
      "title" : "Stepencog: A convolutional lstm autoencoder for near-perfect fmri encoding",
      "author" : [ "Subba Reddy Oota", "Vijay Rowtula", "Manish Gupta", "Raju S Bapi." ],
      "venue" : "2019 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.",
      "citeRegEx" : "Oota et al\\.,? 2019",
      "shortCiteRegEx" : "Oota et al\\.",
      "year" : 2019
    }, {
      "title" : "Using wikipedia to learn semantic feature representations of concrete concepts in neuroimaging experiments",
      "author" : [ "Francisco Pereira", "Matthew Botvinick", "Greg Detre." ],
      "venue" : "Artificial intelligence, 194:240–252.",
      "citeRegEx" : "Pereira et al\\.,? 2013",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2013
    }, {
      "title" : "Decoding of generic mental representations from functional mri data using word embeddings",
      "author" : [ "Francisco Pereira", "Bin Lou", "Brianna Pritchett", "Nancy Kanwisher", "Matthew Botvinick", "Evelina Fedorenko." ],
      "venue" : "bioRxiv, page 057216.",
      "citeRegEx" : "Pereira et al\\.,? 2016",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2016
    }, {
      "title" : "Toward a universal decoder of linguistic meaning from brain activation",
      "author" : [ "Francisco Pereira", "Bin Lou", "Brianna Pritchett", "Samuel Ritter", "Samuel J Gershman", "Nancy Kanwisher", "Matthew Botvinick", "Evelina Fedorenko." ],
      "venue" : "Nature communications,",
      "citeRegEx" : "Pereira et al\\.,? 2018",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2018
    }, {
      "title" : "Visual and linguistic semantic representations are aligned at the border of human visual cortex",
      "author" : [ "Sara F Popham", "Alexander G Huth", "Natalia Y Bilenko", "Fatma Deniz", "James S Gao", "Anwar O Nunez-Elizalde", "Jack L Gallant." ],
      "venue" : "Nature Neuroscience,",
      "citeRegEx" : "Popham et al\\.,? 2021",
      "shortCiteRegEx" : "Popham et al\\.",
      "year" : 2021
    }, {
      "title" : "Time for some a priori thinking about post hoc testing",
      "author" : [ "Graeme D Ruxton", "Guy Beauchamp." ],
      "venue" : "Behavioral ecology, 19(3):690–693.",
      "citeRegEx" : "Ruxton and Beauchamp.,? 2008",
      "shortCiteRegEx" : "Ruxton and Beauchamp.",
      "year" : 2008
    }, {
      "title" : "The neural architecture of language: Integrative reverseengineering converges on a model for predictive",
      "author" : [ "Martin Schrimpf", "Idan Blank", "Greta Tuckute", "Carina Kauf", "Eghbal A Hosseini", "Nancy Kanwisher", "Joshua Tenenbaum", "Evelina Fedorenko" ],
      "venue" : null,
      "citeRegEx" : "Schrimpf et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Schrimpf et al\\.",
      "year" : 2021
    }, {
      "title" : "Inducing brain-relevant bias in natural language processing models",
      "author" : [ "Dan Schwartz", "Mariya Toneva", "Leila Wehbe." ],
      "venue" : "Advances in Neural Information Processing Systems, 32:14123–14133.",
      "citeRegEx" : "Schwartz et al\\.,? 2019",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards sentence-level brain decoding with distributed representations",
      "author" : [ "Jingyuan Sun", "Shaonan Wang", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7047–7054.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural encoding and decoding with distributed sentence representations",
      "author" : [ "Jingyuan Sun", "Shaonan Wang", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, 32(2):589–603.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling task effects on meaning representation in the brain via zero-shot meg prediction",
      "author" : [ "Mariya Toneva", "Otilia Stretcu", "Barnabás Póczos", "Leila Wehbe", "Tom M Mitchell." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Toneva et al\\.,? 2020",
      "shortCiteRegEx" : "Toneva et al\\.",
      "year" : 2020
    }, {
      "title" : "Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)",
      "author" : [ "Mariya Toneva", "Leila Wehbe." ],
      "venue" : "arXiv preprint arXiv:1905.11833.",
      "citeRegEx" : "Toneva and Wehbe.,? 2019",
      "shortCiteRegEx" : "Toneva and Wehbe.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyz-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural taskonomy: Inferring the similarity of taskderived representations from brain activity",
      "author" : [ "Aria Wang", "Michael Tarr", "Leila Wehbe." ],
      "venue" : "Advances in Neural Information Processing Systems, 32:15501– 15511.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Predicting the brain activation pattern associated with the propositional content of a sentence: Modeling neural representations of events and states",
      "author" : [ "Jing Wang", "Vladimir L Cherkassky", "Marcel Adam Just." ],
      "venue" : "Human brain mapping, 38(10):4865–4881.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Fine-grained neural decoding with distributed word representations",
      "author" : [ "Shaonan Wang", "Jiajun Zhang", "Haiyan Wang", "Nan Lin", "Chengqing Zong." ],
      "venue" : "Information Sciences, 507:256–272.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Simultaneously uncovering the patterns of brain regions involved in different story reading subprocesses",
      "author" : [ "Leila Wehbe", "Brian Murphy", "Partha Talukdar", "Alona Fyshe", "Aaditya Ramdas", "Tom Mitchell." ],
      "venue" : "press.",
      "citeRegEx" : "Wehbe et al\\.,? 2014",
      "shortCiteRegEx" : "Wehbe et al\\.",
      "year" : 2014
    }, {
      "title" : "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
      "author" : [ "Daniel LK Yamins", "Ha Hong", "Charles F Cadieu", "Ethan A Solomon", "Darren Seibert", "James J DiCarlo." ],
      "venue" : "Proceedings of the national academy of sciences,",
      "citeRegEx" : "Yamins et al\\.,? 2014",
      "shortCiteRegEx" : "Yamins et al\\.",
      "year" : 2014
    }, {
      "title" : "0.000019 NER NLI 0.000619887 NER SS 0.040 NER PD 0.000399",
      "author" : [ "PD CR" ],
      "venue" : "NLI SS 1.61916e-10",
      "citeRegEx" : "CR,? \\Q1916\\E",
      "shortCiteRegEx" : "CR",
      "year" : 1916
    }, {
      "title" : "0.000298933 NER NLI 0.024695 NER PD 0.0020556",
      "author" : [ "PD CR" ],
      "venue" : "NLI SS 4.16645e-8 NLI Sum",
      "citeRegEx" : "CR,? \\Q2055\\E",
      "shortCiteRegEx" : "CR",
      "year" : 2055
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : ", (Constable et al., 2004)], researchers have been interested in understanding how the neural encoding models predict the fMRI brain activity.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 44,
      "context" : "Several brain encoding models have been developed to (i) understand the ventral stream in biological vision (Yamins et al., 2014; Kietzmann et al., 2019; Bao et al., 2020), and (ii) to study the higher-level cognition like language processing (Gauthier and Levy, 2019; Schrimpf et al.",
      "startOffset" : 108,
      "endOffset" : 171
    }, {
      "referenceID" : 20,
      "context" : "Several brain encoding models have been developed to (i) understand the ventral stream in biological vision (Yamins et al., 2014; Kietzmann et al., 2019; Bao et al., 2020), and (ii) to study the higher-level cognition like language processing (Gauthier and Levy, 2019; Schrimpf et al.",
      "startOffset" : 108,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "Several brain encoding models have been developed to (i) understand the ventral stream in biological vision (Yamins et al., 2014; Kietzmann et al., 2019; Bao et al., 2020), and (ii) to study the higher-level cognition like language processing (Gauthier and Levy, 2019; Schrimpf et al.",
      "startOffset" : 108,
      "endOffset" : 171
    }, {
      "referenceID" : 24,
      "context" : "Some recent studies (Nishida et al., 2015; Huth et al., 2016) have been able to identify brain ROIs that respond to words that have a similar meaning and have thus built a “semantic atlas” of how the human brain organizes language.",
      "startOffset" : 20,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Some recent studies (Nishida et al., 2015; Huth et al., 2016) have been able to identify brain ROIs that respond to words that have a similar meaning and have thus built a “semantic atlas” of how the human brain organizes language.",
      "startOffset" : 20,
      "endOffset" : 61
    }, {
      "referenceID" : 25,
      "context" : "studies (Oota et al., 2018; Jain and Huth, 2018; Hollenstein et al., 2019) have used a wide variety of word embeddings where words represented as vectors in an embedding space are mapped to brain activation for improved neural coding.",
      "startOffset" : 8,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "studies (Oota et al., 2018; Jain and Huth, 2018; Hollenstein et al., 2019) have used a wide variety of word embeddings where words represented as vectors in an embedding space are mapped to brain activation for improved neural coding.",
      "startOffset" : 8,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "studies (Oota et al., 2018; Jain and Huth, 2018; Hollenstein et al., 2019) have used a wide variety of word embeddings where words represented as vectors in an embedding space are mapped to brain activation for improved neural coding.",
      "startOffset" : 8,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "based models like BERT (Devlin et al., 2019) have been found to be very effective across a large number of natural language processing (NLP) tasks.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : "Interestingly, such Transformer-based neural representations have been found to be very effective for brain encoding as well (Schrimpf et al., 2021).",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 32,
      "context" : "Despite the recent advances in mapping between language transformers and the brain activity recorded with reading (Schrimpf et al., 2021), the Transformer features themselves are notoriously difficult to interpret.",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 40,
      "context" : "Recently, a study using multiple computer vision tasks has shown that 3D vision task models predict better fMRI brain activity than 2D vision task models (Wang et al., 2019) for visual stim-",
      "startOffset" : 154,
      "endOffset" : 173
    }, {
      "referenceID" : 40,
      "context" : "Inspired by the success of correlations in the vision field (Wang et al., 2019), and brain encoding study of a variety of language Transformer models (Schrimpf et al.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "counts (Mitchell et al., 2008; Pereira et al., 2013; Huth et al., 2016), syntactic and discourse features (Wehbe et al.",
      "startOffset" : 7,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : "counts (Mitchell et al., 2008; Pereira et al., 2013; Huth et al., 2016), syntactic and discourse features (Wehbe et al.",
      "startOffset" : 7,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "counts (Mitchell et al., 2008; Pereira et al., 2013; Huth et al., 2016), syntactic and discourse features (Wehbe et al.",
      "startOffset" : 7,
      "endOffset" : 71
    }, {
      "referenceID" : 43,
      "context" : ", 2016), syntactic and discourse features (Wehbe et al., 2014).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 28,
      "context" : "resentation models include distributed word embeddings (Pereira et al., 2016; Anderson et al., 2017a; Pereira et al., 2018; Toneva and Wehbe, 2019; Hollenstein et al., 2019; Wang et al., 2020), sentence representation models (Sun et al.",
      "startOffset" : 55,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "resentation models include distributed word embeddings (Pereira et al., 2016; Anderson et al., 2017a; Pereira et al., 2018; Toneva and Wehbe, 2019; Hollenstein et al., 2019; Wang et al., 2020), sentence representation models (Sun et al.",
      "startOffset" : 55,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "resentation models include distributed word embeddings (Pereira et al., 2016; Anderson et al., 2017a; Pereira et al., 2018; Toneva and Wehbe, 2019; Hollenstein et al., 2019; Wang et al., 2020), sentence representation models (Sun et al.",
      "startOffset" : 55,
      "endOffset" : 192
    }, {
      "referenceID" : 37,
      "context" : "resentation models include distributed word embeddings (Pereira et al., 2016; Anderson et al., 2017a; Pereira et al., 2018; Toneva and Wehbe, 2019; Hollenstein et al., 2019; Wang et al., 2020), sentence representation models (Sun et al.",
      "startOffset" : 55,
      "endOffset" : 192
    }, {
      "referenceID" : 16,
      "context" : "resentation models include distributed word embeddings (Pereira et al., 2016; Anderson et al., 2017a; Pereira et al., 2018; Toneva and Wehbe, 2019; Hollenstein et al., 2019; Wang et al., 2020), sentence representation models (Sun et al.",
      "startOffset" : 55,
      "endOffset" : 192
    }, {
      "referenceID" : 42,
      "context" : "resentation models include distributed word embeddings (Pereira et al., 2016; Anderson et al., 2017a; Pereira et al., 2018; Toneva and Wehbe, 2019; Hollenstein et al., 2019; Wang et al., 2020), sentence representation models (Sun et al.",
      "startOffset" : 55,
      "endOffset" : 192
    }, {
      "referenceID" : 18,
      "context" : ", 2020), recurrent neural networks (Jain and Huth, 2018; Oota et al., 2019), and Transformer-based language models (Gauthier and Levy, 2019; Toneva and Wehbe, 2019; Schwartz et al.",
      "startOffset" : 35,
      "endOffset" : 75
    }, {
      "referenceID" : 26,
      "context" : ", 2020), recurrent neural networks (Jain and Huth, 2018; Oota et al., 2019), and Transformer-based language models (Gauthier and Levy, 2019; Toneva and Wehbe, 2019; Schwartz et al.",
      "startOffset" : 35,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : ", 2019), and Transformer-based language models (Gauthier and Levy, 2019; Toneva and Wehbe, 2019; Schwartz et al., 2019).",
      "startOffset" : 47,
      "endOffset" : 119
    }, {
      "referenceID" : 37,
      "context" : ", 2019), and Transformer-based language models (Gauthier and Levy, 2019; Toneva and Wehbe, 2019; Schwartz et al., 2019).",
      "startOffset" : 47,
      "endOffset" : 119
    }, {
      "referenceID" : 33,
      "context" : ", 2019), and Transformer-based language models (Gauthier and Levy, 2019; Toneva and Wehbe, 2019; Schwartz et al., 2019).",
      "startOffset" : 47,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : "degree of association with different attributes of experience, typically on a scale of 0-6 (Anderson et al., 2019, 2020; Berezutskaya et al., 2020; Jat et al., 2020; Caucheteux et al., 2021a; Antonello et al., 2021) or binary (Handjaras et al.",
      "startOffset" : 91,
      "endOffset" : 215
    }, {
      "referenceID" : 9,
      "context" : "degree of association with different attributes of experience, typically on a scale of 0-6 (Anderson et al., 2019, 2020; Berezutskaya et al., 2020; Jat et al., 2020; Caucheteux et al., 2021a; Antonello et al., 2021) or binary (Handjaras et al.",
      "startOffset" : 91,
      "endOffset" : 215
    }, {
      "referenceID" : 4,
      "context" : "degree of association with different attributes of experience, typically on a scale of 0-6 (Anderson et al., 2019, 2020; Berezutskaya et al., 2020; Jat et al., 2020; Caucheteux et al., 2021a; Antonello et al., 2021) or binary (Handjaras et al.",
      "startOffset" : 91,
      "endOffset" : 215
    }, {
      "referenceID" : 9,
      "context" : "The resulting models are better able to disentangle the corresponding brain responses in fMRI (Caucheteux et al., 2021a).",
      "startOffset" : 94,
      "endOffset" : 120
    }, {
      "referenceID" : 29,
      "context" : "As in (Pereira et al., 2018), we focused on nine brain ROIs (regions of interest) corresponding to four brain networks: (i) Default Mode Network (DMN) (linked to the functionality of semantic processing), (ii) Language Network (related to language processing, understanding, word meaning, and sentence comprehension), (iii) Task Positive Network (TP) (related to attention, salience information), and (iv) Visual Network (related to the processing of visual",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 23,
      "context" : "6 hours of unique stimuli (∼43,000 words) was proposed in (Nastase et al., 2021).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Similar to earlier works (Caucheteux et al., 2021b), we analyze data from 82 subjects listening to the story titled ‘PieMan’ with 259 TRs (repetition time – fMRI recorded every 1.",
      "startOffset" : 25,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "We use the multi-modal parcellation of the human cerebral cortex (Glassar Atlas: consists of 180 ROIs in each hemisphere) to display the brain maps (Glasser et al., 2016), since Narratives dataset contains annotations tied to this atlas.",
      "startOffset" : 148,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "Following literature on brain encoding (Caucheteux et al., 2021b; Toneva et al., 2020), we choose to use a ridge regression model instead of more complicated models.",
      "startOffset" : 39,
      "endOffset" : 86
    }, {
      "referenceID" : 36,
      "context" : "Following literature on brain encoding (Caucheteux et al., 2021b; Toneva et al., 2020), we choose to use a ridge regression model instead of more complicated models.",
      "startOffset" : 39,
      "endOffset" : 86
    }, {
      "referenceID" : 39,
      "context" : "(2) We wanted to select tasks that are a part of popular NLP benchmarks like GLUE (Wang et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 31,
      "context" : "Further, post hoc pairwise comparisons (Ruxton and Beauchamp, 2008) confirmed the visual observations that on both 2V2 accuracy",
      "startOffset" : 39,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "This is in line with the left hemisphere dominance for language processing (Binder et al., 2009).",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : "Higher correlations with all the visual brain regions point to the possible alignment of visual and language regions for semantic understanding (Popham et al., 2021) in a reading task.",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "for spoken language understanding (Nastase et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "results are comparatively much higher compared to those obtained using pretrained (task-agnostic) GPT2 model in (Caucheteux et al., 2021a) (τ ranging from 0.",
      "startOffset" : 112,
      "endOffset" : 138
    }, {
      "referenceID" : 31,
      "context" : "Also, Post hoc pairwise comparisons (Ruxton and Beauchamp, 2008) revealed that on both 2V2 accuracy and Pearson correlation measures, tasks such as PD, Sum, and NLI performed significantly better compared to other tasks (see Appendix for pairwise comparison results).",
      "startOffset" : 36,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "While listening the subject is cognitively more involved in the activity compared to reading (Buchweitz et al., 2009).",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : ", POS1 in PMC), together with the superior frontal language region (sfl in DFL) and TPOJ, are part of the well-known language network associated with narrative comprehension (Nastase et al., 2020), and it is heartening to see that task features from PD task also relate to semantic analysis of the ongoing narrative.",
      "startOffset" : 174,
      "endOffset" : 196
    } ],
    "year" : 0,
    "abstractText" : "Several popular Transformer based language models have been found to be successful for text-driven brain encoding. However, existing literature leverages only pretrained text Transformer models and has not explored the efficacy of task-specific learned Transformer representations. In this work, we explore transfer learning from representations learned for ten popular natural language processing tasks (two syntactic and eight semantic) for predicting brain responses from two diverse datasets: Pereira (subjects reading sentences from paragraphs) and Narratives (subjects listening to the spoken stories). Encoding models based on task features are used to predict activity in different regions across the whole brain. Features from coreference resolution, NER, and shallow syntax parsing explain greater variance for the reading activity. On the other hand, for the listening activity, tasks such as paraphrase generation, summarization, and natural language inference show better encoding performance. Experiments across all 10 task representations provide the following cognitive insights: (i) language left hemisphere has higher predictive brain activity versus language right hemisphere, (ii) posterior medial cortex, temporoparieto-occipital junction, dorsal frontal lobe have higher correlation versus early auditory and auditory association cortex, (iii) syntactic and semantic tasks display a good predictive performance across brain regions for reading and listening stimuli resp.",
    "creator" : null
  }
}
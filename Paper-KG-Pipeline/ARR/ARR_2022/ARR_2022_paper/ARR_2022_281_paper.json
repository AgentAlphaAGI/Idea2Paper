{
  "name" : "ARR_2022_281_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Domain Confused Contrastive Learning for Unsupervised Domain Adaptation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have yielded considerable improvements with datasets drawing from various sources. However, the lack of portability of language model to adapt to a new textual domain remains a central issue (Gururangan et al., 2020), especially when the training set and testing set do not follow the same underlying distribution (changing of topic and genres) - usually referred to as domain shift. In this paper, we focus on studying Unsupervised Domain Adaptation (UDA). UDA aims at designing adaptation algorithms that attempt to generalize well on the target domain by learning from both labeled samples from the source domain and unlabeled samples from the target domain. Studying UDA fits real-world scenarios since labeled data in the target domain is usually absent. Moreover, advances in UDA will also help out-of-\ndistribution generalizations (Ramponi and Plank, 2020; Krueger et al., 2021).\nExtensive algorithms have been proposed to mitigate the domain shift problem, for example, domain adversarial neural network (DANN) (Ganin et al., 2016) and distribution matching (Gretton et al., 2012; Zhuang et al., 2015). For DANN, the training process of joint optimization is unstable, requiring extensive effort to tune the hyperparameters (Shah et al., 2018; Du et al., 2020; Karouzos et al., 2021). As for distribution matching, it is very difficult to preserve the discriminative power of the model on the target task while trying to perform instance level alignment (Saito et al., 2017; Lee et al., 2019). To this end, it is essential to develop stable and effective solutions to learn domain invariance and instance-wise matching for UDA.\nRecent advances in self-supervised learning (SSL), such as contrastive learning (CL), have been proven effective at instance level by leveraging raw data to define surrogate tasks that help learn representations (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b). CL benefits\nfrom treating instances as classes and conducting data augmentations to generate positive instance pairs. Regarding CL for UDA, previous works mark cross-domain images with the same labels (for example, real and cartoon dogs) as the positive pairs in contrastive loss (Wang et al., 2021; Park et al., 2020). However, such methods are not applicable to NLP tasks because of the massive semantic and syntactic shifts between two cross-domain sentences. Besides, from the domain adaptation perspective, constructing cross-domain positive samples and aligning domain-agnostic pairs have received less emphasis in related literature, since previous works focus on designing label preserving text transformations, such as back-translation, synonym, dropout and their combinations (Qu et al., 2021; Gao et al., 2021).\nConfronting with limitations mentioned above, we propose the concept of domain puzzles which discard domain-related information to confuse the model, making it difficult to differentiate which domain these puzzles belong to. Instead of directly seeking matched sentences across the source and target domains which is infeasible, we propose to pull the source (target) data and its corresponding domain puzzles closer to reduce the domain discrepancy, as shown in Fig.1. A simple idea to craft domain puzzles is to mask domain-specific tokens. However, token-level operations are too discrete and non-flexible to reflect the complex semantic change of natural languages. Hence, we aim to seek better domain puzzles that retain high-confidence predictions and task-discriminative power in the representation space for each training instance.\nIn this paper, we propose Domain Confused Contrastive Learning (DCCL) to encourage the model to learn similar representations for the original sentence and its curated domain-confused version with contrastive loss. More specifically, we synthesize these domain puzzles by utilizing adversarial examples (Zhu et al., 2020; Jiang et al., 2020). The algorithm will search for an extreme direction that roughly points to the opposite domain and produces most domain-challenging puzzles. We encourage the model to encode original and domain-confused samples closer, gradually pulling examples to the domain decision boundary as training progresses via CL, thus learning the domain invariance. Furthermore, in order to investigate whether CL necessarily benefits UDA, we conduct experiments and find that constructing domain puzzles as paired pos-\nitive samples is favorable for UDA, however, other data augmentation methods such as back translation (Sennrich et al., 2016; Edunov et al., 2018) do not have the same effect. The experiment results show that the proposed DCCL significantly outperforms all the baselines. We also conduct quantitative experiments to measure the domain discrepancy after adaptation demonstrating that DCCL can decrease the divergence between domains in a self-supervised way. Overall, the paper makes the following contributions:\n• First, a new concept of domain puzzles is put forward. We propose to craft the domain puzzles via domain-confused adversarial attack;\n• Second, we propose DCCL, which is able to pull source and target samples closer to the crafted domain puzzles. The DCCL is capable of reducing domain shift and learning domain invariance;\n• Third, experiments demonstrate that the proposed DCCL surpasses baselines with a large margin. We also conduct analyzing experiments to verify the effectiveness."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Unsupervised Domain Adaptation",
      "text" : "Problem Setup Suppose we have access to a source dataset with n labeled points DS = {xi, yi}1,...,n sampled i.i.d. from the source domain, and a target dataset with m unlabeled points DT = {xj}1,...,m sampled i.i.d. from the target domain, where xi, xj are sequences of tokens, yi is the class label for xi. For in-domain training with labeled training instances, the model aims to learn a function f(x; θf , θy) : x → C. θf is the parameter of the deep neural network encoder (e.g., pretrained language model), θy denotes parameters that compute the network’s class label predictions, and C is the label set. The model is learned with the following objective:\nmin θf ,θy ∑ (x,y)∼DS [L(f(x; θf , θy), y)]. (1)\nHowever, for Unsupervised Domain Adaptation (UDA), the goal of the adaptation algorithm is to learn a discriminative classifier from the source domain, which at the same time could generalize well on the target domain by leveraging unlabeled target data and learning a mapping between\nsource and target domains. It is generally acknowledged that the discrepancy between two datasets (domain shift) can be reduced by aligning two distributions (Ben-David et al., 2007, 2010). The methods that learn domain invariant features for domain alignment include KL divergence (Zhuang et al., 2015), Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), and Domain Adversarial Neural Network (DANN) (Ganin et al., 2016) (details of DANN can be found in Appendix. B). DANN suffers from a vanishing gradient problem (Shen et al., 2018), and the training process is unstable (Shah et al., 2018; Du et al., 2020). Hence, more efficient and stable algorithms are essential for UDA (Wu et al., 2019b)."
    }, {
      "heading" : "2.2 Adversarial Training",
      "text" : "Adversarial training with perturbations has been shown to significantly improve the performance of the state-of-the-art language models for many natural language understanding tasks (Madry et al., 2018; Zhu et al., 2020; Jiang et al., 2020; Pereira et al., 2021). The algorithm generally considers adversarial attacks with perturbations to word embeddings and minimizes the resultant adversarial loss around the input samples. In a single domain, adversarial training (Goodfellow et al., 2014) is an inner max, outer min adversarial problem with the objective:\nmin θf ,θy ∑ (x,y)∼D [max δ L(f(x+δ; θf , θy), y)], (2)\nwith (2), the standard adversarial training can also be regularized using virtual adversarial training (Miyato et al., 2018), which encourages smoothness in the embedding space. The αadv controls the trade-off between the two losses, usually set to be 1.\nmin θf ,θy ∑ (x,y)∼D [ L(f(x; θf , θy), y) + αadv\nmax δ\nL(f(x+ δ; θf , θy), f(x; θf , θy)) ] . (3)\nFor (2)(3), the inner maximization can be solved by Projected Gradient Decent (PGD) (Madry et al., 2018) with an additional assumption that the loss function is locally linear. A following iteration can approximate the adversarial perturbation δ:\nδt+1 = Π∥δ∥F≤ϵ(δt + η gadvy (δt)\n∥gadvy (δt)∥F ), (4)\ngadvy (δt) = ∇δ L(f(x+ δt; θf , θy), y), (5)\nwhere Π∥δ∥F≤ϵ performs a projection onto the ϵball. The advantages of PGD lie in that it only relies on the model itself to produce diverse adversarial samples, enabling the model to generalize better to unseen data."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we focus our discussions on the proposed Domain Confused Contrastive Learning (DCCL) under a sentiment classification scenario. The overall framework of our method is illustrated in Fig. 2. The model will take source labeled and target unlabeled sentences as input. It will then augment the input data with domain puzzles by fabricating adversarial perturbations. With the augmented data, the next step produces a hidden representation for each instance with an encoder which will be further used to produce three losses to train the entire model, namely sentiment classification loss, contrastive loss and consistency loss."
    }, {
      "heading" : "3.1 Crafting domain puzzles",
      "text" : "For UDA, Saito et al. (2017) mentions that simply matching the distributions cannot ensure high accuracy on the target domain without the target labels. Moreover, it may cause negative transfer, deteriorating knowledge transfer from source domain to the target domain (Wang et al., 2019). Even if the matched sentences have the same label, due to huge syntactic and semantic shift, instance-based matching strategies that align examples from different domains will introduce noises for pre-trained\nlanguage models, for example, aligning source domain and target domain sentences in Fig. 3.\nAlternatively, we can locate and mask domainspecific tokens which are related to sentence topics and genres. Since sentences in the green box of Fig. 3 become domain-agnostic, we refer to those domain-confused sentences (one cannot tell which domain these sentences belong to) as domain puzzles. Matching distributions between the source domain and the domain puzzles, as well as the target domain and the domain puzzles, will also make language models produce domain invariant representations.\nHowever, due to the discrete nature of natural languages, it is challenging to decide correct tokens to mask without hurting the semantics especially when the sentences are complicated1. Hence, we seek domain puzzles in the representation space and introduce adversarial perturbations, because we can rely on the model itself to produce diverse but targeted domain puzzles. Note that the purpose of adversarial attack here is not to enhance the robustness, but to construct exquisitely produced perturbations for a better domain invariance in the representation space.\nTo generate domain-confused augmentations, we adopt adversarial attack with perturbations for domain classification. The loss for learning a domain classifier with adversarial attack can be specified as follows:\nLdomain = L(f(x; θf , θd), d)+ αadv L(f(x+ δ; θf , θd), f(x; θf , θd)), (6)\nδ = Π∥δ∥F≤ϵ(δ0 + η gadvd (δ0)\n∥gadvd (δ0)∥F )), (7)\nwhere δ0 is the initialized noise, θd is the parameter corresponding to the computation of the domain classification, and d is the domain label. Due to additional overhead incurred during fine-tuning large\n1Masking is not our focus in this paper. More detailed implementation can be found in section 4.4. We will investigate how to extract better domain tokens in our future work.\npre-trained language models, the number of iterations for perturbation estimation is usually 1 (Jiang et al., 2020; Pereira et al., 2021), as shown in Eq. 7. We synthesize the perturbation δ by searching for an extreme direction that perplexes the domain classifier most in the embedding space, and f(x+δ; θf ) is the crafted domain puzzles encoded by the language model."
    }, {
      "heading" : "3.2 Learning invariance with domain puzzles",
      "text" : "After acquiring domain puzzles, simply applying distribution matching will sacrifice discriminative knowledge learned from the source domain (Saito et al., 2017; Lee et al., 2019), and instance-based matching will also overlook global intra-domain information. To learn sentiment-wise discriminative representations in the absence of the target labels, we propose to learn domain invariance via contrastive learning (CL). In general, CL benefits from the definition of the augmented positive and negative pairs by treating instances as classes (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b). Furthermore, the contrastive loss encourages the positive pairs to be close to each other and negative pairs to be far apart. Specifically, maximizing the similarities between positive pairs learns an invariant instance-based representation, and minimizing the similarities between negative pairs learns a uniformly distributed representation from a global view, making instances gathered near the task decision boundary away from each other (Arora et al., 2019; Grill et al., 2020). This will help to enhance task discrimination of the learned model.\nFor positive pairs, intuitively, we hope that the model could encode the original sentence and most domain-challenging examples to be closer in the representation space, gradually pulling examples to the domain decision boundary as training progresses. For negative sampling, it widens the sentiment decision boundary and promotes better sentiment-wise discriminative features for both domains. However, for cross-domain negative sampling, the contrastive loss may push the negative samples in the target (source) domain away from the anchor in the source (target) domain (see Fig. 4 (b) left). This is contradictory to the objective of domain puzzles which try to pull different domains closer. To avoid the detriment of cross-domain repulsion, excluding samples with different domains from the negative set is of great importance. There-\nfore, we write the following contrastive infoNCE loss (Chen et al., 2020a) as follow:\nLcontrast = − 1\nN N∑ i log exp(s(zi, z ′ i)/τ)∑N k 1k ̸=i exp(s(zi, zk)/τ) ,\n(8)\nwhere N is the mini batch size with samples from the same domain, zi = g(f(xi; θf )), and g(·) is one hidden layer projection head. We denote x′ = x+δ as the domain puzzle augmentation, s(·) computes cosine similarity, 1k ̸=i is the indicator function, and τ is the temperature hyperparameter."
    }, {
      "heading" : "3.3 Consistency Regularization",
      "text" : "Given perturbed embedding x+ δ, which is crafted based on domain classification, we also encourage the model to produce consistent sentiment predictions with that of the original instance f(x; θf , θy). For this, we minimize the symmetric KL divergence, which is formulated as:\nLconsist = L(f(x; θf , θy), f(x+ δ; θf , θy)). (9)\nFor overall training objective, we train the neural network in an end-to-end manner with a weighted sum of losses as follows. Details of the algorithm\ncan also be found in Appendix C.\nmin θf ,θy,θd ∑ (x,y)∼DS L(f(x; θf , θy), y)+\n∑ (x,y)∼DS ,DT [αLdomain +λLcontrast +β Lconsist]. (10)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Amazon Benchmark (Blitzer et al., 2007). We conduct experiments on this dataset for completeness since most of previous works report results on it. The dataset contains four domains: Book (BK), DVD (D), Electronics (E) and Kitchen housewares (K). There are 2000 balanced labeled data for each domain, we randomly select 20000 unlabeled reviews for BK, D and E. For K, only 17856 unlabeled reviews are available. Amazon review dataset (He et al., 2018). This dataset considers neutral instances which may not bias the dataset and bring more challenges2. The dataset also contains four domains: Book (BK), Electronics (E), Beauty (BT), and Music (M). Following He et al. (2018), we treat set 1 as labeled dataset containing 6000 instances, and treat set 2 as unlabeled dataset which also contains 6000 instances. More details about two datasets can be found in Appendix A."
    }, {
      "heading" : "4.2 Experiment Settings",
      "text" : "For unsupervised adaptation setting, we should not have access to target labeled data at the training phase, so trained models with minimum classification error on the source validation set is saved for evaluation. At this point, we suppose a good model that generalizes well on the target domain is able to reach high performance on both validation and test set at the same time. We evaluate our model with 5 runs in all experiments, and we report the average score, standard deviation and paired t-test results."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "For pre-trained language model, we use BERT base uncased (Devlin et al., 2019) as the basis for all experiments. The max length is set to 512. For optimizer, we use AdamW (Kingma and Ba, 2015; Loshchilov and Hutter, 2018) with weight decay\n2The original crawled reviews contain star ratings (1 to 5 stars). Comparing with Amazon Benchmark which discards the neutral class, labeling them with rating < 3, > 3, = 3 as negative, positive, and neutral respectively is more reasonable and challenging.\n0.01 (for BERT baseline, we set 1e-4). We set the learning rate as 1e-5, and we use a linear scheduler with warm-up steps 0.1 of total training steps.\nWe set the number of adversarial iterations to be 1, adversarial weighting factor αadv = 1 and we use l2 norm to compute projections. We also follow Zhu et al. (2020) to set other adversarial hyperparameters, e.g., adversarial step size η = 5e-2, perturbation bound ϵ = 5e-2. For weighting factors, we set α = 1e-3, λ = 3e-2 and β = 5. Each adaptation requires half one hour on one A-100."
    }, {
      "heading" : "4.4 Baselines",
      "text" : "BERT base: Fine-tune BERT on the source, without using target unlabeled data, then directly evaluate on the target labeled data. KL: Use symmetric KL-divergence loss of embedded instances between the source and target domains (Zhuang et al., 2015). MMD: Maximum Mean Discrepancy loss (Gretton et al., 2012) measures the distance based on the notion of embedding probabilities in a reproducing kernel Hilbert space. We implement a gaussian kernel which is a common choice. DANN: The adaptation rate is λ = 21+exp(−γp) − 1, p = tT , where t and T are the number of current training steps and total steps. γ requires careful tuning within [0.05, 0.1, 0.15, 0.2]. back-trans+CL: Back translation (Sennrich et al., 2016; Edunov et al., 2018) is one of the widely used data augmentation techniques. We utilize en-de translation model pre-trained on WMT19 and released in fairseq (Ott et al., 2019). The model is trained with contrastive loss on the source and target domain respectively. mask+CL: We mask domain specific tokens to make the augmentation become domainagnostic. Since information extraction is not our focus, we identify domain specific tokens via a simple frequency-ratio method (Li et al., 2018; Wu et al., 2019a): s(u, d) = count(u,D\nd)+λ∑ d′∈D,d′ ̸=d count(u,D d′ )+λ ,\nwhere count(u,Dd) represents the number of times a token u appears in domain d. Smoothing λ is set to 1. When s(u, d) is larger than 5, we mark a token u as a domain specific token. Through counting, the number of masked tokens accounted for 0.06 of the total length. mask: To investigate the effectiveness of contrastive loss, after masking domain-specific tokens, we further let the model train with augmented data without contrastive loss. R-PERL (Ben-David et al., 2020) is a pivot-based method, and DAAT (Du et al., 2020) combines DANN and post-training. All the methods in Tabel 1 and Table 2 are implemented based on BERT model for fair comparisons."
    }, {
      "heading" : "4.5 Results",
      "text" : "Will contrastive learning necessarily help the Unsupervised Domain Adaptation? As discussed earlier, contrastive learning has many benefits in representation learning. However, for some adaptation tasks in Table 1, for example, BT→E, M→E, and E→M, back-trans+CL shows that contrastive learning only gains marginal benefit. When masking domain-specific tokens and pulling original sentence representations to those domain puzzles, the effect of contrastive learning becomes more apparent (mask+CL with average score 65.17 compared with back-trans+CL 63.79 and mask 61.40 in Table.1). This finding helps to explain that choices of positive examples are critical and domain confused augmentations will further benefit adaptation.\nDCCL outperforms baselines. From Table 1 we can observe that the proposed DCCL outperforms all other methods with a large margin and p < 0.05 using paired t-test, and 5.94% improvement over BERT base. From Table 2, we can also observe 1.74% improvement over BERT base, and DCCL also surpasses state-of-the-art methods R-PERL (Ben-David et al., 2020) and DAAT (Du et al.,\n2020). We note that Amazon Benchmark dataset is quite easy, since it discards neutral instances, BERT base model has already achieved high scores on this dataset. Besides, we observe that the effect of distribution matching methods (KL and MMD) is limited on two datasets. The reason might be that pre-trained language models trained with massive and heterogeneous corpora already have strong generalization ability. Learning such cross-domain and instance-based matching will bring perplexity to language models and sacrifice task discrimination (Saito et al., 2017; Lee et al., 2019). On the contrary, the proposed DCCL retains such information in a self-supervised way. Furthermore, we notice that DANN is very unstable, besides adaptation rate λ, the model is also sensitive to other hyperparameters such as learning rate and training epochs because the performance on the target domain will keep decreasing when training with longer steps. Hence, it is difficult for the model to achieve the lowest error rates on both the source and target domains simultaneously. Compared to DANN, DCCL is much more stable and has lower standard deviations on most adaptation tasks.\nContrastive learning designs. We explore different hyperparameter values for the proposed DCCL in E→BK, as is shown in Fig. 5. We find that a temperature of 0.5 combined with the batch size 32 can achieve the best performance. We also notice that setting the temperature too high or too low\ncan significantly affect adaptation performances, while a larger batch size usually brings a relatively smaller improvement."
    }, {
      "heading" : "4.6 Ablation Studies",
      "text" : "We conduct an ablation study on each component in Eq. 10 to inspect the contribution of contrastive learning, as is shown in Table 3. We can see that every single component can render a better adaptation performance. In particular, the effectiveness of Lcontrast is observable with 3~5 performance gain compared to baselines. When training curated domain puzzles as simple augmentations without contrastive loss, we can observe only a slight improvement. This result demonstrates that the performance gain brought by DCCL does not come from data augmentation."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Visualization",
      "text" : "We perform visualizations for trained representations as illustrated in Fig. 6. When training with the source domain and then adapting to the target domain (BERT-base), we can observe a considerable domain shift for BERT encoder on this amazon review dataset. Moreover, as mentioned before, continuing training DANN with larger epochs will substantially drop the score (from the highest point (DANN-best) to the lowest point (DANN-worst)). However, we can also see that DCCL mitigates domain shift but remains good sentiment discrimination on the target domain."
    }, {
      "heading" : "5.2 Quantitative Results",
      "text" : "A-distance measures domain discrepancies (BenDavid et al., 2007, 2010), with the definition as dA = 2(1 − 2ϵ), where ϵ is the domain classification error. To fairly compare with A-distance of the baselines, we use linear SVM to calculate ϵ following previous work (Saito et al., 2017; Du et al., 2020). We randomly select 2000 instances for both source and target domain and split them with 1:1 for train and test for the SVM model. From Fig. 7, we can observe that DCCL can learn a good balance between sentiment classification and domain discrepancy, compared to DANN-best and DANN-worst."
    }, {
      "heading" : "6 Related Work",
      "text" : "Unsupervised Domain Adaptation UDA in NLP has the following approaches: (1) Pivot-based methods use unlabeled data from both domains, trying to discover characteristics that are similar (Pan et al., 2010). Some recent works extend piv-\nots with autoencoders and contextualized embeddings (Ziser and Reichart, 2017; Miller, 2019; BenDavid et al., 2020). (2) Pseudo-labeling leverages a trained classifier to predict labels on unlabeled examples, which are subsequently considered as gold labels (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006). Recent works also combine this technique with pre-trained language models (Lim et al., 2020; Ye et al., 2020). (3) Domain Adversarial Neural Networks (Ganin et al., 2016). Some approaches leverage Wasserstein distance to stabilize adversarial training (Shen et al., 2018; Shah et al., 2018), and combine it with posttraining which can produce better adversarial results (Du et al., 2020). (4) Adaptive pre-training is a more straightforward but effective method (Gururangan et al., 2020; Han and Eisenstein, 2019; Karouzos et al., 2021) by leveraging the objective of masked language model (MLM).\nContrastive learning CL has recently gained popularity as a reliable approach for unsupervised representation learning. For Computer Vision, there are approaches obtaining augmented images using transformations including cropping, rotation, etc. (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b). As for Natural Language Processing, many works study different label-preserving augmentations, such as backtranslations, synonyms, adversaries, dropout, and their combinations (Qu et al., 2021; Gao et al., 2021). In addition, many pre-trained language models trained with contrastive loss are also released. DeCLUTR (Giorgi et al., 2021) and CLEAR (Wu et al., 2020) jointly train the model with a contrastive objective and a masked language model setting. ConSERT (Yan et al., 2021) overcomes the collapse problem of BERT-derived sentence representations and makes them more suitable for downstream applications by using unlabeled texts."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we put forward a new concept, domain puzzles, which can be crafted through domainspecific token mask and domain-confused adversarial attacks. When learning jointly with a contrastive objective, the proposed method DCCL surpasses baselines with a large margin by mitigating domain shift without losing discriminative power on the target domain. Moreover, the proposed framework can also be extended to other NLP tasks demanding adaptations, and we leave this for future work."
    }, {
      "heading" : "A Datasets",
      "text" : "We obtain the Amazon review datasets from He et al. (2018), the dataset is public in 3. This dataset does not remove neutral labels and will not be problematic in UDA situation where the label information of the target domain is not available. In addition, reserving neutral labels also bring challenges for pre-trained language model, making it more favorable for self-supervised representation learning. Summary of this dataset is availble in Table 4.\nEach domain contains two sets, set 1 contains 6000 instances with balanced class labels, and set 2 contains instances that are randomly sampled from the larger dataset (McAuley et al., 2015), preserving authentic label distribution, examples in these two datasets do not overlap. Following (He et al., 2018), we use set 1 from the source domain as the training set for all our experiments. Since label distribution in the target domain is unpredictable and out of control in real life, so it’s more reasonable to use set 2 from the target domain as the unlabeled set, lastly the model will be evaluated in set 1 from target domain. For data split, we randomly sample 1000 instances from the source labeled dataset as validation set. When running UDA experiments, the model will train on 5000 source labeled examples and 6000 target unlabeled examples, then validate on 1000 source labeled examples.\nFor Amazon Benchmark 4 (Blitzer et al., 2007), it also contains four domains: Book (BK), DVD (D), Electronics (E) and Kitchen housewares (K). There are 2000 balanced labeled data for each domain, we randomly select 20000 unlabeled reviews for BK, D and E. For K, only 17856 unlabeled reviews are available, statistics of Amazon Benchmark can be find in Table.5. For data split, 1600\n3https://github.com/ruidan/DAS 4https://www.cs.jhu.edu/ mdredze/datasets/sentiment/\nbalanced samples are randomly sampled from the source labeled dataset, and 400 for validation."
    }, {
      "heading" : "B DANN",
      "text" : "Domain Adversarial Neural Network Ganin et al. (2016) proposes Domain Adversarial Neural Network (DANN), which learns domain invariant and discriminative features simultaneously. This approach is motivated by the idea that an adaptation algorithm could learn good representations for cross-domain transfer if it cannot differentiate the domain of the input observations. The optimization objective is:\nmin θf ,θy ∑ (x,y)∼DS [L(f(x; θf , θy), y) + λRθf ], (11)\nRθf = max θd ∑ (x,d)∼DS ,DT [−L(f(x; θf , θd), d)],\n(12)\nwhere θd is the parameter corresponding to the computation of the domain classification, d is the domain label, Rθf is a regularizer weighted by λ. Objective (11) learns task discrimination by minimizing task classification loss and tries to make features similar across domains by maximizing the domain classification loss.\nAlthough the domain classifier with parameters θd could perfectly classify different domains, the balance between two terms in (11) is hard to maintain. Hence, the training process becomes unstable, requiring an elaborate adaptation rate λ tuning (Shen et al., 2018; Shah et al., 2018; Du et al., 2020). Furthermore, the encoder could learn trivial solutions (Karouzos et al., 2021) which produce features with flipped domain predictions."
    }, {
      "heading" : "C Algorithm of DCCL",
      "text" : "We also present the algorithm for proposed DCCL, for simplicity, we no longer distinguish parameters θf , θy and θd, suppose the parameter of the whole model is θ.\nAlgorithm 1 DCCL Input: For simplicity, θ is the parameter of whole\nmodel. T : the total number of iterations, (x, y) ∼ DS : source dataset with sentiment label y, (x, d) ∼ DS DD: source and target dataset with domain label d, K: the number of iterations for updating δ, σ2: the initialized variance, ϵ: perturbation bound, η: the step size, γ: global learning rate, N : batch size, τ : temperature, g(·):one hidden layer projection head. αadv, α, λ and β: weighting factor.\n1: for epoch = 1, .., T do 2: for minibatch N do 3: δ ← N (0, σ2I) 4: for m = 1, ..,K do 5: gadvd ← ∇δ L(f(x+ δ; θ), d) 6: δ ← Π∥δ∥F≤ϵ(δ + ηg adv d /∥gadvd ∥F ) 7: end for 8: Ldomain ← L(f(x; θ), d) +αadv L(f(x+ δ; θ), d) 9: z = g(f(x; θ))\n10: z′ = g(f(x+ δ; θ)) 11: for i = 1, ..., N and j = 1, ..., N do 12: s′i = z ⊤ i z ′ j/∥zi∥∥z′j∥ 13: si,j = z⊤i zj/∥zi∥∥zj∥ 14: end for 15: Lcontrast ← − 1N N∑ i log exp(s′i/τ)∑N j 1j ̸=i exp(si,j/τ) 16: Lconsist ← L(f(x; θ), f(x+ δ; θ)) 17: gθ ← ∇θ L(f(x; θ), y) + α∇θ Ldomain +λ∇θ Lcontrast +β∇θ Lconsist 18: θ ← θ − γgθ 19: end for 20: end for Output: θ"
    } ],
    "references" : [ {
      "title" : "A theoretical analysis of contrastive unsupervised representation learning",
      "author" : [ "Sanjeev Arora", "H. Khandeparkar", "M. Khodak", "Orestis Plevrakis", "Nikunj Saunshi." ],
      "venue" : "International Conference on Machine Learning.",
      "citeRegEx" : "Arora et al\\.,? 2019",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2019
    }, {
      "title" : "Perl: Pivot-based domain adaptation for pre-trained deep contextualized embedding models",
      "author" : [ "Eyal Ben-David", "Carmel Rabinovitz", "Roi Reichart." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:504–521.",
      "citeRegEx" : "Ben.David et al\\.,? 2020",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2020
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan." ],
      "venue" : "Machine learning, 79(1):151–175.",
      "citeRegEx" : "Ben.David et al\\.,? 2010",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2010
    }, {
      "title" : "Analysis of representations for domain adaptation",
      "author" : [ "Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2007
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira." ],
      "venue" : "Proceedings of the 45th annual meeting of the association of computational linguistics, pages 440–447.",
      "citeRegEx" : "Blitzer et al\\.,? 2007",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "International Conference on Machine Learning, pages 1597–1607. PMLR.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved baselines with momentum contrastive learning",
      "author" : [ "Xinlei Chen", "Haoqi Fan", "Ross Girshick", "Kaiming He." ],
      "venue" : "arXiv preprint arXiv:2003.04297.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "J. Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial and domain-aware bert for cross-domain sentiment analysis",
      "author" : [ "Chunning Du", "Haifeng Sun", "Jingyu Wang", "Qi Qi", "Jianxin Liao." ],
      "venue" : "Proceedings of the 58th annual meeting of the Association for Computational Linguistics, pages 4019–4028.",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding back-translation at scale",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500.",
      "citeRegEx" : "Edunov et al\\.,? 2018",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "Domain-adversarial training of neural networks",
      "author" : [ "Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor Lempitsky." ],
      "venue" : "The journal of machine learning research, 17(1):2096–",
      "citeRegEx" : "Ganin et al\\.,? 2016",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2016
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Declutr: Deep contrastive learning for unsupervised textual representations",
      "author" : [ "John M Giorgi", "Osvald Nitski", "Gary D Bader", "Bo Wang." ],
      "venue" : "Proceedings of Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Giorgi et al\\.,? 2021",
      "shortCiteRegEx" : "Giorgi et al\\.",
      "year" : 2021
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Schölkopf", "Alexander Smola." ],
      "venue" : "The Journal of Machine Learning Research, 13(1):723–773.",
      "citeRegEx" : "Gretton et al\\.,? 2012",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "Bootstrap your own latent: A new approach",
      "author" : [ "Jean-Bastien Grill", "Florian Strub", "Florent Altch’e", "C. Tallec", "Pierre H. Richemond", "Elena Buchatskaya", "Carl Doersch", "B.A. Pires", "Z. Guo", "M.G. Azar", "Bilal Piot", "K. Kavukcuoglu", "R. Munos", "Michal Valko" ],
      "venue" : null,
      "citeRegEx" : "Grill et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Grill et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised domain adaptation of contextualized embeddings for sequence labeling",
      "author" : [ "Xiaochuang Han", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Han and Eisenstein.,? 2019",
      "shortCiteRegEx" : "Han and Eisenstein.",
      "year" : 2019
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Adaptive semi-supervised learning for cross-domain sentiment classification",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3467–",
      "citeRegEx" : "He et al\\.,? 2018",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
      "author" : [ "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Tuo Zhao." ],
      "venue" : "Proceedings of the 58th",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Udalm: Unsupervised domain adaptation through language modeling",
      "author" : [ "Constantinos Karouzos", "Georgios Paraskevopoulos", "Alexandros Potamianos." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Karouzos et al\\.,? 2021",
      "shortCiteRegEx" : "Karouzos et al\\.",
      "year" : 2021
    }, {
      "title" : "Supervised contrastive learning",
      "author" : [ "Prannay Khosla", "Piotr Teterwak", "Chen Wang", "Aaron Sarna", "Yonglong Tian", "Phillip Isola", "Aaron Maschinot", "Ce Liu", "Dilip Krishnan." ],
      "venue" : "arXiv preprint arXiv:2004.11362.",
      "citeRegEx" : "Khosla et al\\.,? 2020",
      "shortCiteRegEx" : "Khosla et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Out-ofdistribution generalization via risk extrapolation (rex)",
      "author" : [ "David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "Dinghuai Zhang", "Remi Le Priol", "Aaron Courville." ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Krueger et al\\.,? 2021",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2021
    }, {
      "title" : "Drop to adapt: Learning discriminative features for unsupervised domain adaptation",
      "author" : [ "Seungmin Lee", "Dongwan Kim", "Namil Kim", "SeongGyun Jeong." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 91–100.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Semi-supervised learning on meta structure: Multi-task tagging and parsing in low-resource scenarios",
      "author" : [ "KyungTae Lim", "Jay Yoon Lee", "Jaime Carbonell", "Thierry Poibeau." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Lim et al\\.,? 2020",
      "shortCiteRegEx" : "Lim et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2018",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Towards deep learning models resistant to adversarial attacks",
      "author" : [ "Aleksander Madry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Madry et al\\.,? 2018",
      "shortCiteRegEx" : "Madry et al\\.",
      "year" : 2018
    }, {
      "title" : "Image-based recommendations on styles and substitutes",
      "author" : [ "Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton Van Den Hengel." ],
      "venue" : "Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval,",
      "citeRegEx" : "McAuley et al\\.,? 2015",
      "shortCiteRegEx" : "McAuley et al\\.",
      "year" : 2015
    }, {
      "title" : "Reranking and self-training for parser adaptation",
      "author" : [ "David McClosky", "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational",
      "citeRegEx" : "McClosky et al\\.,? 2006",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2006
    }, {
      "title" : "Simplified neural unsupervised domain adaptation",
      "author" : [ "Timothy Miller." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
      "citeRegEx" : "Miller.,? 2019",
      "shortCiteRegEx" : "Miller.",
      "year" : 2019
    }, {
      "title" : "Virtual adversarial training: a regularization method for supervised and semisupervised learning",
      "author" : [ "Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Shin Ishii." ],
      "venue" : "IEEE transactions on Pattern Analysis and Machine Intelligence, 41(8):1979–",
      "citeRegEx" : "Miyato et al\\.,? 2018",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2018
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-domain sentiment classification via spectral feature alignment",
      "author" : [ "Sinno Jialin Pan", "Xiaochuan Ni", "Jian-Tao Sun", "Qiang Yang", "Zheng Chen." ],
      "venue" : "Proceedings of the 19th international conference on World wide web, pages 751–760.",
      "citeRegEx" : "Pan et al\\.,? 2010",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2010
    }, {
      "title" : "Joint contrastive learning for unsupervised domain adaptation",
      "author" : [ "Changhwa Park", "Jonghyun Lee", "Jaeyoon Yoo", "Minhoe Hur", "Sungroh Yoon." ],
      "venue" : "arXiv preprint arXiv:2006.10297.",
      "citeRegEx" : "Park et al\\.,? 2020",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2020
    }, {
      "title" : "Targeted adversarial training for natural language understanding",
      "author" : [ "Lis Pereira", "Xiaodong Liu", "Hao Cheng", "Hoifung Poon", "Jianfeng Gao", "Ichiro Kobayashi." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Pereira et al\\.,? 2021",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2021
    }, {
      "title" : "Coda: Contrastenhanced and diversity-promoting data augmentation for natural language understanding",
      "author" : [ "Yanru Qu", "Dinghan Shen", "Yelong Shen", "Sandra Sajeev", "Weizhu Chen", "Jiawei Han." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Qu et al\\.,? 2021",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural unsupervised domain adaptation in nlp—a survey",
      "author" : [ "Alan Ramponi", "Barbara Plank." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 6838–6855.",
      "citeRegEx" : "Ramponi and Plank.,? 2020",
      "shortCiteRegEx" : "Ramponi and Plank.",
      "year" : 2020
    }, {
      "title" : "Asymmetric tri-training for unsupervised domain adaptation",
      "author" : [ "Kuniaki Saito", "Yoshitaka Ushiku", "Tatsuya Harada." ],
      "venue" : "International Conference on Machine Learning, pages 2988–2997. PMLR.",
      "citeRegEx" : "Saito et al\\.,? 2017",
      "shortCiteRegEx" : "Saito et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86–96.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial domain adaptation for duplicate question detection",
      "author" : [ "Darsh Shah", "Tao Lei", "Alessandro Moschitti", "Salvatore Romeo", "Preslav Nakov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Shah et al\\.,? 2018",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2018
    }, {
      "title" : "Wasserstein distance guided representation learning for domain adaptation",
      "author" : [ "Jian Shen", "Yanru Qu", "Weinan Zhang", "Yong Yu." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Crossdomain contrastive learning for unsupervised domain adaptation",
      "author" : [ "Rui Wang", "Zuxuan Wu", "Zejia Weng", "Jingjing Chen", "Guo-Jun Qi", "Yu-Gang Jiang." ],
      "venue" : "arXiv preprint arXiv:2106.05528.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Characterizing and avoiding negative transfer",
      "author" : [ "Zirui Wang", "Zihang Dai", "Barnabás Póczos", "Jaime Carbonell." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11293–11302.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Mask and infill: Applying masked language model for sentiment transfer",
      "author" : [ "Xing Wu", "Tao Zhang", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "International Joint Conference on Artificial Intelligence.",
      "citeRegEx" : "Wu et al\\.,? 2019a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain adaptation with asymmetrically-relaxed distribution alignment",
      "author" : [ "Yifan Wu", "Ezra Winston", "Divyansh Kaushik", "Zachary Lipton." ],
      "venue" : "International Conference on Machine Learning, pages 6872–6881. PMLR.",
      "citeRegEx" : "Wu et al\\.,? 2019b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Clear: Contrastive learning for sentence representation",
      "author" : [ "Zhuofeng Wu", "Sinong Wang", "Jiatao Gu", "Madian Khabsa", "Fei Sun", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2012.15466.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Consert: A contrastive framework for self-supervised sentence representation transfer",
      "author" : [ "Yuanmeng Yan", "Rumei Li", "Sirui Wang", "Fuzheng Zhang", "Wei Wu", "Weiran Xu." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "J. Carbonell", "R. Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised word sense disambiguation rivaling supervised methods",
      "author" : [ "David Yarowsky." ],
      "venue" : "33rd annual meeting of the association for computational linguistics, pages 189–196.",
      "citeRegEx" : "Yarowsky.,? 1995",
      "shortCiteRegEx" : "Yarowsky.",
      "year" : 1995
    }, {
      "title" : "Feature adaptation of pre-trained language models across languages and domains with robust self-training",
      "author" : [ "Hai Ye", "Qingyu Tan", "Ruidan He", "Juntao Li", "Hwee Tou Ng", "Lidong Bing." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in",
      "citeRegEx" : "Ye et al\\.,? 2020",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2020
    }, {
      "title" : "Tri-training: Exploiting unlabeled data using three classifiers",
      "author" : [ "Zhi-Hua Zhou", "Ming Li." ],
      "venue" : "IEEE Transactions on knowledge and Data Engineering, 17(11):1529–1541.",
      "citeRegEx" : "Zhou and Li.,? 2005",
      "shortCiteRegEx" : "Zhou and Li.",
      "year" : 2005
    }, {
      "title" : "Freelb: Enhanced adversarial training for natural language understanding",
      "author" : [ "Chen Zhu", "Yu Cheng", "Zhe Gan", "Siqi Sun", "Tom Goldstein", "Jingjing Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised representation learning: Transfer learning with deep autoencoders",
      "author" : [ "Fuzhen Zhuang", "Xiaohu Cheng", "Ping Luo", "Sinno Jialin Pan", "Qing He." ],
      "venue" : "Twenty-Fourth International Joint Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhuang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural structural correspondence learning for domain adaptation",
      "author" : [ "Yftah Ziser", "Roi Reichart." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 400–410.",
      "citeRegEx" : "Ziser and Reichart.,? 2017",
      "shortCiteRegEx" : "Ziser and Reichart.",
      "year" : 2017
    }, {
      "title" : "2020). Furthermore, the encoder could learn trivial solutions (Karouzos et al., 2021) which produce features with flipped domain predictions. C Algorithm of DCCL",
      "author" : [ "2018 Shen et al", "2018 Shah et al", "Du" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have yielded considerable improvements with datasets drawing from various sources.",
      "startOffset" : 28,
      "endOffset" : 86
    }, {
      "referenceID" : 28,
      "context" : "Pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have yielded considerable improvements with datasets drawing from various sources.",
      "startOffset" : 28,
      "endOffset" : 86
    }, {
      "referenceID" : 51,
      "context" : "Pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have yielded considerable improvements with datasets drawing from various sources.",
      "startOffset" : 28,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "However, the lack of portability of language model to adapt to a new textual domain remains a central issue (Gururangan et al., 2020), especially when the training set and testing set do not follow the same underlying distribution (changing of topic and genres) - usually referred to as domain shift.",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "Extensive algorithms have been proposed to mitigate the domain shift problem, for example, domain adversarial neural network (DANN) (Ganin et al., 2016) and distribution matching (Gretton et al.",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : ", 2016) and distribution matching (Gretton et al., 2012; Zhuang et al., 2015).",
      "startOffset" : 34,
      "endOffset" : 77
    }, {
      "referenceID" : 56,
      "context" : ", 2016) and distribution matching (Gretton et al., 2012; Zhuang et al., 2015).",
      "startOffset" : 34,
      "endOffset" : 77
    }, {
      "referenceID" : 43,
      "context" : "For DANN, the training process of joint optimization is unstable, requiring extensive effort to tune the hyperparameters (Shah et al., 2018; Du et al., 2020; Karouzos et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 180
    }, {
      "referenceID" : 8,
      "context" : "For DANN, the training process of joint optimization is unstable, requiring extensive effort to tune the hyperparameters (Shah et al., 2018; Du et al., 2020; Karouzos et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "For DANN, the training process of joint optimization is unstable, requiring extensive effort to tune the hyperparameters (Shah et al., 2018; Du et al., 2020; Karouzos et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 180
    }, {
      "referenceID" : 41,
      "context" : "As for distribution matching, it is very difficult to preserve the discriminative power of the model on the target task while trying to perform instance level alignment (Saito et al., 2017; Lee et al., 2019).",
      "startOffset" : 169,
      "endOffset" : 207
    }, {
      "referenceID" : 25,
      "context" : "As for distribution matching, it is very difficult to preserve the discriminative power of the model on the target task while trying to perform instance level alignment (Saito et al., 2017; Lee et al., 2019).",
      "startOffset" : 169,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "Recent advances in self-supervised learning (SSL), such as contrastive learning (CL), have been proven effective at instance level by leveraging raw data to define surrogate tasks that help learn representations (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 212,
      "endOffset" : 290
    }, {
      "referenceID" : 22,
      "context" : "Recent advances in self-supervised learning (SSL), such as contrastive learning (CL), have been proven effective at instance level by leveraging raw data to define surrogate tasks that help learn representations (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 212,
      "endOffset" : 290
    }, {
      "referenceID" : 18,
      "context" : "Recent advances in self-supervised learning (SSL), such as contrastive learning (CL), have been proven effective at instance level by leveraging raw data to define surrogate tasks that help learn representations (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 212,
      "endOffset" : 290
    }, {
      "referenceID" : 6,
      "context" : "Recent advances in self-supervised learning (SSL), such as contrastive learning (CL), have been proven effective at instance level by leveraging raw data to define surrogate tasks that help learn representations (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 212,
      "endOffset" : 290
    }, {
      "referenceID" : 45,
      "context" : "Regarding CL for UDA, previous works mark cross-domain images with the same labels (for example, real and cartoon dogs) as the positive pairs in contrastive loss (Wang et al., 2021; Park et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 200
    }, {
      "referenceID" : 37,
      "context" : "Regarding CL for UDA, previous works mark cross-domain images with the same labels (for example, real and cartoon dogs) as the positive pairs in contrastive loss (Wang et al., 2021; Park et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 200
    }, {
      "referenceID" : 39,
      "context" : "Besides, from the domain adaptation perspective, constructing cross-domain positive samples and aligning domain-agnostic pairs have received less emphasis in related literature, since previous works focus on designing label preserving text transformations, such as back-translation, synonym, dropout and their combinations (Qu et al., 2021; Gao et al., 2021).",
      "startOffset" : 323,
      "endOffset" : 358
    }, {
      "referenceID" : 11,
      "context" : "Besides, from the domain adaptation perspective, constructing cross-domain positive samples and aligning domain-agnostic pairs have received less emphasis in related literature, since previous works focus on designing label preserving text transformations, such as back-translation, synonym, dropout and their combinations (Qu et al., 2021; Gao et al., 2021).",
      "startOffset" : 323,
      "endOffset" : 358
    }, {
      "referenceID" : 55,
      "context" : "More specifically, we synthesize these domain puzzles by utilizing adversarial examples (Zhu et al., 2020; Jiang et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "More specifically, we synthesize these domain puzzles by utilizing adversarial examples (Zhu et al., 2020; Jiang et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 126
    }, {
      "referenceID" : 42,
      "context" : "Furthermore, in order to investigate whether CL necessarily benefits UDA, we conduct experiments and find that constructing domain puzzles as paired positive samples is favorable for UDA, however, other data augmentation methods such as back translation (Sennrich et al., 2016; Edunov et al., 2018) do not have the same effect.",
      "startOffset" : 254,
      "endOffset" : 298
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, in order to investigate whether CL necessarily benefits UDA, we conduct experiments and find that constructing domain puzzles as paired positive samples is favorable for UDA, however, other data augmentation methods such as back translation (Sennrich et al., 2016; Edunov et al., 2018) do not have the same effect.",
      "startOffset" : 254,
      "endOffset" : 298
    }, {
      "referenceID" : 56,
      "context" : "The methods that learn domain invariant features for domain alignment include KL divergence (Zhuang et al., 2015), Maximum Mean Discrepancy (MMD) (Gretton et al.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : ", 2015), Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), and Domain Adversarial Neural Network (DANN) (Ganin et al.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : ", 2012), and Domain Adversarial Neural Network (DANN) (Ganin et al., 2016) (details of DANN can be found in Appendix.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 44,
      "context" : "DANN suffers from a vanishing gradient problem (Shen et al., 2018), and the training process is unstable (Shah et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 43,
      "context" : ", 2018), and the training process is unstable (Shah et al., 2018; Du et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : ", 2018), and the training process is unstable (Shah et al., 2018; Du et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 82
    }, {
      "referenceID" : 48,
      "context" : "Hence, more efficient and stable algorithms are essential for UDA (Wu et al., 2019b).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "of the state-of-the-art language models for many natural language understanding tasks (Madry et al., 2018; Zhu et al., 2020; Jiang et al., 2020; Pereira et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 166
    }, {
      "referenceID" : 55,
      "context" : "of the state-of-the-art language models for many natural language understanding tasks (Madry et al., 2018; Zhu et al., 2020; Jiang et al., 2020; Pereira et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "of the state-of-the-art language models for many natural language understanding tasks (Madry et al., 2018; Zhu et al., 2020; Jiang et al., 2020; Pereira et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 166
    }, {
      "referenceID" : 38,
      "context" : "of the state-of-the-art language models for many natural language understanding tasks (Madry et al., 2018; Zhu et al., 2020; Jiang et al., 2020; Pereira et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "In a single domain, adversarial training (Goodfellow et al., 2014) is an inner max, outer min adversarial problem with the objective:",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 34,
      "context" : "with (2), the standard adversarial training can also be regularized using virtual adversarial training (Miyato et al., 2018), which encourages smoothness in the embedding space.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 30,
      "context" : "(3) For (2)(3), the inner maximization can be solved by Projected Gradient Decent (PGD) (Madry et al., 2018) with an additional assumption that the loss function is locally linear.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 46,
      "context" : "Moreover, it may cause negative transfer, deteriorating knowledge transfer from source domain to the target domain (Wang et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "pre-trained language models, the number of iterations for perturbation estimation is usually 1 (Jiang et al., 2020; Pereira et al., 2021), as shown in Eq.",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 38,
      "context" : "pre-trained language models, the number of iterations for perturbation estimation is usually 1 (Jiang et al., 2020; Pereira et al., 2021), as shown in Eq.",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 41,
      "context" : "After acquiring domain puzzles, simply applying distribution matching will sacrifice discriminative knowledge learned from the source domain (Saito et al., 2017; Lee et al., 2019), and instance-based matching will also overlook global intra-domain information.",
      "startOffset" : 141,
      "endOffset" : 179
    }, {
      "referenceID" : 25,
      "context" : "After acquiring domain puzzles, simply applying distribution matching will sacrifice discriminative knowledge learned from the source domain (Saito et al., 2017; Lee et al., 2019), and instance-based matching will also overlook global intra-domain information.",
      "startOffset" : 141,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "In general, CL benefits from the definition of the augmented positive and negative pairs by treating instances as classes (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 122,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : "In general, CL benefits from the definition of the augmented positive and negative pairs by treating instances as classes (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 122,
      "endOffset" : 200
    }, {
      "referenceID" : 18,
      "context" : "In general, CL benefits from the definition of the augmented positive and negative pairs by treating instances as classes (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 122,
      "endOffset" : 200
    }, {
      "referenceID" : 6,
      "context" : "In general, CL benefits from the definition of the augmented positive and negative pairs by treating instances as classes (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 122,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "Specifically, maximizing the similarities between positive pairs learns an invariant instance-based representation, and minimizing the similarities between negative pairs learns a uniformly distributed representation from a global view, making instances gathered near the task decision boundary away from each other (Arora et al., 2019; Grill et al., 2020).",
      "startOffset" : 316,
      "endOffset" : 356
    }, {
      "referenceID" : 15,
      "context" : "Specifically, maximizing the similarities between positive pairs learns an invariant instance-based representation, and minimizing the similarities between negative pairs learns a uniformly distributed representation from a global view, making instances gathered near the task decision boundary away from each other (Arora et al., 2019; Grill et al., 2020).",
      "startOffset" : 316,
      "endOffset" : 356
    }, {
      "referenceID" : 5,
      "context" : "fore, we write the following contrastive infoNCE loss (Chen et al., 2020a) as follow:",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "For pre-trained language model, we use BERT base uncased (Devlin et al., 2019) as the basis for all experiments.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "For optimizer, we use AdamW (Kingma and Ba, 2015; Loshchilov and Hutter, 2018) with weight decay",
      "startOffset" : 28,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "For optimizer, we use AdamW (Kingma and Ba, 2015; Loshchilov and Hutter, 2018) with weight decay",
      "startOffset" : 28,
      "endOffset" : 78
    }, {
      "referenceID" : 56,
      "context" : "KL: Use symmetric KL-divergence loss of embedded instances between the source and target domains (Zhuang et al., 2015).",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "MMD: Maximum Mean Discrepancy loss (Gretton et al., 2012) measures the distance based on the notion of embedding probabilities in a reproducing kernel Hilbert space.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 42,
      "context" : "back-trans+CL: Back translation (Sennrich et al., 2016; Edunov et al., 2018) is one of the widely used data augmentation techniques.",
      "startOffset" : 32,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "back-trans+CL: Back translation (Sennrich et al., 2016; Edunov et al., 2018) is one of the widely used data augmentation techniques.",
      "startOffset" : 32,
      "endOffset" : 76
    }, {
      "referenceID" : 35,
      "context" : "We utilize en-de translation model pre-trained on WMT19 and released in fairseq (Ott et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "R-PERL (Ben-David et al., 2020) is a pivot-based method, and DAAT (Du et al.",
      "startOffset" : 7,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : ", 2020) is a pivot-based method, and DAAT (Du et al., 2020) combines",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "74% improvement over BERT base, and DCCL also surpasses state-of-the-art methods R-PERL (Ben-David et al., 2020) and DAAT (Du et al.",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 41,
      "context" : "Learning such cross-domain and instance-based matching will bring perplexity to language models and sacrifice task discrimination (Saito et al., 2017; Lee et al., 2019).",
      "startOffset" : 130,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : "Learning such cross-domain and instance-based matching will bring perplexity to language models and sacrifice task discrimination (Saito et al., 2017; Lee et al., 2019).",
      "startOffset" : 130,
      "endOffset" : 168
    }, {
      "referenceID" : 41,
      "context" : "To fairly compare with A-distance of the baselines, we use linear SVM to calculate ε following previous work (Saito et al., 2017; Du et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "To fairly compare with A-distance of the baselines, we use linear SVM to calculate ε following previous work (Saito et al., 2017; Du et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "Unsupervised Domain Adaptation UDA in NLP has the following approaches: (1) Pivot-based methods use unlabeled data from both domains, trying to discover characteristics that are similar (Pan et al., 2010).",
      "startOffset" : 186,
      "endOffset" : 204
    }, {
      "referenceID" : 57,
      "context" : "Some recent works extend pivots with autoencoders and contextualized embeddings (Ziser and Reichart, 2017; Miller, 2019; BenDavid et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 143
    }, {
      "referenceID" : 33,
      "context" : "Some recent works extend pivots with autoencoders and contextualized embeddings (Ziser and Reichart, 2017; Miller, 2019; BenDavid et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 143
    }, {
      "referenceID" : 52,
      "context" : "(2) Pseudo-labeling leverages a trained classifier to predict labels on unlabeled examples, which are subsequently considered as gold labels (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006).",
      "startOffset" : 141,
      "endOffset" : 199
    }, {
      "referenceID" : 54,
      "context" : "(2) Pseudo-labeling leverages a trained classifier to predict labels on unlabeled examples, which are subsequently considered as gold labels (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006).",
      "startOffset" : 141,
      "endOffset" : 199
    }, {
      "referenceID" : 32,
      "context" : "(2) Pseudo-labeling leverages a trained classifier to predict labels on unlabeled examples, which are subsequently considered as gold labels (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006).",
      "startOffset" : 141,
      "endOffset" : 199
    }, {
      "referenceID" : 27,
      "context" : "bine this technique with pre-trained language models (Lim et al., 2020; Ye et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 88
    }, {
      "referenceID" : 53,
      "context" : "bine this technique with pre-trained language models (Lim et al., 2020; Ye et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "(3) Domain Adversarial Neural Networks (Ganin et al., 2016).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 44,
      "context" : "Some approaches leverage Wasserstein distance to stabilize adversarial training (Shen et al., 2018; Shah et al., 2018), and combine it with posttraining which can produce better adversarial results (Du et al.",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 43,
      "context" : "Some approaches leverage Wasserstein distance to stabilize adversarial training (Shen et al., 2018; Shah et al., 2018), and combine it with posttraining which can produce better adversarial results (Du et al.",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : ", 2018), and combine it with posttraining which can produce better adversarial results (Du et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "(4) Adaptive pre-training is a more straightforward but effective method (Gururangan et al., 2020; Han and Eisenstein, 2019; Karouzos et al., 2021) by leveraging the objective of masked language model (MLM).",
      "startOffset" : 73,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "(4) Adaptive pre-training is a more straightforward but effective method (Gururangan et al., 2020; Han and Eisenstein, 2019; Karouzos et al., 2021) by leveraging the objective of masked language model (MLM).",
      "startOffset" : 73,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "(4) Adaptive pre-training is a more straightforward but effective method (Gururangan et al., 2020; Han and Eisenstein, 2019; Karouzos et al., 2021) by leveraging the objective of masked language model (MLM).",
      "startOffset" : 73,
      "endOffset" : 147
    }, {
      "referenceID" : 39,
      "context" : "Language Processing, many works study different label-preserving augmentations, such as backtranslations, synonyms, adversaries, dropout, and their combinations (Qu et al., 2021; Gao et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 196
    }, {
      "referenceID" : 11,
      "context" : "Language Processing, many works study different label-preserving augmentations, such as backtranslations, synonyms, adversaries, dropout, and their combinations (Qu et al., 2021; Gao et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 196
    }, {
      "referenceID" : 49,
      "context" : ", 2021) and CLEAR (Wu et al., 2020) jointly train the model with a contrastive objective and a masked language model setting.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 50,
      "context" : "ConSERT (Yan et al., 2021) overcomes the collapse problem of BERT-derived sentence representations and makes them more suitable for downstream applications by using unlabeled texts.",
      "startOffset" : 8,
      "endOffset" : 26
    } ],
    "year" : 0,
    "abstractText" : "In this work, we study Unsupervised Domain Adaptation (UDA) in a challenging selfsupervised approach. One of the difficulties is how to learn task discrimination in the absence of target labels. Unlike previous literature which directly aligns cross-domain distributions or leverages reverse gradient, we propose Domain Confused Contrastive Learning (DCCL), which can bridge the source and target domains via domain puzzles, and retain discriminative representations after adaptation. Technically, DCCL searches for a most domainchallenging direction and exquisitely crafts domain confused augmentations as positive pairs, then it contrastively encourages the model to pull representations towards the other domain, thus learning more stable and effective domain invariances. We also investigate whether contrastive learning necessarily helps with UDA when performing other data augmentations. Extensive experiments demonstrate that DCCL significantly outperforms baselines.",
    "creator" : null
  }
}
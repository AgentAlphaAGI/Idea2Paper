{
  "name" : "ARR_2022_64_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Power of Entity Representations in Multilingual Pretrained Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages with entity representations and show the model consistently outperforms word-based pretrained models in various crosslingual transfer tasks. We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations."
    }, {
      "heading" : "1 Introduction",
      "text" : "Pretrained language models have become crucial for achieving state-of-the-art performance in modern natural language processing. In particular, multilingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Doddapaneni et al., 2021) have attracted considerable attention particularly due to their utility in cross-lingual transfer.\nIn zero-shot cross-lingual transfer, a pretrained encoder is fine-tuned in a single resource-rich language (typically English), and then evaluated on other languages never seen during fine-tuning. A key to solving cross-lingual transfer tasks is to obtain representations that generalize well across languages. Several studies aim to improve multilingual models with cross-lingual supervision such as bilingual word dictionaries (Conneau et al., 2020b) or parallel sentences (Conneau and Lample, 2019).\nAnother source of such information is the crosslingual mappings of Wikipedia entities (articles). Wikipedia entities are aligned across languages via inter-language links and the text contains numerous entity annotations (hyperlinks). With these data, models can learn cross-lingual correspondence such as the words Tokyo and東京 refers to the same entity. Wikipedia entity annotations have been shown to provide rich cross-lingual alignment information to improve multilingual language models (Iacer Calixto and Pasini, 2021; Xiaoze Jian and Duan, 2021). However, previous studies only incorporate entity information through an auxiliary loss function during pretraining, and the models do not explicitly have entity representations used for downstream tasks.\nIn this study, we investigate the effectiveness of entity representations in multilingual language models. Entity representations are known to enhance language models in mono-lingual settings (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Yamada et al., 2020) presumably by introducing real-world knowledge. We argue that using entity representations facilitates cross-lingual transfer by providing languageindependent features. To this end, we present a multilingual extension of LUKE (Yamada et al., 2020). The model is trained with the multilingual masked language modeling (MLM) task as well as the masked entity prediction (MEP) task with Wikipedia entity embeddings.\nWe investigate two ways of using the entity representations in cross-lingual transfer tasks: (1) perform entity linking for the input text, and append the detected entity tokens to the input sequence. The entity tokens are expected to provide languageindependent features to the model. We evaluate this approach with cross-lingual question answering (QA) datasets: XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020); (2) use the entity [MASK] token from the MEP task as a language-\nindependent feature extractor. In the MEP task, word tokens in a mention span are associated with an entity [MASK] token, the contextualized representation of which is used to train the model to predict its original identity. Here, we apply similar input formulations to tasks involving mention-span classification, relation extraction (RE) and named entity recognition (NER): the attribute of a mention or a pair of mentions is predicted using their contextualized entity [MASK] feature. We evaluate this approach with the RELX (Köksal and Özgür, 2020) and CoNLL NER (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) datasets.\nThe experimental results show that these entitybased approaches consistently outperform wordbased baselines. Our analysis reveals that entity representations provide more language-agnostic features to solve the downstream tasks.\nWe also explore solving a multilingual zero-shot cloze prompt task (Liu et al., 2021) with the entity [MASK] token. Recent studies have shown that we can address various downstream tasks by querying a language model for blanks in prompts (Petroni et al., 2019; Cui et al., 2021). Typically, the answer tokens are predicted from the model’s word-piece vocabulary but here we incorporate the prediction from the entity vocabulary queried by the entity [MASK] token. We evaluate our approach with the mLAMA dataset (Kassner et al., 2021) in various languages and show that using the entity [MASK] token reduces language bias and elicits correct factual knowledge more likely than using only the word [MASK] token."
    }, {
      "heading" : "2 Multilingual Language Models with Entity Representations",
      "text" : ""
    }, {
      "heading" : "2.1 Model: mulitlingual LUKE",
      "text" : "To evaluate the effectiveness of entity representations for cross-lingual downstream tasks, we introduce a new multilingual language model based on a bidirectional transformer encoder: Multilingual LUKE (mLUKE), a multilingual extension of LUKE (Yamada et al., 2020). The model is trained with the masked language modeling (MLM) task (Vaswani et al., 2017) as well as the masked entity prediction (MEP) task. In MEP, some of the input entity tokens are randomly masked with the special entity [MASK] token, and the model is trained to predict the original entities. Note that the entity [MASK] token is different from the word [MASK] token for MLM.\nThe model takes as input a tokenized text (w1, w2, ..., wm) and the entities appearing in the text (e1, e2, ..., en), and compute the contextualized representation for each token (hw1 ,hw2 , ...,hwm and he1 ,he2 , ...,hen). The word and entity tokens equally undergo self-attention computation (i.e., no entity-aware self-attention in the original work) after embedding layers.\nThe word and entity embeddings are computed as the summation of the following three embeddings: token embeddings, type embeddings, and position embeddings (Devlin et al., 2019). The entity tokens are associated with the word tokens through position embeddings: the position of an entity token is defined as the positions of its corresponding word tokens, and the entity position embeddings are summed over the positions. Model Configuration. Our model configuration follows XLM-RoBERTa-base (Conneau et al., 2020a), a variant of BERT (Devlin et al., 2019) trained with CommonCrawl data from 100 languages, and the parameters in common (e.g., the weights of the transformer encoder and the word embeddings) are initialized using the checkpoint from the Transformers library.1\nThe size of the entity embeddings is set to 256 and they are projected to the size of the word embeddings before fed into the encoder."
    }, {
      "heading" : "2.2 Training Corpus: Wikipedia",
      "text" : "We use Wikipedia dumps in 24 languages (Appendix A) as the training data. The languages are selected to cover reasonable numbers of languages that appear in downstream cross-lingual datasets. We generate input sequences by splitting the content of each page into sequences of sentences comprising ≤ 512 words with their entity annotations (i.e., hyperlinks). During training, data are sampled from each language with ni items with the following multinomial distribution:\npi = nαi∑N k=1 n α k , (1)\nwhere α is a smoothing parameter and set to 0.7 following multilingual BERT.2 Entity Vocabulary. Entities in the model are defined as Wikipedia articles. The articles from different languages are aligned through inter-language\n1https://huggingface.co/transformers/ 2https://github.com/google-research/\nbert/blob/master/multilingual.md\nlinks3 and the aligned articles are treated as a single entity. We include in the vocabulary the most frequent 1.2M entities in terms of the number of hyperlinks that appear across at least three languages to facilitate cross-lingual learning. Optimization. We optimize the model with a batch size of 2048 for 1M steps in total using AdamW (Loshchilov and Hutter, 2019) with warmup and linear decay of the learning rate. To stabilize training, we perform pretraining in two stages: (1) in the first 500K steps, we update only those parameters that are randomly initialized (e.g., entity embeddings); (2) we update all parameters in the remaining 500K steps. The learning rate scheduler is reset at each training stage. For further details on hyperparameters, please see Appendix A."
    }, {
      "heading" : "2.3 Baseline Models",
      "text" : "We compare the primary model that we investigate, multilingual LUKE used with entity representations (mLUKE-E), against several baseline pretrained models and an ablation model based on word representations: mBERT (Devlin et al., 2019) is one of the earliest multilingual language models. We provide this results as a reference. XLM-R (Conneau et al., 2020a) is the model that mLUKE-E is built on. This result indicates how our additional pretraining step and entity representation impacts the performance. Since earlier studies (Liu et al., 2019; Lan et al., 2020) indicated longer pretraining would simply improve performance, we\n3https://en.wikipedia.org/wiki/Help: Interlanguage_links. We build an inter-language database from the wikidatawiki dump from November 30, 2020.\ntrain another model based on XLM-R with extra MLM pretraining following the same configuration of mLUKE. mLUKE-W is an ablation model of mLUKE-E. This model discards the entity embeddings learned during pretraining and only takes word tokens as input as with the other baseline models. The results from this model indicate the effect of cross-lingual alignment from pretraining with entity information (Iacer Calixto and Pasini, 2021; Xiaoze Jian and Duan, 2021), and the comparison with this model will highlight the effect of using entity representations for downstream tasks in mLUKE-E.\nThe above models are fine-tuned with the same hyperparameter search space and computational budget as described in Appendix B. Readers interested in comparison with other recently proposed pretrained models or ablated models can refer to Appendix C and D."
    }, {
      "heading" : "3 Adding Entities as Language-Agnostic Features in QA",
      "text" : "We evaluate the approach of adding entity embeddings to the input of mLUKE-E with cross-lingual extractive question answering (QA) tasks. The task is, given a question and a context passage, to extract the answer span from the context. Here the entity embeddings provide language-agnostic features and thus are expected to facilitate crosslingual transfer learning."
    }, {
      "heading" : "3.1 Main Experiments",
      "text" : "Datasets. We fine-tune the pretrained models with the SQuAD 1.1 dataset (Rajpurkar et al., 2016), and evaluate them with the two multilingual datasets:\nXQuAD en es de el ru tr ar vi th zh hi avg.\nmBERT 84.5 76.1 73.1 59.0 70.2 53.2 62.1 68.5 40.7 58.3 57.0 63.9 XLM-R 84.0 76.5 76.4 73.9 74.4 67.8 68.1 74.2 66.8 61.5 68.7 72.0 + extra training 86.1 76.9 76.5 73.7 74.7 66.3 68.2 74.5 67.7 64.7 66.6 72.4 mLUKE-W 85.7 78.0 77.4 74.7 75.7 68.3 71.7 75.9 67.1 65.1 69.9 73.6 mLUKE-E 86.3 78.9 78.9 73.9 76.0 68.8 71.4 76.4 67.5 65.9 72.2 74.2\nXQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020). XQuAD is created by translating a subset of the SQuAD development set while the source of MLQA is natural text in Wikipedia. Besides multiple monolingual evaluation data splits, MLQA also offers data to evaluate generalized cross-lingual transfer (G-XLT), where the question and context texts are in different languages.\nModels. All QA models used in this experiment follow Devlin et al. (2019). The model takes the question and context word tokens as input and predicts a score for each span of the context word tokens. The span with the highest score is predicted as the answer to the question.\nThe mLUKE-E model takes entity tokens as additional features in the input (Figure 1) to enrich word representations. The entities are automatically detected using a heuristic string matching based on the original Wikipedia article from which the dataset instance is created. See Appendix E for more details.\nResults. Table 1 summarizes the model’s F1 scores for each language. On the effectiveness of entity representations, mLUKE-E performs better than its word-based counterpart mLUKE-W (0.6 average points improvements in the XQuAD average score, 0.1 points improvements in MLQA), which indicates the input entity tokens provide useful features to facilitate cross-lingual transfer. The usefulness of entities is demonstrated especially in the MLQA’s generalized cross-lingual transfer setting (full results available in Appendix F). mLUKEE exhibits a substantial 1.6 point improvement in\nthe G-XLT average score over mLUKE-W. This suggests that entity representations are beneficial in a challenging situation where the model needs to capture language-agnostic semantics from text segments in different languages.\nWe also observe that on both XQuAD and MLQA, XLM-R benefits from extra training (an improvement of 72.0 to 72.4 in the average score from XQuAD and 65 .7 to 67.8 in MLQA). The mLUKE-W model further improves the average score from XLM-R with extra training (1.2 points improvement in XQuAD and 2.1 points in MLQA), showing the effectiveness of the MEP task to improve multilingual representations."
    }, {
      "heading" : "3.2 Analysis",
      "text" : "How do the entity representations help the model in cross-lingual transfer? In the mLUKE-E model, the input entity tokens annotate mention spans on which the model performs prediction. We hypothesize that this allows the encoder to inject languageagnostic entity knowledge into span representations, which help better align representations across languages. To support this hypothesis, we compare the degree of alignment between span representations before and after adding entity embeddings in the input, i.e., mLUKE-W and mLUKE-E. Task. We quantify the degree of alignment as performance on the contextualized word retrieval (CWR) task (Cao et al., 2020). The task is, given a word within a sentence in the query language, to find the word with the same meaning in the context from a candidate pool in the target language.\nDataset. We use the MLQA dev set (Lewis et al., 2020). As MLQA is constructed from parallel sentences mined from Wikipedia, some sentences and answer spans are aligned and thus the dataset can be easily adapted for the CWR task. As the query and target word, we use the answer span4 annotated in the dataset, which is also parallel across the languages. We use the English dataset as the query language and other languages as the target. We discard query instances that do not have their parallel data in the target language. The candidate pool is all answer spans in the target language data. Models. We evaluate mLUKE-W and mLUKEE without fine-tuning. The retrieval is performed by ranking the cosine similarity of contextualized span representations, which is computed by meanpooling the output word vectors in the span. Results. Table 2 shows the performance in terms of the mean reciprocal rank score. We observe that the scores of mLUKE-E are higher than mLUKEW across all the languages. This demonstrates that adding entities improves the degree of alignment of span representations, which explains the improvement of mLUKE-E in the cross-lingual QA task."
    }, {
      "heading" : "4 The Entity MASK Token as Feature Extractor in Relation Extraction and NER",
      "text" : "In this section, we evaluate the approach of using the entity [MASK] token to extract features from mLUKE-E for two entity-related tasks: relation extraction and named entity recognition.\nWe formulate both tasks as the classification of mention spans. The baseline models extract the feature of spans as the contextualized representations of word tokens, while mLUKE-E extracts the feature as the contextualized representations of the special language-independent entity tokens associated with the mentions (Figure 1). We demonstrate that this approach consistently improves the performance in cross-lingual transfer.\n4Answer spans are not necessarily a word, but here we generalize the task as span retrieval for our purpose."
    }, {
      "heading" : "4.1 Relation Extraction",
      "text" : "Relation Extraction (RE) is a task to determine the correct relation between the two (head and tail) entities in a sentence. Datasets. We fine-tune the models with the English KBP-37 dataset (Zhang and Wang, 2015) and evaluate the models with the RELX dataset (Köksal and Özgür, 2020), which is created by translating a subset of 502 sentences from KBP-37’s test set into four different languages. Following Köksal and Özgür (2020), we report the macro average of F1 scores of the 18 relations. Models. In the input text, the head and tail entities are surrounded with special markers (<ent>, <ent2>). The baseline models extract the feature vectors for the entities as the contextualized vector of the first marker followed by their mentions. The two entity features are concatenated and fed into a linear classifier to predict their relation.\nFor mLUKE-E, we introduce two special entities, [HEAD] and [TAIL], to represent the head and tail entities (Yamada et al., 2020). Their embeddings are initialized with the entity [MASK] embedding. They are added to the input sequence being associated with the entity mentions in the input, and their contextualized representations are extracted as the feature vectors. As with the wordbased models, the features are concatenated and input to a linear classifier."
    }, {
      "heading" : "4.2 Named Entity Recognition",
      "text" : "Named Entity Recognition (NER) is the task to detect entities in a sentence and classify their type. We use the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) as the training data, and evaluate the models with the CoNLL2003 German dataset and the CoNLL-2002 Spanish and Dutch dataset (Tjong Kim Sang, 2002). Models. We adopt the model of Sohrab and Miwa (2018) as the baseline model, which enumerates all possible spans in a sentence and classifies them into the target entity types or non-entity type. In this experiment, we enumerate spans with at most 16 tokens. For the baseline models, the span features are computed as the concatenation of the word representations of the first and last tokens. The span features are fed into a linear classifier to predict their entity type.\nThe input of mLUKE-E contains the entity [MASK] tokens associated with all possible spans. The span features are computed as the contextual-\nized representations of the entity [MASK] tokens. The features are input to a linear classifier as with the word-based models."
    }, {
      "heading" : "4.3 Main Results",
      "text" : "The results are shown in Table 3. In cross-lingual transfer, mLUKE-E performs the best overall except for Spanish (es). The mLUKE-E model outperforms its word-based counterpart mLUKE-W in average score by 1.3 points in RE and 1.1 points in NER, which shows entity-based features are useful in cross-lingual tasks. We also observe that XLMR benefits from extra training (1.8 average points improvement in RE and 0.3 points in NER)."
    }, {
      "heading" : "4.4 Analysis",
      "text" : "The performance gain of mLUKE-E over mLUKEW can be partly explained as the entity [MASK] token extracts better features for predicting entity attributes because it resembles how mLUKE is pretrained with the MEP task. We hypothesize that there exists another factor for the improvement in cross-lingual performance: language neutrality of representations.\nUnlike some word tokens, entity tokens including the entity [MASK] token are shared across languages and their contextualized representations may be less affected by the difference of input languages, resulting in features that generalize well for cross-lingual transfer. To find out if the entity-based features are actually more languageindependent than word-based features, we evaluate the modularity (Fujinuma et al., 2019) of the features extracted for the RELX dataset.\nModularity is computed for the k-nearest neighbor graph of embeddings and measures the degree to which embeddings tend to form clusters within the same language. We refer readers to Fujinuma et al. (2019) for how to compute the metric. Note that the maximum value of modularity is 1 and 0\nmeans the embeddings are completely randomly distributed regardless of language.\nWe compare the modularity of the word features from mLUKE-W and entity features from mLUKEE before fine-tuning. Note that the features here are concatenated vectors of head and tail features. Table 4 shows that the modularity of mLUKE-E is much lower than mLUKE-W, demonstrating that entity-based features are more language-neutral. However, with entity-based features, the modularities are still greater than zero. In particular, the modularity computed with Turkish, which is the most distant language from English here, is significantly higher than the others, indicating that the contextualized entity-based features are still somewhat language-dependent."
    }, {
      "heading" : "5 Cloze Prompt Task with Entity Representations",
      "text" : "In this section, we show that using the entity representations is effective in a cloze prompt task (Liu et al., 2021) with the mLAMA dataset (Kassner et al., 2021). The task is, given a cloze template such as “[X] was born in [Y]” with [X] filled with an entity (e.g., Mozart), to predict a correct entity in [Y] (e.g., Austria). We adopt the typed querying setting (Kassner et al., 2021), where a template has a set of candidate answer entities and the prediction becomes the one with the highest score assigned by the language model.\nModel. As in Kassner et al. (2021), the word-based baseline models compute the candidate score as the log-probability from the MLM classifier. When a candidate entity in [Y] is tokenized into multiple tokens, the same number of the word [MASK] tokens are placed in the input sequence, and the score is computed by taking the average of the logprobabilities for its individual tokens.\nOn the other hand, mLUKE-E computes the logprobability of the candidate entity in [Y] with the entity [MASK] token. Each candidate entity is associated with an entity in mLUKE’s entity vocabulary via string matching. The input sequence has the entity [MASK] token associated with the word [MASK] tokens in [Y], and the candidate score is computed as the log-probability from the MEP classifier. We also try additionally appending the entity token of [X] to the input sequence if the entity is found in the vocabulary.\nTo accurately measure the difference between word-based and entity-based prediction, we restrict the candidate entities to the ones found in the entity vocabulary and exclude the questions if their answers are not included in the candidates. (results with full candidates and questions in the dataset are in Appendix G). Results. We experiment in total with 16 languages which are available both in the mLAMA dataset and the mLUKE’s entity vocabulary. Here we only present the top-1 accuracy results from 9 languages on Table 5, as we can make similar observations with the other languages.\nWe observe that XLM-R performs notably worse than mBERT as mentioned in Kassner et al. (2021). However, with extra training with the Wikipedia corpus, XLM-R shows a significant 9.3 points improvement in the average score and outperforms mBERT (27.8 vs. 27.2). We conjecture that this shows the importance of the training corpus for this task. The original XLM-R is only trained with the CommonCrawl corpus (Conneau et al., 2020a), text scraped from a wide variety of web pages, while mBERT and XLM-R + training are trained on Wikipedia. The performance gaps indicate that Wikipedia is particularly useful for the model to learn factual knowledge.\nThe mLUKE-W model lags behind XLM-R + extra training by 1.7 average points but we can see 5.4 points improvement from XLM-R + extra training to mLUKE-E ([Y]), indicating entity representations are more suitable to elicit correct\nfactual knowledge from mLUKE than word representations. Adding the entity corresponding to [X] to the input (mLUKE-E ([X] & [Y])) further pushes the performance by 11.7 points to 44.9 %, which further demonstrates the effectiveness of entity representations. Analysis of Language Bias. Kassner et al. (2021) notes that the prediction of mBERT is biased by the input language. For example, when queried in Italian (e.g., “[X] e stato creato in [MASK].”), the model tends to predict entities that often appear in Italian text (e.g., Italy) for any question to answer location. We expect that using entity representations would reduce language bias since entities are more language-independent than words.\nWe qualitatively assess the degree of language bias in the models looking at their incorrect predictions. We show the top incorrect prediction for the template “[X] was founded in [Y].” for each model in Table 6, together with the top-1 incorrect ratio, that is, the ratio of the number of the most common incorrect prediction to the total false predictions, which indicates how much the false predictions are dominated by few frequent entities.\nThe examples show that the different models exhibit bias towards different entities as in English and French, although in Japanese the model consistently tends to predict Japan. Looking at the degree of language bias, mLUKE-E ([X]& [Y]) exhibits lower top-1 incorrect ratios overall (27% in fr, 44% in ja, and 30% in fr), which indicates using entity representations reduces language bias. However, lower language bias does not necessarily mean better performance: in French (fr), mLUKE gives a lower top-1 incorrect ratio than mBERT (30% vs. 71%) but their numbers of total false predictions are the same (895). Language bias is only one of several factors in the performance bottleneck."
    }, {
      "heading" : "6 Related Work",
      "text" : ""
    }, {
      "heading" : "6.1 Multilingual Pretrained Language Models",
      "text" : "Multilingual pretrained language models have recently seen a surge of interest due to their effectiveness in cross-lingual transfer learning (Conneau and Lample, 2019; Liu et al., 2020). A straightforward way to train such models is multilingual masked language modeling (mMLM) (Devlin et al., 2019; Conneau et al., 2020a), i.e., training a single model with a collection of monolingual corpora in multiple languages. Although models trained with\nmMLM exhibit a strong cross-lingual ability without any cross-lingual supervision (Karthikeyan K and Roth, 2020; Conneau et al., 2020b), several studies aim to develop better multilingual models with explicit cross-lingual supervision such as bilingual word dictionaries (Conneau et al., 2020b) or parallel sentences (Conneau and Lample, 2019). In this study, we build a multilingual pretrained language model on the basis of XLM-RoBERTa (Conneau et al., 2020a), trained with mMLM as well as the masked entity prediction (MEP) (Yamada et al., 2020) with entity representations."
    }, {
      "heading" : "6.2 Pretrained Language Models with Entity Knowledge",
      "text" : "Language models trained with a large corpus contain knowledge about real-world entities, which is useful for entity-related downstream tasks such as relation classification, named entity recognition, and question answering. Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Févry et al., 2020; Yamada et al., 2020).\nWhen incorporated into multilingual language models, entity information can bring another benefit: entities may serve as anchors for the model to align representations across languages. Multilingual knowledge bases often offer mappings between different surface forms across languages for\nthe same entity. A typical data source is Wikipedia, which covers more than 100 languages and the text in the Wikipedia articles contains numerous entity annotations (hyperlinks). Iacer Calixto and Pasini (2021) fine-tuned the top two layers of multilingual BERT by predicting language-agnostic entity ID from hyperlinks in Wikipedia articles. As our concurrent work, Xiaoze Jian and Duan (2021) trained a model based on XLM-RoBERTa with an entity prediction task along with an object entailment prediction task. While the previous studies focus on improving cross-lingual language representations by pretraining with entity information, our work investigates a multilingual model not only pretrained with entities but also explicitly having entity representations and how to extract better features from such model."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We investigated the effectiveness of entity representations in multilingual language models. Our pretrained model, mLUKE, not only exhibits strong empirical results with the word inputs (mLUKE-W) but also shows even better performance with the entity representations (mLUKE-E) in cross-lingual transfer tasks. We also show that a cloze-promptstyle fact completion task can effectively be solved with the query and answer space in the entity vocabulary. Our results suggest a promising direction to pursue further on how to leverage entity representations in multilingual tasks."
    }, {
      "heading" : "Appendix for “The Power of Entity Representations in Multilingual Pretrained Language Models”",
      "text" : ""
    }, {
      "heading" : "A Details of pretraining",
      "text" : "Dataset. We download the Wikipedia dumps from December 1st, 2020. We show the 24 languages included in the dataset on Table 7, along with the data size and the number of entities in the vocabulary.\nOptimization. We optimize the mLUKE model for 1M steps in total using AdamW (Loshchilov and Hutter, 2019) with learning rate warmup and linear decay of the learning rate. The pretraining consists of two stages: (1) in the first 500K steps, we update only those parameters that are randomly initialized (e.g., entity embeddings); (2) we update all parameters in the remaining 500K steps. The learning rate scheduler is reset at each training stage. The detailed hyper-parameters are shown in Table 8.\nComputing Infrastructure. We run the pretraining on NVIDIA’s PyTorch Docker container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 2 months."
    }, {
      "heading" : "B Details of Downstream Experiments",
      "text" : "Hyperparameter Search. For each downstream task, we perform hyperparameter searching for all the models with the same computational budget to ensure a fair comparison. For each task, we use the final evaluation metric on the validation split of the training English corpus as the validation score. The models are optimized with the AdamW optimizer (Loshchilov and Hutter, 2019) with the weight decay term set to 0.01 and a linear warmup scheduler. The learning rate is linearly increased to a specified value in the first 6 % of training steps, and then gradually decreased to zero towards the end. Table 9 summarizes the task-specific hyperparameter search spaces.\nComputing Infrastructure. We run the fine-tuning on a server with a AMD Ryzen Threadripper 3970X 32-Core Processor CPU and 4 NVIDIA Quadro RTX 5000 GPUs. All experiments are run with a single GPU."
    }, {
      "heading" : "C Comparison with Other Pretrained Models",
      "text" : "We repeat the results of MLQA in Table 10 with the results of other multilingual pretrained models taken from their original papers. ERNIE-M (Ouyang et al., 2020) and InfoXLM (Chi et al., 2021) are trained with parallel corpora while XLM-K (Xiaoze Jian and Duan, 2021) is trained only with monolingual corpora and thus is closer to our setting. Notice, however, that their results are not directly comparable to ours, because the fine-tuning settings are different.\nThe models trained with parallel corpora achieve comparable or better scores (the average score of 67.9 in InfoXLM and 68.7 in ERNIE-M) than mLUKE-E (67.9). This suggests that our model could see further improvement when pretrained with cross-lingual supervision from parallel corpora."
    }, {
      "heading" : "D Ablation study of entity embeddings",
      "text" : "In Section 3 and 4, we have shown that using entity representations in mLUKE improves the crosslingual transfer performance in QA, RE, and NER. Here we conduct an additional ablation study to investigate whether the learned entity embeddings are crucial to the success of our approach. We train an ablated model of mLUKE-E whose entity embeddings are re-initialized randomly before fine-tuning (- ablation). Table 11 and Table 12 show that the ablated model performs significantly worse than the full model (mLUKE-E), indicating that using pretrained entity embeddings is crucial rather than applying our approach during fine-tuning in an ad-hoc manner without entity-aware pretraining.\nXQuAD en es de el ru tr ar vi th zh hi avg.\nmLUKE-E 86.3 78.9 78.9 73.9 76.0 68.8 71.4 76.4 67.5 65.9 72.2 74.2 - ablation 84.3 76.8 76.4 71.9 74.3 67.4 70.2 75.3 67.1 64.4 68.4 72.4"
    }, {
      "heading" : "E Detecting Entities in the QA datasets",
      "text" : "For each question–passage pair in the QA datasets, we first create a mapping from the entity mention strings (e.g., “U.S.”) to their referent Wikipedia entities (e.g., United States) using the entity hyperlinks on the source Wikipedia page of the passage. We then perform simple string matching to extract all entity names in the question and the passage and treat all matched entity names as entity annotations for their referent entities. We ignore an entity name if the name refers to multiple entities on the page. Further, to reduce noise, we also exclude an entity name if its link probability, the probability that the name appears as a hyperlink in Wikipedia, is lower than 1%.\nThe XQuAD datasets are created by translating English Wikipedia articles into target languages. For those translated data, we translate the mention-entity mapping created from the source English articles by the following procedure: for all the entities found in the source article, we map the entity into the corresponding article in the target language through inter-language links, and then collect its possible mention strings (i.e., hyperlinks to the article) from a Wikipedia dump of the target language; the entities in the source article and the collected mention strings form the mention-entity mapping for the translated article."
    }, {
      "heading" : "F Full Results of MLQA",
      "text" : ""
    }, {
      "heading" : "G Full Results of mLAMA",
      "text" : "Table 5 shows the results from the setting where the entity candidates not in the mLUKE’s entity vocabulary are excluded. Here we provide in Table 18 the results with the full candidate set provided in the dataset for ease of comparison with other literature. When the candidate entity is not found in the mLUKE’s entity vocabulary, the log-probability from the word [MASK] tokens are used instead."
    } ],
    "references" : [ {
      "title" : "On the Cross-lingual Transferability of Monolingual Representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual Alignment of Contextual Word Representations",
      "author" : [ "Steven Cao", "Nikita Kitaev", "Dan Klein." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Chi et al\\.,? 2021",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised Cross-lingual Representation Learning at Scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020a",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual Language Model Pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Emerging Cross-lingual Structure in Pretrained Language Models",
      "author" : [ "Alexis Conneau", "Shijie Wu", "Haoran Li", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Conneau et al\\.,? 2020b",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Template-Based Named Entity Recognition Using BART",
      "author" : [ "Leyang Cui", "Yu Wu", "Jian Liu", "Sen Yang", "Yue Zhang." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.",
      "citeRegEx" : "Cui et al\\.,? 2021",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A Primer on Pretrained Multilingual Language Models",
      "author" : [ "Sumanth Doddapaneni", "Gowtham Ramesh", "Anoop Kunchukuttan", "Pratyush Kumar", "Mitesh M. Khapra." ],
      "venue" : "ArXiv, abs/2107.00676.",
      "citeRegEx" : "Doddapaneni et al\\.,? 2021",
      "shortCiteRegEx" : "Doddapaneni et al\\.",
      "year" : 2021
    }, {
      "title" : "Entities as Experts: Sparse Memory Access with Entity Supervision",
      "author" : [ "Thibault Févry", "Livio Baldini Soares", "Nicholas FitzGerald", "Eunsol Choi", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Févry et al\\.,? 2020",
      "shortCiteRegEx" : "Févry et al\\.",
      "year" : 2020
    }, {
      "title" : "A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity",
      "author" : [ "Yoshinari Fujinuma", "Jordan Boyd-Graber", "Michael J. Paul." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Fujinuma et al\\.,? 2019",
      "shortCiteRegEx" : "Fujinuma et al\\.",
      "year" : 2019
    }, {
      "title" : "Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks",
      "author" : [ "Alessandro Raganato Iacer Calixto", "Tommaso Pasini." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Calixto and Pasini.,? 2021",
      "shortCiteRegEx" : "Calixto and Pasini.",
      "year" : 2021
    }, {
      "title" : "Cross-Lingual Ability of Multilingual BERT: An Empirical Study",
      "author" : [ "Stephen Mayhew Karthikeyan K", "Zihan Wang", "Dan Roth." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "K et al\\.,? 2020",
      "shortCiteRegEx" : "K et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models",
      "author" : [ "Nora Kassner", "Philipp Dufter", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Kassner et al\\.,? 2021",
      "shortCiteRegEx" : "Kassner et al\\.",
      "year" : 2021
    }, {
      "title" : "The RELX Dataset and Matching the Multilingual Blanks for Cross-Lingual Relation Classification",
      "author" : [ "Abdullatif Köksal", "Arzucan Özgür." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020.",
      "citeRegEx" : "Köksal and Özgür.,? 2020",
      "shortCiteRegEx" : "Köksal and Özgür.",
      "year" : 2020
    }, {
      "title" : "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "MLQA: Evaluating Cross-lingual Extractive Question Answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
      "author" : [ "Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig." ],
      "venue" : "ArXiv, abs/2107.13586.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Y. Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual Denoising Pre-training for Neural Machine Translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Lin-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Decoupled Weight Decay Regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations. 9",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Erniem: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
      "author" : [ "Xuan Ouyang", "Shuohuan Wang", "Chao Pang", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "ArXiv, abs/2012.15674.",
      "citeRegEx" : "Ouyang et al\\.,? 2020",
      "shortCiteRegEx" : "Ouyang et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge Enhanced Contextual Word Representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Robert Logan", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Language Models as Knowledge Bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep Exhaustive Model for Nested Named Entity Recognition",
      "author" : [ "Mohammad Golam Sohrab", "Makoto Miwa." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Sohrab and Miwa.,? 2018",
      "shortCiteRegEx" : "Sohrab and Miwa.",
      "year" : 2018
    }, {
      "title" : "Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002.",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
      "author" : [ "Xiaozhi Wang", "Tianyu Gao", "Zhaocheng Zhu", "Zhiyuan Liu", "Juan-Zi Li", "J. Tang." ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge",
      "author" : [ "Weizhu Chen Xiaoze Jian", "Yaobo Liang", "Nan Duan." ],
      "venue" : "ArXiv, abs/2109.12573.",
      "citeRegEx" : "Jian et al\\.,? 2021",
      "shortCiteRegEx" : "Jian et al\\.",
      "year" : 2021
    }, {
      "title" : "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model",
      "author" : [ "Wenhan Xiong", "Jingfei Du", "William Yang Wang", "Veselin Stoyanov." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Xiong et al\\.,? 2020",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2020
    }, {
      "title" : "LUKE: Deep Contextualized Entity Representations with Entityaware Self-attention",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Hiroyuki Shindo", "Hideaki Takeda", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Yamada et al\\.,? 2020",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2020
    }, {
      "title" : "Relation Classification via Recurrent Neural Network",
      "author" : [ "Dongxu Zhang", "Dong Wang." ],
      "venue" : "ArXiv, abs/1508.01006.",
      "citeRegEx" : "Zhang and Wang.,? 2015",
      "shortCiteRegEx" : "Zhang and Wang.",
      "year" : 2015
    }, {
      "title" : "ERNIE: Enhanced Language Representation with Informative Entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In particular, multilingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Doddapaneni et al., 2021) have attracted considerable attention particularly due to their utility in cross-lingual transfer.",
      "startOffset" : 44,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "In particular, multilingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Doddapaneni et al., 2021) have attracted considerable attention particularly due to their utility in cross-lingual transfer.",
      "startOffset" : 44,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "In particular, multilingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Doddapaneni et al., 2021) have attracted considerable attention particularly due to their utility in cross-lingual transfer.",
      "startOffset" : 44,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "Several studies aim to improve multilingual models with cross-lingual supervision such as bilingual word dictionaries (Conneau et al., 2020b) or parallel sentences (Conneau and Lample, 2019).",
      "startOffset" : 118,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "Entity representations are known to enhance language models in mono-lingual settings (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Yamada et al., 2020) presumably by introducing real-world knowledge.",
      "startOffset" : 85,
      "endOffset" : 186
    }, {
      "referenceID" : 22,
      "context" : "Entity representations are known to enhance language models in mono-lingual settings (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Yamada et al., 2020) presumably by introducing real-world knowledge.",
      "startOffset" : 85,
      "endOffset" : 186
    }, {
      "referenceID" : 29,
      "context" : "Entity representations are known to enhance language models in mono-lingual settings (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Yamada et al., 2020) presumably by introducing real-world knowledge.",
      "startOffset" : 85,
      "endOffset" : 186
    }, {
      "referenceID" : 31,
      "context" : "Entity representations are known to enhance language models in mono-lingual settings (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Yamada et al., 2020) presumably by introducing real-world knowledge.",
      "startOffset" : 85,
      "endOffset" : 186
    }, {
      "referenceID" : 32,
      "context" : "Entity representations are known to enhance language models in mono-lingual settings (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Yamada et al., 2020) presumably by introducing real-world knowledge.",
      "startOffset" : 85,
      "endOffset" : 186
    }, {
      "referenceID" : 32,
      "context" : "To this end, we present a multilingual extension of LUKE (Yamada et al., 2020).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "We evaluate this approach with cross-lingual question answering (QA) datasets: XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al.",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : ", 2020) and MLQA (Lewis et al., 2020); (2) use the entity [MASK] token from the MEP task as a language-",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "We evaluate this approach with the RELX (Köksal and Özgür, 2020) and CoNLL NER (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) datasets.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "We also explore solving a multilingual zero-shot cloze prompt task (Liu et al., 2021) with the entity [MASK] token.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "Recent studies have shown that we can address various downstream tasks by querying a language model for blanks in prompts (Petroni et al., 2019; Cui et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "Recent studies have shown that we can address various downstream tasks by querying a language model for blanks in prompts (Petroni et al., 2019; Cui et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "We evaluate our approach with the mLAMA dataset (Kassner et al., 2021) in various languages and show that using the entity [MASK] token reduces language bias and elicits correct factual knowledge more likely than using only the word [MASK] token.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "To evaluate the effectiveness of entity representations for cross-lingual downstream tasks, we introduce a new multilingual language model based on a bidirectional transformer encoder: Multilingual LUKE (mLUKE), a multilingual extension of LUKE (Yamada et al., 2020).",
      "startOffset" : 245,
      "endOffset" : 266
    }, {
      "referenceID" : 28,
      "context" : "The model is trained with the masked language modeling (MLM) task (Vaswani et al., 2017) as well as the masked entity prediction (MEP) task.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "The word and entity embeddings are computed as the summation of the following three embeddings: token embeddings, type embeddings, and position embeddings (Devlin et al., 2019).",
      "startOffset" : 155,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "Our model configuration follows XLM-RoBERTa-base (Conneau et al., 2020a), a variant of BERT (Devlin et al.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : ", 2020a), a variant of BERT (Devlin et al., 2019) trained with CommonCrawl data from 100 languages, and the parameters in common (e.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "We optimize the model with a batch size of 2048 for 1M steps in total using AdamW (Loshchilov and Hutter, 2019) with warmup and linear decay of the learning rate.",
      "startOffset" : 82,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "We compare the primary model that we investigate, multilingual LUKE used with entity representations (mLUKE-E), against several baseline pretrained models and an ablation model based on word representations: mBERT (Devlin et al., 2019) is one of the earliest multilingual language models.",
      "startOffset" : 214,
      "endOffset" : 235
    }, {
      "referenceID" : 3,
      "context" : "XLM-R (Conneau et al., 2020a) is the model that mLUKE-E is built on.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "Since earlier studies (Liu et al., 2019; Lan et al., 2020) indicated longer pretraining would simply improve performance, we",
      "startOffset" : 22,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "Since earlier studies (Liu et al., 2019; Lan et al., 2020) indicated longer pretraining would simply improve performance, we",
      "startOffset" : 22,
      "endOffset" : 58
    }, {
      "referenceID" : 24,
      "context" : "1 dataset (Rajpurkar et al., 2016), and evaluate them with the two multilingual datasets:",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "We quantify the degree of alignment as performance on the contextualized word retrieval (CWR) task (Cao et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 33,
      "context" : "We fine-tune the models with the English KBP-37 dataset (Zhang and Wang, 2015) and evaluate the models with the RELX dataset (Köksal and Özgür, 2020), which is created by translating a subset of 502 sentences from KBP-37’s test set into four different languages.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "We fine-tune the models with the English KBP-37 dataset (Zhang and Wang, 2015) and evaluate the models with the RELX dataset (Köksal and Özgür, 2020), which is created by translating a subset of 502 sentences from KBP-37’s test set into four different languages.",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 32,
      "context" : "For mLUKE-E, we introduce two special entities, [HEAD] and [TAIL], to represent the head and tail entities (Yamada et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "To find out if the entity-based features are actually more languageindependent than word-based features, we evaluate the modularity (Fujinuma et al., 2019) of the features extracted for the RELX dataset.",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "In this section, we show that using the entity representations is effective in a cloze prompt task (Liu et al., 2021) with the mLAMA dataset (Kassner et al.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "We adopt the typed querying setting (Kassner et al., 2021), where a template has a set of candidate answer entities and the prediction becomes the one with the highest score assigned by the language model.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "The original XLM-R is only trained with the CommonCrawl corpus (Conneau et al., 2020a), text scraped from a wide variety of web pages, while mBERT and XLM-R + training are trained on Wikipedia.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "Multilingual pretrained language models have recently seen a surge of interest due to their effectiveness in cross-lingual transfer learning (Conneau and Lample, 2019; Liu et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 185
    }, {
      "referenceID" : 19,
      "context" : "Multilingual pretrained language models have recently seen a surge of interest due to their effectiveness in cross-lingual transfer learning (Conneau and Lample, 2019; Liu et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "A straightforward way to train such models is multilingual masked language modeling (mMLM) (Devlin et al., 2019; Conneau et al., 2020a), i.",
      "startOffset" : 91,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "A straightforward way to train such models is multilingual masked language modeling (mMLM) (Devlin et al., 2019; Conneau et al., 2020a), i.",
      "startOffset" : 91,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "mMLM exhibit a strong cross-lingual ability without any cross-lingual supervision (Karthikeyan K and Roth, 2020; Conneau et al., 2020b), several studies aim to develop better multilingual models with explicit cross-lingual supervision such as bilingual word dictionaries (Conneau et al.",
      "startOffset" : 82,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : ", 2020b), several studies aim to develop better multilingual models with explicit cross-lingual supervision such as bilingual word dictionaries (Conneau et al., 2020b) or parallel sentences (Conneau and Lample, 2019).",
      "startOffset" : 144,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "In this study, we build a multilingual pretrained language model on the basis of XLM-RoBERTa (Conneau et al., 2020a), trained with mMLM as well as the masked entity prediction (MEP) (Yamada et al.",
      "startOffset" : 93,
      "endOffset" : 116
    }, {
      "referenceID" : 32,
      "context" : ", 2020a), trained with mMLM as well as the masked entity prediction (MEP) (Yamada et al., 2020) with entity representations.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 34,
      "context" : "Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Févry et al., 2020; Yamada et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 251
    }, {
      "referenceID" : 22,
      "context" : "Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Févry et al., 2020; Yamada et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 251
    }, {
      "referenceID" : 29,
      "context" : "Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Févry et al., 2020; Yamada et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 251
    }, {
      "referenceID" : 31,
      "context" : "Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Févry et al., 2020; Yamada et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 251
    }, {
      "referenceID" : 9,
      "context" : "Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Févry et al., 2020; Yamada et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 251
    }, {
      "referenceID" : 32,
      "context" : "Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2021; Xiong et al., 2020; Févry et al., 2020; Yamada et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 251
    } ],
    "year" : 0,
    "abstractText" : "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages with entity representations and show the model consistently outperforms word-based pretrained models in various crosslingual transfer tasks. We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_74_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Detection of Adversarial Examples in NLP: Benchmark and Baseline via Robust Density Estimation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Adversarial examples in NLP refer to seemingly innocent texts that alter the model prediction to a desired output, yet remain imperceptible to humans. In recent years, adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in sentence classification tasks with increasingly smaller perturbation rate (Jin et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Ren et al., 2019). In the image domain, two main lines of research exist to counteract adversarial attacks : adversarial example detection and defense. The goal of detection is to discriminate an adversarial input from a normal input, whereas adversarial defense intends to predict the correct output of the adversarial input. While works defending these attacks have shown some progress in NLP (Zhou et al., 2021; Keller et al., 2021; Jones et al., 2020), only few efforts have been made in detecting them.\n1https://github.com/anoymous92874838/text-advdetection\nHowever, detecting adversarial examples in NLP may be as crucial as defending them by alerting the users to counteract them. In addition, models used for automation of tasks (e.g. review sentiment analysis, news headline classification, etc) are adopted to efficiently gain information about the true data-generating population (e.g. consumers, news media, etc), rather than the adversary. For such applications, attaining outputs of an adversarial input - whether correct or not - may turn out to be harmful for the system. Accordingly, the discard-rather-than-correct strategy which simply discards the detected adversarial input would be a good countermeasure. Moreover, being able to detect adversarial examples may be a step towards building a more robust defense model as the popular defense paradigm, adversarial training, usually suffers from degraded performance on normal inputs (Bao et al., 2021). With a competent detection system, the normal and adversarial inputs can be processed by two separate mechanisms as proposed by Zhou et al. (2019).\nExisting few works of detecting adversarial examples in NLP (Le et al., 2021; Pruthi et al., 2019) either focus on a single type of attack or is limited to character-level attacks. Le et al. detect a particular type of attack that prepends an identical phrase on all samples, making them inapplicable to various other attack methods, and Pruthi et al. target detecting adversrial misspellings. Aforementioned methods detect adversarial attacks that do not consider either semantics or grammaticality,\nwhich are two key constraints in order to be imperceptible (Morris et al., 2020a). As opposed to this, carefully crafted word-level adversarial attacks can maintain original semantics and remain unsuspicious to human inspectors. Additionally, the works lack uniformity in the experimented attack methods and the experimental settings (Section 2.2). To this end, we release a benchmark for adversarial example detection on four attack methods across four NLP models and three datasets. We also propose a simple but effective baseline that utilizes density estimation in the feature space as shown in Fig. 1.\nInspired by classic works in novelty detection (Bishop, 1994), which utilizes generative models to find anomalies, we fit a parametric density estimation model to the features obtained from a classification model (e.g. BERT) to yield likelihoods of each sample as shown by Fig. 2. However, simply fitting a parametric model suffers from curse of dimensionality characterized by (i) sparse data points and spurious features (ii) and rare outliers that hamper accurate estimation. To tackle these issues, we leverage classical techniques in statistical analysis, namely kernel PCA and Minimum Covariance Determinant, for robust density estimation (RDE).\nWe validate our method with existing works in NLP and find that our method can be used as a\ncompetitive baseline across all the tested attack methods without accessing validation sets of each attack method. Our contributions are two-fold:\n• We release a dataset for adversarial example detection on 4 attacks, 3 datasets, and 4 models and the source code for experimenting on various experimental protocols.\n• We propose a competitive baseline method that does not require validation sets of each attack method through robust parameter estimation, alleviating problems caused by curse of dimensionality.\n• Our method achieves best performance as of AUC on 21 out of 22 dataset-attack-model combinations and best performance as of TPR, F1 , AUC on 17 of them without any assumption on the attacks."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Adversarial Examples",
      "text" : "Given an input space X , a label space Y , a predictive model F : X → Y , and an oracle model F∗ : X → Y a successful adversarial example, xadv, of an input x ∈ X satisfies the following:\nF∗(x) = F(x) 6= F(xadv), Ci(x, xadv) = 1 for i ∈ {1, . . . , c}\n(1)\nwhere Ci is an indicator function for the i-th constraint between the perturbed text and the original text, which is 1 when the two texts are indistinguishable with respect to the constraint. The constraints vary from attack algorithms and is crucial for maintaining the original semantics while providing an adequate search space. For instance, Jin et al. (2020) ensure that the embedding of the two sentences have a cosine similarity larger than 0.5 using the Universal Sentence Encoder (Cer et al., 2018, USE)."
    }, {
      "heading" : "2.2 Detecting Adversarial Examples",
      "text" : "For the purpose of detecting adversarial examples, a dataset, D, consisting of clean samples (Dclean)\nand adversarial samples (Dadv) is required. However, how the dataset should be configured has rarely been discussed in detail and the exact implementation varies by works. Here we discuss two main configurations used in the literature. We denote the test set as Xt and the correctly classified test set as Xc ⊂ Xt. • Scenario 1 : Sample disjoint subsets S1, S2 ⊂ Xt.\nFor the correctly classified examples of S1, adversarial attacks are generated and the successful examples form Dadv. Dclean is formed from S2. • Scenario 2 : Sample subset S ⊂ Xt. For the\ncorrectly classified examples of S, adversarial attacks are generated and the successful examples form Dadv. Dclean is formed from S. Scenario 1 provides more flexibility in choosing the ratio between adversarial samples and clean samples, while in Scenario 2 this is determined by the attack success rate and task accuracy. For instance, an attack with low success rate will have a low adversarial-to-clean sample ratio. In addition, Scenario 2 consists of pairs of adversarial sample and its corresponding clean sample in addition to the incorrect clean samples. A more challenging scenario can be proposed by including failed attacked samples, which may be closer to the real world. Examples of failed and successful samples are provided in Table 1.\nA seminal work (Xu et al., 2018) on adversarial example detection in the image domain assumes the first scenario, whereas existing works in NLP (Le et al., 2021; Mozes et al., 2021) only experiment on the second scenario. Our benchmark framework provides the data and tools for experimenting on both. We provide experiment results on both scenarios."
    }, {
      "heading" : "3 Method",
      "text" : ""
    }, {
      "heading" : "3.1 Benchmark",
      "text" : "We generate adversarial examples on four models, four types of attacks, and three sentence classification datasets. Since some attacks (Garg and\nRamakrishnan, 2020) require hundreds of queries and inference of models per query, vast amount of time is required to create all the adversarial examples (e.g. up to 44 hours for 5,000 examples on the IMDB dataset using TF-adjusted attack). This renders on-the-fly generation and detection of adversarial examples extremely inefficient. Therefore, adversarial examples are created beforehand and sampled according to Section 2.2. Three sentence classification datasets (IMDB, AG-News, SST-2) are chosen to have diverse topics and length. See Table 2 for the summary and the number of generated samples.\nWe choose two non-transformer-based models (Word-CNN Kim (2014); LSTM Hochreiter and Schmidhuber (1997)) and two transformer-based models (RoBERTa Liu et al. (2019); BERT Devlin et al. (2018)) . Recently, numerous adversarial attacks have been proposed. We choose two widely known attacks called Textfooler (Jin et al., 2020, TF) and Probability Weighted Word Saliency (Ren et al., 2019, PWWS) and a recent approach using BERT to generate attacks called BAE (Garg and Ramakrishnan, 2020). Lastly, we also include a variant of TF called TF-adjusted (Morris et al., 2020a, TF-adj), which enforces a stronger similarity constraint to ensure imperceptibility to humans. All attacks are created using the TextAttack library (Morris et al., 2020b). See Appendix A.1 for the summary of attack methods and Appendix A.5 for a code snippet of using our benchmark."
    }, {
      "heading" : "3.2 Estimating Density and Parameters in Feature Space",
      "text" : "Earlier works in novelty detection (Bishop, 1994) have shown that generative models fitted on normal samples are capable of detecting unseen novel samples (e.g. adversarial samples). Since we can assume that the training samples, which were used to train the victim model of a particular task, are available to the victim party, we can similarly design a generative model that estimates input density. However, directly using the inputs is challeng-\ning as modeling the probability distribution of raw texts is non-trivial. To bypass this, we fit a parametric density estimation model in the feature space (i.e. penultimate layer of the classification model). Since a neural network learns to extract important features of the inputs to distinguish classes, the features can be regarded as representations of the raw inputs. For a pre-trained predictive model F , let z ∈ Z ⊂ RD denote the feature given by the feature extractorH : X → Z . Then the entire predictive model can be written as the composition of H and a linear classifier.\nGiven a generative model pθ with mean and covariance as parameters θ = (µ,Σ), we can use the features of the training samples (Xtrain) to estimate the parameters. Then, novel adversarial samples lying in the unobserved feature space are likely to be assigned a low probability, because the generative model only used the normal samples for parameter fitting. For simplicity, we assume the distributions of the feature z follow a multivariate Gaussian, and thus we model the class conditional probability as pθ(z|y = k) ∼ N(µk,Σk) ∝ exp{−(z−µk)TΣ−1k (z−µk)}, where y indicates the class of a given task. Then, the maximum likelihood estimate (MLE) is given by the sample mean µ̃MLE = 1N ∑N i=1 zi and sample covariance Σ̃MLE = 1 N ∑N i=1(zi − µ̃MLE)(zi − µ̃MLE)T .\nHowever, accurate estimation of the parameters is difficult with finite amount of samples especially in high dimensions (D = 768 for transformerbased models) due to curse of dimensionality, thereby (i) leading to sparse data points and spurious features (ii) and occasional outliers that influ-\nence the parameter estimates. In Figure 3, we empirically show that the covariance matrices (blue) of BERT and RoBERTa across all models across all datasets are ill-conditioned, demonstrated by the high maximum eigenvalues and extremely small minimum eigenvalues (≈ 10−12). Due to this, the precision matrix is abnormally inflated in certain dimensions and prone to numerical errors during inversion. More analysis regarding the upperbound of this error is provided in Appendix A.2.\nIn addition, although we have assumed a Gaussian distribution for convenience, the unknown true distribution may be a more general elliptical distribution with thicker tails. This is observed empirically in Figure 4 by visualizing the features into two dimensions by dimensionality reduction. Outliers that are far from the modes of both classes (indicated by color) are present: those that are completely misplaced occasionally exist, while subtle outliers that deviate from the Gaussian distribution assumption are common, which influences the MLE estimation. Thus, to accurately estimate the Gaussian parameters, these outliers should be taken into account. In the following subsection, we tackle these issues through well-known classical techniques."
    }, {
      "heading" : "3.3 RDE using kPCA and Minimum Covariance Determinant",
      "text" : "To address the first issue, we first use kernel PCA (Schölkopf et al., 1998, kPCA) to select top P orthogonal basis that best explains the variance of the data, thereby reducing redundant features. GivenN centered samples Ztrain ∈ RN×D = [z1, . . . , zN ], a mapping function φ : RD → RD′ , and its mapping applied to each sample Φ(Ztrain) ∈ RN×D′ ,\nkPCA projects the data points to the eigenvectors with the P largest eigenvalues of the covariance Φ(Ztrain)TΦ(Ztrain)2. Intuitively, this retains the most meaningful feature dimensions, which explains the data the most, while reducing spurious features and improve stability of inversion by increasing the condition number as shown in Figure 3. By leveraging non-linear φ, we are able to find meaningful non-linear signals in the features as opposed to standard PCA. We use the radial basis function as our kernel. Comparison of performance with standard PCA is provided in Appendix Table A.3. For a complete derivation, please refer to Schölkopf et al. (1997).\nHowever, this does not remove sample-level outliers as shown in Figure 4. Since we have assumed a Gaussian distribution, \"peeling off\" outliers may be favorable for parameter estimation. A principled way of removing outliers for parameter estimation has been an important research area in multivariate statistics and various methods have been developed for robust covariance estimation (Friedman et al., 2008; Ledoit and Wolf, 2004). Among them, Minimum Covariance Determinant (Rousseeuw, 1984, MCD) finds a subset of h ≤ N samples that minimizes the determinant of Σ.3 As the determinant is proportional to the differential entropy of a Gaussian up to a logarithm (shown in Appendix A.3), this results in a robust covariance estimation consisting of centered data points rather than outliers. For a review, see Hubert et al. (2018). Qualitatively, we observe in Figure 4 that for MLE estimates both classes have their means yanked towards the outliers and the contours are disoriented (Blue). MCD estimates (Red) focus on the high density clusters, which leads to higher performance as will be shown in the experiments.\nIn summary, we retain informative features by applying kPCA and obtain robust covariance estiamte by using MCD on the train set. Using the estimated robust parameters, we can evaluate the likelihood of a test sample. We treat those with low likelihood as novel (adversarial) samples. Our algorithm is shown in Algorithm 1 in the Appendix. We empirically validate the effectiveness of two techniques and discuss the effect of hyper-parameter P\n2For simplicity, we assume Φ(Ztrain) is centered. When this assumption does not hold, slightly modified approach is taken. See Appendix B of Schölkopf et al. (1998) for details.\n3Although the possible number of subsets is infeasibly large, Rousseeuw and Driessen (1999) propose an iterative method that converges relatively fast for ≈ 4000 samples with 100 dimensions.\nand h in the following sections."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "We experiment on the three datasets (IMDB, AGNews, SST-2) and four attack methods described in Section 3.1. Our experiment is mainly based on BERT and RoBERTa as they are widely used competent models for various tasks. Since SST-2 only has 1.8K test samples, TF-adjusted attack was unable to create an adequate number of successful adversarial samples (e.g. 80 samples out of 1.7K). Omitting experiments for these, there are 22 combinations of dataset-attack-model in total."
    }, {
      "heading" : "4.2 Compared Methods",
      "text" : "We compare our robust density estimation method (RDE) with a recently proposed detection method in NLP called FGWS (Mozes et al., 2021) which is a word frequency-based method that assumes that rare words appear more often in adversarial samples. We also verify whether Perplexity (PPL) computed by a language model (GPT-2, Radford et al. 2019) is able to distinguish normal and adversarial samples as PPL is often used to compare the fluency of the two samples. FGWS implicitly models the input density via word frequencies, while GPT-2 explicitly computes the conditional probability via an auto-regressive tasks. In addition, we adopt Lee et al. (2018) denoted as MLE, which is a density estimation method from the image domain. For further details, see Section 5. We compare MLE with two variants of our method:\n• MLE: Maximum likelihood estimate of parameters using the raw 768-dimensional features is used for density estimation. • RDE(-MCD) : This is a variant of RDE, in which only kPCA is applied to the features without MCD. The results of applying standard PCA instead of kPCA are reported in Table A.3 of Appendix. • RDE : After applying kPCA, MCD estimate is used. This is the final proposed robust density estimation incorporating both kPCA and MCD."
    }, {
      "heading" : "4.3 Evaluation Metric and Protocol",
      "text" : "Following Xu et al. (2018), we report three widely used metrics in adversarial example detection : (1) True positive rate (TPR) is the fraction of true adversarial samples out of predicted adversarial samples. (2) F1-score (f1) measures the harmonic mean of\nprecision and recall. Since all compared methods are threshold-based methods, we report TPR at a fixed false positive rate (FPR). (3) Area under ROC (AUC) measures the area under TPR vs. FPR curve. For all three metrics, higher denotes better performance.\nNote that whereas AUC considers performance on various FPR’s, TPR and F1 is dependent on one particular FPR. In all our experiments, we fixed FPR= 0.1, which means 10% of normal samples are predicted to be adversarial samples. This threshold should be chosen depending on the context (i.e. the degree of safety-criticalness). We believe this standard should be elevated as more works are proposed in the future. For IMDB and AG-News, 30% of the test set is held out as validation set, and we subsample out of the test set as described in Section 2.2. For quantitative analysis, we report the mean and its standard error of three repetitions of random seeds for test/validation split and subsampled samples."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "We choose the feature z to be the output of the last attention layer (before Dropout and fully connected layer) for BERT and RoBERTa. RDE has two main hyper-parameters, namely the number of retained\ndimensions P of kPCA and the support fraction h of MCD. We fix P = 100 for all experiments as we observe the performance is not sensitive to P . For h, we use the default value proposed in the algorithm, which is N+P+12N . We study the effect of h in Section 4.6. All models are pre-trained models provided by TextAttack and both kPCA and MCD are implemented using scikit-learn (Pedregosa et al., 2011). We use the radial basis function as our kernel. The time required to estimate the parameters of our method is approximately around 25 seconds. For more details, see Appendix A.4.\nFor FGWS, we use the official implementation4\nand use the held-out validation set of each attack to tune the threshold for word frequency δ as done in the original work. For PPL, we use the HuggingFace (Wolf et al., 2020) implementation of GPT-2 (Radford et al., 2019)."
    }, {
      "heading" : "4.5 Results",
      "text" : "Main Results Here we present our main experiments done on Scenario 1. Table 3 demonstrates the results on all datasets and attacks. The highest means out of the five methods are written in bold. Out of the 22 com-\n4https://github.com/maximilianmozes/fgws\nbinations of dataset-attack-model, RDE achieves the best performance on 21 of them on AUC and 17 of them for all three metrics, which shows the competitiveness of our simple method. The motivation of our method is verified by the large margin of improvement from MLE in almost all cases. Using MCD estimation also further improves performance except in the few cases of AG-News. Large language models (PPL) are able to distinguish between adversarial samples and normal samples in expectation as shown by the higher-than-random metric, but the performance is inadequate to be used as a detector. FGWS generally has a higher performance compared to PPL, but is inferior to MLE in most cases. Note the degradation of performance in FGWS for BAE and TF-adj, which are more subtle attacks with stronger constraint, as FGWS relies on use of rare words. This trend is not observed in feature density-based methods (MLE and RDE).\nFGWS outperforms ours on TPR and F1 in five combinations out of 22, but our method has higher AUC on four of them. Interestingly, all the five results are from PWWS attacks, which indicates that our method is relatively susceptible to PWWS. Nonetheless, AUC still remains fairly high: On IMDB and AG-News, the AUC’s are all over 0.9. On the other hand, all methods have degraded performance on SST-2, which may be due to shorter sentence lengths. Some examples of ROC curves are presented in Appendix A.6. Improving detection rate in SST-2 is left as a future work. Comparing with BERT, RoBERTa generally has higher performance on IMDB and SST-2, yet BERT outperforms RoBERTa on AG-News on some attacks. Motivating More Realistic Scenarios In this section, we provide some preliminary results for other more realistic scenarios: (i) Including failed attacks (ii) Imbalanced ratio of clean and adversarial samples. In previous experiments, all failed adversarial attacks were discarded. However, in reality an ad-\nversary will probably have no access to the victim model so some attacks will indeed fail to fool the model. While failed adversarial attacks do not pose threat to the task accuracy of the model, it nevertheless may be harmful if the victim wishes to gain information about a certain population by aggregating data such as sentiment in consumer review about a movie. In addition, as active research in attacks have been made in the past few years, more subtle attacks that are imperceptible to humans naturally have lower attack success ratio (e.g. BAE).\nFigure 5 provides the detection results of RDE and MLE when distinguishing between normal samples and (failed and successful) adversarial attempts by comparing the AUC’s. As an upper bound, we provide the performance of RDE on the original scenario without failed adversarial examples in red. As the first two attacks (TF and PWWS) achieve nearly 100% success rate, only few failed adversarial samples are added. Accordingly, the performances for the two attacks show little difference. However, in more subtle attacks (BAE and TF-adj) the performance drastically drops due to the increased failed adversarial samples, yet RDE outperforms MLE by a considerable margin in most cases. We end on this topic by noting that more comprehensive analysis is called for, because in some cases failed adversarial attempts are (nearly) identical to clean samples. So an attack method with low detection rate does not necessarily imply a crafty attack method in this scenario.\nIn Appendix Table A.4, we provide the results for Scenario 2 described in Section 2.2. The general trend among detection methods and attack methods is similar to Table 3. As noted earlier, for Scenario 2 the ratio of adversarial to clean samples will be low if the attack success rate is low. For instance, in IMDB-(TF-adj)-BERT, the ratio of adversarial to clean samples is around 1:9. Whereas both AUC and TPR are not strongly affected due to the characteristic of the metrics, F1 drastically drops. For\ninstance, for IMDB-(TF-adj)-BERT, RDE achieves 73.7% (as opposed to 95.0% of Scenario 1). On the same set, FGWS achieves 60.1% and MLE achieves 67.6%."
    }, {
      "heading" : "4.6 Discussion",
      "text" : "Hyper-parameter Analysis Although the two main hyper-parameters, support fraction (h) of MCD and the dimension (P ), were fixed in our experiments, they can be fine-tuned in a separate validation set for optimal performance. We show in Figure 6 the performance of our method on various ranges of h and P on the validation set of IMDBTF-BERT combination. We set P = 100 and h to the default value of the algorithm when tuning for the other parameter. We confirm that RDE has a relatively robust performance across wide ranges of values and improves upon the naive version as shown by the \"MLE\" and \"None\", in which MLE estimation and no kPCA were used, respectively. Qualitative Analysis on Support Fraction The support fraction controls the ratio of original samples to be retained by the MCD estimator, thereby controlling the volume of the contour as shown in Figure 7. Large values of h retain more of the deviating samples and lead to a wider probability contour. We empirically demonstrated in our experiment that using all samples for parameter estimation may be detrimental for adversarial sample detection."
    }, {
      "heading" : "5 Related Works",
      "text" : "Detection of adversarial examples is a wellexplored field in the image domain. Earlier works have tackled in various ways such as input transformation (Xu et al., 2018), statistical analysis (Grosse et al., 2017), or training a separate binary classifier (Metzen et al., 2017). However, Carlini and Wagner (2017) has shown that an adversary with par-\ntial knowledge of the detector can easily nullify it. Meanwhile, early works in novelty detection have shown that a generative model can detect anomaly samples (Bishop, 1994). Following this line of research, Lee et al. (2018) have proposed a method to detect out-of-distribution samples by using features of a neural network for maximum likelihood estimation.\nIn the NLP domain, few efforts have been made in detecting word-level adversarial examples. Zhou et al. (2019, DISP) utilize a detector trained on adversarial samples for a joint detect-defense system. FGWS (Mozes et al., 2021) outperforms DISP in detection by building on the observation that attacked samples are composed of rare words. FGWS experiements on 2 proposed attack methods. Le et al. (2021) tackle a particular attack method called UniTrigger (Wallace et al., 2019), which pre-pends or appends an identical phrase in all sentences. While the performance is impressive, applying this method to other attacks requires significant adjustment due to the distinct characteristics of UniTrigger. Meanwhile, Pruthi et al. (2019) tackle character-level adversarial examples and compare with spell correctors. Our work is the first to extensively demonstrate experimental results for 4 attack methods including recent attack methods on 3 datasets and propose a competitive baseline."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose a general framework and benchmark for adversarial example detection in NLP. Along with it, we propose a competitive baseline that does not require training nor validation for each attack method. In the future, an adversary with partial or full knowledge should be considered to motivate further research for stronger detection methods."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Success Rate of Attack Methods and Other Statistics\nHere we briefly describe each attack method and provide some statistics about the attack results. For Table A.1, word transformation method indicates how candidate replacement words are created. Word importance ranking denotes how the ordering of which word to attack is chosen. For constraints, only those related to embedding was listed and the numbers in parenthesis denote the threshold. Higher threshold signifies stronger constraint. For more details, we refer the readers to Morris et al. (2020b). Table A.2 summarizes the attack results on three dataset for BERT. Results for other models can be found in the released dataset.\nA.2 Potential Errors of Parameter Estimation\nAccurate estimation of the parameters is difficult with finite amount of samples especially in high dimensions. Here we demonstrate this through a toy example and derive its relationship with the Mahalanobis distance function, which is proportional to the likelihood. Figure A.1 shows that the MLE error remains high even when 214 samples are used to find the parameters of a noise-free normal distribution for both µ and Σ. This, in turn, leads to an inevitably error-prone µ̃ = µ− µ and ∆̃ = z − µ̃. Moreover, the error is amplified when computing the Mahalanobis distance due to the ill-conditioned Σ̃ with very small eigenvalues, which is observed empirically in all datasets and models (Figure 3) possibly due to the redundant features. The (relative) condition number of the Mahalanobis distance function g(∆) - relative change in the output given\na relative change in the inputs - is bounded by the inverse of the smallest eigenvalue of Σ̃−1.\nκg(∆) = || ∂g∂∆ ||\n||g(∆)||/||∆||\n= ||∆|| ||g(∆)|| ||2Σ −1∆|| ≤ ||∆||||g(∆)||2||Σ −1||||∆||\n(2)\nwhere the first equality follows from the definition of condition number and differentiability of g and C∆. The last equality follows from the Caucy-Schwarz Inequality. The matrix norm induced by the L2 norm is given by the largest singular value (largest eigenvalue for a positive definite square matrix). Given the eigenspectrum of Σ as λmax ≥ · · · ≥ λmin, the eigenspectrum of Σ−1 is given by the reciprocal of that of Σ. Thus, ||Σ−1|| is equal to inverse of the minimum eigenvalue of Σ and the last equality can be further decomposed into\nκg(∆) ≤ ||∆|| ||g(∆)||2||Σ −1||||∆||\n≤ C∆ 1\nλmin\n(3)\nwhere C∆ is a constant for a given ∆. This means that when the smallest eigenvalue is in the scale of 10−12, even a estimation error of scale 10−3 on µ may be amplified by at most by a scale of 109. This leads to a serious problem in density estimation of z.\nA.3 More details on MCD\nWe explain some of the properties of the determinant of the covariance matrix. First, the determinant is directly related to the differential entropy of the Gaussian distribution. For a D-dimensional multivariate Gaussian variable X and its probability density function f , the differential entropy is given by\nH(X) = − ∫ X f(x) log f(x)dx\n= 1\n2 log((2πe)Ddet(Σ)\n∝ det(Σ)\n(4)\nIn addition, the determinant is also proportional to the volume of the ellipsoid for some k, {z ∈ RD : (z − µ)TΣ−1(z − µ) = k2}. We refer the\nAttacks Post-AttackAccuracy Attack Success Rate Average\nNum. Queries IMDB (91.9%)\nTF 0.6% 99% 558 PWWS 3% 97% 1681 BAE 34% 64% 455 TF-adj 84.2% 11% 298 AG-News (94.2%) TF 18% 81% 334 PWWS 41% 57% 362 BAE 82% 14% 122 TF-adj 91% 5% 56 SST-2 (92.43%) TF 4% 96% 91 PWWS 12% 87% 143 BAE 37% 61% 60 TF-adj 89% 5% 25\nTable A.2: Summary of attack results for BERT on three datasets. Original accuracy of each dataset is written in parenthesis next to the dataset.\nreaders to Section 7.5 of Anderson (1962) for the proof. This explain why the MCD estimate forms a much narrower probability contour than MLE as shown in Fig. 4.\nA.4 Implementation Details To meet the memory constraint of computing the kernel matrix, we sample a subset of Xtrain (8,000 samples) for all experiments. All models are pretrained models provided by TextAttack and both kPCA and MCD are implemented using scikitlearn (Pedregosa et al., 2011). We use the radial basis function as our kernel.\nFor subsampling generated attacks described in Section 2.2, we set the maximum number of adversarial samples for each dataset. For IMDB and AG-News, the maximum is set to 2000 and for SST-2 this is set to 1000. Then, the number of target samples (i.e. ||S|| or ||S1||) is initialized to the maximum number divided by adversarial success ratio and task accuracy. Target sample is decremented until ratio between clean and adversarial samples can roughly be 5:5. Algorithm of RDE and MLE is provided in Algorithm 1.\nAlgorithm 1: RDE and MLE Input: Xtrain,Ytrain, D = {Dadv,Dclean} Input: Feature ExtractorH Output: Likelihood L\n1 Ztrain =H(Xtrain) 2 if MLE then 3 for c in Class do 4 µ̃c =\n1 Nc ∑Nc i∈Yc zi\n5 Σ̃c = 1 Nc ∑Nc i∈Yc(zi − µ̃)(zi − µ̃)T\n6 else if RDE then 7 Z = kPCA(Z) 8 for c in Class do 9 µ̃c, Σ̃c = MCD(Zc)\n10 L = [] 11 for x in D do 12 z = H(x) 13 ŷ = argmaxk F(x)k 14 if RDE then 15 z = kPCA(z)\n16 L.append(N (z|µ̃ŷ, Σ̃ŷ))\nA.5 Benchmark Usage Example\nfrom AttackLoader import Attackloader\n#Set seed, scenario, model type, #attack type, dataset, etc. loader = AttackLoader(...)\n#Split test and validation set loader.split_csv_to_testval()\n# Subsample from testset according to chosen scenario sampled, _ = loader.get_attack_from_csv(..)\n\"\"\" Apply detection method \"\"\"\nA.6 ROC Curve Examples Below (Fig. A.2) we provide Receiver Operating Characteristic (ROC) curves of ROBERTA on two attacks. For all plots, samples from the first seed are used.\nA.7 Comparison with PCA In all our experiments, we used a radial basis function for kPCA. This allows finding non-linear patterns in the feature space. When the linear kernel is used, kPCA is equivalent to ordinary PCA. We demonstrate that exploiting non-linearity preserve much more meaningful information by comparing the detection performance in the IMDB dataset (Table A.3).\nA.8 Experiment results on Scenario 2 Here we provide experimental results on Scenario 2. Although pairs of samples are included in the dataset, the general trend is similar to that of Scenario 1. For attack methods with low success rate, the adversarial to clean sample ratio is low, which affects the F1-score."
    } ],
    "references" : [ {
      "title" : "An introduction to multivariate statistical analysis",
      "author" : [ "Theodore Wilbur Anderson." ],
      "venue" : "Technical report, Wiley New York.",
      "citeRegEx" : "Anderson.,? 1962",
      "shortCiteRegEx" : "Anderson.",
      "year" : 1962
    }, {
      "title" : "Defending pre-trained language models from adversarial word substitutions without performance sacrifice",
      "author" : [ "Rongzhou Bao", "Jiayi Wang", "Hai Zhao." ],
      "venue" : "arXiv preprint arXiv:2105.14553.",
      "citeRegEx" : "Bao et al\\.,? 2021",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2021
    }, {
      "title" : "Novelty detection and neural network validation",
      "author" : [ "C.M. Bishop." ],
      "venue" : "IEEE Proceedings - Vision, Image and Signal Processing, 141:217–222(5).",
      "citeRegEx" : "Bishop.,? 1994",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1994
    }, {
      "title" : "Adversarial examples are not easily detected: Bypassing ten detection methods",
      "author" : [ "Nicholas Carlini", "David Wagner." ],
      "venue" : "Proceedings of the 10th ACM workshop on artificial intelligence and security, pages 3–14.",
      "citeRegEx" : "Carlini and Wagner.,? 2017",
      "shortCiteRegEx" : "Carlini and Wagner.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani." ],
      "venue" : "Biostatistics, 9(3):432–441.",
      "citeRegEx" : "Friedman et al\\.,? 2008",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2008
    }, {
      "title" : "Bae: Bert-based adversarial examples for text classification",
      "author" : [ "Siddhant Garg", "Goutham Ramakrishnan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174–6181.",
      "citeRegEx" : "Garg and Ramakrishnan.,? 2020",
      "shortCiteRegEx" : "Garg and Ramakrishnan.",
      "year" : 2020
    }, {
      "title" : "On the (statistical) detection of adversarial examples",
      "author" : [ "Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel." ],
      "venue" : "arXiv preprint arXiv:1702.06280.",
      "citeRegEx" : "Grosse et al\\.,? 2017",
      "shortCiteRegEx" : "Grosse et al\\.",
      "year" : 2017
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Minimum covariance determinant and extensions",
      "author" : [ "Mia Hubert", "Michiel Debruyne", "Peter J Rousseeuw." ],
      "venue" : "Wiley Interdisciplinary Reviews: Computational Statistics, 10(3):e1421.",
      "citeRegEx" : "Hubert et al\\.,? 2018",
      "shortCiteRegEx" : "Hubert et al\\.",
      "year" : 2018
    }, {
      "title" : "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Robust encodings: A framework for combating adversarial typos",
      "author" : [ "Erik Jones", "Robin Jia", "Aditi Raghunathan", "Percy Liang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2752–2765.",
      "citeRegEx" : "Jones et al\\.,? 2020",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT-defense: A probabilistic model based on BERT to combat cognitively inspired orthographic adversarial attacks",
      "author" : [ "Yannik Keller", "Jan Mackensen", "Steffen Eger." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
      "citeRegEx" : "Keller et al\\.,? 2021",
      "shortCiteRegEx" : "Keller et al\\.",
      "year" : 2021
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "CoRR, abs/1408.5882.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "A sweet rabbit hole by darcy: Using honeypots to detect universal trigger’s adversarial attacks",
      "author" : [ "Thai Le", "Noseong Park", "Dongwon Lee." ],
      "venue" : "59th Annual Meeting of the Association for Comp. Linguistics (ACL).",
      "citeRegEx" : "Le et al\\.,? 2021",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2021
    }, {
      "title" : "A wellconditioned estimator for large-dimensional covariance matrices",
      "author" : [ "Olivier Ledoit", "Michael Wolf." ],
      "venue" : "Journal of multivariate analysis, 88(2):365–411.",
      "citeRegEx" : "Ledoit and Wolf.,? 2004",
      "shortCiteRegEx" : "Ledoit and Wolf.",
      "year" : 2004
    }, {
      "title" : "A simple unified framework for detecting outof-distribution samples and adversarial attacks",
      "author" : [ "Kimin Lee", "Kibok Lee", "Honglak Lee", "Jinwoo Shin." ],
      "venue" : "Advances in neural information processing systems, 31.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert-attack: Adversarial attack against bert using bert",
      "author" : [ "Linyang Li", "Ruotian Ma", "Qipeng Guo", "Xiangyang Xue", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6193–6202.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "On detecting adversarial perturbations",
      "author" : [ "Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Metzen et al\\.,? 2017",
      "shortCiteRegEx" : "Metzen et al\\.",
      "year" : 2017
    }, {
      "title" : "Reevaluating adversarial examples in natural language",
      "author" : [ "John Morris", "Eli Lifland", "Jack Lanchantin", "Yangfeng Ji", "Yanjun Qi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 3829–3839.",
      "citeRegEx" : "Morris et al\\.,? 2020a",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2020
    }, {
      "title" : "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
      "author" : [ "John Morris", "Eli Lifland", "Jin Yong Yoo", "Jake Grigsby", "Di Jin", "Yanjun Qi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Morris et al\\.,? 2020b",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2020
    }, {
      "title" : "Frequency-guided word substitutions for detecting textual adversarial examples",
      "author" : [ "Maximilian Mozes", "Pontus Stenetorp", "Bennett Kleinberg", "Lewis Griffin." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Compu-",
      "citeRegEx" : "Mozes et al\\.,? 2021",
      "shortCiteRegEx" : "Mozes et al\\.",
      "year" : 2021
    }, {
      "title" : "Counter-fitting word vectors to linguistic constraints",
      "author" : [ "Nikola Mrkšić", "Diarmuid O Séaghdha", "Blaise Thomson", "Milica Gašić", "Lina Rojas-Barahona", "PeiHao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Mrkšić et al\\.,? 2016",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2016
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Combating adversarial misspellings with robust word recognition",
      "author" : [ "Danish Pruthi", "Bhuwan Dhingra", "Zachary C Lipton." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5582–5591.",
      "citeRegEx" : "Pruthi et al\\.,? 2019",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog,",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating natural language adversarial examples through probability weighted word saliency",
      "author" : [ "Shuhuai Ren", "Yihe Deng", "Kun He", "Wanxiang Che." ],
      "venue" : "Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1085–",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Least median of squares regression",
      "author" : [ "Peter J Rousseeuw." ],
      "venue" : "Journal of the American statistical association, 79(388):871–880.",
      "citeRegEx" : "Rousseeuw.,? 1984",
      "shortCiteRegEx" : "Rousseeuw.",
      "year" : 1984
    }, {
      "title" : "A fast algorithm for the minimum covariance determinant estimator",
      "author" : [ "Peter J Rousseeuw", "Katrien Van Driessen." ],
      "venue" : "Technometrics, 41(3):212–223.",
      "citeRegEx" : "Rousseeuw and Driessen.,? 1999",
      "shortCiteRegEx" : "Rousseeuw and Driessen.",
      "year" : 1999
    }, {
      "title" : "Kernel principal component analysis",
      "author" : [ "Bernhard Schölkopf", "Alexander Smola", "KlausRobert Müller." ],
      "venue" : "International conference on artificial neural networks, pages 583–588. Springer.",
      "citeRegEx" : "Schölkopf et al\\.,? 1997",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1997
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "Bernhard Schölkopf", "Alexander Smola", "KlausRobert Müller." ],
      "venue" : "Neural computation, 10(5):1299–1319.",
      "citeRegEx" : "Schölkopf et al\\.,? 1998",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1998
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing nlp",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Feature squeezing: Detecting adversarial examples in deep neural networks",
      "author" : [ "Weilin Xu", "David Evans", "Yanjun Qi." ],
      "venue" : "Network and Distributed Systems Security Symposium.",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in neural information processing systems, 28:649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Defense against synonym substitution-based adversarial attacks via dirichlet neighborhood ensemble",
      "author" : [ "Yi Zhou", "Xiaoqing Zheng", "Cho-Jui Hsieh", "Kai-Wei Chang", "Xuan-Jing Huang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Associa-",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to discriminate perturbations for blocking adversarial attacks in text classification",
      "author" : [ "Yichao Zhou", "Jyun-Yu Jiang", "Kai-Wei Chang", "Wei Wang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "2020b). Table A.2 summarizes the attack results on three dataset for BERT. Results for other models can be found in the released dataset",
      "author" : [ "Morris" ],
      "venue" : null,
      "citeRegEx" : "Morris,? \\Q2020\\E",
      "shortCiteRegEx" : "Morris",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In recent years, adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in sentence classification tasks with increasingly smaller perturbation rate (Jin et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Ren et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 297
    }, {
      "referenceID" : 17,
      "context" : "In recent years, adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in sentence classification tasks with increasingly smaller perturbation rate (Jin et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Ren et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 297
    }, {
      "referenceID" : 6,
      "context" : "In recent years, adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in sentence classification tasks with increasingly smaller perturbation rate (Jin et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Ren et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 297
    }, {
      "referenceID" : 28,
      "context" : "In recent years, adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in sentence classification tasks with increasingly smaller perturbation rate (Jin et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Ren et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 297
    }, {
      "referenceID" : 38,
      "context" : "While works defending these attacks have shown some progress in NLP (Zhou et al., 2021; Keller et al., 2021; Jones et al., 2020), only few efforts have been made in detecting them.",
      "startOffset" : 68,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "While works defending these attacks have shown some progress in NLP (Zhou et al., 2021; Keller et al., 2021; Jones et al., 2020), only few efforts have been made in detecting them.",
      "startOffset" : 68,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "While works defending these attacks have shown some progress in NLP (Zhou et al., 2021; Keller et al., 2021; Jones et al., 2020), only few efforts have been made in detecting them.",
      "startOffset" : 68,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Moreover, being able to detect adversarial examples may be a step towards building a more robust defense model as the popular defense paradigm, adversarial training, usually suffers from degraded performance on normal inputs (Bao et al., 2021).",
      "startOffset" : 225,
      "endOffset" : 243
    }, {
      "referenceID" : 14,
      "context" : "Existing few works of detecting adversarial examples in NLP (Le et al., 2021; Pruthi et al., 2019) either focus on a single type of attack or is limited to character-level attacks.",
      "startOffset" : 60,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "Existing few works of detecting adversarial examples in NLP (Le et al., 2021; Pruthi et al., 2019) either focus on a single type of attack or is limited to character-level attacks.",
      "startOffset" : 60,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "which are two key constraints in order to be imperceptible (Morris et al., 2020a).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "Inspired by classic works in novelty detection (Bishop, 1994), which utilizes generative models to find anomalies, we fit a parametric density estimation model to the features obtained from a classification model (e.",
      "startOffset" : 47,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "IMDB(Maas et al., 2011) movie review sentiment classification 2 161 25K / 10K 0 / NA",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 37,
      "context" : "AG-News(Zhang et al., 2015) news topic classification 4 44 7.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 33,
      "context" : "6K 0 / NA SST-2(Socher et al., 2013) sentiment classification 2 16 1.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 36,
      "context" : "A seminal work (Xu et al., 2018) on adversarial example detection in the image domain assumes the first scenario, whereas existing works in NLP (Le et al.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : ", 2018) on adversarial example detection in the image domain assumes the first scenario, whereas existing works in NLP (Le et al., 2021; Mozes et al., 2021) only experiment on the second scenario.",
      "startOffset" : 119,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : ", 2018) on adversarial example detection in the image domain assumes the first scenario, whereas existing works in NLP (Le et al., 2021; Mozes et al., 2021) only experiment on the second scenario.",
      "startOffset" : 119,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "Since some attacks (Garg and Ramakrishnan, 2020) require hundreds of queries and inference of models per query, vast amount of time is required to create all the adversarial examples (e.",
      "startOffset" : 19,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : ", 2019, PWWS) and a recent approach using BERT to generate attacks called BAE (Garg and Ramakrishnan, 2020).",
      "startOffset" : 78,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : "All attacks are created using the TextAttack library (Morris et al., 2020b).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "Earlier works in novelty detection (Bishop, 1994) have shown that generative models fitted on normal samples are capable of detecting unseen novel samples (e.",
      "startOffset" : 35,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "A principled way of removing outliers for parameter estimation has been an important research area in multivariate statistics and various methods have been developed for robust covariance estimation (Friedman et al., 2008; Ledoit and Wolf, 2004).",
      "startOffset" : 199,
      "endOffset" : 245
    }, {
      "referenceID" : 15,
      "context" : "A principled way of removing outliers for parameter estimation has been an important research area in multivariate statistics and various methods have been developed for robust covariance estimation (Friedman et al., 2008; Ledoit and Wolf, 2004).",
      "startOffset" : 199,
      "endOffset" : 245
    }, {
      "referenceID" : 23,
      "context" : "We compare our robust density estimation method (RDE) with a recently proposed detection method in NLP called FGWS (Mozes et al., 2021) which is a word frequency-based method that assumes that rare words appear more often in adversarial samples.",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 25,
      "context" : "All models are pre-trained models provided by TextAttack and both kPCA and MCD are implemented using scikit-learn (Pedregosa et al., 2011).",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : "Earlier works have tackled in various ways such as input transformation (Xu et al., 2018), statistical analysis (Grosse et al.",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : ", 2018), statistical analysis (Grosse et al., 2017), or training a separate binary classifier (Metzen et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 20,
      "context" : ", 2017), or training a separate binary classifier (Metzen et al., 2017).",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "Meanwhile, early works in novelty detection have shown that a generative model can detect anomaly samples (Bishop, 1994).",
      "startOffset" : 106,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "FGWS (Mozes et al., 2021) outperforms DISP in detection by building on the observation that attacked samples are composed of rare words.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 34,
      "context" : "(2021) tackle a particular attack method called UniTrigger (Wallace et al., 2019), which pre-pends or appends an identical phrase in all sentences.",
      "startOffset" : 59,
      "endOffset" : 81
    } ],
    "year" : 0,
    "abstractText" : "Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a counter measure, adversarial defense has been explored, but relatively little efforts have been made to detect adversarial examples. However, detecting adversarial examples in NLP may be crucial for automated task (e.g. review sentiment analysis) that wishes to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on three datasets and four NLP models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest AUC on 21 out of 22 dataset-attackmodel combinations.1",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_133_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks (Devlin et al., 2018; Raffel et al., 2019; Brown et al., 2020; Radford et al., 2021). Such large PLMs can learn a new task with a few examples or generalize to a new task without fine-tuning on any training examples, i.e., few-shot and zero-shot learning (Brown et al., 2020; Radford et al., 2021; Tsimpoukelli et al., 2021). Few-shot learning overcomes the challenges of data-hungry supervised learning, where collecting human-labeled data is costly and\nquestion: What position is this man playing? answer: <text_1>\nVQA\nCaptioning\nInput image Input text\n<text_1> pitcher Target text\nslow. However, recent few-shot models such as GPT3 (Brown et al., 2020), Frozen (Tsimpoukelli et al., 2021), and PICa (Yang et al., 2021) are too large to deploy in small or moderate computing machines due to their gigantic model sizes\nIn this paper, we study low-resource learning of VL tasks with our proposed method, FEWVLM, a moderate-sized vision-language model, in which we fine-tune the model with no or a handful of training examples. For FEWVLM, we pre-train a sequence-to-sequence transformer model (Cho et al., 2021; Raffel et al., 2019) with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). This setup is more practical in that training and inference can be run economically using standard computing hardware and it is expensive to obtain a large number of quality training examples in the real world. In such a few-shot setting, task-specific prompts or task descriptions are important and have shown effectiveness in few-shot NLP tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020).\nTo extend the success to VL tasks, we aim to answer the following questions for prompt-based low-resource VL learning. Q1) How does prompt design affect zero/few-shot learning on new tasks? Q2) Does prompt design still matter given larger training? Q3) How do different pre-training objectives affect zero/few-shot learning? To answer these questions, we explore various prompt formats including hand-crafted and noisy prompts on zero/few-shot VL learning datasets. In addition, we study pre-training objectives on few-shot tasks inspired by Raffel et al. (2019): prefix language modeling (PrefixLM) inspired by Raffel et al. (2019) and masked language modeling (MaskedLM). To this end, we investigate the model’s performance on few-shot VL tasks including visual question answering (Goyal et al., 2017; Marino et al., 2019; Hudson and Manning, 2019), captioning (Agrawal et al., 2019; Young et al., 2014) (Fig. 1), and miniImageNet (Vinyals et al., 2016).\nIn our empirical analysis, our FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31× larger than FEWVLM by 18.2% point on zero-shot VQAv2 and achieves comparable results to a 246× larger model, PICa (Yang et al., 2021). Furthermore, we observe that (1) prompts significantly affect zero-shot performance but marginally affect fewshot performance on new tasks (§6.2 and §6.3), (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data (§6.5), and (3) MaskedLM helps few-shot VQA tasks while PrefixLM boosts captioning performance (§6.6)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Vision-language few-shot learning. Recently, several few-shot learners on vision-language tasks were proposed including GPT (Radford et al.,\n2019; Brown et al., 2020), Frozen (Tsimpoukelli et al., 2021), PICa (Yang et al., 2021), and SimVLM (Wang et al., 2021). Frozen (Tsimpoukelli et al., 2021) is a large language model based on GPT-2 (Radford et al., 2019), and is transformed into a multimodal few-shot learner by extending the soft prompting to incorporate a set of images and text. Their approach shows the fewshot capability on visual question answering and image classification tasks. Similarly, PICa (Yang et al., 2021) uses GPT-3 (Brown et al., 2020) to solve VQA tasks in a few-shot manner by providing a few in-context VQA examples. It converts images into textual descriptions so that GPT-3 can understand the images. SimVLM (Wang et al., 2021) is trained with prefix language modeling on weakly-supervised datasets. It demonstrates its effectiveness on a zero-shot captioning task. While these models achieve improvement on few-shot tasks, they are impractical to use in real-world applications due to their model sizes.\nLanguage model prompting. Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020). Among them, GPT models (Radford et al., 2019; Brown et al., 2020) achieved great success in prompting or task demonstrations in NLP tasks. In light of this direction, prompt-based approaches improve small pre-trained models in few-shot text classification tasks (Gao et al., 2020; Schick and Schütze, 2020a,b). CLIP (Radford et al., 2021) also explores prompt templates for image classification which affect zero-shot performance. We follow these core ideas so we aim to improve zero-shot and few-shot performance using prompts in visionlanguage tasks.\nInput image\nTarget textInput text"
    }, {
      "heading" : "3 Analysis Setup",
      "text" : "In this work, we study the zero-shot and few-shot performance of vision-language models L. We introduce our analysis setup: problem formulation, analysis questions, downstream tasks and datasets, evaluation metrics, and baselines."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "For zero-shot tasks, a pre-trained VL model L have no access to training set Dtrain and development set Ddev, and directly makes inference on the test instances Dtest. For few-shot tasks, we compose a dev set Ddev from training data and ensure that |Dtrain| = |Ddev| following Perez et al. (2021); Gao et al. (2020) to tune the hyper-parameters and select the model. We limit the sizes of training and development sets to meet the goal of learning from limited data. The size ofDtrain and Ddev are small — i.e., we set the size of both to 16 in our study."
    }, {
      "heading" : "3.2 Analysis Questions",
      "text" : "We aim to answer the following questions in this study through experiments on multiple VL datasets.\nQ1) How does prompt design affect zero/fewshot learning on new tasks? Providing a pretrained language model with task-specific prompts or significantly improves zero-shot and few-shot performance on NLP domains (Gao et al., 2020; Schick and Schütze, 2020a,b; Brown et al., 2020). For this question, we test several ad-hoc prompts on vision-language tasks and analyze how large zero-shot and few-shot performance is affected by different prompts, hand-crafted and noisy prompts, in Sec. 6.5.\nQ2) Does prompt design still matter given larger training data? As we will see in our experiments, prompts affect the zero/few-shot performance. However, prompts may have different effects when models are given different sizes of training data. To answer this question, we train models with different sizes of training data and various prompts, and compare the performance between different prompts.\nQ3) How do different pre-training objectives affect zero/few-shot learning? We study two different pre-training objectives on few-shot performance: prefix language modeling (PrefixLM) inspired by Raffel et al. (2019) and masked language modeling (MaskedLM). In this setup, we pre-train our model with different objectives and test the model on zero-shot and few-shot tasks in Sec. 6.6."
    }, {
      "heading" : "3.3 Downstream Tasks and Datasets",
      "text" : "In this work, we mainly focus on three tasks: visual question answering, captioning, and categorical learning. The visual question answering task requires models to answer a question to a given context image. We convert the visual question answering task into a generation task so that the model can generate answers in the zero-shot setting. The captioning task requires a model to generate descriptions for a given context image. The categorical learning requires a model to choose the correct category or class. We evaluate our model in an open-ended fashion to quantify fast learning of categories, in which it must generate correct labels unlike other classification methods.\nWe include VQAv2 (Goyal et al., 2017), OKVQA (Marino et al., 2019), and GQA (Hudson and Manning, 2019) for visual question answering tasks, and NoCaps (Agrawal et al., 2019), and Flickr30k (Young et al., 2014) for image captioning.1 We use Karpathy split (Karpathy and Fei-Fei, 2015) for Flickr30k, which re-splits train and val images into 29,000 / 1,014 / 1,000 for train / validation / test. For categorical learning, we include miniImageNet (Vinyals et al., 2016), a meta learning dataset. Following (Tsimpoukelli et al., 2021), we use only meta test data to evaluate FEWVLM in a few-shot manner and test on 5-way k-shot setup, where 5 classes and k examples per class are given.2\n1We include COCO captioning results on Sec. B of Appendix.\n2For VQA and captioning, we include k samples in total, not per class."
    }, {
      "heading" : "3.4 Evaluation Metrics",
      "text" : "To evaluate few-shot performance, we randomly sample 5 different training and dev splits and measure average performance on the 5 splits. We finetune the vision-language models with 200 epochs for the few-shot setup and choose the best checkpoint on the dev set. For NoCaps task, it does not have training data. Thus we use the training data from COCO captioning in the experiments following Wang et al. (2021). We evaluate on the VQAv2 validation set, GQA test-dev, OK-VQA test set, test set of Karpathy split for Flickr30k captioning, and NoCaps validation set. We adopt accuracy for VQA datasets and miniImageNet, and CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics for captioning."
    }, {
      "heading" : "3.5 Baselines",
      "text" : "We evaluate strong zero/few-shot vision-language learners for comparison: Frozen (Tsimpoukelli et al., 2021), PICa (Yang et al., 2021) for VQA datasets and SimVLM (Wang et al., 2021) for captioning datasets. We include Unified VLP (Zhou et al., 2020) for few-shot VQAv2 and Flickr30k. Also, we compare them with fully fine-tuned models Lfull as upper bounds of few-shot models for each task; these models are fine-tuned on the entire datasets while few-shot models can access a small amount of data. For fully fine-tuned models Lfull, we borrow numbers from Uniterlarge (Chen et al., 2019) for VQAv2, Oscar (Li et al., 2020b) for GQA, SimVLM (Wang et al., 2021) and VinVL (Zhang et al., 2021) for NoCaps CIDER and SPICE respectively, and Unified VLP (Zhou et al., 2020) for Flickr30k captioning. We include VL-T5no-vqa as a baseline which is pre-trained without visual question answering datasets (Cho et al., 2021). For miniImageNet, we include Frozen and AFHN (Li\net al., 2020a). Frozen is designed for few-shot learning while AFHN is for meta learning, which is smaller and faster."
    }, {
      "heading" : "4 Method",
      "text" : "Before diving into the analysis, we introduce our model, FEWVLM, to do zero/few-shot learning on VL tasks and answer the analysis questions we raised. We introduce FEWVLM architecture and pre-training objectives."
    }, {
      "heading" : "4.1 Encoder-Decoder Vision-language Model",
      "text" : "We adopt an encoder-decoder architecture (Cho et al., 2021; Vaswani et al., 2017), to encode visual and text inputs and generate target text. We represent an input image with 36 object regions from a Faster R-CNN (Ren et al., 2015) trained on Visual Genome (Krishna et al., 2017). The sets of region representations are fed into the encoder by appending them to the text Cho et al. (2021). We train the model parameters θ by minimizing the negative log-likelihood of target text y tokens given input text x and image v:\nLθ = − |y|∑ i=1 logPθ(yi|y<i, x, v). (1)\nThe model is not task-specific, so it is a good option for zero/few-shot settings."
    }, {
      "heading" : "4.2 Pre-training Objectives",
      "text" : "We pre-train the models with both prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). Fig. 3 illustrates the PrefixLM and MaskedLM.\nPrefix language modeling. We include prefix language modeling (PrefixLM) following Raffel et al. (2019). Given an image and a span of text, this\nobjective randomly splits the text into two separate components; the former component with the given image is used as inputs to the encoder and the latter component is used as target text to be generated by the decoder.\nMasked language modeling. We follow Cho et al. (2021) to do masked language modeling. This objective is to replace random spans with numbered sentinel tokens, e.g., <text_1>, and then the masked text is fed into the encoder. Then the decoder generates the masked spans as target text. We randomly mask 15% of input text tokens and replace them with sentinel tokens.\nPre-training data. To pre-train FEWVLM, we collect image-caption data from MS COCO (Lin et al., 2014; Chen et al., 2015) and Visual Genome (VG) (Krishna et al., 2017)."
    }, {
      "heading" : "5 Low-resource Adaptation",
      "text" : "In downstream tasks, we train our model with few-shot examples. Fig. 2 shows an illustration of FEWVLM in inference time. Given a prompt template P , we first get input text and target text using the template x, y = P(input, label). Then we train model parameters by minimizing the negative log-likelihood in Eq. (1). In inference, we use the same prompt and the model generates the label text. Here we obtain the final label by removing the target prompt template."
    }, {
      "heading" : "5.1 Prompt Design",
      "text" : "Prompts affect the performance of the visionlanguage model (Cho et al., 2021); we study the effect of different prompts on the zero-shot and fewshot performance on downstream tasks. Tables 1 and 11 show prompts we used in our experiments."
    }, {
      "heading" : "5.1.1 Visual Question Answering.",
      "text" : "The visual question answering tasks (VQA, OKVQA, and GQA) require models to answer a question to a given context image. Recent approaches (Chen et al., 2019; Tan and Bansal, 2019; Su et al., 2019; Li et al., 2019, 2020b) tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates. Instead, we approach the visual question answering tasks as a generation task so that the model can produce the answers without introducing any task-specific heads. In this setup, prompts act as constraints to guide the models to generate proper\nformats of answers; models might generate a sentence for VQA, which is not the correct format, without prompts.\nTherefore, we study several prompts for input and output as shown in Tables 1 and 11; we explore hand-crafted prompts (Table 1) and noisy prompts for ablation study (Table 11).\nHand-crafted prompts. For input prompts, we explore three different templates: “question: [Q] answer:” and with the <text_1> sentinel token at the end. Similarly to masked language modeling, we expect models to generate words thanks to the sentinel token. For target prompts, we explore two different templates: “[A]” (an answer) and “<text_1> [A]” (an answer with a sentinel token). Here, we aim to mimic MaskedLM’s target text format, so the similar format helps the model quickly adapt to the new task. We call each prompt ID as in Table 1.\nNoisy prompts. To understand the effect of noisy prompts in zero/few-shot learning, we include irrelevant prompts, noisy tokens, and random sentences as in Table 11. Irrelevant prompts are random questions or instructions that mislead models to answer wrong questions or follow irrelevant instructions. Noisy tokens are randomly selected from T5’s vocabulary, so we test how robust our model is to random tokens. Finally, random sentences are captions from MS COCO and this gives false information to models."
    }, {
      "heading" : "5.1.2 Captioning.",
      "text" : "In NoCaps and Flickr30k, we explore three handcrafted input prompts: “a picture of ”, “a photo of ”, and “an image of ”. We study the effect of different word choices in this captioning task. While the three different words have similar meanings, they show different performance in zero-shot and fewshot tasks as we will see in our experiments.. For target prompts, we just train the model with the original caption without any additional prompts."
    }, {
      "heading" : "5.1.3 MiniImageNet",
      "text" : "In miniImageNet, we train our model with a handcrafted input prompt, “This is <text_1>,” and target prompt, “<text_1> [A].” We compare our model with and without prompts in this dataset to study whether prompts are helpful in categorical learning."
    }, {
      "heading" : "6 Results and Discussion",
      "text" : "In this section, we first discuss our main results on zero-shot and few-shot tasks and then answer the questions we raised: does prompt design matter in zero/few-shot learning?"
    }, {
      "heading" : "6.1 Experiment Details",
      "text" : "For pre-training, we set batch size 1,280 and 800 for FEWVLMbase and FEWVLMlarge, respectively and pre-train them with 30 epochs. We use learning rate 1e-4 with 5% linear warmup. For few-shot learning, we train models with 200 epochs, learning rate 5e-5 and 5% linear warmup and choose the best checkpoint on the dev set. For FEWVLM, we use “question: [Q] answer <text_1>” (P3) as an input prompt and “<text_1> [A]” as a target prompt for visual question answering, and “an image of” (Q3) as an input prompt for captioning, which show the best performance. We will study the effect of different prompts in Sec. 6.5. The sizes of of Dtrain and Ddev are 16 on VQA and captioning tasks. For miniImageNet, we use ‘This is <text_1>,” and “<text_1> [A]” as input and target prompts. In this data, we test with {1, 3, 5}-shots per class."
    }, {
      "heading" : "6.2 Performance on Zero-shot Learning",
      "text" : "We evaluate the existing models in a zero-shot manner, in which models do not have access to any training data. Tables 2 and 4 show the results on VQA and captioning datasets, respectively. First, FEWVLM with the hand-crafted prompt (P3) achieves better performance than other baselines on VQA datasets. In particular, our FEWVLMbase significantly outperforms Frozen which is about 31× larger than ours. Also, PICa based on GPT3 (Brown et al., 2020) shows the best performance on OK-VQA. It is noticeable that our FEWVLMlarge, the 246× smaller model, achieves the comparable result to PICa. Compared to VLT5no-vqa which is the same architecture as ours, FEWVLMbase improves VQAv2 performance by about 30% point. As we will see in the later section, our pre-training objectives and the prompts boost the VQA performance. On NoCaps, SimVLMhuge shows the best performance. Our FEWVLMbase significantly improves the performance compared to VL-T5no-vqa. As we will see in the later section, our pre-training objectives and the prompts boost the VQA and captioning performance."
    }, {
      "heading" : "6.3 Performance on Few-shot Learning",
      "text" : "Tables 3 and 5 show the few-shot performance on VQA and captioning datasets. Sizes of training and validation sets are 16 for FEWVLM, VLT5no-vqa, and Unified VLP; and Frozen and PICa use 4 and 16 in-context demonstration examples, respectively.\nOn VQAv2 and OK-VQA, PICa shows the best performance while our FEWVLMlarge achieves the comparable result on VQAv2. OK-VQA requires external knowledge to answer unlike other VQA datasets, so larger models and large pre-training data (prior knowledge) are necessary to improve. Interestingly, FEWVLM∗base, which is trained with 4 training examples, outperforms Frozen. On captioning data, FEWVLMbase notably outperforms VL-T5no-vqa by 31.1% point on NoCaps CIDEr.\nUnified VLP slightly underperforms FEWVLM on Flickr30k captioning task. We conjecture that their architecture is based on a encoder-decoder transfomer and it is pre-trained with a captioning task (Zhou et al., 2020)."
    }, {
      "heading" : "6.4 MiniImageNet",
      "text" : "Table 6 shows results on miniImageNet, where models must choose the correct class for each image. We train and evaluate FEWVLM in an generative manner; the model must generate correct label text to get the credit. FEWVLM significantly outperforms Frozen in all shots. Note that we train FEWVLM with a few training samples while Frozen uses them as in-context demonstration. Interestingly, FEWVLM with a hand-crafted prompt improves performance a lot on the 1-shot case, while it marginally improves on the 5-shot case."
    }, {
      "heading" : "6.5 Study of Prompt Design",
      "text" : "Here we examine the effect of different prompts on FEWVLMbase in Table 7 and Figs. 6, 5, and 4. We test the model on VQAv2 and Flickr30k datasets."
    }, {
      "heading" : "6.5.1 Zero-shot Predictions",
      "text" : "Table 7 shows the zero-shot performance on VQAv2 and Flickr30k. We observe that zero-shot results are remarkably affected by input prompts on both datasets. For input prompts, <text_1> in P1 and P3 helps the zero-shot predictions significantly compared to “no prompt” and P2. We conjecture that <text_1> guides the model to predict masked spans similarly to MaskedLM, so it improves the performance.\nOn Flickr30k, we examine different word choices of prompts: “a picture of” (Q1), “a photo of” (Q2), and “an image of” (Q3). For instance, using “an image of” outperforms using no prompt by 21.4 point. It is noticeable that different word choices significantly affect the zero-shot results."
    }, {
      "heading" : "6.5.2 Few-shot Predictions",
      "text" : "We study various input prompts including irrelevant prompts, noisy tokens, and random sentences on VQAv2 (Fig. 4). First, noisy prompts and no prompt achieve near 0 accuracy on the zero-shot setting. In few-shot predictions, FEWVLM with noisy prompts learns as quickly as hand-crafted prompts given larger data. For example, our model with noisy prompts achieves comparable results to the best hand-crafted prompt. Among all different types of noisy prompts, random sentences deteriorate performance the most. This is because the random sentences come from captions in MS COCO, so the model might choose the answer from wrong captions not from images. Interestingly, no prompt outperforms the other noisy prompts and even shows similar to or better than the handcrafted prompt with larger training data. We also observe a similar phenomenon on Flickr30k; no prompt performs similar to hand-crafted prompts in Fig. 5.\nIn addition, we explore two different target prompts, “<text_1 [A]” and “[A].” We try to mimic the MaskedLM’s target text format, so we add “<text_1” to target prompt on VQA. This might help the model’s fast adaptation to a new\ntask since they share the same target prompt. In Fig. 6, we notice an interesting phenomenon; the target prompt “[A]” shows a larger variance than the other suggesting that introducing “<text_1” helps the model quickly adapt to a new task. However, both prompts show similar results given larger training data, e.g., 300."
    }, {
      "heading" : "6.6 Pre-training Objectives",
      "text" : "We investigate how pre-training objectives affect different tasks. We pre-train FEWVLM with different pre-training objectives: masked language modeling (MaskedLM) and prefix language modeling (PrefixLM).\nIn Table 8, we observe that MaskedLM helps VQA tasks while PrefixLM helps captioning tasks in zero-shot and few-shot settings. We conjecture that MaskedLM is to predict spans, which is analogous to predict correct answers to questions, and PrefixLM is to generate the rest of the given prefix, which is similar to captioning tasks. In other words, if the pre-training task is similar to the downstream tasks, then it will help performance further. When pre-training with both objectives, they create a synergetic effect and thus improve cross-task generalization."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we present FEWVLM, a few-shot prompt-based learner on vision-language tasks. On diverse datasets, FEWVLM outperforms baselines and shows comparable results to PICa which is 246× larger than ours. We observe that prompts are vital in zero-shot and few-shot tasks and each pre-training objective helps different few-shot tasks. Also, we find out that models with larger training data are not significantly affected by noisy prompts. Future work includes exploring automatic prompt generation and diverse formats of few-shot tasks such as multiple-choice VQA. Finding optimal prompts require exhaustive engineering to achieve the best performance and leads to impressive results. We leave the exploration of these directions to future investigations."
    }, {
      "heading" : "A Model Architectures",
      "text" : "Table 9 shows model parameters in our model, FEWVLM. FEWVLMbase and FEWVLMlarge is based on VL-T5 (Cho et al., 2021) and T5 (Raffel et al., 2019), respectively."
    }, {
      "heading" : "B COCO Captioning",
      "text" : "We evaluate our model with COCO captioning data. We use Karpathy split (Karpathy and Fei-Fei, 2015) for MS COCO captioning, which re-splits train and val images into 113,287 / 5000 / 5000 for train / validation / test. Table 10 shows the results on COCO."
    }, {
      "heading" : "C Prompt Study",
      "text" : "Tables 7, 8, and 9 show the results of each prompt on VQAv2 and Flickr30k with various training sizes."
    }, {
      "heading" : "D Effect of pre-training Data",
      "text" : "We pre-train our model with different datasets: MS COCO and Visual Genome (VG), and Conceptual Captions (CC). We investigate which pre-training dataset helps the downstream tasks in a few-shot manner. In Table 12, we observe that MS COCO and VG datasets are more helpful to the downstream tasks than CC."
    } ],
    "references" : [ {
      "title" : "nocaps: novel object captioning at scale",
      "author" : [ "Harsh Agrawal", "Karan Desai", "Yufei Wang", "Xinlei Chen", "Rishabh Jain", "Mark Johnson", "Dhruv Batra", "Devi Parikh", "Stefan Lee", "Peter Anderson." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference on",
      "citeRegEx" : "Agrawal et al\\.,? 2019",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2019
    }, {
      "title" : "Spice: Semantic propositional image caption evaluation",
      "author" : [ "Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould." ],
      "venue" : "European conference on computer vision, pages 382–398. Springer.",
      "citeRegEx" : "Anderson et al\\.,? 2016",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "arXiv preprint arXiv:1504.00325.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Uniter: Learning universal image-text representations",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Unifying vision-and-language tasks via text generation",
      "author" : [ "Jaemin Cho", "Jie Lei", "Hao Tan", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv:2102.02779.",
      "citeRegEx" : "Cho et al\\.,? 2021",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2021
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2012.15723.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "author" : [ "Drew A Hudson", "Christopher D Manning." ],
      "venue" : "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700–6709.",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128–3137.",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Adversarial feature hallucination networks for fewshot learning",
      "author" : [ "Kai Li", "Yulun Zhang", "Kunpeng Li", "Yun Fu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13470–13479.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Oscar: Objectsemantics aligned pre-training for vision-language tasks",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Pengchuan Zhang", "Xiaowei Hu", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European conference on computer vision, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Ok-vqa: A visual question answering benchmark requiring external knowledge",
      "author" : [ "Kenneth Marino", "Mohammad Rastegari", "Ali Farhadi", "Roozbeh Mottaghi." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-",
      "citeRegEx" : "Marino et al\\.,? 2019",
      "shortCiteRegEx" : "Marino et al\\.",
      "year" : 2019
    }, {
      "title" : "True few-shot learning with language models",
      "author" : [ "Ethan Perez", "Douwe Kiela", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:2105.11447.",
      "citeRegEx" : "Perez et al\\.,? 2021",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning transferable visual models from natural language supervision",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog,",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "Advances in neural information processing systems, 28:91–99.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting cloze questions for few shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2001.07676.",
      "citeRegEx" : "Schick and Schütze.,? 2020a",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "It’s not just size that matters: Small language models are also few-shot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2009.07118.",
      "citeRegEx" : "Schick and Schütze.,? 2020b",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "Vl-bert: Pretraining of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "arXiv preprint arXiv:1908.08530.",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Lxmert: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv:1908.07490.",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Multimodal few-shot learning with frozen language models",
      "author" : [ "Maria Tsimpoukelli", "Jacob Menick", "Serkan Cabi", "SM Eslami", "Oriol Vinyals", "Felix Hill." ],
      "venue" : "arXiv preprint arXiv:2106.13884.",
      "citeRegEx" : "Tsimpoukelli et al\\.,? 2021",
      "shortCiteRegEx" : "Tsimpoukelli et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Daan Wierstra" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Simvlm: Simple visual language model pretraining with weak supervision",
      "author" : [ "Zirui Wang", "Jiahui Yu", "Adams Wei Yu", "Zihang Dai", "Yulia Tsvetkov", "Yuan Cao." ],
      "venue" : "arXiv preprint arXiv:2108.10904.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "An empirical study of gpt-3 for few-shot knowledge-based vqa",
      "author" : [ "Zhengyuan Yang", "Zhe Gan", "Jianfeng Wang", "Xiaowei Hu", "Yumao Lu", "Zicheng Liu", "Lijuan Wang." ],
      "venue" : "arXiv preprint arXiv:2109.05014.",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Vinvl: Revisiting visual representations in vision-language models",
      "author" : [ "Pengchuan Zhang", "Xiujun Li", "Xiaowei Hu", "Jianwei Yang", "Lei Zhang", "Lijuan Wang", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Unified vision-language pre-training for image captioning and vqa",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason Corso", "Jianfeng Gao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13041–",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Experimental results on VQA show that FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31× larger than FEWVLM by 18.",
      "startOffset" : 91,
      "endOffset" : 118
    }, {
      "referenceID" : 30,
      "context" : "2% point and achieves comparable results to a 246× larger model, PICa (Yang et al., 2021).",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks (Devlin et al., 2018; Raffel et al., 2019; Brown et al., 2020; Radford et al., 2021).",
      "startOffset" : 131,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : "Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks (Devlin et al., 2018; Raffel et al., 2019; Brown et al., 2020; Radford et al., 2021).",
      "startOffset" : 131,
      "endOffset" : 215
    }, {
      "referenceID" : 2,
      "context" : "Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks (Devlin et al., 2018; Raffel et al., 2019; Brown et al., 2020; Radford et al., 2021).",
      "startOffset" : 131,
      "endOffset" : 215
    }, {
      "referenceID" : 17,
      "context" : "Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks (Devlin et al., 2018; Raffel et al., 2019; Brown et al., 2020; Radford et al., 2021).",
      "startOffset" : 131,
      "endOffset" : 215
    }, {
      "referenceID" : 2,
      "context" : ", few-shot and zero-shot learning (Brown et al., 2020; Radford et al., 2021; Tsimpoukelli et al., 2021).",
      "startOffset" : 34,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : ", few-shot and zero-shot learning (Brown et al., 2020; Radford et al., 2021; Tsimpoukelli et al., 2021).",
      "startOffset" : 34,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : ", few-shot and zero-shot learning (Brown et al., 2020; Radford et al., 2021; Tsimpoukelli et al., 2021).",
      "startOffset" : 34,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "However, recent few-shot models such as GPT3 (Brown et al., 2020), Frozen (Tsimpoukelli et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : ", 2020), Frozen (Tsimpoukelli et al., 2021), and PICa (Yang et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : ", 2021), and PICa (Yang et al., 2021) are too large to deploy in small or moderate computing machines due to their gigantic model sizes",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "For FEWVLM, we pre-train a sequence-to-sequence transformer model (Cho et al., 2021; Raffel et al., 2019) with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "For FEWVLM, we pre-train a sequence-to-sequence transformer model (Cho et al., 2021; Raffel et al., 2019) with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "In such a few-shot setting, task-specific prompts or task descriptions are important and have shown effectiveness in few-shot NLP tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 225
    }, {
      "referenceID" : 17,
      "context" : "In such a few-shot setting, task-specific prompts or task descriptions are important and have shown effectiveness in few-shot NLP tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 225
    }, {
      "referenceID" : 2,
      "context" : "In such a few-shot setting, task-specific prompts or task descriptions are important and have shown effectiveness in few-shot NLP tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 225
    }, {
      "referenceID" : 8,
      "context" : "To this end, we investigate the model’s performance on few-shot VL tasks including visual question answering (Goyal et al., 2017; Marino et al., 2019; Hudson and Manning, 2019), captioning (Agrawal et al.",
      "startOffset" : 109,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "To this end, we investigate the model’s performance on few-shot VL tasks including visual question answering (Goyal et al., 2017; Marino et al., 2019; Hudson and Manning, 2019), captioning (Agrawal et al.",
      "startOffset" : 109,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : "To this end, we investigate the model’s performance on few-shot VL tasks including visual question answering (Goyal et al., 2017; Marino et al., 2019; Hudson and Manning, 2019), captioning (Agrawal et al.",
      "startOffset" : 109,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : ", 2019; Hudson and Manning, 2019), captioning (Agrawal et al., 2019; Young et al., 2014) (Fig.",
      "startOffset" : 46,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : ", 2019; Hudson and Manning, 2019), captioning (Agrawal et al., 2019; Young et al., 2014) (Fig.",
      "startOffset" : 46,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "In our empirical analysis, our FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31× larger than FEWVLM by 18.",
      "startOffset" : 84,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "2% point on zero-shot VQAv2 and achieves comparable results to a 246× larger model, PICa (Yang et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "Recently, several few-shot learners on vision-language tasks were proposed including GPT (Radford et al., 2019; Brown et al., 2020), Frozen (Tsimpoukelli et al.",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "Recently, several few-shot learners on vision-language tasks were proposed including GPT (Radford et al., 2019; Brown et al., 2020), Frozen (Tsimpoukelli et al.",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : ", 2020), Frozen (Tsimpoukelli et al., 2021), PICa (Yang et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : ", 2021), PICa (Yang et al., 2021), and SimVLM (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 25,
      "context" : "Frozen (Tsimpoukelli et al., 2021) is a large language model based on GPT-2 (Radford et al.",
      "startOffset" : 7,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : ", 2021) is a large language model based on GPT-2 (Radford et al., 2019), and is transformed into a multimodal few-shot learner by extending the soft prompting to incorporate a set of images and text.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 30,
      "context" : "Similarly, PICa (Yang et al., 2021) uses GPT-3 (Brown et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : ", 2021) uses GPT-3 (Brown et al., 2020) to solve VQA tasks in a few-shot manner by providing a few in-context VQA examples.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 29,
      "context" : "SimVLM (Wang et al., 2021) is trained with prefix language modeling on weakly-supervised datasets.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 202
    }, {
      "referenceID" : 17,
      "context" : "Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks (Gao et al., 2020; Radford et al., 2021; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "Among them, GPT models (Radford et al., 2019; Brown et al., 2020) achieved great success in prompting or task demonstrations in NLP tasks.",
      "startOffset" : 23,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Among them, GPT models (Radford et al., 2019; Brown et al., 2020) achieved great success in prompting or task demonstrations in NLP tasks.",
      "startOffset" : 23,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "CLIP (Radford et al., 2021) also explores prompt templates for image classification which affect zero-shot performance.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "Q1) How does prompt design affect zero/fewshot learning on new tasks? Providing a pretrained language model with task-specific prompts or significantly improves zero-shot and few-shot performance on NLP domains (Gao et al., 2020; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 211,
      "endOffset" : 278
    }, {
      "referenceID" : 2,
      "context" : "Q1) How does prompt design affect zero/fewshot learning on new tasks? Providing a pretrained language model with task-specific prompts or significantly improves zero-shot and few-shot performance on NLP domains (Gao et al., 2020; Schick and Schütze, 2020a,b; Brown et al., 2020).",
      "startOffset" : 211,
      "endOffset" : 278
    }, {
      "referenceID" : 8,
      "context" : "We include VQAv2 (Goyal et al., 2017), OKVQA (Marino et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : ", 2017), OKVQA (Marino et al., 2019), and GQA (Hudson and Manning, 2019) for visual question answering tasks, and NoCaps (Agrawal et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : ", 2019), and GQA (Hudson and Manning, 2019) for visual question answering tasks, and NoCaps (Agrawal et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : ", 2019), and GQA (Hudson and Manning, 2019) for visual question answering tasks, and NoCaps (Agrawal et al., 2019), and Flickr30k (Young et al.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 31,
      "context" : ", 2019), and Flickr30k (Young et al., 2014) for image captioning.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "1 We use Karpathy split (Karpathy and Fei-Fei, 2015) for Flickr30k, which re-splits train and val images into 29,000 / 1,014 / 1,000 for train / validation / test.",
      "startOffset" : 24,
      "endOffset" : 52
    }, {
      "referenceID" : 28,
      "context" : "For categorical learning, we include miniImageNet (Vinyals et al., 2016), a meta learning dataset.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 25,
      "context" : "Following (Tsimpoukelli et al., 2021), we use only meta test data to evaluate FEWVLM in a few-shot manner and test on 5-way k-shot setup, where 5 classes and k examples per class are given.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "We adopt accuracy for VQA datasets and miniImageNet, and CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : ", 2015) and SPICE (Anderson et al., 2016) as evaluation metrics for captioning.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "We evaluate strong zero/few-shot vision-language learners for comparison: Frozen (Tsimpoukelli et al., 2021), PICa (Yang et al.",
      "startOffset" : 81,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : ", 2021), PICa (Yang et al., 2021) for VQA datasets and SimVLM (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 29,
      "context" : ", 2021) for VQA datasets and SimVLM (Wang et al., 2021) for captioning datasets.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 33,
      "context" : "We include Unified VLP (Zhou et al., 2020) for few-shot VQAv2 and Flickr30k.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "For fully fine-tuned models Lfull, we borrow numbers from Uniterlarge (Chen et al., 2019) for VQAv2, Oscar (Li et al.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 29,
      "context" : ", 2020b) for GQA, SimVLM (Wang et al., 2021) and VinVL (Zhang et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : ", 2021) and VinVL (Zhang et al., 2021) for NoCaps CIDER and SPICE respectively, and Unified VLP (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 33,
      "context" : ", 2021) for NoCaps CIDER and SPICE respectively, and Unified VLP (Zhou et al., 2020) for Flickr30k captioning.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "We include VL-T5no-vqa as a baseline which is pre-trained without visual question answering datasets (Cho et al., 2021).",
      "startOffset" : 101,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "For miniImageNet, we include Frozen and AFHN (Li et al., 2020a).",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "We adopt an encoder-decoder architecture (Cho et al., 2021; Vaswani et al., 2017), to encode visual and text inputs and generate target text.",
      "startOffset" : 41,
      "endOffset" : 81
    }, {
      "referenceID" : 26,
      "context" : "We adopt an encoder-decoder architecture (Cho et al., 2021; Vaswani et al., 2017), to encode visual and text inputs and generate target text.",
      "startOffset" : 41,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "We represent an input image with 36 object regions from a Faster R-CNN (Ren et al., 2015) trained on Visual Genome (Krishna et al.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "To pre-train FEWVLM, we collect image-caption data from MS COCO (Lin et al., 2014; Chen et al., 2015) and Visual Genome (VG) (Krishna et al.",
      "startOffset" : 64,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "To pre-train FEWVLM, we collect image-caption data from MS COCO (Lin et al., 2014; Chen et al., 2015) and Visual Genome (VG) (Krishna et al.",
      "startOffset" : 64,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "Prompts affect the performance of the visionlanguage model (Cho et al., 2021); we study the effect of different prompts on the zero-shot and fewshot performance on downstream tasks.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Recent approaches (Chen et al., 2019; Tan and Bansal, 2019; Su et al., 2019; Li et al., 2019, 2020b) tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates.",
      "startOffset" : 18,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : "Recent approaches (Chen et al., 2019; Tan and Bansal, 2019; Su et al., 2019; Li et al., 2019, 2020b) tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates.",
      "startOffset" : 18,
      "endOffset" : 100
    }, {
      "referenceID" : 23,
      "context" : "Recent approaches (Chen et al., 2019; Tan and Bansal, 2019; Su et al., 2019; Li et al., 2019, 2020b) tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates.",
      "startOffset" : 18,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "Also, PICa based on GPT3 (Brown et al., 2020) shows the best performance on OK-VQA.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 33,
      "context" : "We conjecture that their architecture is based on a encoder-decoder transfomer and it is pre-trained with a captioning task (Zhou et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 143
    } ],
    "year" : 0,
    "abstractText" : "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FEWVLM, relatively smaller than recent few-shot learners. For FEWVLM, we pre-train a sequenceto-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31× larger than FEWVLM by 18.2% point and achieves comparable results to a 246× larger model, PICa (Yang et al., 2021). In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance.",
    "creator" : null
  }
}
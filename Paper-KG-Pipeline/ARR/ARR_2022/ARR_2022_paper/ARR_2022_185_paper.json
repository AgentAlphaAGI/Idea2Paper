{
  "name" : "ARR_2022_185_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Using Paraphrases to Study Properties of Contextual Embeddings",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Contextualized embedding algorithms, such as BERT (Devlin et al., 2019), have achieved impressive performance on a wide variety of tasks (Huang et al., 2019; Chan and Fan, 2019; Yoosuf and Yang, 2019). One application of BERT is as a measure of sentence similarity (Zhang et al., 2019; Sellam et al., 2020), based on the assumption that BERT will produce similar representations for the words in two sentences with similar semantics.\nWe propose to use paraphrases with alignments between words as a tool for studying how BERT represents words and phrases. Figure 1 shows an example. Critically, when considering an aligned word pair, we can assume the context has a similar impact on both words because we know the phrases are semantically similar. Previously, paraphrases have been used to probe whether compositionality is accurately captured by BERT (Yu and Ettinger, 2020), but we believe they can be used to explore many other questions.\nUsing the Paraphrase Database (Pavlick et al., 2015), we explore how consistent contextual representations are when controlling for the semantics of\nthe context. We find that BERT does consistently represent phrases that are paraphrases. Looking at words, BERT effectively handles variation in spelling, but does less well with spelling errors. BERT effectively handles words of varying levels of polysemy, but the representations for synonyms are surprisingly diverse, with a much broader distribution of similarity scores. These findings confirm results from prior work using other methods, while revealing new details.\nWe also consider a range of other models’ word representations, finding that they have similar patterns to BERT, but with words that are the same and aligned receiving even more consistent representations than from BERT. BERT gives less contextualized representations to paraphrased words than non-paraphrased words, with the exception of punctuation. Finally, we re-evaluate work looking at patterns across BERT’s layers and find that when controlling for semantics the later layers actually produce more similar representations (in contrast to previous work).\nThese results show that paraphrases are a useful tool for studying representations. By controlling for meaning while presenting interesting surface variations, they provide a unique probe of behavior."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 BERTology",
      "text" : "There has been a growing body of research studying the inner workings of BERT and trying to quantify what it learns in various scenarios, dubbed\n“BERTology” (Rogers et al., 2020). Of particular interest to this paper is work that analyzes BERT’s output embeddings. Recent studies have found that embeddings created from the final layer of BERT tend to cluster according to word senses (Wiedemann et al., 2019), though this varies somewhat based on the position of a word in a sentence (Mickus et al., 2020). The final BERT layers also produce more contextualized word embeddings than the earlier layers (Ethayarajh, 2019), a finding we revisit using paraphrases in Section 3.3."
    }, {
      "heading" : "2.2 The Paraphrase Database",
      "text" : "To analyze BERT, we take advantage of the unique properties of paraphrases. We use the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015), a database of paraphrases collected using bilingual pivoting, the process of taking a particular English phrase, looking at all the foreign language phrases it can be translated into, finding all occurrences of these foreign language phrases, and then translating them back into English (Bannard and Callison-Burch, 2005). PPDB 2.0 contains 100m+ English paraphrases, each with word alignment information, an automatically generated quality rating, and, for a subset, a human quality rating.1 Word alignments are the by-product of the bilingual pivoting method used to collect the paraphrases. When using alignments, we only consider phrases from the highest quality section of the PPDB, which are most likely to have accurate alignments. Example paraphrases with their average human annotations and automatically generated scores are shown in Table 1. In general, the phrases in this dataset are short. The longest phrases have six tokens, and the majority have fewer than six.\nHuman quality ratings are included for 26,455 1http://paraphrase.org.\nparaphrase pairs, with five annotations per paraphrase. Agreement is measured using Spearman’s ρ (Spearman, 1910); the average ρ between two workers is 0.57, and the average ρ between each worker with the other four annotators is 0.65.\nThe automatic quality ratings (PPDB score) are generated by using the human annotations to fit a supervised ridge regression model. The input to the model consists of 209 hand-crafted paraphrase features, including WordNet features (Fellbaum, 1998), distributional similarity features, and cosine similarities of generated Multiview Latent Semantic Analysis embeddings (Rastogi et al., 2015). The PPDB score achieves a Spearman’s ρ of 0.71. In comparison, Pavlick et al. (2015) report that using the word2vec embedding of the rarest word in each paraphrase obtains Spearman’s ρ of 0.46."
    }, {
      "heading" : "3 Experiments",
      "text" : "In our experiments, we want to use the PPDB to examine BERT’s ability to consistently represent paraphrase semantics. In order to do this, we consider both phrase-level and word-level embeddings. Except where explicitly indicated otherwise, all experiments are run using the uncased base model of BERT, using a maximum sequence length of 128 and a batch size of 8. We use the pretrained models provided by the Transformers library.2\nThere is a slight mismatch between the PPDB’s tokenization and the format of the BERT training data. The mismatch primarily occurs with contractions and apostrophes (e.g., BERT expects “don’t”, while the PPDB is tokenized “do n’t”). This does not substantially affect the results; less than 8% of the human-annotated paraphrase pairs contain apostrophes. When words are broken into multiple pieces by the wordpiece tokenizer, we use the average of the pieces as the word representation."
    }, {
      "heading" : "3.1 Phrase-Level Embeddings",
      "text" : "First, we consider phrase-level embeddings that capture aggregate information about all of the words in a given phrase. These embeddings show us that BERT is able to distinguish between two paraphrases, and two unrelated phrases.\nWe use 25,736 phrase pairs with human annotations in the PPDB.3 Each human annotation is\n2https://huggingface.co/docs/ transformers/index\n3This is 3% smaller than the entire human-annotated subset. We were unable to map some of the human-annotated data to the data with PPDB scores (even with help from the\nbetween 1 and 5, reflecting the similarity of the two phrases. We run each phrase through the pretrained BERT model. For each pair of phrases, we average together the embeddings for each word to get a phrase embedding. We create phrase embeddings using averaging because previous research has shown that this method is effective. For example, Reimers and Gurevych (2019) created sentence embeddings using three methods: (1) averaging word embeddings, (2) taking the maximum of word embeddings, and (3) using the CLS token vector. They found that averaging created the best sentence embeddings for semantic textual similarity tasks.\nAfter creating phrase embeddings, we take the cosine similarity between the two embeddings. We compare with ground truth annotations using Spearman’s ρ. We do this for each of the twelve BERT layers, and the concatenation of all layers. We use cosine similarity to compare embeddings because this metric is commonly used when working with BERT (e.g., Mahmoud and Torki (2020); Garí Soler and Apidianaki (2020); Kovaleva et al. (2019)).\nWe compare BERT to a more traditional embedding method, the continuous bag-of-words approach in word2vec (w2v) (Mikolov et al., 2013). We train w2v on an English Wikipedia corpus of 5,269,686 sentences,4 using dimension size 200, a window size of five, and a minimum count of five. We choose to train w2v on Wikipedia data, in order to replicate the correlations in Pavlick et al. (2015). We train five w2v models, using five different random seeds.5 For each pair of phrases, we average together the embeddings for each word to get a phrase embedding, and then take the cosine similarity between the two phrase embeddings. We report the average and standard deviation of Spearman’s ρ over the five models.\nComparing sentences and phrases. One difference between our work and the way BERT is normally used is that we have phrases rather than sentences. To check that this does not substantially change BERT’s behavior, we compare the embeddings for phrases in a sentence and the phrases on their own. We take 9,780 paraphrases from the PPDB. We choose paraphrases where one of the phrases has at least six tokens, the paraphrase has a relatively good PPDB score and no syntactic place-\nauthors of the PPDB paper). This may be why our scores for w2v are lower than those reported by Pavlick et al. (2015).\n4This data was used in Tsvetkov et al. (2016) and is available by contacting the authors of that paper.\n52518, 2548, 2590, 29, 401\nholders. This is described further in Section 3.2. For each phrase, we find up to 100 sentences (on average, 80.5 sentences) in Gigaword (Graff et al., 2003; Rush et al., 2015)6 and OpenSubtitles (Tiedemann, 2012)7 that contain that phrase. For each sentence, we run it through BERT and average together the word embeddings for words in the phrase to create a phrase embedding. The phrase embeddings are generally similar across different sentences (average cosine similarity of 0.82± 0.07).\nNow we can compare (1) the average of phrase embeddings derived from sentences, with (2) embeddings for phrases in isolation, to see if BERT will be confused by not having a complete sentence. For each phrase, we take the cosine similarity between the phrase embeddings created using these two methods. The phrase embeddings are very similar (average similarity of 0.74± 0.12). This gives us confidence that BERT produces embeddings for phrases on their own that are very similar to phrases in the context of a sentence. For future experiments, we run phrases individually through BERT, rather than complete sentences, which allows us to focus on the semantics of the phrase itself.\nResults on the PPDB. Table 2 shows results for BERT, w2v, and the PPDB model, broken down by the average length of each paraphrase. For all layers, BERT improves on longer paraphrases. This is intuitive, because the longer the phrase, the more it will be able to leverage contextual information. The last layer of BERT behaves slightly differently\n6https://huggingface.co/datasets/gigaword. 7https://opus.nlpl.eu/OpenSubtitles.php.\nthan the other layers. While it continues to perform better on longer paraphrases, it does substantially worse on short paraphrases and slightly worse on medium paraphrases.\nSimilarly, w2v also improves on longer paraphrases. By taking the average of all the word embeddings for each word in the phrase, w2v has more information to incorporate into its phrase embeddings for longer paraphrases. Though w2v improves as the paraphrases grow longer, it underperforms BERT for all but the shortest paraphrases. We also see that the automatic PPDB score does better on longer paraphrases. This could be because it incorporates distributional information, which is richer when there are more words. Finally, the human annotation scores show that longer paraphrases are more similar.\nFrom Table 2, we see that the final layer of BERT outperforms w2v and performs comparably to the PPDB score on the longest paraphrases. This is not a completely fair comparison; the PPDB model is trained specifically on this data, and has access to outside information that BERT does not, such as WordNet features and additional features derived from the translation process used to create the PPDB. These results give us confidence that BERT can distinguish between phrases that are paraphrases of each other and phrases that are not.\nLooking at BERT’s output, we can see several patterns in the paraphrases that receive high and low similarities. Table 3 shows examples of these patterns. For phrases with high similarity according to BERT, a single word changes or a single word is added. On the surface, these changes have very little impact on the meaning, though the addition of the word ‘special’ in the second case could change who is being referred to. For phrases with low similarity according to BERT, they frequently required world knowledge (e.g., definition of an acronym) or appeared to be errors. We also observed idioms getting reasonably high scores, but not as high as\nthe literal paraphrases. Conclusion: The standard way of using BERT to produce a representation of a phrase is consistent with human scores of paraphrases. All layers are effective, though the last layer struggles with shorter phrases."
    }, {
      "heading" : "3.1.1 One-Word Paraphrases",
      "text" : "In Section 3.1, we saw that BERT does not do as well on short phrases as it does on longer ones. We explore the extreme case of single word paraphrases here. Among the subset of one-word paraphrases, there is a wide range of human annotations (average annotation 2.27± 0.99). To explore this further, we focus on one-word paraphrases with a human annotation of 5, the highest annotation score, indicating that these are the strongest synonyms. Among these high-quality synonyms, there is a wide range of cosine similarities for the last layer of BERT (average similarity 0.76± 0.12).\nTable 4 shows synonyms with both the highest and lowest BERT similarities. Misspelled words (e.g., completly, approximatly) and pairs that involve different languages (e.g., French laboratoires) have low cosine similarities. Numbers appear on both the low end (e.g., 79.0 and seventy-nine) and the high end (e.g.,\n1.350 and 1.35) of the similarity spectrum. One difference between the similar and dissimilar number pairs is that in the similar case they both use digits, while in the dissimilar case, one uses digits while the other uses words.\nConclusion: Looking at single words shows that BERT struggles to identify synonyms, and does particularly poorly with misspellings and crosslingual comparisons."
    }, {
      "heading" : "3.2 Word-Level Embeddings",
      "text" : "PPDB provides alignments between words in the paraphrases, automatically generated as part of bilingual pivoting. We use these alignments to consider four different sets of words:\nSame, Aligned Words that are the same in both phrases and aligned.\nSame, Unaligned Words that are the same in both phrases, but not aligned. These tend to be function words. 90% of our examples are one of (the, of, \", to, i, in, that, as, what). This category may have more examples of other word types if longer paraphrases are considered in future work.\nDifferent, Aligned Words that are aligned, but not the same. This case covers synonyms.\nDifferent, Unaligned Words that are not aligned and not the same (but still one from each phrase in a paraphrase pair). Note, these words are not completely unrelated. They are from the same paraphrase, making them more related than words from two random phrases.\nWe restrict our experiments to the highest quality paraphrases in the PPDB: dataset S. We also\nonly consider long paraphrases (where one of the phrases has at least six tokens), and paraphrases that have no syntactic placeholders (a subset of the PPDB contains general syntactic symbols, such as wishes to be [VP/NP]). From this set, we randomly sample 4,000 paraphrases. Our sample yields 22,751 aligned same words, 25,973 aligned different words, 2,782 unaligned same words, and 163,474 unaligned different words. We randomly sample 2,500 words from each category. For the aligned words, we only use cases where there is a 1-1 alignment.\nTo generate word-level embeddings, we run each phrase through the a set of transformer models and for each pair of words, we take the cosine similarity between the embeddings of the two words."
    }, {
      "heading" : "3.2.1 Results",
      "text" : "Figure 3 shows the distributions of similarity scores for all four sets of words for several models. Same, aligned words consistently have the highest similarity. The other categories tend to overlap. Because we are using paraphrases, we would hope that aligned different words would have higher similarity, but that is not consistently the case.\nComparing the models, there are some notable variations. Between BERT base and BERT large, the biggest shift is that unaligned words that are the same have much lower similarity in BERT large, though there is also a new peak for aligned words around 0.2. Comparing the two BERT models with BART and GPT-2, the width of the peak for same aligned words is quite different. BART is the only model to have a substantial number of negatively correlated word pairs.\nQualitatively looking at examples, we notice that when a token appears in a different position in the paraphrase, the similarity tends to be on the lower end of the distribution (e.g., action in the phrases plans of action for the implementation and action plan for the implementation has a similarity of 0.28). To explore this, we consider 2,181 aligned same words. We measured the cosine similarity of the last layer of BERT broken down by the variation in position (plotted in Figure 7 in the Appendices). Spearman’s ρ = −0.29 (p-value < 10e−42), indicating that similarity decreases for larger changes in position. This supports observations in prior work (Mickus et al., 2020), but now with the knowledge that the overall context has the same meaning. This is not intuitive behavior; because these words\nare aligned in a paraphrase, we would expect that the position of the word would not substantially affect its representation.\nConclusions: (1) Contextual word embedding methods consistently handle aligned words in paraphrases, but with substantial variations across models in how peaked the distributions of same-aligned words are. (2) Even when controlling for the meaning of the context, BERT represents words differently depending on their position."
    }, {
      "heading" : "3.2.2 Punctuation",
      "text" : "Punctuation is a core part of language that functions quite unlike words; punctuation groups words together or separates them, and contributes to the overall structure and meaning of a phrase or sentence. Punctuation plays an important role in distinguishing between different types of text, such as texts by different authors (Soler-Company and Wanner, 2017) or texts produced by different Twitter communities (Tatman and Paullada, 2017). Embeddings are used to generate punctuation for text that is lacking punctuation, such as recorded transcripts (Yi and Tao, 2019). To explore how BERT handles punctuation, we consider the cosine similarity distribution for different sets of punctuation tokens for the last layer of BERT. We find that punctuation has a broader distribution of cosine\nsimilarities than other tokens, indicating that punctuation embeddings vary widely dependent on the surrounding context.\nIn Figure 4, we break these trends down by individual punctuation marks, focusing only on aligned same words. We look at the most common punctuation marks. Of these punctuation marks, the comma and period show the widest distributions. Even when they play the same role in the paraphrase, they can be given very different embeddings, indicating how highly contextualized these punctuation marks are. The question mark and dash are less contextualized; this is most likely because these punctuation marks are used in more prescribed circumstances. In this dataset, in all but one example, the question mark is the last token; the dash is the first token in all but two examples. Table 5 shows examples in context of each of these punctuation marks.\nConclusion: BERT’s representation of punctuation is surprisingly context sensitive, with substantial variation even when we control for meaning."
    }, {
      "heading" : "3.2.3 Polysemy",
      "text" : "Previous work has shown that BERT embeddings form clusters based on word senses (Wiedemann et al., 2019). In the context of aligned words in a paraphrase, we would expect even a highly polysemous word to have similar embeddings in the two phrases.\nTo measure polysemy, we consider the number of WordNet synsets of a word, focusing on same aligned and same unaligned words. In order to have enough data to make a good comparison, we use\nthe 4,000 sampled paraphrases from Section 3.2, as well as an additional random sample of long paraphrases with at least one unaligned same word. We then downsample the aligned same words to get 1,597 instances of both unaligned and aligned same words that are present in WordNet, with up to 52 synsets.8\nIn Figure 5, we show the cosine similarity distributions for both aligned and unaligned words with different levels of polysemy across the last layer of BERT. There is not a substantial difference between words with different synsets, which supports our\n8We look up WordNet synsets using the Python NLTK library (Bird et al., 2009).\nconclusion that BERT successfully captures the semantics of aligned same words in paraphrases. We do see a difference between aligned and unaligned words. Aligned words peak at a high cosine similarity, while unaligned words roughly follow a normal distribution centered around 0.5. Note that for unaligned words with two or three synsets, there is not enough data to draw conclusions about the cosine similarity distributions. Overall, these plots show that even highly polysemous aligned same words have very similar embeddings in the context of a paraphrase.\nConclusion: How polysemous a word is does not substantially impact BERT’s ability to consistently represent it."
    }, {
      "heading" : "3.3 Contextualization in BERT Layers",
      "text" : "In this section, we consider how context-specific the embeddings in a paraphrase are. Ethayarajh (2019) showed that BERT word embeddings are more context-specific in higher layers. They measure this using the self-similarity of words, defined as the average cosine similarity between a word’s contextualized representations across its unique contexts, and show that self-similarity consistently decreases with higher layers of BERT, indicating that the contextualization of words is increasing.\nWe compare this observation to the paraphrase setting that we have been exploring in this paper. Because there are only two phrases in a paraphrase, we cannot implement the full self-similarity metric. Instead, we measure the cosine similarity between two words aligned in a paraphrase, shown in Figure 6. This revised metric measures is similar to self-similarity.\nWe see two trends in Figure 6. The first, decreasing cosine similarity, is seen with same words, whether aligned or unaligned, and is similar to what Ethayarajh (2019) report with decreasing selfsimilarity scores. This trend is stronger with unaligned words than with aligned words, indicating the model is capturing the fact that while these words have the same form, they are being used differently. The second trend that we see is the opposite, increasing cosine similarity, and we see this trend with different words, both aligned and unaligned. This indicates decreasing contextualization.\nConclusion: As seen in prior work, the standard way of using vectors from BERT’s layers does not capture the same level of contextualization in all\nlayers. However, in contrast to prior work, when controlling for semantics of the context, it seems that later layers are capturing more of the context, appropriately making words less similar when they are being used in different ways."
    }, {
      "heading" : "4 Conclusion",
      "text" : "Paraphrases with word alignments are a useful tool for studying the behavior of contextual language models. In this paper, we used them to study several contextual models, with a particular focus on BERT. Where possible, we compare our results with prior work, finding patterns that are consistent with the literature. One exception is that we found that words in a sentence are more similar to each other in later layers of BERT, in contrast to prior work that did not control for meaning using paraphrases. Meanwhile, this method opens up new opportunities, such as the comparison of aligned and unaligned, same and different words, which shows the sensitivity of these models to the specific word used. Paraphrases have the potential to inform exploration of other representation methods, showing which way of using the output of language models most accurately captures semantics consistently. We hope our findings will inform future work on contextualized models, and the applications that rely on them."
    }, {
      "heading" : "A Extra breakdowns of results",
      "text" : "Table 6 presents an expanded version of Table 2, with results for each layer of BERT. All layers perform better with longer paraphrases, but the improvement is largest for the last layer\nFigure 7 shows the specific values for similarity broken down by distance apart of words in the phrases. This shows the pattern of decreasing similarity as words are further away."
    } ],
    "references" : [ {
      "title" : "Paraphrasing with bilingual parallel corpora",
      "author" : [ "Colin Bannard", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association 8",
      "citeRegEx" : "Bannard and Callison.Burch.,? 2005",
      "shortCiteRegEx" : "Bannard and Callison.Burch.",
      "year" : 2005
    }, {
      "title" : "Natural language processing with Python: Analyzing text with the natural language toolkit",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "O’Reilly Media, Inc.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "BERT for question generation",
      "author" : [ "Ying-Hong Chan", "Yao-Chung Fan." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation, pages 173–177, Tokyo, Japan. Association for Computational Linguistics.",
      "citeRegEx" : "Chan and Fan.,? 2019",
      "shortCiteRegEx" : "Chan and Fan.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "WordNet",
      "author" : [ "Christiane Fellbaum." ],
      "venue" : "Wiley Online Library.",
      "citeRegEx" : "Fellbaum.,? 1998",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 1998
    }, {
      "title" : "PPDB: The paraphrase database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "BERT knows punta cana is not just beautiful, it’s gorgeous: Ranking scalar adjectives with contextualised representations",
      "author" : [ "Aina Garí Soler", "Marianna Apidianaki." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Soler and Apidianaki.,? 2020",
      "shortCiteRegEx" : "Soler and Apidianaki.",
      "year" : 2020
    }, {
      "title" : "English gigaword",
      "author" : [ "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda." ],
      "venue" : "Linguistic Data Consortium, Philadelphia, 4(1):34.",
      "citeRegEx" : "Graff et al\\.,? 2003",
      "shortCiteRegEx" : "Graff et al\\.",
      "year" : 2003
    }, {
      "title" : "GlossBERT: BERT for word sense disambiguation with gloss knowledge",
      "author" : [ "Luyao Huang", "Chi Sun", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "AlexUAUX-BERT at SemEval-2020 task 3: Improving BERT contextual similarity using multiple auxiliary contexts",
      "author" : [ "Somaia Mahmoud", "Marwan Torki." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 270–274,",
      "citeRegEx" : "Mahmoud and Torki.,? 2020",
      "shortCiteRegEx" : "Mahmoud and Torki.",
      "year" : 2020
    }, {
      "title" : "What do you mean, BERT? Assessing BERT as a distributional semantics model",
      "author" : [ "Timothee Mickus", "Denis Paperno", "Mathieu Constant", "Kees van Deemeter." ],
      "venue" : "Proceedings of the Society for Computation in Linguistics, 3.",
      "citeRegEx" : "Mickus et al\\.,? 2020",
      "shortCiteRegEx" : "Mickus et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Tomas Mikolov", "Quoc V Le", "Ilya Sutskever." ],
      "venue" : "arXiv preprint arXiv:1309.4168.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification",
      "author" : [ "Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch" ],
      "venue" : null,
      "citeRegEx" : "Pavlick et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2015
    }, {
      "title" : "Multiview LSA: Representation learning via generalized CCA",
      "author" : [ "Pushpendre Rastogi", "Benjamin Van Durme", "Raman Arora." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Rastogi et al\\.,? 2015",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2015
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "arXiv preprint arXiv:2002.12327.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "On the relevance of syntactic and discourse features for author profiling and identification",
      "author" : [ "Juan Soler-Company", "Leo Wanner." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Soler.Company and Wanner.,? 2017",
      "shortCiteRegEx" : "Soler.Company and Wanner.",
      "year" : 2017
    }, {
      "title" : "Correlation calculated from faulty data",
      "author" : [ "Charles Spearman." ],
      "venue" : "British Journal of Psychology, 19041920, 3(3):271–295.",
      "citeRegEx" : "Spearman.,? 1910",
      "shortCiteRegEx" : "Spearman.",
      "year" : 1910
    }, {
      "title" : "Social identity and punctuation variation in the #bluelivesmatter and #blacklivesmatter twitter communities",
      "author" : [ "Rachael Tatman", "Amandalynne Paullada." ],
      "venue" : "The 33rd Northwest Linguistics Conference.",
      "citeRegEx" : "Tatman and Paullada.,? 2017",
      "shortCiteRegEx" : "Tatman and Paullada.",
      "year" : 2017
    }, {
      "title" : "Parallel data, tools and interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 2214–2218, Istanbul, Turkey. European Language Resources Association",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Learning the curriculum with Bayesian optimization for taskspecific word representation learning",
      "author" : [ "Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Brian MacWhinney", "Chris Dyer." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association",
      "citeRegEx" : "Tsvetkov et al\\.,? 2016",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2016
    }, {
      "title" : "Does BERT make any sense? Interpretable word sense disambiguation with contextualized embeddings",
      "author" : [ "Gregor Wiedemann", "Steffen Remus", "Avi Chawla", "Chris Biemann." ],
      "venue" : "Konferenz zur Verarbeitung natürlicher Sprache / Conference on Natu-",
      "citeRegEx" : "Wiedemann et al\\.,? 2019",
      "shortCiteRegEx" : "Wiedemann et al\\.",
      "year" : 2019
    }, {
      "title" : "Self-attention based model for punctuation prediction using word and speech embeddings",
      "author" : [ "J. Yi", "J. Tao." ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing, pages 7270–7274.",
      "citeRegEx" : "Yi and Tao.,? 2019",
      "shortCiteRegEx" : "Yi and Tao.",
      "year" : 2019
    }, {
      "title" : "Fine-grained propaganda detection with fine-tuned BERT",
      "author" : [ "Shehel Yoosuf", "Yin Yang." ],
      "venue" : "Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, pages 87–",
      "citeRegEx" : "Yoosuf and Yang.,? 2019",
      "shortCiteRegEx" : "Yoosuf and Yang.",
      "year" : 2019
    }, {
      "title" : "Assessing phrasal representation and composition in transformers",
      "author" : [ "Lang Yu", "Allyson Ettinger." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4896–4907.",
      "citeRegEx" : "Yu and Ettinger.,? 2020",
      "shortCiteRegEx" : "Yu and Ettinger.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Contextualized embedding algorithms, such as BERT (Devlin et al., 2019), have achieved impressive performance on a wide variety of tasks (Huang et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : ", 2019), have achieved impressive performance on a wide variety of tasks (Huang et al., 2019; Chan and Fan, 2019; Yoosuf and Yang, 2019).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : ", 2019), have achieved impressive performance on a wide variety of tasks (Huang et al., 2019; Chan and Fan, 2019; Yoosuf and Yang, 2019).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 27,
      "context" : ", 2019), have achieved impressive performance on a wide variety of tasks (Huang et al., 2019; Chan and Fan, 2019; Yoosuf and Yang, 2019).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "One application of BERT is as a measure of sentence similarity (Zhang et al., 2019; Sellam et al., 2020), based on the assumption that BERT will produce similar representations for the words in two sentences with similar semantics.",
      "startOffset" : 63,
      "endOffset" : 104
    }, {
      "referenceID" : 28,
      "context" : "Previously, paraphrases have been used to probe whether compositionality is accurately captured by BERT (Yu and Ettinger, 2020), but we believe they can be used to explore many other questions.",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "Using the Paraphrase Database (Pavlick et al., 2015), we explore how consistent contextual representations are when controlling for the semantics of Figure 1: Example paraphrase from the PPDB with word alignment and word cosine similarities using the last layer of BERT.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "BERT tend to cluster according to word senses (Wiedemann et al., 2019), though this varies somewhat based on the position of a word in a sentence (Mickus et al.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : ", 2019), though this varies somewhat based on the position of a word in a sentence (Mickus et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "The final BERT layers also produce more contextualized word embeddings than the earlier layers (Ethayarajh, 2019), a finding we revisit using paraphrases in Section 3.",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "We use the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015), a database of paraphrases collected using bilingual pivoting, the process of taking a particular English phrase, looking at all the foreign language phrases it can be translated into, finding all occurrences of these foreign language phrases, and then translating them back into English (Bannard and Callison-Burch, 2005).",
      "startOffset" : 38,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "We use the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015), a database of paraphrases collected using bilingual pivoting, the process of taking a particular English phrase, looking at all the foreign language phrases it can be translated into, finding all occurrences of these foreign language phrases, and then translating them back into English (Bannard and Callison-Burch, 2005).",
      "startOffset" : 38,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : ", 2015), a database of paraphrases collected using bilingual pivoting, the process of taking a particular English phrase, looking at all the foreign language phrases it can be translated into, finding all occurrences of these foreign language phrases, and then translating them back into English (Bannard and Callison-Burch, 2005).",
      "startOffset" : 296,
      "endOffset" : 330
    }, {
      "referenceID" : 21,
      "context" : "Agreement is measured using Spearman’s ρ (Spearman, 1910); the average ρ between two workers is 0.",
      "startOffset" : 41,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "The input to the model consists of 209 hand-crafted paraphrase features, including WordNet features (Fellbaum, 1998), distributional similarity features, and cosine similarities of generated Multiview Latent Semantic Analysis embeddings (Rastogi et al.",
      "startOffset" : 100,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "The input to the model consists of 209 hand-crafted paraphrase features, including WordNet features (Fellbaum, 1998), distributional similarity features, and cosine similarities of generated Multiview Latent Semantic Analysis embeddings (Rastogi et al., 2015).",
      "startOffset" : 237,
      "endOffset" : 259
    }, {
      "referenceID" : 13,
      "context" : "We compare BERT to a more traditional embedding method, the continuous bag-of-words approach in word2vec (w2v) (Mikolov et al., 2013).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "5 sentences) in Gigaword (Graff et al., 2003; Rush et al., 2015)6 and OpenSubtitles (Tiedemann, 2012)7 that contain that phrase.",
      "startOffset" : 25,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "5 sentences) in Gigaword (Graff et al., 2003; Rush et al., 2015)6 and OpenSubtitles (Tiedemann, 2012)7 that contain that phrase.",
      "startOffset" : 25,
      "endOffset" : 64
    }, {
      "referenceID" : 23,
      "context" : ", 2015)6 and OpenSubtitles (Tiedemann, 2012)7 that contain that phrase.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "This supports observations in prior work (Mickus et al., 2020), but now with the knowledge that the overall context has the same meaning.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 20,
      "context" : "Punctuation plays an important role in distinguishing between different types of text, such as texts by different authors (Soler-Company and Wanner, 2017) or texts produced by different Twitter communities (Tatman and Paullada, 2017).",
      "startOffset" : 122,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "Punctuation plays an important role in distinguishing between different types of text, such as texts by different authors (Soler-Company and Wanner, 2017) or texts produced by different Twitter communities (Tatman and Paullada, 2017).",
      "startOffset" : 206,
      "endOffset" : 233
    }, {
      "referenceID" : 26,
      "context" : "Embeddings are used to generate punctuation for text that is lacking punctuation, such as recorded transcripts (Yi and Tao, 2019).",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 25,
      "context" : "Previous work has shown that BERT embeddings form clusters based on word senses (Wiedemann et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "We look up WordNet synsets using the Python NLTK library (Bird et al., 2009).",
      "startOffset" : 57,
      "endOffset" : 76
    } ],
    "year" : 0,
    "abstractText" : "We use paraphrases as a unique source of data to analyze contextualized embeddings, with a particular focus on BERT. Because paraphrases naturally encode consistent word and phrase semantics, they provide a unique lens for investigating properties of embeddings. Using the Paraphrase Database’s alignments, we study words within paraphrases as well as phrase representations. We find that contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different representations in many cases. We confirm previous findings that BERT is sensitive to word order, but find slightly different patterns than prior work in terms of the level of contextualization across BERT’s layers.",
    "creator" : null
  }
}
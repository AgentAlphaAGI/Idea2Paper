{
  "name" : "ARR_2022_244_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Vision-language tasks have been studied to build intelligent systems that can perceive information from multiple modalities, such as images, videos, and text. Extended from image-grounded tasks, e.g. (Antol et al., 2015), recently Jang et al. (2017); Lei et al. (2018) propose to use video as the grounding features. This modification poses a significant challenge to previous image-based models with the additional temporal variance through video frames. Recently Alamri et al. (2019) further develop videogrounded language research into the dialogue domain. In the proposed task, video-grounded dialogues, the dialogue agent is required to answer questions about a video over multiple dialogue turns. Using Figure 1 as an example, to answer\nquestions correctly, a dialogue agent has to resolve references in dialogue context, e.g. “he” and “it”, and identify the original entity, e.g. “a boy\" and “a backpack\". Besides, the agent also needs to identify the actions of these entities, e.g. “carrying a backpack” to retrieve information from the video.\nCurrent state-of-the-art approaches to videogrounded dialogue tasks, e.g. (Le et al., 2019b; Fan et al., 2019) have achieved remarkable performance through the use of deep neural networks to retrieve grounding video signals based on language inputs. However, these approaches often assume the reasoning structure, including resolving references of entities and detecting the corresponding actions to retrieve visual cues, is implicitly learned. An explicit reasoning structure becomes more beneficial as the tasks complicate in two scenarios: video with complex spatial and temporal dynamics, and language inputs with sophisticated semantic dependencies, e.g. questions positioned in a dialogue context. These scenarios often challenge researchers to interpret model hidden layers, identify errors, and assess model reasoning capability.\nSimilar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues (Agrawal et al., 2016; Goyal et al., 2017; Feng et al., 2018; Serrano and Smith, 2019). Andreas et al. (2016b) propose neu-\nral module networks (NMNs) by decomposing a question into sub-sequences called program and assembling a network of neural operations. Motivated by this line of research, we propose a new approach, VGNMN, to video-grounded language tasks. Our approach benefits from integrating neural networks with a compositional reasoning structure to exploit low-level information signals in video. An example of the reasoning structure can be seen on the right side of Figure 1.\nVideo-grounded Neural Module Network (VGNMN) tackles video understanding through action and entity-paramterized NMNs to retrieve video features. We first decompose question into a set of entities and extract video features related to these entities. VGNMN then extracts the temporal steps by focusing on relevant actions that are associated with these entities. VGNMN is analogous to how human processes information by gradually retrieving signals from input modalities using a set of discrete subjects and their actions.\nTo tackle dialogue understanding, VGNMN is trained to resolve any co-reference in language inputs, e.g. questions in a dialogue context, to identify the unique entities in each dialogue. Previous approaches to video-grounded dialogues often obtain question global representations in relation to dialogue context. These approaches might be suitable to represent general semantics in open-domain dialogues (Serban et al., 2016). However, they are not ideal to detect fine-grained information in a video-grounded dialogue which frequently entails dependencies between questions and past dialogue turns in the form of entity references.\nIn summary, our contributions include:\n• VGNMN, a neural module network-based approach for video-grounded dialogues.\n• The approach includes a modularized system that creates a reasoning pipeline parameterized by entity and action-based representations from both dialogue and video contexts.\n• Our experiments are conducted on the challenging benchmark for video-grounded dialogues, Audio-visual Scene-Aware Dialogues (AVSD) (Alamri et al., 2019) as well as TGIFQA (Jang et al., 2017) for video QA task.\n• Our results indicate strong performance of VGNMN as well as improved model interpretability and robustness to difficult scenarios of dialogues, videos, and question structures."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Video-Language Understanding",
      "text" : "The research of video-language understanding aims to develop a model’s joint understanding capability of language, video, and their interactions. Jang et al. (2017); Gao et al. (2018); Jiang et al. (2020) propose to learn attention guided by question global representation to retrieve spatial-level and temporal-level visual features. Li et al. (2019); Fan et al. (2019); Jiang and Han (2020) model interaction between all pairs of question tokenlevel representations and temporal-level features of the input video through similarity matrix, memory networks, and graph networks respectively. Gao et al. (2019); Le et al. (2019c, 2020b); Lei et al. (2020); Huang et al. (2020) extends the previous approach by dividing a video into equal segments, sub-sampling video frames, or considering objectlevel representations of input video. We propose to replace token-level and global question representations with question representations composed of specific entities and actions.\nRecently, we have witnessed emerging techniques in video-language systems that exploit deep transformer-based architectures such as BERT (Devlin et al., 2019) for pretraining multimodal representations (Li et al., 2020a; Yang et al., 2020; Kim et al., 2021; Tang et al., 2021; Lei et al., 2021; Zellers et al., 2021) in very large-scale videolanguage datasets. While these systems can achieve impressive performance, they are not straightforward to apply in domains with limited data such as video-grounded dialogues. Moreover, as we shown in our qualitative examples, our approach facilitates better interpretability through the output of decoded functional programs."
    }, {
      "heading" : "2.2 Video-grounded Dialogues",
      "text" : "Extended from video QA, video-grounded dialogue is an emerging task that combines dialogue response generation and video-language understanding research. This task entails a novel requirement for models to learn dialogue semantics and decode entity co-references in questions. Nguyen et al. (2018); Hori et al. (2019); Hori et al. (2019); Sanabria et al. (2019); Le et al. (2019a,b) extend traditional QA models by adding dialogue history neural encoders. Kumar et al. (2019) enhances dialogue features with topic-level representations to express the general topic in each dialogue. Schwartz et al. (2019) treats each dialogue\nturn as an independent sequence and allows interaction between questions and each dialogue turn. Le et al. (2019b) encodes dialogue history as a sequence with embedding and positional representations. Different from prior work, we dissect the question sequence and explicitly detect and decode any entities and their references. Our approach also enables insights on how models extract deductive bias from dialogues to extract video information."
    }, {
      "heading" : "2.3 Neural Module Network",
      "text" : "Neural Module Network (NMN) (Andreas et al., 2016b,a) is introduced to address visual QA by decomposing questions into linguistic sub-structures, known as programs, to instantiate a network of neural modules. NMN models have achieved success in synthetic image domains where a multi-step reasoning process is required (Johnson et al., 2017b; Hu et al., 2018; Han et al., 2019). Yi et al. (2018); Han et al. (2019); Mao et al. (2019) improve NMN models by decoupling visual-language understanding and visual concept learning. Our work is related to the recent work (Kottur et al., 2018; Jiang and Bansal, 2019; Gupta et al., 2020) that extended NMNs to image reasoning in dialogues and reading comprehension reasoning. Our approach follows the previous approaches that learn to generate program structure and require no parser at evaluation time. Compared to prior work, we use NMN to learn dependencies between the composition in language inputs and the spatio-temporal dynamics in videos. Specifically, we propose to construct a reasoning structure from text, from which detected entities are used to extract visual information in the spatial space and detected actions are used to find visual information in the temporal space."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we present the design of our model. An overview of the model can be seen in Figure 2."
    }, {
      "heading" : "3.1 Task Definition",
      "text" : "The input to the model consists of a dialogue D which is grounded on a video V . The input components include the question of current dialogue turn Q, dialogue history H, and the features of the input video, including visual and audio input. The output is a dialogue response, denoted as R. Each text input component is a sequence of words w1, ..., wm ∈ Vin, the input vocabulary. Similarly, the output responseR is a sequence of tokens\nw1, ..., wn ∈ Vout, the output vocabulary. The objective of the task is the generation objective that output answers of the current dialogue turn t:\nR̂t = argmax Rt P (Rt|V,Ht,Qt; θ)\n= argmax Rt LR∏ n=1 Pm(wn|Rt,1:n−1,V,Ht,Qt; θ)\nIn a Video-QA task, the dialogue historyH is simply absent and the output response is typically collapsed to a single-token response."
    }, {
      "heading" : "3.2 Encoders",
      "text" : "Text Encoder. A text encoder is shared to encode text inputs, including dialogue history, questions, and captions. The text encoder converts each text sequence X = w1, ..., wm into a sequence of embeddings X ∈ Rm×d. We use a trainable embedding matrix to map token indices to vector representations of d dimensions through a mapping function φ. These vectors are then integrated with ordering information of tokens through a positional encoding function with layer normalization (Ba et al., 2016; Vaswani et al., 2017). The embedding and positional representations are combined through element-wise summation. The encoded dialogue history and question of the current turn are defined as H = Norm(φ(H) + PE(H)) ∈ RLH×d and Q = Norm(φ(Q) + PE(Q)) ∈ RLQ×d.\nVideo Encoder. To encode video, we use pretrained models to extract visual and audio features. We denote F as the sampled video frames or video clips. For object-level visual features, we denote O as the maximum number of objects considered in each frame. The resulting output from a pretrained object detection model is Zobj ∈ RF×O×dvis . We concatenate each object representation with the corresponding coordinates projected to dvis dimensions. We also make use of a CNN-based pretrained model to obtain features of temporal dimension Zcnn ∈ RF×dvis . The audio feature is\nobtained through a pretrained audio model, Zaud ∈ RF×daud . We passed all video features through a linear transformation layer with ReLU activation to the same embedding dimension d."
    }, {
      "heading" : "3.3 Neural Modules",
      "text" : "We introduce neural modules that are used to assemble an executable program constructed by the generated sequence from question parsers. We provide an overview of neural modules in Table 1 and demonstrate dialogue understanding and video understanding modules in Figure 3 and 4 respectively. Each module parameter, e.g. “a backpack”, is extracted from the parsed program (See Section 3.4). For each parameter, we denote P ∈ Rd as the average pooling of component token embeddings. find(P,H)→Hent. This module handles entity tracing by obtaining a distribution over tokens in the dialogue history. We use an entity-todialogue-history attention mechanism applied from an entity Pi to all tokens in the dialogue history. Any neural network that learn to generate attention between two tensors is applicable .e.g. (Bahdanau et al., 2015; Vaswani et al., 2017). The attention matrix normalized by softmax, Afind,i ∈ RLH , is used to compute the weighted sum of dialogue history token representations. The output is combined with entity embedding Pi to obtain contextual entity representation Hent,i ∈ Rd. summarize(Hent,Q)→Qctx. For each contextual entity representation Hent,i, i = 1, ..., Nent, it is projected to LQ dimensions and is combined with question token embeddings through elementwise summation to obtain entity-aware question representation Qent,i ∈ RLQ×d. It is fed to a onedimensional CNN with max-pooling layer (Kim, 2014) to obtain a contextual entity-aware question representation. We denote the final output as Qctx ∈ RNent×d.\nWhile previous models usually focus on global or token-level dependencies (Hori et al., 2019; Le et al., 2019b) to encode question features, our modules compress fine-grained question represen-\ntations at the entity level. Specifically, find and summarize modules can generate entitydependent local and global representations of question semantics. We show that our modularized approach can achieve better performance and transparency than traditional approaches to encode dialogue context (Serban et al., 2016; Vaswani et al., 2017) (Section 4). where(P,V)→Vent. Similar to the find module, this module handles entity-based attention to the video input. However, the entity representation P , in this case, is parameterized by the original entity in dialogue rather than in question (See Section 3.4 for more description). Each entity Pi is stacked to match the number of sampled video frames/clips F . An attention network is used to obtain entity-to-object attention matrix Awhere,i ∈ RF×O. The attended feature are compressed through weighted sum pooling along the spatial dimension, resulting in Vent,i ∈ RF×d, i = 1, ..., Nent. when(P,Vent)→Vent+act. This module follows a similar architecture as the where module. However, the action parameter Pi is stacked to match Nent dimensions. The attention matrix Awhen,i ∈ RF is then used to compute the visual entity-action representations through weighted sum along the temporal dimension. We denote the output for all actions Pi as Vent+act ∈ RNent×Nact×d describe(P,Vent+act)→Vctx. This module is a linear transformation to compute Vctx = Wdesc\nT [Vent+act;Pstack] ∈ RNent×Nact×d where Wdesc ∈ R2d×d, Pstack is the stacked representations of parameter embedding P to Nent ×Nact dimensions, and [; ] is the concatenation operation. Note that the parameter P here is extracted from questions, often as the type of questions e.g. “what” and “how”. This eliminates the need to have different modules for different question types. However, we noted the current design may be challenged in rare cases in which an utterance contain numerous questions (refer to Figure 5).\nThe exist module is used when the questions\nare “yes/no” questions. This module is a special case of describe module where the parameter P is simply the average pooled question embeddings. The above where module is applied to object-level features. For temporal-based features such as CNN-based and audio features, the same neural operation is applied along the temporal dimension. Each resulting entity-aware output is then incorporated to frame-level features through element-wise summation.\nAn advantage of our architecture is that it separates dialogue and video understanding. We adopt a transparent approach to solve linguistic entity references during the dialogue understanding phase. The resolved entities are fed to the video understanding phase to learn entity-action dynamics in the video. We show that our approach is robust when dialogue evolves to many turns and video extends over time (Please refer to Section 4)."
    }, {
      "heading" : "3.4 Question Parsers",
      "text" : "To learn compositional programs, we follow (Johnson et al., 2017a; Hu et al., 2017) and consider program generation as a sequence-tosequence task. We adopt a simple template “〈param1〉〈module1〉〈param2〉〈module2〉...” as the target sequence. The resulting target sequences for dialogue and video understanding programs are sequences Pdial and Pvid respectively.\nThe parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding. Each parser is a vanilla Transformer decoder, including multi-head attention layers on questions and past dialogue turns (Please refer to Appendix A.1 for more technical details)."
    }, {
      "heading" : "3.5 Response Decoder",
      "text" : "System response is decoded by incorporating the dialogue context and video context outputs from the corresponding reasoning programs to target token representations. We follows a vanilla Transformer decoder architecture (Le et al., 2019b), which consists of 3 attention layers: self-attention to attend on existing tokens, attention to Qctx from dialogue understanding program execution, and attention to Vctx from video understanding program execution.\nA(1)res = Attention(R| j−1 0 , R| j−1 0 , R| j−1 0 ) ∈ R j×d A(2)res = Attention(A (1) res , Qctx, Qctx) ∈ Rj×d A(3)res = Attention(A (2) res , Vctx, Vctx) ∈ Rj×d\nMultimodal Fusion. For video features come from multiple modalities, visual and audio, the contextual features, denoted Vctx, is obtained through a weighted sum of component modalities, e.g. contextual visual features V visctx and contextual audio features V audctx . The scores Sfusion to compute the weighted sum is defined as:\nSfusion = Softmax(W T fusion[Qstack;V vis ctx ;V aud ctx ])\nwhere Qstack is the mean pooling output of question embeddings Q which is then stacked to Nent + Nact dimensions, and Wfusion ∈ R3d×2 are trainable model parameters. The resulting Sfusion has a dimension of ∈ R(Nent+Nact)×2.\nResponse Generation. To generate response sequences, a special token “_sos” is concatenated as the first tokenw0. The decoded tokenw1 is then appended to w0 as input to decode w2 and so on. Similarly to input source sequences, at decoding time step j, the input target sequence is encoded to obtain representations of system response R|j−10 . We\ncombine vocabulary of input and output sequences and share the embedding matrixE ∈ R|V|×d where V = Vin ∩ Vout. During training time, we directly use the ground-truth responses as input to the decoder and optimize VGNMN with a cross-entropy loss to decode the next ground-truth tokens. During test time, responses are generated auto-regressively through beam search with beam size 5. Note that we apply the same procedure to generate reasoning programs from question parsers."
    }, {
      "heading" : "4 Experiments",
      "text" : "Experimental Setup and Training We use the AVSD benchmark from the Dialogue System Technology Challenge 7 (DSTC7) (Hori et al., 2019). The benchmark consists of dialogues grounded on the Charades videos (Sigurdsson et al., 2016). Each dialogue contains up to 10 dialogue turns, each turn consists of a question and expected response about a given video. For visual features, we use the 3D CNN-based features from a pretrained I3D model (Carreira and Zisserman, 2017) and object-level features from a pretrained FasterRNN model (Ren et al., 2015b). The audio features are obtained from a pretrained VGGish model (Hershey et al., 2017). In the experiments with AVSD, we consider two settings: one with video summary and one without video summary as input. In the setting with video summary, the summary is concatenated to the dialogue history before the first dialogue turn. We optimize models by joint training to minimize the cross-entropy losses to generate responses and functional programs. We also adapt VGNMN to the video QA benchmark TGIF-QA (Jang et al., 2017). For more details of data and training, please refer to Appendix B and C.\nAVSD Results. We evaluate model performance by the objective metrics, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015), between each generated response and 6 reference gold responses. As seen in Table 2, our models outperform most of existing approaches. We observed that our approach did not outperform the GPT-based baselines (Li et al., 2020b; Le and Hoi, 2020) in the setting that allows video summary/caption input However, the performance of our model in the setting without video summary/caption input is on par with the GPT-based baseline (Li et al., 2020b), even though our model did not rely on deep pretrained represen-\ntations on large-scale text data. These observations imply that GPT-based models can better capture video context from video caption/summary through rich pretrained representations. However, without access to video caption/summary, these models may fail to understand video from visual-only representations. In this setting, GPT-based models may be inferior to VGNMN, which explicitly exploits the compositional structures from textual inputs to integrate visual features. We also found that VGNMN applied to object-level features is competitive to the model applied to CNN-based features. The flexibility of VGNMN neural programs show when we treat the caption as an input equally to visual or audio inputs and execute entity-action level neural operations on the encoded caption sequence.\nRobustness. To evaluate model robustness, we report BLEU4 and CIDEr of model variants in various experimental settings. Specifically, we compare against performance of output responses in the first dialogue turn position (i.e. 2nd-10th turn vs. the 1st turn), or responses grounded on the shortest video length range (video ranges are intervals of 0-10th, 10-20th percentile and so on). We report results of the following model variants: (1) w/o video NMN: VGNMN without using video-based\nVideo (seconds) BLEU4 CIDEr VGNMN (1) VGNMN (1)\n1-23 0.432 0.447 1.298 1.355 23-28 0.436 0.433 1.264 1.165 28-30 0.398 0.376 1.203 1.164\n30-30.6 0.441 0.418 1.220 1.202 30.6-31 0.413 0.411 1.250 1.166 31-31.6 0.439 0.451 1.249 1.295 31.6-32 0.430 0.419 1.217 1.192 32-33 0.468 0.445 1.343 1.237 33-37 0.388 0.381 1.149 1.124 37-75 0.356 0.365 0.910 0.962\n(a) Performance by video length between VGNMN and variant (1) (w/o video NMN).\nDial. Turn BLEU4 CIDEr (1) (2) (1) (2)\n1 0.579 0.587 1.623 1.650 2 0.429 0.430 1.155 1.142 3 0.275 0.289 0.867 0.846 4 0.309 0.305 0.859 0.855 5 0.355 0.335 1.088 1.023 6 0.357 0.329 1.044 0.950 7 0.342 0.325 0.896 0.847 8 0.361 0.332 1.025 0.973 9 0.383 0.431 1.043 1.182 10 0.395 0.371 0.931 0.977\n(b) Performance by dialogue turn between variants (1) (w/o video NMN) and (2) (no NMN)\nQuestion structure BLEU-4 CIDEr VGNMN (2) VGNMN (2)\nYes/No 0.474 0.447 1.306 1.244 Wh- 0.266 0.265 0.706 0.699 How 0.636 0.663 1.817 1.878\nOthers 0.287 0.318 0.701 0.768 1Sent+Que 0.374 0.357 0.854 0.822 2Sent+Que 0.303 0.225 0.554 0.487 >2Sent+Que 0.000 0.000 0.000 0.000 2SubQue 0.196 0.180 0.489 0.460 3SubQue 0.332 0.000 0.653 0.112\n(c) Performance by utterance structures between VGNMN and variant (2) (no NMN): Single-question utterances (Top) vs. Multipart utterances (Bottom).\nTable 3: VGNMN and model variants by configurations of dialogues, videos, and question structures\nmodules, e.g. when and where. Video features are retrieved through a token-level representation of questions (Le et al., 2019b). (2) no NMN: (1) + without dialogue-based modules, e.g. find and summarize. Dialogue history is encoded by a hierarchical LSTM encoder (Hori et al., 2019).\nRobustness to video length: In Table 3a, we noted that the performance gap between VGNMN and (1) is quite distinct, with 7/10 cases of video ranges in which VGNMN outperforms. However, in lower ranges (i.e. 1-23 seconds) and higher ranges (37-75 seconds), VGNMN performs not as well as model (1). We observed that related factors might affect the discrepancy, such as the complexity of the questions for these short and long-range videos. Potentially, our question parser for the video understanding program needs to be improved (e.g. for tree-based programs) to retrieve information in these ranges.\nRobustness to dialogue turn: In Table 3b, we observed that model (1) performs better than model (2) overall, especially in higher turn positions, i.e. from the 4th turn to 8th turn. Interestingly, we noted some mixed results in very low turn position, i.e. the 2nd and 3rd turn, and very high turn position, i.e. the 10th turn. Potentially, with a large dialogue turn position, the neural-based approach such as hierarchical RNN can better capture the global dependencies within dialogue context than the entity-based compositional NMN method.\nRobustness to question structure: Finally, we compared performance of VGNMN with the noNMN variant (1) in different cases of question structures: single-question vs. multiple-part structure. In single-question structures, we examined by the question types (e.g. yes/no, wh-questions). In multi-part structures, we further classified whether there are sentences preceding the question (e.g. “1Sent+Que”) or there are smaller (sub-)questions\n(e.g. “2SubQue”) within the question. In Table 3c, we observed that VGNMN has clearer performance gains in multi-part structures than singlequestion structures. In multi-part structures, we observed higher gaps between VGNMN and model (1) in highly complex cases e.g. “2Sent+Que” vs. “1Sent+Que”. These observations indicate the robustness of VGNMN and the underlying compositionality principle to deal with complex question structures. We also noted that VGNMN is still susceptible to extremely long questions (“>2Sent+Que”) and future work is needed to address these scenarios.\nInterpretability. In Figure 5, we show both success and failure cases of generated responses and corresponding generated functional programs. In each example, we marked predicted outputs as incorrect if they do not match the ground-truth completely (even though the outputs might be partially correct). From Figure 5, we observe that in cases where generated dialogue programs and video programs match or are close to the gold labels, the model can generate generally correct responses. For cases where some module parameters do not exactly match but are closed to the gold labels, the model can still generate responses with the correct visual information (e.g. the 4th turn in example B). In cases of wrong predicted responses, we can further look at how the model understands the questions based on predicted programs. In the 3rd turn of example A, the output response is missing a minor detail as compared to the label response because the video program fails to capture “rooftop” as a where parameter. These subtle yet important details can determine whether output responses can fully address user queries. In the 3rd turn of example B, the model wrongly identifies “what room” as a where parameter and subsequently generates a wrong response that it is “a living room”.\nTGIF-QA Results. We report the result using the L2 loss in Count task and accuracy in other tasks. From Table 4, VGNMN outperforms the majority of the baseline models in all tasks by a large margin. Compared to AVSD experiments, the TGIF-QA experiments emphasize the video understanding ability of the models, removing the requirement for dialogue understanding and natural language generation. Since TGIF-QA questions follow a very specific question type distribution (count, action, transition, and frameQA), the question structures are simpler and easier to learn than AVSD. Using exact-match accuracy of parsed programs vs. label programs as a metric, our question parser can achieve a performance 81% to 94% accuracy in TGIF-QA vs. 41-45% in AVSD. The higher accuracy in decoding a reasoning structure translates to better adaptation between training and test time, resulting in higher performance gains.\nCascading Errors. Compared to prior approaches, we noted that VGNMN is a modularized system which may result in cascading errors to downstream modules. One major error is the error of generated programs which is used as pa-\nrameters in neural modules. To gauge this error, we compare the performance of VGNMN between 2 cases: with generated programs and with groundtruth programs. From Table 5, we noticed some performance gaps between these cases. These observations imply that: (1) program generations and response generations are positively correlated and more accurate programs can lead to better responses; and (2) current question parsers are not perfect, resulting in wrong parameters to instantiate neural modules. Future work may focus on learning better question parsers or directly deploying a better off-the-shelf parser tool.\nFor additional experiment results, qualitative samples, and analysis between model variants, refer to Appendix D and E."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we introduce Video-grounded Neural Module Network (VGNMN). VGNMN consists of dialogue and video understanding neural modules, each of which performs entity and action-level operations on language and video components. Our comprehensive experiments on AVSD and TGIFQA benchmarks show that our models can achieve competitive performance while promoting a compositional and interpretable learning approach."
    }, {
      "heading" : "6 Broader Impacts",
      "text" : "During the duration of this work, there have been no ethical concerns regarding the model implementation, training, and testing. The data used in this work has been carefully reviewed and accordingly to the description from the original authors, we did not find any concerns on any significant biases. For any potential application or extension of this work, we would like to highlight some specific concerns. First, as the work is developed to build an intelligent dialogue agents, models should not be used with the intention to create fake human profiles for any harmful purposes (e.g. fishing or spreading fake news). For wider use of dialogue systems, the application of work might result in certain impacts to some stakeholders whose jobs may be affected by this application (e.g. customer service call agents). We hope any application should be carefully considered against these potential risks."
    }, {
      "heading" : "A Additional Model Details",
      "text" : "A.1 Question Parsers\nTo learn compositional programs, we follow (Johnson et al., 2017a; Hu et al., 2017) and consider program generation as a sequence-tosequence task. We adopt a simple template “〈param1〉〈module1〉〈param2〉〈module2〉...” as the target sequence. The resulting target sequences for dialogue and video understanding programs are sequences Pdial and Pvid respectively.\nThe parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding. Each parser is an attention-based Transformer decoder. The Transformer attention is a multi-head attention on query q, key k, and value v tensors, denoted as Attention(q, k, v). For each token in the q sequence , the distribution over tokens in the k\nsequence is used to obtain the weighted sum of the corresponding representations in the v sequence.\nAttention(q, k, v) = softmax( qkT√ dk )v ∈ RLq×dq\nEach attention is followed by a feed-forward network applied to each position identically. We exploit the multi-head and feed-forward architecture, which show good performance in NLP tasks such as NMT and QA (Vaswani et al., 2017; Dehghani et al., 2019), to efficiently incorporate contextual cues from dialogue components to parse question into reasoning programs. At decoding step 0, we simply use a special token _sos as the input to the parser. In each subsequent decoding step, we concatenate the prior input sequence with the generated token to decode in an auto-regressive manner. We share the vocabulary sets of input and output components and thus, use the same embedding matrix. Given the encoded question Q, to decode the program for dialogue understanding, the contextual signals are integrated through 2 attention layers: one attention on previously generated tokens, and the other on question tokens. At time step j, we denote the output from an attention layer as Adial,j.\nA (1) dial = Attention(Pdial| j−1 0 , Pdial| j−1 0 , Pdial| j−1 0 ) A (2) dial = Attention(A (1) dial, Q,Q) ∈ R j×d\nTo generate programs for video understanding, the contextual signals are learned and incorporated in a similar manner. However, to exploit dialogue contextual cues, the execution output of dialogue understanding neural modules Qctx is incorporated to each vector in Pdial through an additional attention layer. This layer integrates the resolved entity information to decode the original entities for video understanding. It is equivalent to a reasoning process that converts the question from its original multi-turn semantics to single-turn semantics.\nA (1) vid = Attention(Pvid| j−1 0 , Pvid| j−1 0 , Pvid| j−1 0 ) A (2) vid = Attention(A (1) vid, Q,Q) ∈ R j×d A (3) vid = Attention(A (2) vid, Qctx, Qctx) ∈ R j×d\nA.2 How to locate entities? Noted that in the neural modules described in Section 3.3, during training, we simply feed the ground-truth programs to optimize these modules. For instance, the neural module where received\nthe ground truth entities P which is then used to instantiate the neural network and retrieve from video V . During test time, we decode the programs token by token through the question parsers, and feed the predicted entities P̂ to neural modules. Note that we do not assume, and hence not train model to retrieve ground-truth locations of visual entities in videos. This strategy enables the applicability of VGNMN as we consider these entity annotations mostly unavailable in real-world systems."
    }, {
      "heading" : "B Additional Dataset Details",
      "text" : "Different from AVSD, TGIF-QA contains a diverse set of QA tasks:\n• Count: an open-ended task which counts the number of repetitions of an action\n• Action: a multiple-choice (MC) task which asks about a certain action occurring for a fixed number of times\n• Transition: an MC task which emphasizes temporal transition in video\n• Frame: an open-ended task which can be answered from visual contents of one of the video frames\nFor the TGIF-QA benchmark, we use the extracted features from a pretrained ResNet model (He et al., 2016)."
    }, {
      "heading" : "C Additional Training Procedure Details",
      "text" : "We follow prior approaches (Hu et al., 2017, 2018; Kottur et al., 2018) by obtaining the annotations of the programs through a language parser (Hu et al., 2016) and a reference resolution model (Clark and Manning, 2016). During training, we directly use these as ground-truth labels of programs to train our models. The ground-truth responses are augmented with label smoothing technique (Szegedy et al., 2016). During inference time, we generate\nall programs and responses from given dialogues and videos. We run beam search to enumerate programs for dialogue and video understanding and dialogue responses.\nWe use a training batch size of 32 and embedding dimension d = 128 in all experiments. Where Transformer attention is used, we fix the number of attention heads to 8 in all attention layers. In neural modules with MLP layers, the MLP network is fixed to 2 linear layers with a ReLU activation in between. In neural modules with CNN, we adopt a vanilla CNN architecture for text classification (without the last MLP layer) where the number of input channels is 1, the kernel sizes are {3, 4, 5}, and the number of output channels is d. We initialize models with uniform distribution (Glorot and Bengio, 2010). During training, we adopt the Adam optimizer (Kingma and Ba, 2015) and a decaying learning rate (Vaswani et al., 2017) where we fix the warm-up steps to 15K training steps. We employ dropout (Srivastava et al., 2014) of 0.2 at all networks except the last linear layers of question parsers and response decoder. We train models up to 50 epochs and select the best models based on the average loss per epoch in the validation set.\nAll models are trained in a V100 GPU with a capacity of 16GB. We approximated each training epoch took about 20 minutes to run. For each model experiment with VGNMN, we obtained at least 2 runs and reported the average results.\nOptimization. We optimize models by joint training to minimize the cross-entropy losses to generate responses and functional programs.\nL = αLdial + βLvid + Lres = α ∑ j − log(Pdial(Pdial,j))\n+ β ∑ l − log(Pvideo(Pvideo,l))\n+ ∑ n − log(Pres(Rn))\nwhere P is the probability distribution of an output token. The probability is computed by passing output representations from the parsers and decoder to a linear layer W ∈ Rd×V with softmax activation. We share the parameters between W and embedding matrix E."
    }, {
      "heading" : "D Additional Experimental Results",
      "text" : "D.1 Non-NMN Models We experiment with several Non-NMN based variants of our models. As can be seen in Table 7, our approach to video and dialogue understanding through compositional reasoning programs exhibits better performance than non-compositional approaches. Compared to the approaches that directly process frame-level features in videos (Row B) or token-level features in dialogues (Row C, D), our full VGNMN (Row A) considers entitylevel and action-level information extraction and thus, avoids unnecessary and possibly noisy extraction. Compared to the approaches that obtain dialogue contextual cues through a hierarchical encoding architecture (Row E, F) such as (Serban et al., 2016; Hori et al., 2019), VGNMN directly addresses the challenge of entity references in dialogues. As mentioned, we hypothesize that the hierarchical encoding architecture is more appropriate for less entity-sensitive dialogues such as chit-chat and open-domain dialogues.\nD.2 Dialogue context integration Experimenting with different ways to integrate dialogue context representations, we observe that adding an attention layer attending to question during response decoding (Row G) is not necessary. This can be explained as the representation Qctx obtained from dialogue understanding program already contains contextual information of both dialogue history and question and question input is no longer needed in the decoding phase. Furthermore, we investigate the model sensitivity to natural language generation through its ability to construct linguistically correct programs and responses. To generate responses that are linguistically appropriate, VGNMN needs dialogue context representation Qctx as input to the response decoder (Row H). The model also needs encoded question Q as input to the video understanding program parser to be able to decompose this sequence to entity and action module parameters (Row I).\nE Interpretability\nWe extract the predicted programs and responses for some example dialogues in Figure 6, 7, 8, and 9 and report our observations:\n• We observe that when the predicted programs are correct, the output responses generally\n# Model Variant generated programs ground-truth programsBLEU4 CIDEr BLEU4 CIDEr A Full VGNMN 0.421 1.171 0.423 1.167 B ↪→ video NMNs ∼ response-decoder-to-video attn. 0.415 1.159 - - C ↪→ dial. NMNs ∼ res-decoder-to-(history→question) attn. 0.412 1.151 - - :q!D ↪→ dial. NMNs ∼ res-decoder-to-concat(history+question) attn. 0.411 1.133 - - E ↪→ dial. NMNs ∼ HREDLSTM(history) + question attn. 0.414 1.153 - - F ↪→ dial. NMNs ∼ HREDGRU(history) + question attn. 0.415 1.138 - - G ↪→ + response-decoder-to-question attn. 0.424 1.166 0.426 1.164 H ↪→ - response-decoder-to-dialogue-context attn. 0.405 1.124 0.404 1.123 I ↪→ - video-understanding-prog-parser-to-question attn. 0.414 1.146 0.424 1.166"
    } ],
    "references" : [ {
      "title" : "Analyzing the behavior of visual question answering models",
      "author" : [ "Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1955–1960, Austin, Texas. Asso-",
      "citeRegEx" : "Agrawal et al\\.,? 2016",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2016
    }, {
      "title" : "Audio-visual scene-aware dialog",
      "author" : [ "Huda Alamri", "Vincent Cartillier", "Abhishek Das", "Jue Wang", "Stefan Lee", "Peter Anderson", "Irfan Essa", "Devi Parikh", "Dhruv Batra", "Anoop Cherian", "Tim K. Marks", "Chiori Hori." ],
      "venue" : "Proceedings of the IEEE Conference on",
      "citeRegEx" : "Alamri et al\\.,? 2019",
      "shortCiteRegEx" : "Alamri et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to compose neural networks for question answering",
      "author" : [ "Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-",
      "citeRegEx" : "Andreas et al\\.,? 2016a",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural module networks",
      "author" : [ "Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39–48.",
      "citeRegEx" : "Andreas et al\\.,? 2016b",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2425–2433.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Quo vadis, action recognition? a new model and the kinetics dataset",
      "author" : [ "Joao Carreira", "Andrew Zisserman." ],
      "venue" : "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299–6308.",
      "citeRegEx" : "Carreira and Zisserman.,? 2017",
      "shortCiteRegEx" : "Carreira and Zisserman.",
      "year" : 2017
    }, {
      "title" : "Multi-step joint-modality attention network for scene-aware dialogue system",
      "author" : [ "Yun-Wei Chu", "Kuan-Yen Lin", "Chao-Chun Hsu", "Lun-Wei Ku." ],
      "venue" : "DSTC Workshop @ AAAI.",
      "citeRegEx" : "Chu et al\\.,? 2020",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep reinforcement learning for mention-ranking coreference models",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2256–2262, Austin, Texas. Asso-",
      "citeRegEx" : "Clark and Manning.,? 2016",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Universal transformers",
      "author" : [ "Mostafa Dehghani", "Stephan Gouws", "Oriol Vinyals", "Jakob Uszkoreit", "Lukasz Kaiser." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dehghani et al\\.,? 2019",
      "shortCiteRegEx" : "Dehghani et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Heterogeneous memory enhanced multimodal attention model for video question answering",
      "author" : [ "Chenyou Fan", "Xiaofan Zhang", "Shu Zhang", "Wensheng Wang", "Chi Zhang", "Heng Huang." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pat-",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Pathologies of neural models make interpretations difficult",
      "author" : [ "Shi Feng", "Eric Wallace", "Alvin Grissom II", "Mohit Iyyer", "Pedro Rodriguez", "Jordan Boyd-Graber." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Feng et al\\.,? 2018",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "author" : [ "Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach." ],
      "venue" : "9",
      "citeRegEx" : "Fukui et al\\.,? 2016",
      "shortCiteRegEx" : "Fukui et al\\.",
      "year" : 2016
    }, {
      "title" : "Motion-appearance co-memory networks for video question answering",
      "author" : [ "Jiyang Gao", "Runzhou Ge", "Kan Chen", "Ram Nevatia." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6576–6585.",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "Structured two-stream attention network for video question answering",
      "author" : [ "Lianli Gao", "Pengpeng Zeng", "Jingkuan Song", "YuanFang Li", "Wu Liu", "Tao Mei", "Heng Tao Shen." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256.",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "CVPR, volume 1, page 3.",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural module networks for reasoning over text",
      "author" : [ "Nitish Gupta", "Kevin Lin", "Dan Roth", "Sameer Singh", "Matt Gardner." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Gupta et al\\.,? 2020",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual concept-metaconcept learning",
      "author" : [ "Chi Han", "Jiayuan Mao", "Chuang Gan", "Josh Tenenbaum", "Jiajun Wu." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5002–5013.",
      "citeRegEx" : "Han et al\\.,? 2019",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Cnn architectures for largescale audio classification",
      "author" : [ "Shawn Hershey", "Sourish Chaudhuri", "Daniel PW Ellis", "Jort F Gemmeke", "Aren Jansen", "R Channing Moore", "Manoj Plakal", "Devin Platt", "Rif A Saurous", "Bryan Seybold" ],
      "venue" : null,
      "citeRegEx" : "Hershey et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hershey et al\\.",
      "year" : 2017
    }, {
      "title" : "Endto-end audio visual scene-aware dialog using multimodal attention-based video features",
      "author" : [ "C. Hori", "H. Alamri", "J. Wang", "G. Wichern", "T. Hori", "A. Cherian", "T.K. Marks", "V. Cartillier", "R.G. Lopes", "A. Das", "I. Essa", "D. Batra", "D. Parikh." ],
      "venue" : "ICASSP",
      "citeRegEx" : "Hori et al\\.,? 2019",
      "shortCiteRegEx" : "Hori et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint student-teacher learning for audio-visual scene-aware dialog",
      "author" : [ "Chiori Hori", "Anoop Cherian", "Tim K Marks", "Takaaki Hori." ],
      "venue" : "Proc. Interspeech 2019, pages 1886–1890.",
      "citeRegEx" : "Hori et al\\.,? 2019",
      "shortCiteRegEx" : "Hori et al\\.",
      "year" : 2019
    }, {
      "title" : "Explainable neural computation via stack neural module networks",
      "author" : [ "Ronghang Hu", "Jacob Andreas", "Trevor Darrell", "Kate Saenko." ],
      "venue" : "Proceedings of the European conference on computer vision (ECCV), pages 53–69.",
      "citeRegEx" : "Hu et al\\.,? 2018",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to reason: End-to-end module networks for visual question answering",
      "author" : [ "Ronghang Hu", "Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Kate Saenko." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Natural language object retrieval",
      "author" : [ "Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4555–4564.",
      "citeRegEx" : "Hu et al\\.,? 2016",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Locationaware graph convolutional networks for video question answering",
      "author" : [ "Deng Huang", "Peihao Chen", "Runhao Zeng", "Qing Du", "Mingkui Tan", "Chuang Gan." ],
      "venue" : "The AAAI Conference on Artificial Intelligence (AAAI), volume 1.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Tgif-qa: Toward spatiotemporal reasoning in visual question answering",
      "author" : [ "Yunseok Jang", "Yale Song", "Youngjae Yu", "Youngjin Kim", "Gunhee Kim." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758–2766.",
      "citeRegEx" : "Jang et al\\.,? 2017",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering",
      "author" : [ "Jianwen Jiang", "Ziqiang Chen", "Haojie Lin", "Xibin Zhao", "Yue Gao." ],
      "venue" : "The AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reasoning with heterogeneous graph alignment for video question answering",
      "author" : [ "Pin Jiang", "Yahong Han." ],
      "venue" : "The AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Jiang and Han.,? 2020",
      "shortCiteRegEx" : "Jiang and Han.",
      "year" : 2020
    }, {
      "title" : "Self-assembling modular networks for interpretable multi-hop reasoning",
      "author" : [ "Yichen Jiang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Jiang and Bansal.,? 2019",
      "shortCiteRegEx" : "Jiang and Bansal.",
      "year" : 2019
    }, {
      "title" : "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "author" : [ "Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE Conference",
      "citeRegEx" : "Johnson et al\\.,? 2017a",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Inferring and executing programs for visual reasoning",
      "author" : [ "Justin Johnson", "Bharath Hariharan", "Laurens Van Der Maaten", "Judy Hoffman", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE International",
      "citeRegEx" : "Johnson et al\\.,? 2017b",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Self-supervised pretraining and contrastive representation learning for multiple-choice video qa",
      "author" : [ "Seonhoon Kim", "Seohyeong Jeong", "Eunbyul Kim", "Inho Kang", "Nojun Kwak." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):13171–",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Lin-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederick P Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Visual coreference resolution in visual dialog using neural module networks",
      "author" : [ "Satwik Kottur", "José MF Moura", "Devi Parikh", "Dhruv Batra", "Marcus Rohrbach." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 153–169.",
      "citeRegEx" : "Kottur et al\\.,? 2018",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2018
    }, {
      "title" : "Leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog",
      "author" : [ "Shachi H Kumar", "Eda Okur", "Saurav Sahay", "Jonathan Huang", "Lama Nachman." ],
      "venue" : "arXiv preprint arXiv:1912.10131.",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end multimodal dialog systems with hierarchical multimodal attention on video features",
      "author" : [ "Hung Le", "S Hoi", "Doyen Sahoo", "N Chen." ],
      "venue" : "DSTC7 at AAAI2019 workshop.",
      "citeRegEx" : "Le et al\\.,? 2019a",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2019
    }, {
      "title" : "Video-grounded dialogues with pretrained generation language models",
      "author" : [ "Hung Le", "Steven C.H. Hoi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5842–5848, Online. Association for Compu-",
      "citeRegEx" : "Le and Hoi.,? 2020",
      "shortCiteRegEx" : "Le and Hoi.",
      "year" : 2020
    }, {
      "title" : "Multimodal transformer networks for end-toend video-grounded dialogue systems",
      "author" : [ "Hung Le", "Doyen Sahoo", "Nancy Chen", "Steven Hoi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5612–5623,",
      "citeRegEx" : "Le et al\\.,? 2019b",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2019
    }, {
      "title" : "BiST: Bi-directional spatio-temporal reasoning for video-grounded dialogues",
      "author" : [ "Hung Le", "Doyen Sahoo", "Nancy Chen", "Steven C.H. Hoi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Le et al\\.,? 2020a",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to reason with relational video representation for question answering",
      "author" : [ "Thao Minh Le", "Vuong Le", "Svetha Venkatesh", "Truyen Tran." ],
      "venue" : "arXiv preprint arXiv:1907.04553.",
      "citeRegEx" : "Le et al\\.,? 2019c",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical conditional relation networks for video question answering",
      "author" : [ "Thao Minh Le", "Vuong Le", "Svetha Venkatesh", "Truyen Tran." ],
      "venue" : "arXiv preprint arXiv:2002.10698.",
      "citeRegEx" : "Le et al\\.,? 2020b",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "Dstc8-avsd: Multimodal semantic transformer network with retrieval style word generator",
      "author" : [ "Hwanhee Lee", "Seunghyun Yoon", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Kyomin Jung." ],
      "venue" : "DSTC Workshop @ AAAI 2020.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Multiquestion learning for visual question answering",
      "author" : [ "Chenyi Lei", "Lei Wu", "Dong Liu", "Zhao Li", "Guoxin Wang", "Haihong Tang", "Houqiang Li." ],
      "venue" : "The AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Lei et al\\.,? 2020",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "Less is more: Clipbert for video-and-language learning via sparse sampling",
      "author" : [ "Jie Lei", "Linjie Li", "Luowei Zhou", "Zhe Gan", "Tamara L Berg", "Mohit Bansal", "Jingjing Liu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
      "citeRegEx" : "Lei et al\\.,? 2021",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2021
    }, {
      "title" : "TVQA: Localized, compositional video question answering",
      "author" : [ "Jie Lei", "Licheng Yu", "Mohit Bansal", "Tamara Berg." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1369–1379, Brussels, Belgium.",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "HERO: Hierarchical encoder for Video+Language omnirepresentation pre-training",
      "author" : [ "Linjie Li", "Yen-Chun Chen", "Yu Cheng", "Zhe Gan", "Licheng Yu", "Jingjing Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond rnns: Positional self-attention with co-attention for video question answering",
      "author" : [ "Xiangpeng Li", "Jingkuan Song", "Lianli Gao", "Xianglong Liu", "Wenbing Huang", "Xiangnan He", "Chuang Gan." ],
      "venue" : "The 33rd AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Bridging text and video: A universal multimodal transformer for video-audio scene-aware dialog",
      "author" : [ "Zekang Li", "Zongjia Li", "Jinchao Zhang", "Yang Feng", "Cheng Niu", "Jie Zhou." ],
      "venue" : "arXiv preprint arXiv:2002.00163.",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
      "author" : [ "Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu." ],
      "venue" : "International Conference on Learning Representa-",
      "citeRegEx" : "Mao et al\\.,? 2019",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2019
    }, {
      "title" : "From film to video: Multiturn question answering with multi-modal context",
      "author" : [ "Dat Tien Nguyen", "Shikhar Sharma", "Hannes Schulz", "Layla El Asri." ],
      "venue" : "AAAI 2019 Dialog System Technology Challenge (DSTC7) Workshop.",
      "citeRegEx" : "Nguyen et al\\.,? 2018",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Exploring models and data for image question answering",
      "author" : [ "Mengye Ren", "Ryan Kiros", "Richard Zemel." ],
      "venue" : "Advances in neural information processing systems, pages 2953–2961.",
      "citeRegEx" : "Ren et al\\.,? 2015a",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Ren et al\\.,? 2015b",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Cmu sinbad’s submission for the dstc7 avsd challenge",
      "author" : [ "Ramon Sanabria", "Shruti Palaskar", "Florian Metze." ],
      "venue" : "DSTC7 at AAAI2019 workshop, volume 6.",
      "citeRegEx" : "Sanabria et al\\.,? 2019",
      "shortCiteRegEx" : "Sanabria et al\\.",
      "year" : 2019
    }, {
      "title" : "Factor graph attention",
      "author" : [ "Idan Schwartz", "Seunghak Yu", "Tamir Hazan", "Alexander G Schwing." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2039– 2048.",
      "citeRegEx" : "Schwartz et al\\.,? 2019",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2019
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931–2951, Florence, Italy",
      "author" : [ "Sofia Serrano", "Noah A. Smith." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Serrano and Smith.,? 2019",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Hollywood in homes: Crowdsourcing data collection for activity understanding",
      "author" : [ "Gunnar A Sigurdsson", "Gül Varol", "Xiaolong Wang", "Ali Farhadi", "Ivan Laptev", "Abhinav Gupta." ],
      "venue" : "European Conference on Computer Vision, pages 510–526. Springer.",
      "citeRegEx" : "Sigurdsson et al\\.,? 2016",
      "shortCiteRegEx" : "Sigurdsson et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The Journal of Machine Learning Research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "DeCEMBERT: Learning from noisy instructional videos via dense captions and entropy minimization",
      "author" : [ "Zineng Tang", "Jie Lei", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Tang et al\\.,? 2021",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Bert representations for video question answering",
      "author" : [ "Zekun Yang", "Noa Garcia", "Chenhui Chu", "Mayu Otani", "Yuta Nakashima", "Haruo Takemura." ],
      "venue" : "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1556–1565.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural-symbolic vqa: Disentangling reasoning from vision and language understanding",
      "author" : [ "Kexin Yi", "Jiajun Wu", "Chuang Gan", "Antonio Torralba", "Pushmeet Kohli", "Josh Tenenbaum." ],
      "venue" : "Advances in Neural Information Processing Systems, pages",
      "citeRegEx" : "Yi et al\\.,? 2018",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end concept word detection for video captioning, retrieval, and question answering",
      "author" : [ "Youngjae Yu", "Hyungjin Ko", "Jongwook Choi", "Gunhee Kim." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3165–",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Merlot: Multimodal neural script knowledge models",
      "author" : [ "Rowan Zellers", "Ximing Lu", "Jack Hessel", "Youngjae Yu", "Jae Sung Park", "Jize Cao", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:2106.02636.",
      "citeRegEx" : "Zellers et al\\.,? 2021",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2021
    }, {
      "title" : "VGNMN directly addresses the challenge of entity references in dialogues",
      "author" : [ "E F Row" ],
      "venue" : "(Serban et al.,",
      "citeRegEx" : "Row,? \\Q2016\\E",
      "shortCiteRegEx" : "Row",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 43,
      "context" : "(Le et al., 2019b; Fan et al., 2019) have achieved remarkable performance through the use of deep neural networks to retrieve grounding video signals based on language inputs.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "(Le et al., 2019b; Fan et al., 2019) have achieved remarkable performance through the use of deep neural networks to retrieve grounding video signals based on language inputs.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Similar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues (Agrawal et al., 2016; Goyal et al., 2017; Feng et al., 2018; Serrano and Smith, 2019).",
      "startOffset" : 172,
      "endOffset" : 258
    }, {
      "referenceID" : 19,
      "context" : "Similar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues (Agrawal et al., 2016; Goyal et al., 2017; Feng et al., 2018; Serrano and Smith, 2019).",
      "startOffset" : 172,
      "endOffset" : 258
    }, {
      "referenceID" : 14,
      "context" : "Similar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues (Agrawal et al., 2016; Goyal et al., 2017; Feng et al., 2018; Serrano and Smith, 2019).",
      "startOffset" : 172,
      "endOffset" : 258
    }, {
      "referenceID" : 63,
      "context" : "Similar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues (Agrawal et al., 2016; Goyal et al., 2017; Feng et al., 2018; Serrano and Smith, 2019).",
      "startOffset" : 172,
      "endOffset" : 258
    }, {
      "referenceID" : 62,
      "context" : "These approaches might be suitable to represent general semantics in open-domain dialogues (Serban et al., 2016).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "• Our experiments are conducted on the challenging benchmark for video-grounded dialogues, Audio-visual Scene-Aware Dialogues (AVSD) (Alamri et al., 2019) as well as TGIFQA (Jang et al.",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 30,
      "context" : ", 2019) as well as TGIFQA (Jang et al., 2017) for video QA task.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "Recently, we have witnessed emerging techniques in video-language systems that exploit deep transformer-based architectures such as BERT (Devlin et al., 2019) for pretraining multimodal representations (Li et al.",
      "startOffset" : 137,
      "endOffset" : 158
    }, {
      "referenceID" : 51,
      "context" : ", 2019) for pretraining multimodal representations (Li et al., 2020a; Yang et al., 2020; Kim et al., 2021; Tang et al., 2021; Lei et al., 2021; Zellers et al., 2021) in very large-scale videolanguage datasets.",
      "startOffset" : 51,
      "endOffset" : 165
    }, {
      "referenceID" : 70,
      "context" : ", 2019) for pretraining multimodal representations (Li et al., 2020a; Yang et al., 2020; Kim et al., 2021; Tang et al., 2021; Lei et al., 2021; Zellers et al., 2021) in very large-scale videolanguage datasets.",
      "startOffset" : 51,
      "endOffset" : 165
    }, {
      "referenceID" : 36,
      "context" : ", 2019) for pretraining multimodal representations (Li et al., 2020a; Yang et al., 2020; Kim et al., 2021; Tang et al., 2021; Lei et al., 2021; Zellers et al., 2021) in very large-scale videolanguage datasets.",
      "startOffset" : 51,
      "endOffset" : 165
    }, {
      "referenceID" : 67,
      "context" : ", 2019) for pretraining multimodal representations (Li et al., 2020a; Yang et al., 2020; Kim et al., 2021; Tang et al., 2021; Lei et al., 2021; Zellers et al., 2021) in very large-scale videolanguage datasets.",
      "startOffset" : 51,
      "endOffset" : 165
    }, {
      "referenceID" : 49,
      "context" : ", 2019) for pretraining multimodal representations (Li et al., 2020a; Yang et al., 2020; Kim et al., 2021; Tang et al., 2021; Lei et al., 2021; Zellers et al., 2021) in very large-scale videolanguage datasets.",
      "startOffset" : 51,
      "endOffset" : 165
    }, {
      "referenceID" : 73,
      "context" : ", 2019) for pretraining multimodal representations (Li et al., 2020a; Yang et al., 2020; Kim et al., 2021; Tang et al., 2021; Lei et al., 2021; Zellers et al., 2021) in very large-scale videolanguage datasets.",
      "startOffset" : 51,
      "endOffset" : 165
    }, {
      "referenceID" : 35,
      "context" : "NMN models have achieved success in synthetic image domains where a multi-step reasoning process is required (Johnson et al., 2017b; Hu et al., 2018; Han et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 167
    }, {
      "referenceID" : 26,
      "context" : "NMN models have achieved success in synthetic image domains where a multi-step reasoning process is required (Johnson et al., 2017b; Hu et al., 2018; Han et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "NMN models have achieved success in synthetic image domains where a multi-step reasoning process is required (Johnson et al., 2017b; Hu et al., 2018; Han et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 167
    }, {
      "referenceID" : 39,
      "context" : "Our work is related to the recent work (Kottur et al., 2018; Jiang and Bansal, 2019; Gupta et al., 2020) that extended NMNs to image reasoning in dialogues and reading comprehension reasoning.",
      "startOffset" : 39,
      "endOffset" : 104
    }, {
      "referenceID" : 33,
      "context" : "Our work is related to the recent work (Kottur et al., 2018; Jiang and Bansal, 2019; Gupta et al., 2020) that extended NMNs to image reasoning in dialogues and reading comprehension reasoning.",
      "startOffset" : 39,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : "Our work is related to the recent work (Kottur et al., 2018; Jiang and Bansal, 2019; Gupta et al., 2020) that extended NMNs to image reasoning in dialogues and reading comprehension reasoning.",
      "startOffset" : 39,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "These vectors are then integrated with ordering information of tokens through a positional encoding function with layer normalization (Ba et al., 2016; Vaswani et al., 2017).",
      "startOffset" : 134,
      "endOffset" : 173
    }, {
      "referenceID" : 68,
      "context" : "These vectors are then integrated with ordering information of tokens through a positional encoding function with layer normalization (Ba et al., 2016; Vaswani et al., 2017).",
      "startOffset" : 134,
      "endOffset" : 173
    }, {
      "referenceID" : 37,
      "context" : "It is fed to a onedimensional CNN with max-pooling layer (Kim, 2014) to obtain a contextual entity-aware question representation.",
      "startOffset" : 57,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "While previous models usually focus on global or token-level dependencies (Hori et al., 2019; Le et al., 2019b) to encode question features, our modules compress fine-grained question representations at the entity level.",
      "startOffset" : 74,
      "endOffset" : 111
    }, {
      "referenceID" : 43,
      "context" : "While previous models usually focus on global or token-level dependencies (Hori et al., 2019; Le et al., 2019b) to encode question features, our modules compress fine-grained question representations at the entity level.",
      "startOffset" : 74,
      "endOffset" : 111
    }, {
      "referenceID" : 62,
      "context" : "We show that our modularized approach can achieve better performance and transparency than traditional approaches to encode dialogue context (Serban et al., 2016; Vaswani et al., 2017) (Section 4).",
      "startOffset" : 141,
      "endOffset" : 184
    }, {
      "referenceID" : 68,
      "context" : "We show that our modularized approach can achieve better performance and transparency than traditional approaches to encode dialogue context (Serban et al., 2016; Vaswani et al., 2017) (Section 4).",
      "startOffset" : 141,
      "endOffset" : 184
    }, {
      "referenceID" : 34,
      "context" : "To learn compositional programs, we follow (Johnson et al., 2017a; Hu et al., 2017) and consider program generation as a sequence-tosequence task.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 27,
      "context" : "To learn compositional programs, we follow (Johnson et al., 2017a; Hu et al., 2017) and consider program generation as a sequence-tosequence task.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 43,
      "context" : "We follows a vanilla Transformer decoder architecture (Le et al., 2019b), which consists of 3 attention layers: self-attention to attend on existing tokens, attention to Qctx from dialogue understanding program execution, and attention to Vctx from video understanding program execution.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : "Experimental Setup and Training We use the AVSD benchmark from the Dialogue System Technology Challenge 7 (DSTC7) (Hori et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 64,
      "context" : "The benchmark consists of dialogues grounded on the Charades videos (Sigurdsson et al., 2016).",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "For visual features, we use the 3D CNN-based features from a pretrained I3D model (Carreira and Zisserman, 2017) and object-level features from a pretrained FasterRNN model (Ren et al.",
      "startOffset" : 82,
      "endOffset" : 112
    }, {
      "referenceID" : 59,
      "context" : "For visual features, we use the 3D CNN-based features from a pretrained I3D model (Carreira and Zisserman, 2017) and object-level features from a pretrained FasterRNN model (Ren et al., 2015b).",
      "startOffset" : 173,
      "endOffset" : 192
    }, {
      "referenceID" : 23,
      "context" : "The audio features are obtained from a pretrained VGGish model (Hershey et al., 2017).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : "We also adapt VGNMN to the video QA benchmark TGIF-QA (Jang et al., 2017).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 57,
      "context" : "We evaluate model performance by the objective metrics, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 54,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al.",
      "startOffset" : 52,
      "endOffset" : 63
    }, {
      "referenceID" : 69,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015), between each generated response and 6 reference gold responses.",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 53,
      "context" : "We observed that our approach did not outperform the GPT-based baselines (Li et al., 2020b; Le and Hoi, 2020) in the setting that allows video summary/caption input However, the performance of our model in the setting without video summary/caption input is on par with the GPT-based baseline (Li et al.",
      "startOffset" : 73,
      "endOffset" : 109
    }, {
      "referenceID" : 42,
      "context" : "We observed that our approach did not outperform the GPT-based baselines (Li et al., 2020b; Le and Hoi, 2020) in the setting that allows video summary/caption input However, the performance of our model in the setting without video summary/caption input is on par with the GPT-based baseline (Li et al.",
      "startOffset" : 73,
      "endOffset" : 109
    }, {
      "referenceID" : 53,
      "context" : ", 2020b; Le and Hoi, 2020) in the setting that allows video summary/caption input However, the performance of our model in the setting without video summary/caption input is on par with the GPT-based baseline (Li et al., 2020b), even though our model did not rely on deep pretrained represenModel PT Vis.",
      "startOffset" : 209,
      "endOffset" : 227
    }, {
      "referenceID" : 24,
      "context" : "B-4 M R C Without Video Summary/Caption Baseline (Hori et al., 2019) - I - 0.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 40,
      "context" : "059 With Video Summary/Caption TopicEmb (Kumar et al., 2019) - I A 0.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 43,
      "context" : "Video features are retrieved through a token-level representation of questions (Le et al., 2019b).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "Dialogue history is encoded by a hierarchical LSTM encoder (Hori et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 78
    } ],
    "year" : 0,
    "abstractText" : "Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded dialogue tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross-turn dependencies. Motivated by recent NMN approaches on image-grounded tasks, we introduce Videogrounded Neural Module Network (VGNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules. VGNMN first decomposes all language components in dialogues to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Our experiments show that VGNMN can achieve promising performance on a challenging video-grounded dialogue benchmark as well as a video QA benchmark.",
    "creator" : null
  }
}
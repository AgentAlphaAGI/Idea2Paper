{
  "name" : "ARR_2022_242_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CogTaskonomy: Cognitively Inspired Task Taxonomy Is Beneficial to Transfer Learning in NLP",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Transfer learning (TL) has attracted extensive research interests in natural language processing with a wide range of forms, e.g., TL from pretrained language models (PLM) to downstream tasks (Devlin et al., 2018; Radford et al., 2018), from a task with rich labeled data to a task with low resource (Chu and Wang, 2018; Yu et al., 2021), from highresource languages to low-resource languages (Gu et al., 2018; Ko et al., 2021), etc.1 A high-level\n1In this paper, we focus on cross-task transfer learning in the same language.\nconcept or question on cross-task transfer learning is how these involved tasks are related to each other. Is sentiment analysis related to paraphrasing? Is textual entailment more related to question answering than named entity recognition? All these sub-questions resolve themselves into whether a structure exists among NLP tasks. Such task taxonomy is of notable values to transfer learning in NLP in that it has the potential to guide TL and reduce redundancies across tasks (Zamir et al., 2018).\nIn this paper, partially inspired by the task taxonomy in visual tasks (Zamir et al., 2018), we study the hierarchical task structure for NLP tasks. But significantly different from the visual Taskonomy (Zamir et al., 2018), we construct NLP taskonomy from a cognitively inspired perspective.\nCognitively inspired NLP is the intersection of NLP and cognitive neuroscience that aims at uncovering cognitive processes in the brain, including cognition in language comprehension. With the increasing availability of cognitively annotated data, on the one hand, cognitive processing signals (e.g., eye-tracking, EEG, fMRI) have been explored to enhance neural models for a wide range of NLP tasks (Barrett and Søgaard, 2015; Bingel et al., 2016; Hollenstein and Zhang, 2019; Hollenstein et al., 2019a). On the other hand, representations learned in NLP models are used to predict brain activation patterns recorded in cognitive processing data (Mitchell et al., 2008; Pereira et al., 2018; Hale et al., 2018; Hollenstein et al., 2019b). These studies on the bidirectional association between the two areas demonstrate that information underlying cognitive processing data is closely related to tasks and representations in NLP. Hence we want to know whether it is feasible to isolate task representations from cognitive processing data and use them to learn task taxonomy in NLP.\nTo examine this hypothesis, we propose CogTaskonomy, a Cognitively Inspired Task Taxonomy framework, as illustrated in Figure 1, to\nlearn a task structure for NLP tasks. CogTaskonomy consists of two main cognitively inspired components: Cognitive Representation Analytics (CRA) and Cognitive-Neural Mapping (CNM). CRA extracts task representations from NLP models and employs Representational Similarity Analysis (RSA) (Kriegeskorte et al., 2008), which is commonly used to measure the correlation between brain activity and computational model, to estimate NLP task similarity. CNM trains fully connected neural networks to build the mapping from sentence representations of pretrained models finetuned on specific tasks to fMRI signals recorded when human subjects read those sentences. It then uses mapping correlation coefficients as task representations to compute task similarity.\nBoth methods require sentence representations to compute task representations. We use pretrained language models fine-tuned on specific tasks, particularly BERT (Devlin et al., 2018) and TinyBERT (Jiao et al., 2020), to obtain sentence representations.\nWe compare the proposed CogTaskonomy against the Analytic Hierarchy Process (AHP) used in Taskonomy (Zamir et al., 2018). We guide TL across tasks with the learned task structure and evaluate the effectiveness of these methods by estimating TL performance from various source to target tasks.\nContributions Our main contributions include:\n• We propose CogTaskonomy, a cognitively inspired framework to measure task similarity and to build the task taxonomy in NLP. This is the first attempt to study NLP task structures with cognitive processing data.\n• We present two cognitively inspired methods, CRA and CNM, and compare them against AHP. Different from AHP, the two methods do not require O(m2) exhaustive pairwise transfer learning for task similarity estimation.\n• We build a taxonomy tree for 12 NLP tasks, including sentiment analysis, question answering, natural language inference, semantic textual similarity, passage ranking, etc., to guide transfer learning across them.\n• TL experiments and analyses validate the effectiveness of the proposed CogTaskonomy and find that CNM is able to learn stable task relations that are general to different underlying models."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is related to cognitively inspired NLP and a variety of learning formalisms that involve knowledge transfer across different tasks. We briefly review these topics within the scope of NLP and the constraint of space."
    }, {
      "heading" : "2.1 Cognitively Inspired NLP",
      "text" : "Using NLP Representations for Brain Activity Prediction Since the pioneering work (Mitchell et al., 2008), connecting statistical NLP representations with cognition has attracted widespread attention. Chang et al. (2009) explore adjective-noun composition in fMRI based on co-occurrence statistics. Huth et al. (2016) use distributed word representations to map fMRI data to activated brain regions, revealing a semantic map of how words are distributed in the human cerebral cortex. A great deal of research (Murphy et al., 2012; Anderson et al., 2016; Søgaard, 2016; Bulat et al., 2017) has been devoted to word decoding. Pereira et al. (2018) extend brain decoding to sentence stimuli, suggesting that neural network language models can be used to interpret sentences in a long-term context. Augmenting NLP Models with Cognitive Processing Signals Recent years have witnessed that many efforts have been devoted to exploring cognitive processing signals (e.g., eye-tracking, EEG, fMRI) in neural NLP models. Muttenthaler et al. (2020) use cognitive data to regularize attention weights in NLP models. Hollenstein et al. (2019a) evaluate word embeddings using cognitive data. Toneva and Wehbe (2019) utilize fMRI scans to interpret and improve BERT. Many other works use cognitive processing signals to improve NLP models (Barrett and Søgaard, 2015; Bingel et al., 2016; Gauthier and Levy, 2019; Hollenstein and Zhang, 2019), just to name a few."
    }, {
      "heading" : "2.2 Learning across Tasks",
      "text" : "A very important trend in recent NLP is that models, algorithms, and solutions are not developed for only a single task, but for multiple tasks or across tasks (Devlin et al., 2018; Radford et al., 2018; McCann et al., 2018; Worsham and Kalita, 2020). Learning methods that are capable of handling a set of tasks simultaneously or sequentially, e.g., multi-task learning, transfer learning, meta learning, have attracted growing research interests in NLP. Beyond learning methods, yet another important dimension to this research trend is task relation\nlearning, which is the topic of this work.2\nMulti-task Learning is to jointly train all tasks of interests with task linkages, e.g., in the form of regularization or sharing parameters across tasks (Collobert et al., 2011). It is important in multitask learning to find related tasks for target tasks as auxiliary tasks (Ruder, 2017).\nTransfer Learning targets at transferring knowledge from a source task to a target task. According to the task and domain difference in the source and target, TL is divided into transductive TL (same task, different domain, a.k.a. domain adaptation), inductive TL (same domain, different task) and unsupervised TL (both different) (Eaton and desJardins, 2011; Ghifary et al., 2014; Wang et al., 2019; Yuan and Wen, 2021). If the source and target are dissimilar, negative transfer may hurt TL (Niu et al., 2020).\nMeta Learning aims to gain experience over a set of related tasks for improving the learning algorithm itself (Hospedales et al., 2020). Existing meta learning methods implicitly assume that tasks are similar to each other, but it is often unclear how to quantify task similarities and their roles in learning (Venkitaraman and Wahlberg, 2020).\nLifelong Learning is to learn continuously and accumulate knowledge along a sequence of tasks and uses it for future learning (Chen and Liu, 2018). The system is tuned to be able to select the most related prior knowledge to bias the learning towards a new task favourably (Silver et al., 2013).\n2Task taxonomy learned by our methods could be applicable to other learning formalisms beyond transfer learning. We leave this to our future work."
    }, {
      "heading" : "2.3 Learning Task Relations",
      "text" : "As task relatedness is important for cross-task learning formalisms mentioned in Section 2.2, efforts have also been made to learn task relations. Crawshaw (2020) groups previous methods on task relationship learning into three categories. The first is task grouping or clustering, which divides a set of tasks into clusters so that tasks in the same cluster can be jointly trained (Bingel and Søgaard, 2017; Standley et al., 2019). The second is learning transfer relationships, which analyzes whether transfer between tasks is beneficial to learning, regardless of whether tasks are related or not (Zamir et al., 2018; Dwivedi and Roig, 2019; Song et al., 2019). The third is task embedding, which learns a specific representation space for tasks (James et al., 2018; Lan et al., 2019).\nOur research can be considered as a mix of these categories. CNM learns cognition-based task representations while both CNM and CRA learn task relations aiming at transfer learning. Additionally, significantly different from previous studies, we learn task structures from a cognitive perspective3, which is expected to estimate task relatedness in a cognitively tuned space. As will be demonstrated below, our cognitively motivated methods incur a low computation cost and exhibit generalization across underlying models to some extend."
    }, {
      "heading" : "3 CogTaskonomy",
      "text" : "Figure 1 illustrates the basic framework of CogTaskonomy. First, we obtain task-specific sentence\n3Dwivedi and Roig (2019) also use RSA to learn task taxonomy, in some way similar to our CRA. But they learn relations for visual tasks and use different correlation functions from our CRA.\nrepresentations of text stimuli from cognitive data by feeding them into fine-tuned or distilled pretrained language models on 12 downstream tasks (Section 3.1). Subsequently, task-specific representations are fed into two cognitively inspired components, cognitive representation analytics (Section 3.2) and cognitive-neural mapping (Section 3.3), for estimating task similarity and inducing task taxonomy."
    }, {
      "heading" : "3.1 Task-Specific Sentence Representations",
      "text" : "Fine-tuning a pretrained language model for an end task is a widely used strategy for quickly and efficiently building a model for that task with limited labeled data. Zhou and Srikumar (2021) find that fine-tuning reconfigures underlying semantic space to adjust pretrained representations to downstream tasks. In view of this, we take sentence-level textual stimuli of cognitive data as input data for a specific fine-tuned model to obtain representations that contain information specific to that task. Additionally, Cheng et al. (2020) suggest that knowledge distillation (KD) helps models to be more focused on task-relevant concept. Therefore, without loss of generality, we use BERT and TinyBERT (performing KD) to obtain task-specific sentence representations. BERT Following Devlin et al. (2018), we prepend a special classification token [CLS] to each input sentence in order to extract the contextualized representation of the corresponding sentence. Merchant et al. (2020) find that fine-tuning primarily affects top layers of BERT. Hence, we take the hidden state of the prepended token of each sequence in the last layer as the sentence representation. TinyBERT TinyBERT (Jiao et al., 2020) performs knowledge distillation at both the pretraining and fine-tuning stage. By leveraging KD, TinyBERT learns to transfer knowledge encoded in the large teacher BERT (Devlin et al., 2018) to itself. As a result, TinyBERT can capture both general and task-specific knowledge. Similarly, we use the hidden state of [CLS] token in the last layer as the contextualized representation for a given sentence."
    }, {
      "heading" : "3.2 Cognitive Representation Analytics",
      "text" : "With task-specific representations learned by feeding text stimuli of cognitive data into a fine-tuned model, we can estimate pairwise task similarity for any two tasks in a given task list T = {t1, t2, ..., tm}. The first cognitively inspired method is the cognitive representation analytics\nthat adapts a common method in computational neuroscience to our scenario. We first briefly introduce the common method, representational similarity analysis, and then elaborate the adaptation.\nRepresentational Similarity Analysis is widely applied in cognitive neuroscience, which can not only realize cross-modal cognitive data comparison but also quantitatively relate brain activity measurements to computational models. It first calculates a representation dissimilarity matrix (RDM) of different modal data, and then estimates the correlation between RDMs. In this way, it successfully captures cross-modal data relationships (Kriegeskorte et al., 2008). RSA can be also applied for the comparison between computational models and cognitive data. The RDM of a computational model is obtained by comparing the dissimilarity of data representations obtained from the computational model in pairs. It is then compared with the RDM of brain activity measurements.4\nWe take all sentence representations Ri generated by a task-specific model PLMFTi (a pretrained language model (either BERT or TinyBERT) finetuned on the ith task) as the base to simulate cognitive representations required by RSA. For each pair of sentence representations (Rij , Rij′) for the jth and j′th sentence of the ith task, we compute a dissimilarity score in three metrics (e): Euclidean distance (euclidean), Canberra distance (canberra) and Pearson correlation coefficient (ρ). Among them, the first two distance metrics can naturally represent dissimilarity (Dis), while the last ρ needs to be converted to 1 − ρ to indicate dissimilarity, as follows:\nDisijj′ = { e(Rij , Rij′) e is not ρ 1− e(Rij , Rij′) e is ρ\n(1)\nRDM for the ith task consists of the dissimilarity scores of all sentence pairs. We formulate it as follows:\nRDMi = [Disi12,Dis i 13, . . . ,Dis i 1n, . . .\nDisijj′ , . . . ,Dis i (n−1)n], j ̸= j\n′ (2)\nwhere n is the number of sentences. RDMs computed in this way are then used for estimating similarity between NLP tasks. The pairwise similarity Simii′ of the ith and i′th task is computed as fol-\n4In our CRA, only RDMs from computational models are used. This is because we don’t have cognitive data that are curated for specific NLP tasks. In our preliminary experiments, we have created pseudo cognitive data for different NLP tasks by predicting cognitive signals with a mapping model similar to that used in CNM. But it performs poorly.\nlows: Simii′ = Similarity(RDM ⊺ i · RDMi′) (3) Similarity(·) is a similarity function, which can be Spearman rank correlation (rs), ρ and cosine (cos, by default).\nIn summary, we calculate the similarity between each RDM pair and finally obtain a similarity matrix for a set of tasks, as shown in Figure 2."
    }, {
      "heading" : "3.3 Cognitive-Neural Mapping",
      "text" : "The idea behind cognitive-neural mapping is to project sentence representations of NLP models fine-tuned in a specific task to cognitive signals (i.e., fMRI voxels in this paper) recorded when humans read those sentences with a neural network. The connections between the specific task and cognitive signals learned in this way could be transformed into cognitively inspired task representations for further task similarity estimation. The mapping can be considered as a way to isolate brain activity related to the specific task from fMRI cognitive signals. Particularly, for the ith task and sth subject, we use a fully connected 3-layer feed-forward neural network to project sentence representation Rij specific to this task to fMRI yisj of the sth subject reading the jth sentence as follows:\nyisj = W i 2(ReLU(W i 1(Rij)) (4)\nTo optimize the mapping model, we use the mean squared error (MSE) as loss function. 5-fold cross-validation is performed for each mapping model. Before training, grid search is conducted, and the optimal number of hidden layer units in the mapping network is obtained by three times of cross-validation on the verification set accounting for 20% training data.\nEach mapping is run 5 times. We average models over all subjects and 5 runs and then evaluate mapping model performance in all voxels. Particularly, we compute the cognitively inspired task representation CogRi for the ith task, which consists of the correlation coefficients on all voxels between predicted values and ground-truth values,\ndefined as follows : CogRi =[c(ŷ i 0,y0), . . . , c(ŷ i k,yk),\n. . . , c(ŷiv,yv)], 0 ≤ k ≤ v (5)\nwhere ŷik is a vector of all predicted values for the kth voxel from all input sentences by the mapping model tuned for the ith task, yk is a vector of the ground-truth values for the kth voxel from all sentence-level signals of text stimuli in fMRI data, v is the number of voxels used, and c(·) is a function for comparing two input vectors. We instantiate c in two functions: the coefficient of determination (R2) and ρ.5\nWe then use cosine similarity to calculate pairwise task similarity as follows:\nSimii′ = cos(CogR ⊺ i · CogRi′) (6)"
    }, {
      "heading" : "4 Experiments",
      "text" : "We conducted experiments with widely-used NLP benchmark datasets and cognitive data to evaluate the effectiveness of CogTaskonomy."
    }, {
      "heading" : "4.1 Cognitive Dataset",
      "text" : "The brain fMRI dataset in our experiments is from Pereira et al. (2018), which is recorded on a wholebody 3-Tesla Siemens Trio scanner with a 32- channel head coil by showing 627 natural language sentences to 5 adult subjects.6 Since voxels were randomly selected, Z-Score standardization was carried out for voxels obtained from different stimuli at each location on the basis of the original data set to avoid the influence of outliers. Subjects are asked to read each encyclopedic statement carefully, while the fMRI scanner records brain signals at this point. As a result, each fMRI scan covers multiple words at a time, subject to continuous stimulation. Each fMRI recording contains a number of voxels. We flattened 3d fMRI images into 1d vectors. v voxels were randomly selected, yielding matrices Is ∈ R627×v for each subject s."
    }, {
      "heading" : "4.2 Tasks",
      "text" : "We selected 8 NLP tasks from the GLUE benchmark (Wang et al., 2018), including CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2, STS-B. These\n5R2 is a statistical measure that examines how much a model is able to predict or explain an outcome, usually defined as the square of the correlation between predicted values and actual values. According to the results in Appendix A.1, we set R2 as c in CNM by default.\n6This dataset is publicly available at https://osf. io/crwz7/. The cognitive data of subjects who both participated in experiments 2 and 3 were chosen in this paper.\ntasks are considered important for generalizable natural language understanding, exhibiting diversity in domains, dataset sizes, and difficulties (Wang et al., 2018). To cover the spectrum of NLP tasks as much as possible, we also included Extractive Question Answering (QA), Relation Extraction (RE), Named Entity Recognition (NER), and Passage Reranking (PR). The datasets of these four tasks are SQuAD 2.0 (Rajpurkar et al., 2018), Semeval-2010 task 8 (Hendrickx et al., 2010), CoNLL 2003 (Sang and Meulder, 2003), MS MARCO (Nguyen et al., 2016; Craswell et al., 2020), respectively."
    }, {
      "heading" : "4.3 Baselines and Settings",
      "text" : "We mainly used two methods as our baselines, including Direct Similarity Estimation (DSE), Analytic Hierarchy Process (AHP) (Zamir et al., 2018). Detailed experimental settings are shown in Appendix A.2.\nDirect Similarity Estimation (DSE) A straightforward way to estimate pairwise task similarity is to calculate sentence-level similarities based on task-specific sentence representations and then average them. Concretely, let Rij be the task-specific representation for the jth sentence in the ith task. The task similarity Simii′ for a task pair (i, i′) is computed as follows:\nRij = PLMFTi (xj) (7)\nSimii′ =\n∑ j Similarity(R ⊺ ij ·Ri′j)\nn (8)\nwhere PLMFTi is the pretrained language model fine-tuned on the ith task. PLM can be instantiated as TinyBERT or BERT.\nAnalytic Hierarchy Process (AHP) The main idea is to construct a matrix Wt for each target task t, where the element at (i, i′) in the matrix shows how many times the ith source task is better than i′th source task in terms of the transferability to the target task on a held-out set. The principal eigenvector of Wt is then taken as the task representation for the corresponding task, and all task representations are stacked up to obtain an affinity matrix.7 The affinity matrix is then viewed as the task similarity matrix.\n7For more details about AHP, please refer to (Zamir et al., 2018). Since the test sets of our NLP tasks are not publicly available, we obtain AHP results based on the validation set of each task except the NER task of which the test set is publicly available. In all experiments, the hyper-parameters are the same for all tasks."
    }, {
      "heading" : "4.4 Evaluation Metric",
      "text" : "Task Transferring To assess the similarity between tasks, all models fine-tuned on non-target tasks will be used as source models, and continue to be fine-tuned in the same way to transfer on the target task. In task transferring, all parameters of source models are fine-tuned (i.e., not fixed). We used the same learning rate and a number of training steps for all task transferring. This allows a fair comparison between different source tasks.\nOracle Task Ranking The final similarity ranking of source tasks to a given target task is based on the results obtained from the task transferring experiments. Generally speaking, the better the source-to-target transfer performance is, the more similar the two tasks are, since the essence of TL is to apply knowledge learned in the source task to the target task. Based on this concept, we rank tasks in terms of transfer learning performance, for more details please see Appendix A.3.\nTask Ranking Score Based on similarity results computed by each task estimation method, we can obtain the most similar task for each target task. We then check the ranking position of the most similar task in the oracle task ranking. We average ranking positions of all target tasks as the final task ranking score for the corresponding task estimation method. Note that we exclude the transfer to the target task itself in computing task ranking scores.8"
    }, {
      "heading" : "4.5 Main Results",
      "text" : "Task ranking scores (using the ranking of task transferring as the oracle ranking) of different task similarity estimation methods are shown in Table 1. From these results, we have the following observations:\n• Both CRA and CNM are better than random ranking and DSE, suggesting that cognitively inspired task similarity estimation is able to capture relations of NLP tasks. • When TinyBERT is used, DSE is even worse than random ranking. This suggests that simply using task-specific sentence representations cannot well detect task relations and dis-\n8Generally, the lower the task ranking score, the better the task similarity estimation method. A perfect estimation similarity method would yield a task ranking score of 1 on each target task. A random method would yield a ranking score of 0.5(1 + 11) = 6 in our experiments theoretically. We have also conducted random sampling 5000 times on TinyBERT and BERT, and obtained mean task ranking scores of 6.05 and 6.04 respectively. Hence, we take the 6 as the task ranking score for random ranking.\ntinguish different tasks. • TinyBERT performs better than BERT across\nthree task estimation methods (i.e., CRA, CNM and AHP) although the number of parameters in the former is only half of that in the latter. We conjecture that TinyBERT uses knowledge distillation, making sentence representations more relevant to individual tasks and hence resulting in better task similarity estimation. • We can also combine CRA and CNM (CRA+ CNM) by averaging task similarity scores estimated by them. Such combination is better than both methods alone.\nAlthough AHP is better than our methods, it directly uses the results of transfer learning to measure similarities between different tasks, which is very time-consuming. if we have m tasks, we have to perform O(m2) transfer learning to obtain the task similarity matrix across all task pairs. In contrast, our methods do not require any costly transfer learning between tasks. It is hence easier to perform and able to guide transfer learning across tasks. We further evaluated the actual transfer learning performance of each target task from the most similar source task according to different task similarity estimation methods. Results are shown in Appendix A.4, which further validate the effectiveness of our methods and shows that CRA+CNM is very close to that of AHP. In later experiments and analyses, we will show more advantages of our methods over AHP."
    }, {
      "heading" : "4.6 Evaluating CRA with Different Dissimilarity/Similarity Measurement Combinations",
      "text" : "CRA adopts RSA to transform the dissimilarity of task-specific sentence representations into the similarity of tasks. We have different options for dissimilarity measurement (e.g., euclidean,canberra) in sentences and for similarity measurement (e.g.,\ncos,ρ) in tasks. Hence we want to know the impact of the combinations of different measurements in sentence dissimilarity and task similarity on final performance. Results are provided in Table 2. Again, we have several interesting observations. First, with different combinations of these measurements, our CRA significantly outperforms random ranking in almost all cases. This suggests that RSA is able to be adapted to NLP task structure detection. Second, in comparison to the combination of ρ and rs in the original RSA (Kriegeskorte et al., 2008), in our case, the combination of ρ and cos is better than other combinations in the majority of cases. Third, TinyBERT is robust to these different combinations than BERT."
    }, {
      "heading" : "4.7 Evaluating CNM with Different PLMs and Numbers of Voxels",
      "text" : "Since CNM bridges pretrained language models on the input side and voxels in fMRI images on the output side, we further evaluated CNM by varying the selection of PLMs (either BERT or TinyBERT) and the numbers of voxels. Results are displayed in Figure 3. It is interesting to find that with a small number of cognitive signals (voxels), TinyBERT for CNM can achieve a good task ranking score. By contrast, without sufficient cognitive signals, BERT for CNM fails in task similarity estimation, obtaining a task ranking score worse than random ranking. This is consistent with our previous finding in the main results that TinyBERT (with KD) captures more task-relevant knowledge than BERT for task relation detection. We further analyzed\nthe influence of pretrained language models and subjects on CNM from the model voxel prediction effect. This result in Appendix A.5 resonates with the main results, indicating that TinyBERT is better than BERT on all tasks."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Analysis on the Generality of Task Similarity Estimation to Underlying PLMs",
      "text" : "Models underlying our cross-task transfer learning are different pretrained language models, which is a widely acknowledged practice for transfer learning in NLP. We therefore want to investigate how general our task similarity estimation methods (e.g., CNM, CRA, AHP) are to the underlying models. This is important as we want to find a task taxonomy method that is not sensitive to underlying models. That is, the learned task taxonomy can be used to guide transfer learning for any model. For this, we first computed the Pearson correlation coefficient (ρ) and the Spearman rank correlation (rs) between task similarities obtained with TinyBERT and those with BERT using the same similarity estimation method. The correlation coefficients between BERT-based and TinyBERT-based task similarity matrices obtained by the CRA, CNM and AHP are (ρ = 0.23, rs = 0.11), (ρ = 0.85, rs = 0.76) and (ρ = 0.36, rs = 0.34) respectively. Both AHP and CRA show pool correlations between task similarity matrices using BERT and TinyBERT. On the contrary, CNM is very robust to the variations of underlying models. We speculate that both CRA and AHP capture task relations specific to underlying models while CNM could remove such bias by building the task taxonomy based on the cognitive data. In other words, CNM is able to detect model-agnostic task relations, yet another desirable advantage over AHP with exhaustive computation cost.\nTo further examine this hypothesis, we used the task ranking estimated with another underlying PLM x to guide transfer learning with an underlying PLM y. In our work, this would be using TinyBERT to guide BERT (TB → B) or vice versa. For each target task, we used the top k source tasks according to the task ranking with the guiding PLM x for transfer learning with the PLM y. The results were compared to the actual performance of transfer learning to the target task from the top 6 source tasks according to the task ranking with the PLM y itself. The probability of the top k source tasks\noccurring in the real top 6 tasks shows how much transferability learned with the PLM x can be used for the PLM y. Results are shown in Table 3, which again suggests the superiority of CNM over AHP.\nWe further analyzed the generality of CNM to different subjects of cognitive data used in CNM, which can be found in Appendix A.6. The experimental results show that the CNM is also robust to different subjects."
    }, {
      "heading" : "5.2 Taxonomy Tree of 12 NLP Tasks",
      "text" : "We visualize all pairwise task similarities for 12 tasks learned by CNM (averaged over 5 subjects) as a heatmap, shown in Figure 4(a). It is clear to see from the heatmap that 6 GLUE tasks (i.e., CoLA, QNLI, RTE, MNLI, SST-2, and MRPC) form a cluster. These tasks are all related to sentence understanding. We further perform hierarchical clustering over the 12 tasks according to their similarities to create a taxonomy tree, which is illustrated in Figure 4(b)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we have presented a cognitively inspired framework, termed CogTaxonomy, to learn relation and structure for NLP tasks. Experiments demonstrate that the task taxonomy detected by CogTaxonomy can be used to guide transfer learning across 12 different NLP tasks. Both CRA and CNM, the two essential components of CogTaxonomy, do not require exhaustive transfer learning across all source-target task pairs. The former is robust to different combinations of dissimilarity/similarity measurements. The latter resorts to cognitive signals to learn model-agnostic task relations."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Correlation Selection in Cognitive-Neural Mapping\nWe have different options for the calculation of CogR (e.g., R2 and ρ). Therefore, we computed task ranking scores with different options for CNM with BERT and TinyBERT. Results are shown in Figure 5. We find that R2 is better than ρ in almost all cases.\nA.2 Experimental Setting\nFine-tuning and Transferring For most tasks, we fine-tuned BERT to obtain task-specific representations, with the exception of RE and PR. REDN (Li and Tian, 2020) was used for RE tasks, and Cross-Encoder (Reimers and Gurevych, 2019) was used for PR. Table 4 shows the hyperparameters and configuration for task training in our experiments.\nFor source-to-target task transfer learning, all source models were fine-tuned on the target task dataset with the same settings as the target model.\nKnowledge Distillation Since TinyBERT has officially released models distilled on GLUE, we directly used them on our 8 GLUE tasks. For the PR task, we used the open-source model (Reimers and Gurevych, 2019) with the same TinyBERT ar-\nchitecture directly. For NER, QA, and RE tasks, we adopted the above fine-tuned models as the teacher model and used the open-source General TinyBERT for task-specific distillation, following the recommended practice of TinyBERT (Jiao et al., 2020). The number of epochs in the task-specific distillation phase is shown in Table 5, and the settings of other parameters are consistent with finetuning. In CoNLL 2003 (Sang and Meulder, 2003), we also carried out data augmentation according to the method proposed by TinyBERT (Jiao et al., 2020), while no data augmentation was performed on other datasets.\nA.3 Oracle Task Ranking\nEvaluation Metrics for 12 Tasks Among the 8 GLUE tasks, Matthews correlation was used in the CoLA task, Spearman correlation coefficient was used in the STS-B task. For the passage reranking task, NDCG@10 was used. All other tasks used F1 score as the evaluation metric.\nPairwise Transfer Learning Results and Oracle Ranking We obtain pair-wise transfer learning results based on the validation set of each task except the NER task of which the test set is publicly available. The results with BERT and TinyBERT are shown in the Table 6 and Table 7 respectively. Sorted by transfer performance, the oracle ranking of each source task to a target task is marked in parentheses.\nA.4 Actual Transfer Learning Performance from the Top 1 Source Task Selected by Different Task Similarity Estimation Methods to Each Target Task\nTransfer Learning with Same Underlying PLMs We use different task similarity estimation methods to find the most similar source task for each target task and obtain the transfer learning performance from the most similar source task to the target task. Both the task similarity estimation and transfer learning use the same underlying PLM. For each task similarity estimation method, average performance over all target tasks are reported in Table 8. It can be seen that the CRA, CNM, and CRA+CNM methods show good performance. Significantly, in terms of average target task performance, CRA+CNM is very close\nto AHP that requires exhaustive transfer learning across all source-target task pairs.\nTransfer Learning with Different Underlying PLMs This time the underlying PLMs for task similarity estimation and transfer learning are different from each other. Results are displayed in Table 9. Similarly, CRA+CNM achieves very competitive results to AHP in TinyBERT → BERT and even better results than AHP in BERT → TinyBERT.\nIn the two tables, we calculate the average transfer learning performance over 12 tasks shown in the last column of the two tables for easy comparison. The results of \"Random\" in the two tables are averaged over 5000 times of random sampling. Specifically, each round of random sampling selects a task other than itself for each target task as\nthe source task for transfer learning.\nA.5 CNM: Voxel Prediction Evaluation\nWe conducted experiments to take a deep look into the feed-forward neural mapping model in CNM. The number of voxels was set to 30K.\nPretrained Language Models We compared the prediction performance (measured by MSE between predicted results and ground-truth voxels) across different tasks using BERT vs. TinyBERT as the pretrained language model to obtain task-specific sentence representations. Results are shown in Figure 6(a). We can clearly see that both BERT and TinyBERT are better than the random\nbaseline across all tasks. And TinyBERT is better than BERT on all tasks, which resonates with the main results shown in Section 4.5.\nSubjects We analyzed prediction performance across different subjects, as shown in Figure 6(b). Although the prediction performance varies across different tasks, the shapes of the prediction performance curve over 12 tasks for different subjects are similar to each other, indicating that similar brain activities are activated for these tasks across different subjects.\nA.6 Analysis on the Generality of CNM across Subjects\nWe calculate task similarity matrices that are averaged over 5 subjects in CNM. Are results for individual subjects are consistent with each other? Hence we separately calculated task similarity matrices for each subject and used the Spearman correlation coefficient to measure task similarity matrix correlations among subjects. Results are shown in Figure 7, which indicates high correlations among subjects."
    } ],
    "references" : [ {
      "title" : "Representational similarity encoding for fmri: Pattern-based synthesis to predict brain activity using stimulus-model-similarities",
      "author" : [ "Andrew J. Anderson", "Benjamin Zinszer", "Rajeev D.S. Raizada." ],
      "venue" : "NeuroImage, 128:44–53.",
      "citeRegEx" : "Anderson et al\\.,? 2016",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "Reading behavior predicts syntactic categories",
      "author" : [ "Maria Barrett", "Anders Søgaard." ],
      "venue" : "Proceedings of the 19th Conference on Computational Natural Language Learning, CoNLL 2015, Beijing, China, July 30-31, 2015, pages 345–349. ACL.",
      "citeRegEx" : "Barrett and Søgaard.,? 2015",
      "shortCiteRegEx" : "Barrett and Søgaard.",
      "year" : 2015
    }, {
      "title" : "Extracting token-level signals of syntactic processing from fmri - with an application to pos induction",
      "author" : [ "Joachim Bingel", "Maria Barrett", "Anders Søgaard." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Bingel et al\\.,? 2016",
      "shortCiteRegEx" : "Bingel et al\\.",
      "year" : 2016
    }, {
      "title" : "Identifying beneficial task relations for multi-task learning in deep neural networks",
      "author" : [ "Joachim Bingel", "Anders Søgaard." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017,",
      "citeRegEx" : "Bingel and Søgaard.,? 2017",
      "shortCiteRegEx" : "Bingel and Søgaard.",
      "year" : 2017
    }, {
      "title" : "Volume 2: Short Papers, pages 164–169",
      "author" : [ "Valencia", "Spain", "April" ],
      "venue" : "Association for Computational Linguistics",
      "citeRegEx" : "Valencia et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Valencia et al\\.",
      "year" : 2017
    }, {
      "title" : "Speaking, seeing, understanding: Correlating semantic models with conceptual representation in the brain",
      "author" : [ "Luana Bulat", "Stephen Clark", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Bulat et al\\.,? 2017",
      "shortCiteRegEx" : "Bulat et al\\.",
      "year" : 2017
    }, {
      "title" : "Quantitative modeling of the neural representation of adjectivenoun phrases to account for fmri activation",
      "author" : [ "Kai-min Kevin Chang", "Vladimir Cherkassky", "Tom M. Mitchell", "Marcel Adam Just." ],
      "venue" : "ACL 2009, Proceedings of the 47th Annual Meeting of the",
      "citeRegEx" : "Chang et al\\.,? 2009",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2009
    }, {
      "title" : "Lifelong Machine Learning, Second Edition",
      "author" : [ "Zhiyuan Chen", "Bing Liu." ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers.",
      "citeRegEx" : "Chen and Liu.,? 2018",
      "shortCiteRegEx" : "Chen and Liu.",
      "year" : 2018
    }, {
      "title" : "Explaining knowledge distillation by quantifying the knowledge",
      "author" : [ "Xu Cheng", "Zhefan Rao", "Yilan Chen", "Quanshi Zhang." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey of domain adaptation for neural machine translation",
      "author" : [ "Chenhui Chu", "Rui Wang." ],
      "venue" : "In",
      "citeRegEx" : "Chu and Wang.,? 2018",
      "shortCiteRegEx" : "Chu and Wang.",
      "year" : 2018
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa." ],
      "venue" : "CoRR, abs/1103.0398.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Overview of the TREC 2019 deep learning track",
      "author" : [ "Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Daniel Campos", "Ellen M. Voorhees." ],
      "venue" : "CoRR, abs/2003.07820.",
      "citeRegEx" : "Craswell et al\\.,? 2020",
      "shortCiteRegEx" : "Craswell et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task learning with deep neural networks: A survey",
      "author" : [ "Michael Crawshaw." ],
      "venue" : "CoRR, abs/2009.09796.",
      "citeRegEx" : "Crawshaw.,? 2020",
      "shortCiteRegEx" : "Crawshaw.",
      "year" : 2020
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Representation similarity analysis for efficient task taxonomy & transfer learning",
      "author" : [ "Kshitij Dwivedi", "Gemma Roig." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 12387–",
      "citeRegEx" : "Dwivedi and Roig.,? 2019",
      "shortCiteRegEx" : "Dwivedi and Roig.",
      "year" : 2019
    }, {
      "title" : "Selective transfer between learning tasks using task-based boosting",
      "author" : [ "Eric Eaton", "Marie desJardins." ],
      "venue" : "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press.",
      "citeRegEx" : "Eaton and desJardins.,? 2011",
      "shortCiteRegEx" : "Eaton and desJardins.",
      "year" : 2011
    }, {
      "title" : "Linking artificial and human neural representations of language",
      "author" : [ "Jon Gauthier", "Roger Levy." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
      "citeRegEx" : "Gauthier and Levy.,? 2019",
      "shortCiteRegEx" : "Gauthier and Levy.",
      "year" : 2019
    }, {
      "title" : "Domain adaptive neural networks for object recognition",
      "author" : [ "Muhammad Ghifary", "W. Bastiaan Kleijn", "Mengjie Zhang." ],
      "venue" : "CoRR, abs/1409.6041.",
      "citeRegEx" : "Ghifary et al\\.,? 2014",
      "shortCiteRegEx" : "Ghifary et al\\.",
      "year" : 2014
    }, {
      "title" : "Universal neural machine translation for extremely low resource languages",
      "author" : [ "Jiatao Gu", "Hany Hassan", "Jacob Devlin", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Finding syntax in human encephalography with beam search",
      "author" : [ "John Hale", "Chris Dyer", "Adhiguna Kuncoro", "Jonathan Brennan." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne,",
      "citeRegEx" : "Hale et al\\.,? 2018",
      "shortCiteRegEx" : "Hale et al\\.",
      "year" : 2018
    }, {
      "title" : "Semeval-2010 task 8: Multiway classification of semantic relations between pairs",
      "author" : [ "Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Sebastian Padó", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz" ],
      "venue" : null,
      "citeRegEx" : "Hendrickx et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hendrickx et al\\.",
      "year" : 2010
    }, {
      "title" : "Advancing NLP with cognitive language processing signals",
      "author" : [ "Nora Hollenstein", "Maria Barrett", "Marius Troendle", "Francesco Bigiolli", "Nicolas Langer", "Ce Zhang." ],
      "venue" : "CoRR, abs/1904.02682.",
      "citeRegEx" : "Hollenstein et al\\.,? 2019a",
      "shortCiteRegEx" : "Hollenstein et al\\.",
      "year" : 2019
    }, {
      "title" : "Cognival: A framework for cognitive word embedding evaluation",
      "author" : [ "Nora Hollenstein", "Antonio de la Torre", "Nicolas Langer", "Ce Zhang." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning, CoNLL 2019, Hong Kong,",
      "citeRegEx" : "Hollenstein et al\\.,? 2019b",
      "shortCiteRegEx" : "Hollenstein et al\\.",
      "year" : 2019
    }, {
      "title" : "Entity recognition at first sight: Improving NER with eye movement information",
      "author" : [ "Nora Hollenstein", "Ce Zhang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Hollenstein and Zhang.,? 2019",
      "shortCiteRegEx" : "Hollenstein and Zhang.",
      "year" : 2019
    }, {
      "title" : "Meta-learning in neural networks: A survey",
      "author" : [ "Timothy M. Hospedales", "Antreas Antoniou", "Paul Micaelli", "Amos J. Storkey." ],
      "venue" : "CoRR, abs/2004.05439.",
      "citeRegEx" : "Hospedales et al\\.,? 2020",
      "shortCiteRegEx" : "Hospedales et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural speech reveals the semantic maps that tile human cerebral cortex",
      "author" : [ "Alexander G. Huth", "Wendy A. de Heer", "Thomas L. Griffiths", "Frédéric E. Theunissen", "Jack L. Gallant." ],
      "venue" : "Nat., 532(7600):453– 458.",
      "citeRegEx" : "Huth et al\\.,? 2016",
      "shortCiteRegEx" : "Huth et al\\.",
      "year" : 2016
    }, {
      "title" : "Task-embedded control networks for fewshot imitation learning",
      "author" : [ "Stephen James", "Michael Bloesch", "Andrew J. Davison." ],
      "venue" : "2nd Annual Conference on Robot Learning, CoRL 2018, Zürich, Switzerland, 29-31 October 2018, Proceedings, volume 87 of Pro-",
      "citeRegEx" : "James et al\\.,? 2018",
      "shortCiteRegEx" : "James et al\\.",
      "year" : 2018
    }, {
      "title" : "Tinybert: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event,",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Adapting high-resource NMT models to translate low-resource related languages",
      "author" : [ "Wei-Jen Ko", "Ahmed El-Kishky", "Adithya Renduchintala", "Vishrav Chaudhary", "Naman Goyal", "Francisco Guzmán", "Pascale Fung", "Philipp Koehn", "Mona T. Diab" ],
      "venue" : null,
      "citeRegEx" : "Ko et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ko et al\\.",
      "year" : 2021
    }, {
      "title" : "Representational similarity analysis – connecting the branches of systems neuroscience",
      "author" : [ "N. Kriegeskorte", "Marieke Mur", "P. Bandettini." ],
      "venue" : "Frontiers in Systems Neuroscience, 2.",
      "citeRegEx" : "Kriegeskorte et al\\.,? 2008",
      "shortCiteRegEx" : "Kriegeskorte et al\\.",
      "year" : 2008
    }, {
      "title" : "Meta reinforcement learning with task embedding and shared policy",
      "author" : [ "Lin Lan", "Zhenguo Li", "Xiaohong Guan", "Pinghui Wang." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China,",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Downstream model design of pre-trained language model for relation extraction task",
      "author" : [ "Cheng Li", "Ye Tian." ],
      "venue" : "CoRR, abs/2004.03786.",
      "citeRegEx" : "Li and Tian.,? 2020",
      "shortCiteRegEx" : "Li and Tian.",
      "year" : 2020
    }, {
      "title" : "The natural language decathlon: Multitask learning as question answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "What happens to BERT embeddings during fine-tuning",
      "author" : [ "Amil Merchant", "Elahe Rahimtoroghi", "Ellie Pavlick", "Ian Tenney" ],
      "venue" : "In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,",
      "citeRegEx" : "Merchant et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Merchant et al\\.",
      "year" : 2020
    }, {
      "title" : "Predicting human brain activity associated with the meanings of nouns",
      "author" : [ "Tom Michael Mitchell", "S. Shinkareva", "Andrew Carlson", "K. Chang", "Vicente L. Malave", "R. Mason", "M. Just." ],
      "venue" : "Science, 320:1191 – 1195.",
      "citeRegEx" : "Mitchell et al\\.,? 2008",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2008
    }, {
      "title" : "Selecting corpus-semantic models for neurolinguistic decoding",
      "author" : [ "Brian Murphy", "Partha P. Talukdar", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the First Joint Conference on Lexical and Computational Semantics, *SEM 2012, June 7-8, 2012, Montréal, Canada,",
      "citeRegEx" : "Murphy et al\\.,? 2012",
      "shortCiteRegEx" : "Murphy et al\\.",
      "year" : 2012
    }, {
      "title" : "Human brain activity for machine attention",
      "author" : [ "Lukas Muttenthaler", "Nora Hollenstein", "Maria Barrett." ],
      "venue" : "CoRR, abs/2006.05113.",
      "citeRegEx" : "Muttenthaler et al\\.,? 2020",
      "shortCiteRegEx" : "Muttenthaler et al\\.",
      "year" : 2020
    }, {
      "title" : "MS MARCO: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "Proceedings of the Workshop on Cognitive Computation: Integrat-",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "A decade survey of transfer learning (2010-2020)",
      "author" : [ "Shuteng Niu", "Yongxin Liu", "Jian Wang", "Houbing Song." ],
      "venue" : "IEEE Trans. Artif. Intell., 1(2):151– 166.",
      "citeRegEx" : "Niu et al\\.,? 2020",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "Toward a universal decoder of linguistic meaning from brain activation",
      "author" : [ "Francisco Pereira", "Bin Lou", "Brianna Pritchett", "Samuel Ritter", "Samuel J Gershman", "Nancy Kanwisher", "Matthew Botvinick", "Evelina Fedorenko." ],
      "venue" : "Nature communications,",
      "citeRegEx" : "Pereira et al\\.,? 2018",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "An overview of multi-task learning in deep neural networks",
      "author" : [ "Sebastian Ruder." ],
      "venue" : "CoRR, abs/1706.05098.",
      "citeRegEx" : "Ruder.,? 2017",
      "shortCiteRegEx" : "Ruder.",
      "year" : 2017
    }, {
      "title" : "The analytic hierarchy process—what it is and how it is used",
      "author" : [ "R.W. Saaty." ],
      "venue" : "Mathematical Modelling, 9(3):161–176.",
      "citeRegEx" : "Saaty.,? 1987",
      "shortCiteRegEx" : "Saaty.",
      "year" : 1987
    }, {
      "title" : "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Lifelong machine learning systems: Beyond learning algorithms",
      "author" : [ "Daniel L. Silver", "Qiang Yang", "Lianghao Li." ],
      "venue" : "Lifelong Machine Learning, Papers from the 2013 AAAI Spring Symposium, Palo Alto, California, USA, March 25-27, 2013, volume SS-13-",
      "citeRegEx" : "Silver et al\\.,? 2013",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2013
    }, {
      "title" : "Evaluating word embeddings with fmri and eye-tracking",
      "author" : [ "Anders Søgaard." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, RepEval@ACL 2016, Berlin, Germany, August 2016, pages 116–121. Association for Com-",
      "citeRegEx" : "Søgaard.,? 2016",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2016
    }, {
      "title" : "Deep model transferability from attribution maps",
      "author" : [ "Jie Song", "Yixin Chen", "Xinchao Wang", "Chengchao Shen", "Mingli Song." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Which tasks should be learned together in multi-task learning? CoRR, abs/1905.07553",
      "author" : [ "Trevor Standley", "Amir Roshan Zamir", "Dawn Chen", "Leonidas J. Guibas", "Jitendra Malik", "Silvio Savarese" ],
      "venue" : null,
      "citeRegEx" : "Standley et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Standley et al\\.",
      "year" : 2019
    }, {
      "title" : "Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)",
      "author" : [ "Mariya Toneva", "Leila Wehbe." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Pro-",
      "citeRegEx" : "Toneva and Wehbe.,? 2019",
      "shortCiteRegEx" : "Toneva and Wehbe.",
      "year" : 2019
    }, {
      "title" : "Tasksimilarity aware meta-learning through nonparametric kernel regression",
      "author" : [ "Arun Venkitaraman", "Bo Wahlberg." ],
      "venue" : "CoRR, abs/2006.07212.",
      "citeRegEx" : "Venkitaraman and Wahlberg.,? 2020",
      "shortCiteRegEx" : "Venkitaraman and Wahlberg.",
      "year" : 2020
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "CoRR, abs/1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Transfer learning via minimizing the performance gap between domains",
      "author" : [ "Boyu Wang", "Jorge A. Mendez", "Mingbo Cai", "Eric Eaton." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-task learning for natural language processing in the 2020s: Where are we going? Pattern Recognit",
      "author" : [ "Joseph Worsham", "Jugal Kalita." ],
      "venue" : "Lett., 136:120–126.",
      "citeRegEx" : "Worsham and Kalita.,? 2020",
      "shortCiteRegEx" : "Worsham and Kalita.",
      "year" : 2020
    }, {
      "title" : "Adaptsum: Towards low-resource domain adaptation for abstractive summarization",
      "author" : [ "Tiezheng Yu", "Zihan Liu", "Pascale Fung." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Yu et al\\.,? 2021",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2021
    }, {
      "title" : "A new semisupervised inductive transfer learning framework: Co-transfer",
      "author" : [ "Zhe Yuan", "Yimin Wen." ],
      "venue" : "CoRR, abs/2108.07930.",
      "citeRegEx" : "Yuan and Wen.,? 2021",
      "shortCiteRegEx" : "Yuan and Wen.",
      "year" : 2021
    }, {
      "title" : "Taskonomy: Disentangling task transfer learning",
      "author" : [ "Amir Roshan Zamir", "Alexander Sax", "William B. Shen", "Leonidas J. Guibas", "Jitendra Malik", "Silvio Savarese." ],
      "venue" : "2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,",
      "citeRegEx" : "Zamir et al\\.,? 2018",
      "shortCiteRegEx" : "Zamir et al\\.",
      "year" : 2018
    }, {
      "title" : "A closer look at how fine-tuning changes BERT",
      "author" : [ "Yichu Zhou", "Vivek Srikumar." ],
      "venue" : "CoRR, abs/2106.14282. 12",
      "citeRegEx" : "Zhou and Srikumar.,? 2021",
      "shortCiteRegEx" : "Zhou and Srikumar.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 57,
      "context" : "Is there a principle to guide transfer learning across tasks in natural language processing (NLP)? Taxonomy (Zamir et al., 2018) finds that a structure exists among visual tasks, as a principle underlying transfer learning for them.",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 44,
      "context" : "Experiments on 12 NLP tasks, where BERT/TinyBERT are used as the underlying models for transfer learning, demonstrate that the proposed CogTaxonomy is able to guide transfer learning, achieving performance competitive to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al.",
      "startOffset" : 252,
      "endOffset" : 265
    }, {
      "referenceID" : 57,
      "context" : "Experiments on 12 NLP tasks, where BERT/TinyBERT are used as the underlying models for transfer learning, demonstrate that the proposed CogTaxonomy is able to guide transfer learning, achieving performance competitive to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018) but without requiring exhaustive pairwise O(m(2)) task transferring.",
      "startOffset" : 291,
      "endOffset" : 311
    }, {
      "referenceID" : 13,
      "context" : ", TL from pretrained language models (PLM) to downstream tasks (Devlin et al., 2018; Radford et al., 2018), from a task with rich labeled data to a task with low resource (Chu and Wang, 2018; Yu et al.",
      "startOffset" : 63,
      "endOffset" : 106
    }, {
      "referenceID" : 40,
      "context" : ", TL from pretrained language models (PLM) to downstream tasks (Devlin et al., 2018; Radford et al., 2018), from a task with rich labeled data to a task with low resource (Chu and Wang, 2018; Yu et al.",
      "startOffset" : 63,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : ", 2018), from a task with rich labeled data to a task with low resource (Chu and Wang, 2018; Yu et al., 2021), from highresource languages to low-resource languages (Gu et al.",
      "startOffset" : 72,
      "endOffset" : 109
    }, {
      "referenceID" : 55,
      "context" : ", 2018), from a task with rich labeled data to a task with low resource (Chu and Wang, 2018; Yu et al., 2021), from highresource languages to low-resource languages (Gu et al.",
      "startOffset" : 72,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : ", 2021), from highresource languages to low-resource languages (Gu et al., 2018; Ko et al., 2021), etc.",
      "startOffset" : 63,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : ", 2021), from highresource languages to low-resource languages (Gu et al., 2018; Ko et al., 2021), etc.",
      "startOffset" : 63,
      "endOffset" : 97
    }, {
      "referenceID" : 57,
      "context" : "Such task taxonomy is of notable values to transfer learning in NLP in that it has the potential to guide TL and reduce redundancies across tasks (Zamir et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 57,
      "context" : "In this paper, partially inspired by the task taxonomy in visual tasks (Zamir et al., 2018), we study the hierarchical task structure for NLP tasks.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 57,
      "context" : "But significantly different from the visual Taskonomy (Zamir et al., 2018), we construct NLP taskonomy from a cognitively inspired perspective.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : ", eye-tracking, EEG, fMRI) have been explored to enhance neural models for a wide range of NLP tasks (Barrett and Søgaard, 2015; Bingel et al., 2016; Hollenstein and Zhang, 2019; Hollenstein et al., 2019a).",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 2,
      "context" : ", eye-tracking, EEG, fMRI) have been explored to enhance neural models for a wide range of NLP tasks (Barrett and Søgaard, 2015; Bingel et al., 2016; Hollenstein and Zhang, 2019; Hollenstein et al., 2019a).",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 23,
      "context" : ", eye-tracking, EEG, fMRI) have been explored to enhance neural models for a wide range of NLP tasks (Barrett and Søgaard, 2015; Bingel et al., 2016; Hollenstein and Zhang, 2019; Hollenstein et al., 2019a).",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 21,
      "context" : ", eye-tracking, EEG, fMRI) have been explored to enhance neural models for a wide range of NLP tasks (Barrett and Søgaard, 2015; Bingel et al., 2016; Hollenstein and Zhang, 2019; Hollenstein et al., 2019a).",
      "startOffset" : 101,
      "endOffset" : 205
    }, {
      "referenceID" : 34,
      "context" : "On the other hand, representations learned in NLP models are used to predict brain activation patterns recorded in cognitive processing data (Mitchell et al., 2008; Pereira et al., 2018; Hale et al., 2018; Hollenstein et al., 2019b).",
      "startOffset" : 141,
      "endOffset" : 232
    }, {
      "referenceID" : 39,
      "context" : "On the other hand, representations learned in NLP models are used to predict brain activation patterns recorded in cognitive processing data (Mitchell et al., 2008; Pereira et al., 2018; Hale et al., 2018; Hollenstein et al., 2019b).",
      "startOffset" : 141,
      "endOffset" : 232
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, representations learned in NLP models are used to predict brain activation patterns recorded in cognitive processing data (Mitchell et al., 2008; Pereira et al., 2018; Hale et al., 2018; Hollenstein et al., 2019b).",
      "startOffset" : 141,
      "endOffset" : 232
    }, {
      "referenceID" : 22,
      "context" : "On the other hand, representations learned in NLP models are used to predict brain activation patterns recorded in cognitive processing data (Mitchell et al., 2008; Pereira et al., 2018; Hale et al., 2018; Hollenstein et al., 2019b).",
      "startOffset" : 141,
      "endOffset" : 232
    }, {
      "referenceID" : 29,
      "context" : "CRA extracts task representations from NLP models and employs Representational Similarity Analysis (RSA) (Kriegeskorte et al., 2008), which is commonly used to measure the correlation between brain activity and computational model, to estimate NLP task similarity.",
      "startOffset" : 105,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "We use pretrained language models fine-tuned on specific tasks, particularly BERT (Devlin et al., 2018) and TinyBERT (Jiao et al.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : ", 2018) and TinyBERT (Jiao et al., 2020), to obtain sentence representations.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 57,
      "context" : "We compare the proposed CogTaskonomy against the Analytic Hierarchy Process (AHP) used in Taskonomy (Zamir et al., 2018).",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 34,
      "context" : "Using NLP Representations for Brain Activity Prediction Since the pioneering work (Mitchell et al., 2008), connecting statistical NLP representations with cognition has attracted widespread attention.",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 35,
      "context" : "A great deal of research (Murphy et al., 2012; Anderson et al., 2016; Søgaard, 2016; Bulat et al., 2017) has been devoted to word decoding.",
      "startOffset" : 25,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "A great deal of research (Murphy et al., 2012; Anderson et al., 2016; Søgaard, 2016; Bulat et al., 2017) has been devoted to word decoding.",
      "startOffset" : 25,
      "endOffset" : 104
    }, {
      "referenceID" : 47,
      "context" : "A great deal of research (Murphy et al., 2012; Anderson et al., 2016; Søgaard, 2016; Bulat et al., 2017) has been devoted to word decoding.",
      "startOffset" : 25,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "A great deal of research (Murphy et al., 2012; Anderson et al., 2016; Søgaard, 2016; Bulat et al., 2017) has been devoted to word decoding.",
      "startOffset" : 25,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "Many other works use cognitive processing signals to improve NLP models (Barrett and Søgaard, 2015; Bingel et al., 2016; Gauthier and Levy, 2019; Hollenstein and Zhang, 2019), just to name a few.",
      "startOffset" : 72,
      "endOffset" : 174
    }, {
      "referenceID" : 2,
      "context" : "Many other works use cognitive processing signals to improve NLP models (Barrett and Søgaard, 2015; Bingel et al., 2016; Gauthier and Levy, 2019; Hollenstein and Zhang, 2019), just to name a few.",
      "startOffset" : 72,
      "endOffset" : 174
    }, {
      "referenceID" : 16,
      "context" : "Many other works use cognitive processing signals to improve NLP models (Barrett and Søgaard, 2015; Bingel et al., 2016; Gauthier and Levy, 2019; Hollenstein and Zhang, 2019), just to name a few.",
      "startOffset" : 72,
      "endOffset" : 174
    }, {
      "referenceID" : 23,
      "context" : "Many other works use cognitive processing signals to improve NLP models (Barrett and Søgaard, 2015; Bingel et al., 2016; Gauthier and Levy, 2019; Hollenstein and Zhang, 2019), just to name a few.",
      "startOffset" : 72,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "A very important trend in recent NLP is that models, algorithms, and solutions are not developed for only a single task, but for multiple tasks or across tasks (Devlin et al., 2018; Radford et al., 2018; McCann et al., 2018; Worsham and Kalita, 2020).",
      "startOffset" : 160,
      "endOffset" : 250
    }, {
      "referenceID" : 40,
      "context" : "A very important trend in recent NLP is that models, algorithms, and solutions are not developed for only a single task, but for multiple tasks or across tasks (Devlin et al., 2018; Radford et al., 2018; McCann et al., 2018; Worsham and Kalita, 2020).",
      "startOffset" : 160,
      "endOffset" : 250
    }, {
      "referenceID" : 32,
      "context" : "A very important trend in recent NLP is that models, algorithms, and solutions are not developed for only a single task, but for multiple tasks or across tasks (Devlin et al., 2018; Radford et al., 2018; McCann et al., 2018; Worsham and Kalita, 2020).",
      "startOffset" : 160,
      "endOffset" : 250
    }, {
      "referenceID" : 54,
      "context" : "A very important trend in recent NLP is that models, algorithms, and solutions are not developed for only a single task, but for multiple tasks or across tasks (Devlin et al., 2018; Radford et al., 2018; McCann et al., 2018; Worsham and Kalita, 2020).",
      "startOffset" : 160,
      "endOffset" : 250
    }, {
      "referenceID" : 10,
      "context" : ", in the form of regularization or sharing parameters across tasks (Collobert et al., 2011).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 43,
      "context" : "It is important in multitask learning to find related tasks for target tasks as auxiliary tasks (Ruder, 2017).",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "domain adaptation), inductive TL (same domain, different task) and unsupervised TL (both different) (Eaton and desJardins, 2011; Ghifary et al., 2014; Wang et al., 2019; Yuan and Wen, 2021).",
      "startOffset" : 100,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : "domain adaptation), inductive TL (same domain, different task) and unsupervised TL (both different) (Eaton and desJardins, 2011; Ghifary et al., 2014; Wang et al., 2019; Yuan and Wen, 2021).",
      "startOffset" : 100,
      "endOffset" : 189
    }, {
      "referenceID" : 53,
      "context" : "domain adaptation), inductive TL (same domain, different task) and unsupervised TL (both different) (Eaton and desJardins, 2011; Ghifary et al., 2014; Wang et al., 2019; Yuan and Wen, 2021).",
      "startOffset" : 100,
      "endOffset" : 189
    }, {
      "referenceID" : 56,
      "context" : "domain adaptation), inductive TL (same domain, different task) and unsupervised TL (both different) (Eaton and desJardins, 2011; Ghifary et al., 2014; Wang et al., 2019; Yuan and Wen, 2021).",
      "startOffset" : 100,
      "endOffset" : 189
    }, {
      "referenceID" : 38,
      "context" : "If the source and target are dissimilar, negative transfer may hurt TL (Niu et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "Meta Learning aims to gain experience over a set of related tasks for improving the learning algorithm itself (Hospedales et al., 2020).",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 51,
      "context" : "Existing meta learning methods implicitly assume that tasks are similar to each other, but it is often unclear how to quantify task similarities and their roles in learning (Venkitaraman and Wahlberg, 2020).",
      "startOffset" : 173,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "Lifelong Learning is to learn continuously and accumulate knowledge along a sequence of tasks and uses it for future learning (Chen and Liu, 2018).",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 46,
      "context" : "The system is tuned to be able to select the most related prior knowledge to bias the learning towards a new task favourably (Silver et al., 2013).",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "The first is task grouping or clustering, which divides a set of tasks into clusters so that tasks in the same cluster can be jointly trained (Bingel and Søgaard, 2017; Standley et al., 2019).",
      "startOffset" : 142,
      "endOffset" : 191
    }, {
      "referenceID" : 49,
      "context" : "The first is task grouping or clustering, which divides a set of tasks into clusters so that tasks in the same cluster can be jointly trained (Bingel and Søgaard, 2017; Standley et al., 2019).",
      "startOffset" : 142,
      "endOffset" : 191
    }, {
      "referenceID" : 57,
      "context" : "The second is learning transfer relationships, which analyzes whether transfer between tasks is beneficial to learning, regardless of whether tasks are related or not (Zamir et al., 2018; Dwivedi and Roig, 2019; Song et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 230
    }, {
      "referenceID" : 14,
      "context" : "The second is learning transfer relationships, which analyzes whether transfer between tasks is beneficial to learning, regardless of whether tasks are related or not (Zamir et al., 2018; Dwivedi and Roig, 2019; Song et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 230
    }, {
      "referenceID" : 48,
      "context" : "The second is learning transfer relationships, which analyzes whether transfer between tasks is beneficial to learning, regardless of whether tasks are related or not (Zamir et al., 2018; Dwivedi and Roig, 2019; Song et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 230
    }, {
      "referenceID" : 26,
      "context" : "The third is task embedding, which learns a specific representation space for tasks (James et al., 2018; Lan et al., 2019).",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : "The third is task embedding, which learns a specific representation space for tasks (James et al., 2018; Lan et al., 2019).",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 27,
      "context" : "TinyBERT TinyBERT (Jiao et al., 2020) performs knowledge distillation at both the pretraining and fine-tuning stage.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "By leveraging KD, TinyBERT learns to transfer knowledge encoded in the large teacher BERT (Devlin et al., 2018) to itself.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 29,
      "context" : "In this way, it successfully captures cross-modal data relationships (Kriegeskorte et al., 2008).",
      "startOffset" : 69,
      "endOffset" : 96
    }, {
      "referenceID" : 52,
      "context" : "We selected 8 NLP tasks from the GLUE benchmark (Wang et al., 2018), including CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2, STS-B.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 52,
      "context" : "tasks are considered important for generalizable natural language understanding, exhibiting diversity in domains, dataset sizes, and difficulties (Wang et al., 2018).",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 41,
      "context" : "0 (Rajpurkar et al., 2018), Semeval-2010 task 8 (Hendrickx et al.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 20,
      "context" : ", 2018), Semeval-2010 task 8 (Hendrickx et al., 2010), CoNLL 2003 (Sang and Meulder, 2003), MS MARCO (Nguyen et al.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 45,
      "context" : ", 2010), CoNLL 2003 (Sang and Meulder, 2003), MS MARCO (Nguyen et al.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 37,
      "context" : ", 2010), CoNLL 2003 (Sang and Meulder, 2003), MS MARCO (Nguyen et al., 2016; Craswell et al., 2020), respectively.",
      "startOffset" : 55,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : ", 2010), CoNLL 2003 (Sang and Meulder, 2003), MS MARCO (Nguyen et al., 2016; Craswell et al., 2020), respectively.",
      "startOffset" : 55,
      "endOffset" : 99
    }, {
      "referenceID" : 57,
      "context" : "We mainly used two methods as our baselines, including Direct Similarity Estimation (DSE), Analytic Hierarchy Process (AHP) (Zamir et al., 2018).",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 57,
      "context" : "For more details about AHP, please refer to (Zamir et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 29,
      "context" : "Second, in comparison to the combination of ρ and rs in the original RSA (Kriegeskorte et al., 2008), in our case, the combination of ρ and cos is better than other combinations in the majority of cases.",
      "startOffset" : 73,
      "endOffset" : 100
    } ],
    "year" : 0,
    "abstractText" : "Is there a principle to guide transfer learning across tasks in natural language processing (NLP)? Taxonomy (Zamir et al., 2018) finds that a structure exists among visual tasks, as a principle underlying transfer learning for them. In this paper, we propose a cognitively inspired framework, CogTaskonomy, to learn taxonomy for NLP tasks. The framework consists of Cognitive Representation Analytics (CRA) and Cognitive-Neural Mapping (CNM). The former employs Representational Similarity Analysis, which is commonly used in computational neuroscience to find a correlation between brainactivity measurement and computational modeling, to estimate task similarity with taskspecific sentence representations. The latter learns to detect task relations by projecting neural representations from NLP models to cognitive signals (i.e., fMRI voxels). Experiments on 12 NLP tasks, where BERT/TinyBERT are used as the underlying models for transfer learning, demonstrate that the proposed CogTaxonomy is able to guide transfer learning, achieving performance competitive to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018) but without requiring exhaustive pairwise O(m) task transferring. Analyses further discover that CNM is capable of learning model-agnostic task taxonomy.",
    "creator" : null
  }
}
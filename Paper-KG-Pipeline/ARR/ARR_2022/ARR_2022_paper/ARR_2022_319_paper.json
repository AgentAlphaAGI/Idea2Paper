{
  "name" : "ARR_2022_319_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Logic Traps in Evaluating Attribution Scores",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The opaqueness of deep models has grown in tandem with their power (Doshi-Velez and Kim, 2017), which has motivated efforts to interpret how these black-box models work (Sundararajan et al., 2017; Belinkov and Glass, 2019). Post-hoc explanation aims to explain a trained model and reveal how the model arrives at a decision (Jacovi and Goldberg, 2020; Molnar, 2020). This goal is usually approached with attribution method, which assesses the influence of features on model predictions as shown in Figure 1. Recent years have witnessed an increasing number of attribution methods being developed. For example, Erasure-based\nmethod calculate attribution scores by measuring the change of output after removing target features (Li et al., 2016; Feng et al., 2018; Chen et al., 2020); Gradient-based method uses gradients to study the influence of features on model predictions (Sundararajan et al., 2017; Wallace et al., 2019; Hao et al., 2020); Meanwhile, these methods also received much scrutiny, arguing that the generated attribution scores are fragile or unreliable (AlvarezMelis and Jaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020).\nAs an explanation method, the evaluation criteria of attribution methods should be how accurately it reflects the true reasoning process of the model (faithfulness), not how convincing it is to humans (plausibility) Jacovi and Goldberg (2020). Meanwhile, since the reasoning process of deep models is inaccessible, researchers design various evaluation methods to support their arguments, some of which appear valid and are widely used in the research field. For example, meaningful perturbation is used for making comparison in many works (Samek et al., 2016; Chen et al., 2018; DeYoung et al., 2019; Chen et al., 2020; Kim et al., 2020). The philosophy of meaningful perturbation is simple, i.e., modifications to the input instances, which are in accordance with the generated attribution scores, can bring about significant differences to model predictions if the attribution scores are faithful to the target system.\nHowever, some crucial logic traps existing in these evaluation methods are ignored in most works, causing inaccurate evaluation and unfair\ncomparison. For example, we found that we can manipulate the evaluation results when using meaningful perturbation to make comparisons: by choosing the modification strategy, we can assign any of the three candidate attribution methods as the best method. The neglect of these traps has damaged the community in many aspects: First, the existence of logic traps will lead to an inaccurate evaluation and unfair comparison, making the conclusions unreliable; Second, the wide use of evaluation metrics with logic traps brings pressure to newly proposed works, requiring them to compare with other works using the same metrics; Last, the over-belief in existing evaluation metrics encourages efforts to propose more accurate attribution methods, notwithstanding the evaluation system is unreliable.\nIn this paper, we systematically review existing methods for evaluating attribution scores and categorize them into classes. We summarize the logic traps in these methods and further conduct experiments to demonstrate the existence of each logical trap. Though strictly accurate evaluation metrics for attribution methods might be a “unicorn” which will likely never be found, we should not just ignore logic traps in existing evaluation methods and draw conclusions recklessly. Through both theoretical and experimental analysis, we hope to increase attention on the inaccurate evaluation of attribution scores. Moreover, with this paper, we suggest stopping focusing on improving performance under unreliable evaluation systems and starting efforts on reducing the impact of proposed logic traps."
    }, {
      "heading" : "2 Evaluation Methods and Corresponding Logic Traps",
      "text" : ""
    }, {
      "heading" : "2.1 Part I",
      "text" : "Evaluation 1: Using Human Annotated Explanations As the Ground Truth\nEvaluation 1 verifies the validity of the attribution scores by comparing them with the human problem-solving process. In this evaluation, works (e.g., Murdoch et al. (2018); Kim et al. (2020); Sundararajan et al. (2017)) often give examples consistent with human understandings to demonstrate the superiority of their proposed method. For example, as shown in Table 1, Murdoch et al. (2018) shows heat maps for a yelp review generated by different attribution techniques. They argue that the proposed method: Contextual decomposition, is better than others because only it can identify\n‘favorite’ as positive and ‘used to be’ as negative, which is consistent with human understandings.\nFurthermore, resorting to human-annotated explanations, works can also evaluate attribution methods quantitatively in evaluation 1. For example, the SST-2 (Socher et al., 2013) corpus provides not only sentence-level labels, but also five-class word-level sentiment tags ranging from very negative to very positive. Thus, many works (Lei et al., 2016; Li et al., 2016; Tsang et al., 2020; Kim et al., 2020) perform quantitative evaluation of attribution scores by comparing them with the word-level tags in SST-2.\nLogic Trap 1: The decision-making process of neural networks is not equal to the decisionmaking process of humans.\nFirst, we cannot completely deny the rationality of evaluation 1. Since many attribution methods work without any human-annotated information, such as erasure-based and gradient-based methods, the similarity between human-annotated explanations and generated attribution scores can be seen as drawing from the reasoning process of target models. However, since the deep model often rely on unreasonable correlations, even when producing correct predictions, attribution scores preposterous to humans may reflect the reasoning process of the deep model faithfully. Thus we cannot deny the validity of an attribution score through its inconsistency to human-annotated explanations and cannot use human-annotated explanations to conduct a quantitative evaluation.\nExperiment 1: In experiment 1, we give an example to demonstrate that the model might rely on correlations inconsistent with human understandings to get the prediction: though trained with questions, a question-answering model could maintain the same prediction for a large ratio of samples when the question information is missing, which is obviously\ndifferent from humans. We experiment on RACE (Lai et al., 2017), a large-scale question-answering dataset. As shown in Table 2, RACE requires the model to choose the right answer from candidate options according to the given question and document.\nWe first train a model with BERTbase (Devlin et al., 2018) as encoder1 with questions, and achieve 65.7% accuracy on the development set. Then, we replace the development set questions with empty strings and feed them into the trained model. Surprisingly, the trained MRC model maintained the original prediction on 64.0% of the test set samples (68.4% on correctly answered samples and 55.4% on wrongly answered samples). Moreover, we analyze the model confidence change in these unchanged samples, where the probability on the predicted label is used as the confidence score. As shown in Figure 2, most of the samples have confidence decrease smaller than 0.1, demonstrating question information are not essential for the model to get predictions in these samples.\nSince question information is usually crucial for humans to answer the question, attribution scores faithfully reflect the reasoning process of this model may be inconsistent with human annotations. Thus, it is improper to use human-annotation\n1Our implementations of experiment 1 and experiment 2 are based on the Huggingface’s transformer model hub (https://github.com/huggingface/ transformers), and we use its default model architectures without change for corresponding tasks.\nexplanations as the ground truth to evaluate attribution methods."
    }, {
      "heading" : "2.2 Part II",
      "text" : "Evaluation 2: Evaluation Based on Meaningful Perturbation\nMost existing methods for quantitatively evaluating attribution scores can be summarized as evaluations based on meaningful perturbation. The philosophy of meaningful perturbation is simple, i.e., modifications to the input instances, which are in accordance with the generated attribution scores, can bring about significant differences to the target model’s predictions if the attribution scores are faithful to the target model.\nFor example, Samek et al. (2016); Nguyen (2018); Chen and Ji (2020) use the area over the perturbation curve (AOPC) (Samek et al., 2016) as evaluation metrics. Specifically, given the attribution scores of a set of features, AOPC(k) modifies the top k% features and calculates the average change in the prediction probability as follows,\nAOPC(K) = 1\nN N∑ i=1 { p(ŷ|xi)− p(ŷ|x̃(k)i ) } where ŷ is the predicted label, N is the number of examples, p(ŷ|) is the probability on the predicted class, and x̃(k)i is modified sample. Higher AOPCs is better, which means that the features chosen by attribution scores are more important; Feng et al. (2018); Petsiuk et al. (2018); Kim et al. (2020) use area under the curve (AUC) to evaluate attribution scores. As shown in Figure 3, AUC plots a prediction probability curve about modified feature numbers where features are modified in order of attribution scores. The argument is if attribution scores are faithful, then the curve will drop rapidly, resulting in a small area under a curve.\nBesides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons. The main difference between evaluation metrics in these works is the difference in the modification strategy. For example, to evaluate word-level attribution scores for SST-2, Chen et al. (2020) uses deleting tokens as modification while Kim et al. (2020) uses replacing tokens with tokens sampled from the distribution inferred by BERT.\nLogic Trap 2: Using an attribution method as the ground truth to evaluate the target attribution method.\nEvaluation methods based on meaningful perturbation can be seen as an attribution method too. For example, AOPC(k), which assesses the importance of k% features, can be seen as an attribution method calculating an attribution score for k% features. Specifically, when using deleting tokens as modification and narrowing the k% to one token, AOPC(k) degenerates into the basic attribution method: leave-one-out (Li et al., 2016). Thus, evaluation 2 uses an attribution method as the ground truth to evaluate the target attribution method, which measures the similarity between two attribution methods instead of faithfulness.\nSince meaningful perturbation assesses the importance of features by calculating output change after modifications, its results are mainly depend on how to conduct the modifications, which means different modification strategies might lead to different evaluation results. Evaluation 2 is widely used to compare attribution methods in the research field. Accordingly, the neglect of logic trap 2 has led to a high risk of unfair comparisons and unreliable conclusions.\nExperiment 2: In experiment 2, we give an example of unfair comparisons in evaluation 2: the more similar the target attribution method to the modification strategy, the better the evaluation results. Specifically, by modifying the modification strategies in APOC and AUC, we can assign any of the three candidate attribution methods as the best method. We conduct experiments on on widely used SST-2 task of the GLUE benchmark (Wang et al., 2018)), and use BERTbase as encoder to build the target model1\n(achieve 86.4% accuracy).\nAttribution Methods We experiment with three attribution methods: leave-one-out (LOO) (Li et al., 2016), HEDGE (Chen et al., 2020) and Marg (Kim et al., 2020). The schemes of these attribution methods are shown in Figure 4, LOO assign attribution scores to the target word ‘good’ by deleting it from the sentence and observing change in the model predictions; Marg marginalizes the target word ‘good’ out considering the likelihoods of all candidate words, which uses BERT to measure the likelihoods of candidate words to replace the target word; HEDGE builds hierarchical explanations by recursively detecting the weakest interactions and then dividing large text spans into smaller ones. HEDGE assign attribution scores to spans by using ’[PAD]’ token to replace other words in a sentence and measuring how far the prediction is to the prediction boundary.\nEvaluation metrics and Results We first evaluate three attribution methods with metrics drawn from Marg and HEDGE papers. Marg uses AUC as evaluation metrics and modifies words by gradually replacing them with a token sampled from the distribution inferred by BERT, denoted as AUCrep; HEDGE uses AOPC as evaluation metrics and modifies words by deleting them directly, denoted as AOPCdel. Both papers modify 20% of words in the sentence. The results are shown in Table 3. As shown in Table 3, Marg performs best in AUCrep while LOO performs best in AOPCdel. Since the modification strategy of AOPCdel is consistent with\nLOO, and that of AUCrep is most similar to Marg, the evaluation results are consistent with the inference in logic trap 2: the more similar the target evaluated method to the modification strategy, the better the evaluation results.\nManipulate Evaluation Results We further conduct ablation experiments by changing the modification strategies in AOPCdel and AUCrep. Concretely, we switched perturbing strategy in AOPCdel and AUCrep and get new evaluation metrics: AOPCrep and AUCdel. As shown in Table 3, different from the initial results, Marg performs best in APOC metric while LOO performs best in AUC metric. The opposite results demonstrate that evaluation results mainly depend on the modification strategies, and we can manipulate evaluation results by changing them. Moreover, we note that HEDGE performs worst in all four evaluation metrics. Thus, we further customize the modification strategy to HEDGE’s advantage: padding unimportant features according to the attribution scores, denoted as AOPCpad and AUCpad. Not surprisingly, results in Table 3 show that HEDGE perform best in customized metrics.\nSummarization Because of the existence of logic trap 2, we can manipulate the evaluation results in evaluation 2 by changing the modification strategies, assigning any of the three candidate attribution methods as the best method. In fact, because we cannot simply assign a modification strategy as faithful, we should not use evaluation 2 to quantitatively evaluate attribution scores and make comparisons. Since the wide use of evaluation 2, the neglect of logic trap 2 has negatively impacted the research field for a long time. First, it brings a risk of unfair comparisons: works can customize evaluation metrics to their advantage and thus achieve the best performance. Second, the wide use of evaluation 2 also brings pressure to new proposed works, forcing them to make comparisons to others in such evaluation."
    }, {
      "heading" : "2.3 Part III",
      "text" : "Evaluation 3: Disprove Attribution Methods by Examining the Consistency of Attribution Scores\nIn this evaluation, works evaluate attribution methods by examining the consistency of attribution scores for similar inputs. The philosophy of Evaluation 3 is that semantically similar inputs which share the same model predictions should have similar attribution scores if the attribution method is reliable. Evaluation 3 is often used to disprove the effectiveness of attribution methods by searching for counterexamples.\nFor example, ExplainFooler (Sinha et al., 2021) attacks Integrated Gradients and (Sundararajan et al., 2017) and LIME (Sundararajan et al., 2017), which are two popular attribution methods in NLP, by searching adversarial sentences with different attribution scores. As shown in Figure 5, these adversarial sentences are semantically similar to the original sentence and share the same model predictions. However, the attribution scores of these sentences are very different from that of the original sentence. Sinha et al. (2021) observes the rank order correlation drops by over 20% when less than 10% of words are changed on average and thus draws the conclusion that Integrated Gradients and LIME are fragile.\nA lot of works (Alvarez-Melis and Jaakkola, 2018; Kindermans et al., 2019; Ghorbani et al.,\n2019; Ding and Koehn, 2021; Sinha et al., 2021) use evaluation 3 to examine the validity of existing attribution methods. For example, Ghorbani et al. (2019) argues that interpretations of neural networks are fragile by showing that systematic perturbations can lead to different interpretations without changing the label; Alvarez-Melis and Jaakkola (2018) argues that a crucial property that attribution methods should satisfy is robustness to local perturbations of the input.\nLogic Trap 3: The change in attribution scores maybe because the model reasoning process is really changed rather than the attribution method is unreliable.\nWhen solving similar samples like those shown in Figure 5, humans tend to use similar reasoning methods. However, deep models are not as robust enough as humans and often rely on unreasonable correlations. Semantically similar texts often cause different reasoning processes in deep models. For example, it is well known that deep models are vulnerable to adversarial samples (Goodfellow et al., 2014; Papernot et al., 2016). By deliberately adding some subtle interference that people cannot detect to the input sample, the target model will give a different prediction with high confidence. The success in adversarial attacks on deep models demonstrates similar inputs for humans can share very different reasoning processes in deep models.\nThe main difference between attributionattacking methods and model-attacking is that attribution-attacking methods require the model to give the same prediction for adversarial samples. However, giving the same prediction is very weak to constraint model reasoning because deep models have compressed the complicated calculation process into limited classes in the prediction. For example, there is always half probability of giving the same prediction for a binary classification task even with totally random reasoning. Thus, it is no surprise that attribution-attacking methods can find adversarial samples which share the same prediction label to the original sample yet have different attribution scores.\nThe logic trap in evaluation 3 is that the change in attribution scores may be because the model reasoning process is really changed rather than the attribution method is unreliable. As shown in Figure 6. (b), an attribution method should generate different attribution scores for the original and ad-\nversarial samples if it faithfully reflects the model reasoning. However, it will be regarded as fragile or unreliable in evaluation 3. Unfortunately, existing works ignore this logic trap and propose various methods to attack attribution methods. Since the high susceptibility of deep models to adversarial samples, not surprisingly, all of these works get the same conclusion: existing attribution methods are fragile or unreliable.\nExperiment 3: In experiment 3, we demonstrate that deep models can assign the same label to semantically similar samples yet use different reasoning. We experiment on widely used SST-2 and MNLI tasks of the GLUE benchmark (Wang et al., 2018)). MNLI requires the model to predict whether the premise entails the hypothesis, contradicts it, or is neutral.\nModel Since the attribution methods are defaulted as unreliable in evaluation 3, we cannot use existing attribution methods to judge whether the model reasoning is different. To solve the problem, we use a two-stage model framework, where the model first extracts a subset of inputs and gives prediction based only on the subset information. This way, we can observe whether the model reasoning is changed from the chosen subset, i.e., different subsets means the model chooses to use different information to make the final decision.\nThe overview of our model is shown in Figure 7. To guarantee that only the subset information\nis included in the classifier, we discretely select the words and pass words instead of the hidden states of the extractor to the classifier. Since gradients do not flow through discrete samples, we resort to HardKuma (Bastings et al., 2019) to jointly train the model, which gives support to binary outcomes. HardKuma allows for setting the percentage of selected words and is proved more effective and stable than REINFORCE (Williams, 1992) in such scenarios. We set the selection ratio as 20% for SST-2 and 40% for MNLI because larger ratios will not cause further performance improvement. Finally, We get 85.6% accuracy on SST-2 and 66.2/65.5 % accuracy on MNLI-m/mm.\nAdversarial Attack Method We use TextFooler (Jin et al., 2020) to generate adversarial samples. We use the same settings to Jin et al. (2020) to guarantee the semantical similarity of adversarial samples. The only difference is that we search for samples with minimal similarity in the selected subset instead of the model prediction. We guarantee that the model makes the same predictions, which is often used as the constraint for model reasoning in evaluation 3. We generate adversarial samples with 10% and 20% perturbation ratios.\nResults We use F1-score to compute the similarity score between subsets and report the Macroaveraging F1-score of the whole development set. A lower score is better, reflecting a larger difference in selected subsets. Note that since some words in original samples are replaced with their synonyms in adversarial samples, synonyms are seen as identical to their original words when evaluating. We evaluate all samples in the SST-2 development set\nand the first 1000 samples in MNLI-m/mm development sets. The results are shown in Table 4\nAs shown in Table 4, though semantically similar to the original samples and share the same model predictions, the adversarial samples can have subsets with low similarity to the original subset. Moreover, with a 10% perturbation ratio, 31.8% of samples in SST-2 have an adversarial subset with none word overlap with the original subset. This result increases to 50.5% with a 20% perturbation ratio. With no overlap between the two subsets, there is no way we can hypothesis the adversarial samples share similar model reasoning to the original samples.\nSummarization Though evaluation 3 seems reasonable, sharing similar semantics and the same model predictions is a weak constraint for similar model reasoning. Thus the change in attribution scores may come from different model reasoning instead of the instability of attribution methods. Because of deep models’ high sensitivity to adversarial samples, works resorting to evaluation 3 all get the same conclusion that existing attribution methods are fragile or unreliable. We argue we should find a more strict constraint for model reasoning first instead of ignoring logic trap 3 and disproving attribution methods recklessly."
    }, {
      "heading" : "3 Discussion",
      "text" : ""
    }, {
      "heading" : "3.1 Attacking attribution methods by replacing the target model.",
      "text" : "Besides resorting to methods in evaluation 3, there are works (Jain and Wallace, 2019; Wang et al., 2020; Slack et al., 2020) disprove the reliability of attribution methods by replacing the target model which attribution methods should work on.\nFor example, Slack et al. (2020) trains an adversarial classifier e(x) to distinguish whether the inputs have been perturbed or not and then uses a different sub-model to process perturbed instances. Specifically, if we want to attack the LOO method, we can build a loo set from the original dataset and\ntrain e(x) in the following form:\ne(x) = { f(x), if x ∈ original set ψ(x), if x ∈ loo set\nThis way, ψ(x), a model irrelevant to model predictions, is used when using LOO to calculate attribution scores, making generated attribution scores meaningless. Slack et al. (2020) assert that results of perturbation-based attribution methods such as LIME and SHAP (Lundberg and Lee, 2017) are easily attacked by their method. Similarly, Wang et al. (2020) add an extra model to the original model, which has uniform outputs but large gradients for some particular tokens such as ‘CLS’ in BERT. Since the extra model generates uniform outputs, it will not affect predictions of the original model. However, the extra model’s gradients will add to the original model and thus can confuse gradient-based attribution methods.\n3.2 Should We Use Attribution Methods in a Black-Box Way?\nThe attack methods in Section 3.1 fool the attribution methods through designing a special structure and require attribution methods to be used in a black-box way. In this setting, the attribution methods are easily attacked and generate meaningless results. However, the question is: as a tool to help humans understand how deep models work, is it necessary to use attribution methods in a black-box way? Take the linear model as an example. The linear model is regarded as a white-box model, and humans don’t need attribution methods to understand how it works. However, the understanding of a linear model is based on the analysis of its calculation process. Meanwhile, the deep model is regarded as a black-box model because its calculation process is too complicated to understand for humans, not because its calculation process is inaccessible. Thus, we believe there are no compelling reasons to require attribution methods to be used in a black-box way. The attacks in Wang et al. (2020); Slack et al. (2020) will fail when humans use attribution methods with knowing the model structures."
    }, {
      "heading" : "3.3 Reducing the impact of proposed logic traps.",
      "text" : "Since logic traps in existing evaluation methods can cause an inaccurate evaluation, we believe reducing the impact of these traps is the next question in the\nresearch field of post-hoc interpretations. In this section, we provide some thoughts for reducing the impact of logic trap 3:\nThe change in attribution scores may be because the model reasoning process is changed rather than the attribution method is unreliable.\nTo reduce the impact of this logic trap, we should try to guarantee the similarity in model reasoning when processing semantically similar inputs. In other words, we hope the target model used to test attribution methods more robustness to adversarial samples, which can be conducted through the following ways:\n1 Enhancing the target model. The success of adversarial attacks on deep models motivates efforts to defend against such attacks. Thus, we can use these defense techniques, such as adversarial training (Tramèr et al., 2017) and randomization (Xie et al., 2017), to enhance the target model and make it more robustness to adversarial samples.\n2 Excluding predictions with low confidence. The deep model will give a prediction for a sample regardless of whether knowing how to deal with it. The randomness of reasoning increases with the uncertainty in model decisions (Bella et al., 2010). Thus, we can guarantee the stability of model reasoning by excluding low-confident predictions. For example, we can resorting to Confidence Calibration techniques (Guo et al., 2017; Seo et al., 2019), which calculate confidence interval for a predicted response."
    }, {
      "heading" : "3.4 Conclusions",
      "text" : "The proposed logic traps in existing evaluation methods have been ignored for a long time and negatively affected the research field. Though strictly accurate evaluation metrics for evaluating attribution methods might be a “unicorn” which will likely never be found, we should not just ignore these logic traps and draw conclusions recklessly. With a clear statement and awareness of these logic traps, we should reduce the focus on improving performance under such unreliable evaluation systems and shift it to reducing the impact of proposed logic traps. Moreover, other aspects of the research field should give rise to more attention, such as the applications of attribution scores (denoising data, improving the model performance, etc.) and proposing new explanation forms."
    }, {
      "heading" : "A Experimental Details",
      "text" : "In this section, we provide the experimental details of our experiments. Moreover, we will release our code and model within two months.\nA.1 Experiment 1\nWe merged dev-high and dev-middle sets as the development set. As shown in Figure 8, the document D, question Q, and one of the choices C are concatenated together as the input of model, and we replace the development set questions with empty strings in our experiment.\nA.2 Experiment 2 We use the tokenizer of BERT to split the sentence into words in experiment 2. We modify 20% of words in the sentences in experiment 2. Since the word number in a sentence is not necessarily a multiple of five, we need to choose between rounding up or down. We use the same setting in code of HEDGE, i.e., rounding down. Specifically, we modify one word when word number is smaller than five.\nA.3 Experiment 3 Since HardKuma allows for setting the percentage of selected words, we first experiment with settings ranging from 10% to 100%. The results are shown in Figure 9. Under the premise of maintaining model performance, we choose the smallest setting (20% setting for SST-2 and 40% setting for MNLI). We use beam search to find adversarial samples and set the maximum reserved sample number to 100."
    } ],
    "references" : [ {
      "title" : "On the robustness of interpretability methods",
      "author" : [ "David Alvarez-Melis", "Tommi S Jaakkola." ],
      "venue" : "arXiv preprint arXiv:1806.08049.",
      "citeRegEx" : "Alvarez.Melis and Jaakkola.,? 2018",
      "shortCiteRegEx" : "Alvarez.Melis and Jaakkola.",
      "year" : 2018
    }, {
      "title" : "Interpretable neural predictions with differentiable binary variables",
      "author" : [ "Jasmijn Bastings", "Wilker Aziz", "Ivan Titov." ],
      "venue" : "arXiv preprint arXiv:1905.08160.",
      "citeRegEx" : "Bastings et al\\.,? 2019",
      "shortCiteRegEx" : "Bastings et al\\.",
      "year" : 2019
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "Calibration of machine learning models",
      "author" : [ "Antonio Bella", "Cèsar Ferri", "José Hernández-Orallo", "María José Ramírez-Quintana." ],
      "venue" : "Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques, pages 128–146.",
      "citeRegEx" : "Bella et al\\.,? 2010",
      "shortCiteRegEx" : "Bella et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning variational word masks to improve the interpretability of neural text classifiers",
      "author" : [ "Hanjie Chen", "Yangfeng Ji." ],
      "venue" : "arXiv preprint arXiv:2010.00667.",
      "citeRegEx" : "Chen and Ji.,? 2020",
      "shortCiteRegEx" : "Chen and Ji.",
      "year" : 2020
    }, {
      "title" : "Generating hierarchical explanations on text classification via feature interaction detection",
      "author" : [ "Hanjie Chen", "Guangtao Zheng", "Yangfeng Ji." ],
      "venue" : "arXiv preprint arXiv:2004.02015.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "L-shapley and c-shapley: Efficient model interpretation for structured data",
      "author" : [ "Jianbo Chen", "Le Song", "Martin J Wainwright", "Michael I Jordan." ],
      "venue" : "arXiv preprint arXiv:1808.02610.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Eraser: A benchmark to evaluate rationalized nlp models",
      "author" : [ "Jay DeYoung", "Sarthak Jain", "Nazneen Fatema Rajani", "Eric Lehman", "Caiming Xiong", "Richard Socher", "Byron C Wallace." ],
      "venue" : "arXiv preprint arXiv:1911.03429.",
      "citeRegEx" : "DeYoung et al\\.,? 2019",
      "shortCiteRegEx" : "DeYoung et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating saliency methods for neural language models",
      "author" : [ "Shuoyang Ding", "Philipp Koehn." ],
      "venue" : "arXiv preprint arXiv:2104.05824.",
      "citeRegEx" : "Ding and Koehn.,? 2021",
      "shortCiteRegEx" : "Ding and Koehn.",
      "year" : 2021
    }, {
      "title" : "Towards a rigorous science of interpretable machine learning",
      "author" : [ "Finale Doshi-Velez", "Been Kim." ],
      "venue" : "arXiv preprint arXiv:1702.08608.",
      "citeRegEx" : "Doshi.Velez and Kim.,? 2017",
      "shortCiteRegEx" : "Doshi.Velez and Kim.",
      "year" : 2017
    }, {
      "title" : "Pathologies of neural models make interpretations difficult",
      "author" : [ "Shi Feng", "Eric Wallace", "Alvin Grissom II", "Mohit Iyyer", "Pedro Rodriguez", "Jordan Boyd-Graber." ],
      "venue" : "arXiv preprint arXiv:1804.07781.",
      "citeRegEx" : "Feng et al\\.,? 2018",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "Interpretation of neural networks is fragile",
      "author" : [ "Amirata Ghorbani", "Abubakar Abid", "James Zou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3681–3688.",
      "citeRegEx" : "Ghorbani et al\\.,? 2019",
      "shortCiteRegEx" : "Ghorbani et al\\.",
      "year" : 2019
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q Weinberger." ],
      "venue" : "International Conference on Machine Learning, pages 1321–1330. PMLR.",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Self-attention attribution: Interpreting information interactions inside transformer",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu." ],
      "venue" : "arXiv preprint arXiv:2004.11207.",
      "citeRegEx" : "Hao et al\\.,? 2020",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness? arXiv preprint arXiv:2004.03685",
      "author" : [ "Alon Jacovi", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Jacovi and Goldberg.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jacovi and Goldberg.",
      "year" : 2020
    }, {
      "title" : "Attention is not explanation",
      "author" : [ "Sarthak Jain", "Byron C Wallace." ],
      "venue" : "arXiv preprint arXiv:1902.10186.",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "Alignment rationale for natural language inference",
      "author" : [ "Zhongtao Jiang", "Yuanzhe Zhang", "Zhao Yang", "Jun Zhao", "Kang Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
      "citeRegEx" : "Jiang et al\\.,? 2021",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Interpretation of nlp models through input marginalization",
      "author" : [ "Siwon Kim", "Jihun Yi", "Eunji Kim", "Sungroh Yoon." ],
      "venue" : "arXiv preprint arXiv:2010.13984.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "The (un) reliability of saliency methods",
      "author" : [ "Pieter-Jan Kindermans", "Sara Hooker", "Julius Adebayo", "Maximilian Alber", "Kristof T Schütt", "Sven Dähne", "Dumitru Erhan", "Been Kim." ],
      "venue" : "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning,",
      "citeRegEx" : "Kindermans et al\\.,? 2019",
      "shortCiteRegEx" : "Kindermans et al\\.",
      "year" : 2019
    }, {
      "title" : "Race: Large-scale reading comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1704.04683.",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Rationalizing neural predictions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "arXiv preprint arXiv:1606.04155.",
      "citeRegEx" : "Lei et al\\.,? 2016",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding neural networks through representation erasure",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1612.08220.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A unified approach to interpreting model predictions",
      "author" : [ "Scott M Lundberg", "Su-In Lee." ],
      "venue" : "Advances in neural information processing systems, pages 4765–4774.",
      "citeRegEx" : "Lundberg and Lee.,? 2017",
      "shortCiteRegEx" : "Lundberg and Lee.",
      "year" : 2017
    }, {
      "title" : "Interpretable Machine Learning",
      "author" : [ "Christoph Molnar." ],
      "venue" : "Lulu. com.",
      "citeRegEx" : "Molnar.,? 2020",
      "shortCiteRegEx" : "Molnar.",
      "year" : 2020
    }, {
      "title" : "Beyond word importance: Contextual decomposition to extract interactions from lstms",
      "author" : [ "W James Murdoch", "Peter J Liu", "Bin Yu." ],
      "venue" : "arXiv preprint arXiv:1801.05453.",
      "citeRegEx" : "Murdoch et al\\.,? 2018",
      "shortCiteRegEx" : "Murdoch et al\\.",
      "year" : 2018
    }, {
      "title" : "Comparing automatic and human evaluation of local explanations for text classification",
      "author" : [ "Dong Nguyen." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Nguyen.,? 2018",
      "shortCiteRegEx" : "Nguyen.",
      "year" : 2018
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami." ],
      "venue" : "2016 IEEE European symposium on security and privacy (EuroS&P), pages 372–387. IEEE.",
      "citeRegEx" : "Papernot et al\\.,? 2016",
      "shortCiteRegEx" : "Papernot et al\\.",
      "year" : 2016
    }, {
      "title" : "Rise: Randomized input sampling for explanation of blackbox models",
      "author" : [ "Vitali Petsiuk", "Abir Das", "Kate Saenko." ],
      "venue" : "arXiv preprint arXiv:1806.07421.",
      "citeRegEx" : "Petsiuk et al\\.,? 2018",
      "shortCiteRegEx" : "Petsiuk et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to deceive with attention-based explanations",
      "author" : [ "Danish Pruthi", "Mansi Gupta", "Bhuwan Dhingra", "Graham Neubig", "Zachary C Lipton." ],
      "venue" : "arXiv preprint arXiv:1909.07913.",
      "citeRegEx" : "Pruthi et al\\.,? 2019",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating the visualization of what a deep neural network has learned",
      "author" : [ "Wojciech Samek", "Alexander Binder", "Grégoire Montavon", "Sebastian Lapuschkin", "Klaus-Robert Müller." ],
      "venue" : "IEEE transactions on neural networks and learning systems,",
      "citeRegEx" : "Samek et al\\.,? 2016",
      "shortCiteRegEx" : "Samek et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning for single-shot confidence calibration in deep neural networks through stochastic inferences",
      "author" : [ "Seonguk Seo", "Paul Hongsuck Seo", "Bohyung Han." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
      "citeRegEx" : "Seo et al\\.,? 2019",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning important features through propagating activation differences",
      "author" : [ "Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje." ],
      "venue" : "arXiv preprint arXiv:1704.02685.",
      "citeRegEx" : "Shrikumar et al\\.,? 2017",
      "shortCiteRegEx" : "Shrikumar et al\\.",
      "year" : 2017
    }, {
      "title" : "Perturbing inputs for fragile interpretations in deep natural language processing",
      "author" : [ "Sanchit Sinha", "Hanjie Chen", "Arshdeep Sekhon", "Yangfeng Ji", "Yanjun Qi." ],
      "venue" : "arXiv preprint arXiv:2108.04990.",
      "citeRegEx" : "Sinha et al\\.,? 2021",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "Fooling lime and shap: Adversarial attacks on post hoc explanation methods",
      "author" : [ "Dylan Slack", "Sophie Hilgard", "Emily Jia", "Sameer Singh", "Himabindu Lakkaraju." ],
      "venue" : "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 180–186.",
      "citeRegEx" : "Slack et al\\.,? 2020",
      "shortCiteRegEx" : "Slack et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "arXiv preprint arXiv:1703.01365.",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "Ensemble adversarial training: Attacks and defenses",
      "author" : [ "Florian Tramèr", "Alexey Kurakin", "Nicolas Papernot", "Ian Goodfellow", "Dan Boneh", "Patrick McDaniel." ],
      "venue" : "arXiv preprint arXiv:1705.07204.",
      "citeRegEx" : "Tramèr et al\\.,? 2017",
      "shortCiteRegEx" : "Tramèr et al\\.",
      "year" : 2017
    }, {
      "title" : "How does this interaction affect me? interpretable attribution for feature interactions",
      "author" : [ "Michael Tsang", "Sirisha Rambhatla", "Yan Liu." ],
      "venue" : "arXiv preprint arXiv:2006.10965.",
      "citeRegEx" : "Tsang et al\\.,? 2020",
      "shortCiteRegEx" : "Tsang et al\\.",
      "year" : 2020
    }, {
      "title" : "Allennlp interpret: A framework for explaining predictions of nlp models",
      "author" : [ "Eric Wallace", "Jens Tuyls", "Junlin Wang", "Sanjay Subramanian", "Matt Gardner", "Sameer Singh." ],
      "venue" : "arXiv preprint arXiv:1909.09251.",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Gradient-based analysis of nlp models is manipulable",
      "author" : [ "Junlin Wang", "Jens Tuyls", "Eric Wallace", "Sameer Singh." ],
      "venue" : "arXiv preprint arXiv:2010.05419.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams." ],
      "venue" : "Machine learning, 8(3):229–256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Mitigating adversarial effects through randomization",
      "author" : [ "Cihang Xie", "Jianyu Wang", "Zhishuai Zhang", "Zhou Ren", "Alan Yuille." ],
      "venue" : "arXiv preprint arXiv:1711.01991.",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "The opaqueness of deep models has grown in tandem with their power (Doshi-Velez and Kim, 2017), which has motivated efforts to interpret how these black-box models work (Sundararajan et al.",
      "startOffset" : 67,
      "endOffset" : 94
    }, {
      "referenceID" : 38,
      "context" : "The opaqueness of deep models has grown in tandem with their power (Doshi-Velez and Kim, 2017), which has motivated efforts to interpret how these black-box models work (Sundararajan et al., 2017; Belinkov and Glass, 2019).",
      "startOffset" : 169,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : "The opaqueness of deep models has grown in tandem with their power (Doshi-Velez and Kim, 2017), which has motivated efforts to interpret how these black-box models work (Sundararajan et al., 2017; Belinkov and Glass, 2019).",
      "startOffset" : 169,
      "endOffset" : 222
    }, {
      "referenceID" : 16,
      "context" : "Post-hoc explanation aims to explain a trained model and reveal how the model arrives at a decision (Jacovi and Goldberg, 2020; Molnar, 2020).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 26,
      "context" : "Post-hoc explanation aims to explain a trained model and reveal how the model arrives at a decision (Jacovi and Goldberg, 2020; Molnar, 2020).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "For example, Erasure-based method calculate attribution scores by measuring the change of output after removing target features (Li et al., 2016; Feng et al., 2018; Chen et al., 2020); Gradient-based method uses gradients to study the influence of features on model predictions (Sundararajan et al.",
      "startOffset" : 128,
      "endOffset" : 183
    }, {
      "referenceID" : 11,
      "context" : "For example, Erasure-based method calculate attribution scores by measuring the change of output after removing target features (Li et al., 2016; Feng et al., 2018; Chen et al., 2020); Gradient-based method uses gradients to study the influence of features on model predictions (Sundararajan et al.",
      "startOffset" : 128,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "For example, Erasure-based method calculate attribution scores by measuring the change of output after removing target features (Li et al., 2016; Feng et al., 2018; Chen et al., 2020); Gradient-based method uses gradients to study the influence of features on model predictions (Sundararajan et al.",
      "startOffset" : 128,
      "endOffset" : 183
    }, {
      "referenceID" : 38,
      "context" : ", 2020); Gradient-based method uses gradients to study the influence of features on model predictions (Sundararajan et al., 2017; Wallace et al., 2019; Hao et al., 2020); Meanwhile, these methods also received much scrutiny, arguing that the generated attribution scores are fragile or unreliable (AlvarezMelis and Jaakkola, 2018; Pruthi et al.",
      "startOffset" : 102,
      "endOffset" : 169
    }, {
      "referenceID" : 41,
      "context" : ", 2020); Gradient-based method uses gradients to study the influence of features on model predictions (Sundararajan et al., 2017; Wallace et al., 2019; Hao et al., 2020); Meanwhile, these methods also received much scrutiny, arguing that the generated attribution scores are fragile or unreliable (AlvarezMelis and Jaakkola, 2018; Pruthi et al.",
      "startOffset" : 102,
      "endOffset" : 169
    }, {
      "referenceID" : 15,
      "context" : ", 2020); Gradient-based method uses gradients to study the influence of features on model predictions (Sundararajan et al., 2017; Wallace et al., 2019; Hao et al., 2020); Meanwhile, these methods also received much scrutiny, arguing that the generated attribution scores are fragile or unreliable (AlvarezMelis and Jaakkola, 2018; Pruthi et al.",
      "startOffset" : 102,
      "endOffset" : 169
    }, {
      "referenceID" : 31,
      "context" : ", 2020); Meanwhile, these methods also received much scrutiny, arguing that the generated attribution scores are fragile or unreliable (AlvarezMelis and Jaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 228
    }, {
      "referenceID" : 43,
      "context" : ", 2020); Meanwhile, these methods also received much scrutiny, arguing that the generated attribution scores are fragile or unreliable (AlvarezMelis and Jaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 228
    }, {
      "referenceID" : 36,
      "context" : ", 2020); Meanwhile, these methods also received much scrutiny, arguing that the generated attribution scores are fragile or unreliable (AlvarezMelis and Jaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 228
    }, {
      "referenceID" : 32,
      "context" : "For example, meaningful perturbation is used for making comparison in many works (Samek et al., 2016; Chen et al., 2018; DeYoung et al., 2019; Chen et al., 2020; Kim et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "For example, meaningful perturbation is used for making comparison in many works (Samek et al., 2016; Chen et al., 2018; DeYoung et al., 2019; Chen et al., 2020; Kim et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "For example, meaningful perturbation is used for making comparison in many works (Samek et al., 2016; Chen et al., 2018; DeYoung et al., 2019; Chen et al., 2020; Kim et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "For example, meaningful perturbation is used for making comparison in many works (Samek et al., 2016; Chen et al., 2018; DeYoung et al., 2019; Chen et al., 2020; Kim et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "For example, meaningful perturbation is used for making comparison in many works (Samek et al., 2016; Chen et al., 2018; DeYoung et al., 2019; Chen et al., 2020; Kim et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 179
    }, {
      "referenceID" : 37,
      "context" : "For example, the SST-2 (Socher et al., 2013) corpus provides not only sentence-level labels, but also five-class word-level sentiment tags ranging from very negative to very positive.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "Thus, many works (Lei et al., 2016; Li et al., 2016; Tsang et al., 2020; Kim et al., 2020) perform quantitative evaluation of attribution scores by comparing them with the word-level tags in SST-2.",
      "startOffset" : 17,
      "endOffset" : 90
    }, {
      "referenceID" : 24,
      "context" : "Thus, many works (Lei et al., 2016; Li et al., 2016; Tsang et al., 2020; Kim et al., 2020) perform quantitative evaluation of attribution scores by comparing them with the word-level tags in SST-2.",
      "startOffset" : 17,
      "endOffset" : 90
    }, {
      "referenceID" : 40,
      "context" : "Thus, many works (Lei et al., 2016; Li et al., 2016; Tsang et al., 2020; Kim et al., 2020) perform quantitative evaluation of attribution scores by comparing them with the word-level tags in SST-2.",
      "startOffset" : 17,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Thus, many works (Lei et al., 2016; Li et al., 2016; Tsang et al., 2020; Kim et al., 2020) perform quantitative evaluation of attribution scores by comparing them with the word-level tags in SST-2.",
      "startOffset" : 17,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "We experiment on RACE (Lai et al., 2017), a large-scale question-answering dataset.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "We first train a model with BERTbase (Devlin et al., 2018) as encoder1 with questions, and achieve 65.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 32,
      "context" : "(2016); Nguyen (2018); Chen and Ji (2020) use the area over the perturbation curve (AOPC) (Samek et al., 2016) as evaluation metrics.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 34,
      "context" : "Besides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons.",
      "startOffset" : 36,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "Besides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons.",
      "startOffset" : 36,
      "endOffset" : 172
    }, {
      "referenceID" : 28,
      "context" : "Besides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons.",
      "startOffset" : 36,
      "endOffset" : 172
    }, {
      "referenceID" : 8,
      "context" : "Besides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons.",
      "startOffset" : 36,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "Besides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons.",
      "startOffset" : 36,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "Besides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons.",
      "startOffset" : 36,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "Besides these works, a lot of works (Shrikumar et al., 2017; Chen et al., 2018; Nguyen, 2018; DeYoung et al., 2019; Chen et al., 2020; Hao et al., 2020; Jiang et al., 2021) use similar metrics to perform evaluation and comparisons.",
      "startOffset" : 36,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "Specifically, when using deleting tokens as modification and narrowing the k% to one token, AOPC(k) degenerates into the basic attribution method: leave-one-out (Li et al., 2016).",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 42,
      "context" : "We conduct experiments on on widely used SST-2 task of the GLUE benchmark (Wang et al., 2018)), and use BERTbase as encoder to build the target model1 (achieve 86.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "Attribution Methods We experiment with three attribution methods: leave-one-out (LOO) (Li et al., 2016), HEDGE (Chen et al.",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 35,
      "context" : "For example, ExplainFooler (Sinha et al., 2021) attacks Integrated Gradients and (Sundararajan et al.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 38,
      "context" : ", 2021) attacks Integrated Gradients and (Sundararajan et al., 2017) and LIME (Sundararajan et al.",
      "startOffset" : 41,
      "endOffset" : 68
    }, {
      "referenceID" : 38,
      "context" : ", 2017) and LIME (Sundararajan et al., 2017), which are two popular attribution methods in NLP, by searching adversarial sentences with different attribution scores.",
      "startOffset" : 17,
      "endOffset" : 44
    }, {
      "referenceID" : 35,
      "context" : "Figure 5: Examples taken from ExplainFooler (Sinha et al., 2021), which attacks attribution methods by searching adversarial sentences with different attribution scores.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "For example, it is well known that deep models are vulnerable to adversarial samples (Goodfellow et al., 2014; Papernot et al., 2016).",
      "startOffset" : 85,
      "endOffset" : 133
    }, {
      "referenceID" : 29,
      "context" : "For example, it is well known that deep models are vulnerable to adversarial samples (Goodfellow et al., 2014; Papernot et al., 2016).",
      "startOffset" : 85,
      "endOffset" : 133
    }, {
      "referenceID" : 42,
      "context" : "We experiment on widely used SST-2 and MNLI tasks of the GLUE benchmark (Wang et al., 2018)).",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "Since gradients do not flow through discrete samples, we resort to HardKuma (Bastings et al., 2019) to jointly train the model, which gives support to binary outcomes.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 44,
      "context" : "and stable than REINFORCE (Williams, 1992) in such scenarios.",
      "startOffset" : 26,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : "Adversarial Attack Method We use TextFooler (Jin et al., 2020) to generate adversarial samples.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "Besides resorting to methods in evaluation 3, there are works (Jain and Wallace, 2019; Wang et al., 2020; Slack et al., 2020) disprove the reliability of attribution methods by replacing the target model which attribution methods should work on.",
      "startOffset" : 62,
      "endOffset" : 125
    }, {
      "referenceID" : 43,
      "context" : "Besides resorting to methods in evaluation 3, there are works (Jain and Wallace, 2019; Wang et al., 2020; Slack et al., 2020) disprove the reliability of attribution methods by replacing the target model which attribution methods should work on.",
      "startOffset" : 62,
      "endOffset" : 125
    }, {
      "referenceID" : 36,
      "context" : "Besides resorting to methods in evaluation 3, there are works (Jain and Wallace, 2019; Wang et al., 2020; Slack et al., 2020) disprove the reliability of attribution methods by replacing the target model which attribution methods should work on.",
      "startOffset" : 62,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "(2020) assert that results of perturbation-based attribution methods such as LIME and SHAP (Lundberg and Lee, 2017) are easily attacked by their method.",
      "startOffset" : 91,
      "endOffset" : 115
    }, {
      "referenceID" : 39,
      "context" : "Thus, we can use these defense techniques, such as adversarial training (Tramèr et al., 2017) and randomization (Xie et al.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 45,
      "context" : ", 2017) and randomization (Xie et al., 2017), to enhance the target model and make it more robustness to adversarial samples.",
      "startOffset" : 26,
      "endOffset" : 44
    } ],
    "year" : 0,
    "abstractText" : "Modern deep learning models are notoriously opaque, which has motivated the development of methods for interpreting how deep models predict. This goal is usually approached with attribution method, which assesses the influence of features on model predictions. As an explanation method, the evaluation criteria of attribution methods is how accurately it reflects the actual reasoning process of the model (faithfulness). Meanwhile, since the reasoning process of deep models is inaccessible, researchers design various evaluation methods to demonstrate their arguments. However, some crucial logic traps in these evaluation methods are ignored in most works, causing inaccurate evaluation and unfair comparison. This paper systematically reviews existing methods for evaluating attribution scores and summarizes the logic traps in these methods. We further conduct experiments to demonstrate the existence of each logic trap. Through both theoretical and experimental analysis, we hope to increase attention on the inaccurate evaluation of attribution scores. Moreover, with this paper, we suggest stopping focusing on improving performance under unreliable evaluation systems and starting efforts on reducing the impact of proposed logic traps.",
    "creator" : null
  }
}
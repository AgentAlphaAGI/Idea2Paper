{
  "name" : "ARR_2022_43_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incremental Intent Detection for Medical Domain with Contrast Replay Networks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Medical intent detection aims to identify intents of medical queries and classify them into specific categories (Chen et al., 2012; Howard and Cambria, 2013; Guo et al., 2014; Cai et al., 2017). In medical scenarios, understanding query intent is very important for medical question answer systems (Wu et al., 2020; Mrini et al., 2021). Typically, to perform medical intent detection, existing methods pre-define a class set of fixed medical intent categories in advance, and train the whole collected dataset with the fixed class set. However, novel medical intents incessantly emerge with new data in the real world. When given a query with new intent category that is out of the pre-defined class set, these models can do nothing about it.\nA straightforward solution is to store and re-train the whole data every time new data and intents come in. However, it is almost infeasible with limited storage budget and computation cost in practice\n(Wang et al., 2019). Consider the above problems, we propose to address this issue in a incremental learning way (Ring et al., 1994; Thrun, 1998), where the number of intent categories is allowed to incrementally increase and the system can incessantly learn emerged novel intents from continually arriving data of new intents, which is illustrated as Figure 1.\nNaturally, one simple incremental method is to directly finetune the model by new intents data. However, this method suffers from the serious catastrophic forgetting problem (McCloskey and Cohen, 1989; French, 1999; Wang et al., 2019). After fitting emerged data of new intent, the performance of the model on old classes will inevitably drop a lot. There are some studies that have made effects to overcome the catastrophic forgetting problem. Typically, they are divided into parameter-based methods that preserve parameters important to the previous classes when updating (Kirkpatrick et al., 2017; Aljundi et al., 2018), and memory-based methods that store a few examples for each old class and replay them with arriving data of new classes (Rebuffi et al., 2017; Hou et al., 2018, 2019). Due to the simplicity and effectiveness, memory-based methods dominated this field.\nHowever, when applying memory-based methods to incremental intent detection for medical domain, these methods face two new challenges – training data imbalance and medical rare words. Training Data Imbalance: While the amount of new classes data is often large, only a few examples for old classes are stored in the limited memory.\nTherefore, the model can be drastically altered by the richer new classes data and ignore old classes (He and Garcia, 2009; Wu et al., 2019; Zhang et al., 2017). Medical Rare Words: Compared with common domains, the medical domain typically contains many domain-specific rare words1. These rare words are usually not well learned by the model and bring disturbance to representations of examples, which is adverse to selecting representative examples as memory for old classes replay.\nConsidering the above problems, we propose contrast reply networks to enhance incremental intent detection for medical domain. Specifically, to address training data imbalance, we devise multilevel distillation to make the current model mimic the behaviors of the original model. To address medical rare words, we devise contrast objective to push examples from the same intent close and examples from different intents further apart. Experimental results demonstrate that our method outperforms previous state-of-the-art models."
    }, {
      "heading" : "2 Problem Definition",
      "text" : "Medica intent detection (MID) is a classification task. In real medical applications, new intent classes incessantly emerge. Therefore, a practical MID system should be able to incrementally learn new query intent classes. We introduce a new problem, incremental intent detection. Suppose that there is a class-incremental data stream, denoted as {X 1,X 2, . . . ,X (M)}. Each X (k) contains training/validation/testing data (X (k)train,X (k) valid,X (k) test) and its own intent class set C(k). Note that any two intent class sets are disjoint2, i.e., C(i)∩C(j) = ∅(i 6= j). At the k-th step, the MID model optimizes its parameters using the training data X (k)train and the updated model should still perform well on historical classes, i.e., classes from 1 to k − 1. Thus, for testing at the k-th step, we evaluate the updated model on the testing data of all old classes, i.e., ∪ki=1X (i) test. Given an input from X (j)(j ≤ k), the model needs to give a prediction from ∪ki=1C(i), instead of C(j). To alleviate catastrophic forgetting, memory-based incremental learning methods allow limited memory to store a few examples for each old class. Therefore, every time new classes data\n1In our experiments, more than 60% queries contain at least one rare word (such as diseases, proteins and chemicals) in the context.\n2X (k) contains one or more new classes represented by C(k).\narrive, the MID model utilizes the stored old classes data and new classes data X (k)train as training data to re-train parameters."
    }, {
      "heading" : "3 Method",
      "text" : "In this paper, we propose Contrastive Reply Networks for incremental medical intent detection (CRN). CRN consists of three components: 1) Intent classifier with memory, 2) Multilevel distillation and 3) Contrast objective."
    }, {
      "heading" : "3.1 Intent Classifier with Memory",
      "text" : ""
    }, {
      "heading" : "3.1.1 Memory-based Framework",
      "text" : "We use BERT (Devlin et al., 2018) as the medical intent classifier. When the new medical intent classes C(k) arrives, the corresponding new training data is denoted as X (k)train = {(Xi, Yi), 1 ≤ i ≤ K}, where K is the number of training examples, Xi is the query and Yi denotes the intent label of the query Xi. The memory stores the representative examples for old m classes, i.e., m = | ∪k−1i=1 C(i)|, we denoted it as P = {P(1), · · · ,P(m)}, where P(i) is the set of stored examples for the i-th class. We combine the stored old data and new classes data, which is denoted asN = P ∪X (k)train, to train the current model. The current label set Co contains all observed intent categories, i.e., Co = ∪ki=1Ci. Then a softmax classifier is to predict intent categories with representations of “[CLS]” in BERT. Finally, we use the cross entropy to train the intent detection model, denoted as loss Lce. Note that the current label set size is the number of all observed classes, i.e., |Co|."
    }, {
      "heading" : "3.1.2 Memory Updating",
      "text" : "To overcome the catastrophic forgetting problem of old classes, memory-based methods store examples for each old class in the memory. All classes are treated equally in our model. Therefore, if m classes have been learned so far and B is the total number of examples stored in the memory, our model will store n = B/m examples for each old class. Inspired by prototype learning (Snell et al., 2017; Yang et al., 2018), we select the top n example closest to the centroid of examples (based on “[CLS]” embeddings) of each class and store them into the memory as representative ones for old classes. When new data comes in, these examples in the memory are trained with the new data. Before the next new class arrives, we remove B/m − B/(m + t) stored examples of each old\nclass in the memory and allocate space to store B/(m + t) current new classes examples based on the centroid, where t = |Ck| is the number of current new classes."
    }, {
      "heading" : "3.2 Multilevel Distillation",
      "text" : "Although storing a few examples for each old class as memory is useful to avoid catastrophic forgetting, there is a serious data imbalance problem between memory and new classes data, which makes the model have an obvious bias towards the new classes, resulting in severely forgetting classification ability of previous classes. To address it, we devise multilevel distillation to make the current model mimic the behaviors of the original model.\nInspired by (Hinton et al., 2015), we first perform it at prediction level. We encourage the current predictions on old classes to match the soft labels by the original model. Formally,\nLpl = − 1 |N | ∑ X∈N ∑ x∈X m∑ i=1 α∗i log(αi) (1)\nwhere α∗i and αi is the output probability for the i-th label of the original and current model, respectively. This loss function is performed for all arriving new classes data and stored old classes data in the memory.\nThen, we also perform it at feature level. We encourage the feature representations (“[CLS]” of BERT) of the current model don’t greatly deviate from the ones of the original model. Formally,\nLfl = − 1 |N | ∑ X∈N ∑ x∈X cos(f∗(x), f(x)) (2)\nwhere cos(f∗(x), f(x)) measures the cosine distance between feature vectors of the original and current model. This loss is computed for all samples from the new classes and stored examples in the memory."
    }, {
      "heading" : "3.3 Contrast Objective",
      "text" : "Medical rare words are usually not well learned by the model and bring disturbance to representations of examples, which is detrimental to memory selection for old classes. Inspired by (Chen et al., 2020b), we devise contrast objective to push examples from the same intent close and examples from different intents further apart. As a result, we can obtain better representations for examples and select more representative examples for reply.\nSpecifically, we first perform data augmentation to obtain more examples. We use a medical dictionary that contains medical rare words3 to randomly add, delete or replace rare words over the original examples. After that, we employ an objective to push examples from the same intent close and examples from different intents further apart: Lco = − ∑ i 1 Nyi − 1 ∑ j 6=i (1yi=yj−1yi 6=yj ) log sij (3) where sij =\nexp(f(xi)·f(xj))∑ k 6=i exp(f(xi)·f(xk))\n, f(x) denotes the embeddings of “[CLS]” token of example x. Finally, our model is optimized by the total loss L = Lce + Lpl + Lfl + Lco."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Benchmarks",
      "text" : "We use two public medical datasets KUAKE-QIC in CBLUE (Zhang et al., 2021) and CMID (Chen et al., 2020a) to construct benchmarks for incremental learning setting. For a medical intent detection dataset, its intent classes are arranged in a fixed order. Then, methods are trained in a classincremental way on the available training data4. Due to its long-tail frequency distribution, we use the data of the top 10/20 most frequent classes for KUAKE-QIC/CMID."
    }, {
      "heading" : "4.2 Evaluation and Implementation",
      "text" : "We use accuracy as the evaluation metric. Every time the model finishes training on the new classes data, we report the accuracy on the whole testing data of all observed classes. Besides, we also report the performance at the last step, containing Average accuracy (macro-averaging) and Whole accuracy (micro-averaging).\nWe use base BERT as the classifier. The learning rate is set to 2e-5. The batch size is 8. For the two benchmarks, both the capacity of memory is B = 200."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare CRN with 4 baselines: 1) EWC (Kirkpatrick et al., 2017): The representative parameterbased method that keeps the network parameters close to the optimal parameters for the previous\n3We build it by collecting medical entities in OpenKG at http://www.openkg.cn/dataset.\n4Only one new class is available for the model at each time, i.e., t = |C(k)| = 1.\nclasses when training new classes data. 2) EMR (Wang et al., 2019): The representative memorybased method that avoids catastrophic forgetting via randomly storing a few examples of old classes. 3) EMAR (Han et al., 2020): The latest memorybased method that utilizes prototypes for memory reconsolidation exercise to keep a stable understanding of old classes. 4) Finetune: The lower bound that simply finetunes the model on arriving data of new classes. 5) Upperbound: The upper bound that stores and trains all observed samples."
    }, {
      "heading" : "4.4 Compared with State-of-the-art Methods",
      "text" : "We conduct experiments on KUAKE-QIC and CMID. The accuracies over all observed classes during the whole incremental learning process are plotted in Figure 2. We also show the results at the last step in Table 1. We can find that our method outperforms all other baselines by a large margin. Specifically, compared with the state-of-theart model EMAR, our method achieves 5.7% and 9.1% improvements of whole accuracy score on the KUAKE-QIC and CMID, respectively. It demonstrates the effectiveness of our proposed CRN. Besides, Finetuning always obtains the worst perfor-\nmance on both benchmarks and becomes the lower bound, which indicates that the catastrophic forgetting problem is serious. Moreover, the large gap between all methods and Upperbound indicates that this issue is still challenging."
    }, {
      "heading" : "4.5 Ablation Experiment",
      "text" : "To investigate the effectiveness of the different parts in our method, we conduct ablation studies. The results are shown in Table 2. We can see: (1) Effectiveness of distillation at prediction level: The performance drops with removing Lpl (“w/o PL”). It demonstrates that it is useful to handle training data imbalance. (2) Effectiveness of distillation at feature level: The performance drops with removing Lfl (“w/o FL”). It demonstrates that it is effective to address training data imbalance. (3) Effectiveness of contrast objective: The performance drops with removing Lco (“w/o CO”). It demonstrates that it is helpful to handle medical rare words. We report extra experiments in Appendix."
    }, {
      "heading" : "5 Conlusion",
      "text" : "We explore to incrementally learning medical intent detection. We propose contrast reply networks to handle training data imbalance and medical rare words. Experiments demonstrate that our method outperforms previous state-of-the-art models."
    }, {
      "heading" : "A The Effect of the Number of Stored Examples",
      "text" : "To show the effect of different numbers of stored examples, we compare our CRN with another memory-based method EMAR on KUAKE-QIC , where the memory size to store examples is from 50 to 200. We can observe the results in Table 3.\nFirst, the more examples stored, the better performance for both memory-based methods. We also see that our method performs better, which demonstrates the effectiveness of our method. Even with fewer examples stored, our CRN still performs better, demonstrating that our method is effective to address training data imbalance.\nB Visualization\nTo show the effectiveness of introduced contrast objective for medical rare words, we also give the visualization in Figure 3 to show the feature spaces learned by our CRN (Ours) and EMAR (i.e., the latest representative work of memory-based method) on KUAKE-QIC.\nSpecifically, we extract the representations of “[CLS]” and use t-SNE to implement visualization. We can see that the feature space of CRN is more sparse and features from different intents are more distinguishable. However, the features learned by EMAR are more difficult to distinguish. Examples\nfrom the same intent in CRN are closer than ones in EMAR. It is because medical rare words are easy to bring disturbance and make the queries confused in EMAR. This result shows that our contrast objective in CRN can learn better representations for queries against medical rare words."
    } ],
    "references" : [ {
      "title" : "Memory aware synapses: Learning what (not) to forget",
      "author" : [ "Rahaf Aljundi", "Francesca Babiloni", "Mohamed Elhoseiny", "Marcus Rohrbach", "Tinne Tuytelaars." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 139–154.",
      "citeRegEx" : "Aljundi et al\\.,? 2018",
      "shortCiteRegEx" : "Aljundi et al\\.",
      "year" : 2018
    }, {
      "title" : "An cnn-lstm attention approach to understanding user query intent from online health communities",
      "author" : [ "Ruichu Cai", "Binjun Zhu", "Lei Ji", "Tianyong Hao", "Jun Yan", "Wenyin Liu." ],
      "venue" : "2017 ieee international conference on data mining workshops (icdmw),",
      "citeRegEx" : "Cai et al\\.,? 2017",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding user intent in community question answering",
      "author" : [ "Long Chen", "Dell Zhang", "Levene Mark." ],
      "venue" : "Proceedings of the 21st international conference on world wide web, pages 823–828.",
      "citeRegEx" : "Chen et al\\.,? 2012",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "A benchmark dataset and case study for chinese medical question intent classification",
      "author" : [ "Nan Chen", "Xiangdong Su", "Tongyang Liu", "Qizhi Hao", "Ming Wei." ],
      "venue" : "BMC Medical Informatics and Decision Making, 20(3):1–7.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "International conference on machine learning, pages 1597–1607. PMLR.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Catastrophic forgetting in connectionist networks",
      "author" : [ "Robert M French." ],
      "venue" : "Trends in cognitive sciences, 3(4):128–135.",
      "citeRegEx" : "French.,? 1999",
      "shortCiteRegEx" : "French.",
      "year" : 1999
    }, {
      "title" : "Joint semantic utterance classification and slot filling with recursive neural networks",
      "author" : [ "Daniel Guo", "Gokhan Tur", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "2014 IEEE Spoken Language Technology Workshop (SLT), pages 554–559. IEEE.",
      "citeRegEx" : "Guo et al\\.,? 2014",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Continual relation learning via episodic memory activation and reconsolidation",
      "author" : [ "Xu Han", "Yi Dai", "Tianyu Gao", "Yankai Lin", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning from imbalanced data",
      "author" : [ "Haibo He", "Edwardo A Garcia." ],
      "venue" : "IEEE Transactions on knowledge and data engineering, 21(9):1263–1284.",
      "citeRegEx" : "He and Garcia.,? 2009",
      "shortCiteRegEx" : "He and Garcia.",
      "year" : 2009
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Lifelong learning via progressive distillation and retrospection",
      "author" : [ "Saihui Hou", "Xinyu Pan", "Chen Change Loy", "Zilei Wang", "Dahua Lin." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 437–452.",
      "citeRegEx" : "Hou et al\\.,? 2018",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning a unified classifier incrementally via rebalancing",
      "author" : [ "Saihui Hou", "Xinyu Pan", "Chen Change Loy", "Zilei Wang", "Dahua Lin." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 831–839.",
      "citeRegEx" : "Hou et al\\.,? 2019",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2019
    }, {
      "title" : "Intention awareness: improving upon situation awareness in human-centric environments",
      "author" : [ "Newton Howard", "Erik Cambria." ],
      "venue" : "Human-centric Computing and Information Sciences, 3(1):1–17.",
      "citeRegEx" : "Howard and Cambria.,? 2013",
      "shortCiteRegEx" : "Howard and Cambria.",
      "year" : 2013
    }, {
      "title" : "Overcoming catastrophic forgetting in neural networks",
      "author" : [ "James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 2017
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "Michael McCloskey", "Neal J Cohen." ],
      "venue" : "Psychology of learning and motivation, volume 24, pages 109–165. Elsevier.",
      "citeRegEx" : "McCloskey and Cohen.,? 1989",
      "shortCiteRegEx" : "McCloskey and Cohen.",
      "year" : 1989
    }, {
      "title" : "A gradually soft multi-task and data-augmented approach to medical question understanding",
      "author" : [ "Khalil Mrini", "Franck Dernoncourt", "Seunghyun Yoon", "Trung Bui", "Walter Chang", "Emilia Farcas", "Ndapa Nakashole." ],
      "venue" : "Proceedings of the 59th Annual",
      "citeRegEx" : "Mrini et al\\.,? 2021",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2021
    }, {
      "title" : "icarl: Incremental classifier and representation learning",
      "author" : [ "Sylvestre-Alvise Rebuffi", "Alexander Kolesnikov", "Georg Sperl", "Christoph H Lampert." ],
      "venue" : "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010.",
      "citeRegEx" : "Rebuffi et al\\.,? 2017",
      "shortCiteRegEx" : "Rebuffi et al\\.",
      "year" : 2017
    }, {
      "title" : "Continual learning in reinforcement environments",
      "author" : [ "Mark Bishop Ring" ],
      "venue" : null,
      "citeRegEx" : "Ring,? \\Q1994\\E",
      "shortCiteRegEx" : "Ring",
      "year" : 1994
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard Zemel." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 4080–4090.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Lifelong learning algorithms",
      "author" : [ "Sebastian Thrun." ],
      "venue" : "Learning to learn, pages 181–209. Springer.",
      "citeRegEx" : "Thrun.,? 1998",
      "shortCiteRegEx" : "Thrun.",
      "year" : 1998
    }, {
      "title" : "Sentence embedding alignment for lifelong relation extraction",
      "author" : [ "Hong Wang", "Wenhan Xiong", "Mo Yu", "Xiaoxiao Guo", "Shiyu Chang", "William Yang Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "An attention-based multi-task model for named entity recognition",
      "author" : [ "Chaochen Wu", "Guan Luo", "Chao Guo", "Yin Ren", "Anni Zheng", "Cheng Yang" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Large scale incremental learning",
      "author" : [ "Yue Wu", "Yinpeng Chen", "Lijuan Wang", "Yuancheng Ye", "Zicheng Liu", "Yandong Guo", "Yun Fu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 374–382.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Robust classification with convolutional prototype learning",
      "author" : [ "Hong-Ming Yang", "Xu-Yao Zhang", "Fei Yin", "ChengLin Liu." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3474–3482.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Cblue: A chinese biomedical language understanding evaluation benchmark. arXiv preprint arXiv:2106.08087",
      "author" : [ "Ningyu Zhang", "Zhen Bi", "Xiaozhuan Liang", "Lei Li", "Xiang Chen", "Shumin Deng", "Luoqiu Li", "Xin Xie", "Hongbin Ye", "Xin Shang" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Range loss for deep face recognition with long-tailed training data",
      "author" : [ "Xiao Zhang", "Zhiyuan Fang", "Yandong Wen", "Zhifeng Li", "Yu Qiao." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 5409–5418.",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Medical intent detection aims to identify intents of medical queries and classify them into specific categories (Chen et al., 2012; Howard and Cambria, 2013; Guo et al., 2014; Cai et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 13,
      "context" : "Medical intent detection aims to identify intents of medical queries and classify them into specific categories (Chen et al., 2012; Howard and Cambria, 2013; Guo et al., 2014; Cai et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : "Medical intent detection aims to identify intents of medical queries and classify them into specific categories (Chen et al., 2012; Howard and Cambria, 2013; Guo et al., 2014; Cai et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : "Medical intent detection aims to identify intents of medical queries and classify them into specific categories (Chen et al., 2012; Howard and Cambria, 2013; Guo et al., 2014; Cai et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 22,
      "context" : "In medical scenarios, understanding query intent is very important for medical question answer systems (Wu et al., 2020; Mrini et al., 2021).",
      "startOffset" : 103,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "In medical scenarios, understanding query intent is very important for medical question answer systems (Wu et al., 2020; Mrini et al., 2021).",
      "startOffset" : 103,
      "endOffset" : 140
    }, {
      "referenceID" : 20,
      "context" : "Consider the above problems, we propose to address this issue in a incremental learning way (Ring et al., 1994; Thrun, 1998), where the number of intent categories is allowed to incrementally increase and the system can incessantly learn emerged novel intents from continually arriving data of new intents, which is illustrated as Figure 1.",
      "startOffset" : 92,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "However, this method suffers from the serious catastrophic forgetting problem (McCloskey and Cohen, 1989; French, 1999; Wang et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "However, this method suffers from the serious catastrophic forgetting problem (McCloskey and Cohen, 1989; French, 1999; Wang et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "However, this method suffers from the serious catastrophic forgetting problem (McCloskey and Cohen, 1989; French, 1999; Wang et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "Typically, they are divided into parameter-based methods that preserve parameters important to the previous classes when updating (Kirkpatrick et al., 2017; Aljundi et al., 2018), and memory-based methods that store a few examples for each old class and replay them with arriving data of new classes (Rebuffi et al.",
      "startOffset" : 130,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "Typically, they are divided into parameter-based methods that preserve parameters important to the previous classes when updating (Kirkpatrick et al., 2017; Aljundi et al., 2018), and memory-based methods that store a few examples for each old class and replay them with arriving data of new classes (Rebuffi et al.",
      "startOffset" : 130,
      "endOffset" : 178
    }, {
      "referenceID" : 17,
      "context" : ", 2018), and memory-based methods that store a few examples for each old class and replay them with arriving data of new classes (Rebuffi et al., 2017; Hou et al., 2018, 2019).",
      "startOffset" : 129,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "Therefore, the model can be drastically altered by the richer new classes data and ignore old classes (He and Garcia, 2009; Wu et al., 2019; Zhang et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 160
    }, {
      "referenceID" : 23,
      "context" : "Therefore, the model can be drastically altered by the richer new classes data and ignore old classes (He and Garcia, 2009; Wu et al., 2019; Zhang et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "Therefore, the model can be drastically altered by the richer new classes data and ignore old classes (He and Garcia, 2009; Wu et al., 2019; Zhang et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "We use BERT (Devlin et al., 2018) as the medical intent classifier.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "Inspired by prototype learning (Snell et al., 2017; Yang et al., 2018), we select the top n example closest to the centroid of examples (based on “[CLS]” embeddings) of each class and store them into the memory as representative ones for old classes.",
      "startOffset" : 31,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "Inspired by prototype learning (Snell et al., 2017; Yang et al., 2018), we select the top n example closest to the centroid of examples (based on “[CLS]” embeddings) of each class and store them into the memory as representative ones for old classes.",
      "startOffset" : 31,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "Inspired by (Hinton et al., 2015), we first perform it at prediction level.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "Inspired by (Chen et al., 2020b), we devise contrast objective to push examples from the same intent close and examples from different intents further apart.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "We use two public medical datasets KUAKE-QIC in CBLUE (Zhang et al., 2021) and CMID (Chen et al.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : ", 2021) and CMID (Chen et al., 2020a) to construct benchmarks for incremental learning setting.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "We compare CRN with 4 baselines: 1) EWC (Kirkpatrick et al., 2017): The representative parameterbased method that keeps the network parameters close to the optimal parameters for the previous",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 21,
      "context" : "2) EMR (Wang et al., 2019): The representative memorybased method that avoids catastrophic forgetting via randomly storing a few examples of old classes.",
      "startOffset" : 7,
      "endOffset" : 26
    } ],
    "year" : 0,
    "abstractText" : "Conventional approaches to medical intent detection require fixed pre-defined intent categories. However, due to the incessant emergence of new medical intents in the real world, such requirement is not practical. Considering that it is computationally expensive to store and re-train the whole data every time new data and intents come in, we propose to incrementally learn emerged intents while avoiding catastrophically forgetting old intents. We first formulate incremental learning for medical intent detection. Then, we employ a memorybased method to handle incremental learning. We further propose to enhance the method with contrast replay networks, which use multilevel distillation and contrast objective to address training data imbalance and medical rare words respectively. Experiments show that the proposed method outperforms the state-of-theart model by 5.7% and 9.1% of accuracy on two benchmarks respectively.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_12_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning the Beauty in Songs: Neural Singing Voice Beautifier",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We are interested in a novel task, singing voice beautifying (SVB). Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre. Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality. Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone. In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve. Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one. To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions. Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics."
    }, {
      "heading" : "1 Introduction",
      "text" : "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019; Blaauw and Bonada, 2020; Ren et al., 2020; Lu et al., 2020; Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al., 2021; Wang et al., 2021a). However, the Singing Voice Beautifying (SVB) remains an important and challenging endeavor for researchers. SVB aims to improve the intonation1 and the vocal tone of the voice, while\n1Intonation refers to the accuracy of pitch in singing.\nkeeping the content and vocal timbre2. SVB is extensively required both in the professional recording studios and the entertainment industries in our daily life, since it is impractical to record flawless singing audio.\nNowadays in real-life scenarios, SVB is usually performed by professional sound engineers with adequate domain knowledge, who manipulate commercial vocal correction tools such as Melodyne3 and Autotune4 (Yong and Nam, 2018). Most current automatic pitch correction works are shown to be an attractive alternative, but they may 1) show weak alignment accuracy (Luo et al., 2018) or pitch accuracy (Wager et al., 2020); 2) cause the tuned recording and the reference recording to be homogeneous in singing style (Yong and Nam, 2018). Besides, they typically focus on the intonation but ignore the overall aesthetic quality (audio quality and vocal tone) (Rosenzweig et al., 2021; Zhuang et al., 2021).\nTo tackle these challenges, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014; Sohn et al., 2015) as the backbone to generate high-quality audio and learns the latent representation of vocal tone. In NSVB, we dichotomize the SVB task into pitch correction and vocal tone improvement: 1) To correct the intonation, a straightforward method is aligning the amateur recording with the template pitch curve, and then putting them together to resynthesize a new singing sample. Previous works (Wada et al., 2017; Luo et al., 2018) implemented this by figuring out the alignment through\n2The differences between the vocal tone and vocal timbre is that: the former represents one’s skills of singing, such as airflow controlling ability, muscle strength of vocal folds and vocal placement; the latter represents the identical, overall sound of one’s vocal.\n3https://www.celemony.com/en/start 4https://www.antarestech.com/\nDynamic Time Warping (DTW) (Müller, 2007) or Canonical Time Warping (CTW) (Zhou and Torre, 2009). We propose a novel Shape-Aware DTW algorithm, which ameliorates the robustness of existing time-warping approaches by considering the shape of the pitch curve rather than low-level features when calculating the optimal alignment path. 2) To improve the vocal tone, we propose a latentmapping algorithm in the latent space, which converts the latent variables of the amateur vocal tone to those of the professional ones. This process is optimized by maximizing the log-likelihood of the converted latent variables. To retain the vocal timbre during the vocal tone mapping, we also propose a new dataset named PopBuTFy containing parallel singing recordings of both amateur and professional versions. Besides, thanks to the autoencoder structure, NSVB inherently supports semisupervised learning, where the additional unpaired, unlabeled5 singing data could be leveraged to facilitate the learning of the latent representations. Extensive experiments on both Chinese and English songs show that NSVB outperforms previous methods by a notable margin, and each component in NSVB is effective, in terms of both objective and subjective metrics. The main contributions of this work are summarized as follows6:\n• We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention.\n• We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which meliorates the robustness of the previous time-warping algorithm.\n• We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one’s, and contribute a new dataset PopBuTFyto train the latent-mapping function, which will be released upon the paper is published.\n• We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage\n5“unpaired, unlabeled” means the recordings sung by any people, in any vocal tone without label.\n6Audio samples are available at https://neuralsvb. github.io.\nunpaired, unlabeled singing data for better performance."
    }, {
      "heading" : "2 Related Works",
      "text" : ""
    }, {
      "heading" : "2.1 Singing Voice Conversion",
      "text" : "Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015; Serrà et al., 2019; Popov et al., 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021). Mainstream SVC models can be grouped into three categories (Zhao et al., 2020): 1) parallel spectral feature mapping models, which learn the conversion function between source and target singers relying on parallel singing data (Villavicencio and Bonada, 2010; Kobayashi et al., 2015; Sisman et al., 2019); 2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al., 2017; Kaneko et al., 2019), where an adversarial loss and a cycleconsistency loss are concurrently used to learn the forward and inverse mappings simultaneously (Sisman and Li, 2020); 3) encoder-decoder models, such as PPG-SVC (Li et al., 2021), which leverage a singing voice synthesis (SVS) system for SVC (Zhang et al., 2020), and auto-encoder (Qian et al., 2019a; Wang et al., 2021b; Yuan et al., 2020) based SVC (Wang et al., 2021a). The models of the latter two categories can be utilized with nonparallel data. In our work, we aim to convert the intonation and the vocal tone while keeping the content and the vocal timbre, which is quite different from the SVC task."
    }, {
      "heading" : "2.2 Automatic Pitch Correction",
      "text" : "Automatic Pitch Correction (APC) works attempt to minimize the manual effort in modifying the flawed singing voice (Yong and Nam, 2018). Luo et al. (2018) propose Canonical Time Warping (CTW) (Zhou and Torre, 2009; Zhou and De la Torre, 2012) which aligns amateur singing recordings to professional ones according to the pitch curves only. Wager et al. (2020) propose a datadriven approach to predict pitch shifts depending on both amateur recording and its accompaniment. Rosenzweig et al. (2021) propose a pitch shift method for Cappella recordings. Zhuang et al. (2021) propose a pitch-controllable SVS system to resynthesize the audio with correctly predicted pitch curves. Besides modifying pitch, Yong and\nPitch (Aligned in Stage 2 and Inference)\nPitch Modul\ne\nType equation here.\nPitch\nModul\nTarget mel\nVAE Dec\nVAE Enc\nAmateur Cond\nProfessional Cond\nNam (2018) propose to modify pitch and energy information to improve the singing expressions of an amateur singing recording. However, this method heavily relies on a reference recording, causing the tuned recording and the reference recording to be homogeneous in singing style (Zhuang et al., 2021). Our work adopts the non-parametric and data-free pitch correction method like Luo et al. (2018), but improves the accuracy of alignment."
    }, {
      "heading" : "3 Methdology",
      "text" : "In this section, we describe the overview of NSVB, which is shown in Figure 1. At Stage 1 in the figure, we reconstruct the input mel-spectrogram through the CVAE backbone (Section 3.1) based on the pitch, content and vocal timbre conditions extracted from the input by the pitch encoder, content encoder and timbre encoder, and optimize the CVAE by maximizing evidence lower bound and adversarial learning. At Stage 2/Inference in the figure, firstly we infer the latent variable za based on the amateur conditions; secondly we prepare the amateur content vectors aligned with the professional pitch by SADTW algorithm (Section 3.2); thirdly we map za to zp by the latent-mapping algorithm (Section 3.3); finally, we mix the professional pitch, the aligned amateur content vectors, and the amateur vocal timbre to obtain a new condition, which is leveraged along with the mapped zp by the decoder of CVAE to generate a new beautified mel-spectrogram. The training/inference details and model structure of each component in NSVB are described in Section 3.4 and Section 3.5."
    }, {
      "heading" : "3.1 Conditional Variational Generator with Adversarial Learning",
      "text" : "As shown in Figure 2, to generate audio with high quality and learn the latent representations of vocal tone, we introduce a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014; Sohn et al., 2015) as the mel-spectrogram generator, with the optimizing objective of maximizing the evidence lower bound (ELBO) of the intractable marginal log-likelihood of melspectrogram log pθ(x|c):\nlog pθ(x|c) ≥ ELBO(ϕ, θ) ≡\nEz∼qϕ(z|x,c)\n[ log pθ(x|z, c)− log\nqϕ(z|x, c) p(z)\n] ,\nwhere x, c, z denote the input/output melspectrogram, the mix of content, vocal timbre and pitch conditions, and the latent variable representing the vocal tone respectively; ϕ and θ denote the model parameters of CVAE encoder and CVAE decoder; qϕ(z|x, c) is the posterior distribution approximated by the CVAE encoder; pθ(x|z, c) is the likelihood function that generates mel-spectrograms given latent variable z and condition c; p(z) is the prior distribution of the latent variables z, and we choose the standard normal distribution as p(z) for simplification. Furthermore, to address the over-smoothing problem (Qian et al., 2019b) in CVAE, we utilize an adversarial discriminator (D) (Mao et al., 2017) to refine the output mel-spectrogram:\nLadv(ϕ, θ) = E[(D(x̃)− 1)2], Ladv(D) = E[(D(x)− 1)2] + E[D(x̃)2], (1)\nwhere x is the ground-truth and x̃ is the output of CVAE. The descriptions for the model structure of each component are in Section 3.5."
    }, {
      "heading" : "3.2 Shape-Aware Dynamic Time Warping",
      "text" : "To implement the pitch correction, a straightforward method is aligning the amateur recording with the template pitch curve, and then concatenating them to resynthesize a new singing sample with improved intonation. Since the source pitch curve of amateur recordings and template one show a high degree of natural correlation along the time axis, applying a proper time-warping algorithm on them is crucial. However, original DTW (Müller, 2007) could result in a poor alignment when certain parts of the axis move to higher frequencies, and other parts to lower ones, or vice versa (Sundermann and Ney, 2003). Luo et al. (2018) adopt an advanced algorithm CTW (Zhou and Torre, 2009), which combines the canonical correlation analysis (CCA) and DTW to extract the feature sequences of two pitch curves, and then apply DTW on them. However, the alignment accuracy of CTW leaves much to be desired.\nWe elaborate a non-parametric and data-free algorithm, Shape-Aware DTW (SADTW), based on the prior knowledge that the source pitch curve and the template one have analogous local shape contours. Specifically, we replace the Euclidean distance in the original DTW distance matrix with\nthe shape context descriptor distance. The shape context descriptor of a time point fi in one pitch curve is illustrated in Figure 3. Inspired by (Mori et al., 2005), we divide the data points around fi into m ∗ n bins by m time windows and n angles. We calculate the number of all points falling in the k-th bin. Then the descriptor for fi is defined as the histogram hi ∈ Rm∗n:\nhi(k) = |{fj ̸= fi, fj ∈ bin(k)}|,\nwhere | · | means the cardinality of a set. This histogram represents the distribution over relative positions, which is a robust, compact and discriminative descriptor. Then, it is natural to use the X 2-test statistic on this distribution descriptor as the “distance” of two points fa and fp:\nC(a, p) = 1\n2 m∗n∑ k=1 [ha(k)− hp(k)]2 ha(k) + hp(k) ,\nwhere ha and hp are the normalized histograms corresponding to the point fa from the amateur pitch curve and the point fp from the template pitch curve. C(a, p) ranges from 0 to 1. Finally, we run DTW on the distance matrix C to obtain the alignment with least distance cost between two curves."
    }, {
      "heading" : "3.3 Latent-mapping Algorithm",
      "text" : "Define a pair of mel-spectrograms (xa,xp): the contents of xa and yp are the same sentence of a song from the same singer7, who sings these two recordings using the amateur tone and the professional tone respectively. Given the CVAE model,\n7The singers all major in vocal music.\nwe can infer the posterior distribution qϕ(za|xa, ca) and qϕ(zp|xp, cp) corresponding to xa and xp through the encoder of CVAE. To achieve the conversion of vocal tone, we introduce a mapping function M to convert the latent variables from qϕ(za|xa, ca) to qϕ(zp|xp, cp). Concretely, we sample a latent variable of amateur vocal tone za from qϕ(za|xa, ca), and map za to M(za). Then, M can be optimized by minimizing the negative log-likelihood of M(za):\nLmap1(M) = − log qϕ(M(za)|xp, cp).\nDefine ĉp as the mix of 1) the content vectors from the amateur recording aligned by SADTW, 2) vocal timbre embedding encoded by timbre encoder, and 3) template pitch8 embeddings encoded by pitch encoder. To make sure the converted latent variable could work well together with ĉp to generate a high-quality audio sample (with the correct pitch and improved vocal tone), we send M(za) to the CVAE decoder to generate x̂, and propose an additional loss:\nLmap2(M) = ∥x̂− xp∥1 + λ(D(x̂)− 1)2,\nwhere D has been optimized by Eq. (1); λ is a hyper-parameter."
    }, {
      "heading" : "3.4 Training and Inference",
      "text" : "There are two training stages for NSVB: in the first training stage, we optimize CVAE by minimizing the following loss function\nL(ϕ, θ) = −ELBO(ϕ, θ) + λLadv(ϕ, θ),\nand optimize the discriminator (D) by minimizing Eq. (1). Note that, the first stage is the reconstruction process of mel-spectrograms, where any unpaired, unlabeled singing data beyond PopBuTFy could be leveraged to facilitate the learning of the latent representations. In the second training stage, we optimize M on the parallel dataset PopBuTFy by minimizing the following loss function\nL(M) = Lmap1(M) + Lmap2(M).\nϕ, θ, and D are not optimized in this stage. In inference, the encoder of CVAE encodes xa with the condition ca to predict za. Secondly, we map za to M(za), and run SADTW to align the\n8During training, template pitch is extracted from the waveform corresponding to xp.\namateur recordings with the template pitch curve. The template pitch curve can be derived from a reference recording with good intonation or a pitch predictor with the input of music notes. Then, we obtain ĉp defined in Section 3.3 and send M(za) together with ĉp in the decoder of CVAE to generate x̂. Finally, by running a pre-trained vocoder conditioned on x̂, a new beautified recording is produced."
    }, {
      "heading" : "3.5 Model Structure",
      "text" : "The encoder of CVAE consists of a 1-D convolutional layer (stride=4), an 8-layer WaveNet structure (Oord et al., 2016; Rethage et al., 2018) and 3 1-D convolutional layers (stride=2) with ReLU activation function and batch normalization followed by a mean pooling, which outputs the mean and log scale standard deviation parameters in the posterior distribution of z. The decoder of CVAE consists of a 4-layer WaveNet structure and a 1-D convolutional layer, which outputs the mel-spectrogram with 80 channels. The discriminator adopts the same structure as (Wu and Luan, 2020), which consists of multiple random window discriminators. The latent-mapping function is composed of 2 linear layers to encode the vocal timbre as the mapping condition, and 3 linear layers to map za. The pitch encoder is composed of 3 convolutional layers. In addition, given a singing recording, 1) to obtain its content vectors, we train an Automatic Speech Recognition (ASR) model based on Conformer (Gulati et al., 2020) with both speech and singing data, and extract the hidden states from the ASR encoder (viewed as the content encoder) output as the linguistic content information, which are also called phonetic posterior-grams (PPG); 2) to obtain the vocal timbre, we leverage the open-source API resemblyzer9 as the timbre encoder, which is a deep learning model designed for speaker verification (Wan et al., 2018), to extract the identity information of a singer. More details of model structure can be found in Appendix A."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "In this section, we first introduce PopBuTFy, the dataset for SVB, and then describe the implementation details in our work. Finally, we explain the evaluation method we adopt in this paper.\n9https://github.com/resemble-ai/ Resemblyzer\nDataset Since there is no publicly available highquality, unaccompanied and parallel singing dataset for the SVB task, we collect and annotate a dataset containing both Chinese Mandarin and English pop songs: PopBuTFy. To collect PopBuTFy for SVB, the qualified singers majoring in vocal music are asked to sing a song twice, using the amateur vocal tone for one time and the professional vocal tone for another. Note that some of the amateur recordings are sung off-key by one or more semi-tones for the pitch correction sub-task. The parallel setting could make sure that the personal vocal timbre will keep still during the beautifying process. In all, PopBuTFy consists of 99 Chinese pop songs (∼10.4 hours in total) from 12 singers and 443 English pop songs (∼40.4 hours in total) from 22 singers. All the audio files are recorded in a professional recording studio by qualified singers, male and female. Every song is sampled at 22050 Hz with 16-bit quantization. We randomly choose 617 pieces in English and 274 pieces in Chinese for validation and test. For subjective evaluations, we choose 60 samples in the test set from different singers, half in English and Chinese. All testing samples are included for objective evaluations. We will release PopBuTFy and corresponding annotations once the paper is published.\nImplementation Details We train the Neural Singing Beautifier on a single 32G Nividia V100 GPU with the batch size of 64 sentences for both 100k steps in Stage 1 and Stage 2 respectively. Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi, 2020) for Chinese and LibriTTS (Zen et al., 2019) for English. For the semi-supervised learning mentioned in Section 1 and Section 3.4, we leverage an internal Chinese singing dataset (∼30 hours without labeled vocal tone) in the first training stage described in Section 3.4 for Chinese experiments. The output melspectrograms of our model are transformed into audio samples using a HiFi-GAN vocoder (Kong et al., 2020) trained with singing data in advance. We set the λ metioned in Section 3.3 to 0.1. We transform the raw waveform with the sampling rate 22050 Hz into mel-spectrograms with the frame size 1024 and the hop size 128. We extract F0 (fundamental frequency) as pitch information from the raw waveform using Parselmouth10, following\n10https://github.com/YannickJadoul/ Parselmouth\nWu and Luan (2020); Blaauw and Bonada (2020); Ren et al. (2020). To obtain the ground truth pitch alignment between the amateur recordings and the professional ones for evaluating the accuracy of pitch alignment algorithm, we run the Montreal Forced Aligner tool (McAuliffe et al., 2017) on all the singing recordings to obtain their alignments to lyrics. Then the ground-truth pitch alignment can be derived since the lyrics are shared in a pair of data in PopBuTFy.\nPerformance Evaluation We employ both subjective metrics: Mean Opinion Score (MOS), Comparison Mean Opinion Score (CMOS), and an objective metric: Mean Cepstral Distortion (MCD) to evaluate the audio quality on the test-set. Besides, we use F0 Root Mean Square Error (F0 RMSE) and Pitch Alignment Accuracy (PAA) to estimate the pitch correction performance. For audio, we analyze the MOS and CMOS in two aspects: audio quality (naturalness, pronunciation and sound quality) and vocal tone quality. MOS-Q/CMOS-Q and MOS-V/CMOS-V correspond to the MOS/CMOS of audio quality and vocal tone quality respectively. More details about subjective evaluations are placed in Appendix C."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "In this section, we conduct extensive experiments to present our proposed model in regard to 1) the performance of pitch conversion; 2) the audio quality and vocal tone quality."
    }, {
      "heading" : "4.2.1 Pitch Correction",
      "text" : "Firstly, we provide the comparison among timewarping algorithms in terms of PAA in Table 1. Normed DTW means two pitch curves will be normalized before running DTW (Müller, 2007); CTW means the Canonical Time Warping (Zhou and Torre, 2009), which is used for pitch correction in Luo et al. (2018). It can be seen that, SADTW surpasses existing methods by a large margin. We also visualize an alignment example of DTW, CTW, and SADTW in Figure 4.\nSecondly, to check whether the amateur recordings are corrected to the good intonation after being beautified by NSVB, we calculate the F0 RMSE metric of the amateur recordings and the audio generated by NSVB, and list the results in Table 2. We can see that F0 RMSE has been improved significantly, which means NSVB successfully achieve pitch correction."
    }, {
      "heading" : "4.2.2 Audio Quality and Vocal Tone Quality",
      "text" : "To thoroughly evaluate our proposed model in audio quality and vocal tone quality, we compare subjective metric MOS-Q, MOS-V and objective metric MCD of audio samples generated by NVSB with the systems including: 1) GT Mel, amateur (A) and professional (P) version, where we first convert ground truth audio into mel-spectrograms, and then convert the mel-spectrograms back to audio using HiFi-GAN introduced in Section 4.1; 2) Baseline: the baseline model for SVB based on WaveNet with the number of parameters similar to NSVB, which\nadopts the same pitch correction method (SADTW) as NSVB does, and takes in the condition ĉp defined in Section 3.3 to generate the mel-spectrogram optimized by the L1 distance to xp. MCD is calculated using the audio samples of GT Mel P as references.\nThe subjective and objective results on both Chinese and English datasets are shown in Table 3. We can see that 1) NSVB achieves promising results, with MOS-Q being less than those for ground truth professional recordings by only 0.1 and 0.12 on both datasets; 2) NSVB surpasses the GT Mel A in terms of MOS-V by a large margin, which indicates that NSVB successfully accomplishes the vocal tone improvement. 3) NSVB surpasses the baseline model on all the metrics distinctly, which proves the superiority of our proposed model; 4) GT Mel P, NSVB and Baseline all outperform GT Mel A in terms of MOS-V, which demonstrates that the proposed dataset PopBuTFy is reasonably labeled in respect of vocal tone."
    }, {
      "heading" : "4.3 Ablation Studies",
      "text" : "We conduct some ablation studies to demonstrate the effectiveness of our proposed methods and some designs in our model, including latentmapping, additional loss Lmap2 in the second training stage, and semi-supervised learning with extra unpaired, unlabeled data on Chinese songs."
    }, {
      "heading" : "4.3.1 Latent Mapping",
      "text" : "We compare audio samples from NSVB with and without latent-mapping in terms of CMOS-V and MCD. From Table 4, we can see that the latentmapping brings CMOS-V and MCD gains, which demonstrates the improvements in vocal tone from latent-mapping in our model. We visualize linearspectrograms of GT Mel A, GT Mel P, NSVB, NSVB w/o mapping in Appendix B. The patterns of highfrequency parts in NVSB samples are comparatively similar to those in GT Mel P samples while NSVB w/o mapping sample resembles GT Mel A samples.\n4.3.2 Additional Loss Lmap2 As shown in Table 5, all the compared metrics show the effectiveness of Lmap2, which means that the additional loss Lmap2 is beneficial to optimizing the latent mapping function M, working as a complement to the basic loss Lmap1."
    }, {
      "heading" : "4.3.3 Semi-supervised Learning",
      "text" : "To illustrate the advantage of the CVAE architecture that allows semi-supervised training, we compare NSVB trained with and without extra unpaired, unlabeled data on Chinese songs. The corresponding results are shown in Table 6. The compared metrics indicate the advantage of semi-supervised learning, which facilitates the learning of the latent representations for better sample reconstruction (audio quality) and better latent conversion (vocal tone quality)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we propose Neural Singing Voice Beautifier, the first generative model for the SVB task, which is based on a CVAE model allowing semi-supervised learning. For pitch correction, we propose a robust alignment algorithm: ShapeAware Dynamic Time Warping (SADTW). For vocal tone improvement, we propose a latent mapping algorithm. To retain the vocal timbre during the vocal tone mapping, we also propose a new specialized SVB dataset named PopBuTFy containing parallel singing recordings of both amateur and professional versions. The experiments conducted on the dataset of Chinese and English songs show that NSVB accomplishes the SVB task (pitch correction and vocal tone improvement), and extensional ablation studies demonstrate the effectiveness of the proposed methods mentioned above."
    }, {
      "heading" : "A Details of Model Structure",
      "text" : "The details of the adversarial discriminator, the content encoder, and WaveNet structure are shown in Figure 5, Figure 8, and Figure 6. The hidden size of CVAE model, latent variable and discriminator are 256, 128 and 128 respectively. We train NSVB on a single V100 32G GPU for almost 22 hours to finish two-stage training.\nA.1 Multi-window Discriminator\nAs shown in Figure 5, our multi-window discriminator consists of 2 unconditional discriminator parts with fixed window sizes. Each unconditional discriminator contains N layers of Conv units. In our model, we set N = 3. The Conv units are all 1-D convolutional networks with ReLU activation and spectral normalization. The outputs of these unconditional discriminators are then concatenated and linearly projected to form the output.\nA.2 WaveNet\nAs shown in Figure 6, the WaveNet unit used in the VAE encoder and decoder of NVSB consists of a 1D convolution layer with ReLU to preprocess the input, and a group of sub-layers with residual connection between adjacent ones. Each sub-layer contains a 1× 1 convolutional layer to process the input condition and a 3× 3 convolutional layer for\nresidual input. After that, they got fused by being added up, then processed by tanh and sigmoid separately and then multiplied together. Finally, they produce a residual output for the next sub-layer and a skip-out. Lastly, two layers of 1D convolution and a ReLU process the summed skip-out to produce output.\nA.3 Content Encoder\nAs shown in Figure 8, the content encoder is the combination of several conformer encoder layers in pink rectangle along with a 3-layer prenet. The kernel size of the convolutional layer for prenet is 5. The hidden size is 256. We use 4 heads in the multi-head self-attention part. And we use 31 stacked conformer encoder layers to form this module. During pre-training, an ASR transformer decoder is attached to decode texts out for regular ASR training. After pre-training, only the encoder and the prenet part is used to extract PPG features from mel-spectrograms of audio samples."
    }, {
      "heading" : "B Linear-spectrograms Visualizations",
      "text" : "We visualize four linear-spectrograms generated with the same content. It seems that the professional vocal tone is related to certain patterns in the high-frequency region of the spectrograms. In the future, SVB may be accomplished in a more fine-grained way together with the knowledge in vocal music."
    }, {
      "heading" : "C Details in subjective evaluations",
      "text" : "During testing, each audio sample is listened to by at least 10 qualified testers, all majoring in vocal music. We tell all testers to focus on one aspect and ignore the other aspect when scoring MOS/CMOS of each aspect. For MOS, each tester is asked to evaluate the subjective naturalness of a sentence on a 1-5 Likert scale. For CMOS, listeners are asked to compare pairs of audio generated by systems A and B and indicate which of the two audio they prefer and choose one of the following scores: 0 indicating no difference, 1 indicating small difference, 2 indicating a large difference. For audio quality evaluation (MOS-Q and CMOS-Q), we tell listeners to \"focus on examining the naturalness, pronunciation and sound quality, and ignore the differences of singing vocal tone\". For vocal tone evaluations (MOS-V and CMOS-V), we tell listeners to \"focus on examining singing vocal tone of the song, and ignore the differences of audio quality (e.g., environmental noise, timbre)\". We split evaluations for main experiments and ablation studies into several groups for them. They are asked to take a break for 15 minutes between each group of experiments to remain focused during subjective\nevaluations. All testers get reasonably paid."
    }, {
      "heading" : "D Ethical Considerations",
      "text" : "Since in this paper we present a new dataset, we answer the questions in the “Ethics” part of the “Author Checklist”:\nD.1 Does the paper describe how intellectual property (copyright, etc) was respected in the data collection process\nYes, we sign an agreement with the participants that we only allow these singing recordings for non-commercial use.\nD.2 Does the paper describe how participants’ privacy rights were respected in the data collection process?\nThe participants are asked to use bogus names in the data collection process, which means we do not know who sang these recordings.\nE Does the paper describe how crowd workers or other annotators were fairly compensated and how the compensation was determined to be fair?\nYes. We pay each worker $320 per hour when recording these songs, which is determined after the market research by the data annotation company.\nF Does the paper indicate that the data collection process was subjected to any necessary review by an appropriate review board?\nYes. The data collection process was subjected to review by the review board of the data annotation company."
    } ],
    "references" : [ {
      "title" : "Gpufriendly local regression for voice conversion",
      "author" : [ "Taylor Berg-Kirkpatrick", "Dan Klein." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Berg.Kirkpatrick and Klein.,? 2015",
      "shortCiteRegEx" : "Berg.Kirkpatrick and Klein.",
      "year" : 2015
    }, {
      "title" : "Sequence-tosequence singing synthesis using the feed-forward transformer",
      "author" : [ "Merlijn Blaauw", "Jordi Bonada." ],
      "venue" : "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7229–7233. IEEE.",
      "citeRegEx" : "Blaauw and Bonada.,? 2020",
      "shortCiteRegEx" : "Blaauw and Bonada.",
      "year" : 2020
    }, {
      "title" : "Conformer: Convolution-augmented transformer for speech recognition",
      "author" : [ "Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang." ],
      "venue" : "Interspeech 2020,",
      "citeRegEx" : "Gulati et al\\.,? 2020",
      "shortCiteRegEx" : "Gulati et al\\.",
      "year" : 2020
    }, {
      "title" : "Cyclegan-vc2: Improved cyclegan-based non-parallel voice conversion",
      "author" : [ "Takuhiro Kaneko", "Hirokazu Kameoka", "Kou Tanaka", "Nobukatsu Hojo." ],
      "venue" : "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Kaneko et al\\.,? 2019",
      "shortCiteRegEx" : "Kaneko et al\\.",
      "year" : 2019
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling." ],
      "venue" : "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Statistical singing voice conversion based on direct waveform modification with global variance",
      "author" : [ "Kazuhiro Kobayashi", "Tomoki Toda", "Graham Neubig", "Sakriani Sakti", "Satoshi Nakamura." ],
      "venue" : "Sixteenth Annual Conference of the International Speech Com-",
      "citeRegEx" : "Kobayashi et al\\.,? 2015",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2015
    }, {
      "title" : "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "author" : [ "Jungil Kong", "Jaehyeon Kim", "Jaekyoung Bae." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 17022–17033. Curran Associates,",
      "citeRegEx" : "Kong et al\\.,? 2020",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarially trained end-to-end korean singing voice synthesis system",
      "author" : [ "Juheon Lee", "Hyeong-Seok Choi", "Chang-Bin Jeon", "Junghyun Koo", "Kyogu Lee." ],
      "venue" : "Proc. Interspeech 2019, pages 2588–2592.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Ppg-based singing voice conversion with adversarial representation learning",
      "author" : [ "Zhonghao Li", "Benlai Tang", "Xiang Yin", "Yuan Wan", "Ling Xu", "Chen Shen", "Zejun Ma." ],
      "venue" : "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Xiaoicesing: A high-quality and integrated",
      "author" : [ "Peiling Lu", "Jie Wu", "Jian Luan", "Xu Tan", "Li Zhou" ],
      "venue" : null,
      "citeRegEx" : "Lu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "Singing voice correction using canonical time warping",
      "author" : [ "Yin-Jyun Luo", "Ming-Tso Chen", "Tai-Shih Chi", "Li Su." ],
      "venue" : "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 156–160. IEEE.",
      "citeRegEx" : "Luo et al\\.,? 2018",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2018
    }, {
      "title" : "Least squares generative adversarial networks",
      "author" : [ "Xudong Mao", "Qing Li", "Haoran Xie", "Raymond YK Lau", "Zhen Wang", "Stephen Paul Smolley." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2794–2802.",
      "citeRegEx" : "Mao et al\\.,? 2017",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2017
    }, {
      "title" : "Montreal forced aligner: Trainable text-speech alignment using kaldi",
      "author" : [ "Michael McAuliffe", "Michaela Socolof", "Sarah Mihuc", "Michael Wagner", "Morgan Sonderegger." ],
      "venue" : "Interspeech, pages 498–502.",
      "citeRegEx" : "McAuliffe et al\\.,? 2017",
      "shortCiteRegEx" : "McAuliffe et al\\.",
      "year" : 2017
    }, {
      "title" : "Efficient shape matching using shape contexts",
      "author" : [ "Greg Mori", "Serge Belongie", "Jitendra Malik." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(11):1832–1837.",
      "citeRegEx" : "Mori et al\\.,? 2005",
      "shortCiteRegEx" : "Mori et al\\.",
      "year" : 2005
    }, {
      "title" : "Dynamic time warping",
      "author" : [ "Meinard Müller." ],
      "venue" : "Information retrieval for music and motion, pages 69–84.",
      "citeRegEx" : "Müller.,? 2007",
      "shortCiteRegEx" : "Müller.",
      "year" : 2007
    }, {
      "title" : "Wavenet: A generative model for raw audio",
      "author" : [ "Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu." ],
      "venue" : "9th ISCA Speech Synthesis Work-",
      "citeRegEx" : "Oord et al\\.,? 2016",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Diffusion-based voice conversion with fast maximum likelihood sampling scheme",
      "author" : [ "Vadim Popov", "Ivan Vovk", "Vladimir Gogoryan", "Tasnima Sadekova", "Mikhail Kudinov", "Jiansheng Wei." ],
      "venue" : "arXiv preprint arXiv:2109.13821.",
      "citeRegEx" : "Popov et al\\.,? 2021",
      "shortCiteRegEx" : "Popov et al\\.",
      "year" : 2021
    }, {
      "title" : "Autovc: Zeroshot voice style transfer with only autoencoder loss",
      "author" : [ "Kaizhi Qian", "Yang Zhang", "Shiyu Chang", "Xuesong Yang", "Mark Hasegawa-Johnson." ],
      "venue" : "International Conference on Machine Learning, pages 5210–5219. PMLR.",
      "citeRegEx" : "Qian et al\\.,? 2019a",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2019
    }, {
      "title" : "AutoVC: Zeroshot voice style transfer with only autoencoder loss",
      "author" : [ "Kaizhi Qian", "Yang Zhang", "Shiyu Chang", "Xuesong Yang", "Mark Hasegawa-Johnson." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings",
      "citeRegEx" : "Qian et al\\.,? 2019b",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2019
    }, {
      "title" : "Deepsinger: Singing voice synthesis with data mined from the web",
      "author" : [ "Yi Ren", "Xu Tan", "Tao Qin", "Jian Luan", "Zhou Zhao", "Tie-Yan Liu." ],
      "venue" : "arXiv preprint arXiv:2007.04590.",
      "citeRegEx" : "Ren et al\\.,? 2020",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2020
    }, {
      "title" : "A wavenet for speech denoising",
      "author" : [ "Dario Rethage", "Jordi Pons", "Xavier Serra." ],
      "venue" : "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5069–5073. IEEE.",
      "citeRegEx" : "Rethage et al\\.,? 2018",
      "shortCiteRegEx" : "Rethage et al\\.",
      "year" : 2018
    }, {
      "title" : "Adaptive pitchshifting with applications to intonation adjustment in a cappella recordings",
      "author" : [ "Sebastian Rosenzweig", "Simon Schwär", "Jonathan Driedger", "Meinard Müller." ],
      "venue" : "Proceedings of the International Conference on Digital Audio Effects (DAFx),",
      "citeRegEx" : "Rosenzweig et al\\.,? 2021",
      "shortCiteRegEx" : "Rosenzweig et al\\.",
      "year" : 2021
    }, {
      "title" : "Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion",
      "author" : [ "Joan Serrà", "Santiago Pascual", "Carlos Segura Perales." ],
      "venue" : "Advances in Neural Information Processing Systems, 32:6793– 6803.",
      "citeRegEx" : "Serrà et al\\.,? 2019",
      "shortCiteRegEx" : "Serrà et al\\.",
      "year" : 2019
    }, {
      "title" : "Generative adversarial networks for singing voice conversion with and without parallel data",
      "author" : [ "Berrak Sisman", "Haizhou Li." ],
      "venue" : "Speaker Odyssey, pages 238–244.",
      "citeRegEx" : "Sisman and Li.,? 2020",
      "shortCiteRegEx" : "Sisman and Li.",
      "year" : 2020
    }, {
      "title" : "Singan: Singing voice conversion with generative adversarial networks",
      "author" : [ "Berrak Sisman", "Karthika Vijayan", "Minghui Dong", "Haizhou Li." ],
      "venue" : "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA",
      "citeRegEx" : "Sisman et al\\.,? 2019",
      "shortCiteRegEx" : "Sisman et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Sohn et al\\.,? 2015",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Vtlnbased voice conversion",
      "author" : [ "David Sundermann", "Hermann Ney." ],
      "venue" : "Proceedings of the 3rd IEEE International Symposium on Signal Processing and Information Technology (IEEE Cat. No. 03EX795), pages 556–559. IEEE.",
      "citeRegEx" : "Sundermann and Ney.,? 2003",
      "shortCiteRegEx" : "Sundermann and Ney.",
      "year" : 2003
    }, {
      "title" : "Applying voice conversion to concatenative singing-voice synthesis",
      "author" : [ "Fernando Villavicencio", "Jordi Bonada." ],
      "venue" : "Eleventh annual conference of the international speech communication association.",
      "citeRegEx" : "Villavicencio and Bonada.,? 2010",
      "shortCiteRegEx" : "Villavicencio and Bonada.",
      "year" : 2010
    }, {
      "title" : "An adaptive karaoke system that plays accompaniment parts of music audio signals synchronously with users’singing voices",
      "author" : [ "Yusuke Wada", "Yoshiaki Bando", "Eita Nakamura", "Katsutoshi Itoyama", "Kazuyoshi Yoshii." ],
      "venue" : "SMC, pages 110–116.",
      "citeRegEx" : "Wada et al\\.,? 2017",
      "shortCiteRegEx" : "Wada et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep autotuner: A pitch correcting network for singing performances",
      "author" : [ "Sanna Wager", "George Tzanetakis", "Cheng-i Wang", "Minje Kim." ],
      "venue" : "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages",
      "citeRegEx" : "Wager et al\\.,? 2020",
      "shortCiteRegEx" : "Wager et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalized end-to-end loss for speaker verification",
      "author" : [ "Li Wan", "Quan Wang", "Alan Papir", "Ignacio Lopez Moreno." ],
      "venue" : "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4879–4883. IEEE.",
      "citeRegEx" : "Wan et al\\.,? 2018",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding",
      "author" : [ "Chao Wang", "Zhonghao Li", "Benlai Tang", "Xiang Yin", "Yuan Wan", "Yibiao Yu", "Zejun Ma" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
      "author" : [ "Disong Wang", "Liqun Deng", "Yu Ting Yeung", "Xiao Chen", "Xunying Liu", "Helen Meng." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Wang et al\\.,? 2021b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Adversarially trained multi-singer sequence-to-sequence singing synthesizer",
      "author" : [ "Jie Wu", "Jian Luan." ],
      "venue" : "arXiv preprint arXiv:2006.10317.",
      "citeRegEx" : "Wu and Luan.,? 2020",
      "shortCiteRegEx" : "Wu and Luan.",
      "year" : 2020
    }, {
      "title" : "Aishell-3: A multi-speaker mandarin tts corpus and the baselines",
      "author" : [ "Hui Bu" ],
      "venue" : null,
      "citeRegEx" : "Shi and Bu.,? \\Q2020\\E",
      "shortCiteRegEx" : "Shi and Bu.",
      "year" : 2020
    }, {
      "title" : "Singing expression transfer from one voice to another for a given song",
      "author" : [ "Sangeon Yong", "Juhan Nam." ],
      "venue" : "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 151–155. IEEE.",
      "citeRegEx" : "Yong and Nam.,? 2018",
      "shortCiteRegEx" : "Yong and Nam.",
      "year" : 2018
    }, {
      "title" : "Improving zero-shot voice style transfer via disentangled representation learning",
      "author" : [ "Siyang Yuan", "Pengyu Cheng", "Ruiyi Zhang", "Weituo Hao", "Zhe Gan", "Lawrence Carin." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Yuan et al\\.,? 2020",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Libritts: A corpus derived from librispeech for textto-speech",
      "author" : [ "Heiga Zen", "Viet Dang", "Rob Clark", "Yu Zhang", "Ron J. Weiss", "Ye Jia", "Zhifeng Chen", "Yonghui Wu." ],
      "venue" : "Interspeech 2019, 20th Annual Conference of the International Speech Communication",
      "citeRegEx" : "Zen et al\\.,? 2019",
      "shortCiteRegEx" : "Zen et al\\.",
      "year" : 2019
    }, {
      "title" : "Durian-sc: Duration informed attention network based singing voice conversion system",
      "author" : [ "Liqiang Zhang", "Chengzhu Yu", "Heng Lu", "Chao Weng", "Chunlei Zhang", "Yusong Wu", "Xiang Xie", "Zijin Li", "Dong Yu." ],
      "venue" : "Interspeech 2020, 21st Annual Conference",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Visinger: Variational inference with adversarial learning for end-to-end singing voice synthesis",
      "author" : [ "Yongmao Zhang", "Jian Cong", "Heyang Xue", "Lei Xie", "Pengcheng Zhu", "Mengxiao Bi" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Voice conversion challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion",
      "author" : [ "Yi Zhao", "Wen-Chin Huang", "Xiaohai Tian", "Junichi Yamagishi", "Rohan Kumar Das", "Tomi Kinnunen", "Zhenhua Ling", "Tomoki Toda." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalized time warping for multi-modal alignment of human motion",
      "author" : [ "Feng Zhou", "Fernando De la Torre." ],
      "venue" : "2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 1282–1289. IEEE.",
      "citeRegEx" : "Zhou and Torre.,? 2012",
      "shortCiteRegEx" : "Zhou and Torre.",
      "year" : 2012
    }, {
      "title" : "Canonical time warping for alignment of human behavior",
      "author" : [ "Feng Zhou", "Fernando Torre." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc. 10",
      "citeRegEx" : "Zhou and Torre.,? 2009",
      "shortCiteRegEx" : "Zhou and Torre.",
      "year" : 2009
    }, {
      "title" : "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "author" : [ "Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2223–2232.",
      "citeRegEx" : "Zhu et al\\.,? 2017",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2017
    }, {
      "title" : "Karatuner: Towards end to end natural pitch correction for singing voice in karaoke",
      "author" : [ "Xiaobin Zhuang", "Huiran Yu", "Weifeng Zhao", "Tao Jiang", "Peng Hu", "Simon Lui", "Wenjiang Zhou" ],
      "venue" : null,
      "citeRegEx" : "Zhuang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019; Blaauw and Bonada, 2020; Ren et al., 2020; Lu et al., 2020; Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al.",
      "startOffset" : 120,
      "endOffset" : 218
    }, {
      "referenceID" : 1,
      "context" : "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019; Blaauw and Bonada, 2020; Ren et al., 2020; Lu et al., 2020; Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al.",
      "startOffset" : 120,
      "endOffset" : 218
    }, {
      "referenceID" : 19,
      "context" : "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019; Blaauw and Bonada, 2020; Ren et al., 2020; Lu et al., 2020; Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al.",
      "startOffset" : 120,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019; Blaauw and Bonada, 2020; Ren et al., 2020; Lu et al., 2020; Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al.",
      "startOffset" : 120,
      "endOffset" : 218
    }, {
      "referenceID" : 39,
      "context" : "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019; Blaauw and Bonada, 2020; Ren et al., 2020; Lu et al., 2020; Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al.",
      "startOffset" : 120,
      "endOffset" : 218
    }, {
      "referenceID" : 23,
      "context" : ", 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al., 2021; Wang et al., 2021a).",
      "startOffset" : 43,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : ", 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020; Li et al., 2021; Wang et al., 2021a).",
      "startOffset" : 43,
      "endOffset" : 101
    }, {
      "referenceID" : 35,
      "context" : "Nowadays in real-life scenarios, SVB is usually performed by professional sound engineers with adequate domain knowledge, who manipulate commercial vocal correction tools such as Melodyne3 and Autotune4 (Yong and Nam, 2018).",
      "startOffset" : 203,
      "endOffset" : 223
    }, {
      "referenceID" : 10,
      "context" : "Most current automatic pitch correction works are shown to be an attractive alternative, but they may 1) show weak alignment accuracy (Luo et al., 2018) or pitch accuracy (Wager et al.",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 29,
      "context" : ", 2018) or pitch accuracy (Wager et al., 2020); 2) cause the tuned recording and the reference recording to be homogeneous in singing style (Yong and Nam, 2018).",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 35,
      "context" : ", 2020); 2) cause the tuned recording and the reference recording to be homogeneous in singing style (Yong and Nam, 2018).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "Besides, they typically focus on the intonation but ignore the overall aesthetic quality (audio quality and vocal tone) (Rosenzweig et al., 2021; Zhuang et al., 2021).",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 44,
      "context" : "Besides, they typically focus on the intonation but ignore the overall aesthetic quality (audio quality and vocal tone) (Rosenzweig et al., 2021; Zhuang et al., 2021).",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "To tackle these challenges, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014; Sohn et al., 2015) as the backbone to generate high-quality audio and learns the latent representation of vocal tone.",
      "startOffset" : 189,
      "endOffset" : 234
    }, {
      "referenceID" : 25,
      "context" : "To tackle these challenges, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014; Sohn et al., 2015) as the backbone to generate high-quality audio and learns the latent representation of vocal tone.",
      "startOffset" : 189,
      "endOffset" : 234
    }, {
      "referenceID" : 28,
      "context" : "Previous works (Wada et al., 2017; Luo et al., 2018) implemented this by figuring out the alignment through",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Previous works (Wada et al., 2017; Luo et al., 2018) implemented this by figuring out the alignment through",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "Dynamic Time Warping (DTW) (Müller, 2007) or Canonical Time Warping (CTW) (Zhou and Torre, 2009).",
      "startOffset" : 27,
      "endOffset" : 41
    }, {
      "referenceID" : 42,
      "context" : "Dynamic Time Warping (DTW) (Müller, 2007) or Canonical Time Warping (CTW) (Zhou and Torre, 2009).",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015; Serrà et al., 2019; Popov et al., 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al.",
      "startOffset" : 70,
      "endOffset" : 144
    }, {
      "referenceID" : 22,
      "context" : "Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015; Serrà et al., 2019; Popov et al., 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al.",
      "startOffset" : 70,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015; Serrà et al., 2019; Popov et al., 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al.",
      "startOffset" : 70,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : ", 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).",
      "startOffset" : 174,
      "endOffset" : 191
    }, {
      "referenceID" : 40,
      "context" : "Mainstream SVC models can be grouped into three categories (Zhao et al., 2020): 1) parallel spectral feature mapping models, which learn the conversion function between source and target singers relying on parallel singing data (Villavicencio and Bonada, 2010; Kobayashi et al.",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : ", 2020): 1) parallel spectral feature mapping models, which learn the conversion function between source and target singers relying on parallel singing data (Villavicencio and Bonada, 2010; Kobayashi et al., 2015; Sisman et al., 2019); 2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al.",
      "startOffset" : 157,
      "endOffset" : 234
    }, {
      "referenceID" : 5,
      "context" : ", 2020): 1) parallel spectral feature mapping models, which learn the conversion function between source and target singers relying on parallel singing data (Villavicencio and Bonada, 2010; Kobayashi et al., 2015; Sisman et al., 2019); 2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al.",
      "startOffset" : 157,
      "endOffset" : 234
    }, {
      "referenceID" : 24,
      "context" : ", 2020): 1) parallel spectral feature mapping models, which learn the conversion function between source and target singers relying on parallel singing data (Villavicencio and Bonada, 2010; Kobayashi et al., 2015; Sisman et al., 2019); 2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al.",
      "startOffset" : 157,
      "endOffset" : 234
    }, {
      "referenceID" : 43,
      "context" : ", 2019); 2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al., 2017; Kaneko et al., 2019), where an adversarial loss and a cycleconsistency loss are concurrently used to learn the forward and inverse mappings simultaneously (Sisman and Li, 2020); 3) encoder-decoder models, such as PPG-SVC (Li et al.",
      "startOffset" : 72,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : ", 2019); 2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al., 2017; Kaneko et al., 2019), where an adversarial loss and a cycleconsistency loss are concurrently used to learn the forward and inverse mappings simultaneously (Sisman and Li, 2020); 3) encoder-decoder models, such as PPG-SVC (Li et al.",
      "startOffset" : 72,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : ", 2019), where an adversarial loss and a cycleconsistency loss are concurrently used to learn the forward and inverse mappings simultaneously (Sisman and Li, 2020); 3) encoder-decoder models, such as PPG-SVC (Li et al.",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : ", 2019), where an adversarial loss and a cycleconsistency loss are concurrently used to learn the forward and inverse mappings simultaneously (Sisman and Li, 2020); 3) encoder-decoder models, such as PPG-SVC (Li et al., 2021), which leverage a singing voice synthesis (SVS) system for SVC (Zhang et al.",
      "startOffset" : 208,
      "endOffset" : 225
    }, {
      "referenceID" : 38,
      "context" : ", 2021), which leverage a singing voice synthesis (SVS) system for SVC (Zhang et al., 2020), and auto-encoder (Qian et al.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : ", 2020), and auto-encoder (Qian et al., 2019a; Wang et al., 2021b; Yuan et al., 2020) based SVC (Wang et al.",
      "startOffset" : 26,
      "endOffset" : 85
    }, {
      "referenceID" : 32,
      "context" : ", 2020), and auto-encoder (Qian et al., 2019a; Wang et al., 2021b; Yuan et al., 2020) based SVC (Wang et al.",
      "startOffset" : 26,
      "endOffset" : 85
    }, {
      "referenceID" : 36,
      "context" : ", 2020), and auto-encoder (Qian et al., 2019a; Wang et al., 2021b; Yuan et al., 2020) based SVC (Wang et al.",
      "startOffset" : 26,
      "endOffset" : 85
    }, {
      "referenceID" : 35,
      "context" : "Automatic Pitch Correction (APC) works attempt to minimize the manual effort in modifying the flawed singing voice (Yong and Nam, 2018).",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 42,
      "context" : "(2018) propose Canonical Time Warping (CTW) (Zhou and Torre, 2009; Zhou and De la Torre, 2012) which aligns amateur singing recordings to professional ones according to the pitch curves only.",
      "startOffset" : 44,
      "endOffset" : 94
    }, {
      "referenceID" : 44,
      "context" : "However, this method heavily relies on a reference recording, causing the tuned recording and the reference recording to be homogeneous in singing style (Zhuang et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 174
    }, {
      "referenceID" : 4,
      "context" : "As shown in Figure 2, to generate audio with high quality and learn the latent representations of vocal tone, we introduce a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014; Sohn et al., 2015) as the mel-spectrogram generator, with the optimizing objective of maximizing the evidence lower bound (ELBO) of the intractable marginal log-likelihood of melspectrogram log pθ(x|c):",
      "startOffset" : 168,
      "endOffset" : 213
    }, {
      "referenceID" : 25,
      "context" : "As shown in Figure 2, to generate audio with high quality and learn the latent representations of vocal tone, we introduce a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014; Sohn et al., 2015) as the mel-spectrogram generator, with the optimizing objective of maximizing the evidence lower bound (ELBO) of the intractable marginal log-likelihood of melspectrogram log pθ(x|c):",
      "startOffset" : 168,
      "endOffset" : 213
    }, {
      "referenceID" : 18,
      "context" : "Furthermore, to address the over-smoothing problem (Qian et al., 2019b) in CVAE, we utilize an adversarial discriminator (D) (Mao et al.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : ", 2019b) in CVAE, we utilize an adversarial discriminator (D) (Mao et al., 2017) to refine the output mel-spectrogram:",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "However, original DTW (Müller, 2007) could result in a poor alignment when certain parts of the axis move to higher frequencies, and other parts to lower ones, or vice versa (Sundermann and Ney, 2003).",
      "startOffset" : 22,
      "endOffset" : 36
    }, {
      "referenceID" : 26,
      "context" : "However, original DTW (Müller, 2007) could result in a poor alignment when certain parts of the axis move to higher frequencies, and other parts to lower ones, or vice versa (Sundermann and Ney, 2003).",
      "startOffset" : 174,
      "endOffset" : 200
    }, {
      "referenceID" : 42,
      "context" : "(2018) adopt an advanced algorithm CTW (Zhou and Torre, 2009), which combines the canonical correlation analysis (CCA) and DTW to extract the feature sequences of two pitch curves, and then apply DTW on them.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "Inspired by (Mori et al., 2005), we divide the data points around fi into m ∗ n bins by m time windows and n angles.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "The encoder of CVAE consists of a 1-D convolutional layer (stride=4), an 8-layer WaveNet structure (Oord et al., 2016; Rethage et al., 2018) and 3 1-D convolutional layers (stride=2) with ReLU activation function and batch normalization followed by a mean pooling, which outputs the mean and log scale standard deviation parameters in the posterior distribution of z.",
      "startOffset" : 99,
      "endOffset" : 140
    }, {
      "referenceID" : 20,
      "context" : "The encoder of CVAE consists of a 1-D convolutional layer (stride=4), an 8-layer WaveNet structure (Oord et al., 2016; Rethage et al., 2018) and 3 1-D convolutional layers (stride=2) with ReLU activation function and batch normalization followed by a mean pooling, which outputs the mean and log scale standard deviation parameters in the posterior distribution of z.",
      "startOffset" : 99,
      "endOffset" : 140
    }, {
      "referenceID" : 33,
      "context" : "The discriminator adopts the same structure as (Wu and Luan, 2020), which consists of multiple random window discriminators.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "In addition, given a singing recording, 1) to obtain its content vectors, we train an Automatic Speech Recognition (ASR) model based on Conformer (Gulati et al., 2020) with both speech and singing data, and extract the hidden states from the ASR encoder (viewed as the content encoder) output as the linguistic content information, which are also called phonetic posterior-grams (PPG); 2) to obtain the vocal timbre, we leverage the open-source API resemblyzer9 as the timbre encoder, which is a deep learning model designed for speaker verification (Wan et al.",
      "startOffset" : 146,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : ", 2020) with both speech and singing data, and extract the hidden states from the ASR encoder (viewed as the content encoder) output as the linguistic content information, which are also called phonetic posterior-grams (PPG); 2) to obtain the vocal timbre, we leverage the open-source API resemblyzer9 as the timbre encoder, which is a deep learning model designed for speaker verification (Wan et al., 2018), to extract the identity information of a singer.",
      "startOffset" : 390,
      "endOffset" : 408
    }, {
      "referenceID" : 37,
      "context" : "Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi, 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.",
      "startOffset" : 160,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : "The output melspectrograms of our model are transformed into audio samples using a HiFi-GAN vocoder (Kong et al., 2020) trained with singing data in advance.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "To obtain the ground truth pitch alignment between the amateur recordings and the professional ones for evaluating the accuracy of pitch alignment algorithm, we run the Montreal Forced Aligner tool (McAuliffe et al., 2017) on all the singing recordings to obtain their alignments to lyrics.",
      "startOffset" : 198,
      "endOffset" : 222
    }, {
      "referenceID" : 14,
      "context" : "Normed DTW means two pitch curves will be normalized before running DTW (Müller, 2007); CTW means the Canonical Time Warping (Zhou and Torre, 2009), which is used for pitch correction in Luo et al.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 42,
      "context" : "Normed DTW means two pitch curves will be normalized before running DTW (Müller, 2007); CTW means the Canonical Time Warping (Zhou and Torre, 2009), which is used for pitch correction in Luo et al.",
      "startOffset" : 125,
      "endOffset" : 147
    } ],
    "year" : 0,
    "abstractText" : "We are interested in a novel task, singing voice beautifying (SVB). Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre. Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality. Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone. In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve. Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one. To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions. Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_229_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Challenging America: Modeling language in longer time scales",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The dominant approach in the design of current NLP solutions consists in (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2018; Raffel et al., 2019). The solutions are evaluated on benchmarks such as GLUE ((Wang et al., 2018)) or SuperGLUE ((Wang et al., 2019)), which allow comparing the performance of various methods designed for the same purpose. A main feature of a good NLP benchmark is the clear separation between train and test sets. This requirement prevents data contamination, when the model (pre-)trained on huge data might have “seen” the test set.\nThe expansion of digital information is proceeding in two directions on the temporal axis. In the forward direction, new data are made publicly available on the Internet every second. What is less\nobvious is that, in the backward direction, older and older historical documents are digitized and disseminated publicly.\nTo the best of our knowledge, our paper introduces the first benchmark which serves to use and evaluate the “pre-train and fine-tune scenario” applied to a massive collection of historical texts.\nThe very idea of building language models on historical data is not new. The Google Ngram Viewer (Michel et al., 2011) is based on large amounts of texts from digitized books. The corpus as a whole is not open for the NLP community – only raw n-gram statistics are available. The temporal information is crude (at best, the year of publication is given) and the corpus is heterogeneous (in fact, it is a dump of digitized books of any origin).\nIn our research, we use one of the richest sources of homogeneous historical documents, Chronicling America, a collection of digitized newspapers that cover the publication period of over 300 years (with significant coverage of 150 years), and design an NLP benchmark that may open new opportunities for the modeling of the historical language.\nRecently, time-aware language models such as Temporal T5 (Dhingra et al., 2021) and TempoBERT (Rosin et al., 2021) have been proposed. They focus on modern texts dated yearly, whereas we extend language modeling towards both longer time scales and more fine-grained (daily) resolution, using massive amounts of historical texts.\nThe contribution of this paper is as follows:\n• We extracted a large corpus of English historical texts that may serve to pre-train historical language models (Section 5).\nThese are the main features of the corpus:\n– the corpus size is 201 GB, which is comparable with contemporary text data for\ntraining massive language models, such as GPT-2, RoBERTa or T5;\n– the corpus is free of spam and noisy data (although the quality of OCR processing varies);\n– texts are dated with a daily resolution, hence a new dimension of time (on a fine-grained level) can be introduced into language modeling;\n– the whole corpus is made publicly available;\n• Based on selected excerpts from Chronicling America, we define a suite of challenges (named Challanging America, or ChallAm in short) with three ML tasks combining layout recognition, information extraction and semantic inference (Section 7). We hope that ChallAm will give rise to a historical equivalent of the GLUE (Wang et al., 2018) or SuperGLUE (Wang et al., 2019) benchmarks.\n– In particular, we provide a tool for the intrinsic evaluation of language models based on a word-gap task, which calculates the model perplexity in a comparative scenario (the tool may be used in competitive shared-tasks) (Section 7.3).\n• We propose a “future-proof” methodology for the creation of NLP challenges: a challenge is automatically updated whenever the underlying corpus is enriched (Section 6.3).\n• We introduce a method for data preparation that prevents data contamination (Section 6.3).\n• We train base Transformer (RoBERTa) models for historical texts (Section 5). The models are trained on texts spanning 100 years, dated with a daily resolution.\n• We provide strong baselines for three ChronAm challenges (Section 8).\n• We take under consideration the issue of discrimination and hate speech in the historical American texts. To this end we have applied up-to date methods to filter out the abusive content from the data (Section 9)."
    }, {
      "heading" : "2 Chronicling America",
      "text" : "In 2005 a partnership between the National Endowment for the Humanities and the Library of\nCongress launched the National Digital Newspaper Program, to develop a database of digitized documents with easy access. The result of this 15-year effort is Chronicling America – a website1 which provides access to selected digitized newspapers, published from 1690 to the present. The collection includes approximately 140 000 bibliographic title entries and 600 000 library holdings records, converted to the MARCXML format. The portal supports an API which allows accessing of the data in various ways, such as the JSON format, BulkData (bulk access to data) or Linked Data,2 or searching of the database with the OpenSearch protocol.3. The accessibility of data in various forms makes Chronicling America a valuable source for the creation of datasets and benchmarks.\nThe portal serves as a resource for various research activities. Cultural historians may track performances and events of their interest in a resource which is easily and openly accessible, as opposed to commercial databases or “relatively small collections of cultural heritage organizations whose online resources are isolated and difficult to search” (Clark, 2014). The database enables searching for the first historical usages of word terms. For instance, thanks to the Chronicling America portal, it was discovered in (Cibaroğlu, 2019) that the term “fake news” was first used in 1889 in the Polish newspaper Ameryka.\nThe resource is helpful in research aiming to improve the output of the OCR process. The authors of (Nguyen et al., 2019) study OCR errors occurring in several digital databases – including Chronicling America – and compare them with human-generated misspellings. The research results in several suggestions for the design of OCR post-processing methods. The implementation of an unsupervised approach in the correction of OCR documents is described in (Dong and Smith, 2018). Two million issues from the Chronicling America collection of historic U.S. newspapers are used in a sequence-to-sequence model with attention.\nChronicling America is a type of digitized resource that may be of wide use for both humanities and computational research. We prepared datasets and challenges based on the data from the Chronicling America resource. We hope that our initiative will bring about research that will facilitate the\n1https://chroniclingamerica.loc.gov 2https://www.w3.org/standards/semanticweb/data 3https://opensearch.org/\ndevelopment of ML-based processing tools, and consequently increase access to digitized resources for the humanities.\nAn example of an ML tool based on Chronicling America is described in (Lee et al., 2020). The task consisted in predicting bounding boxes around various types of visual content: photographs, illustrations, comics, editorial cartoons, maps, headlines and advertisements. The training set was crowdsourced and included over 48K bounding boxes for seven classes. Using a pre-trained Faster-RCNN detection object, the researchers achieved an average accuracy of 63.4%. Both the training set and the model weights file are publicly available. Still, it is difficult to estimate the value of the results achieved without any comparison with other models trained on the same data.\nIn our proposal we go a step further. We provide and make available training data from Chronicling America for three ML tasks. For each task we develop and share baseline solutions. Alternative solutions can be submitted to an evaluation platform to be evaluated automatically and compares against the baselines."
    }, {
      "heading" : "3 Similar Machine Learning datasets and challenges",
      "text" : "This section concerns ML challenges which deliver labeled OCR documents as training data, a definition of the processing task, and an evaluation environment to estimate the performance of uploaded solutions. More often than not, such challenges concern either layout recognition (localization of layout elements) or Key Information Extraction (finding, in a document, precisely specified business-actionable pieces of information). Layout recognition in Japanese historical texts is described in (Shen et al., 2020). The authors use deep learning-based approaches to detect seven types of layout element categories: Page Frame, Text Region, Text Row, Title Region, etc. Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021). The two datasets described there contain, respectively, NDA documents and financial reports from charity organizations. The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending. The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the\nKIE research area. A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) – the challenge is opened for the recognition of OCR-scanned receipts. In this competition (named ICDAR2019) three tasks are set up: Scanned Receipt Text Localization, Scanned Receipt OCR, and Key Information Extraction from Scanned Receipts.\nA common feature of the above-mentioned challenges is the goal of retrieving information that is explicit in the data (a text fragment or layout coordinates). Our tasks in ChallAm go a step further: the goal is to infer the information from the OCR image rather than just retrieve it.\nSimilar challenges for two out of the three tasks introduced in this paper have been proposed before for the Polish language:\n• a challenge for temporal identification (Graliński and Wierzchoń, 2018); the challenge was based on a set of texts coming from Polish digital libraries, dated between the years 1814 and 2013;\n• a challenge for “filling the gap” (RetroGap) (Graliński, 2017) with the same training set as above.\nThe training sets for those challenges were purely textual. Here, we introduce the challenges with the addition of original images (clippings), though we do not use graphical features in baselines yet."
    }, {
      "heading" : "4 Data processing",
      "text" : "The PDF files were downloaded from Chronicling America and processed using a pipeline primarily developed for extracting texts from Polish digital libraries (Graliński, 2013, 2019). Firstly, the metadata (including URL addresses for PDF files) were extracted by a custom web crawler and then normalized; for instance, titles were normalized using regular expressions (e.g. The Bismarck tribune. [volume], May 31, 1921 was normalized to THE BISMARCK TRIBUNE). Secondly, the PDF files were downloaded and the English texts were processed into DjVu files (as this is the target format for the pipeline) using the pdf2dvju tool4. The original OCR text layer was retained (the files were not re-OCRed, even though, in some cases, the quality of OCR was low).\n4http://jwilk.net/software/pdf2djvu\nTable 1 shows a summary of the data obtained at each processing step. Two factors were responsible for the fact that not 100% of files were retained at each phase: (1) issues in the processing procedures (e.g. download failures due to random network problems or errors in the PDF-to-DjVu procedure that might be handled later); (2) some files are simply yet to be finally processed in the ongoing procedure.\nThe procedure is executed in a continuous manner to allow the future processing of new files that are yet to be digitized and made public by the Chronicling America initiative. This solution requires a future-proof procedure for splitting and preparing data for machine-learning challenges. For instance, the assignment of documents to the training, development and test sets should not change when the raw data set is expanded. Such a procedure is described in Section 6."
    }, {
      "heading" : "5 Data for unsupervised training",
      "text" : "The state of the art in most NLP tasks is obtained by training a neural-network language model on a large collection of texts in an unsupervised manner and fine-tuning the model on a given downstream task. At present, the most popular architectures for language models are Transformer (Devlin et al., 2018) models (earlier, e.g. Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al., 2017)). The data on which such models are trained are almost always modern Internet texts. The high volume of texts available at Chronicling America, on the other hand, makes it possible to train large Transformer models for historical texts.\nUsing a pre-trained language model on a downstream task bears the risk of data contamination – the model might have been trained on the task test set and this might give it an unfair edge (see (Brown et al., 2020) for a study of data contamination in the case of the GPT-3 model when used for popular English NLP test sets). This issue should be taken into account from the very beginning. In our case, we release a dump of all Chronicling\nAmerica texts (for pre-training language models), but limited only to the 50% of texts that would be assigned to the training set (according to the MD5 hash). This dump contains all the texts, not just the excerpts described in Section 6.2. As the size of the dump is 74.0G characters, it is on par with the text material used to train, for instance, the GPT-2 model.\nWe also release a RoBERTa Base ChallAm model trained on the text corpus. The model was trained from scratch, i.e. it was not based on the weights of the original RoBERTa model (Liu et al., 2019). The BPE dictionary was also induced anew.\nTwo versions of the RoBERTa ChallAm model were prepared: one was trained with temporal metadata encoded as a prefix of the form year: YYYY, month: MM, day: DD, weekday: WD, another, for comparison, without such a prefix. The ChallAm models have the same numbers of parameters as the original RoBERTa Base (125M). Each model was trained on two Tesla V100 32GB GPUs for 9 days."
    }, {
      "heading" : "6 Procedure for preparing challenges",
      "text" : "We created a pipeline that can generate various machine learning challenges. The pipeline input should consist of DjVu image files, text (OCR image), and metadata. Our main goals are to keep a clear distinction between dataset splits and to assure the reproducibility of the pipeline. This allows potential improvement to current challenges and the generation of new challenges without dataset leaks in the future. We achieved this by employing stable pseudo-randomness by calculating an MD5 hash on a given ID and taking the modulo remainder from integers from certain preset intervals. These pseudo-random assignments are not dependent on any library, platform, or programming language (using a fixed seed for the pseudo-random generator might not give the same guarantees as using MD5 hashes), so they are easy to reproduce.\nThis procedure is crucial to make sure that challenges are future-proof, i.e.:\n• when the challenges are re-generated on the same Chronicling America files, exactly the same results are obtained (including text and image excerpts; see Section 6.2);\n• when the challenges are re-generated on a larger set of files (e.g. when new files are digitized for the Chronicling America project),\nthe assignments of existing items to the train/dev/test sets will not change."
    }, {
      "heading" : "6.1 Dataset structure",
      "text" : "All three of our machine learning challenges consist of training (train), development (dev), and test sets. Each document in each set consists of excerpts from a newspaper edition. One newspaper edition provides a maximum of one excerpt. Excerpts in the datasets are available as both a cropped PNG file from the newspaper scan (a “clipping”) and its OCR text. This makes it possible to employ image features in machine learning models (e.g. font features, paper quality). A solution might even disregard the existing OCR text layer and re-OCR the clipping or just employ an end-to-end model. (The OCR layer is given as it is, with no manual correction done – this is to simulate realistic conditions in which a downstream task is to be performed without a perfect text layer.)\nSometimes additional metadata are given. For the train and dev datasets, we provide the expected data. For the test dataset, the expected data are not released. These data are used by the evaluation platform during submission evaluation. All newspaper and edition IDs are encoded to prevent participants from checking the newspaper edition in the Chronicling America database. The train and dev data may consist of all documents which meet our criteria for text excerpts, so the data may be unbalanced with respect to publishing years and locations. We tried to balance the test sets as regards the years of publication (the year-prediction and word-gap challenges) or locations (the geo-prediction challenge), though it is not always possible due to large imbalances in the original material."
    }, {
      "heading" : "6.2 Selecting text excerpts",
      "text" : "The details of the procedure for selection of text excerpts is given in Appendix A. A sample excerpt is shown in Figure 1a. Note that excerpts are selected using a stable pseudo-random procedure based on the newspaper edition ID (similarly to the way the train/dev/test split is done, see Section 6.3)."
    }, {
      "heading" : "6.3 Train/dev/test split",
      "text" : "Each newspaper has its newspaper ID (i.e. normalized title, as described in Section ), and each newspaper edition has its newspaper edition ID. We separate newspapers within datasets, so for instance, if one newspaper edition is assigned to the dev set, all editions of that newspaper are assigned to the\ndev set. All challenges share common train and dev datasets and no challenges share the same test set. This prevents one from checking expected data from other challenges. The set splits are as follows: 50% for train, 10% for dev, 5% for each challenge test set. This makes it possible to generate eight challenges with different test sets. In other words, there is room for another five challenges in the future (again this is consistent with the “future-proof” principle of the whole endeavor)."
    }, {
      "heading" : "7 Challenging America tasks",
      "text" : "In this section, we describe the three tasks defined in the challenge. They are released on an evaluation platform, which enables the calculation of metrics both offline and online, as well as the submission of solutions. An example of text from an excerpt given in those tasks is shown in Figure 1b."
    }, {
      "heading" : "7.1 RetroTemp",
      "text" : "This is a temporal classification task. Given a normalized newspaper title and a text excerpt, the task is to predict the publishing date. The date should be given in fractional year format (e.g. 1 June 1918 is represented as the number 1918.4137, and 31 December 1870 as 1870.9973).\nHence, solutions to the challenge should predict the publication date with the greatest precision possible (i.e. day if possible). The fractional format will make it easy to accommodate even more precise timestamps, for example, if modern Internet texts (e.g. tweets) are to be added to the dataset.\nDue to the regression nature of the problem, the evaluation metric is RMSE (root mean square error).\nThe motivation behind the RetroTemp challenge is to design tools that may help supplement the missing metadata for historical texts (the older the document, the more often it is not labeled with a time stamp). Even if all documents in a collection are time-stamped, such tools may be useful for finding errors and anomalies in metadata."
    }, {
      "heading" : "7.2 RetroGeo",
      "text" : "The task is to predict the place where the newspaper was published, given a normalized newspaper title, text excerpt, and publishing date in fractional year format. The expected format is a latitude and longitude. In the evaluation the distance on the sphere between output and expected data is calculated using the haversine formula, and the mean\nvalue of errors is reported. The motivation for the task (besides the supplementation of missing or wrong data) is to allow research on news propagation. Even if a news article is labeled with the localization of its issue, an automatic tool may infer that it was originally published somewhere else."
    }, {
      "heading" : "7.3 RetroGap",
      "text" : "This is a task for language modeling. The middle word of an excerpt is removed in the input document (in both text and image), and the task is to predict the removed word, given the normalized newspaper title, the text excerpt, and the publishing date in fractional year format (in other words, it is a cloze task). The output should contain a probability distribution for the removed word (not just a word or a single probability). The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool (Graliński et al., 2019), the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.e. avoid self-reported probabilities and ensure objective comparison of all reported solutions, including out-of-vocabulary words)."
    }, {
      "heading" : "7.4 Statistics",
      "text" : "The data consists of the text excerpts written between the years 1798 and 1963. The mean publi-\ncation year of the text excerpts is 1891. Excerpts between the years 1833 and 1925 make up about 96% of the data in the train set (cf. Figure 2a), but only 85% in the dev and test sets, which are more uniform (due to balancing described in Section 6.3, cf. Figure 2c). There are 432 000 excerpts in the train set, 10 500 in the dev set and 8 500 in the test set. These numbers are consistent across the challenges. The average excerpt length is 1 745 characters with 323.8 words, each one containing from 150 words up to 583 words.\nThe length of each text in the excerpts seems to have a negative correlation with publication date – the later the text was published, the shorter snippet text (on average) it contains (see Figure 2b and 2d)."
    }, {
      "heading" : "8 Baselines",
      "text" : "Baselines for all three tasks are available at the evaluation platform.5 The baselines (see Tables 2 and 3) include, for each model, its score in the appropriate metric as well as the Git SHA1 reference code (in curly brackets).6\nWe distinguish between self-contained submissions, which use only data provided in the task, and non-self-contained submissions, which use external data, e.g. publicly available pre-trained transform-\n5To be revealed after review 6The outputs and some of the scripts used are available in supplementary materials, later to be revealed at the evaluation platform.\ners. Our baselines take into account only textual features.\nMore detailed analysis of the baseline performance is given in Appendix C. The current top performing models have the most difficulty with texts which (1) are older, (2) contain OCR noise, (3) come from less popular locations (especially, in the west)."
    }, {
      "heading" : "8.1 RetroTemp and RetroGeo",
      "text" : "The baseline solutions for RetroTemp and RetroGeo were prepared similarly. RetroGeo requires two values (latitude and longitude) – we treat them separately and train two separate models for them.\nFor the self-contained models we provide the mean value from the train test, the linear regression based on TF-IDF and the BiLSTM (bidirectional long short-term memory) method.\nFor non-self-contained submissions, we incorporate RoBERTa (Liu et al., 2019) models released in two versions: base (125M params) and large (355M params). The output features are averaged, and the linear layer is added on top of this. Both RoBERTa and the linear layer were fine-tuned during training.\nThe best self-contained models are BiLSTM submissions in both tasks. Non-self-contained submissions result in much higher scores than self-contained models. In both tasks, RoBERTa-\nlarge with linear layer provides better results than RoBERTa-base.\nFor the RetroTemp challenge we also provide results obtained with the RoBERTa model pretrained from scratch (see Section 5). Even though the model without time-related prefix was used, the results are significantly better than the original RoBERTa Base: the confidence intervals obtained with bootstrap sampling are, respectively, 10.81±0.21 and 12.10±0.22 (single runs are reported).\nHyperparameter setup is described in Appendix B."
    }, {
      "heading" : "8.2 RetroGap",
      "text" : "For non-self-contained submissions, we applied RoBERTa in base and large version without any fine-tuning. Since standard RoBERTa training does not incorporate any data, but text, we didn’t include temporal metadata during inference.\nFor self-contained submissions, we applied RoBERTa Challam base both in version with a date and without a date.\nRoBERTa ChallAm base with date is better than RoBERTa ChallAm base without date. This means the incorporation of temporal metadata has a positive impact on MLM task. Both self-contained submissions are better than the standard RoBERTa base, so our models trained on historical data per-\nforms better than model trained on regular data if the same base model size is considered. Since we didn’t train RoBERTa ChallAm large, we can’t confirm this holds true, when it comes to large RoBERTa models. The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain."
    }, {
      "heading" : "9 Ethical issues",
      "text" : "We share the data from Chronicling America, following the statement of the Library of Congress: “The Library of Congress believes that the newspapers in Chronicling America are in the public domain or have no known copyright restrictions.”7\nHistorical texts from American newspapers may be discriminatory, either explicitly or implicitly, particularly regarding race and gender. Recent years have seen research on the detection of discriminatory texts. In (Xia et al., 2020) adversarial training is used to mitigate racial bias. In (Field and Tsvetkov, 2020) the authors “take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias.” The most recent experiments on the topic ((Caselli et al., 2021), (Aluru et al., 2020)) result in re-trained BERT mod-\n7https://chroniclingamerica.loc.gov/about\nels for abusive language detection in English. We use one of them, DeHateBERT (Aluru et al., 2020), to filter out the abusive texts in the ChallAm dataset. We filtered out items that either (1) are marked as abusive speech by DeHateBERT with the probability greater than 0.75 or (2) contain words from a list of blocked words. The fraction of filtered out texts was 2.04-2.40% (depending on the challenge and set)."
    }, {
      "heading" : "10 Conclusions",
      "text" : "This paper has introduced a challenge based on OCR excerpts from the Chronicling America portal. The challenge consists of three tasks: guessing the publication date, guessing the publication location, and filling a gap with a word. We propose baseline solutions for all three tasks.\nChronicling America is an ongoing project, as we define our challenge in such a way that it can easily evolve in parallel with the development of Chronicling America. Firstly, any new materials appearing on the portal can be automatically incorporated into our challenge. Secondly, the challenge is open for five yet undefined ML tasks."
    }, {
      "heading" : "A Procedure for selecting text excerpts",
      "text" : "The OCR text follows the newspaper layout, which is defined by the following entities: page, column, line. Each entity has x0, y0, x1, y1 coordinates of text in the DjVu document. Still, various errors may occur in the OCR newspaper layout (e.g. two columns may be split into one). We intend to select only excerpts which preserve the correct output. To this end, we select only excerpts that fulfill the following conditions:\n1. There are between 150 and 600 text tokens in the excerpt. The tokens are words separated by whitespaces.\n2. The y coordinates of each line are below the y coordinates of the previous line.\n3. The x0 coordinate of each line does not differ by more than 15% from the x0 coordinate of the previous line.\n4. The x1 coordinate is not shifted to the right more than 15% from the x1 coordinate of the previous line.\nIf the newspaper edition contains no such excerpts, we reject it. If there is more than one such excerpt, we select one excerpt using a stable\npseudo-random procedure based on the newspaper edition ID.\nThis procedure produces text excerpts with images consisting of OCR texts only. The excerpts are downsized to reduce the size to an appropriate degree to maintain good quality. We do not pre-process images in any other way, so excerpts may have different sizes, height-to-width ratios, and colors."
    }, {
      "heading" : "B Hyperparameter setup",
      "text" : "Hyperparameters were determined on the development set, training on a limited number of examples. In particular, for fine-tuning RoBERTa models the following hyperparameters were used:\n• optimizer: AdamW\n• learning rate: 0.000001\n• batch size: 4\n• early-stopping patience: 3\n• warm-up steps: 10000"
    }, {
      "heading" : "C Analysis of the best baselines",
      "text" : "See Table 4 and 5 for the list of top 30 features correlating most with, respectively, the worst and bad results in ChallAm challenges (as returned by the GEval tool with the option -worst-features -numerical-features (Graliński et al., 2019)). The features are tokens within the input (in:), expected output (exp:) and the actual output (out:), or numerical features such as high/low value (:=+/:=-) or length/shortness of a text (:+#/:-#).\nAs can be seen the bottleneck for the current best model is due to:\n• old texts (:=- in RetroTemp),\n• OCR noise (cf. short words such ni, ol, j or punctuation marks likely to be introduced by OCR misrecognitions),\n• less popular publication locations (especially far west).\nObviously, year references (1902, 1904) make it easy to guess the publication texts (in RetroTemp), whereas in RetroGap some non-content words such as the, and, of are easy to guess for the language model (even if their garbaged form, e.g. ot, ol, needs to be accounted for in the probability distribution)."
    } ],
    "references" : [ {
      "title" : "Deep learning models",
      "author" : [ "Sai Saket Aluru", "Binny Mathew", "Punyajoy Saha", "Animesh Mukherjee" ],
      "venue" : null,
      "citeRegEx" : "Aluru et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Aluru et al\\.",
      "year" : 2020
    }, {
      "title" : "HateBERT: Retraining BERT for abusive language detection in english",
      "author" : [ "Tommaso Caselli", "Valerio Basile", "Jelena Mitrović", "Michael Granitzer" ],
      "venue" : null,
      "citeRegEx" : "Caselli et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Caselli et al\\.",
      "year" : 2021
    }, {
      "title" : "Post-truth in social media",
      "author" : [ "Mehmet Cibaroğlu." ],
      "venue" : "6:87–99.",
      "citeRegEx" : "Cibaroğlu.,? 2019",
      "shortCiteRegEx" : "Cibaroğlu.",
      "year" : 2019
    }, {
      "title" : "A survey of online digital newspaper and genealogy archives: Resources, cost, and access",
      "author" : [ "Maribeth Clark." ],
      "venue" : "Journal of the Society for American Music, 8:277–283.",
      "citeRegEx" : "Clark.,? 2014",
      "shortCiteRegEx" : "Clark.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Time-aware language models as temporal knowledge bases",
      "author" : [ "Bhuwan Dhingra", "Jeremy R. Cole", "Julian Martin Eisenschlos", "Daniel Gillick", "Jacob Eisenstein", "William W. Cohen" ],
      "venue" : null,
      "citeRegEx" : "Dhingra et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-input attention for unsupervised OCR correction",
      "author" : [ "Rui Dong", "David Smith." ],
      "venue" : "pages 2363–2372.",
      "citeRegEx" : "Dong and Smith.,? 2018",
      "shortCiteRegEx" : "Dong and Smith.",
      "year" : 2018
    }, {
      "title" : "Unsupervised discovery of implicit gender bias",
      "author" : [ "Anjalie Field", "Yulia Tsvetkov." ],
      "venue" : "CoRR, abs/2004.08361.",
      "citeRegEx" : "Field and Tsvetkov.,? 2020",
      "shortCiteRegEx" : "Field and Tsvetkov.",
      "year" : 2020
    }, {
      "title" : "RetroC—A Corpus for Evaluating Temporal Classifiers",
      "author" : [ "Filip Graliński", "Piotr Wierzchoń." ],
      "venue" : "Human Language Technology. Challenges for Computer Science and Linguistics. 7th Language and Technology Conference, LTC 2015, pages 101–111. Springer.",
      "citeRegEx" : "Graliński and Wierzchoń.,? 2018",
      "shortCiteRegEx" : "Graliński and Wierzchoń.",
      "year" : 2018
    }, {
      "title" : "GEval: Tool for debugging NLP datasets and models",
      "author" : [ "Filip Graliński", "Anna Wróblewska", "Tomasz Stanisławek", "Kamil Grabowski", "Tomasz Górecki." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural",
      "citeRegEx" : "Graliński et al\\.,? 2019",
      "shortCiteRegEx" : "Graliński et al\\.",
      "year" : 2019
    }, {
      "title" : "Polish digital libraries as a text corpus",
      "author" : [ "Filip Graliński." ],
      "venue" : "Proceedings of 6th Language & Technology Conference, pages 509–513, Poznań. Fundacja Uniwersytetu im. Adama Mickiewicza.",
      "citeRegEx" : "Graliński.,? 2013",
      "shortCiteRegEx" : "Graliński.",
      "year" : 2013
    }, {
      "title" : "Temporal) language models as a competitive challenge",
      "author" : [ "Filip Graliński." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Graliński.,? 2017",
      "shortCiteRegEx" : "Graliński.",
      "year" : 2017
    }, {
      "title" : "Against the Arrow of Time",
      "author" : [ "Filip Graliński." ],
      "venue" : "Theory and Practice of Mining Massive Corpora of Polish Historical Texts for Linguistic and Historical Research. Wydawnictwo Naukowe UAM, Poznań.",
      "citeRegEx" : "Graliński.,? 2019",
      "shortCiteRegEx" : "Graliński.",
      "year" : 2019
    }, {
      "title" : "ICDAR2019 competition on scanned receipt OCR and information extraction",
      "author" : [ "Zheng Huang", "Kai Chen", "Jianhua He", "Xiang Bai", "Dimosthenis Karatzas", "Shijian Lu", "C.V. Jawahar." ],
      "venue" : "2019 International Conference on Document Analysis and Recognition",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "The newspaper navigator dataset: Extracting and analyzing visual content from 16 million historic newspaper",
      "author" : [ "Benjamin Lee", "Jaime Mears", "Eileen Jakeway", "Meghan Ferriter", "Chris Adams", "Nathan Yarasavage", "Deborah Thomas", "Kate Zwaard", "Daniel Weld" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Quantitative analysis of culture using millions of digitized books",
      "author" : [ "Jean-Baptiste Michel", "Yuan Kui Shen", "Aviva Presser Aiden", "Adrian Veres", "Matthew K Gray", "Joseph P Pickett", "Dale Hoiberg", "Dan Clancy", "Peter Norvig", "Jon Orwant" ],
      "venue" : null,
      "citeRegEx" : "Michel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep statistical analysis of OCR errors for effective post-OCR processing",
      "author" : [ "Thi-Tuyet-Hai Nguyen", "Adam Jatowt", "Mickael Coustaty", "Nhu-Van Nguyen", "Antoine Doucet." ],
      "venue" : "Proceedings of the 18th Joint Conference on Digital Libraries, JCDL ’19,",
      "citeRegEx" : "Nguyen et al\\.,? 2019",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised sequence tagging with bidirectional language models",
      "author" : [ "Matthew E Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power." ],
      "venue" : "arXiv preprint arXiv:1705.00108.",
      "citeRegEx" : "Peters et al\\.,? 2017",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2017
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Time masking for temporal language models",
      "author" : [ "Guy D. Rosin", "Ido Guy", "Kira Radinsky" ],
      "venue" : null,
      "citeRegEx" : "Rosin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Rosin et al\\.",
      "year" : 2021
    }, {
      "title" : "A large dataset of historical Japanese documents with complex layouts",
      "author" : [ "Zejiang Shen", "Kaixuan Zhang", "Melissa Dell." ],
      "venue" : "CoRR, abs/2004.08686.",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "Kleister: Key information extraction datasets involving long documents with complex layouts",
      "author" : [ "Tomasz Stanisławek", "Filip Graliński", "Anna Wróblewska", "Dawid Lipiński", "Agnieszka Kaliska", "Paulina Rosalska", "Bartosz Topolski", "Przemysław Biecek." ],
      "venue" : "Docu-",
      "citeRegEx" : "Stanisławek et al\\.,? 2021",
      "shortCiteRegEx" : "Stanisławek et al\\.",
      "year" : 2021
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "arXiv preprint 1905.00537.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Demoting racial bias in hate speech detection",
      "author" : [ "Mengzhou Xia", "Anjalie Field", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, pages 7–14, Online. Association for Computational",
      "citeRegEx" : "Xia et al\\.,? 2020",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The dominant approach in the design of current NLP solutions consists in (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2018; Raffel et al., 2019).",
      "startOffset" : 239,
      "endOffset" : 281
    }, {
      "referenceID" : 20,
      "context" : "The dominant approach in the design of current NLP solutions consists in (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2018; Raffel et al., 2019).",
      "startOffset" : 239,
      "endOffset" : 281
    }, {
      "referenceID" : 25,
      "context" : "The solutions are evaluated on benchmarks such as GLUE ((Wang et al., 2018)) or SuperGLUE ((Wang et al.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : ", 2018)) or SuperGLUE ((Wang et al., 2019)), which allow comparing the performance of various methods designed for the same purpose.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "The Google Ngram Viewer (Michel et al., 2011) is based on large amounts of texts from digitized books.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "Recently, time-aware language models such as Temporal T5 (Dhingra et al., 2021) and TempoBERT (Rosin et al.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : ", 2021) and TempoBERT (Rosin et al., 2021) have been proposed.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 25,
      "context" : "We hope that ChallAm will give rise to a historical equivalent of the GLUE (Wang et al., 2018) or SuperGLUE (Wang et al.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "Cultural historians may track performances and events of their interest in a resource which is easily and openly accessible, as opposed to commercial databases or “relatively small collections of cultural heritage organizations whose online resources are isolated and difficult to search” (Clark, 2014).",
      "startOffset" : 289,
      "endOffset" : 302
    }, {
      "referenceID" : 2,
      "context" : "For instance, thanks to the Chronicling America portal, it was discovered in (Cibaroğlu, 2019) that the term “fake news” was first used in 1889 in the Polish newspaper Ameryka.",
      "startOffset" : 77,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "The authors of (Nguyen et al., 2019) study OCR errors occurring in several digital databases – including Chronicling America – and compare them with human-generated misspellings.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "The implementation of an unsupervised approach in the correction of OCR documents is described in (Dong and Smith, 2018).",
      "startOffset" : 98,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "An example of an ML tool based on Chronicling America is described in (Lee et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : "Layout recognition in Japanese historical texts is described in (Shen et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021).",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) – the challenge is opened for the recognition of OCR-scanned receipts.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "• a challenge for temporal identification (Graliński and Wierzchoń, 2018); the challenge was based on a set of texts coming from Polish digital libraries, dated between the years 1814 and 2013;",
      "startOffset" : 42,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "• a challenge for “filling the gap” (RetroGap) (Graliński, 2017) with the same training set as above.",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "At present, the most popular architectures for language models are Transformer (Devlin et al., 2018) models (earlier, e.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "it was not based on the weights of the original RoBERTa model (Liu et al., 2019).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool (Graliński et al., 2019), the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.",
      "startOffset" : 103,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : ", 2019), the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "For non-self-contained submissions, we incorporate RoBERTa (Liu et al., 2019) models released in two versions: base (125M params) and large (355M params).",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : "In (Xia et al., 2020) adversarial training is used to mitigate racial bias.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "In (Field and Tsvetkov, 2020) the authors “take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias.",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "” The most recent experiments on the topic ((Caselli et al., 2021), (Aluru et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : ", 2021), (Aluru et al., 2020)) result in re-trained BERT mod-",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "We use one of them, DeHateBERT (Aluru et al., 2020), to filter out the abusive texts in the ChallAm dataset.",
      "startOffset" : 31,
      "endOffset" : 51
    } ],
    "year" : 0,
    "abstractText" : "The aim of the paper is to apply, for historical texts, the methodology used commonly to solve various NLP tasks defined for contemporary data, i.e. pre-train and fine-tune large Transformer models. This paper introduces an ML challenge, named Challenging America (ChallAm), based on OCR-ed excerpts from historical newspapers collected from the Chronicling America portal. ChallAm provides a dataset of clippings, labeled with metadata on their origin, and paired with their textual contents retrieved by an OCR tool. Three, publicly available, ML tasks are defined in the challenge: to determine the article date, to detect the location of the issue, and to deduce a word in a text gap (cloze test). Strong baselines are provided for all three ChallAm tasks. In particular, we pretrained a RoBERTa model from scratch from the historical texts. We also discuss the issues of discrimination and hate-speech present in the historical American texts.",
    "creator" : null
  }
}
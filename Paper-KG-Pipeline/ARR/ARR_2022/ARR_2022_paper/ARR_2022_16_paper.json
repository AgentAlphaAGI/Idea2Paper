{
  "name" : "ARR_2022_16_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improving Robustness of Language Models from a Geometry-aware Perspective",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks (DNNs) outperform humans on many natural language processing (NLP) tasks (Kim, 2014; Vaswani et al., 2017; Devlin et al., 2019). However, recent studies have shown that DNNs are vulnerable to crafted adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014). For instance, an attacker can mislead an online sentiment analysis system by making minor changes to the input sentences (Papernot et al., 2016; Liang et al., 2017). It has raised concerns among researchers about the security of DNN-based NLP systems. As a result, a growing number of studies are focusing on enhancing robustness to defend against textual adversarial attacks (Jia et al., 2019; Ye et al., 2020; Jones et al., 2020; Zhu et al., 2020).\nExisting adversarial defense methods fall into two categories: empirical and certified defenses.\nEmpirical defenses include gradient-based adversarial training (AT) and discrete adversarial data augmentation (ADA). Certified defenses provide a provable guaranteed robustness boundary for NLP models. This work focuses on empirical defenses.\nThere was a common belief that gradient-based AT methods in NLP was ineffective compared with ADA in defending against textual adversarial attacks (Li and Qiu, 2021; Si et al., 2021). Li et al. (2021) find that removing the norm-bounded projection and increasing the number of search steps in adversarial training can significantly improve robustness. Nonetheless, we observe that increasing the number of search steps further does not significantly improve robustness but hurts accuracy.\nWe give a possible explanation from a geometryaware perspective. Removing the norm-bounded projection enlarge the search space. Appropriately increasing the number of search steps brings the adversarial data closer to the decision boundary. In\nthis case, the model learns a robust decision boundary. Further increasing the number of search steps can make the adversarial data cross the decision boundary too far, hindering the training of natural data and hurting natural accuracy.\nTo verify our hypothesis, we train a base model using adversarial data, which are generated by adversarial word substitution (AWS) on the SST-2 (Socher et al., 2013) dataset. We report its training accuracy (“ada training acc”) on adversarial data and test accuracy (“ada test acc”) on the clean test set in Figure 1. Although achieving nearly 100% training accuracy, its test accuracy is only about 15%, which implicates the adversarial data make the test performance degraded. Then we train another base model, whose training data is more “friendly”. We just recover their last modified words to return to the correct class, namely friendly adversarial data augmentation (FADA). It means that only one word is different in each sentence. Surprisingly, it achieves a high test accuracy of ∼93%.\nThis preliminary inspired us to address two existing problems:\n• The number of search steps is always large, which is computationally inefficient.\n• A too large number of steps leads to degraded test performance.\nGeometrically speaking, the friendly adversarial data are close to the ideal decision boundary. We can address the above two issues in one fell swoop if we perform gradient-based adversarial training on these friendly adversarial data. It is\nlike we start one step before the end, allowing us to obtain strong robustness through a tiny number of search steps. We name it geometry-aware adversarial training (GAT). Figure 2 illustrates our proposed GAT.\nIn addition, the friendly adversarial data only need to be generated once per dataset. It can be reused, so it is computationally efficient. It can also be updated for every iteration or epoch but computationally expensive.\nOur contributions are summarized as follows:\n1) We propose FADA to generate friendly adversarial data which are close to the decision boundary (but not crossing it).\n2) We propose GAT, a geometry-aware adversarial training method that adds FADA to the training set and performs gradient-based adversarial training.\n3) GAT is computationally efficient, and it outperforms state-of-the-art baselines even if using the simplest FGM. We further provide extensive ablation studies and in-depth analyses on GAT, contributing to a better understanding of robustness."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Standard Adversarial Training",
      "text" : "Let fθ(x) be our neural network, L(fθ(x), y) be the loss function (e.g., cross entropy), where x ∈ X is the input data and y ∈ Y is the true label. The learning objective of standard adversarial training is\nmin θ E(X,Y )∼D [ max ∥δ∥≤ϵ L(fθ(X + δ), y) ] , (1)\nwhere D is the data distribution, δ is the minor perturbation, ϵ is the allowed perturbation size. To optimize the intractable min-max problem, we search for the optimal δ to maximize the inner loss and then minimize the outer loss w.r.t the parameters θ, step by step.\nThe gradient g of the inner loss w.r.t the input x is used to find the optimal perturbation δ. Goodfellow et al. (2014) proposed fast gradient sign method (FGSM) to obtain δ by one step:\nδ = ϵ · sgn(g), (2)\nwhere sgn(·) is the signum function. Madry et al. (2018) proposed projected gradient descent (PGD)\nto solve the inner maximization as follows:\nδ(t+1) = Π α · g(t)/∥g(t)∥, ∀t ≥ 0, (3)\nwhere α > 0 is the step size (i.e., adversarial learning rate), Π is the projection function that projects the perturbation onto the ϵ-norm ball. Conventionally PGD stops after a predefined number of search steps K, namely PGD-K. In addition, TRADES (Zhang et al., 2019), MART (Wang et al., 2020) and FAT (Zhang et al., 2020) are also effective adversarial training methods for boosting model robustness.\nRegarding FAT, the authors propose to stop adversarial training in a predefined number of steps after crossing the decision boundary, which is a little different from our definition of “friendly”."
    }, {
      "heading" : "2.2 Adversarial Training in NLP",
      "text" : "Gradient-based adversarial training has significantly improved model robustness in vision, while researchers find it helps generalization in NLP. Miyato et al. (2016) find that adversarial and virtual adversarial training have good regularization performance. Sato et al. (2018) propose an interpretable adversarial training method that generates reasonable adversarial texts in the embedding space and enhance models’ performance. Zhu et al. (2020) develop FreeLB to improve natural language understanding.\nThere is also a lot of work focused on robustness. Wang et al. (2021) improve model robustness from an information theoretic perspective. Dong et al. (2021) use a convex hull to capture and defense against adversarial word substitutions. Zhou et al. (2021) train robust models by augmenting training data using Dirichlet Neighborhood Ensemble (DNE).\nBesides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021). However, it only works when the augmentation happens to be generated by the same attacking method and often hurts accuracy.\nIt is worth noting that recent empirical results have shown that previous gradient-based adversarial training methods have little effect on defending against textual adversarial attacks (Li et al., 2021; Si et al., 2021). The authors benchmark existing defense methods and conclude that gradient-based\nAlgorithm 1 Friendly Adversarial Data Augmentation (FADA) Input: The original text x, ground truth label\nytrue, base model fθ, adversarial word substitution function AWS(·)\nOutput: The friendly adversarial example xf 1: Initialization: 2: xf ← x 3: the last modified word w∗← None 4: the last modified index i∗← 0 5: xadv, w\n∗, i∗ = AWS(x, ytrue, fθ) 6: if w∗ = None then 7: return xf 8: end if 9: Replace wi∗ in xadv with w∗\n10: xf ← xadv 11: return xf\nAT can achieve the strongest robustness by removing the norm bounded projection and increasing the search steps."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Friendly Adversarial Data Augmentation",
      "text" : "For a sentence x ∈ X with a length of n, it can be denoted as x = w1w2...wi...wn−1wn, where wi is the i-th word in x. Its adversarial counterpart xadv can be denoted as w′1w ′ 2...w ′ i...w ′ n−1w ′ n. In this work, xadv is generated by adversarial word substitution, so xadv has the same length with x. Conventional adversarial data augmentation generates adversarial data fooling the victim model and mixes them with the original training set. As we claim in section 1, these adversarial data can hurt test performance. An interesting and critical question is when it becomes detrimental to test accuracy.\nOne straightforward idea is to recover all the xadv to x word by word and evaluate their impact on test accuracy. We train models only with these adversarial data and test models with the original test set. We are excited that the test accuracy immediately returns to the normal level when we recover the last modified word. We denote these data with only one word recovered as xf . Geometrically, the only difference between xadv and xf is whether they have crossed the decision boundary.\nTo conclude, when the adversarial data cross the decision boundary, they become incredibly harmful to the test performance. We name all the xf\nAlgorithm 2 Ideal Geometry-aware Adversarial Training (GAT) Input: Our base network fθ , cross entropy loss LCE , training\nset D = {xi, yi}ni=1, number of epochs T , batch size m, number of batches M Output: robust network fθ 1: for epoch = 1 to T do 2: for batch = 1 to M do 3: Sample a mini-batch b = {(xi, yi)}mi=1 4: for all xi in b do 5: Generate friendly adversarial example xfi via\nAlgorithm 1 6: Apply an adversarial training method (e.g.,\nFreeLB++) on both xi and xfi to obtain their adversarial counterpart x̃i and x̃fi\n7: end for 8: Update fθ via ∇xLCE(fθ(x̃i), yi) and ∇xLCE(fθ(x̃fi ), yi) 9: end for\n10: end for\nas friendly adversarial examples (FAEs) because they improve model robustness without hurting accuracy. Similarly, we name the generation of FAEs as friendly adversarial data augmentation (FADA). We show our proposed FADA in Algorithm 1."
    }, {
      "heading" : "3.2 Geometry-aware Adversarial Training",
      "text" : ""
    }, {
      "heading" : "3.2.1 Seeking for the optimal δ",
      "text" : "Recall the inner maximization issue of the learning objective in Eq. (1). Take PGD-K for instance. It divides the search for the optimal perturbation δ into K search steps, and each step requires a backpropagation (BP), which is computationally expensive.\nWe notice that random initialization of δ0 is widely used in adversarial training, where δ0 is always confined to a ϵ-ball centered at x. However, we initialize the clean data via discrete adversarial word substitution in NLP. It is similar to data augmentation (DA), with the difference that we perturb clean data in the direction towards the decision boundary, whereas the direction of data augmentation is random.\nBy doing so, we decompose the δ into two parts, which can be obtained by word substitution and gradient-based adversarial training, respectively. We denote them as δl and δs. Therefore, the inner maximization can be reformulated as\nmax ∥δl+δs∥≤ϵ\nL(fθ(X + δl + δs), y). (4)\nWe aim to find the maximum δl that helps improve robustness without hurting accuracy. As we\nclaim in Section 3.1, FADA generates friendly adversarial data which are close to the decision boundary. Furthermore, the model trained with these friendly adversarial data keeps the same test accuracy as the original training set (Figure 1). Therefore we find the maximum δl which is harmless to the test accuracy through FADA.\nDenote Xf as the friendly adversarial data generated by FADA, Eq. (4) can be reformulated as\nmax ∥δs∥≤ϵ\nL(fθ(Xf + δs), y). (5)\nThe tiny δs can be obtained by some gradient-based adversarial training methods (e.g., FreeLB++ (Li et al., 2021)) in few search steps. As a result, a large number of search steps are saved to accelerate adversarial training. We show our proposed geometryaware adversarial training in Algorithm 2."
    }, {
      "heading" : "3.2.2 Final Learning Objective",
      "text" : "It is computationally expensive to update friendly adversarial data for every mini-batch. In practice, we generate static augmentation (Xf ,Y) for the training dataset (X,Y) and find it works well with GAT. The static augmentation (Xf ,Y) is reusable. Therefore, GAT is computationally efficient.\nThrough such a tradeoff, our final objective function can be formulated as\nL =LCE(X,Y, θ)\n+ LCE(X̃, Y, θ) + LCE(X̃f , Y, θ), (6)\nwhere LCE is the cross entropy loss, X̃ and X̃f are generated from X and Xf using gradient-based adversarial training methods, respectively."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conduct experiments on the SST-2 (Socher et al., 2013) and IMDb (Maas et al., 2011) datasets which are widely used for textual adversarial learning. Statistical details are shown in Table 1. We use the GLUE (Wang et al., 2019) version of the SST-2 dataset whose test labels are unavailable. So we report its accuracy on the develop set in our experiments."
    }, {
      "heading" : "4.2 Attacking Methods",
      "text" : "Follow Li et al. (2021), we adopt TextFooler (Jin et al., 2019), TextBugger (Li et al., 2018) and BAE (Garg and Ramakrishnan, 2020) as attackers. TextFooler and BAE are word-level attacks and TextBugger is a multi-level attacking method. We also impose restrictions on these attacks for a fair comparison, including:\n1. The maximum percentage of perturbed words pmax\n2. The minimum semantic similarity εmin between the original input and the generated adversarial example\n3. The maximum size Ksyn of one word’s synonym set\nSince the average sentence length of IMDb and SST-2 are different, pmax is set to 0.1 and 0.15, respectively; εmin is set to 0.84; and Ksyn is set to 50. All settings are referenced from previous work."
    }, {
      "heading" : "4.3 Adversarial Training Baselines",
      "text" : "We use BERTbase (Devlin et al., 2019) as the base model to evaluate the impact of the following variants of adversarial training on accuracy and robustness and provide a comprehensive comparison with our proposed GAT.\n• Adversarial Data Augmentation • ASCC (Dong et al., 2021) • DNE (Zhou et al., 2021)\n• InfoBERT (Wang et al., 2021) • TAVAT (Li and Qiu, 2021) • FreeLB (Zhu et al., 2020) • FreeLB++ (Li et al., 2021)\nASCC and DNE adopt a convex hull during training. InfoBERT improves robustness using mutual information. TAVAT establishes a token-aware robust training framework. FreeLB++ removes the norm bounded projection and increases search steps.\nWe only compare GAT with adversarial trainingbased defense methods and leave comparisons with other defense methods (e.g., certified defenses) for future work."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "We implement ASCC, DNE, InfoBERT, and TAVAT models based on TextDefender (Li et al., 2021). We implement FGM, FreeLB, FreeLB++, and our GAT based on HuggingFace Transformers 1. We implement ADA and FADA based on TextAttack 2. All the adversarial hyper-parameters settings are following their original papers. All the models are trained on two GeForce RTX 2080 GPUs and eight Tesla T4 GPUs.\nRegarding the training settings and hyperparameters, the optimizer is AdamW (Loshchilov and Hutter, 2019); the learning rate is 2e−5; the number of epochs is 10; the batch size is 64 for SST-2 and 24 for IMDb.\n1https://huggingface.co/transformers 2https://github.com/QData/TextAttack"
    }, {
      "heading" : "4.5 Main Results",
      "text" : "Our proposed GAT can easily combine with other adversarial training methods. In our experiments, we combine GAT with FGM (GATFGM ) and FreeLB++ (GATFreeLB++), respectively. We aim to evaluate if GAT can bring improvements to the simplest (FGM) and the most effective (FreeLB++) AT methods.\nWe summarize the main defense results on the SST-2 dataset in Table 2. When GAT works with the simplest adversarial training method, FGM, the resulting robustness improvement exceeds FreeLB++50. The effectiveness and efficiency of GAT allow us to obtain strong robustness while saving many search steps. Further combining FreeLB++ on GAT can obtain stronger robustness and outperform all other methods.\nRegarding the accuracy, FreeLB++30 obtains the highest 93.4%. GAT also significantly improves accuracy.\nIn addition, ADA is effective in improving robustness but hurts accuracy. It is not surprising that ASCC and DNE suffer from significant performance losses. However, there is no improvement in robustness and even weaker robustness under TextFooler and TextBugger attacks than the other methods.\nTable 3 shows the defense results on the IMDb dataset. The defense performances are generally consistent with that on the SST-2 dataset. It is worth noting that GATFGM achieved an extremely high RA % with a medium #Query, which needs further exploration.\nAWS AT method Clean % RA % #Query"
    }, {
      "heading" : "5 Discussions",
      "text" : "We further explore other factors that affect robustness and provide comprehensive empirical results."
    }, {
      "heading" : "5.1 Ablation Studies",
      "text" : "We conduct ablation studies on the SST-2 dataset to assess the impact of each component of GAT.\nAs shown in Table 4, “FADA” consistently outperforms “ADA” and “None” with different adversarial training methods. Furthermore, “FADA&FGM” achieve a higher RA% than “None&FreeLB++30”, which implies that “FADA” can obtain strong robustness in one adversarial search step. “ADA” also helps improve robustness. However, as the number of search steps increases, so does the hurt it does to Clean %. On the contrary, “FADA” does not harm Clean % but improves it, implying its friendliness."
    }, {
      "heading" : "5.2 Results with Other Attacks",
      "text" : "We have shown that GAT brings significant improvement in robustness against three greedy-based attacks. We investigate whether GAT is effective under combinatorial optimization attacks, such as PSO (Zang et al., 2020) and FastGA (Jia et al., 2019).\nWe can see from Table 5 that GATFreeLB++30 obtain the highest RA % against the two attacks and GATFreeLB++10 has the highest clean accuracy. The results demonstrate that our proposed GAT consistently outperforms other defenses against combinatorial optimization attacks."
    }, {
      "heading" : "5.3 Results with More Steps",
      "text" : "As we claim in Section 1, the accuracy should degrade with a large number of search steps. But what happens for robustness?\nWe aim to see if RA % can be further improved. Figure 3(a) shows that the RA % gradually increases against TextFooler and TextBugger attacks.\nHowever, RA % decreases against BAE with steps more than 30, which needs more investigation. As the steps increase, the growth rate of RA % decreases, and the Clean % decreases. We conclude that a reasonable number of steps will be good for both RA % and Clean %. It is unnecessary to search for too many steps since robustness grows very slowly in the late adversarial training period while accuracy drops."
    }, {
      "heading" : "5.4 Impact of Step Size",
      "text" : "A large step size (i.e., adversarial learning rate) will cause performance degradation for conventional adversarial training. Nevertheless, what impact does it have on robustness? We explore the impact of different step sizes on robustness and accuracy. As shown in Figure 3(b), the clean test accuracy slightly drops as the step size increases. The robust accuracy under TextFooler attack increases, while the robust accuracy under Textbugger and BAE attacks decrease. Overall, the impact of step size on robustness needs further study."
    }, {
      "heading" : "5.5 Impact of Training Epochs",
      "text" : "Ishida et al. (2020) have shown that preventing further reduction of the training loss when reaching a small value and keeping training can help generalization. In adversarial training, it is naturally hard to achieve zero training loss due to the insufficient capacity of the model (Zhang et al., 2021). Therefore, we investigate whether more training iterations result in stronger robustness in adversarial training. We report the RA % achieved by GATFreeLB++30 at each epoch in Figure 3(c). We observe that the RA % tends to improve slowly, implying that more training iterations result in stronger model robustness using GAT."
    }, {
      "heading" : "5.6 Results with Other Models",
      "text" : "We show that GAT can work on more advanced models. We choose RoBERTabase (Liu et al., 2019) and DeBERTabase (He et al., 2021), two improved versions of BERT, as the base models. As shown in Table 6 and Table 7, GAT slightly improve robustness of RoBERTa and DeBERTa models. We can also conclude that DeBERTa is significantly more robust than RoBERTa."
    }, {
      "heading" : "5.7 Limitations",
      "text" : "We discuss the limitations of this work as follows.\n• As we clarify in Section 3.2.2, instead of dynamically generating friendly adversarial data in training, we choose to pre-generate static augmentation. We do this for efficiency, as dynamically generating discrete sentences in training is computationally expensive. Although it still significantly improves robustness in our experiments, such a tradeoff may lead to failure because the decision boundary changes continuously during training.\n• GAT performs adversarial training on friendly adversarial data. It may help if we consider the decision boundaries when performing gradientbased adversarial training—for example, stopping early when the adversarial data crosses the decision boundary. We consider this as one of the directions for future work."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we study how to improve robustness from a geometry-aware perspective. We first pro-\npose FADA to generate friendly adversarial data that are close to the decision boundary. Then we combine gradient-based adversarial training methods on FADA to save a large number of search steps, termed geometry-aware adversarial training (GAT). GAT can efficiently achieve state-of-the-art defense performance without hurting test accuracy.\nWe conduct extensive experiments to give indepth analysis, and we hope this work can provide helpful insights on robustness in NLP."
    } ],
    "references" : [ {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards robustness against natural language word substitutions",
      "author" : [ "Xinshuai Dong", "Anh Tuan Luu", "Rongrong Ji", "Hong Liu." ],
      "venue" : "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
      "citeRegEx" : "Dong et al\\.,? 2021",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2021
    }, {
      "title" : "Hotflip: White-box adversarial examples for NLP",
      "author" : [ "Javid Ebrahimi", "Anyi Rao", "Daniel Lowd", "Dejing Dou." ],
      "venue" : "CoRR, abs/1712.06751.",
      "citeRegEx" : "Ebrahimi et al\\.,? 2017",
      "shortCiteRegEx" : "Ebrahimi et al\\.",
      "year" : 2017
    }, {
      "title" : "BAE: bert-based adversarial examples for text classification",
      "author" : [ "Siddhant Garg", "Goutham Ramakrishnan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages",
      "citeRegEx" : "Garg and Ramakrishnan.,? 2020",
      "shortCiteRegEx" : "Garg and Ramakrishnan.",
      "year" : 2020
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Deberta: decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "Do we need zero training loss after achieving zero training error",
      "author" : [ "Takashi Ishida", "Ikko Yamane", "Tomoya Sakai", "Gang Niu", "Masashi Sugiyama" ],
      "venue" : "In Proceedings of the 37th International Conference on Machine Learning,",
      "citeRegEx" : "Ishida et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ishida et al\\.",
      "year" : 2020
    }, {
      "title" : "Certified robustness to adversarial word substitutions",
      "author" : [ "Robin Jia", "Aditi Raghunathan", "Kerem Göksel", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
      "citeRegEx" : "Jia et al\\.,? 2019",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "arXiv e-prints, page arXiv:1907.11932.",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Robust encodings: A framework for combating adversarial typos",
      "author" : [ "Erik Jones", "Robin Jia", "Aditi Raghunathan", "Percy Liang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2752–2765, Online. Asso-",
      "citeRegEx" : "Jones et al\\.,? 2020",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2020
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Textbugger: Generating adversarial text against real-world applications",
      "author" : [ "Jinfeng Li", "Shouling Ji", "Tianyu Du", "Bo Li", "Ting Wang." ],
      "venue" : "CoRR, abs/1812.05271.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT-ATTACK: Adversarial attack against BERT using BERT",
      "author" : [ "Linyang Li", "Ruotian Ma", "Qipeng Guo", "Xiangyang Xue", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6193–",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Token-aware virtual adversarial training in natural language understanding",
      "author" : [ "Linyang Li", "Xipeng Qiu." ],
      "venue" : "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,",
      "citeRegEx" : "Li and Qiu.,? 2021",
      "shortCiteRegEx" : "Li and Qiu.",
      "year" : 2021
    }, {
      "title" : "Searching for an effective defender: Benchmarking defense against adversarial word substitution",
      "author" : [ "Zongyi Li", "Jianhan Xu", "Jiehang Zeng", "Linyang Li", "Xiaoqing Zheng", "Qi Zhang", "Kai-Wei Chang", "Cho-Jui Hsieh." ],
      "venue" : "CoRR, abs/2108.12777.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep text classification can be fooled",
      "author" : [ "Bin Liang", "Hongcheng Li", "Miaoqiang Su", "Pan Bian", "Xirong Li", "Wenchang Shi." ],
      "venue" : "CoRR, abs/1704.08006.",
      "citeRegEx" : "Liang et al\\.,? 2017",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2017
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of ACL, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Towards deep learning models resistant to adversarial attacks",
      "author" : [ "A. Madry", "Aleksandar Makelov", "L. Schmidt", "D. Tsipras", "Adrian Vladu." ],
      "venue" : "ArXiv, abs/1706.06083.",
      "citeRegEx" : "Madry et al\\.,? 2018",
      "shortCiteRegEx" : "Madry et al\\.",
      "year" : 2018
    }, {
      "title" : "Virtual adversarial training for semi-supervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M. Dai", "Ian J. Goodfellow." ],
      "venue" : "CoRR, abs/1605.07725.",
      "citeRegEx" : "Miyato et al\\.,? 2016",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2016
    }, {
      "title" : "Crafting adversarial input sequences for recurrent neural networks",
      "author" : [ "Nicolas Papernot", "Patrick D. McDaniel", "Ananthram Swami", "Richard E. Harang." ],
      "venue" : "CoRR, abs/1604.08275.",
      "citeRegEx" : "Papernot et al\\.,? 2016",
      "shortCiteRegEx" : "Papernot et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating natural language adversarial examples through probability weighted word saliency",
      "author" : [ "Shuhuai Ren", "Yihe Deng", "Kun He", "Wanxiang Che." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085–",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Interpretable adversarial perturbation in input embedding space for text",
      "author" : [ "Motoki Sato", "Jun Suzuki", "Hiroyuki Shindo", "Yuji Matsumoto." ],
      "venue" : "CoRR, abs/1805.02917.",
      "citeRegEx" : "Sato et al\\.,? 2018",
      "shortCiteRegEx" : "Sato et al\\.",
      "year" : 2018
    }, {
      "title" : "Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning",
      "author" : [ "Chenglei Si", "Zhengyan Zhang", "Fanchao Qi", "Zhiyuan Liu", "Yasheng Wang", "Qun Liu", "Maosong Sun." ],
      "venue" : "Findings of the Association for Com-",
      "citeRegEx" : "Si et al\\.,? 2021",
      "shortCiteRegEx" : "Si et al\\.",
      "year" : 2021
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of EMNLP, pages 1631–1642, Seattle,",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Ian Erhan, Dumitru", "Goodfellow,", "Rob Fergus." ],
      "venue" : "arXiv preprint arXiv:1312.6199.",
      "citeRegEx" : "Dumitru et al\\.,? 2013",
      "shortCiteRegEx" : "Dumitru et al\\.",
      "year" : 2013
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "CoRR, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "7th International Conference on Learning Representations,",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Infobert: Improving robustness of language models from an information theoretic perspective",
      "author" : [ "Boxin Wang", "Shuohang Wang", "Yu Cheng", "Zhe Gan", "Ruoxi Jia", "Bo Li", "Jingjing Liu." ],
      "venue" : "9th International Conference on Learning Representations, ICLR 2021,",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving adversarial robustness requires revisiting misclassified examples",
      "author" : [ "Yisen Wang", "Difan Zou", "Jinfeng Yi", "James Bailey", "Xingjun Ma", "Quanquan Gu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "SAFER: A structure-free approach for certified robustness to adversarial word substitutions",
      "author" : [ "Mao Ye", "Chengyue Gong", "Qiang Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3465–",
      "citeRegEx" : "Ye et al\\.,? 2020",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2020
    }, {
      "title" : "Word-level textual adversarial attacking as combinatorial optimization",
      "author" : [ "Yuan Zang", "Fanchao Qi", "Chenghao Yang", "Zhiyuan Liu", "Meng Zhang", "Qun Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Zang et al\\.,? 2020",
      "shortCiteRegEx" : "Zang et al\\.",
      "year" : 2020
    }, {
      "title" : "Theoretically principled trade-off between robustness and accuracy",
      "author" : [ "Hongyang Zhang", "Yaodong Yu", "Jiantao Jiao", "Eric Xing", "Laurent El Ghaoui", "Michael Jordan." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, volume 97",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Geometry-aware instance-reweighted adversarial training",
      "author" : [ "Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Defense against synonym substitution-based adversarial attacks via dirichlet neighborhood ensemble",
      "author" : [ "Yi Zhou", "Xiaoqing Zheng", "Cho-Jui Hsieh", "Kai-Wei Chang", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Freelb: Enhanced adversarial training for natural language understanding",
      "author" : [ "Chen Zhu", "Yu Cheng", "Zhe Gan", "Siqi Sun", "Tom Goldstein", "Jingjing Liu." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Deep neural networks (DNNs) outperform humans on many natural language processing (NLP) tasks (Kim, 2014; Vaswani et al., 2017; Devlin et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 148
    }, {
      "referenceID" : 27,
      "context" : "Deep neural networks (DNNs) outperform humans on many natural language processing (NLP) tasks (Kim, 2014; Vaswani et al., 2017; Devlin et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "Deep neural networks (DNNs) outperform humans on many natural language processing (NLP) tasks (Kim, 2014; Vaswani et al., 2017; Devlin et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "However, recent studies have shown that DNNs are vulnerable to crafted adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014).",
      "startOffset" : 92,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "For instance, an attacker can mislead an online sentiment analysis system by making minor changes to the input sentences (Papernot et al., 2016; Liang et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 164
    }, {
      "referenceID" : 15,
      "context" : "For instance, an attacker can mislead an online sentiment analysis system by making minor changes to the input sentences (Papernot et al., 2016; Liang et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "As a result, a growing number of studies are focusing on enhancing robustness to defend against textual adversarial attacks (Jia et al., 2019; Ye et al., 2020; Jones et al., 2020; Zhu et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 197
    }, {
      "referenceID" : 31,
      "context" : "As a result, a growing number of studies are focusing on enhancing robustness to defend against textual adversarial attacks (Jia et al., 2019; Ye et al., 2020; Jones et al., 2020; Zhu et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 197
    }, {
      "referenceID" : 9,
      "context" : "As a result, a growing number of studies are focusing on enhancing robustness to defend against textual adversarial attacks (Jia et al., 2019; Ye et al., 2020; Jones et al., 2020; Zhu et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 197
    }, {
      "referenceID" : 36,
      "context" : "As a result, a growing number of studies are focusing on enhancing robustness to defend against textual adversarial attacks (Jia et al., 2019; Ye et al., 2020; Jones et al., 2020; Zhu et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "There was a common belief that gradient-based AT methods in NLP was ineffective compared with ADA in defending against textual adversarial attacks (Li and Qiu, 2021; Si et al., 2021).",
      "startOffset" : 147,
      "endOffset" : 182
    }, {
      "referenceID" : 24,
      "context" : "There was a common belief that gradient-based AT methods in NLP was ineffective compared with ADA in defending against textual adversarial attacks (Li and Qiu, 2021; Si et al., 2021).",
      "startOffset" : 147,
      "endOffset" : 182
    }, {
      "referenceID" : 25,
      "context" : "To verify our hypothesis, we train a base model using adversarial data, which are generated by adversarial word substitution (AWS) on the SST-2 (Socher et al., 2013) dataset.",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 33,
      "context" : "In addition, TRADES (Zhang et al., 2019), MART (Wang et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 30,
      "context" : ", 2019), MART (Wang et al., 2020) and FAT (Zhang et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 11,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 22,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 8,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 32,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 12,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 3,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 24,
      "context" : "Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017; Li et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020; Si et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 249
    }, {
      "referenceID" : 14,
      "context" : "It is worth noting that recent empirical results have shown that previous gradient-based adversarial training methods have little effect on defending against textual adversarial attacks (Li et al., 2021; Si et al., 2021).",
      "startOffset" : 186,
      "endOffset" : 220
    }, {
      "referenceID" : 24,
      "context" : "It is worth noting that recent empirical results have shown that previous gradient-based adversarial training methods have little effect on defending against textual adversarial attacks (Li et al., 2021; Si et al., 2021).",
      "startOffset" : 186,
      "endOffset" : 220
    }, {
      "referenceID" : 25,
      "context" : "We conduct experiments on the SST-2 (Socher et al., 2013) and IMDb (Maas et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : ", 2013) and IMDb (Maas et al., 2011) datasets which are widely used for textual adversarial learning.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 28,
      "context" : "We use the GLUE (Wang et al., 2019) version of the SST-2 dataset whose test labels are unavailable.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "(2021), we adopt TextFooler (Jin et al., 2019), TextBugger (Li et al.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : ", 2019), TextBugger (Li et al., 2018) and BAE (Garg and Ramakrishnan, 2020) as attackers.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "We use BERTbase (Devlin et al., 2019) as the base model to evaluate the impact of the following variants of adversarial training on accuracy and robustness and provide a comprehensive comparison with our proposed GAT.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "• Adversarial Data Augmentation • ASCC (Dong et al., 2021) • DNE (Zhou et al.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 35,
      "context" : ", 2021) • DNE (Zhou et al., 2021) • InfoBERT (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 29,
      "context" : ", 2021) • InfoBERT (Wang et al., 2021) • TAVAT (Li and Qiu, 2021) • FreeLB (Zhu et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : ", 2021) • TAVAT (Li and Qiu, 2021) • FreeLB (Zhu et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 36,
      "context" : ", 2021) • TAVAT (Li and Qiu, 2021) • FreeLB (Zhu et al., 2020) • FreeLB++ (Li et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "We implement ASCC, DNE, InfoBERT, and TAVAT models based on TextDefender (Li et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "Regarding the training settings and hyperparameters, the optimizer is AdamW (Loshchilov and Hutter, 2019); the learning rate is 2e−5; the number of epochs is 10; the batch size is 64 for SST-2 and 24 for IMDb.",
      "startOffset" : 76,
      "endOffset" : 105
    }, {
      "referenceID" : 32,
      "context" : "We investigate whether GAT is effective under combinatorial optimization attacks, such as PSO (Zang et al., 2020) and FastGA (Jia et al.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 34,
      "context" : "In adversarial training, it is naturally hard to achieve zero training loss due to the insufficient capacity of the model (Zhang et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "We choose RoBERTabase (Liu et al., 2019) and DeBERTabase (He et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : ", 2019) and DeBERTabase (He et al., 2021), two improved versions of BERT, as the base models.",
      "startOffset" : 24,
      "endOffset" : 41
    } ],
    "year" : 0,
    "abstractText" : "Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate “friendly” adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training (e.g., FGM) on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via less steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.",
    "creator" : null
  }
}
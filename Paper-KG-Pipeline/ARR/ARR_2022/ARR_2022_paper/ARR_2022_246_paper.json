{
  "name" : "ARR_2022_246_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Word sense disambiguation has been a longstanding problem in natural language processing area. The task can benefit many downstream applications (Navigli, 2009), such as machine translation (Vickrey et al., 2005; Pu et al., 2018) and information retrieval (Zhong and Ng, 2012; Stokoe et al., 2003).\nThe goal of WSD task is to disambiguate word senses given contexts. For example, the word “lift” in context “Lift a load” and “The detective carefully lifted some fingerprints from the table” has different meanings. The former one means “raise from a lower to a higher position” and the latter one means “remove from a surface”. From semantic recognition of human being, the former sense is easier to disambiguate as it is the most common sense of the word while the latter one is a rare sense.\nA skewed distribution exists in SemCor (Miller et al., 1993), one commonly used human-labeled dataset for the WSD task, where most common senses have many training examples while rare senses have much fewer examples. A large coverage of senses even have no training samples, which are called zero-shot senses. Many deep neural networks based methods are affected by this imbalanced training corpora (Luo et al., 2018; Huang et al., 2019b).\nPrevious approaches attempt to address this problem by designing a new dataset specifically for the rare senses and zero-shot senses (Holla et al., 2020; Blevins et al., 2021) or enriching the sense embeddings by incorporating external lexical knowledge (Kumar et al., 2019; Blevins and Zettlemoyer, 2020). Different from these methods, we address the unbalanced training from the perspective of adjusting learning process.\nAn interesting human language phenomenon is that it follows a statistical distribution described by Zipf’s law (Zipf, 1949), which also exists in many corpora including SemCor. From the linguistic perspective, an explanation for Zipf’s law is that people tend to use more common words to minimize communication effort (Zipf, 1949). Inspired by this, we consider a word with top rank in frequency should be assigned high training weight.\nFrom the statistical perspective, two laws have been proposed to explain Zipf’s law in word frequency, namely meaning-frequency law (Zipf, 1945) and Zipf’s law of abbreviation (Florence, 1950; Grzybek, 2006). Meaning-frequency law proposes that more frequent words have higher polysemy values, where polysemy is defined as the number of different senses belonging to a word. Based on this, we calculate the polysemy distribution of words in SemCor and use a mathematical function to fit the relation between word rank and polysemy. Based on the relation, we design the Zreweighting strategy on word level to help models\ngeneralize better on rare and zero-shot senses. To the best of our knowledge, we are the first to leverage linguistic distribution to address the training bias on the WSD task. Our method improves the generalization ability of deep neural models on rare senses and zero-shot senses. Results on all English words WSD evaluation benchmarks show that our system achieves significant improvement on rare and zero-shot senses by 2.1% and 3.6% on F1 score. Furthermore, our strategy outperforms the system without any reweighting strategy and achieves a performance gain on the F1 score on all senses."
    }, {
      "heading" : "2 Related works",
      "text" : ""
    }, {
      "heading" : "2.1 Word Sense Disambiguation",
      "text" : "Word sense disambiguation is to distinguish the sense of a specific word given a context sentence. Current methods can be broadly classified into two streams, supervised learning based and knowledge based. Supervised learning based approaches view the WSD task as classification problems. For example, Zhong and Ng (2010) learns classifiers independently for each word. Knowledge-based methods, such as (Banerjee et al., 2003; Basile et al., 2014), mainly exploit two kinds of knowledge: 1) the gloss, usually in the form of a sentence defining the word sense; 2) graph structure of lexical resources.\nRecent researches integrate supervised learning and knowledge into a unified system and achieve better performance than systems relying on knowledge only. For utilizing gloss, GlossBert (Huang et al., 2019b) constructs context-gloss pairs and conducts sentence-pair classification training. Biencoder (Blevins and Zettlemoyer, 2020) proposes an end-to-end learning system to train the embedding space of context words and senses together. For utilizing structure properties, EWISE (Kumar et al., 2019) injects gloss and knowledge graph embedding into sense embeddings. EWISER (Bevilacqua and Navigli, 2020) further injects relational knowledge as additional supervision.\nDifferent with the previous approaches, we focus on addressing training bias caused by the imbalanced distribution in training dataset. In this paper, we analyze the formulation of the distribution and propose Z-reweighting method to improve performance on rare and unseen senses."
    }, {
      "heading" : "2.2 Zipf’s Law in Word Frequency",
      "text" : "Power law widely exists in human language, where the word frequency can be described by Zipf’s law (Zipf, 1949). Previous works show that the linguistic law exists in many corpora, including SemCor (Miller et al., 1993), CHILDES (MacWhinney, 2000), and Wikipedia (Grefenstette, 2016). SemCor is also one of the largest training datasets for WSD task, which also include Ontonotes (Marcus et al., 2011) and OMSTI (Taghipour and Ng, 2015).\nManin (2008) argues from the semantic view and proposes that the word semantics are influenced by the expansion of word meanings and competition of synonyms results in the law. Zipf (1945) proposes that word frequency is related to its polysemy, in which more frequent words have higher polysemy values. Recently Casas et al. (2019) investigates the law from the perspective of both polysemy and word length. Similarly, our work takes consideration of polysemy distribution and utilizes it for balanced training on WSD task."
    }, {
      "heading" : "2.3 Learning Imbalanced Dataset",
      "text" : "There are many approaches to address the influence on learning brought by imbalanced training data under a supervised setting. Most of the algorithms belong to re-weighting (Huang et al., 2016, 2019a) or re-sampling (Buda et al., 2018; Cui et al., 2019). Re-weighting methods adjust the weights of different classes. Re-sampling methods balance the learning by over-sampling minority classes or under-sampling the frequent classes. Another line of works incorporate the idea of angular margin, aiming to enlarge the intra-class margin (Liu et al., 2016; Wang et al., 2018; Cao et al., 2019).\nOur work follows the line of re-weighting. We take consideration of the word polysemy distribution and propose Z-reweighting method for WSD task, which is quite different with previous reweighting methods."
    }, {
      "heading" : "3 Distribution Analysis in SemCor",
      "text" : "In this section, we first show the overall word and sense distribution in SemCor1 (Miller et al., 1993). Since we manage to utilize the polysemy distribution as the basis for Z-reweighting strategy, we further look into the relationship between word rank, frequency, and polysemy.\n1http://lcl.uniroma1.it/wsdeval/training-data"
    }, {
      "heading" : "3.1 Imbalanced Data Distribution in SemCor",
      "text" : "As mentioned in (Kilgarriff, 2004), a Zipfian distribution exists in the word senses of human language. In this part, we investigate the details of the distribution in training data SemCor both on the word level and the sense level.\nFrom the definitions of WordNet (Miller, 1998), a word has a most common sense denoted as MCS and several least common senses denoted as LCS. Following this definition, we calculate the distribution of training data in the SemCor corpus, and the resulting distribution is shown in Table 1. SemCor contains 226,036 training instances, where each instance is a sentence with a labeled sense of one word. Among all the instances, 73.5% are training instances for MCS, belonging to 22,320 words and the rest are for LCS. LCS has 5.43 training instances for each sense, much lower than MCS which has 7.45 instances on average.\nWe further investigate the polysemy distribution of training words labeled with MCS and LCS respectively. The polysemy value defined in WordNet2 is utilized to calculate the distribution. The average polysemy value for training words labeled with LCS is 4.77, much greater than that of MCS. This shows that words labeled with LCS have a larger coverage of senses to distinguish. The words with LCS in training data SemCor has higher polysemy while with fewer training instances. Therefore, we can see that disambiguating LCS is much more challenging than MCS in the WSD task."
    }, {
      "heading" : "3.2 Word Rank, Frequency and Polysemy",
      "text" : "To investigate the details of the Zipfian distribution in SemCor, we calculate the number of training instances and polysemy for each word and sort\n2https://wordnet.princeton.edu\nthem by frequency in descending order. We apply a binning technique to reduce noise and get a better view of Zipf’s law on word distribution. Specifically, every adjacent 300 words belong to a bin for clear analysis in this part. The distribution of instance number with sorted word rank by decreasing frequency is shown in Figure 1. As we can see, top ranked words have much more training cases than low ranked words both for training words labeled with MCS and LCS.\nTo get a deeper understanding of the statistical law in word frequency, we further analyze the relation between word polysemy and sorted word rank. Similar to training instances, we calculate the average polysemy of every 300 words in a bin and get the distribution of word polysemy with sorted word rank by decreasing frequency. As shown in Figure 2, the words with top rank have higher polysemy values than words with low rank. This shows that words with top frequency rank have more senses to disambiguate. Moreover, words with LCS are mostly with top ranks."
    }, {
      "heading" : "4 Algorithms",
      "text" : "In this section, we first introduce the terminology for the WSD task. Then we illustrate our Z-reweighting strategy on adjusting the training loss for the imbalanced training dataset."
    }, {
      "heading" : "4.1 Terminology",
      "text" : "WSD task is to disambiguate the meanings of a set of words w = {w1, w2, ..., wn} given a context sentence S. Each context word wi, i ∈ [1, n] in a sentence S has several candidate senses {s1, s2, ..., sm}. Each sense is described by a definition sentence, also called gloss in WordNet (Miller, 1998). The candidate senses have a corresponding gloss set {g1, g2, ..., gm}."
    }, {
      "heading" : "4.2 Z-Reweighting Strategy",
      "text" : "To alleviate the influence brought by the imbalanced training dataset, we propose the Zreweighting strategy to balance the learning between MCS and LCS during training, resulting in a stronger capacity of the model in disambiguating rare and zero-shot senses while maintaining comparable performance on MCS at the same time.\nTraining words in SemCor are denoted in form W = {W1,W2, ...,WN} with descending order of frequency. The polysemy of a word represents the number of senses belonging to the word. P = {p1, p2, ..., pN} is the polysemy array of the words.\nA bin parameter K means average polysemy is calculated for every K words. An averaged polysemy array is calculated as:\np ′ o = o(K+1)∑ d=oK pd K , o ∈ [1, N K ], d ∈ [1, N ] (1)\nP ′ = {p′1, ..., p ′ N K } (2)\nAs analyzed in (Casas et al., 2019), a power law exists between word frequency and polysemy in the corpora CHILDES (MacWhinney, 2000). Similarily, we utilize a function f(x) = a ln(x+ b)+ c to fit the relation between polysemy P ′ and word rank o = [1, 2, ..., NK ] in SemCor mathematically, where a, b, c are parameters. The fitting function is monotonic decreasing with word rank. An example of the fitting curve and original polysemy distribution with word frequency at K = 300 is shown in Figure 2.\nWith the same word ranks, a smoothed polysemy array can be calculated from the fitting curve as:\nP f = {pf1 , ..., p f N K } (3)\nThe discrete fitting polysemy array is normalized for further processing:\nP r = P f\nmax(P f ) (4)\nSince the number of words is too large to assign each word a weight, the NK bins of words are further split into M groups. For word in k-th bin, k ∈ [1, NK ], belonging to group j ∈ [1,M ], the regularized polysemy satisfying:\nptj+1 ≤ prk < ptj , (5)\nwhere P t = {pt1, ..., ptM} is the threshold array to split the groups. The words in group j ∈ [1,M ] are assigned weight:\nαj = (p t j) η, (6)\nwhere η is a power parameter. Assume the predicted output probabilities from a model for candidate sense set as z = [z1, z2, ..., zm], the standard cross entropy loss given true word sense label y is:\nCEsoftmax(z, y) = − log( exp(zy)∑m i=1 exp(zi) ) (7)\nIn Z-reweighting strategy, the weight αj is used to adjust the training on word level. The new weighted training loss is:\nloss(Wl, j) = −αj log( exp(zy)∑m i=1 exp(zi) ), (8)\nwhere l ∈ [1, N ] and j ∈ [1,M ], representing the word with rank l in group j has training weight αj ."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we first introduce the training dataset and evaluation metrics. Then we show different baseline methods. Finally, details of the training process are presented."
    }, {
      "heading" : "5.1 Dataset",
      "text" : "SemCor 3.0 is used as the training dataset. Five standard WSD datasets from Senseval and SemEval competitions are used as evaluation set. Among them, semeval 2007 (Pradhan et al., 2007) is used as development dataset for selecting the\nbest model. Other four datasets including senseval2 (Palmer et al., 2001), senseval-3 (Snyder and Palmer, 2004), semeval2012 (Navigli et al., 2013) and semeval2015 (Moro and Navigli, 2015) are used as test datasets. F1 score is used as the evaluation metric. We also report the overall score on the above five datasets, denoted as ALL dataset (Raganato et al., 2017). For further analysis, F1 scores on MCS, LCS, and zero-shot senses are also calculated."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "BEM framework (Blevins and Zettlemoyer, 2020) without any balanced strategy is a baseline system. In addition, different balanced training methods applied on BEM framework are used as three more baseline systems. The balanced methods are classified into two levels, namely, the sense-level (balanced reweighting and margin based method LDAW (Cao et al., 2019)), and the word-level (balanced resampling).\nBiencoder Model (BEM). The model utilizes the gloss knowledge from WordNet. The two encoders are initialized with the same pre-trained language model. The encoders take a context sentence and glosses as input, generating representations for word wi and corresponding gloss set {g1, g2, ..., gm} as Ei and {G1, G2, ..., Gm} separately. Based on the representations, the similarity score between word and glosses are calculated as:\nzj = Ei ·Gj , j ∈ [1,m]\nA standard cross-entropy loss is used in training,\nloss(wi, y) = − log exp(zy)∑m j=1 exp(zj) ,\nwhere y is the sense label.\nBalanced Reweighting Method (B-reweighting). The B-reweighting strategy is applied on the sense level. For each word, the weights of senses is proportional to the inverse of training instances.\nlossbal(wi, y) = −βy log exp(zy)∑m j=1 exp(zj) ,\nwhere βj = ∑m j=1 nj\nnj for j ∈ [1, ...,m] and nj is\nthe number of training instances on sense j of word wi.\nBalanced Resampling Method (B-resampling). The B-resampling method is applied on the word level. Firstly each word is sampled with the same probability. Then the training cases of the selected word are sampled randomly. Standard cross-entropy loss is used in this method.\nLDAM. The margin-based method adjusts the training on the sense level. The goal of LDAM is to solve the class-imbalance problem by utilizing a label-distribution-aware margin loss. We apply the LDAW loss on the sense level as another baseline.\nThe smoothed relaxation of LDAM in the crossentropy loss with enhanced margins is as follows:\nlossmargin(wi, y) = − log ezy−∆y ezy−∆y + ∑ j ̸=y e zj ,\nwhere ∆j = C n 1/4 j , for j ∈ {1, . . . ,m} and C is a constant. nj is the training instances number of sense j for word wi.\nStandard LDAW is trained in two stages. Firstly the label-distribution-aware margin loss is applied to train the model for three epochs and Breweighting loss is used for further training. In both stages, the learning rate is the same as 1e5. For first stage training, C is tuned to make the largest margin as 0.5."
    }, {
      "heading" : "5.3 Implementation details",
      "text" : "Baseline Systems. Each system is trained for 20 epochs. AdamW (Kingma and Ba, 2014) is selected as the optimization algorithm. The learning rate is fixed at 1e-5 during training. The encoders in the biencoder framework both are initialized with bert-base (110M parameters) or bert-large (336M parameters) (Devlin et al., 2018). The experiments in which encoders initialized with bert-base are run on RTX 2080 and the experiments in which encoders initialized with Bert-large are run on RTX 3090. Average running hours is 30 hours for Bert-base and 40 hours for Bert-large.\nZ-reweighting. To simplify the mathematical function fitting of polysemy distribution, we first split the words into bins by setting a fixed group number K. For the second grouping stage, to simplify the reweighting strategy on word level, we use thresholds to group the smoothed values calculated by fitting curve given word rank. In the experiments, we use weights of 1 decimal place\nas thresholds. The defined threshold array is as P t = [1.0, 0.9, ..., 0.1], where the gap between thresholds is 0.1. For assigning weights, η = 1, 2 is used to adjust the value of weight. For example, words with regularized polysemy value in [0.3, 0.4) are in a group, assigning weight 0.16 when η = 2. The weight is further rounded with one decimal number as 0.2. If the rounding weight is less than 0.1, we use a weight of 0.1. For comparison with baselines, we set K = 300, η = 2 in the Z-reweighting strategy."
    }, {
      "heading" : "6 Results",
      "text" : "In this section, we first analyze the overall performance on the test datasets using different training strategies. Then the details of improvement on MCS, LCS, and zero-shot senses for word groups are presented. Finally, we analyze the impacts\nbrought by hyper-parameters in Z-reweighting and impacts brought by backbone models."
    }, {
      "heading" : "6.1 Overall Performance",
      "text" : "The performance on test datasets by different systems are shown in Table 2. WordNet S1 uses the most common sense in WordNet and MFS uses the most frequent sense in the training dataset. Both the baselines achieve much lower performances than previous learning based systems, including BERTbase (Devlin et al., 2018), GlossBERT (Huang et al., 2019b) and BEM3. The sense embeddings in EWISE is fixed during training, which explains its much lower performance than GlossBERT and BEM.\nComparing Z-reweighting with previous systems, our F1 score on the ALL dataset achieves the highest, demonstrating that our strategy can benefit the model in generalizing overall word senses. Specifically, our system even outperforms the original BEM system. The F1 score on MCS, LCS and zero-shot senses in ALL testset, with 4,603, 2,650, and 1,139 test instances respectively, are reported in Table 3. Details show that the performance gain mainly comes from LCS and zero-shot senses, 2.1% and 3.6% separately.\nBesides the Z-reweighting method, Breweighting and LDAW also show performance improvement on LCS and zero-shot senses, comparing with the BEM baseline. However, these\n3We use original open source code for paper (Blevins and Zettlemoyer, 2020): https://github.com/facebookresearch/wsdbiencoders.\nbalanced strategies deteriorate the system ability in disambiguating MCS, resulting in the drop of F1 score on the ALL dataset.\nAmong all the balanced training strategies, Bresampling performs the worst. Equally sampling the words leads to insufficient training of topranked words, which results in poor performance. Our Z-reweighting strategy outperforms all the other balanced training strategies, indicating that our method is effective in improving the generalization ability of the model."
    }, {
      "heading" : "6.2 LCS, Zero-shot Senses in Word Groups",
      "text" : "Among all the balanced strategies, only Zreweighting outperforms baseline system BEM on the F1 score of ALL dataset. To look into how the Z-reweighting strategy works, we analyze the details of performance in the word groups. Noting that according to the Z-reweighting strategy, words in each group are assigned the same weight. The hyper-parameters are K = 300, η = 2 for our results. Under the setting, there are six groups of words from training dataset. These six groups of words are sorted by decreasing frequency order. The left words belong to a oov group in which words are not shown in the training dataset.\nWe calculate the F1 score of LCS and zeroshot senses of ALL test set and plot the results in Figure 3 and Figure 4 separately. In Figure 3, our system outperforms BEM on group one, in which the words are with highest frequency. The Z-reweighting strategy assigns the largest weight in this group and the F1 score improves 3.4%. For group 4 to group 7, our algorithm also shows con-\nsistent improvements. The performance gain drops in group 2. The reason behind this is that we manage to improve the performance of all words on the WSD task and use semeval2007 as development set for model selection.\nFor zero-shot senses shown in Figure 4, our system achieves improvement in five out of seven word groups, at most 24.3% for group five. The results show that the Z-reweighting strategy enables the model to generalize better to unseen senses. For words in group seven, the performance of zero-shot senses also improves, which shows that our method can further generalize better to senses of unseen words."
    }, {
      "heading" : "6.3 Impact of K and η in Z-reweighting",
      "text" : "Each different bin number K results in a set of distinct weights for training words. In our experiments, we set K = 50, 100, 200, 300, 400 and η = 1, 2.\nThe performances of our system under different hyper-parameter settings are shown in Figure 5 and Table 4. In Figure 5, we can see that η = 2 achieves higher performance than η = 1 in most settings. This shows that the training weights with larger disparity on top and low ranked words result in higher performance on the overall score. The best performance achieves at K = 300, η = 2. It is interesting to see that with different hyperparameters, the system has various overall scores. When K = 400, the overall score achieves lowest both for η = 1 and η = 2. It indicates that large K eliminates the weight distinctness between words during training, leading to drop on overall performance.\nTo explore the details of effects brought by hyper-parameters, we further show the F1 score of MCS, LCS, and zero-shot senses in Table 4. The accuracy of MCS varies from 92.8% to 93.3% under different settings. For most of the groups, MCS achieves higher performance with η = 1 than η = 2. When the gap between weights becomes larger with η = 1 changing to η = 2, the influences on LCS and zero-shot senses are greater than those on MCS. LCS achieves the best score 54.3%, 1.4% higher than the lowest score. Zero-shot senses achieve the best score of 72.3%, 1.4% higher than the lowest score. For all the combinations, we can see improvements on LCS and zero-shot senses compared to baseline BEM, demonstrating the effectiveness of our strategy."
    }, {
      "heading" : "6.4 Impact of Backbone Models",
      "text" : "In this part, we show the influences brought by backbone models in BEM. The encoders of BEM are initialized by Bert-base and Bert-large models respectively for comparison. For Z-reweighting strategy, we use K = 300, η = 2. Training parameter settings are the same with the two backbone models. We experiment with different balanced training strategies. The results are presented in Figure 6. From the figure we can see that Bert-large achieves better performance on BEM and LDAM systems. For B-reweighting and Z-reweighting systems, the overall scores remain almost the same. However, for the B-resampling strategy, the performance drop 1%. Since the performances on Bert-large are nearly the same or even worse than Bert-base, we use Bert-base as the backbone model for training efficiency."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we address the problem in learning imbalanced training dataset on the WSD task. Words with top frequency rank have more senses to disambiguate both for MCS and LCS. We assume these words should be assigned larger weights during training. Specifically, we use a mathematical function to fit the relation between word rank and word polysemy, and utilize smoothed polysemy to design the Z-reweighting strategy for all words English WSD task. The strategy leads to improvement on the performance of LCS and zero-shot senses on standard English WSD evaluation benchmarks. Furthermore, our method achieves performance gain on the F1 score for all senses. The results demonstrate the effectiveness of our methods. ALL our codes and results will be released."
    } ],
    "references" : [ {
      "title" : "Extended gloss overlaps as a measure of semantic relatedness",
      "author" : [ "Satanjeev Banerjee", "Ted Pedersen" ],
      "venue" : "In Ijcai,",
      "citeRegEx" : "Banerjee and Pedersen,? \\Q2003\\E",
      "shortCiteRegEx" : "Banerjee and Pedersen",
      "year" : 2003
    }, {
      "title" : "An enhanced lesk word sense disambiguation algorithm through a distributional semantic model",
      "author" : [ "Pierpaolo Basile", "Annalina Caputo", "Giovanni Semeraro." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguis-",
      "citeRegEx" : "Basile et al\\.,? 2014",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2014
    }, {
      "title" : "Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information",
      "author" : [ "Michele Bevilacqua", "Roberto Navigli." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Bevilacqua and Navigli.,? 2020",
      "shortCiteRegEx" : "Bevilacqua and Navigli.",
      "year" : 2020
    }, {
      "title" : "Fews: Large-scale, low-shot word sense disambiguation with the dictionary",
      "author" : [ "Terra Blevins", "Mandar Joshi", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:2102.07983.",
      "citeRegEx" : "Blevins et al\\.,? 2021",
      "shortCiteRegEx" : "Blevins et al\\.",
      "year" : 2021
    }, {
      "title" : "Moving down the long tail of word sense disambiguation with gloss-informed biencoders",
      "author" : [ "Terra Blevins", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:2005.02590.",
      "citeRegEx" : "Blevins and Zettlemoyer.,? 2020",
      "shortCiteRegEx" : "Blevins and Zettlemoyer.",
      "year" : 2020
    }, {
      "title" : "A systematic study of the class imbalance problem in convolutional neural networks",
      "author" : [ "Mateusz Buda", "Atsuto Maki", "Maciej A Mazurowski." ],
      "venue" : "Neural Networks, 106:249–259.",
      "citeRegEx" : "Buda et al\\.,? 2018",
      "shortCiteRegEx" : "Buda et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning imbalanced datasets with label-distribution-aware margin loss",
      "author" : [ "Kaidi Cao", "Colin Wei", "Adrien Gaidon", "Nikos Arechiga", "Tengyu Ma." ],
      "venue" : "arXiv preprint arXiv:1906.07413.",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Polysemy and brevity versus frequency in language",
      "author" : [ "Bernardino Casas", "Antoni Hernández-Fernández", "Neus Catala", "Ramon Ferrer-i Cancho", "Jaume Baixeries." ],
      "venue" : "Computer Speech & Language, 58:19–50.",
      "citeRegEx" : "Casas et al\\.,? 2019",
      "shortCiteRegEx" : "Casas et al\\.",
      "year" : 2019
    }, {
      "title" : "Class-balanced loss based on effective number of samples",
      "author" : [ "Yin Cui", "Menglin Jia", "Tsung-Yi Lin", "Yang Song", "Serge Belongie." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9268–9277.",
      "citeRegEx" : "Cui et al\\.,? 2019",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Human behaviour and the principle of least effort",
      "author" : [ "P Sargant Florence" ],
      "venue" : null,
      "citeRegEx" : "Florence.,? \\Q1950\\E",
      "shortCiteRegEx" : "Florence.",
      "year" : 1950
    }, {
      "title" : "Extracting weighted language lexicons from wikipedia",
      "author" : [ "Gregory Grefenstette." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 1365– 1368.",
      "citeRegEx" : "Grefenstette.,? 2016",
      "shortCiteRegEx" : "Grefenstette.",
      "year" : 2016
    }, {
      "title" : "Contributions to the science of text and language: word length studies and related issues, volume 31",
      "author" : [ "Peter Grzybek." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Grzybek.,? 2006",
      "shortCiteRegEx" : "Grzybek.",
      "year" : 2006
    }, {
      "title" : "Learning to learn to disambiguate: Meta-learning for few-shot word sense disambiguation",
      "author" : [ "Nithin Holla", "Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "arXiv preprint arXiv:2004.14355.",
      "citeRegEx" : "Holla et al\\.,? 2020",
      "shortCiteRegEx" : "Holla et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning deep representation for imbalanced classification",
      "author" : [ "Chen Huang", "Yining Li", "Chen Change Loy", "Xiaoou Tang." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5375–5384.",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep imbalanced learning for face recognition and attribute prediction",
      "author" : [ "Chen Huang", "Yining Li", "Chen Change Loy", "Xiaoou Tang." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 42(11):2781–2794.",
      "citeRegEx" : "Huang et al\\.,? 2019a",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Glossbert: Bert for word sense disambiguation with gloss knowledge",
      "author" : [ "Luyao Huang", "Chi Sun", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1908.07245.",
      "citeRegEx" : "Huang et al\\.,? 2019b",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "How dominant is the commonest sense of a word? In International conference on text, speech and dialogue, pages 103–111",
      "author" : [ "Adam Kilgarriff." ],
      "venue" : "Springer.",
      "citeRegEx" : "Kilgarriff.,? 2004",
      "shortCiteRegEx" : "Kilgarriff.",
      "year" : 2004
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Zero-shot word sense disambiguation using sense definition embeddings",
      "author" : [ "Sawan Kumar", "Sharmistha Jat", "Karan Saxena", "Partha Talukdar." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5670–",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Large-margin softmax loss for convolutional neural networks",
      "author" : [ "Weiyang Liu", "Yandong Wen", "Zhiding Yu", "Meng Yang." ],
      "venue" : "ICML, volume 2, page 7.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating glosses into neural word sense disambiguation",
      "author" : [ "Fuli Luo", "Tianyu Liu", "Qiaolin Xia", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "arXiv preprint arXiv:1805.08028.",
      "citeRegEx" : "Luo et al\\.,? 2018",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2018
    }, {
      "title" : "The childes project: Tools for analyzing talk: Volume i: Transcription format and programs, volume ii: The database",
      "author" : [ "Brian MacWhinney" ],
      "venue" : null,
      "citeRegEx" : "MacWhinney.,? \\Q2000\\E",
      "shortCiteRegEx" : "MacWhinney.",
      "year" : 2000
    }, {
      "title" : "Zipf’s law and avoidance of excessive synonymy",
      "author" : [ "Dmitrii Y Manin." ],
      "venue" : "Cognitive Science, 32(7):1075– 1098.",
      "citeRegEx" : "Manin.,? 2008",
      "shortCiteRegEx" : "Manin.",
      "year" : 2008
    }, {
      "title" : "Ontonotes: A large training corpus for enhanced processing",
      "author" : [ "Ralph Weischedel Eduard Hovy Mitchell Marcus", "Martha Palmer", "Robert Belvin Sameer Pradhan Lance Ramshaw", "Nianwen Xue." ],
      "venue" : "Joseph Olive, Caitlin Christianson, andJohn Mc-",
      "citeRegEx" : "Marcus et al\\.,? 2011",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 2011
    }, {
      "title" : "WordNet: An electronic lexical",
      "author" : [ "George A Miller" ],
      "venue" : null,
      "citeRegEx" : "Miller.,? \\Q1998\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 1998
    }, {
      "title" : "A semantic concordance",
      "author" : [ "Ross T Bunker" ],
      "venue" : null,
      "citeRegEx" : "Bunker.,? \\Q1993\\E",
      "shortCiteRegEx" : "Bunker.",
      "year" : 1993
    }, {
      "title" : "Word sense disambiguation: A",
      "author" : [ "Roberto Navigli" ],
      "venue" : "(SemEval",
      "citeRegEx" : "Navigli.,? \\Q2015\\E",
      "shortCiteRegEx" : "Navigli.",
      "year" : 2015
    }, {
      "title" : "One million sense-tagged instances for word sense disambiguation and induction",
      "author" : [ "Kaveh Taghipour", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the nineteenth conference on computational natural language learning, pages 338–344.",
      "citeRegEx" : "Taghipour and Ng.,? 2015",
      "shortCiteRegEx" : "Taghipour and Ng.",
      "year" : 2015
    }, {
      "title" : "Word-sense disambiguation for machine translation",
      "author" : [ "David Vickrey", "Luke Biewald", "Marc Teyssier", "Daphne Koller." ],
      "venue" : "Proceedings of human language technology conference and conference on empirical methods in natural language processing,",
      "citeRegEx" : "Vickrey et al\\.,? 2005",
      "shortCiteRegEx" : "Vickrey et al\\.",
      "year" : 2005
    }, {
      "title" : "Additive margin softmax for face verification",
      "author" : [ "Feng Wang", "Jian Cheng", "Weiyang Liu", "Haijun Liu." ],
      "venue" : "IEEE Signal Processing Letters, 25(7):926–930.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "It makes sense: A wide-coverage word sense disambiguation system for free text",
      "author" : [ "Zhi Zhong", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the ACL 2010 system demonstrations, pages 78–83.",
      "citeRegEx" : "Zhong and Ng.,? 2010",
      "shortCiteRegEx" : "Zhong and Ng.",
      "year" : 2010
    }, {
      "title" : "Word sense disambiguation improves information retrieval",
      "author" : [ "Zhi Zhong", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273–282.",
      "citeRegEx" : "Zhong and Ng.,? 2012",
      "shortCiteRegEx" : "Zhong and Ng.",
      "year" : 2012
    }, {
      "title" : "The meaning-frequency relationship of words",
      "author" : [ "George Kingsley Zipf." ],
      "venue" : "The Journal of general psychology, 33(2):251–256.",
      "citeRegEx" : "Zipf.,? 1945",
      "shortCiteRegEx" : "Zipf.",
      "year" : 1945
    }, {
      "title" : "Human behaviour and the principle of least-effort",
      "author" : [ "George Kingsley Zipf." ],
      "venue" : "cambridge ma edn. Reading: Addison-Wesley.",
      "citeRegEx" : "Zipf.,? 1949",
      "shortCiteRegEx" : "Zipf.",
      "year" : 1949
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "The task can benefit many downstream applications (Navigli, 2009), such as machine translation (Vickrey et al., 2005; Pu et al., 2018) and information retrieval (Zhong and Ng, 2012; Stokoe et al.",
      "startOffset" : 95,
      "endOffset" : 134
    }, {
      "referenceID" : 32,
      "context" : ", 2018) and information retrieval (Zhong and Ng, 2012; Stokoe et al., 2003).",
      "startOffset" : 34,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "Many deep neural networks based methods are affected by this imbalanced training corpora (Luo et al., 2018; Huang et al., 2019b).",
      "startOffset" : 89,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "Many deep neural networks based methods are affected by this imbalanced training corpora (Luo et al., 2018; Huang et al., 2019b).",
      "startOffset" : 89,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "Previous approaches attempt to address this problem by designing a new dataset specifically for the rare senses and zero-shot senses (Holla et al., 2020; Blevins et al., 2021) or enriching the sense embeddings by incorporating external lexical knowledge (Kumar et al.",
      "startOffset" : 133,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "Previous approaches attempt to address this problem by designing a new dataset specifically for the rare senses and zero-shot senses (Holla et al., 2020; Blevins et al., 2021) or enriching the sense embeddings by incorporating external lexical knowledge (Kumar et al.",
      "startOffset" : 133,
      "endOffset" : 175
    }, {
      "referenceID" : 19,
      "context" : ", 2021) or enriching the sense embeddings by incorporating external lexical knowledge (Kumar et al., 2019; Blevins and Zettlemoyer, 2020).",
      "startOffset" : 86,
      "endOffset" : 137
    }, {
      "referenceID" : 4,
      "context" : ", 2021) or enriching the sense embeddings by incorporating external lexical knowledge (Kumar et al., 2019; Blevins and Zettlemoyer, 2020).",
      "startOffset" : 86,
      "endOffset" : 137
    }, {
      "referenceID" : 34,
      "context" : "An interesting human language phenomenon is that it follows a statistical distribution described by Zipf’s law (Zipf, 1949), which also exists in many corpora including SemCor.",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 34,
      "context" : "From the linguistic perspective, an explanation for Zipf’s law is that people tend to use more common words to minimize communication effort (Zipf, 1949).",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 33,
      "context" : "From the statistical perspective, two laws have been proposed to explain Zipf’s law in word frequency, namely meaning-frequency law (Zipf, 1945) and Zipf’s law of abbreviation (Florence, 1950; Grzybek, 2006).",
      "startOffset" : 132,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "From the statistical perspective, two laws have been proposed to explain Zipf’s law in word frequency, namely meaning-frequency law (Zipf, 1945) and Zipf’s law of abbreviation (Florence, 1950; Grzybek, 2006).",
      "startOffset" : 176,
      "endOffset" : 207
    }, {
      "referenceID" : 12,
      "context" : "From the statistical perspective, two laws have been proposed to explain Zipf’s law in word frequency, namely meaning-frequency law (Zipf, 1945) and Zipf’s law of abbreviation (Florence, 1950; Grzybek, 2006).",
      "startOffset" : 176,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : "Knowledge-based methods, such as (Banerjee et al., 2003; Basile et al., 2014), mainly exploit two kinds of knowledge: 1) the gloss, usually in the form of a sentence defining the word sense; 2) graph structure of lexical resources.",
      "startOffset" : 33,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "For utilizing gloss, GlossBert (Huang et al., 2019b) constructs context-gloss pairs and conducts sentence-pair classification training.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "Biencoder (Blevins and Zettlemoyer, 2020) proposes an end-to-end learning system to train the embedding space of context words and senses together.",
      "startOffset" : 10,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "For utilizing structure properties, EWISE (Kumar et al., 2019) injects gloss and knowledge graph embedding into sense embeddings.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "EWISER (Bevilacqua and Navigli, 2020) further injects relational knowledge as additional supervision.",
      "startOffset" : 7,
      "endOffset" : 37
    }, {
      "referenceID" : 34,
      "context" : "Power law widely exists in human language, where the word frequency can be described by Zipf’s law (Zipf, 1949).",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : ", 1993), CHILDES (MacWhinney, 2000), and Wikipedia (Grefenstette, 2016).",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : ", 1993), CHILDES (MacWhinney, 2000), and Wikipedia (Grefenstette, 2016).",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "SemCor is also one of the largest training datasets for WSD task, which also include Ontonotes (Marcus et al., 2011) and OMSTI (Taghipour and Ng, 2015).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "Another line of works incorporate the idea of angular margin, aiming to enlarge the intra-class margin (Liu et al., 2016; Wang et al., 2018; Cao et al., 2019).",
      "startOffset" : 103,
      "endOffset" : 158
    }, {
      "referenceID" : 30,
      "context" : "Another line of works incorporate the idea of angular margin, aiming to enlarge the intra-class margin (Liu et al., 2016; Wang et al., 2018; Cao et al., 2019).",
      "startOffset" : 103,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Another line of works incorporate the idea of angular margin, aiming to enlarge the intra-class margin (Liu et al., 2016; Wang et al., 2018; Cao et al., 2019).",
      "startOffset" : 103,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "As mentioned in (Kilgarriff, 2004), a Zipfian distribution exists in the word senses of human language.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "From the definitions of WordNet (Miller, 1998), a word has a most common sense denoted as MCS and several least common senses denoted as LCS.",
      "startOffset" : 32,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "Each sense is described by a definition sentence, also called gloss in WordNet (Miller, 1998).",
      "startOffset" : 79,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "As analyzed in (Casas et al., 2019), a power law exists between word frequency and polysemy in the corpora CHILDES (MacWhinney, 2000).",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : ", 2019), a power law exists between word frequency and polysemy in the corpora CHILDES (MacWhinney, 2000).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "BEM framework (Blevins and Zettlemoyer, 2020) without any balanced strategy is a baseline system.",
      "startOffset" : 14,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "The balanced methods are classified into two levels, namely, the sense-level (balanced reweighting and margin based method LDAW (Cao et al., 2019)), and the word-level (balanced resampling).",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 18,
      "context" : "AdamW (Kingma and Ba, 2014) is selected as the optimization algorithm.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "The encoders in the biencoder framework both are initialized with bert-base (110M parameters) or bert-large (336M parameters) (Devlin et al., 2018).",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "Both the baselines achieve much lower performances than previous learning based systems, including BERTbase (Devlin et al., 2018), GlossBERT (Huang et al.",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "We use original open source code for paper (Blevins and Zettlemoyer, 2020): https://github.",
      "startOffset" : 43,
      "endOffset" : 74
    } ],
    "year" : 0,
    "abstractText" : "Word sense disambiguation (WSD) is a crucial problem in natural language processing (NLP). Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models. However, the imbalanced training dataset leads to poor performance of the models on rare senses and zero-shot senses. Words with top frequency ranks have more training instances and senses than those with low frequency ranks in the training dataset. We investigate the statistical relation between word frequency rank and word polysemy distribution. Based on the relation, we propose a Z-reweighting method on word level to adjust the training on the imbalanced dataset. Our strategy enables the models to generalize better on rare and zero-shot senses. The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark. Especially for rare and zero-shot senses, our approach achieves 2.1% and 3.6% improvements on the F1 score, respectively.",
    "creator" : null
  }
}
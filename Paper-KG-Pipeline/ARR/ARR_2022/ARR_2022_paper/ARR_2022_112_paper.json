{
  "name" : "ARR_2022_112_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SKILLSPAN: Hard and Soft Skill Extraction from English Job Postings",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Job markets are under constant development— often due to developments in technology, migration, and digitization—and so are the skill sets required. Job vacancy data is emerging on a variety of platforms in big quantities, and can provide insights on labor market skill set demands and aid job matching (Balog et al., 2012). Automatic skill extraction (SE) is to extract the competences necessary for any occupation from unstructured text.\nPrevious work in SE shows promising progress, but is halted by lack of available datasets and annotation guidelines. Two out of 14 studies release their dataset, which limit themselves to crowd-sourced labels (Sayfullina et al., 2018) or annotations from a predefined list of skills on the document-level (Bhola et al., 2020). Additionally,\nnone of the 14 previously mentioned studies release their annotation guidelines, which obscures the meaning of a competence. Job markets change, as do the skills in, e.g., the European Skills, Competences, Qualifications and Occupations (ESCO; le Vrang et al., 2014) taxonomy (Section 3). Hence, it is important to cover for possible emerging skills.\nWe propose SKILLSPAN, a novel SE dataset annotated at the span-level for skill and knowledge components (SKCs) in job postings (JPs). As illustrated in Figure 1, SKCs can be nested inside skills. SKILLSPAN allows for extracting possibly undiscovered competences and to diminish the lack of coverage of predefined skill inventories.\nOur analysis (Figure 2) shows that SKCs contain on average longer sequences than typical Named Entity Recognition (NER) tasks. Albeit we additionally study models optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), some underperform. Overall, we find specialized domain BERT models (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020) perform better than their non-adapted counterparts. We explore the benefits of domain-adaptive pretraining on the JP domain (Han and Eisenstein, 2019; Gururangan et al., 2020). Last, given the examples from Figure 1, we formulate the task as both as a sequence labeling and a multi-task learning (MTL) problem, i.e., training on both skill and knowledge components jointly (Caruana, 1997).\nContributions In this paper: 1 We release SKILLSPAN, a novel skill extraction dataset, with annotation guidelines, and our open-source code.1 2 We present strong baselines for the task including a new SpanBERT (Joshi et al., 2020) trained from scratch, and domain-adapted variants (Gururangan et al., 2020), which we will release on the HuggingFace platform (Wolf et al., 2020). To the best of our knowledge, we are the first to investigate the extraction of skills and knowledge from job postings with state-of-the-art language models. 3 We give an analysis on single-task versus multi-task learning in the context of skill extraction, and show that for this particular task single-task learning outperforms multi-task learning."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a pool of prior work relating to SE. We summarize it in Table 1, depicting state-of-the-art approaches, level of annotations, what kind of competences are annotated, the modeling approaches, the size of the dataset (if available), type of skills annotated for, baseline models, and whether they release their annotations and guidelines.\nAs can be seen in Table 1, many works do not release their data (apart from Sayfullina et al., 2018 and Bhola et al., 2020) and none release their annotation guidelines. In addition, none of the previous studies approach SE as a span-level extraction task with state-of-the-art language models, nor did they release a dataset of this magnitude with manually annotated (long) spans of competences by domain experts.\n1https://anonymous.4open.science/r/ SkillSpan-5DB5/README.md\nIn particular, Sayfullina et al. (2018) explored several approaches to SE. To create the data, they extracted all text snippets containing one soft skill from a predetermined list. Crowdworkers then annotated the highlighted skill whether it was a soft skill referring to the candidate or not. They show that an LSTM (Hochreiter et al., 1997) performs best on tagging the skill in the sentence. In our work, we annotated a dataset three times their size (Table 2) for both hard and soft skills with domain experts.\nTamburri et al. (2020) classifies sentences that contain skills in the JP. The authors manually labeled their dataset with domain experts. They annotated whether a sentence contains a skill or not. Once the sentence is identified as containing a skill, the skill cited within is extracted. In contrast, we directly annotate for the span within the sentence.\nBhola et al. (2020) cast the task of skill extraction as a multi-label skill classification at the document-level. There is a predefined set of unique skills given the job descriptions and they predict multiple skills that are connected to a given job description using BERT (Devlin et al., 2019). In addition, they experiment with several additional layers for better prediction performance. We instead explore domain-adaptive pre-training for SE.\nThe work closest to ours is by Chernova (2020), who approach the task similarly with span-level annotations (including longer spans) but approach this for the Finnish language. It is unclear whether they annotated by domain experts. Also, neither the data nor the annotation guidelines are released. For a comprehensive overview with respect to SE, we refer to Khaouja et al. (2021)."
    }, {
      "heading" : "3 Skill & Knowledge Definition",
      "text" : "There is an abundance of competences and there have been large efforts to categorize them. For example, the The International Standard Classification of Occupations (ISCO; Elias, 1997) is one of the main international classifications of occupations and skills. It belongs to the international family of economic and social classifications. Another example, the European Skills, Competences, Qualifications and Occupations (ESCO; le Vrang et al., 2014) taxonomy is the European standard terminology linking skills and competences and qualifications to occupations and derived from ISCO. The ESCO taxonomy mentions three categories of competences: Skill, knowledge, and attitudes. ESCO defines knowledge as follows:\n“Knowledge means the outcome of the assimilation of information through learning. Knowledge is the body of facts, principles, theories and practices that is related to a field of work or study.” 2\nFor example, a person can acquire the Python programming language through learning. This is denoted as a knowledge component and can be considered a hard skill. However, one also needs to be able to apply the knowledge component to a certain task. This is known as a skill component. ESCO formulates it as:\n“Skill means the ability to apply knowledge and use know-how to complete tasks and solve problems.” 3\nIn ESCO, the soft skills are referred to as attitudes. ESCO considers attitudes as skill components:\n“The ability to use knowledge, skills and personal, social and/or methodological abilities, in work or study situations and professional and personal development.” 4\nTo sum up, hard skills are usually referred to as knowledge components, and applying these hard skills to something is considered a skill component. Then, soft skills are referred to as attitudes, these are part of skill components. There has been no work, to the best of our knowledge, in annotating skill and knowledge components in JPs.\n2https://ec.europa.eu/esco/portal/ escopedia/Knowledge\n3https://ec.europa.eu/esco/portal/ escopedia/Skill\n4http://data.europa.eu/esco/skill/A"
    }, {
      "heading" : "4 SKILLSPAN Dataset",
      "text" : "Data5 We continuously collected JPs via web data extraction between June 2020–September 2021. Our JPs come from the three sources:\n1. BIG: A large job platform with various types of JPs, with entry-level to director-level positions;\n2. HOUSE: A static in-house dataset consisting of similar types of jobs as BIG. Dates range from 2012–2020;\n3. TECH: The StackOverflow JP platform that consisted mostly of technical jobs (e.g., developer positions).\nWe plan to release the anonymized raw data and annotations of the parts with permissible licenses, i.e., HOUSE (from a govermental agency which is our collaborator) and TECH. For anonymization, we perform it via manual annotation of job-related sensitive and personal data regarding\n5Our data statement (Bender and Friedman, 2018) can be found in Appendix A.\nOrganization, Location, Contact, and Name following the work by Jensen et al. (2021). Table 2 shows the statistics of SKILLSPAN, with 391 annotated JPs from the three sources containing 14.5K sentences and 232.2K tokens. The unlabeled JPs (only to be released as pre-trained model) consist of 126.8K posts, 3.2M sentences, and 460.5M tokens. What stands out is that there are 2–5 times as many annotated knowledge components in TECH in contrast to the other sources, despite a similar amount of JPs. We expect this to be due the numerous KCs depicted in this domain (e.g., programming languages), while we observe considerably fewer soft skills (e.g., “work flexibly”). The amount of skills is more balanced across the three sources. Furthermore, overlapping spans follow a consistent trend among splits, with the train split containing the most.\nData Annotation We annotate competences related to SKCs in two levels as illustrated in Figure 1. We started the process in March 2021, with initial annotation rounds to construct and refine the annotation guidelines (as outlined further below). The annotation process spanned eight months in total. Our final annotation guidelines can be found in Appendix B. The guidelines were developed by largely following example spans given in the ESCO taxonomy. However, at this stage, we focus on span identification, and we do not take the fine-grained taxonomy codes from ESCO for labeling the spans, leaving the mapping to ESCO and taxonomy enrichment as future work.\nFurther Details on the Annotation Process The development of the annotation guidelines and our\nannotation process can be depicted as follows: 1 We wrote base guidelines derived from a small number of JPs. 2 We had three small pre-rounds consisting of three JPs each. After each round, we modified, improved and finalized the guidelines. 3 Then, we had three larger annotation rounds consisting of 30 JPs each. We re-annotated the previous 11 JPs in 1 and 2 . 4 After these rounds, one of the annotators (the hired linguist) annotated JPs in batches of 50. The data in 1 , 2 , and 3 was annotated by three annotators (101 JPs).\nWe used an open source text annotation tool named DOCCANO (Nakayama et al., 2018). There are around 57.5K tokens (approximately 4.6K sentences, in 101 job posts) that we calculated agreement on. The annotations were compared using Cohen’s κ (Fleiss and Cohen, 1973) between pairs of annotators, and Fleiss’ κ (Fleiss, 1971), which generalises Cohen’s κ to more than two concurrent annotations. We consider two levels of κ calculations: TOKEN is calculated on the token level, comparing the agreement of annotators on each token (including non-entities) in the annotated dataset. SPAN refers to the agreement between annotators on the exact span match over the surface string, regardless of the type of named entity, i.e., we only check the position of tag without regarding the type of the named entity. The observed agreements scores over the three annotators from step 3 are between 0.70–0.75 Fleiss’ κ for both levels of calculation which is considered a substantial agreement (Landis and Koch, 1977) and a κ value greater than 0.81 indicates almost perfect agreement. Given the difficulty of this task, we consider the aforementioned κ score to be strong. Particularly, we observed a large improvement in annotation agreement from the earlier rounds (step 1 and 2 ), where our Fleiss’ κ was 0.59 on token-level and 0.62 for the span-level.\nOverall, we observe higher annotator agreement for knowledge components (3–5% higher) compared to skills which tend to be longer, the TECH domain is the most consistent for agreement while BIG shows more variation over rounds, likely due to the broader nature of the domains of JPs.\nAnnotation Span Statistics A challenge is the length of spans varying in range, SKCs being in different domains (e.g., business versus technical components), and the ambiguity in the skills or knowledge components (e.g., “being able to work together” versus “teamwork”). Figure 2 shows the statistics of our annotations in violin plots. For\nthe training set, the median length (white dot) of skills is around 4 for BIG and HOUSE, for TECH this is a median of 5. In the development set, the median stays at length 4 across all sources. Another notable statistic is the upper and lower percentile lengths of skills and knowledge, indicated with the thick bars. Here, we highlight the fact that skill components could consist of many tokens, for example, up to length 7 in the HOUSE source split (see blue-colored violins). For knowledge components, the spans are usually shorter, where it is consistently below 5 tokens (see orange-colored violins). All statistics follow a similar distribution across train, development, and sources in terms of length and distribution. This gives a further strong indication that consistent annotation length has been conducted across splits and sources.\nQualitative Analysis of Annotations Qualitative differences in SKCs over the three sources are shown (lowercased) in Table 3. With respect to skill components, all sources follow a similar usage of skills. The annotated skills mostly relate to the attitude of a person. This indicates that the skills mostly consist of soft skills. With respect to the knowledge components, we can see consistent patterns between the three different sources. First, on the source-level, the knowledge components vastly differ between BIG and TECH. BIG postings seems to cover more business related components, whereas TECH has more engineering components. HOUSE seems to be a mix of the other two sources. Lastly, note that both the skill and knowledge components between the splits diverge in terms of the type of annotated spans. This indicates the variation of the annotated components. We show the top–10 skills annotated in the train, development, and test splits for SKCs in Appendix C."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "For the task of SE formulated as a sequence labeling problem. Formally, we consider a set of JPs D, where d ∈ D is a set of sequences (i.e., entire JPs) with the ith input sequence X id = {x1, x2, ..., xT } and a target sequence of BIOlabels Y id = {y1, y2, ..., yT } (e.g., “B-SKILL”, “I-KNOWLEDGE”, “O”). The goal is to use D to train a sequence labeling algorithm h : X 7→ Y to accurately predict entity spans by assigning an output label yt for each token xt.\nAs baseline we consider BERT and we investigate more recent variants, and we also train models from scratch. Models are chosen due to their stateof-the-art performance, or in particular, for their strong performance on longer spans.\nBERTbase (Devlin et al., 2019) An out-ofthe-box BERTbase model (bert-base-cased) from the HuggingFace library (Wolf et al., 2020) functioning as a baseline.\nSpanBERT (Joshi et al., 2020) A BERT-style model that focuses on span representations as opposed to single token representations. SpanBERT is trained by masking contiguous spans of tokens and optimizing two objectives: (1) masked language modeling, which predicts each masked token from its own vector representation. (2) The span boundary objective, which predicts each masked token from the representations of the unmasked tokens at the start and end of the masked span.\nWe train a SpanBERTbase model from scratch on the BooksCorpus (Zhu et al., 2015) and English Wikipedia using cased Wordpiece tokens (Wu et al., 2016). We use AdamW (Kingma and Ba, 2015) for 2.4M training steps with batches of 256 sequences of length 512. The learning rate is warmed up for 10K steps to a maximum value of 1e-4, after which it has a decoupled weight decay (Loshchilov and Hutter, 2019) of 0.1. We add a dropout rate of 0.1 across all layers. Pretraining was done on a v3-8 TPU on the GCP and took 14 days to complete. We take the official TensorFlow implementation of SpanBERT by Ram et al. (2021).\nJobBERT We apply domain-adaptive pretraining (Gururangan et al., 2020) to a BERTbase model using the 3.2M unlabeled JP sentences (Table 2). Domain-adaptive pretraining relates to the continued self-supervised pre-training of a large language model on domain-\nspecific text. This approach improves the modeling of text for downstream tasks within the domain. We continue training the BERT model for three epochs (default in HuggingFace) with a batch size of 16.\nJobSpanBERT We also do adaptive pre-training of our SpanBERT on 3.2M unlabeled JP sentences. We keep the parameters identical to the vanilla SpanBERT, but we change the number of steps to 40K to have three passes over the unlabeled data.\nExperiments We have 391 annotated JPs (Table 2) that we divide across three splits: Train, dev. and test set. We use 101 JPs that all three annotators annotated as the gold standard test set with aggregated annotations via majority voting. The 101 postings are divided between the sources as: 36 BIG, 33 HOUSE, and 32 TECH. The remaining 290 JPs were annotated by one annotator. We use 90 JPs (30 from each source, namely BIG, HOUSE, and TECH) as the dev. set. The remaining 200 JPs are used as the train set. The sources in the train set are divided into 60 BIG, 60 HOUSE, and 80 TECH.\nSetup The data is structured as CoNLL format (Tjong Kim Sang, 2002). For the nested annotations, the skill tags are appearing only in the first column and the knowledge tags are only appearing in the second column of the file and they\nare allowed to overlap each other. We perform experiments with single-task learning (STL) on either the skill or knowledge components, MTL for predicting both skill and knowledge tags at the same time, while evaluating the MTL models also on either skills or knowledge components. We used a single joint MTL model with hard-parameter sharing (Caruana, 1997). All models are with a final Conditional Random Field (CRF; Lafferty et al., 2001) layer. Earlier research, such as Souza et al. (2019); Jensen et al. (2021) show that BERT models with a CRF-layer improve or perform similarly to its simpler variants when comparing the overall F1 and make no tagging errors (e.g., B-tag follows I-tag). In the case of MTL we use one for each tag type (skill and knowledge). In the STL experiments we use one CRF for the given tag type.\nWe use the MACHAMP toolkit (van der Goot et al., 2021) for our experiments. For each setup we do five runs (i.e., five random seeds).6 For evaluation we use span-level precision, recall, and F1, where the F1 for the MTL setting is calculated as described in Benikova et al. (2014)."
    }, {
      "heading" : "6 Results",
      "text" : "The results of the experiments can be found in Figure 3. We show the average performance of each\n6For reproducibility, we refer to Appendix D.\nSTL -BER\nT\nSTL -Job\nBER T\nSTL -Job\nSpa nBE\nRT MTL -BER T\nMTL -Job\nBER T\nMTL -Job\nSpa nBE\nRT\nSTL-BERT\nSTL-JobBERT\nSTL-JobSpanBERT\nMTL-BERT\nMTL-JobBERT\nMTL-JobSpanBERT\n1.0 1.0 1.0 1.0 1.0\n0.0 1.0 0.0 0.04 1.0\n0.0 0.0 0.0 0.0 0.91\n0.0 1.0 1.0 1.0 1.0\n0.0 0.96 1.0 0.0 1.0\n0.0 0.0 0.09 0.0 0.0\nSkill Test\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ne_m in\nSTL -BER\nT\nSTL -Job\nBER T\nSTL -Job\nSpa nBE\nRT MTL -BER T\nMTL -Job\nBER T\nMTL -Job\nSpa nBE\nRT\nSTL-BERT\nSTL-JobBERT\nSTL-JobSpanBERT\nMTL-BERT\nMTL-JobBERT\nMTL-JobSpanBERT\n1.0 0.0 0.09 1.0 0.01\n0.0 0.0 0.0 0.0 0.0\n1.0 1.0 1.0 1.0 1.0\n0.91 1.0 0.0 1.0 1.0\n0.0 1.0 0.0 0.0 0.0\n0.99 1.0 0.0 0.0 1.0\nKnowledge Test\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ne_m in\nSTL -BER\nT\nSTL -Job\nBER T\nSTL -Job\nSpa nBE\nRT MTL -BER T\nMTL -Job\nBER T\nMTL -Job\nSpa nBE\nRT\nSTL-BERT\nSTL-JobBERT\nSTL-JobSpanBERT\nMTL-BERT\nMTL-JobBERT\nMTL-JobSpanBERT\n1.0 1.0 0.3 1.0 1.0\n0.0 0.0 0.0 0.0 0.0\n0.0 1.0 0.0 1.0 1.0\n0.7 1.0 1.0 1.0 1.0\n0.0 1.0 0.0 0.0 0.05\n0.0 1.0 0.0 0.0 0.95\nCombined Test\n0.0\n0.2\n0.4\n0.6\n0.8 1.0 e_m\nin\nFigure 4: Almost Stochastic Order Scores of the Test Set. ASO scores expressed in ϵmin. The significance level α = 0.05 is adjusted accordingly by using the Bonferroni correction (Bonferroni, 1936). Read from row to column: E.g., in COMBINED STL-JobBERT (row) is stochastically dominant over STL-BERTbase (column) with ϵmin of 0.00.\nmodel in F1 and respective standard deviation over the development and test split. Exact scores on each source split and other metric details are provided in Appendix E. As mentioned before, we experiment with the following settings: SKILL, we train and predict only on skills. KNOWLEDGE, train and only predict for knowledge. COMBINED, we merge the STL predictions of both skills and knowledge. We also train the models in an MTL setting, predicting both skills and knowledge simultaneously. We evaluate the MTL model on both SKILL and KNOWLEDGE separately, and also compare it against the aggregated STL predictions.\nPerformance on Development Set In Figure 3, we show the results on the development set in the upper plot. We observe similar performance between the domain-adapted STL models—JobBERT and JobSpanBERT—have similar span-F1 for SKILL: 60.05±0.70 vs. 60.07±0.70. In contrast, for KNOWLEDGE, BERTbase and JobBERT are closest in predictive performance: 60.44±0.58 vs. 60.66±0.43. In the COMBINED setting, JobBERT performs highest with a span-F1 of 60.32±0.39. On average, JobBERT performs best over all three settings. Surprisingly, the models for both SKILL and KNOWLEDGE perform similarly (around 60 spanF1), despite the sources’ differences in properties and length Figure 2. In addition, we find that MTL is not performing better than STL across sources, although the tasks are “close”. For exact numbers and source-level (i.e., BIG, HOUSE, TECH), we refer again to Appendix E.\nPerformance on Test Set We select the best performing models in the development set evaluation and apply it to the test set. In Figure 3, in the bottom plot, we show the performance on the test set. Since JobBERT and JobSpanBERT\nare performing similarly, we apply both to the test set and BERTbase. We observe a deviation from the development set to the test set: JobSpanBERT 60.07±0.30→56.64±0.83 on SKILL, JobBERT 60.66±0.43→63.88±0.28 on KNOWLEDGE. For COMBINED, JobBERT performs slightly worse: 60.32±0.39→59.73±0.38. Similar to the development set, we find that on all three methods of evaluation (i.e., SKILL, KNOWLEDGE, and COMBINED), STL still outperforms MTL. For SKILL and KNOWLEDGE, STL is almost stochastically dominant over MTL (i.e., significant), and for COMBINED there is stochastic dominance of STL over MTL, indicated in the next paragraph.\nSignificance We compare all pairs of models based on five random seeds each using Almost Stochastic Order (ASO; Dror et al., 2019) tests with a confidence level of α = 0.05. The ASO scores of the test set are indicated in Figure 4. We show that MTL-JobSpanBERT for SKILL shows almost stochastic dominance (ϵmin < 0.5) over all other models. For KNOWLEDGE and COMBINED, We show that STL-JobBERT is stochastically dominant (ϵmin = 0.0) over all the other models. For more details, we refer to Appendix F for ASO scores on the development set."
    }, {
      "heading" : "7 Discussion",
      "text" : "What Didn’t Work Additionally, we experiment whether representing the entire JP for extracting tokens yields better results than the experiments so far, which were sentence-by-sentence processing setups. To handle entire JPs and hence much longer sequences we use a pre-trained Longformerbase (Beltagy et al., 2020) model. The document length we use in the experiments is 4096 tokens. Results of the Longformer on the test\nset are lower: For skills, JobSpanBERT against Longformer results in 56.64±0.83 vs. 52.55±2.39. For KNOWLEDGE, JobBERT against Longformer shows 63.88±0.28 vs. 57.26±1.05. Last, for COMBINED, JobBERT against Longformer results in 59.73±0.38 vs. 55.05±0.71. This drop in performance is difficult to attribute to a concrete reason: e.g., the Longformer is trained on more varied sources than BERT, but not specifically for JPs, which may have contributed to this gap. Since the vanilla Longformer already performs worse than BERTbase overall, we did not opt to apply domainadaptive pre-training. Overall, we show that representing the full JP is not beneficial for SE.\nDifference in Length of Predictions The main motivation of selecting models optimized for long spans was the length of the annotations (Figure 2). We investigate the average length of predictions of each model (Figure 5) to find out whether the models that are adapted to handle longer sequences truly predict longer spans. Interestingly, the average length of predicted skills are longer than the annotations over all three sources. There is a consistent trend among SKILL: BIG and TECH have similar length over predictions (>4), while HOUSE is usually lower than length 3. For both BIG and TECH, JobSpanBERT predicts the longest skill spans (4.51 and 4.48 respectively). We suspect due to the domain-adaptive pre-training on JPs, it improved the span prediction performance. In contrast, the Longformer predicts shorter spans. Note that the Longformer is not domain-adapted to JPs.\nRegarding KNOWLEDGE, there is also a consis-\ntent trend: BIG has the overall longest prediction length while TECH has the lowest. The Longformer predicts the longest spans on average for BIG and TECH. Knowledge components are representative of a normal-length NER task and might not need a specialized model for long sequences. We show the exact numbers in Table 7 (Appendix E) and the number of predicted SKILL and KNOWLEDGE: JobBERT and JobSpanBERT predict more skills in general than the other models.\nContinuous Pretraining helps SE As previously mentioned, due to the domain specialization of the domain-adapted pre-trained BERT models, they predict more skills and frequently perform better in terms of precision, recall, and F1 as compared to their non-adaptive counterparts. This is especially encouraging as we confirm findings that continuous pre-training helps to adapt models to a specific domain (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020). Particularly in Table 5 on TEST for KNOWLEDGE, BERTbase comes closer in predictive performance to JobBERT (difference of 1.5 F1) than skills. Our intuition is that knowledge components are often already in the pre-training data (e.g., Wikipedia pages of certain competences like Python, Java etc.) and therefore adaptive pre-training does not substantially boost performance."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We present a novel dataset for skill extraction: SKILLSPAN and domain-adapted BERT models: JobBERT and JobSpanBERT. We outline the dataset and guidelines, annotated for hard and soft skills on the span-level by domain experts. Our analysis shows that domain-adaptive pre-training helps to improve performance on the task for both skills and knowledge. Our domain-adapted JobSpanBERT performs best on skills and JobBERT for knowledge. Both models achieve almost stochastic dominance over all other models for skills and knowledge, whereas JobBERT in the MTL setting achieves stochastic dominance over all other models. With the rapid emergence of new competences, our new approach of skill extraction has the potential to enrich knowledge bases such as ESCO with unseen skills or knowledge components in the future, and in general, contribute to providing insights into labor market dynamics, which we leave for future work. We hope this dataset initiates further research in this area."
    }, {
      "heading" : "A Data Statement SKILLSPAN",
      "text" : "Following Bender and Friedman (2018), the following outlines the data statement for SKILLSPAN:\nA. CURATION RATIONALE: Collection of job postings in the English language for spanlevel sequence labeling, to study the impact of sequence labeling on the extraction of skill and knowledge components from job postings.\nB. LANGUAGE VARIETY: The non-canonical data was collected from the StackOverflow job posting platform, an in-house job posting collection from our national labor agency collaboration partner (which will be elaborated upon acceptance), and web extracted job postings from a large job posting platform. US (en-US) and British (en-GB) English are involved.\nC. SPEAKER DEMOGRAPHIC: Gender, age, raceethnicity, socioeconomic status are unknown.\nD. ANNOTATOR DEMOGRAPHIC: Three hired project participants (age range: 25–30), gender: one female and two males, white European and Asian (non-Hispanic). Native language: Danish, Dutch. Socioeconomic status: higher-education students. Female annotator is a professional annotator with a background in Linguistics and the two males with a background in Computer Science.\nE. SPEECH SITUATION: Standard American or British English used in job postings. Time frame of the data is between 2012–2021.\nF. TEXT CHARACTERISTICS: Sentences are from job postings posted on official job vacancy platforms.\nG. RECORDING QUALITY: N/A.\nH. OTHER: N/A.\nI. PROVENANCE APPENDIX: More info will be released upon acceptance."
    }, {
      "heading" : "B Annotation Guidelines",
      "text" : "B.1 Span Specifications\nLegend: Skill , Knowledge , “•” indicates an example sentence.\n1. A skill starts with a VERB, otherwise (ADJECTIVE) + NOUN\n1.1 Modal verbs are not tagged:\n• Can [put personal touch on the menu]SKILL .\n• Will [train new staff]SKILL .\n2. Split up phrases with prepositions and/or conjunctions\n2.1 Unless the conjunction coordinates two nouns functioning as one argument:\n• [Coordinate parties and conferences]SKILL .\n2.2 Do not tag skills with anaphoric pronouns, only tag preceding skill:\n• [Prioritizing tasks]SKILL and identifying those that are most important.\n2.3 Split nouns and adjectives that are coordinated if they do not have a verb attached:\n• Be [inquisitive]SKILL and [proactive]SKILL .\n• Prior in-house experience with [media]KNOWLEDGE , [publishing]KNOWLEDGE or\n[internet companies]KNOWLEDGE .\n2.4 If there is a listing of skill tags and they lead up to different subtasks, we split them:\n• [keep up the high level of quality in our team]SKILL through [reviews]SKILL , [pairing]SKILL and\n[mentoring]SKILL .\n3. If there is relevant information appended after irrelevant information (e.g., info specific to a company) we try to make the skill as short as possible:\n• [providing the best solution]SKILL for Siemens Gamesa in a very [structured]SKILL and\n[analytic]SKILL manner.\n4. Note also the words skills and knowledge can be included in the span of the component if leaving it out makes it nonsensical:\n• [personal skills]SKILL → just [personal] would make it nonsensical.\n5. Parentheses after a skill tag are included if they elaborate the component before them or if they are an abbreviation of the component.\n6. Inclusion of adverbials in components. Adverbials are included if it concerns the manner of doing something. All others are excluded:\n• like to [solve technical challenges independently] SKILL .\n• [communicates openly] SKILL .\n• [striving for the best]SKILL in all that they do.\n• [Deliver first class customer service]SKILL to our guests.\n• [Making the right decisions]SKILL early in the process.\n7. Attitudes as skills. We annotate attitudes as a skill:\n• a [can-do-approach]SKILL → we leave out articles from the attitude.\n8. Attitudes are not tagged if they contain skill/knowledge components—then only the span of the skill is tagged.\n• like to [solve technical challenges independently]SKILL .\n• Passion for [automation]KNOWLEDGE .\n• enjoy [working in a team]SKILL .\n9. Miscellaneous:\n9.1 Do not tag ironic skills (e.g., lazy).\n9.2 Avoid nesting of skills, annotate it as one span.\n9.3 We annotate all skills that are part of sections such as “requirements”, “good-to-haves”, “great-to-knows”, “optionals”, “after this x months of training you’ll be able to...”, “At the job you’re going to...”.\n9.4 When there is a general standard that can be added to the skill, we add these:\n• [Process payments according to the [...] standards]SKILL .\nB.2 Knowledge Specifications\n1. Rule-of-thumb: knowledge is something that one possesses, and cannot (usually) physically execute:\n• [Python]KNOWLEDGE (programming language).\n• [Business]KNOWLEDGE .\n• [Relational Databases]KNOWLEDGE .\n2. If there is a component between parentheses that belongs to the knowledge component, we add it:\n• [(non-) relational databases]KNOWLEDGE .\n• [Driver License (UK/EU)]KNOWLEDGE .\n3. Licenses and certifications: We add the additional words “certificate”, “card”, “license”, et cetera. to the knowledge component.\n4. If the knowledge component looks like a skill, but the preceding verb is vague and empty (e.g., follow, use, comply with, work with) → only tag the knowledge component:\n• Comply with [Food Code of Practice]KNOWLEDGE .\n• Work with [AWS infrastructure]KNOWLEDGE .\n5. We annotate only specified knowledge components:\n• [MongoDB]KNOWLEDGE or other [NoSQL database]KNOWLEDGE .\n• [JEST]KNOWLEDGE or other test libraries. → “other test libraries” is under-specified.\n6. Knowledge components can be nested in skill components.\n• [Design, execution and analysis of [phosphoproteomics]KNOWLEDGE experiments]SKILL .\n7. If all components coordinate/share one knowledge tag, we annotate it as one:\n• [application, data and infrastructure architecture]KNOWLEDGE . → The knowledge tags coordinate to “architecture”.\n• [chemical/biochemical engineering]KNOWLEDGE .\n8. If there is a listing of knowledge tags, we annotate all knowledge tags separately:\n• [Bachelor Degree]KNOWLEDGE in [Mathematics]KNOWLEDGE , [Computer Science]KNOWLEDGE , or\n[Engineering]KNOWLEDGE .\nB.3 Other Specifications 1. Rule-of-thumb: If in doubt, annotate it as a skill. 2. We are preferring skills over knowledge components.\n3. We prioritize skills over attitudes; if there is a skill within the attitude, only tag the skill:\n• Passionate around [solving business problems]SKILL through\n[innovation & engineering practices]KNOWLEDGE .\n4. Skill or knowledge components in the top headlines of the JP are not tagged (e.g., title of a JP). If it is a sub-headline or in the rest of the posting, tag it.\n5. We try to keep the skill/knowledge components as short as possible (i.e., exclude information at the end if it makes it too specific for the job).\n6. We do not include “fluff” and “triggers” (i.e., words that indicate a skill or knowledge component will follow: “advanced knowledge of [...]KNOWLEDGE ”) around the components, including degree. This goes for both before and after:\n• Working proficiency in [developmental toolsets]KNOWLEDGE .\n• Advanced knowledge of [application data and architecture infrastructure]KNOWLEDGE disciplines.\n• [Manual handling]SKILL tasks.\n• [CI/CD]KNOWLEDGE experience.\n• You master [English]KNOWLEDGE on level C1.\n• Proficient in [Python]KNOWLEDGE and [English]KNOWLEDGE .\n• Fluent in spoken and written [English]KNOWLEDGE .\n7. Pay attention to expressions such as “participation in...”, “contributing”, and “transfer (knowledge)”. These are usually not considered skills.\n• Contribute to the enjoyable and collaborative work environment.\n• Participation in the Department’s regular research activities.\n• Desire to be part of something meaningful and innovative.\n8. Skills and Knowledge components that are found in not-so-straightforward places (e.g., project descriptions) are annotated as well, if they relate to the position.\n9. In the pattern of “skill” followed by some elaboration, see if it can be annotated with a skill and a knowledge tag:\n• [Ensure food storage and preparation areas are maintained]SKILL according to\n[Health & Safety and Audit standards]KNOWLEDGE .\n10. Occupations and positions in companies/academia should be excluded.\n11. If there’s a knowledge/skill component in the position, we exclude it as well.\n• Experienced Java Engineer. → completely untagged.\n12. Only annotate the skills that are related to the position.\n12.1. This includes skills that are specific for the position as well (e.g., skills of a ruminants professor versus math professor).\n12.2 Also skills that the person for the position is expected to do in the future.\n12.3 This does not include skills, knowledge or attitudes describing only the company, the group you will join in the department, and so on. Only annotate if it is specified or implied that the employee should possess the skill as well.\n13. We annotate industries and fields (that the employee will be working in) as knowledge components."
    }, {
      "heading" : "C Type of Skills Annotated",
      "text" : "In both Table 8 and Table 9, we show the top-10 skill and knowledge components that have been annotated. We split the top-10 among the data splits (i.e., train, development, and test set), and also between source splits (i.e., BIG, HOUSE, TECH)."
    }, {
      "heading" : "D Reproducibility",
      "text" : "We use the default hyperparameters in MACHAMP (van der Goot et al., 2021) as shown in Table 4. For more details we refer to their paper. For the five random seeds we use 3477689, 4213916, 6828303, 8749520, and 9364029. All experiments with MACHAMP were ran on an NVIDIA® TITAN X (Pascal) 12 GB GPU and an Intel® Xeon® Silver 4214 CPU."
    }, {
      "heading" : "E Exact Number of Performance",
      "text" : "In Table 5, we show the exact numbers of the plot indicated in Figure 3. In addition, we also show the results of each respective split.\nFor the STL models, we observe differences in performances over the sources which is particularly pronounced for knowledge components: The TECH source is the easiest to process (and has most SKCs), while SKCs identification performance is the lowest for BIG. This might be due to the broad nature of this source.\nIn the exact results table (Table 5) we add a (†) next to the highest span-F1 if the model is truly stochastically dominant (ϵmin = 0.0) over all the other models. (*) denotes that the best model achieved almost stochastic dominance (ϵmin < 0.5) over—at minimum—one other model (e.g., in TEST rows w.r.t COMBINED: MTL-JobBERT ⪰ MTL-JobSpanBERT with ϵmin = 0.06) and stochastically dominant over the rest.\nIn Table 6, we report the precision and recall of the models, SKILL and KNOWLEDGE show the precision and recall of the STL models. MULTI shows the precision and recall of the MTL models.\nLast, in Table 7, we show the exact numbers of the length of predictions Figure 5. We also add the number of predicted SKILL and KNOWLEDGE Overall, JobBERT and JobSpanBERT predict more skills in general than the other models. This is also the case for knowledge components. We hypothesize that this might be due to the BERT models now being more specialized towards the JP domain and recognizing more SKCs."
    }, {
      "heading" : "F Significance Testing",
      "text" : "Recently, the ASO test (Dror et al., 2019)7 has been proposed to test statistical significance for deep neural networks over multiple runs. Generally, the ASO test determines whether a stochastic order (Reimers and Gurevych, 2018) exists between two models or algorithms based on their respective sets of evaluation scores. Given the single model scores over multiple random seeds of two algorithms A and B, the method computes a test-specific value (ϵmin) that indicates how far algorithm A is from being significantly better than algorithm B. When distance ϵmin = 0.0, one can claim that A stochastically dominant over B with a predefined significance level. When ϵmin < 0.5 one can say A ⪰ B. On the contrary, when we have ϵmin = 1.0, this means B ⪰ A. For ϵmin = 0.5, no order can be determined. We took 0.05 for the predefined significance level. In Figure 6, we show the ASO scores on the development set.\n7Implementation of Dror et al. (2019) can be found at https://github.com/Kaleidophon/ deep-significance (Ulmer, 2021)\nSk ill\ns\nSr c.\nTr ai\nn D\nev el\nop m\nen t\nTe st\nBIG\nen th\nus ia\nst ic\nam bi\ntio us\ncu st\nom er\nse rv\nic e\nfle xi\nbl e\npr oa\nct iv\ne co\nm m\nun ic\nat or\nte am\npl ay\ner w\nor k\nin de\npe nd\nen tly\nfle xi\nbl e\nfr ie\nnd ly\nat te\nnt io\nn to\nde ta\nil at\nte nt\nio n\nto de\nta il\nat te\nnt io\nn to\nde ta\nil m\not iv\nat ed\nam bi\ntio us\nco m\nm un\nic at\nor re\nlia bl\ne de\nsi gn\nan d\nre fin\ne ev\ner y\nto uc\nhp oi\nnt of\nth e\ncu st\nom er\njo ur\nne y\npa ss\nio na\nte fle\nxi bl\ne en\nab le\nin cl\nus io n co m m un ic at io n w ill in gn es s to le ar n co m m un ic at e\nef fe\nct iv\nel y\nco nfi\nde nt\nse lf\n-m ot\niv at\ned in\nte rp\ner so\nna ls\nki lls\nfle xi\nbl e\nap pr\noa ch\nw or\nk as\npa rt\nof an\nes ta\nbl is\nhe d\nte am\npr oa\nct iv\ne\nHOUSE\nco m\nm un\nic at\nio n\nsk ill\ns st\nru ct\nur ed\nte ac\nhi ng\nm ot\niv at\ned te\nac hi\nng re\nse ar ch st ru ct ur ed co m m un ic at io n sk ill s co m m\nun ic\nat io\nn sk\nill s\npr oa\nct iv\ne pr\noj ec\ntm an\nag em\nen t\nou tg\noi ng\nan al\nyt ic\nal dr\niv e\nfle xi\nbl e\nco m\nm un\nic at\nio n\npr ob\nle m\nso lv\nin g\nen er\nge tic\nse lf\n-d riv\nen co\nm m\nun ic\nat io\nn re\nsp on\nsi bl e te am pl ay er vi si tc us to m er s en th us ia st ic te ac hi ng cu ri ou s te am pl ay er cu ri ou s w or k in de pe nd en tly co m m un ic at\nio n\nTECH\nco m\nm un\nic at\nio n\nsk ill\ns ha\nnd s-\non so\nlv in\ng bu\nsi ne\nss pr\nob le\nm s\npa ss\nio na\nte co\nm m\nun ic\nat io\nn sk\nill s\nap pl\ny yo\nur de\npt h\nof kn\now le\ndg e\nan d\nex pe\nrt is e ap pl y yo ur de pt h of kn ow le dg e an d ex pe rt is e le ad er sh ip pa rt ne rc on tin uo us ly w ith yo ur m an y st ak eh\nol de rs pa rt ne rc on tin uo us ly w ith yo ur m an y st ak eh ol de rs pa ss io na te ac hi ev e or ga ni za tio na lg oa ls so lv in g bu si ne ss pr ob le m s th ro ug h in no va tio n an d en gi ne er in g pr ac tic es op en -m in de d bu ild in g an in no va tiv e cu ltu re w or k in la rg e co lla bo ra tiv e te am s co de re vi ew s st ay fo cu se d on co m m on go al s ha nd son in de pe nd en t w or k in la rg e co lla bo ra tiv e te am s bu ild in g an in no va tiv e cu ltu re so ft w ar e de ve lo pm en t de si gn te am pl ay er pi on ee rn ew ap pr oa ch es de ve lo pm en t de ve lo p an al yt ic al sk ill s co m m un ic at e\nTa bl\ne 8:\nM os\ntF re\nqu en\ntS ki\nlls in\nth e\nD at\na. To\np– 10\nsk ill\nco m\npo ne\nnt s\nin ou\nrd at\na in\nte rm\ns of\nfr eq\nue nc\ny."
    } ],
    "references" : [ {
      "title" : "Publicly available clinical BERT embeddings",
      "author" : [ "Emily Alsentzer", "John Murphy", "William Boag", "WeiHung Weng", "Di Jindi", "Tristan Naumann", "Matthew McDermott." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language Processing Workshop,",
      "citeRegEx" : "Alsentzer et al\\.,? 2019",
      "shortCiteRegEx" : "Alsentzer et al\\.",
      "year" : 2019
    }, {
      "title" : "Expertise retrieval",
      "author" : [ "Krisztian Balog", "Yi Fang", "Maarten De Rijke", "Pavel Serdyukov", "Luo Si." ],
      "venue" : "Foundations and Trends in Information Retrieval, 6(2–3):127–256.",
      "citeRegEx" : "Balog et al\\.,? 2012",
      "shortCiteRegEx" : "Balog et al\\.",
      "year" : 2012
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "ArXiv preprint, abs/2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "NoSta-D named entity annotation for German: Guidelines and dataset",
      "author" : [ "Darina Benikova", "Chris Biemann", "Marc Reznicek." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2524–2531, Reyk-",
      "citeRegEx" : "Benikova et al\\.,? 2014",
      "shortCiteRegEx" : "Benikova et al\\.",
      "year" : 2014
    }, {
      "title" : "Retrieving skills from job descriptions: A language model based extreme multilabel classification framework",
      "author" : [ "Akshay Bhola", "Kishaloy Halder", "Animesh Prasad", "Min-Yen Kan." ],
      "venue" : "Proceedings of the 28th International Conference on Computational",
      "citeRegEx" : "Bhola et al\\.,? 2020",
      "shortCiteRegEx" : "Bhola et al\\.",
      "year" : 2020
    }, {
      "title" : "Teoria statistica delle classi e calcolo delle probabilita",
      "author" : [ "Carlo Bonferroni." ],
      "venue" : "Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze, 8:3–62.",
      "citeRegEx" : "Bonferroni.,? 1936",
      "shortCiteRegEx" : "Bonferroni.",
      "year" : 1936
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Occupational skills extraction with FinBERT",
      "author" : [ "Mariia Chernova." ],
      "venue" : "Master’s Thesis.",
      "citeRegEx" : "Chernova.,? 2020",
      "shortCiteRegEx" : "Chernova.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep dominance - how to properly compare deep neural models",
      "author" : [ "Rotem Dror", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 57th Annual",
      "citeRegEx" : "Dror et al\\.,? 2019",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2019
    }, {
      "title" : "Occupational classification (isco-88): Concepts, methods, reliability, validity and crossnational comparability",
      "author" : [ "Peter Elias." ],
      "venue" : "Technical report, OECD Publishing.",
      "citeRegEx" : "Elias.,? 1997",
      "shortCiteRegEx" : "Elias.",
      "year" : 1997
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin, 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "author" : [ "Joseph L Fleiss", "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 33(3):613–619.",
      "citeRegEx" : "Fleiss and Cohen.,? 1973",
      "shortCiteRegEx" : "Fleiss and Cohen.",
      "year" : 1973
    }, {
      "title" : "Implicit skills extraction using document embedding and its use in job recommendation",
      "author" : [ "Akshay Gugnani", "Hemant Misra." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of",
      "citeRegEx" : "Gugnani and Misra.,? 2020",
      "shortCiteRegEx" : "Gugnani and Misra.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised domain adaptation of contextualized embeddings for sequence labeling",
      "author" : [ "Xiaochuang Han", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Han and Eisenstein.,? 2019",
      "shortCiteRegEx" : "Han and Eisenstein.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber", "Corso Elvezia." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter et al\\.,? 1997",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Large-scale occupational skills normalization for online recruitment",
      "author" : [ "Faizan Javed", "Phuong Hoang", "Thomas Mahoney", "Matt McNair." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, Cal-",
      "citeRegEx" : "Javed et al\\.,? 2017",
      "shortCiteRegEx" : "Javed et al\\.",
      "year" : 2017
    }, {
      "title" : "De-identification of privacy-related entities in job postings",
      "author" : [ "Kristian Nørgaard Jensen", "Mike Zhang", "Barbara Plank." ],
      "venue" : "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 210–221, Reykjavik, Ice-",
      "citeRegEx" : "Jensen et al\\.,? 2021",
      "shortCiteRegEx" : "Jensen et al\\.",
      "year" : 2021
    }, {
      "title" : "Representation of jobskill in artificial intelligence with knowledge graph analysis",
      "author" : [ "Shanshan Jia", "Xiaoan Liu", "Ping Zhao", "Chang Liu", "Lianying Sun", "Tao Peng." ],
      "venue" : "2018 IEEE Symposium on Product Compliance Engineering-Asia (ISPCE-CN), pages 1–6.",
      "citeRegEx" : "Jia et al\\.,? 2018",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2018
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on skill identification from online job ads",
      "author" : [ "Imane Khaouja", "Ismail Kassou", "Mounir Ghogho." ],
      "venue" : "IEEE Access, 9:118134–118153.",
      "citeRegEx" : "Khaouja et al\\.,? 2021",
      "shortCiteRegEx" : "Khaouja et al\\.",
      "year" : 2021
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "A graph-based approach to skill extraction from text",
      "author" : [ "Ilkka Kivimäki", "Alexander Panchenko", "Adrien Dessy", "Dries Verdegem", "Pascal Francq", "Hugues Bersini", "Marco Saerens." ],
      "venue" : "Proceedings of TextGraphs-8 Graph-based Methods for Natural Lan-",
      "citeRegEx" : "Kivimäki et al\\.,? 2013",
      "shortCiteRegEx" : "Kivimäki et al\\.",
      "year" : 2013
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning (ICML",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J Richard Landis", "Gary G Koch." ],
      "venue" : "biometrics, pages 159–174.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "Esco: Boosting job matching in europe with semantic interoperability",
      "author" : [ "Martin le Vrang", "Agis Papantoniou", "Erika Pauwels", "Pieter Fannes", "Dominique Vandensteen", "Johan De Smedt." ],
      "venue" : "Computer, 47(10):57–64.",
      "citeRegEx" : "Vrang et al\\.,? 2014",
      "shortCiteRegEx" : "Vrang et al\\.",
      "year" : 2014
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep job understanding at linkedin",
      "author" : [ "Shan Li", "Baoxu Shi", "Jaewon Yang", "Ji Yan", "Shuai Wang", "Fei Chen", "Qi He." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event,",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning multi-graph neural network for data-driven job skill prediction",
      "author" : [ "Liting Liu", "Wenzheng Zhang", "Jie Liu", "Wenxuan Shi", "Yalou Huang." ],
      "venue" : "2021",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "doccano: Text annotation tool for human",
      "author" : [ "Hiroki Nakayama", "Takahiro Kubo", "Junya Kamura", "Yasufumi Taniguchi", "Xu Liang." ],
      "venue" : "Software available from https://github.com/doccano/doccano.",
      "citeRegEx" : "Nakayama et al\\.,? 2018",
      "shortCiteRegEx" : "Nakayama et al\\.",
      "year" : 2018
    }, {
      "title" : "BERTweet: A pre-trained language model for English tweets",
      "author" : [ "Dat Quoc Nguyen", "Thanh Vu", "Anh Tuan Nguyen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 9–14, On-",
      "citeRegEx" : "Nguyen et al\\.,? 2020",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "Few-shot question answering by pretraining span selection",
      "author" : [ "Ori Ram", "Yuval Kirstain", "Jonathan Berant", "Amir Globerson", "Omer Levy." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Ram et al\\.,? 2021",
      "shortCiteRegEx" : "Ram et al\\.",
      "year" : 2021
    }, {
      "title" : "Why comparing single performance scores does not allow to draw conclusions about machine learning approaches",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "ArXiv preprint, abs/1803.09578.",
      "citeRegEx" : "Reimers and Gurevych.,? 2018",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2018
    }, {
      "title" : "Learning representations for soft skill matching",
      "author" : [ "Luiza Sayfullina", "Eric Malmi", "Juho Kannala." ],
      "venue" : "International Conference on Analysis of Images, Social Networks and Texts, pages 141–152.",
      "citeRegEx" : "Sayfullina et al\\.,? 2018",
      "shortCiteRegEx" : "Sayfullina et al\\.",
      "year" : 2018
    }, {
      "title" : "Salience and market-aware skill extraction for job targeting",
      "author" : [ "Baoxu Shi", "Jaewon Yang", "Feng Guo", "Qi He." ],
      "venue" : "KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020,",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "Syntax-based skill extractor for job advertisements",
      "author" : [ "Ellery Smith", "Martin Braschler", "Andreas Weiler", "Thomas Haberthuer." ],
      "venue" : "2019 6th Swiss Conference on Data Science (SDS), pages 80–81. IEEE.",
      "citeRegEx" : "Smith et al\\.,? 2019",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2019
    }, {
      "title" : "Skill extraction for domain-specific text retrieval in a job-matching platform",
      "author" : [ "Ellery Smith", "Andreas Weiler", "Martin Braschler." ],
      "venue" : "International Conference of the Cross-Language Evaluation Forum for European Languages, pages 116–128. Springer.",
      "citeRegEx" : "Smith et al\\.,? 2021",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2021
    }, {
      "title" : "Portuguese named entity recognition using bert-crf",
      "author" : [ "Fábio Souza", "Rodrigo Nogueira", "Roberto Lotufo." ],
      "venue" : "ArXiv preprint, abs/1909.10649.",
      "citeRegEx" : "Souza et al\\.,? 2019",
      "shortCiteRegEx" : "Souza et al\\.",
      "year" : 2019
    }, {
      "title" : "Dataops for societal intelligence: a data pipeline for labor market skills extraction and matching",
      "author" : [ "Damian A Tamburri", "Willem-Jan Van Den Heuvel", "Martin Garriga." ],
      "venue" : "2020 IEEE 21st International",
      "citeRegEx" : "Tamburri et al\\.,? 2020",
      "shortCiteRegEx" : "Tamburri et al\\.",
      "year" : 2020
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "deep-significance: Easy and Better Significance Testing for Deep Neural Networks",
      "author" : [ "Dennis Ulmer." ],
      "venue" : "https://github.com/Kaleidophon/ deep-significance.",
      "citeRegEx" : "Ulmer.,? 2021",
      "shortCiteRegEx" : "Ulmer.",
      "year" : 2021
    }, {
      "title" : "Massive choice, ample tasks (MaChAmp): A toolkit for multitask learning in NLP",
      "author" : [ "Rob van der Goot", "Ahmet Üstün", "Alan Ramponi", "Ibrahim Sharaf", "Barbara Plank." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Associa-",
      "citeRegEx" : "Goot et al\\.,? 2021",
      "shortCiteRegEx" : "Goot et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human and machine",
      "author" : [ "Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "SKILL: A system for skill identification and normalization",
      "author" : [ "Meng Zhao", "Faizan Javed", "Ferosh Jacob", "Matt McNair." ],
      "venue" : "Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, pages 4012–",
      "citeRegEx" : "Zhao et al\\.,? 2015",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "2015 IEEE Interna-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "SITUATION: Standard American or British English used in job postings. Time frame of the data is between 2012–2021",
      "author" : [ "E. SPEECH" ],
      "venue" : null,
      "citeRegEx" : "SPEECH,? \\Q2021\\E",
      "shortCiteRegEx" : "SPEECH",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al.",
      "startOffset" : 100,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al.",
      "startOffset" : 100,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : ", 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997).",
      "startOffset" : 59,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : ", 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997).",
      "startOffset" : 59,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "Our results show that the domain-adapted models perform best overall, they are significant (Dror et al., 2019) over their non-adapted counterparts, and single-task outperforms multi-task learning.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "Job vacancy data is emerging on a variety of platforms in big quantities, and can provide insights on labor market skill set demands and aid job matching (Balog et al., 2012).",
      "startOffset" : 154,
      "endOffset" : 174
    }, {
      "referenceID" : 36,
      "context" : "Two out of 14 studies release their dataset, which limit themselves to crowd-sourced labels (Sayfullina et al., 2018) or annotations from a predefined list of skills on the document-level (Bhola et al.",
      "startOffset" : 92,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : ", 2018) or annotations from a predefined list of skills on the document-level (Bhola et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "Albeit we additionally study models optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), some underperform.",
      "startOffset" : 61,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "Albeit we additionally study models optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), some underperform.",
      "startOffset" : 61,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "Overall, we find specialized domain BERT models (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020) perform better than their non-adapted counterparts.",
      "startOffset" : 48,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "Overall, we find specialized domain BERT models (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020) perform better than their non-adapted counterparts.",
      "startOffset" : 48,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "Overall, we find specialized domain BERT models (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020) perform better than their non-adapted counterparts.",
      "startOffset" : 48,
      "endOffset" : 136
    }, {
      "referenceID" : 33,
      "context" : "Overall, we find specialized domain BERT models (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020) perform better than their non-adapted counterparts.",
      "startOffset" : 48,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "We explore the benefits of domain-adaptive pretraining on the JP domain (Han and Eisenstein, 2019; Gururangan et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "We explore the benefits of domain-adaptive pretraining on the JP domain (Han and Eisenstein, 2019; Gururangan et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : ", training on both skill and knowledge components jointly (Caruana, 1997).",
      "startOffset" : 58,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "1 2 We present strong baselines for the task including a new SpanBERT (Joshi et al., 2020) trained from scratch, and domain-adapted variants (Gururangan et al.",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : ", 2020) trained from scratch, and domain-adapted variants (Gururangan et al., 2020), which we will release on the HuggingFace platform (Wolf et al.",
      "startOffset" : 58,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "They show that an LSTM (Hochreiter et al., 1997) performs best on tagging the skill in the sentence.",
      "startOffset" : 23,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "There is a predefined set of unique skills given the job descriptions and they predict multiple skills that are connected to a given job description using BERT (Devlin et al., 2019).",
      "startOffset" : 160,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "For example, the The International Standard Classification of Occupations (ISCO; Elias, 1997) is one of the main international classifications of occupations and skills.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "For anonymization, we perform it via manual annotation of job-related sensitive and personal data regarding (5)Our data statement (Bender and Friedman, 2018) can be found in Appendix A.",
      "startOffset" : 130,
      "endOffset" : 157
    }, {
      "referenceID" : 32,
      "context" : "We used an open source text annotation tool named DOCCANO (Nakayama et al., 2018).",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "The annotations were compared using Cohen’s κ (Fleiss and Cohen, 1973) between pairs of annotators, and Fleiss’ κ (Fleiss, 1971), which generalises Cohen’s κ to more than two concurrent annotations.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "The annotations were compared using Cohen’s κ (Fleiss and Cohen, 1973) between pairs of annotators, and Fleiss’ κ (Fleiss, 1971), which generalises Cohen’s κ to more than two concurrent annotations.",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 26,
      "context" : "75 Fleiss’ κ for both levels of calculation which is considered a substantial agreement (Landis and Koch, 1977) and a κ value greater than 0.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "BERTbase (Devlin et al., 2019) An out-ofthe-box BERTbase model (bert-base-cased) from the HuggingFace library (Wolf et al.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "SpanBERT (Joshi et al., 2020) A BERT-style model that focuses on span representations as opposed to single token representations.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 48,
      "context" : "We train a SpanBERTbase model from scratch on the BooksCorpus (Zhu et al., 2015) and English Wikipedia using cased Wordpiece tokens (Wu et al.",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 46,
      "context" : ", 2015) and English Wikipedia using cased Wordpiece tokens (Wu et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 31,
      "context" : "The learning rate is warmed up for 10K steps to a maximum value of 1e-4, after which it has a decoupled weight decay (Loshchilov and Hutter, 2019) of 0.",
      "startOffset" : 117,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "JobBERT We apply domain-adaptive pretraining (Gururangan et al., 2020) to a BERTbase model using the 3.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "We used a single joint MTL model with hard-parameter sharing (Caruana, 1997).",
      "startOffset" : 61,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "All models are with a final Conditional Random Field (CRF; Lafferty et al., 2001) layer.",
      "startOffset" : 53,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "05 is adjusted accordingly by using the Bonferroni correction (Bonferroni, 1936).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Significance We compare all pairs of models based on five random seeds each using Almost Stochastic Order (ASO; Dror et al., 2019) tests with a confidence level of α = 0.",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "To handle entire JPs and hence much longer sequences we use a pre-trained Longformerbase (Beltagy et al., 2020) model.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "This is especially encouraging as we confirm findings that continuous pre-training helps to adapt models to a specific domain (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 214
    }, {
      "referenceID" : 28,
      "context" : "This is especially encouraging as we confirm findings that continuous pre-training helps to adapt models to a specific domain (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 214
    }, {
      "referenceID" : 15,
      "context" : "This is especially encouraging as we confirm findings that continuous pre-training helps to adapt models to a specific domain (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 214
    }, {
      "referenceID" : 33,
      "context" : "This is especially encouraging as we confirm findings that continuous pre-training helps to adapt models to a specific domain (Alsentzer et al., 2019; Lee et al., 2020; Gururangan et al., 2020; Nguyen et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 214
    } ],
    "year" : 0,
    "abstractText" : "Skill Extraction (SE) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowdsourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different sources annotated for hard and soft skills by domain experts. We introduce a BERT baseline (Devlin et al., 2019). To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997). Our results show that the domain-adapted models perform best overall, they are significant (Dror et al., 2019) over their non-adapted counterparts, and single-task outperforms multi-task learning.",
    "creator" : null
  }
}
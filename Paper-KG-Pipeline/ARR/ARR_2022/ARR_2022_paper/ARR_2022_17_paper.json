{
  "name" : "ARR_2022_17_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al., 2018; Rivière et al., 2020; Chung et al., 2019; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Liu et al., 2020b,a; Ravanelli et al., 2020; Ling et al., 2020; Ling and Liu, 2020). Self-supervised learning (SSL) is the main driver of this paradigm, an effective and scalable way to learn high-level\nrepresentation of language that transfers to a variety of tasks. SSL entails learning from the input or some perturbation of it without the need for labelled data. This has unlocked the usage of large amounts of cheaply available unlabelled data. It lends naturally to neural network models that have been shown to possess impressive scaling characteristics such that it is often enough to increase the model and data sizes to improve downstream performance (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019).\nSpeech signal consists of acoustic, linguistic, prosodic, and speaker characteristics. SSL algorithms in speech must be evaluated in their ability to produce representations that are useful for tasks that demand understanding of linguistic, speaker, and prosodic elements of spoken language as well as high-level semantics. Researchers have used auto-regressive, contrastive, discriminative and multi-task learning objectives to pre-train models, and have investigated their capabilities across tasks like phoneme recognition (van den Oord et al., 2018; Chung et al., 2019), automatic speech recognition (ASR) (Liu et al., 2020a; Schneider et al., 2019; Ling and Liu, 2020; Ravanelli et al., 2020; Hsu et al., 2021; Chang et al., 2021), speaker verification (Fan et al., 2020), speaker identification (Chung et al., 2019; Liu et al., 2020b), emotion recognition (Macary et al., 2021), speech translation (Chung et al., 2019), voice conversion (Lin et al., 2020; Huang et al., 2021a), spoken language understanding (Lai et al., 2021), and text-tospeech (Álvarez et al., 2019). However, the methodology in such studies vary in the use of datasets, fine-tuning strategy and task-specific model architecture. To bridge this gap, SUPERB (Yang et al., 2021) introduced a standardized benchmark of 10 speech tasks to compare 13 pre-trained models and a Log Mel-Filterbank baseline. It studied the models’ performance in tasks focusing on linguistic\n(phoneme recognition and automatic speech recognition, keyword spotting and query by example), shallow semantic (intent classification and slot filling), speaker (speaker identification, speaker verification and speaker diarization), and prosodic (emotion recognition) characteristics.\nIn this paper, we introduce SUPERB-SG, a benchmark with 5 new tasks, which are speech translation, out-of-domain ASR, voice conversion, speech separation, and speech enhancement, with an emphasis on evaluating the semantic and generative capabilities of pre-trained models that require high-level representations to capture linguistic, semantic, and speaker characteristics. These tasks go beyond speech recognition by focusing on various other aspects that are essential to building intelligent speech interfaces. Further, we show that while SSL models achieve close to state-of-the-art performance on many tasks, there isn’t one model that outperforms all others, and that a simple Log Mel-Filterbank can perform competitively on some tasks. We also demonstrate the robustness of our methodology with an ablation study over different task-specific model architectures and data sizes.\nThe introduction of these new tasks of varying difficulty takes us closer to a more comprehensive unified standard speech benchmark. We hope that this will motivate the development of more powerful, generalizable, and reusable pre-trained models to democratize the advancement of speech research. To facilitate this, we will release the code and integrate the tasks with the SUPERB benchmark."
    }, {
      "heading" : "2 Related Work",
      "text" : "As more powerful SSL models are proposed with promising performance on various tasks, researchers continually try to find extensive evaluation methods to assess model performance, and wish to holistically understand the capability of the learned representations in these models. SUPERB (Yang et al., 2021) is a framework to benchmark the SSL models on 10 speech tasks by learning task-specific prediction heads on top of the frozen shared SSL models. Although the tasks in SUPERB span across different domains, most of them are simple classification problems, or only require utilization of shallow semantics. In contrast, we focus on harder semantic and generative tasks.\nAnother recently proposed benchmark is the LeBenchmark (Evain et al., 2021), investigating the performance of SSL models trained on French data with three semantic tasks. However, they only consider wav2vec 2.0 (Baevski et al., 2020) with different architectures as their upstream models (i.e., networks pre-trained with SSL). Here, we evaluate a diverse set of SSL models, and offer a more comprehensive analysis.\nThe Zero Resource Speech Benchmark 2021 (Nguyen et al., 2020) introduces unsupervised speech processing tasks, particularly the spoken language modeling problem. They evaluate the SSL models via zero-shot probings at four linguistic levels. While their benchmark task is specific for certain domain, we use various tasks to evaluate different aspects of SSL models.\nThe HEAR 2021 Challenge1 aims to develop general-purpose audio representation by focusing on audio tasks beyond speech that include sound event detection, speech commands and pitch chroma classification. We specifically focus on various aspects of speech processing, thus providing a wide variety of spoken language tasks."
    }, {
      "heading" : "3 SUPERB-SG",
      "text" : ""
    }, {
      "heading" : "3.1 Tasks and Datasets",
      "text" : "This section introduces the tasks in SUPERB-SG, including why we choose these tasks and how we design the task-specific heads for fine-tuning. Following SUPERB’s methodology, we use a lightweight fine-tuning approach wherein we freeze the pre-trained model parameters and only keep\n1https://neuralaudio.ai/hear2021-holistic-evaluation-ofaudio-representations.html\nUpstream Network #Params Stride Input Corpus Pretraining Official Github\nFBANK - 0 10ms waveform - - -\nPASE+ SincNet, 7-Conv, 1-QRNN 7.83M 10ms waveform LS 50 hr multi-task santi-pdp / pase\nAPC 3-GRU 4.11M 10ms FBANK LS 360 hr F-G iamyuanchung / APC VQ-APC 3-GRU 4.63M 10ms FBANK LS 360 hr F-G + VQ iamyuanchung / VQ-APC NPC 4-Conv, 4-Masked Conv 19.38M 10ms FBANK LS 360 hr M-G + VQ Alexander-H-Liu / NPC Mockingjay 12-Trans 85.12M 10ms FBANK LS 360 hr time M-G s3prl / s3prl TERA 3-Trans 21.33M 10ms FBANK LS 960 hr time/freq M-G s3prl / s3prl DeCoAR 2.0 12-Trans 89.84M 10ms FBANK LS 960 hr time M-G + VQ awslabs / speech-representations\nModified CPC 5-Conv, 1-LSTM 1.84M 10ms waveform LL 60k hr F-C facebookresearch / CPC_audio wav2vec 19-Conv 32.54M 10ms waveform LS 960 hr F-C pytorch / fairseq vq-wav2vec 20-Conv 34.15M 10ms waveform LS 960 hr F-C + VQ pytorch / fairseq wav2vec 2.0 Base 7-Conv 12-Trans 95.04M 20ms waveform LS 960 hr M-C + VQ pytorch / fairseq wav2vec 2.0 Large 7-Conv 24-Trans 317.38M 20ms waveform LL 60k hr M-C + VQ pytorch / fairseq HuBERT Base 7-Conv 12-Trans 94.68M 20ms waveform LS 960 hr M-P + VQ pytorch / fairseq HuBERT Large 7-Conv 24-Trans 316.61M 20ms waveform LL 60k hr M-P + VQ pytorch / fairseq\nTable 1: Details of the investigated SSL representations. LibriSpeech and LibriLight are denoted as LS and LL, respectively. For the pretraining methods, we abbreviate \"vector quantization\" as VQ, \"future\" as F, \"masked\" as M, \"generation\" as G, \"contrastive discrimination\" as C, and \"token prediction/classification\" as P. Parameters for both pretraining and inference are counted.\nthe task-specific head’s parameters trainable. This setting serves the dual purpose of evaluating the robustness as well as the generalizability of the speech representations, and provides a resourceefficient way of fine-tuning the models that is inclusive of participants with constrained compute resources. We call the pre-trained model as upstream model and the task-specific heads as downstream model. We now discuss the newly added tasks in SUPERB-SG in the following sub-sections."
    }, {
      "heading" : "3.1.1 Speech Translation",
      "text" : "Speech translation (ST) involves translating the acoustic speech signals in the source language into the words in the target language. We use it to evaluate the semantic capability of SSL models, and how they benefit the translation task. We use the CoVoST2 EnÑDe (Wang et al., 2020) dataset with their official train, validation, and test splits while removing all the samples containing \"REMOVE\", resulting in 425.8, 25.9 and 24.5 hours respectively. For text, we keep original case, normalize punctuation, and build character vocabulary with 100% train-set coverage. We report case-sensitive detokenized BLEU using sacreBLEU (Post, 2018). Our downstream model has an encoder-decoder architecture with 3 layers of Transformers (Vaswani et al., 2017) each. A convolutional sub-sampler is used to reduce the sequence length of the input before feeding it to the encoder. We train our model with label-smoothing using a probability of 0.1. A beam size of 20 is used for inference."
    }, {
      "heading" : "3.1.2 Out-of-domain ASR",
      "text" : "Although an ASR is included in SUPERB, it only examines SSL models on read English corpus LibriSpeech (Panayotov et al., 2015). Therefore, we introduce out-of-domain ASR (OOD-ASR), which aims to evaluate the models’ capabilities across languages, and out-of-domain scenarios. The OODASR tasks are categorized into cross-lingual and spontaneous speech tasks. For the cross-lingual tasks, we choose the Mexican Spanish (es), Mandarin (zh), and Arabic (ar) subsets from Common Voice 7.0 (Ardila et al., 2020) containing 21.5, 31.2, and 30.7 hours of training data respectively. The validation set sizes are 1.2 hours, 14.4 hours and 12.24 hours, and the test set sizes are 0.6 hour, 15.3 hours and 12.5 hours for es, zh and ar respectively. For the spontaneous speech task (spon), we use the Santa Barbara Corpus of Spoken American English (SBCSAE) (Du Bois et al., 2000 – 2005), consisting of 60 conversations over different topics spanning 16.7 hours of data. The validation and test set sizes are 1.6 hours and 2.2 hours respectively. For evaluation, we use word error rate (WER) as the metric except for Mandarin which character error rate (CER) is used. The error rates are averaged across all sub-tasks to offer an overall score. The ASR model is a 3-layer BLSTM (Hochreiter and Schmidhuber, 1997) with hidden states of 1024 dimension. The training objective is to minimize the Connectionist Temporal Classification (CTC) loss (Graves et al., 2006). During inference, we use CTC greedy decoding without language model\nre-scoring to simplify the process and to highlight the impact of the learned acoustic representations."
    }, {
      "heading" : "3.1.3 Voice Conversion",
      "text" : "For voice conversion (VC), we consider the intralingual VC task in VCC2020 (Zhao et al., 2020) under the any-to-one (A2O) setting. A2O VC aims to convert speech from any arbitrary speaker into that of a predefined target speaker. We use the task to evaluate the speaker transferability as well as the generalizability of the SSL models. We use 60 utterances from the target speaker that spans 5 minutes for training, and 25 utterances for testing that span 2 minutes. No validation set was used. We use the commonly used mel-cepstrum distortion (MCD), word error rate (WER) and automatic speaker verification (ASV) accept rate from off-theshelf ASR and ASV models as evaluation metrics. The downstream model is trained to reconstruct the acoustic feature from the upstream representations in a target-speaker-dependent manner. In the conversion phase, given the representations extracted by the upstream, the model generates the converted acoustic features, which are then sent to a neural vocoder to synthesize the converted waveform. We adopted Tacotron2 (Shen et al., 2018) as the downstream model, which is an autoregressive network consisting of convolutional and LSTM layers. For the neural vocoder, we used the Hifi-GAN (Kong et al., 2020). We follow an implementation described in (Huang et al., 2021b)."
    }, {
      "heading" : "3.1.4 Speech Separation",
      "text" : "Speech separation (SS) is the task of separating target speech from background interference (Wang and Chen, 2018). It is an important step in speech processing, especially for noisy and multi-speaker scenarios. We investigate the speech separation problem on a dataset simulated from LibriSpeech (Cosentino et al., 2020) and WHAM! (Wichern et al., 2019) noise. We use 16kHz version of the dataset containing 2 speakers, and focus on the mix_clean condition. The train and evaluation sets contain 43.3 and 4.2 hours of speech simulated from LibriSpeech’s train-clean100 and test-clean. This task is used to evaluate the generative capability of SSL models when input is a mixture of acoustic signals. We use the scaleinvariant signal-to-distortion ratio improvement (SI-SDRi) as the evaluation metric. For the downstream model, we use a 3-layer BLSTM model with dimension of 896 for each direction to predict\nthe short-time Fourier transform (STFT) masks for each speaker, and the predictions are transformed back to the time domain using inverse short-time Fourier transform (iSTFT). Permutation invariant training (PIT) (Yu et al., 2017) is performed to optimize the mean square error between the predicted mask and Ideal Non-negative Phase Sensitive Mask (INPSM) (Erdogan et al., 2015; Kolbæk et al., 2017). We choose frequency domain method instead of a time domain based method because of the stride size constraint and computational cost."
    }, {
      "heading" : "3.1.5 Speech Enhancement",
      "text" : "Speech enhancement (SE) is the task of removing background noise from a degraded speech signal, and it aims to improve the perceived quality and intelligibility of the signal. We include this task to evaluate the generative capability under noisy conditions. In SUPERB-SG, we discuss the speech enhancement problem on the VoicebankDEMAND (Veaux et al., 2013) corpus. The train, validation, and test sets contain 8.8, 0.6 and 0.6 hours of speech respectively. Our evaluation metrics are Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI). For the downstream model, we follow the mask-based speech enhancement pipeline in (Kolbæk et al., 2017). A 3-layer BLSTM model similar to the speech separation task is trained to predict the spectral mask for the clean signal. The mean square error between the predicted mask and INPSM is used as the objective."
    }, {
      "heading" : "3.2 Self-supervised Models",
      "text" : "We evaluate the tasks on 15 upstream models that span across different architectures, sizes, and learning objectives. Some of these models also use vector quantization which has an added benefit of signal compression. For grounding, we use Log Mel Filterbank as our baseline. The detailed properties of upstream models are shown in Table 1."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Following SUPERB, we fix upstream models parameters for all downstream tasks’ training. We extract the frame-level representations for each hidden layer of the upstream models from raw waveform, and use a trainable task-specific weightedsum mechanism to summarize all layers’ representations into a sequence of vectors. The summarized representations are then used as the downstream\nmodel’s input. An overview of the training procedure is demonstrated in Figure 1. This procedure is consistent for all experiments, offering a fair and simple evaluation strategy for all upstream models."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Main result",
      "text" : "The results of the upstream models evaluated on SUPERB-SG are shown in Table 2. We only report the averaged WER for OOD-ASR. Full results can be found in Appendix A. For speech-to-text tasks (ST and OOD-ASR), wav2vec 2.0 and HuBERT offer competitive results, while DeCoAR 2.0 shows some improvements. In speech generation tasks (VC, SS, and SE), FBANK yields comparable or superior performance than some SSL models, especially for those metrics that take the quality of the output signal into account. For VC, the 3 reported metrics have the same trend for respective models. Here, vq-wav2vec achieves the best performance on MCD and ASV, while HuBERT performs the best on WER. For SS, Hubert-Large achieves the best performance, followed by Modified CPC. PASE+, which is pre-trained with denoising tasks, performs better than half the SSL models, but this observation doesn’t transfer to the other tasks. For SE, all upstream models perform comparably. The largest gap is only 0.17 in PESQ and 1.1 in STOI.\nOverall, no model outperforms all others on all tasks. However, HuBERT-Large performs most competitively on all downstream tasks, especially those requiring linguistic and semantic signals."
    }, {
      "heading" : "5.2 Correlation between tasks",
      "text" : "We analyze the correlations between tasks in SUPERB-SG to understand the similarity between tasks, and verify if the experimental results agree with the common understanding of related tasks based on shared representation they require.\nTo compute the correlation, we first change all metrics into a higher-better manner. Then, we compute the Spearman’s rank correlation coefficients (Spearman’s ρ) between all pairs of tasks. For multiple metrics contained in a single task, such as MCD/WER/ASV in VC as well as PESQ/STOI in SE, we compute each of them separately.\nTo make our analysis more representative and generalized to all speech domains, we bring back the six tasks from SUPERB (Yang et al., 2021) that are considered representative of the following four domains: (i) Content recognition tasks containing Phoneme Recognition (PR), Automatic Speech Recognition (ASR) (ii) Speaker identity tasks including Identification (SID), Automatic Speaker Verification (ASV) (iii) Semantics task which is Intent Classification (IC) and (iv) Prosodic task which is Emotion Recognition (ER). Together with the 5 tasks introduced in this paper, we show the results of total 11 downstream tasks with the 14 corresponding metrics in Figure 2.\nOverall, results show that all tasks except SS and SE have strong positive correlation among them. One possible explanation for SS and SE not showing strong correlation is that the low-level information closely related to audio signals is more criti-\ncal as they need to reconstruct clean speech from interfering speakers and background noise by estimating the STFT masks. As a result, high-level information extracted from SSL models has little benefit for these tasks but is helpful for other tasks. As noted earlier, there is only a small gap in performance between FBANK and SSL models. If we leave SS and SE out, all correlation coefficients are greater than 0.58, showing that the SSL model representations are useful for multiple domains.\nAlthough the Spearman’s ρ are large in general in Figure 2, differences between tasks are observable. Here, we focus on the relation between correlation and similarity of tasks. We list the most and the least two correlated tasks comparing with ST, OOD-ASR, VC, SS, and SE. SS and SE are skipped as candidates for for the least correlated tasks since they dominate the results. For VC, we average the correlation coefficients across the three metrics. The results are shown in Table 3. ST and OOD-ASR are highly correlated with ASR since they both transform speech signals into discrete text tokens. IC is also correlated with ST since semantic information is required to perform\nboth tasks. Moreover, ASV and VC are the least correlated tasks since they primarily focus on the speaker information with lesser regard to the semantic content. However, the absolute correlation values are still larger than 0.7. For VC, the speaker information needs to be removed while the content has to be kept, similar to PR and ASR but different from SID. SS and SE are correlated with each other and have a much lower correlation with speaker identity and semantics tasks, supporting our assumption. Overall, we find that empirically highly-correlated tasks require similar knowledge or understanding ability.\nTo give a broader view of our correlation results, we further cluster the downstream tasks by their\ncorrelation with each other using K-means. In this way, all the tasks are considered simultaneously, and the grouping is driven automatically by the empirical correlation results. If more than one metric are used in a downstream task, we cluster them independently. The clustering results are shown in Table 4 and a rearranged correlation map is shown in Figure 3. The result shows that the clusters of the tasks align with our empirical knowledge. Cluster A includes tasks that require content information, while tasks in cluster B are more sensitive to speaker and prosodic features. Cluster C contains metrics MCD and ASV of VC, which are used to evaluate the signal quality and the rates of speaker\ntransfer. It is worth noting that WER in VC belongs to cluster A, showing that it is more similar to content-related tasks. Furthermore, clusters D, E, and F each contain one of the metrics in SS and SE, aligning with our assumption that these tasks utilize different types of information compared to other tasks. Overall, the clustering results roughly categorize tasks with respect to the signals they need from the SSL representations.\nWith the analysis of the correlation between tasks, we empirically confirm the reliability of the results, and show that we increase the heterogeneity among speech tasks over SUPERB. We further discover shared properties between tasks with clustering, and the result is aligned with our common understanding of related tasks."
    }, {
      "heading" : "5.3 Robustness of SUPERB-SG",
      "text" : "To study the impact of downstream model architecture and the data sizes used in SUPERB-SG we evaluate the robustness of SUPERB-SG with variations in downstream model as well as training data size, and show that our conclusions still hold true.\nWe choose ST, OOD-ASR and SS as the downstream tasks for evaluation with an aim to cover semantic, content recognition, and generative task types. For the upstream models, FBANK, TERA, CPC, wav2vec 2.0 Base and HuBERT Base are used to cover different SSL algorithms."
    }, {
      "heading" : "5.3.1 Downstream model",
      "text" : "For each task, 2 additional downstream architectures are created by modifying the number of layers and the hidden dimensions compared to our default setting. We create small and large models that are roughly the half and twice of default in terms of the number of trainable parameters. A detailed com-\nparison of the downstream architectures is shown in Table 5. The results are shown in Table 6.\nWe show that the ranking of the upstream models is almost fixed when the model sizes are varied. As expected, the small architecture has worse performance than default, while large has better. Moreover, the scores causing the change in ranking are negligible, e.g., TERA/CPC in SS and wav2vec 2.0 Base/HuBERT Base in OOD-ASR with large. The results show that the relative performance achieved by different upstream models is agnostic to the downstream architecture, confirming the robustness of the framework used in SUPERB-SG."
    }, {
      "heading" : "5.3.2 Training data size",
      "text" : "To study the effect of data size, we create 3 pseudo datasets per task by sub-sampling 10%, 5% and 1% from the original training set while fixing the validation and test sets. The statistics of the datasets are shown in Table 7, and the results are in Table 8.\nThe ranking of the upstream models remains almost the same for 10% of training data. When that is further reduced to 5%, there is a change in ranking in SS due to a performance drop in Modified CPC. Excluding Modified CPC, the ranking is still fixed showing that the relative performance of the upstream models is agnostic to data size.\nFurthermore, when using only 1% of training data, most of the SSL models fail on the 3 downstream tasks. This phenomenon is caused by insufficient task-specific knowledge due to limited training data size. Although SSL models learn highlevel representations from the unlabeled speech signal, acquisition of task-specific knowledge such as translingual ability in ST, text-level token mapping in OOD-ASR, and mask prediction in SS, requires non-trivial supervision.\nWe note that fewer training examples speeds training up but sacrifices the evaluation quality. Overall, we show the robustness of SUPERB-SG\nto variations in data size even when the training data is reduced to 5%, showing the reliability of the benchmark."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduce SUPERB-SG, a set of 5 new tasks that include speech translation, out-of-domain ASR, voice conversion, speech separation, and speech enhancement to evaluate the deep semantic and generative capabilities of SSL models. We evaluate 15 SSL models, and do a comprehensive analysis of the task correlations to demonstrate the reliability of our methodology. We test and confirm the robustness of SUPERB-SG in terms of the downstream model architecture as well as the training data size. The latest introduction of the semantic and generative tasks increases the diversity and difficulty of SUPERB, which can boost a more comprehensive understanding of the capability of various SSL models’ representations, and help researchers discover the hidden properties of SSL techniques in development."
    }, {
      "heading" : "A Complete Out-of-domain ASR Results",
      "text" : "Here, we provide complete results of OOD-ASR tasks, as shown in Tables 9, 10, 11. All upstream models used in this paper are trained with English speech data, but we are also interested in multilingual pre-trained models in OOD-ASR. Therefore, we evaluate the wav2vec 2.0 XLSR model on the OOD-ASR tasks, as shown in the last row of Table 9. XLSR has identical architecture as wav2vec 2.0 Large, but is trained with 56k hours of speech including 53 different languages. The pre-training data of XLSR cover our cross-lingual tasks’ training data. As expected, using multilingual data improves OOD-ASR tasks and achieves the best performance among all upstream models."
    } ],
    "references" : [ {
      "title" : "Problem-agnostic speech embeddings for multi-speaker text-to-speech with samplernn",
      "author" : [ "David Álvarez" ],
      "venue" : "Proc. 10th ISCA Speech Synthesis Workshop, pages 35–39.",
      "citeRegEx" : "Álvarez,? 2019",
      "shortCiteRegEx" : "Álvarez",
      "year" : 2019
    }, {
      "title" : "Common voice: A massivelymultilingual speech corpus",
      "author" : [ "Rosana Ardila", "Megan Branson", "Kelly Davis", "Michael Kohler", "Josh Meyer", "Michael Henretty", "Reuben Morais", "Lindsay Saunders", "Francis Tyers", "Gregor Weber." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Ardila et al\\.,? 2020",
      "shortCiteRegEx" : "Ardila et al\\.",
      "year" : 2020
    }, {
      "title" : "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "author" : [ "Alexei Baevski", "Yuhao Zhou", "Abdelrahman Mohamed", "Michael Auli" ],
      "venue" : "NeurIPS",
      "citeRegEx" : "Baevski et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2020
    }, {
      "title" : "DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT",
      "author" : [ "Heng-Jui Chang", "Shu-wen Yang", "Hung-yi Lee." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Chang et al\\.,? 2021",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2021
    }, {
      "title" : "An Unsupervised Autoregressive Model for Speech Representation Learning",
      "author" : [ "Yu-An Chung", "Wei-Ning Hsu", "Hao Tang", "James Glass." ],
      "venue" : "Interspeech, pages 146–150.",
      "citeRegEx" : "Chung et al\\.,? 2019",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Conneau et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Librimix: An open-source dataset for generalizable speech separation",
      "author" : [ "Joris Cosentino", "Manuel Pariente", "Samuele Cornell", "Antoine Deleforge", "Emmanuel Vincent." ],
      "venue" : "arXiv preprint arXiv:2005.11262.",
      "citeRegEx" : "Cosentino et al\\.,? 2020",
      "shortCiteRegEx" : "Cosentino et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks",
      "author" : [ "Hakan Erdogan", "John R Hershey", "Shinji Watanabe", "Jonathan Le Roux." ],
      "venue" : "2015 IEEE International Conference on Acoustics, Speech and Signal",
      "citeRegEx" : "Erdogan et al\\.,? 2015",
      "shortCiteRegEx" : "Erdogan et al\\.",
      "year" : 2015
    }, {
      "title" : "Lebenchmark: A reproducible framework for assessing self-supervised representation learning from speech",
      "author" : [ "Fabien Ringeval", "Didier Schwab", "Laurent Besacier" ],
      "venue" : null,
      "citeRegEx" : "Ringeval et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ringeval et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring wav2vec 2.0 on speaker verification and language identification",
      "author" : [ "Zhiyun Fan", "Meng Li", "Shiyu Zhou", "Bo Xu" ],
      "venue" : "arXiv preprint arXiv:2012.06185",
      "citeRegEx" : "Fan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2020
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of the 23rd international conference on Ma-",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep learning scaling is predictable, empirically",
      "author" : [ "Joel Hestness", "Sharan Narang", "Newsha Ardalani", "Gregory Diamos", "Heewoo Jun", "Hassan Kianinejad", "Md. Mostofa Ali Patwary", "Yang Yang", "Yanqi Zhou." ],
      "venue" : "arXiv preprint arXiv:1712.00409.",
      "citeRegEx" : "Hestness et al\\.,? 2017",
      "shortCiteRegEx" : "Hestness et al\\.",
      "year" : 2017
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "author" : [ "Wei-Ning Hsu", "Benjamin Bolte", "Yao-Hung Hubert Tsai", "Kushal Lakhotia", "Ruslan Salakhutdinov", "Abdelrahman Mohamed." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Hsu et al\\.,? 2021",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2021
    }, {
      "title" : "Any-to-One Sequence-to-Sequence Voice Conversion using Self-Supervised Discrete Speech Representations",
      "author" : [ "W.-C. Huang", "Y.-C. Wu", "T. Hayashi", "T. Toda." ],
      "venue" : "Proc. ICASSP, pages 5944– 5948.",
      "citeRegEx" : "Huang et al\\.,? 2021a",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "S3prl-vc: Open-source voice conversion framework with self-supervised speech representations",
      "author" : [ "Wen-Chin Huang", "Shu-Wen Yang", "Tomoki Hayashi", "Hung-Yi Lee", "Shinji Watanabe", "Tomoki Toda." ],
      "venue" : "arXiv preprint arXiv:2110.06280.",
      "citeRegEx" : "Huang et al\\.,? 2021b",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu." ],
      "venue" : "arXiv preprint arXiv:1602.02410.",
      "citeRegEx" : "Jozefowicz et al\\.,? 2016",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks",
      "author" : [ "Morten Kolbæk", "Dong Yu", "Zheng-Hua Tan", "Jesper Jensen." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Pro-",
      "citeRegEx" : "Kolbæk et al\\.,? 2017",
      "shortCiteRegEx" : "Kolbæk et al\\.",
      "year" : 2017
    }, {
      "title" : "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "author" : [ "J. Kong", "J. Kim", "J. Bae." ],
      "venue" : "Proc. NeurIPS, volume 33, pages 17022–17033. 9",
      "citeRegEx" : "Kong et al\\.,? 2020",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "Semi-supervised spoken language understanding via self-supervised speech and language model pretraining",
      "author" : [ "Cheng-I Lai", "Yung-Sung Chuang", "Hung-Yi Lee", "ShangWen Li", "James Glass." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Lai et al\\.,? 2021",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2021
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Fragmentvc: Anyto-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention",
      "author" : [ "Yist Y Lin", "Chung-Ming Chien", "Jheng-Hao Lin", "Hungyi Lee", "Lin-shan Lee." ],
      "venue" : "arXiv preprint arXiv:2010.14150.",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep contextualized acoustic representations for semi-supervised speech recognition",
      "author" : [ "Shaoshi Ling", "Yuzong Liu", "Julian Salazar", "Katrin Kirchhoff." ],
      "venue" : "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Ling et al\\.,? 2020",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2020
    }, {
      "title" : "Tera: Self-supervised learning of transformer encoder representation for speech",
      "author" : [ "Andy T Liu", "Shang-Wen Li", "Hung-yi Lee." ],
      "venue" : "arXiv preprint arXiv:2007.06028.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "author" : [ "Andy T. Liu", "Shu-wen Yang", "Po-Han Chi", "Po-chun Hsu", "Hung-yi Lee." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "On the use of selfsupervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "author" : [ "Manon Macary", "Marie Tahon", "Yannick Estève", "Anthony Rousseau." ],
      "venue" : "2021 IEEE Spoken Language Technology Workshop",
      "citeRegEx" : "Macary et al\\.,? 2021",
      "shortCiteRegEx" : "Macary et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the limits of weakly supervised pretraining",
      "author" : [ "Dhruv Mahajan", "Ross Girshick", "Vignesh Ramanathan", "Kaiming He", "Manohar Paluri", "Yixuan Li", "Ashwin Bharambe", "Laurens van der Maaten." ],
      "venue" : "Proceedings of the European Conference on Com-",
      "citeRegEx" : "Mahajan et al\\.,? 2018",
      "shortCiteRegEx" : "Mahajan et al\\.",
      "year" : 2018
    }, {
      "title" : "The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling",
      "author" : [ "Tu Anh Nguyen" ],
      "venue" : "NeurIPS Workshop on Self-Supervised Learning for Speech and Audio Processing.",
      "citeRegEx" : "Nguyen,? 2020",
      "shortCiteRegEx" : "Nguyen",
      "year" : 2020
    }, {
      "title" : "Librispeech: an asr corpus based on public domain audio books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206–5210.",
      "citeRegEx" : "Panayotov et al\\.,? 2015",
      "shortCiteRegEx" : "Panayotov et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "NAACL, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Belgium, Brussels. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task self-supervised learning for robust speech recognition",
      "author" : [ "Mirco Ravanelli", "Jianyuan Zhong", "Santiago Pascual", "Pawel Swietojanski", "Joao Monteiro", "Jan Trmal", "Yoshua Bengio." ],
      "venue" : "ICASSP, pages 6989–6993.",
      "citeRegEx" : "Ravanelli et al\\.,? 2020",
      "shortCiteRegEx" : "Ravanelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised pretraining transfers well across languages",
      "author" : [ "Morgane Rivière", "Armand Joulin", "Pierre-Emmanuel Mazaré", "Emmanuel Dupoux." ],
      "venue" : "ICASSP, pages 7414–7418.",
      "citeRegEx" : "Rivière et al\\.,? 2020",
      "shortCiteRegEx" : "Rivière et al\\.",
      "year" : 2020
    }, {
      "title" : "wav2vec: Unsupervised pre-training for speech recognition",
      "author" : [ "Steffen Schneider", "Alexei Baevski", "Ronan Collobert", "Michael Auli." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Schneider et al\\.,? 2019",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2019
    }, {
      "title" : "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "author" : [ "Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1701.06538.",
      "citeRegEx" : "Shazeer et al\\.,? 2017",
      "shortCiteRegEx" : "Shazeer et al\\.",
      "year" : 2017
    }, {
      "title" : "Natural TTS Synthesis by Conditioning WaveNet on MEL Spectrogram Predictions",
      "author" : [ "J. Shen", "R. Pang", "R.J. Weiss", "M. Schuster", "N. Jaitly", "Z. Yang", "Z. Chen", "Y. Zhang", "Y. Wang", "R. SkerryRyan", "R.A. Saurous", "Y. Agiomyrgiannakis", "Y. Wu." ],
      "venue" : "In",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aäron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "CoRR, abs/1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "The voice bank corpus: Design, collection and data analysis of a large regional accent speech database",
      "author" : [ "Christophe Veaux", "Junichi Yamagishi", "Simon King." ],
      "venue" : "2013 international conference oriental COCOSDA held jointly with 2013 conference on",
      "citeRegEx" : "Veaux et al\\.,? 2013",
      "shortCiteRegEx" : "Veaux et al\\.",
      "year" : 2013
    }, {
      "title" : "CoVoST 2: A massively multilingual speech-to-text translation corpus",
      "author" : [ "Changhan Wang", "Anne Wu", "Juan Pino" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised speech separation based on deep learning: An overview",
      "author" : [ "DeLiang Wang", "Jitong Chen." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(10):1702– 1726.",
      "citeRegEx" : "Wang and Chen.,? 2018",
      "shortCiteRegEx" : "Wang and Chen.",
      "year" : 2018
    }, {
      "title" : "Wham!: Extending speech separation to noisy environments",
      "author" : [ "Gordon Wichern", "Joe Antognini", "Michael Flynn", "Licheng Richard Zhu", "Emmett McQuinn", "Dwight Crow", "Ethan Manilow", "Jonathan Le Roux." ],
      "venue" : "arXiv preprint arXiv:1907.01160.",
      "citeRegEx" : "Wichern et al\\.,? 2019",
      "shortCiteRegEx" : "Wichern et al\\.",
      "year" : 2019
    }, {
      "title" : "SUPERB: Speech processing universal performance benchmark",
      "author" : [ "Shang-Wen Li", "Shinji Watanabe", "Abdelrahman Mohamed", "Hung yi Lee." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "arXiv preprint arXiv:1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Permutation invariant training of deep models for speaker-independent multi-talker speech separation",
      "author" : [ "Dong Yu", "Morten Kolbæk", "Zheng-Hua Tan", "Jesper Jensen." ],
      "venue" : "2017 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Voice Conversion Challenge 2020 - Intra-lingual semi-parallel and cross-lingual voice conversion ",
      "author" : [ "Y. Zhao", "W.-C. Huang", "X. Tian", "J. Yamagishi", "R.K. Das", "T. Kinnunen", "Z. Ling", "T. Toda." ],
      "venue" : "Proc. Joint Workshop for the BC and VCC 2020,",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 7,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 28,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 22,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 8,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 49,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 36,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 23,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 5,
      "context" : "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing (NLP) (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Dong et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2019; Conneau et al., 2020), and speech processing (van den Oord et al.",
      "startOffset" : 119,
      "endOffset" : 298
    }, {
      "referenceID" : 4,
      "context" : "Researchers have used auto-regressive, contrastive, discriminative and multi-task learning objectives to pre-train models, and have investigated their capabilities across tasks like phoneme recognition (van den Oord et al., 2018; Chung et al., 2019), automatic speech recognition (ASR) (Liu et al.",
      "startOffset" : 202,
      "endOffset" : 249
    }, {
      "referenceID" : 26,
      "context" : ", 2019), automatic speech recognition (ASR) (Liu et al., 2020a; Schneider et al., 2019; Ling and Liu, 2020; Ravanelli et al., 2020; Hsu et al., 2021; Chang et al., 2021), speaker verification (Fan et al.",
      "startOffset" : 44,
      "endOffset" : 169
    }, {
      "referenceID" : 39,
      "context" : ", 2019), automatic speech recognition (ASR) (Liu et al., 2020a; Schneider et al., 2019; Ling and Liu, 2020; Ravanelli et al., 2020; Hsu et al., 2021; Chang et al., 2021), speaker verification (Fan et al.",
      "startOffset" : 44,
      "endOffset" : 169
    }, {
      "referenceID" : 37,
      "context" : ", 2019), automatic speech recognition (ASR) (Liu et al., 2020a; Schneider et al., 2019; Ling and Liu, 2020; Ravanelli et al., 2020; Hsu et al., 2021; Chang et al., 2021), speaker verification (Fan et al.",
      "startOffset" : 44,
      "endOffset" : 169
    }, {
      "referenceID" : 15,
      "context" : ", 2019), automatic speech recognition (ASR) (Liu et al., 2020a; Schneider et al., 2019; Ling and Liu, 2020; Ravanelli et al., 2020; Hsu et al., 2021; Chang et al., 2021), speaker verification (Fan et al.",
      "startOffset" : 44,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : ", 2019), automatic speech recognition (ASR) (Liu et al., 2020a; Schneider et al., 2019; Ling and Liu, 2020; Ravanelli et al., 2020; Hsu et al., 2021; Chang et al., 2021), speaker verification (Fan et al.",
      "startOffset" : 44,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : ", 2021), speaker verification (Fan et al., 2020), speaker identification (Chung et al.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : ", 2020), speaker identification (Chung et al., 2019; Liu et al., 2020b), emotion recognition (Macary et al.",
      "startOffset" : 32,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : ", 2020), speaker identification (Chung et al., 2019; Liu et al., 2020b), emotion recognition (Macary et al.",
      "startOffset" : 32,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : ", 2020b), emotion recognition (Macary et al., 2021), speech translation (Chung et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : ", 2021), speech translation (Chung et al., 2019), voice conversion (Lin et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : ", 2019), voice conversion (Lin et al., 2020; Huang et al., 2021a), spoken language understanding (Lai et al.",
      "startOffset" : 26,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : ", 2019), voice conversion (Lin et al., 2020; Huang et al., 2021a), spoken language understanding (Lai et al.",
      "startOffset" : 26,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : ", 2021a), spoken language understanding (Lai et al., 2021), and text-tospeech (Álvarez et al.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "0 (Baevski et al., 2020) with different architectures as their upstream models (i.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 45,
      "context" : "We use the CoVoST2 EnÑDe (Wang et al., 2020) dataset with their official train, validation, and test splits while removing all the samples containing \"REMOVE\", resulting in 425.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 34,
      "context" : "We report case-sensitive detokenized BLEU using sacreBLEU (Post, 2018).",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 43,
      "context" : "Our downstream model has an encoder-decoder architecture with 3 layers of Transformers (Vaswani et al., 2017) each.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "Although an ASR is included in SUPERB, it only examines SSL models on read English corpus LibriSpeech (Panayotov et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "The ASR model is a 3-layer BLSTM (Hochreiter and Schmidhuber, 1997) with hidden states of 1024 dimension.",
      "startOffset" : 33,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "The training objective is to minimize the Connectionist Temporal Classification (CTC) loss (Graves et al., 2006).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 51,
      "context" : "For voice conversion (VC), we consider the intralingual VC task in VCC2020 (Zhao et al., 2020) under the any-to-one (A2O) setting.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 41,
      "context" : "We adopted Tacotron2 (Shen et al., 2018) as the downstream model, which is an autoregressive network consisting of convolutional and LSTM layers.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "For the neural vocoder, we used the Hifi-GAN (Kong et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "We follow an implementation described in (Huang et al., 2021b).",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 46,
      "context" : "Speech separation (SS) is the task of separating target speech from background interference (Wang and Chen, 2018).",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "We investigate the speech separation problem on a dataset simulated from LibriSpeech (Cosentino et al., 2020) and WHAM! (Wichern et al.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 50,
      "context" : "Permutation invariant training (PIT) (Yu et al., 2017) is performed to optimize the mean square error between the predicted mask and Ideal Non-negative Phase Sensi-",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 44,
      "context" : "speech enhancement problem on the VoicebankDEMAND (Veaux et al., 2013) corpus.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "For the downstream model, we follow the mask-based speech enhancement pipeline in (Kolbæk et al., 2017).",
      "startOffset" : 82,
      "endOffset" : 103
    } ],
    "year" : 0,
    "abstractText" : "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pretrained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focused on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pretrained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_126_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MReD: A Meta-Review Dataset for Structure-Controllable Text Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Text generation entered a new era because of the development of neural network based generation techniques. Along the dimension of the mapping relation between the input information and the output text, we can roughly group the recent tasks into three clusters: more-to-less, less-to-more, and neck-to-neck. The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization\n1We will release our code and data at “anonymous URL”.\n(Tan et al., 2017; Kryściński et al., 2018). The lessto-more generation tasks generate a more abundant output from some obviously simpler input, such as prompt-based story generation (Fan et al., 2018b). The neck-to-neck generation aims at generating an output text which conveys the same quantity of knowledge as the input but in natural language, such as typical RDF triples to text tasks (Gardent et al., 2017).\nTo some extent, the existing task settings are not so adequate because they do not have deep understanding of the domains they are working on, i.e., domain knowledge. Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and news titles. However, it does not tell why a particular piece of news content should have that corresponding title, for example for the same earnings report, why one media emphasizes its new business success in the title, but another emphasizes its net income. Obviously, there is not a standard answer regarding right or wrong. For such cases, if we can specify a control signal, e.g., “emphasizing new business”, the generated text would make more sense to users using the text generator.\nTo allow controlling not only the intent of a single generated sentence but also the whole structure of a generated passage, we prepare a new dataset MReD (short for Meta-Review Dataset) with in-depth understanding of the structure of meta-reviews in a peer-reviewing system, namely the open review system of ICLR. MReD for the first time allows a generator to be trained by simultaneously taking the text (i.e. reviews) and the structure control signal as input to generate a meta-review which is not only derivable from the reviews but also complies with the control intent. Thus from the same input text, the trained generator can generate varied outputs according to the given control signal. For example, if the area chair is inclined to accept a borderline paper, he or she may invoke our generator with a structure of “abstract | strength | decision” to generate a meta-review, or may use a structure of “abstract | weakness | suggestion” otherwise. Note that for ease of preparation and explanation, we ground our dataset in the peer review domain. However, the data preparation methodology and proposed models are transferable to other domains, which is indeed what we hope to motivate with this effort.\nSpecifically, we collect 7,089 meta-reviews of ICLR in recent years (2018 - 2021) and fully annotate the dataset. Each sentence in a meta-review is classified into one of the 9 pre-defined intent categories: abstract, strength, weakness, rating summary, area chair (AC) disagreement, rebuttal process, suggestion, decision, and miscellaneous (misc). Table 1 shows an annotated example, where each sentence is classified into a single category that best describes the intent of this sentence. Our MReD is obviously different from previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage. Our proposed task is also noticeably different from existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017; Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.\nTo summarize, our contributions are as follows. (1) We introduce a fully-annotated meta-review dataset to make better use of the domain knowledge for text generation. Thorough data analysis\nprovides useful insights into the domain characteristics. (2) We propose a new task of controllable generation focusing on controlling the passage macro structures. It offers stronger generation flexibility and applicability for practical use cases. (3) We design simple yet effective control methods that are independent of the model architecture. We show the effectiveness of enforcing different generation structures with a detailed model analysis. We will release our full dataset, code, and detailed settings to the community."
    }, {
      "heading" : "2 MReD: Meta-Review Dataset",
      "text" : "In this paper, we explore a new task, named the structure-controllable text generation, in a new domain, namely the meta-reviews in the peer reviewing system. Unlike previous datasets that mainly focus on domains like news, meta-review is a worthstudying domain containing essential and highdensity opinions. Specifically, during the peer review process of scientific papers, a senior reviewer or area chair will recommend a decision and manually write a meta-review to summarize the opinions from different reviews written by the reviewers. We first introduce the data collection process and then describe the annotation details, followed by dataset analysis."
    }, {
      "heading" : "2.1 Data Collection",
      "text" : "We collect the meta-review related data from an online peer reviewing platform for ICLR 2 from 2018 to 2021. Note that the submissions from earlier years are not collected because their meta-reviews are not released. To prepare our dataset for controllable text generation, for each submission, we collect multiple reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage. Table 2 shows the statistics of data collected from each year. Initially, 7,894 submissions are collected. After filtering, 7,089 meta-reviews are retained with their corresponding 23,675 reviews. Note that even without\n2https://openreview.net/\nany further annotation, the dataset can already naturally serve the purpose of multi-document summarization (MDS). Compared with those conventional datasets for MDS, such as TAC (Owczarzak and Dang, 2011) and DUC (Over and Yen, 2004), which contain in total a few hundred input articles (equivalent to reviews in MReD), our dataset is more than 10 times larger."
    }, {
      "heading" : "2.2 Data Annotation",
      "text" : "As aforementioned, the structure-controllable text generation aims at controlling the structure of the generated passage. Therefore, we need to comprehensively understand the structures of metareviews so as to enable a model to learn how to generate outputs complying with certain structures.\nSpecifically, based on the nature of meta-reviews, we pre-define 9 intent categories: abstract, strength, weakness, suggestion, rebuttal process, rating summary, area chair (AC) disagreement, decision, and miscellaneous (misc). Table 3 shows the definition for each category (see example sentences in Appendix A.1). The identification of category for some sentences is fairly straightforward, while some sentences are relatively ambiguous. Therefore, besides following the definition of each category, the annotators are also required to follow the additional rules as elaborated in Appendix A.2\nFor conducting the annotation work, 14 professional data annotators from a data company are initially trained, and 12 of them are selected for the task according to their annotation quality during a trial round. These 12 annotators are fully paid for their work. Each meta-review sentence is independently labeled by 2 different annotators, and a third annotator resolves any disagreement between the first two annotators. We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen’s kappa is 0.778 between the two annotators, showing that the annotation is of quite high quality."
    }, {
      "heading" : "2.3 Data Analysis",
      "text" : "To better understand the MReD dataset, we conduct the following analysis along different dimensions.\nSentence distribution across categories. The sentence numbers in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject). Among 7,089 submissions, there are 2,368 accepted and 4,721 rejected. Among all submissions and the rejected submissions, “weakness” accounts for the largest proportion, while across the accepted ones, “abstract” and “strength” take up a great proportion. To some extent, these three categories which dominate in meta-reviews could be easily summarized from the reviewers’ comments. However, some minor or subjective categories (e.g., “ac disagreement”) are hard to generate.\nBreakdown analysis by meta-review lengths and average rating scores. We present the percentage of meta-reviews of different lengths in each score range, as shown in Figure 2. For example, among the meta-reviews that receive the reviewers’ average score below 2 (i.e., the first column in the figure), 28% are less than or equal to 50 words, and 38% fall in the length range of 51 to 100 words. We can observe that the meta-reviews tend to be longer for those submissions receiving scores in the middle range, while shorter for those with lower scores or higher scores. This coincides with our commonsense that for high-score and low-score sub-\nmissions, the decision tends to be a clear accept or reject so that meta-reviews can be relatively shorter, while for those borderline submissions, area chairs have to carefully weigh the pros and cons to make the final decision (see Appendix B.1 for borderline submission analysis). As shown in Figure 3, the meta-reviews with more than 150 words generally have a larger proportion of sentences describing “weakness” and “suggestion” for authors to improve the submissions. Additional analysis on the category breakdown for accepted and rejected papers across the score ranges is shown in Appendix B.2.\nMeta-review patterns. To study the common structures of meta-reviews, we present the transition matrix of different category segments in Figure 4, where the sum of each row is 1. Note that each segment represents the longest consecutive sentences with the same category. We add “<start>” and “<end>” tokens before and after each metareview accordingly to investigate which categories tend to be at the start/end of the meta-reviews. It is clear to see that “abstract” usually positions at the beginning of the meta-review, while “suggestion” and “decision” usually appear at the end. There are also some clear patterns appearing in the metareviews, such as “abstract | strength | weakness”, “rating summary | weakness | rebuttal process”, and “abstract | weakness | decision”."
    }, {
      "heading" : "3 Structure-Controllable Text Generation",
      "text" : ""
    }, {
      "heading" : "3.1 Task Definition",
      "text" : "As aforementioned, in uncontrolled generation, users cannot instruct the model to emphasize on desired aspects. However, in a domain such as meta-reviews, given the same review inputs, one AC may emphasize more on the “strength” of the paper following a structure of “abstract | strength | decision”, whereas another AC may prefer a different structure with more focus on reviewers’ opinions and suggestions (i.e., “rating summary” and\n“suggestion”). To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review which is derivable from the reviews and presents the required structure."
    }, {
      "heading" : "3.2 Explored Methods",
      "text" : "As the recent generation works (Vaswani et al., 2017; Liu and Lapata, 2019; Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task. Thus in this subsection, we mainly present how to re-organize the input reviews and the control structure as an input sequence of the encoder. We also explore other baselines in the experiments later.\nIn order to summarize multiple reviews into a meta-review showing a required structure, we explicitly specify the control label sequence that a model should comply with during generation. Specifically, we intuitively add the control sequence in front of the input text. By directly combining both the control and textual information as a single input, our control method is independent of any specially designed encoder and decoder structures. Moreover, by placing the short control sequence in front, an encoder can immediately observe the control signal at the very beginning, thus avoids the possible interference by the subsequent sequence. Moreover, the control sequence in front will never be truncated when the encoder truncates the input to a certain length limit.\nGiven the multiple review inputs, we need to linearize them into a single input. One simple method to combine multiple inputs for encoder-decoder models is to concatenate all inputs one after an-\nother (Fabbri et al., 2019). Beside the text inputs, the review rating is also crucial information for writing meta reviews, which cannot be found in the review passages but exists in the field of rating score. Therefore, we create a rating sentence that consists of the extracted ratings given by the corresponding reviewers and prepend it to our concatenated review texts to obtain the final input. We name this method rate-concat (see Table 4, upper). We also show explorations with other review combination methods in Appendix C.1.\nAs aforementioned, we place the control sequence in front of the re-organized review information. Specifically, we explore two different control methods, namely, sent-ctrl and seg-ctrl. Sent-ctrl uses one control label per target sentence and controls generation on a sentence-level. Note that this method can allow implicit control on the length (i.e., number of sentences) of the generation. Segctrl treats consecutive sentences of the same label as one segment and only uses one label for a single segment. Example inputs of different control settings are shown in Table 4 (lower). For instance, sent-ctrl repeats “abstract” in its control sequence whereas seg-ctrl does not. This is because seg-ctrl treats the 1st and 2nd target sentences of “abstract” as the same segment and only uses a single label to indicate it in the sequence. Additionally, we provide a vanilla setting for uncontrolled generation, unctrl, where no control sequence is used.\nUsing the above input sequence as the source and the corresponding meta-review as the target, we can train an encoder-decoder model for controllable generation. Many transformer-based models have achieved state-of-the-art performance. Common abstractive summarization models include BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and PEGASUS (Zhang et al., 2020). In this paper we\nfocus on the bart-large-cnn model, one variant of the BART model (results on other pretrained models can be found in Appendix D.1). More specifically, we use the pytorch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020). Hence, all our future usage of the word “Transformers” refers to bart-large-cnn in the Transformers library ."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Baselines",
      "text" : "Extractive Baselines. We employ three common extractive summarization baselines each of which basically provides a mechanism to rank the input sentences. LexRank (Erkan and Radev, 2004) represents sentences in a graph and uses eigenvector centrality to calculate sentence importance scores. TextRank (Mihalcea and Tarau, 2004) is another graph-based sentence ranking method that obtains vertex scores by running a “random-surfer model” until convergence. MMR (Carbonell and Goldstein, 1998) calculates sentence scores by balancing the redundancy score with the information relevance score. After ranking with the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings. For the uncontrolled setting, we simply select the top k sentences as the generated output, where k is a hyperparameter deciding the size of the generated output. For the controlled setting, we select only the top sentences with the right category labels according to the control sequence. To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the sentence labels of each input review. Refer to Appendix D.2 for more details of the tagger.\nGeneric Sentence Baselines. Considering the nature of meta-reviews, we could imagine some categories may have common phrases inflating the Rouge scores, such as “This paper proposes ...” for abstract, and “I recommend acceptance.” for decision, etc. To examine such impact, we select sentences that are generic in each category and combine these sentences to generate outputs according to the control sequences. For instance, if the control sequence is “abstract | strength | decision”, we take the most generic sentences from the categories of “abstract”, “strength” and “decision” respectively to form the output. Specifically, we create two generic sentence baselines by obtaining\ngeneric sentences from the training data from either the meta-review references (i.e., target) or the input reviews (i.e., source), namely “Target Generic” and “Source Generic”. Moreover, we also study such impact on the high-score and low-score submissions respectively, since an AC may write more succinct meta-reviews for clear-cut papers, as suggested by Figure 2. See Appendix D.3 for more details and results on generic sentence baselines."
    }, {
      "heading" : "4.2 Experimental Setting",
      "text" : "To conduct text generation experiments, we preprocess our MReD dataset by filtering to ensure the selected meta-reviews have 20 to 400 words, as certain meta-review passages are extremely short or long. After preprocessing, we obtain 6,693 sourcetarget pairs, for which we randomly split into train, validation, and test sets by a ratio of 8:1:1. We evaluate our generated outputs against the reference meta-reviews using the F1 scores of ROUGE1, ROUGE2, and ROUGEL (Lin, 2004) 3. For the extractive and generic baselines, a key hyperparameter is the sentence number k, which we set to the number of labels in the sent-ctrl control sequence. More setting details are shown in Appendix D.4"
    }, {
      "heading" : "4.3 Main Results",
      "text" : "We show results in Table 5. Only the best settings of rate-concat (Table 12 in Appendix C.1) and input truncation of 2048 tokens (Appendix D.5) for\n3We use the Hugging Face Transformers’ Rouge evaluation script, which has the field “use_stemmer” enabled.\nthe Transformers are included. Amongst the extractive baselines, TextRank performs the best in both unctrl and sent-ctrl settings. Nevertheless, all controlled methods outperform their unctrl settings (same for the Transformers). This validates our intuition that structure-controlled generation is more suitable for user-subjective writings such as meta-reviews, because the model can better satisfy different structure requirements when supplied with the corresponding control sequences. On the other hand, for the Transformers, sent-ctrl is the best, followed by seg-ctrl. This is most likely due to the former’s more fine-grained sentence-level control that provides a clearer structure outline, as compared to the coarser segment-level control.\nMoreover, the Transformers far outperform the extractive baselines, showing that the extractionbased methods are insufficient for MReD. This also suggests that meta-review writings are different from the input reviews, therefore copying full review sentences to form meta-reviews doesn’t work well. This is again validated by the “Target Generic” baseline’s significant improvement over the “Source Generic” baseline, which shows that generic sentences from meta-reviews can suit generation much better than those in reviews. Nevertheless, all Transformers results are still much better than the “Target Generic” sentence baseline, showing that despite generic phrases in some categories contributing to Rouge, the Transformers model is capable of capturing content-specific information for each input."
    }, {
      "heading" : "4.4 Case Study",
      "text" : "We study some cases for a better understanding of the structure-controllable generation.\nIdentify the control label for each sentence. We first evaluate whether the model is able to attend to the correct control label during generation. For each generation step, we obtain the cross attention weights from the decoder’s output token towards the control labels and plot them in Figure 5. The given control sequence is “abstract | weakness |\ndecision”. When generating each sentence, we can see that the attention weights of the corresponding control token are the highest, which demonstrates that our model can effectively pay attention to the correct control label and thus generate the content complying with the intent.\nExtract information from the input sentences. To understand what information the model attends to when generating each sentence, we aggregate the cross attention weights to obtain the attention scores from each generated sentence towards all input sentences (Appendix D.6). Then, we select the top 3 input sentences with the highest attention scores for each generated sentence, and visualize the normalized attention weights on all tokens in the selected sentences and the control sequence in Table 7. As shown, the model can correctly extract relevant information from the source sentences. For\nexample, it identifies important phrases such as “interesting”, “clarity” and “lack of comparison to baselines” when generating “Sent 2”.\nGenerate varied outputs given different control sequences. To further investigate the effectiveness of the control sequence, we change the control sequence of the above example and re-generate the meta-reviews given the same input reviews. In Table 6, we first show the gold meta-review and the model output using the original control sequence in Row 0 and Row 1, and then show the model outputs with alternative control sequences in Row 2 and Row 3. From the outputs, we can see that indeed each generated sentence corresponds to its control label well. In Row 2, we add an additional control label in the sequence and by repeating the “abstract” label, the generator can further elaborate more details of the studied method. This is one key advantage of our sent-ctrl compared to the seg-ctrl, which allows the control of length and the level of the generation details. In Row 3, a very comprehensive control sequence is specified. We can see that the output meta-review is quite fluent and polite to reject the borderline paper. See Appendix D.7 for more examples."
    }, {
      "heading" : "4.5 Human Evaluation",
      "text" : "In addition to the Rouge evaluation, we ask 3 human judges to manually assess the generation quality of the Transformers models from Table 5 on 100 random test instances. For each test instance, we provide the judges with the input reviews and randomly ordered generations from different models,\nand ask them to individually evaluate the generations based on the following criteria: (1) Fluency: is the generation fluent, grammatical, and without unnecessary repetitions? (2) Content Relevance: does the generation reflect the review content well, or does it produce general but trivial sentences? (3) Structure Similarity: how close does the generation structure resemble the gold structure (i.e., the control sequence)? (4) Decision Correctness: does the generation agree with the gold human decision? We grade fluency and content relevance on a scale of 1 to 5, whereas structure similarity and decision correctness are calculated from 0 to 1 (Appendix D.8). For structure similarity, because sent-ctrl and seg-ctrl have different control sequences, we evaluate the two models on sentence-level (sent) and segment-level (seg) structures respectively, and provide both evaluations for unctrl.\nAs shown in Table 8, both sent-ctrl and seg-ctrl models show significant improvements on the generation structure over the uncontrolled baseline, which affirms the effectiveness of our proposed methods for structure-controllable generation. Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit the readability and decision generation. For the content relevance, the scores of all methods are reasonably good, and significance tests cannot prove any best model (p > 0.08). Nevertheless, it is possible that the looser control a method applies, the better relevance score it achieves. It is because a tighter control narrows the content that a model can use from the reviews."
    }, {
      "heading" : "5 Related Work",
      "text" : "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-\nments include Sharma et al. (2019), Cohan et al. (2018), and Fisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD’s reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.\nResearchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AMPERE (Hua et al., 2019) for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999; Liakata et al., 2010; Lauscher et al., 2018), which differ in nature from the peer review domain.\nA wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality (Wang et al., 2019), domains (Takeno et al., 2017) and persona (Zhang et al., 2018)) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997; Shao et al., 2019; Hua and Wang, 2019), which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This paper introduces a fully-annotated text generation dataset MReD in a new domain, i.e., the meta-reviews in the peer review system, and provides thorough data analysis to better understand the data characteristics. With such rich annotations, we propose simple yet effective methods for structure-controllable text generation. Extensive experimental results are presented as baselines for future study and thorough result analysis is conducted to shed light on the control mechanisms."
    }, {
      "heading" : "A Data Annotation",
      "text" : ""
    }, {
      "heading" : "A.1 Category definitions",
      "text" : "We show category examples in Table 9."
    }, {
      "heading" : "A.2 Additional annotation rules",
      "text" : "The additional rules for annotation are as follows: First, instead of only labeling the individual sentences per se, the annotators are given a complete paragraph of meta-review to label the sentences with context information. For example, if the area chair writes a sentence providing some extra background knowledge in the discussion of the weakness of the submission, that sentence itself can be considered as “misc”. However, it should be labeled as “weakness” to be consistent in context.\nSecond, not every sentence can be strictly classified into a single category. When a sentence contains information from multiple categories, the annotators should consider its main point and primary purpose. One example is: “Although the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited.” Although the first half of the sentence discusses the strength of the submission, the primary purpose of this sentence is to point out its weakness, and therefore it should be labeled as weakness.\nFurthermore, there are still some cases where the main point of the sentence is hard to differentiate from multiple categories. We then define a priority order of these 9 categories according to the importance of each category for annotators to\nfollow: decision > rating summary > strength ?= weakness > ac disagreement > rebuttal process > abstract > suggestion > miscellaneous. We use the sign “ ?=” because there are some rare cases where a sentence contains both “strength” and “weakness” while there is no obvious emphasis on either, and it is hard to tell whether “strength” should have a priority over “weakness” or the other way round. We then label this sentence based on the final decision: if this submission is accepted, we label the sentence as “strength”, and vice versa."
    }, {
      "heading" : "B Data Analysis",
      "text" : ""
    }, {
      "heading" : "B.1 Borderline papers",
      "text" : "We further analyze the category distribution in borderline papers. As shown in Table 10, for submissions within the score range of [4.5,6), there are 713 accepted submissions and 2,588 rejected submissions. One clear difference is the percentage of “strength” and “weakness”. Another difference is the percentage of “ac disagreement”, where the accepted papers have four times the value than rejected ones. This suggests that for the accepted borderline papers, the area chair tends to share different opinions with reviewers, and thus deciding to accept the borderline submissions.\nB.2 Percentage of each category for accepted and rejected papers across score ranges\nWe further analyze the occurrence of each category for accepted papers and rejected papers separately across different score ranges, as shown in Table 11. For accepted papers, as the score increases, the percentage of meta-reviews having “weakness” and “suggestion” drops because the high-score submissions are more likely to be accepted. Even the percentage of “decision” drops following the same trend. In addition, the proportion of meta-reviews\nhaving “rebuttal process” is larger for submissions with lower scores. This suggests that the rebuttal process plays an important role in the peer review process, especially in helping the borderline papers to be accepted.\nOn the other hand, for rejected papers, the percentage of meta-reviews having “strength” increases as the average score increases. This coincides with our common sense that the submissions receiving higher scores tend to have more strengths. One interesting finding here is that the percentage of “weakness” and “suggestion” also increases as the average rating score increases. This may be due to two main reasons. First, to reject a submission with higher scores, the area chair has to explain the weakness with more details and provide more suggestions for authors to further improve their submissions. Second, compared to the percentage of “strength”, “weakness” definitely has a larger percentage within any range of rating scores. The difference in the percentage of “strength” and “weakness” is intuitively different between the accepted papers and the rejected papers."
    }, {
      "heading" : "C Structure-Controllable Text Generation",
      "text" : ""
    }, {
      "heading" : "C.1 Review combination methods",
      "text" : "We explore alternative methods to linearize the multiple reviews of the same submission, namely, con-\ncat and merge. For the concat, we simply concatenate all reviews one after another according to their reviewers’ sequence. For merge, we can obtain the merged content as follows: From all review inputs, we use the longest one as a backbone. We segment all reviews’ content on a paragraph level, and encode them using SentenceTransformers (Reimers and Gurevych, 2019). Then, for each paragraph embedding in the non-backbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding, and insert it after the backbone paragraph with which it has the highest similarity score. We repeat the process for all paragraphs in non-backbone reviews to obtain a single passage. Additionally, we provide a baseline setting longestreview, which does not combine reviews but only uses the longest review as the input. Moreover, we add rating sentences in front of the results of concat and merge to obtain rate-concat and rate-merge, respectively.\nAs shown in Table 12, the longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information. rate-concat setting has the best overall performance, which is the setting used throughout the main paper."
    }, {
      "heading" : "D Experiments",
      "text" : ""
    }, {
      "heading" : "D.1 Additional transformers models",
      "text" : "We provide baselines of uncontrolled generation and controlled generation on MReD using other common Transformer pretrained models in Table 13."
    }, {
      "heading" : "D.2 Tagger for source sentences",
      "text" : "To obtain labels on source input, we train a tagger based on the human-annotated meta-reviews,\nthen use it to predict labels on the input sentences. Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks with a conditional random field (CRF) (Lafferty et al., 2001) (i.e., LSTM-CRF (Lample et al., 2016)) model on the annotated MReD dataset. The same data split as the meta-review generation task is used. We adopt the standard IOBES tagging scheme (Ramshaw, 1995; Ratinov and Roth, 2009), and fine-tune BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019) models in Hugging Face. All models are trained for 30 epochs with an early stop of 20, and each epoch takes about 30 minutes. We select the best model parameters based on the best micro F1 score on the development set and apply it to the test set for evaluation. All models are run with V100 GPU. We use Adam (Kingma and Ba, 2014) with an initial learning rate of 2e-5.\nWe report the F1 scores for each category as well as the overall micro F1 and macro F1 scores in Table 14. Micro F1 is the overall accuracy regardless of the categories, whereas macro F1 is an average of per category accuracy evaluation. Since some of the category labels (eg. “ac disagreement”) are very rare, their classification accuracy is low. Overall, micro F1 is a more important metric since it suggests general performance. The results stand proof that the majority of the categories have their own characteristics that can be identified from other categories. RoBERTabase is the best performing model, therefore we use this model for review sentence label prediction."
    }, {
      "heading" : "D.3 Generic sentence baselines",
      "text" : "Besides the baselines of “Source Generic” and “Target Generic”, we explore subsets of papers with high scores (average reviewers’ rating > 7) or low scores (average reviewers’ rating 6 3) to obtain 4 additional generic baselines: “Source High Score”, “Source Low Score”, “Target High Score”, “Target Low Score”. We use “Target Generic” as an example to explain how we obtain the generic sentences: We first group all meta-review sentences\nfrom the training set according to their label categories, and then re-arrange the sentences in each category using TextRank (our best performing extractive model). Since TextRank ranks the input sentences based on each sentence’s content connection with others, sentences with higher rankings are also more general in the sense that they have more shared content with others. Similarly, different sets of generic sentences can be obtained for the other 5 baselines.\nAfter obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence. We avoid using the same sentence twice inside the same generation, so if the same label appears multiple times in a control sequence, we will use the same number of generic sentences for that category down the ranking order.\nWe show results in Table 15. The low score baselines perform the best amongst both source and target baselines, suggesting that the sentences from low score submissions are more typical for both reviews and meta-reviews."
    }, {
      "heading" : "D.4 Experimental setting details",
      "text" : "For preprocessing, besides filtering based on metareview length, we also remove submissions with only one or two reviews, since the majority of the submissions have more than 3 reviews.\nFor the extractive baselines, recall that under the sent-ctrl setting, the control sequence length is the same as the sentence number of the target metareview. Therefore, to conduct a fair comparison, we set the hyperparameter k equal to the number of\nlabels in the control sequence for both controlled and uncontrolled extractive baselines, and sent-ctrl is used for all controlled extractive baselines. We also adopt the same k for the generic baselines.\nFor the Transformers, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU. During finetuning, we set the Transformers’ hyperparameters of “minimum_target_length” to 20, and “maximum_target_length” to 400, according to our filter range on the meta-review lengths. For the rest of the hyperparameters, we use the pretrained model’s default values. Due to long inputs (see Table 17), we experiment with different source truncation lengths of 1024, 2048, and 3072 tokens. Due to the limitation of GPU space, we cannot explore truncation length of more than 3072 tokens."
    }, {
      "heading" : "D.5 Ablation on truncation length",
      "text" : "By default, the Transformers truncate the source to 1024 tokens. We further investigate the performance of different source truncation lengths using rate-concat. As shown in Table 18, truncating the source to 2048 tokens consistently achieves the best performance."
    }, {
      "heading" : "D.6 Attention aggregation method",
      "text" : "During generation, we can obtain the attention weights of each output token towards all input tokens. Specifically, we average all decoder layers’ cross attention weights for the same output token generated at each decoding step. We then calculate an attention value for that output token on each input sentence, by aggregating the token’s attention weights on the list of input tokens that belong to the same sentence by max pooling. Finally, we can calculate an output-sentence-to-input-sentence attention score, by adding up these attention values for the output tokens that belong to the same\nsentence. Common attention aggregation methods include summation, average-pooling, and max-pooling. We use max-pooling to aggregate attention for same-sentence input tokens, because summation gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out. With max-pooling, we can correctly identify sentences with spiked attention at important phrases, regardless of sentence lengths. For attention aggregation on the same-sentence output tokens, summation is used and can be viewed as allowing each output token to vote an attention score on all input sentences, so that the input sentence receiving the highest total score is the most relevant. We conduct trial runs of all aggregation methods on input tokens with summation for output-token aggregation for multiple generation examples, and indeed max-pooling outperforms the other two by identifying more relevant input sentences with the generated sentence.\nOnce we have the attention scores, we can attribute the generation of each output sentence to a few topmost relevant input sentences. Then, we can draw a color map of the input tokens in the selected sentences based on their relative attention weights."
    }, {
      "heading" : "D.7 Structure-controlled generation examples",
      "text" : "We show examples of the generation results using alternative control sequences on another submission in Table 16. We can see the effectiveness of controlling the output structure using our proposed method."
    }, {
      "heading" : "D.8 Human evaluation",
      "text" : "For structure similarity, we instruct the judges to label each generated sentence with the closest category. We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, then deduct this value from 1.\nFor decision correctness, we evaluate it on a binary scale where 1 indicates complete correctness and 0 otherwise. More specifically, we give 0 if the generation produces contradictory decisions and a wrong decision, or the generation does not show enough hints for rejection or acceptance."
    } ],
    "references" : [ {
      "title" : "Metagen: An academic meta-review generation system",
      "author" : [ "Chaitanya Bhatia", "Tribikram Pradhan", "Sukomal Pal." ],
      "venue" : "Proceedings of ACM-SIGIR.",
      "citeRegEx" : "Bhatia et al\\.,? 2020",
      "shortCiteRegEx" : "Bhatia et al\\.",
      "year" : 2020
    }, {
      "title" : "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
      "author" : [ "Jaime Carbonell", "Jade Goldstein." ],
      "venue" : "Proceedings of ACM-SIGIR.",
      "citeRegEx" : "Carbonell and Goldstein.,? 1998",
      "shortCiteRegEx" : "Carbonell and Goldstein.",
      "year" : 1998
    }, {
      "title" : "Argument pair extraction from peer review and rebuttal via multi-task learning",
      "author" : [ "Liying Cheng", "Lidong Bing", "Qian Yu", "Wei Lu", "Luo Si." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "Pre-train and plug-in: Flexible conditional text generation with variational autoencoders",
      "author" : [ "Yuguang Duan", "Canwen Xu", "Jiaxin Pei", "Jialong Han", "Chenliang Li." ],
      "venue" : "Proceedings of the ACL.",
      "citeRegEx" : "Duan et al\\.,? 2020",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2020
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R Radev." ],
      "venue" : "Artificial Intelligence Research.",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander Richard Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir Radev." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "Controllable abstractive summarization",
      "author" : [ "Angela Fan", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of WNGT.",
      "citeRegEx" : "Fan et al\\.,? 2018a",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Fan et al\\.,? 2018b",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "A multi-layered annotated corpus of scientific papers",
      "author" : [ "Beatriz Fisas", "Francesco Ronzano", "Horacio Saggion." ],
      "venue" : "Proceedings of LREC.",
      "citeRegEx" : "Fisas et al\\.,? 2016",
      "shortCiteRegEx" : "Fisas et al\\.",
      "year" : 2016
    }, {
      "title" : "The webnlg challenge: Generating text from rdf data",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of INLG.",
      "citeRegEx" : "Gardent et al\\.,? 2017",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
      "author" : [ "Max Grusky", "Mor Naaman", "Yoav Artzi" ],
      "venue" : "In Proceedings of NAACL",
      "citeRegEx" : "Grusky et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grusky et al\\.",
      "year" : 2018
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomás Kociskỳ", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Proceedings of NIPS.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Argument mining for understanding peer reviews",
      "author" : [ "Xinyu Hua", "Mitko Nikolov", "Nikhil Badugu", "Lu Wang." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Hua et al\\.,? 2019",
      "shortCiteRegEx" : "Hua et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentence-level content planning and style specification for neural text generation",
      "author" : [ "Xinyu Hua", "Lu Wang." ],
      "venue" : "Proceedings of EMNLP-IJCNLP.",
      "citeRegEx" : "Hua and Wang.,? 2019",
      "shortCiteRegEx" : "Hua and Wang.",
      "year" : 2019
    }, {
      "title" : "A dataset of peer reviews (peerread): Collection, insights and nlp applications",
      "author" : [ "Dongyeop Kang", "Waleed Ammar", "Bhavana Dalvi", "Madeleine van Zuylen", "Sebastian Kohlmeier", "Eduard Hovy", "Roy Schwartz." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Kang et al\\.,? 2018",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin Ming-Wei Chang Kenton", "Lee Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Kenton and Toutanova.,? 2019",
      "shortCiteRegEx" : "Kenton and Toutanova.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv e-prints.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Improving abstraction in text summarization",
      "author" : [ "Wojciech Kryściński", "Romain Paulus", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Kryściński et al\\.,? 2018",
      "shortCiteRegEx" : "Kryściński et al\\.",
      "year" : 2018
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando CN Pereira." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing",
      "author" : [ "Anne Lauscher", "Goran Glavaš", "Kai Eckert." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Lauscher et al\\.,? 2018",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2018
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Corpora for the conceptualisation and zoning of scientific papers",
      "author" : [ "Maria Liakata", "Simone Teufel", "Advaith Siddharthan", "Colin Batchelor." ],
      "venue" : "Proceedings of LREC.",
      "citeRegEx" : "Liakata et al\\.,? 2010",
      "shortCiteRegEx" : "Liakata et al\\.",
      "year" : 2010
    }, {
      "title" : "QuaSE: Sequence editing under quantifiable guidance",
      "author" : [ "Yi Liao", "Lidong Bing", "Piji Li", "Shuming Shi", "Wai Lam", "Tong Zhang." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Liao et al\\.,? 2018",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2018
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv e-prints.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Politeness transfer: A tag and generate approach",
      "author" : [ "Aman Madaan", "Amrith Setlur", "Tanmay Parekh", "Barnabás Póczos", "Graham Neubig", "Yiming Yang", "Ruslan Salakhutdinov", "Alan W Black", "Shrimai Prabhumoye." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Madaan et al\\.,? 2020",
      "shortCiteRegEx" : "Madaan et al\\.",
      "year" : 2020
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Çağlar Gulçehre", "Bing Xiang." ],
      "venue" : "Proceedings of SIGNLL.",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Annotated gigaword",
      "author" : [ "Courtney Napoles", "Matthew R Gormley", "Benjamin Van Durme." ],
      "venue" : "Proceedings of AKBC-WEKEX.",
      "citeRegEx" : "Napoles et al\\.,? 2012",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2012
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "An introduction to duc-2004",
      "author" : [ "Paul Over", "James Yen." ],
      "venue" : "Proceedings of DUC.",
      "citeRegEx" : "Over and Yen.,? 2004",
      "shortCiteRegEx" : "Over and Yen.",
      "year" : 2004
    }, {
      "title" : "Overview of the tac 2011 summarization track: Guided task and aesop task",
      "author" : [ "Karolina Owczarzak", "Hoa Trang Dang." ],
      "venue" : "Proceedings of TAC.",
      "citeRegEx" : "Owczarzak and Dang.,? 2011",
      "shortCiteRegEx" : "Owczarzak and Dang.",
      "year" : 2011
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Text chunking using transformation-based learning",
      "author" : [ "LA Ramshaw." ],
      "venue" : "Proceedings of Third Workshop on Very Large Corpora.",
      "citeRegEx" : "Ramshaw.,? 1995",
      "shortCiteRegEx" : "Ramshaw.",
      "year" : 1995
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev Ratinov", "Dan Roth." ],
      "venue" : "Proceedings of CoNLL.",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Building applied natural language generation systems",
      "author" : [ "Ehud Reiter", "Robert Dale." ],
      "venue" : "Natural Language Engineering.",
      "citeRegEx" : "Reiter and Dale.,? 1997",
      "shortCiteRegEx" : "Reiter and Dale.",
      "year" : 1997
    }, {
      "title" : "The new york times annotated corpus",
      "author" : [ "Evan Sandhaus." ],
      "venue" : "Linguistic Data Consortium, Philadelphia.",
      "citeRegEx" : "Sandhaus.,? 2008",
      "shortCiteRegEx" : "Sandhaus.",
      "year" : 2008
    }, {
      "title" : "Semi-supervised text style transfer: Cross projection in latent space",
      "author" : [ "Mingyue Shang", "Piji Li", "Zhenxin Fu", "Lidong Bing", "Dongyan Zhao", "Shuming Shi", "Rui Yan." ],
      "venue" : "Proceedings of EMNLP-IJCNLP.",
      "citeRegEx" : "Shang et al\\.,? 2019",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2019
    }, {
      "title" : "Long and diverse text generation with planning-based hierarchical variational model",
      "author" : [ "Zhihong Shao", "Minlie Huang", "Jiangtao Wen", "Wenfei Xu", "Xiaoyan Zhu." ],
      "venue" : "Proceedings of EMNLP-IJCNLP.",
      "citeRegEx" : "Shao et al\\.,? 2019",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2019
    }, {
      "title" : "Bigpatent: A large-scale dataset for abstractive and coherent summarization",
      "author" : [ "Eva Sharma", "Chen Li", "Lu Wang." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Sharma et al\\.,? 2019",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2019
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi S Jaakkola." ],
      "venue" : "Proceedings of NIPS.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Controlling target features in neural machine translation via prefix constraints",
      "author" : [ "Shunsuke Takeno", "Masaaki Nagata", "Kazuhide Yamamoto." ],
      "venue" : "Proceedings of WAT.",
      "citeRegEx" : "Takeno et al\\.,? 2017",
      "shortCiteRegEx" : "Takeno et al\\.",
      "year" : 2017
    }, {
      "title" : "Abstractive document summarization with a graphbased attentional neural model",
      "author" : [ "Jiwei Tan", "Xiaojun Wan", "Jianguo Xiao." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Tan et al\\.,? 2017",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2017
    }, {
      "title" : "Targetguided open-domain conversation",
      "author" : [ "Jianheng Tang", "Tiancheng Zhao", "Chenyan Xiong", "Xiaodan Liang", "Eric Xing", "Zhiting Hu." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "An annotation scheme for discourse-level argumentation in research articles",
      "author" : [ "Simone Teufel", "Jean Carletta", "Marc Moens." ],
      "venue" : "Proceedings of EACL.",
      "citeRegEx" : "Teufel et al\\.,? 1999",
      "shortCiteRegEx" : "Teufel et al\\.",
      "year" : 1999
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Harnessing pre-trained neural networks with rules for formality style transfer",
      "author" : [ "Yunli Wang", "Yu Wu", "Lili Mou", "Zhoujun Li", "Wenhan Chao." ],
      "venue" : "Proceedings of EMNLP-IJCNLP.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-theart natural language processing",
      "author" : [ "Thomas Wolf", "Julien Chaumond", "Lysandre Debut", "Victor Sanh", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic generation of citation texts in scholarly papers: A pilot study",
      "author" : [ "Xinyu Xing", "Xiaosheng Fan", "Xiaojun Wan." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Xing et al\\.,? 2020",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2020
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter Liu." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "Proceedings of ACL",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "The lessto-more generation tasks generate a more abundant output from some obviously simpler input, such as prompt-based story generation (Fan et al., 2018b).",
      "startOffset" : 138,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "The neck-to-neck generation aims at generating an output text which conveys the same quantity of knowledge as the input but in natural language, such as typical RDF triples to text tasks (Gardent et al., 2017).",
      "startOffset" : 187,
      "endOffset" : 209
    }, {
      "referenceID" : 31,
      "context" : "Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and news titles.",
      "startOffset" : 91,
      "endOffset" : 115
    }, {
      "referenceID" : 45,
      "context" : ", text style transfer on sentiment polarity (Shen et al., 2017; Liao et al., 2018) and formality (Shang et al.",
      "startOffset" : 44,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : ", text style transfer on sentiment polarity (Shen et al., 2017; Liao et al., 2018) and formality (Shang et al.",
      "startOffset" : 44,
      "endOffset" : 82
    }, {
      "referenceID" : 42,
      "context" : ", 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 35,
      "context" : "Compared with those conventional datasets for MDS, such as TAC (Owczarzak and Dang, 2011) and DUC (Over and Yen, 2004),",
      "startOffset" : 63,
      "endOffset" : 89
    }, {
      "referenceID" : 34,
      "context" : "Compared with those conventional datasets for MDS, such as TAC (Owczarzak and Dang, 2011) and DUC (Over and Yen, 2004),",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 50,
      "context" : "As the recent generation works (Vaswani et al., 2017; Liu and Lapata, 2019; Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task.",
      "startOffset" : 31,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : "As the recent generation works (Vaswani et al., 2017; Liu and Lapata, 2019; Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task.",
      "startOffset" : 31,
      "endOffset" : 94
    }, {
      "referenceID" : 53,
      "context" : "As the recent generation works (Vaswani et al., 2017; Liu and Lapata, 2019; Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task.",
      "startOffset" : 31,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "Common abstractive summarization models include BART (Lewis et al., 2020), T5 (Raffel et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 36,
      "context" : ", 2020), T5 (Raffel et al., 2020) and PEGASUS (Zhang et al.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 52,
      "context" : "More specifically, we use the pytorch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020).",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "LexRank (Erkan and Radev, 2004) represents sentences in a graph and uses eigenvector centrality to calculate sentence importance scores.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 30,
      "context" : "TextRank (Mihalcea and Tarau, 2004) is another graph-based sentence ranking method that obtains vertex scores by running a “random-surfer model” until convergence.",
      "startOffset" : 9,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "MMR (Carbonell and Goldstein, 1998) calculates sentence scores by balancing the redundancy score with the information relevance score.",
      "startOffset" : 4,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the sentence labels of each input review.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 26,
      "context" : "We evaluate our generated outputs against the reference meta-reviews using the F1 scores of ROUGE1, ROUGE2, and ROUGEL (Lin, 2004) 3.",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 41,
      "context" : "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al.",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 32,
      "context" : "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al.",
      "startOffset" : 171,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : ", 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : ", 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AMPERE (Hua et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : ", 2018) for paper decision predictions, AMPERE (Hua et al., 2019) for proposition classification in reviews, and RR (Cheng et al.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : ", 2019) for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 49,
      "context" : "There are also some explorations on research articles (Teufel et al., 1999; Liakata et al., 2010; Lauscher et al., 2018), which differ in nature from the peer review domain.",
      "startOffset" : 54,
      "endOffset" : 120
    }, {
      "referenceID" : 24,
      "context" : "There are also some explorations on research articles (Teufel et al., 1999; Liakata et al., 2010; Lauscher et al., 2018), which differ in nature from the peer review domain.",
      "startOffset" : 54,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "There are also some explorations on research articles (Teufel et al., 1999; Liakata et al., 2010; Lauscher et al., 2018), which differ in nature from the peer review domain.",
      "startOffset" : 54,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : ", sentiments (Duan et al., 2020), politeness (Madaan et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : ", 2020), politeness (Madaan et al., 2020), formality (Wang et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 51,
      "context" : ", 2020), formality (Wang et al., 2019), domains (Takeno et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 46,
      "context" : ", 2019), domains (Takeno et al., 2017) and persona (Zhang et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 55,
      "context" : ", 2017) and persona (Zhang et al., 2018)) and content control (e.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : ", 2020), entities (Fan et al., 2018a), and keywords (Tang et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 40,
      "context" : "Our task also differs from content planning (Reiter and Dale, 1997; Shao et al., 2019; Hua and Wang, 2019), which involves explicitly selecting and arranging the input content.",
      "startOffset" : 44,
      "endOffset" : 106
    }, {
      "referenceID" : 43,
      "context" : "Our task also differs from content planning (Reiter and Dale, 1997; Shao et al., 2019; Hua and Wang, 2019), which involves explicitly selecting and arranging the input content.",
      "startOffset" : 44,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "Our task also differs from content planning (Reiter and Dale, 1997; Shao et al., 2019; Hua and Wang, 2019), which involves explicitly selecting and arranging the input content.",
      "startOffset" : 44,
      "endOffset" : 106
    } ],
    "year" : 0,
    "abstractText" : "When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain. 1",
    "creator" : null
  }
}
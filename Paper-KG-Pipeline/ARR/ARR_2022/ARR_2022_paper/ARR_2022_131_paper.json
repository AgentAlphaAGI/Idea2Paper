{
  "name" : "ARR_2022_131_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Comprehensive Multi-Modal Interactions for Referring Image Segmentation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Traditional computer vision tasks like detection and segmentation have dealt with a pre-defined set of categories, limiting their scalability and practicality. Substituting the pre-defined categories with natural language expressions (NLE) is a logical extension to counteract the above problems. Indeed, this is how humans interact with objects in their environment; for example, the phrase “the kid running after the butterfly\" requires localizing only the child running after the butterfly and not the other kids. Formally, the task of localizing objects based on NLE is known as Visual Grounding. Existing works either approach the grounding problem by predicting a bounding box around the referred object or a segmentation mask corresponding to the referred object. We focus on the latter approach, as a segmentation mask can effectively pinpoint\nthe exact location and capture the actual shape of the referred object. The task is formally known as Referring Image Segmentation (RIS).\nRIS requires understanding both visual and linguistic modalities at an individual level, specifically word-word and region-region interactions. Additionally, a mutual understanding of both modalities is required to identify the referred object from the linguistic expression and localize it in the image. For instance, to ground a sentence “whatever is on the truck\", it is necessary to understand the relationship between words as grounding just the individual words will not work. Similarly, regionto-region interactions in visual modality help group semantically similar regions, e.g., all regions belonging to the truck. Finally, to identify the referent regions, we need to transfer the distinctive information about the referent from the linguistic modality to the visual modality; this is taken care of by the cross-modal word-region interactions. The current SOTA methods (Yang et al., 2021; Feng et al., 2021; Huang et al., 2020; Hui et al., 2020; Hu et al., 2020) take a modular approach, where these interactions happen in parts, sequentially.\nDifferent methods differ in how they model these interactions. (Huang et al., 2020) first perform a\nregion-word alignment (cross-modal interaction). The second stage takes these alignments as input to select relevant image regions corresponding to the referent. (Yang et al., 2021) and (Hui et al., 2020) use the dependency tree structure of the referring expression for the reasoning stage instead. (Hu et al., 2020) select a suitable combination of words for each region, followed by selecting the relevant regions corresponding to referent based on the affinities with other regions. The performance of the initial stages bounds these approaches. Furthermore, they ignore the crucial intra-modal interactions for RIS.\nIn this paper, we perform all three forms of interactions simultaneously. We propose a Synchronous Multi-Modal Fusion Module (SFM) which captures the inter-modal and intra-modal interactions between visual and linguistic modalities in a single step. Intra-modal interactions handle the cases for identifying the relevant set of words and semantically similar image regions. Inter-modal interactions transfer contextual information across modalities. Additionally, we propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM) to exchange contextual information relevant to referent across visual hierarchies and refine the referred object’s segmentation mask.\nWe motivate the benefits of simultaneous interactions over sequential in Figure 1 by presenting a failure case of the latter. For the given referring expression \"anywhere, not on the people\", sequential approaches fail to identify the correct word to be grounded, and the error gets propagated till the end. CMPC (Huang et al., 2020) which predicts the referent word from the expression in the first stage, identifies \"people\" as the referent (middle image in Figure 1) and completely misses \"anywhere\" which is the correct entity to ground. Similarly, (Yang et al., 2021), and (Hui et al., 2020), which utilize dependency tree structure to govern their reasoning process, identify the referred entity \"anywhere\" as an adverb from the dependency tree. However, considering the expression in context with the image, the word \"anywhere\" should be perceived as a \"pronoun\". The proposed SFM module successfully addresses the aforementioned limitations. Overall, our work makes the following contributions:-\n1. We propose SFM to reason over regions, words, and region-word features in a synchronous manner, allowing each modality to focus on relevant semantic information to\nidentify the referred object. 2. We propose a novel HCAM module, which\nroutes hierarchical visual information through linguistic features to produce a refined segmentation mask. 3. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our approach and show notable performance gains on four RIS benchmarks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Referring Expression Comprehension: Localizing a bounding box/proposals based on an NLE is a task commonly referred to as Referring Expression Comprehension (REC). The majority of methods for REC learn a joint embedding space for visual and linguistic modalities and differ in how joint space is computed and how it is used. Earlier methods, (Hu et al., 2016b; Rohrbach et al., 2016; Plummer et al., 2018) used joint embedding space as a metric space to rank proposal features with linguistic features. Later methods like (Yang et al., 2019; Deng et al., 2018; Liu et al., 2020) utilized attention over the proposals to select the appropriate one. More Recent Methods like (Lu et al., 2019; Chen et al., 2020) utilize transformer-based architecture to project multi-modal features to common semantic space. Specifically, they utilize a self-attention mechanism to align proposal-level features with linguistic features. In our work, we utilize pixel-level image features which are crucial for the task of RIS. Additionally, compared to (Lu et al., 2019), we explicitly capture inter-modal and intra-modal interactions between visual and linguistic modalities.\nReferring Image Segmentation: Bounding Boxbased methods in REC are limited in their capabilities to capture the inherent shape of the referred object, which led to the proposal of the RIS task. It was first introduced in (Hu et al., 2016a), where they generate the referent’s segmentation mask by directly concatenating visual features from CNN with tiled language features from LSTM. (Li et al., 2018) generates refined segmentation masks by incorporating multi-scale semantic information from the image. Since each word in expression makes a different contribution in identifying the desired object, (Shi et al., 2018) model visual context for each word separately using query attention. (Ye et al., 2019) uses a self-attention mechanism to capture long-range correlations between visual and textual\nmodalities. Recent works (Hu et al., 2020; Huang et al., 2020; Hui et al., 2020) utilize cross-modal attention to model multi-modal context, (Hui et al., 2020; Yang et al., 2021) use dependency tree structure and (Huang et al., 2020) use coarse labelling for each word in the expression for selective context modelling. Most of the existing works capture Inter and Intra modal interactions separately to model the context for referent. In this work, we concurrently model the comprehensive interactions across visual and linguistic modalities."
    }, {
      "heading" : "3 Method",
      "text" : "Given an image and a natural language referring expression, the goal is to predict a pixel-level segmentation mask corresponding to the referred entity described by the expression. The overall architecture of the network is illustrated in Figure 2. Visual features for the image are extracted using a CNN backbone, and linguistic features for the referring expression are extracted using a LSTM. A Synchronous Multi-Modal Fusion Module (SFM) simultaneously aligns visual regions with textual words and jointly reasons about both modalities to identify the multi-modal context relevant to the referent. SFM is applied to hierarchical visual features extracted from CNN backbone since hierarchical features are better suited for segmentation tasks (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020). A novel Hierarchical Cross-Modal Aggregation module (HCAM) is applied to effectively fuse SFM’s multi-level output and produce a refined segmentation mask for the referent. We describe\nthe feature extraction process in the next section, and both SFM and HCAM modules are described in the subsequent sections."
    }, {
      "heading" : "3.1 Feature Extraction",
      "text" : "Our network takes an image and a natural language expression as input. We extract hierarchical visual features for an image from a CNN backbone. Through pooling and convolution operations, all hierarchical visual features are transformed to the same spatial resolution and channel dimension. Final visual features for each level are of shape RCv×H×W , with H , W and Cv being the height, width, and channel dimension of the visual features. Final visual features are denoted as {V2, V3, V4}, corresponding to layers 2, 3 and 4 of the CNN backbone. For ease of readability, we denote the visual features as V . GloVe embeddings for each word in the referring expression are then passed as input to LSTM. The hidden feature of LSTM at ith time step li ∈ RCl , is used to denote the word feature for the ith word in the expression. The final linguistic feature of the expression is denoted as L = {l1, l2, ..., lT }, where T is the number of words in the referring expression."
    }, {
      "heading" : "3.2 Synchronous Multi-Modal Fusion",
      "text" : "In this section, we describe the Synchronous MultiModal Fusion Module (SFM). To successfully segment the referent, we need to identify the semantic information relevant to it in both the visual and linguistic modalities. We capture comprehensive intra-modal and inter-modal interactions explicitly\nin a synchronous manner, allowing us to jointly reason about visual and linguistic modalities while considering the contextual information from both.\nHierarchical visual features V ∈ RCv×H×W and linguistic word-level features L ∈ RCl×T are passed as input to SFM, with Cv = Cl = C. We flatten the spatial dimensions of visual features and perform a lengthwise concatenation with linguistic feature, followed by layer normalization to get multi-modal feature X of shape RC×(HW+T ). We then add separate positional embedding Pv and Pl to visual Xv ∈ RC×HW and linguistic Xl ∈ RC×T part of X to distinguish between visual and linguistic part. Finally, we apply multi-head attention over X to capture the inter-modal and intra-modal interactions between visual and linguistic modalities. Specifically, pixel-pixel, word-word and wordpixel interactions are captured. Pixel-pixel and word-word interactions help in independently identifying semantically similar pixels and words in their respective modalities, pixel-word interaction helps in identifying corresponding pixels and words with similar contextual semantics across modalities.\nX = LayerNorm(V ⊙ L) X = X + (Pv ⊙ Pl) F = MultiHead(X)\n(1)\nHere, ⊙ is length-wise concatenation, F is the final output of SFM module having same shape as X . We process all hierarchical visual features {V2, V3, V4} individually through SFM, resulting in hierarchical cross-modal output {F2, F3, F4}."
    }, {
      "heading" : "3.3 Hierarchical Cross-Modal Aggregation",
      "text" : "Hierarchical visual features of CNN capture different aspects of images. As a result, depending on the hierarchy, visual features can focus on different aspects of the linguistic expression. In order to predict a refined segmentation mask, different hierarchies should be in agreement regarding the image regions to focus on. Therefore, all visual hierarchical features should also focus on image regions corresponding to linguistic context from other hierarchies. This will ensure that all hierarchical features are focusing on common regions. We propose a novel Hierarchical Cross-Modal Aggregation (HCAM) module for this purpose. HCAM includes two key steps: (1) Hierarchical CrossModal Exchange, and (2) Hierarchical Aggregation. Both steps are illustrated in Figure 3.\nHierarchical Cross-Modal Exchange: During the HCME step, we calculate the affinity weights Λij between the jth layer’s linguistic context f lj and the spatial regions for ith layer’s visual features fvi , where fvi and f l i are the visual and linguistic part of ith layer’s output of SFM Fi.\nΛij = σ(Conv([f v i ; f lavg j ])) (2)\nHere Λij ∈ RC×H×W , f lavg j ∈ RC is the global linguistic context for jth layer and is computed as length-wise average of linguistic features f lj , σ is the sigmoid function. Here, f lavgj act as a bridge to route linguistic context from jth layer to spatial regions of ith layer’s visual hierarchy. Similarly, Λik is computed with i ̸= j ̸= k, allowing for cross-modal exchange between all permutations of visual and linguistic hierarchical features.\nHierarchical Aggregation: After computing the affinity weights Λij , we perform a layer-wise contextual aggregation. For each layer, visual context from other hierarchies is aggregated in the following way:\ngi = f v i + ∑ j ̸=i Λij ◦ fvj\nG = Conv3D([g2; g3; g4])\n(3)\nHere, ◦ is element-wise product and [; ] represents stacking features along length dimension, ie:R3×C×H×W dimensional feature. gi ∈ RC×H×W contains the relevant regions corresponding to the linguistic context from the other two hierarchies. Finally, we use 3D convolution to aggregate gi’s to include the common regions corresponding to the linguistic context from all visual hierarchies. G is the final multi-modal context for referent."
    }, {
      "heading" : "3.4 Mask Generation",
      "text" : "Finally, G is passed through Atrous Spatial Pyramid Pooling (ASPP) decoder (Chen et al., 2018) and Up-sampling convolution to predict final segmentation mask S. Pixel-level binary cross-entropy loss is applied to predicted segmentation map S and the ground truth segmentation mask Y to train the entire network end-to-end."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We conduct experiments on four Referring Image Segmentation datasets. UNC (Yu et al., 2016) contains 19,994 images taken from MS-COCO (Lin et al., 2014) with 142,209 referring expressions corresponding to 50,000 objects. Referring Expressions for this dataset contain words indicating the location of the object. UNC+ (Yu et al., 2016) is also based on images from MS-COCO. It contains 19,992 images, with 141,564 referring expressions corresponding to 50,000 objects. In UNC+, the expression describes the object based on their appearance and context within the scene without using spatial words. G-Ref (Mao et al., 2016) is also curated using images from MS-COCO. It contains 26,711 images, with 104,560 referring expressions for 50,000 objects. G-Ref contains longer sentences with an average length of 8.4 words; compared to other datasets which have an average sentence length of less than 4 words. Referit (Kazemzadeh et al., 2014) comprises of 19,894 images collected from IAPR TC-12 dataset. It includes 130,525 expressions for 96,654 objects. It contains unstructured regions (e.g., sky, mountains, and ground) as ground truth segmentations."
    }, {
      "heading" : "4.2 Implementation details",
      "text" : "We experiment with two backbones, DeepLabv3+ (Chen et al., 2018) and Resnet-101 for image feature extraction. Like previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), DeepLabv3+ is pre-trained on Pascal VOC semantic segmentation task while Resnet-101 is pre-trained on Imagenet Classification task, and both backbone’s parameters are fixed during training. For multi-level features, we extract features from the last three blocks of CNN backbone. We conduct experiments at two different image resolutions, 320× 320 and 448× 448 with H = W = 18. We use GLoVe embeddings (Pennington et al., 2014) pre-trained on Common\nCrawl 840B tokens to initialize word embedding for words in the expressions. The maximum number of words in the linguistic expression is set to 25. We use LSTM for extracting textual features. The network is trained using AdamW optimizer with batch size set to 50; the initial learning rate is set to 1.2e−4 and weight decay of 9e−5 is used. The initial learning rate is gradually decreased using polynomial decay with a power of 0.7. We train our network on each dataset separately.\nEvaluation Metrics: Following previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), we evaluate the performance of our model using overall Intersection-over-Union (overall IoU) and Precision@X as metrics. Overall IoU metric calculates the ratio of the intersection and the union computed between the predicted segmentation mask and the ground truth mask over all test samples. Precision@X metric calculates the percentage of test samples having IoU greater than the threshold X , with X ∈ {0.5, 0.6, 0.7, 0.8, 0.9}."
    }, {
      "heading" : "4.3 Comparison with State of the Art",
      "text" : "We evaluate our method’s performance on four benchmark datasets and present the results in Table 1. Since three of the datasets are derived from MS-COCO and have significant overlap with each other, pre-training on MS-COCO can give misleading results and should be avoided. Hence, we only compare against methods for which the backbone is pre-trained on Pascal VOC. Our approach, SHNet (SFM+HCAM), achieves state-of-the-art performance on three datasets without post-processing. In contrast, most previous methods present results after post-processing through a Dense Conditional Random Field (Dense CRF).\nThe expressions in UNC+ avoid using positional words while referring to objects; instead, they are more descriptive about their attributes and relationships. Consistent performance gains on the UNC+ dataset at all splits showcases the effectiveness of utilizing comprehensive interactions simultaneously across visual and linguistic modalities. Similarly, our approach gains 1.68% over the next best performing method EFN (Feng et al., 2021) on the Referit dataset, reflecting its ability to ground unstructured regions (e.g., the sky, free space). We also achieve solid performance gains on the UNC dataset at both resolutions, indicating that our method can effectively utilize the positional words to localize the correct instance of an object\nfrom multiple ones. EFN (Feng et al., 2021) (underlined in Table 1) gives the best performance on G-Ref dataset; however, it is fine-tuned on the UNC pre-trained model. With similar fine-tuning, SHNet achieves 56.44% overall IoU, surpassing EFN by a large margin. However, such an experimental setup is incorrect, as there is a significant overlap between G-Ref test and UNC training set. Hence, in Table 1 we report performance on a model trained on G-Ref from scratch. Performance of SHNet is marginally below BusNet on the G-Ref dataset. Feature maps in SHNet have a lower resolution of 18 × 18 compared to 40 × 40 resolution used by other methods and that possibly leads to a drop in performance on G-Ref, which has extremely small target objects. We could not train SHNet on higher resolution feature maps due to memory limits induced by multi-head attention (on RTX 2080Ti GPU); however, training on higher resolution input improves results."
    }, {
      "heading" : "4.4 Ablation Studies",
      "text" : "We perform ablation studies on the UNC dataset’s validation split. All methods are evaluated on Precision@X and Overall IoU metrics, and the results are illustrated in Table 2. Unless specified, the backbone used for ablations is DeepLabv3+\ntrained at 320× 320 resolution. The feature extraction process described in Section 3.1 is used for all ablation studies. ASPP + ConvUpsample decoder is also common to all the experiments.\nBaseline: The baseline model involves direct concatenation of visual features with the tiled textual feature to result in multi-modal feature of shape R(Cv+Cl)×H×W . This multi-modal feature is passed as input to ASPP + ConvUpsample decoder.\nHCAM without SFM: “Only HCAM\" network differs with baseline method only on the fusion process of hierarchical multi-modal features. Introducing the HCAM module over baseline results in 4.83 % improvement on the Overall IoU metric and an improvement of 2.5 % on the prec@0.9 metric (illustrated in Table 2), indicating that the HCAM module results in refined segmentation masks.\nSFM without HCAM: Similarly, the “Only SFM\" network differs from the baseline method in how different types of visual-linguistic interactions are captured. We observe significant performance gains of 7.46 % over the baseline, indicating that simultaneous interactions help identify the referent.\nSFM + X: We replace HCAM module with other multi-level fusion techniques like ConvLSTM and Conv3D. Comparing the performance\nof SFM+ConvLSTM with SHNet (SFM+HCAM), we observe that HCAM is indeed effective at fusing hierarchical multi-modal features (Table 2). For SFM+Conv3D, we stack multi-level features along a new depth dimension resulting in 3D features, and perform 3D convolution on them. The same filter is applied to different level features that result in each level feature converging on a common region in the image. SFM+Conv3D achieves a similar performance as SFM+ConvLSTM while using fewer parameters. Using Conv3D achieves higher Precision@0.8 and Precision@0.9 than ConvLSTM, suggesting that it leads to more refined maps. It is worth noting that HCAM also uses Conv3D at the end, and the additional gains of SHNet over SFM+Conv3D suggest the benefits of hierarchical information exchange in HCAM.\nGlove and Positional Embeddings: We verify Glove embeddings’ significance by replacing it with one hot embedding. We also validate the usefulness of Positional Embeddings (P.E.) by training a model without them. Both variants observe a drop in performance (Table 2), with the drop being more significant in the variant without Glove em-\nbeddings. These ablations suggest the importance of capturing word-level semantics and positionalaware features.\nIn Table 3, we present ablations with different backbones at different resolution. The results demonstrate that our approach does not heavily rely on backbone for its performance gains, as even with a vanilla Imagenet pre-trained Resnet101 backbone, not fine-tuned on segmentation task, we outperform existing methods at both resolutions. Predictably, using a backbone fine-tuned on a segmentation task gives further performance gain.\nWe also present ablations with different aggregation modules in Table 4. We use the modules presented in MGATE (Ye et al., 2019), TGFE (Huang et al., 2020) and GBFM (Hui et al., 2020), for\nwhich codes were publicly available. HCAM consistently outperforms other methods by clear margins at both resolution."
    }, {
      "heading" : "4.5 Qualitative Results",
      "text" : "Figure 4 presents qualitative results comparing SHNet against the baseline model. SHNet localizes heavily occluded objects (Figure 4 (a) and (b)); reasons on the overall essence of the highly ambiguous sentences (e.g. “person you cannot see\", “right photo not left photo\") and; distinguishes among multiple instances of the same type of object based on attributes and appearance cues (Figure 4 (b), (c), and (e)). While, without any reasoning stage, the baseline model struggles to segment the correct instance and confuse it with similar objects. Figure 4 (d) and (f) illustrate the ability of SHNet to localize unstructured non-explicit objects like “dark area\" and “blue thing\". The potential of SHNet to perform relative positional reasoning is highlighted\nin Figure 4 (b), (e), and (f). We outline the contributions of both SFM and HCAM modules in Figure 5. “Only HCAM\" network does not involve any reasoning, however, it manages to predict the left sandwich with refined boundaries. “Only SFM\" network understands the concept of “the right half of the sandwich\" and leads to much better output; however, the output mask bleeds around the boundaries, and an extra small noisy segment is visible. The full model benefits from the reasoning in “SFM,\" and when combined with HCAM facilitates information exchange across hierarchies to predict correct refined mask as output. In Figure 6, we anchor an image and vary the linguistic expression. SHNet is able to reason about different linguistic expressions successfully and ground them. Inter-modal and Intramodal interactions captured by SFM are illustrated in Figure 7. Pixel-pixel interactions highlight image regions corresponding to the referent. For the given expression, “squares\" contains the differentiating information and is assigned high importance for different words. Additionally, for each word appropriate region in the image is attended.\nAdditional qualitative examples with success and failure cases are provided in the supplementary material."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we tackled the task of Referring Image Segmentation. We proposed a simple yet effective SFM to capture comprehensive interactions between modalities in a single step, allowing us to simultaneously consider the contextual information from both modalities. Furthermore, we introduced a novel HCAM module to aggregate multi-modal context across hierarchies. Our approach achieves strong performance on RIS benchmarks without any post-processing. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of all the proposed components."
    } ],
    "references" : [ {
      "title" : "See-throughtext grouping for referring image segmentation",
      "author" : [ "Ding-Jie Chen", "Songhao Jia", "Yi-Chen Lo", "HwannTzong Chen", "Tyng-Luh Liu." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Encoderdecoder with atrous separable convolution for semantic image segmentation",
      "author" : [ "Liang-Chieh Chen", "Yukun Zhu", "George Papandreou", "Florian Schroff", "Hartwig Adam." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Uniter: Universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "Computer Vision – ECCV 2020, pages 104–120, Cham. Springer International",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual grounding via accumulated attention",
      "author" : [ "Chaorui Deng", "Qi Wu", "Qingyao Wu", "Fuyuan Hu", "Fan Lyu", "Mingkui Tan." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Deng et al\\.,? 2018",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2018
    }, {
      "title" : "Encoder fusion network with co-attention embedding for referring image segmentation",
      "author" : [ "Guang Feng", "Zhiwei Hu", "Lihe Zhang", "Huchuan Lu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15506–",
      "citeRegEx" : "Feng et al\\.,? 2021",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "Segmentation from natural language expressions",
      "author" : [ "Ronghang Hu", "Marcus Rohrbach", "Trevor Darrell." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Hu et al\\.,? 2016a",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language object retrieval",
      "author" : [ "Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Hu et al\\.,? 2016b",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Bi-directional relationship inferring network for referring image segmentation",
      "author" : [ "Zhiwei Hu", "Guang Feng", "Jiayu Sun", "Lihe Zhang", "Huchuan Lu." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Referring image segmentation via cross-modal progressive comprehension",
      "author" : [ "Shaofei Huang", "Tianrui Hui", "Si Liu", "Guanbin Li", "Yunchao Wei", "Jizhong Han", "Luoqi Liu", "Bo Li." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Linguistic structure guided context modeling for referring image segmentation",
      "author" : [ "Tianrui Hui", "Si Liu", "Shaofei Huang", "Guanbin Li", "Sansi Yu", "Faxi Zhang", "Jizhong Han." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Hui et al\\.,? 2020",
      "shortCiteRegEx" : "Hui et al\\.",
      "year" : 2020
    }, {
      "title" : "Referit game: Referring to objects in photographs of natural scenes",
      "author" : [ "Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L. Berg." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kazemzadeh et al\\.,? 2014",
      "shortCiteRegEx" : "Kazemzadeh et al\\.",
      "year" : 2014
    }, {
      "title" : "Referring image segmentation via recurrent refinement networks",
      "author" : [ "Ruiyu Li", "Kaican Li", "Yi-Chun Kuo", "Michelle Shu", "Xiaojuan Qi", "Xiaoyong Shen", "Jiaya Jia." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "M. Maire", "Serge J. Belongie", "James Hays", "P. Perona", "D. Ramanan", "Piotr Dollár", "C.L. Zitnick." ],
      "venue" : "ArXiv, abs/1405.0312.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning cross-modal context graph for visual grounding",
      "author" : [ "Yongfei Liu", "Bo Wan", "Xiaodan Zhu", "Xuming He." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):11645–11652.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Generation and comprehension of unambiguous object descriptions",
      "author" : [ "Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan L Yuille", "Kevin Murphy." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "citeRegEx" : "Mao et al\\.,? 2016",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Conditional image-text embedding networks",
      "author" : [ "Bryan A. Plummer", "Paige Kordas", "M. Hadi Kiapour", "Shuai Zheng", "Robinson Piramuthu", "Svetlana Lazebnik." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "Plummer et al\\.,? 2018",
      "shortCiteRegEx" : "Plummer et al\\.",
      "year" : 2018
    }, {
      "title" : "Grounding of textual phrases in images by reconstruction",
      "author" : [ "Anna Rohrbach", "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Rohrbach et al\\.,? 2016",
      "shortCiteRegEx" : "Rohrbach et al\\.",
      "year" : 2016
    }, {
      "title" : "Key-word-aware network for referring expression image segmentation",
      "author" : [ "Hengcan Shi", "Hongliang Li", "Fanman Meng", "Qingbo Wu." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Shi et al\\.,? 2018",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2018
    }, {
      "title" : "Crossmodal relationship inference for grounding referring expressions",
      "author" : [ "Sibei Yang", "Guanbin Li", "Yizhou Yu." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bottom-up shift and reasoning for referring image segmentation",
      "author" : [ "Sibei Yang", "Meng Xia", "Guanbin Li", "Hong-Yu Zhou", "Yizhou Yu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11266–11275.",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Cross-modal self-attention network for referring image segmentation",
      "author" : [ "Linwei Ye", "Mrigank Rochan", "Zhi Liu", "Yang Wang." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Ye et al\\.,? 2019",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling context in referring expressions",
      "author" : [ "Licheng Yu", "Patrick Poirson", "Shan Yang", "Alexander C Berg", "Tamara L Berg." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "The current SOTA methods (Yang et al., 2021; Feng et al., 2021; Huang et al., 2020; Hui et al., 2020; Hu et al., 2020) take a modular approach, where these interactions happen in parts, sequentially.",
      "startOffset" : 25,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "The current SOTA methods (Yang et al., 2021; Feng et al., 2021; Huang et al., 2020; Hui et al., 2020; Hu et al., 2020) take a modular approach, where these interactions happen in parts, sequentially.",
      "startOffset" : 25,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "The current SOTA methods (Yang et al., 2021; Feng et al., 2021; Huang et al., 2020; Hui et al., 2020; Hu et al., 2020) take a modular approach, where these interactions happen in parts, sequentially.",
      "startOffset" : 25,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "The current SOTA methods (Yang et al., 2021; Feng et al., 2021; Huang et al., 2020; Hui et al., 2020; Hu et al., 2020) take a modular approach, where these interactions happen in parts, sequentially.",
      "startOffset" : 25,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "The current SOTA methods (Yang et al., 2021; Feng et al., 2021; Huang et al., 2020; Hui et al., 2020; Hu et al., 2020) take a modular approach, where these interactions happen in parts, sequentially.",
      "startOffset" : 25,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : ", 2021) and (Hui et al., 2020) use the dependency tree structure of the referring expression for the reasoning stage instead.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "CMPC (Huang et al., 2020) which predicts the referent word from the expression in the first stage, identifies \"people\" as the referent (middle image in Figure 1) and completely misses \"anywhere\" which is the correct entity to ground.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : ", 2021), and (Hui et al., 2020), which utilize dependency tree structure to govern their reasoning process, identify the referred entity \"anywhere\" as an adverb from the dependency tree.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "Earlier methods, (Hu et al., 2016b; Rohrbach et al., 2016; Plummer et al., 2018) used joint embedding space as a metric space to rank proposal features with",
      "startOffset" : 17,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "Earlier methods, (Hu et al., 2016b; Rohrbach et al., 2016; Plummer et al., 2018) used joint embedding space as a metric space to rank proposal features with",
      "startOffset" : 17,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "Earlier methods, (Hu et al., 2016b; Rohrbach et al., 2016; Plummer et al., 2018) used joint embedding space as a metric space to rank proposal features with",
      "startOffset" : 17,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "Later methods like (Yang et al., 2019; Deng et al., 2018; Liu et al., 2020) utilized attention over the proposals to select the appropriate one.",
      "startOffset" : 19,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Later methods like (Yang et al., 2019; Deng et al., 2018; Liu et al., 2020) utilized attention over the proposals to select the appropriate one.",
      "startOffset" : 19,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "Later methods like (Yang et al., 2019; Deng et al., 2018; Liu et al., 2020) utilized attention over the proposals to select the appropriate one.",
      "startOffset" : 19,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "More Recent Methods like (Lu et al., 2019; Chen et al., 2020) utilize transformer-based",
      "startOffset" : 25,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "More Recent Methods like (Lu et al., 2019; Chen et al., 2020) utilize transformer-based",
      "startOffset" : 25,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Additionally, compared to (Lu et al., 2019), we explicitly capture inter-modal and intra-modal interactions between visual and linguistic modalities.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "It was first introduced in (Hu et al., 2016a), where they generate the referent’s segmentation mask by directly concatenating visual features from CNN with tiled language features from LSTM.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "(Li et al., 2018) generates refined segmentation masks by incorporating multi-scale semantic information from the image.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "Since each word in expression makes a different contribution in identifying the desired object, (Shi et al., 2018) model visual context for each word separately using query attention.",
      "startOffset" : 96,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "(Ye et al., 2019) uses a self-attention mechanism to capture long-range correlations between visual and textual",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : ", 2020) utilize cross-modal attention to model multi-modal context, (Hui et al., 2020; Yang et al., 2021) use dependency tree structure and (Huang et al.",
      "startOffset" : 68,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : ", 2020) utilize cross-modal attention to model multi-modal context, (Hui et al., 2020; Yang et al., 2021) use dependency tree structure and (Huang et al.",
      "startOffset" : 68,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : ", 2021) use dependency tree structure and (Huang et al., 2020) use coarse labelling for each word in the expression for selective context",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "SFM is applied to hierarchical visual features extracted from CNN backbone since hierarchical features are better suited for segmentation tasks (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020).",
      "startOffset" : 144,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "SFM is applied to hierarchical visual features extracted from CNN backbone since hierarchical features are better suited for segmentation tasks (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020).",
      "startOffset" : 144,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "SFM is applied to hierarchical visual features extracted from CNN backbone since hierarchical features are better suited for segmentation tasks (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020).",
      "startOffset" : 144,
      "endOffset" : 197
    }, {
      "referenceID" : 1,
      "context" : "Finally, G is passed through Atrous Spatial Pyramid Pooling (ASPP) decoder (Chen et al., 2018) and Up-sampling convolution to predict final segmentation mask S.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "UNC (Yu et al., 2016) contains 19,994 images taken from MS-COCO (Lin et al.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : ", 2016) contains 19,994 images taken from MS-COCO (Lin et al., 2014) with 142,209 referring expressions",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "UNC+ (Yu et al., 2016) is also based on images from MS-COCO.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "G-Ref (Mao et al., 2016) is also curated using images from MS-COCO.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "We experiment with two backbones, DeepLabv3+ (Chen et al., 2018) and Resnet-101 for image feature extraction.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "Like previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), DeepLabv3+ is pre-trained on Pascal VOC semantic segmentation task while Resnet-101 is pre-trained on Imagenet Classification task, and both backbone’s parameters are fixed during training.",
      "startOffset" : 20,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Like previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), DeepLabv3+ is pre-trained on Pascal VOC semantic segmentation task while Resnet-101 is pre-trained on Imagenet Classification task, and both backbone’s parameters are fixed during training.",
      "startOffset" : 20,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Like previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), DeepLabv3+ is pre-trained on Pascal VOC semantic segmentation task while Resnet-101 is pre-trained on Imagenet Classification task, and both backbone’s parameters are fixed during training.",
      "startOffset" : 20,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "We use GLoVe embeddings (Pennington et al., 2014) pre-trained on Common Crawl 840B tokens to initialize word embedding for words in the expressions.",
      "startOffset" : 24,
      "endOffset" : 49
    }, {
      "referenceID" : 22,
      "context" : "Evaluation Metrics: Following previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), we evaluate the performance of our model using overall Intersection-over-Union (overall IoU) and Precision@X as metrics.",
      "startOffset" : 45,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Evaluation Metrics: Following previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), we evaluate the performance of our model using overall Intersection-over-Union (overall IoU) and Precision@X as metrics.",
      "startOffset" : 45,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "Evaluation Metrics: Following previous works (Ye et al., 2019; Chen et al., 2019; Hu et al., 2020), we evaluate the performance of our model using overall Intersection-over-Union (overall IoU) and Precision@X as metrics.",
      "startOffset" : 45,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "68% over the next best performing method EFN (Feng et al., 2021) on the Referit dataset, reflecting its ability to ground unstructured regions (e.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "Method UNC UNC+ G-Ref Referit val testA testB val testA testB val test RRN (Li et al., 2018) 55.",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 22,
      "context" : "We use the modules presented in MGATE (Ye et al., 2019), TGFE (Huang et al.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : ", 2019), TGFE (Huang et al., 2020) and GBFM (Hui et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "Aggregation Module Overall IOU 320x320 448x448 MGATE (Ye et al., 2019) 62.",
      "startOffset" : 53,
      "endOffset" : 70
    } ],
    "year" : 0,
    "abstractText" : "We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the natural language description. Addressing RIS efficiently requires considering the interactions happening across visual and linguistic modalities and the interactions within each modality. Existing methods are limited because they either compute different forms of interactions sequentially (leading to error propagation) or ignore intramodal interactions. We address this limitation by performing all three interactions simultaneously through a Synchronous Multi-Modal Fusion Module (SFM). Moreover, to produce refined segmentation masks, we propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where linguistic features facilitate the exchange of contextual information across the visual hierarchy. We present thorough ablation studies and validate our approach’s performance on four benchmark datasets, showing considerable performance gains over the existing state-ofthe-art (SOTA) methods.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_102_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "From the Detection of Toxic Spans in Online Discussions to the Analysis of Toxic-to-Civil Transfer",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In social media and online fora, toxic content can be defined as rude, disrespectful, or unreasonable posts that would make users want to leave the conversation (Borkan et al., 2019). Although several toxicity detection datasets (Wulczyn et al., 2017; Borkan et al., 2019) and models (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) exist, most of them classify whole posts, without identifying the specific spans that make a text toxic. But highlighting such toxic spans can assist human moderators (e.g., news portal moderators) who often deal with lengthy comments, and who prefer attribution instead of just a system-generated unexplained toxicity score per post. Locating toxic spans within a text is thus a crucial step towards successful semi-automated moderation and healthier online discussions.\nTo promote research on this new task, we release the first dataset of English posts with annotations of toxic spans, called TOXICSPANS.1 We discuss\n1URL hidden to avoid revealing the identity of the authors. Part of the dataset was used in a challenge with the permission of the authors. We do not provide further information about\nhow it was created and we propose an evaluation framework for toxic spans detection. We consider methods that (i) perform sequence labeling (tag words) or (ii) rely on an attentional binary classifier to predict if a post is toxic or not, then invoke its attention at inference time to obtain toxic spans as in rationale extraction. The latter approach allows leveraging larger existing training datasets, which provide gold labels indicating which posts are toxic or not, without providing gold toxic span annotations. Although sequence labeling performed overall better, the binary attentional classifier performed surprisingly well too, despite having been trained on data without span annotations.\nWe then study some characteristics of supervised and self-supervised toxic-to-civil transfer models (Laugier et al., 2021) by comparing them on several datasets, including a recently released parallel toxic-to-civil dataset (Dementieva et al., 2021) and the new TOXICSPANS dataset. Using the latter, we introduce a measure to evaluate the elimination of explicit toxicity, and we use this measure to compare the behavior and performance of toxicto-civil models. Lastly, by applying toxic span detection systems, we assess the performance of human crowdworkers on the toxic-to-civil task."
    }, {
      "heading" : "2 Related work",
      "text" : "Toxicity detection systems (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) are typically trained on datasets annotated at the post level (a text is annotated as toxic or not) (Wulczyn et al., 2017; Borkan et al., 2019). Our work differs from general toxicity detection in that we detect toxic spans, instead of assigning toxicity labels to entire texts. Toxic spans detection can be seen as a case of attribution or rationale extraction (Li et al., 2016; Ribeiro et al., 2016), but specifically for toxic posts, a task that has never been\nthe challenge to preserve anonymity. The full dataset and the code of this work will be released with a CC0 licence.\nconsidered in general toxicity detection before. Detecting spans, instead of entire posts, was recently also considered in propaganda (Martino et al., 2020) and hate speech detection (Mathew et al., 2021). Although the ground truth type is similar (spans), propaganda detection is a different task from ours. Hate speech is a particular type of toxicity (Borkan et al., 2019), which can be tackled by more general toxicity detectors (Van Aken et al., 2018), but not the other way round; i.e., we address a broader problem. This probably explains why a pattern-matching baseline, based on the data of Mathew et al. (2021), achieved only slightly better results than a random baseline on our dataset.\nSuggesting civil rephrases of posts found to be toxic (Nogueira dos Santos et al., 2018; Laugier et al., 2021) is the next step towards healthier online discussions, and can be viewed as a form of style transfer (Shen et al., 2017; Fu et al., 2018; Lample et al., 2019). We show how toxic spans detection can also contribute in the assessment of toxic-tocivil transfer, linking the two tasks together for the first time."
    }, {
      "heading" : "3 The new TOXICSPANS dataset",
      "text" : "We used posts (comments) from the publicly available Civil Comments dataset (Borkan et al., 2019), which already provides whole-post toxicity annotations. We followed the toxicity definition that was used in Civil Comments, i.e., we use ‘toxic’ as an umbrella term that covers abusive language phenomena, such as insults, hate speech, identity attack, or profanity. This definition of toxicity has been used extensively in previous work (Hosseini et al., 2017; Van Aken et al., 2018; Karan and Šnajder, 2019; Han and Tsvetkov, 2020; Pavlopoulos et al., 2020). We asked crowd annotators to highlight the spans that constitute “anything that is rude, disrespectful, or unreasonable that would make someone want to leave a conversation”. Besides toxicity our annotators were also asked to select a subtype for each highlighted span, choosing between insult, threat, identity-based attack,\nprofane/obscene, or other toxicity. Asking the annotators to also select a category was intended as a priming exercise to increase their engagement, but it may have also helped them align their notions of toxicity further, increasing inter-annotator agreement. For the purposes of our experiments, we collapsed all the subtypes into a single toxic class, and we did not study them further; but the subtypes are included in the new dataset we release.\nAnnotation From the original Civil Comments dataset (1.2M posts), we retained only posts that had been found toxic by at least half of the crowdraters. This left approximately 30k toxic posts. We selected a random 11k subset of the 30k posts for toxic spans annotation. We used the crowdannotation platform of Appen.2 We employed three crowd-raters per post, all of whom were warned for explicit content. Raters were selected from the smallest group of the most experienced and accurate contributors. The raters were asked to mark the toxic word sequences (spans) of each post by highlighting each toxic span on their screen. If they believed a post was not actually toxic, or that the entire post would have to be annotated, they were instructed to select the appropriate tick-boxes in the interface, without highlighting any span.\nIt is not possible to annotate toxic spans for every toxic post. For example, in some posts the core message being conveyed may be inherently toxic (e.g., a sarcastic post indirectly claiming that people of a particular origin are inferior) and, hence, it may be difficult to attribute the toxicity of those posts to particular spans. In such cases, the posts may end up having no toxic span annotations, according to the guidelines given to the annotators; see the last post of Table 1 for an example. In other cases, however, it is easier to identify particular spans (possibly multiple per post) that make a post toxic, and these toxic spans often cover only a small part of the post (see Table 1 for examples).\n2https://appen.com/\nAgreement We measured inter-annotator agreement on 87 randomly selected posts of our dataset, using 5 crowd-annotators per post in this case. We calculated the mean pairwise (for a pair of annotators) Cohen’s kappa per post, using character offsets as instances being classified as toxic (included in a toxic span) or non-toxic; we then averaged over the posts. Although our dataset contains only posts found toxic by at least half of the original crowd-raters, only 31 of the 87 posts were found toxic by all five of our annotators, and 51 were found toxic by the majority of our annotators; this is an indicator of the well-known subjectivity of toxicity detection. On the 31, 51, and 87 posts, the average kappa score was 65%, 55%, 48%, respectively, indicating that when the raters agree (at least by majority) about the toxicity of the post, there is also reasonable agreement regarding the toxic spans. Note that the toxic spans are typically short. This leads to class imbalance (most offsets are marked as non-toxic), increases agreement by chance (on the non-toxic offsets), and leads to low kappa scores (kappa adjusts for chance agreement). Another reason behind this modest (compared to other tasks) inter-annotator agreement is the inherent subjectivity of deciding if a post is toxic or not. Our kappa score is in fact slightly higher than in previous work on toxicity detection, classifying posts as toxic or not (Sap et al., 2020; Pavlopoulos et al., 2017a), and in that sense our inter-annotator agreement can be seen as an improvement.\nGround truth To obtain the ground truth of our dataset, we averaged the labels per character of the annotators per post. We used the following process: for each post t, first we mapped each annotated span of each rater to its character offsets. We then assigned a toxicity score to each character offset of t, computed as the fraction of raters who annotated that character offset as toxic (included it in their toxic spans). We retained only character offsets with toxicity scores higher than 50%; i.e., at least two raters must have included each character offset in their spans. Table 1 shows examples.\nThe dataset TOXICSPANS contains the 11,035 posts we annotated for toxic spans. The unique posts are actually 11,006, since a few were duplicates and were removed; a few other posts were used as quiz questions to check the reliability of candidate annotators and have also been removed.\nExploratory analysis Although we instructed the crowd-raters to click the appropriate tick-box and not highlight any span when the whole post would have to be highlighted, the ground truth of 34 out of the 11k posts covers the entire post. However, 14 out of the 34 posts are single-word texts, while the other posts are very short (Appendix A shows more details); it seems that in very short posts the raters sometimes did not realize they ended up highlighting the entire post. Furthermore, about 5k of the 11k posts have an empty ground truth set of toxic character offsets (as in the last post of Table 1), even though all the posts of our dataset had been found toxic by the original raters. This is partly due to the fact that we include in the ground truth only character offsets that were included in the toxic spans of the majority of our annotators. It also confirms it is not always possible to attribute (at least not by consensus) the toxicity of a post to particular toxic spans. In almost all posts, the ground truth covers less than half of the post; and in the vast majority, less than 20% of the post. A dense toxic span of a post is a maximal sequence of contiguous toxic characters. There exist posts with more than one dense toxic span, but most posts include only one. Table 2 provides further statistics."
    }, {
      "heading" : "4 Evaluation framework for toxic spans",
      "text" : "For the newly introduced toxic spans detection task, we evaluate systems in terms of F1 score, as in the work of Da San Martino et al. (2019). Given a test post t, let system Ai return a set StAi of character offsets, for parts of the post found to be toxic. Let StG be the character offsets of the ground truth annotations of t. We compute the F1 score of system Ai with respect to the ground truth G for post t:\nF t1(Ai, G) = 2 · P t(Ai, G) ·Rt(Ai, G) P t(Ai, G) +Rt(Ai, G)\n(1)\nP t(Ai, G) = |StAi ∩ S t G|\n|StAi | , Rt(Ai, G) =\n|StAi ∩ S t G|\n|StG| (2)\nIf StG is empty for some post t (no gold spans are given for t), we set F t1(Ai, G) = 1 if S t Ai\nis also empty, and F t1(Ai, G) = 0 otherwise. We average F t1(Ai, G) over all test posts t to obtain a single score for system Ai. We use F1 as the main evaluation measure in experiments reported below."
    }, {
      "heading" : "5 Methods for toxic spans detection",
      "text" : "TRAIN-MATCH, classifies as toxic any tokens encountered inside toxic spans of the training data.\nHATE-MATCH operates similarly but the lookup is within the hateful/offensive spans of the data of Mathew et al. (2021). A naive baseline, RAND-SEQ, randomly classifies tokens as toxic or not."
    }, {
      "heading" : "5.1 Supervised sequence labelling",
      "text" : "Toxic spans detection can be seen as sequence labeling (tagging words). As a baseline of this kind, we employ SPACY’S Convolutional Neural Network, which is pre-trained for tagging, parsing, entity recognition (Honnibal and Montani, 2017). We call this model CNN-SEQ and fine-tune it on dense toxic spans, treated as ‘entities’. We also train a bidirectional LSTM (BILSTM-SEQ),3 and fine-tune BERT (Devlin et al., 2019) and SPANBERT (Joshi et al., 2020) for toxic spans (BERTSEQ, SPAN-BERT-SEQ).4 These methods require training data manually annotated with toxic spans."
    }, {
      "heading" : "5.2 Weakly supervised learning",
      "text" : "We trained binary classifiers to predict the toxicity label of each post, and we employed attention as a rationale extraction mechanism at inference to obtain toxic spans, an approach Pavlopoulos et al. (2017b) found to work reasonably well in toxicity detection.5 We experimented with two classifiers: a BILSTM with deep self-attention as in the work of Pavlopoulos et al. (2017b), but training with a regression objective and probabilistic labels following D’Sa et al. (2020) and Wulczyn et al. (2017); and BERT with a dense layer and sigmoid on the [CLS] embedding. To detect toxic spans, we used the attention scores of the BILSTM and the attention scores from the heads of BERT’s last layer averaged over the heads, respectively. In both cases, we obtain a sequence of binary decisions (toxic, nontoxic) for the tokens of the post (inherited by their character offsets) by using a probability threshold (tuned on development data) applied to the attention scores. We refer to these two attention-based\n3We used the probabilistic ground truth for training and mean square error as the loss function of BILSTM-SEQ, which yielded best results in preliminary experiments.\n4More details can be found in the Appendix A.3. 5See Wiegreffe and Pinter (2019); Kobayashi et al. (2020); Ferrando and Costa-jussà (2021) for a broader discussion of attention as an explainability mechanism.\nrationale extraction methods as BILSTM+ARE and BERT+ARE, respectively. These methods require training posts annotated only with toxicity labels per post (no toxic span annotations)."
    }, {
      "heading" : "6 Experimental results for toxic spans",
      "text" : "We used a 5-fold Monte Carlo cross-validation (5 random training/development/test splits) on the 11k posts of TOXICSPANS. In each fold, we use 10% of the data for testing, 10% for development, and 80% for training. In ARE-based methods, which rely on an underlying classifier to predict if a post is toxic or not, the classifier is trained on the training part of the fold (which contains only toxic posts, ignoring the toxic span annotations) and a randomly selected equal number of non-toxic posts from Civil Comments that are not included in our dataset. When measuring the (binary) classification performance of the underlying classifier, the classifier is evaluated on a new equally balanced test set of 3k randomly sampled unseen posts from Civil Comments.\nBoth look-up methods (TRAIN-MATCH, HATEMATCH) outperform the random baseline (Table 3). However, TRAIN-MATCH performs much better, which agrees with our hypothesis that toxicity detection is a broader problem than hate speech detection. Both look-up methods are outperformed by the sequence labeling models (-SEQ), especially SPAN-BERT-SEQ, which is pre-trained to predict spans. These results show that the tokens of toxic spans are context-dependent and their meaning is not captured well by context-unaware look-up lexicons. An error analysis of the best-performing\nSPAN-BERT-SEQ showed that its most common mistakes are false negatives (e.g., incorrectly returning an empty span, 1st row of Table 4) and false positives (2nd and 3rd row). BERT+ARE performs worse than BILSTM+ARE, despite the fact that the underlying BERT classifier is much better (ROC AUC 96.1%) at separating toxic from nontoxic posts than the underlying BILSTM (90.9%). Interestingly, the BILSTM binary toxicity classifier with the attention-based toxic span detection mechanism (Pavlopoulos et al., 2017b) is close in performance with BILSTM-SEQ, despite the fact that the latter is directly trained on toxic span annotations, whereas the former is trained with binary post-level annotations only (toxic, non-toxic post).\nSeveral large datasets with post-level toxicity annotations are publicly available (Pavlopoulos et al., 2019). Therefore, attribution-based toxic span detectors, such as BILSTM+ARE, can in principle perform even better if the underlying binary classifier is trained on a larger existing dataset. To investigate this, we increased the training set of the underlying BILSTM classifier of BILSTM+ARE. We added to the training set of each cross-validation fold 80k further toxic and non-toxic posts (still equally balanced, without toxic spans) from the dataset of Borkan et al. (2019), excluding posts used in TOXICSPANS. The ROC AUC score of the underlying BILSTM (in the task of separating toxic from nontoxic posts) improved from 90.9% to 94.2%, and the F1 score of BILSTM+ARE (in toxic spans detection) improved from 57.7% to 58.8%, almost reaching the performance of BILSTM-SEQ.6"
    }, {
      "heading" : "7 Toxic spans in toxic-to-civil transfer",
      "text" : "As shown in Section 6, a toxic span detection method can be used to highlight toxic parts of a post, to assist, for instance, human moderators. The new TOXICSPANS dataset and toxic span detection methods, however, can assist in more ways. This section describes how we combined the new dataset and the best-performing toxic span detector (SPANBERT-SEQ) to show how they can be useful in toxic-\n6Appendix A reports results for less added data.\nto-civil text transfer (Nogueira dos Santos et al., 2018; Laugier et al., 2021). In the context of detoxifying comments to nudge healthier conversations online, this task aims at suggesting civil rephrasings of toxic posts. More specifically, we study the following research question: “Can TOXICSPANS data and toxic span detectors be used to assess the mitigation of explicit toxicity in toxic-to-civil transfer?” To answer this question, we proceeded in two ways: (i) evaluating the transfer of toxic spans in system-detoxified posts, and (ii) studying any remaining toxic spans in human-detoxified posts."
    }, {
      "heading" : "7.1 System-detoxified posts",
      "text" : "We first compare the performance of two toxic-tocivil transfer models, CAE-T5 and SED-T5, both based on the T5 transformer encoder-decoder architecture (Raffel et al., 2019); they both fine-tune the weights of the same pre-trained model, namely T5-large. CAE-T5 (Laugier et al., 2021) is a selfsupervised Conditional Auto-Encoder, fine-tuned on a large non-parallel (NP) dataset based on preprocessed posts from the Civil Comments (CC) dataset, the dataset (with post level annotations) that TOXICSPANS was also based on. SED-T5 is a Supervised Encoder-Decoder. We fine-tuned it on a smaller parallel (P) dataset created by Dementieva et al. (2021), consisting of pairs of comments: a toxic comment and a detoxified paraphrase written by a crowd-worker.\nTable 5 summarizes statistics of the two datasets (P, NP) and highlights a trade-off between the level of supervision and number of samples: there is a 1:40 ratio between toxic comments in P (direct supervision, parallel data) and NP (indirect supervision, no parallel data). Table 6 shows our exper-\nimental results. Following Laugier et al. (2021), we report accuracy (ACC), perplexity (PPL), similarity (SIM) and the geometric mean (GM) of ACC, 1/PPL, SIM. Accuracy measures the rate of successful transfers from toxic to civil, and computes the fraction of posts whose civil version is classified as non-toxic by a BERT toxicity classifier.7 Perplexity is used here as a measure of fluency and it is computed with GPT-2 (Radford et al., 2019). Similarity measures content preservation between the original toxic text and its system-rephrased civil version (self-SIM) or between the original toxic text and its gold (human) civil rephrasing (ref-SIM, only for P); in both cases, it is computed as the cosine similarity between the single-vector representations of the two texts, produced by the universal sentence encoder (Cer et al., 2018).\nAs can be seen in Table 6, CAE-T5 has better aggregated results (higher GM) than SED-T5 in all three datasets, which are due to lower perplexity and (in NP and TOXICSPANS) higher accuracy. However, SED-T5 learned to preserve content better (higher SIM in all three datasets), because of the parallel data (P, with gold rephrases) it was trained on. By contrast, CAE-T5 was trained without parallel data (NP) using a cycle-consistency loss, which leads to more frequent hallucinations of content that was not present in the original post (Laugier et al., 2021). These hallucinations may also help CAE-T5 obtain better perplexity scores, by generating fluent civil ‘rephrases’ that do not preserve, however, the original semantics. Also, although the general trends are similar in all three datasets (SED-T5 preserves content better, CAE-T5 is better in perplexity and GM), there are several differences too across the three datasets. For example, CAET5 is much better than SED-T5 in accuracy (posts detoxified) on NP and TOXICSPANS, but both systems have the same accuracy on P; and the scores of the systems vary a lot across the three datasets.\nThese considerations motivated us to seek ways to further analyse the behavior of toxic-to-civil transfer models. TOXICSPANS and toxic span detectors are an opportunity to move towards this direction, by studying how well transfer models cope with explicit toxicity, i.e., spans that can be explicitly pointed to as sources of toxicity. We leave for future work the flip side of this study, i.e., studying cases where transfer models rephrase spans not explicitly marked (by toxic span detectors or human\n7We reused the BERT model of Laugier et al. (2021).\nannotators) as explicitly toxic."
    }, {
      "heading" : "7.2 Explicit Toxicity Removal Accuracy",
      "text" : "Recall that the accuracy (ACC) scores of Table 6 measure the percentage of toxic posts that the transfer models (CAE-T5, SED-T5) rephrased to forms that a (BERT-based) toxicity classifier considered non-toxic. One could question, however, if it is possible (even for humans) to produce a civil rephrase of a toxic post when it is impossible to point to particular spans of the post that cause its toxicity (as in the last post of Table 1). Detoxifying posts of this kind may constitute a mission impossible for most models (possibly even for humans); the only way to produce a non-toxic ‘rephrase’ may be to change the original post beyond recognition, which may be rewarding systems like CAE-T5 that often hallucinate in their rephrases, as already discussed.\nHence, it makes sense to focus on posts that contain explicit toxic spans, marked by human annotators (for TOXICSPANS) or our best toxic span detector (SPAN-BERT-SEQ). Using these toxic spans, we define three additional variants of accuracy: ACC2 is the same as ACC, but ignores (in its denominator) posts that do not contain at least one toxic span; ACC3 also considers (in its denominator) only posts that contained at least one toxic span, but computes the fraction of these posts that had all of their toxic spans rephrased (even partly) by the transfer model;\nACC4 is a stricter version of ACC3 that requires the posts to also be judged non-toxic by the (BERTbased) toxicity classifier.\nTable 6 shows that restricting ACC to consider only posts with at least one toxic post (ACC2) substantially improves the performance of both models on the NP dataset, indicating that it contains many ‘mission impossible’ instances (posts with no toxic spans) that the original ACC considers. By contrast, switching from ACC to ACC2 leads to mostly negligible changes on the P and TOXICSPANS datasets, which is in accordance with the fact that they contain fewer posts with no toxic spans (11.5% and 48.7%, respectively, compared to 67.4% for NP). Another interesting observation is that ACC4 is always substantially lower than ACC3 (for both systems, on all three datasets), indicating that the models often successfully detect toxic spans and try to rephrase them, but the rephrases are still toxic, at least according to the toxicity classifier."
    }, {
      "heading" : "7.3 Human-detoxified posts",
      "text" : "In this experiment, we wished to study the extent to which humans rephrase known toxic spans, when asked to produce civil rephrases of toxic posts. We used the P dataset, the only one of the three considered that contains human rephrases.8 Since P does not contain gold toxic spans, we again employed SPAN-BERT-SEQ to add toxic spans to the source posts and retained only the 1,354 (out of 2,778 in total) source-target pairs of posts with at least one toxic span in their source post.9 In all but 6 of the 1,354 posts, the humans have rephrased (in the gold target post they provided) all the toxic spans of the source post. The 6 posts were mainly cases where the human changed the context to mitigate toxicity, while retaining the original toxic span. For example, “he’s not that stupid” became “he’s not stupid” (original toxic span shown in bold); in this case removing the ‘that’ from the context arguably makes the post less offensive. Overall, we conclude that humans did rephrase almost all cases of explicit toxicity in the toxic posts they were given.\nWe also applied SPAN-BERT-SEQ to the gold target (rephrased) posts that the humans provided to check if any explicit toxicity remained or was introduced by the rephrases. This flagged 93 gold target posts as comprising at least one toxic span. A manual inspection of the 93 posts revealed that they\n8We used all the P data, since no training was involved. 9The most frequent spans were ‘sh*t’, ‘st*p*d’, ‘f*ck’.\nfall in two main categories. The first category comprises cases where a toxic span of the source post was rephrased, but the rephrase might not be considered totally civil; e.g., “how freaking narcissistic do you have to be?” became “how narcissistic do you have to be?”, where SPAN-BERT-SEQ marked the ‘narcissistic’ of the rephrase as a toxic span. The second category comprises cases where SPANBERT-SEQ produced false positives; e.g., the source post “most of the information is total garbage” became “most of the information is totally useless”, but SPAN-BERT-SEQ marked (arguably incorrectly) ‘useless’ as a toxic span."
    }, {
      "heading" : "8 Discussion",
      "text" : "The posts we annotated for toxic spans were extracted from an already heavily studied public domain benchmark dataset (Civil Comments) that has been examined by thousands of teams in a Kaggle competition,10 and that has been cited in over 50 academic publications. The Civil Comments dataset was filtered to remove any potential personally identifiable information before it was released. Our annotation cost was $21,089 for 59,486 judgements, paying $0.30 per item. All raters were warned for the explicit content of the job and only high accuracy raters were selected (70+%), based on performance on quiz questions. The most common countries of origin of our crowd-annotators were Venezuela and USA (Fig. 6 in Appendix A.1). In the contributor satisfaction survey, 51 participants gave an overall task rating of 3.6/5.0, with pay and test question fairness rated slightly higher than ease of job and clarity of instructions.\nWe note that it is more difficult and costly (approximately 3 times more) to manually annotate toxic spans, instead of just labeling entire posts as toxic or not. This is why we also explored adding rationale extraction components on top of toxicity classifiers trained on existing much larger datasets. We showed that BILSTM+ARE has the potential to reach the performance of BILSTM-SEQ, which is important for future work aiming to build toxic span detectors without any toxic span annotations in the training data. This may be particularly useful in low-resourced languages with limited resources for text toxicity (Zampieri et al., 2020).\nHaving two separate systems, one for toxicity detection and one for toxic spans identification, is more easily compatible with existing deployed toxi-\n10shorturl.at/hqEJ3\ncity detectors. One can simply add a component for toxic spans at the end of a pipeline for toxicity detection, and the new component would be invoked only when toxicity would be detected, leaving the rest of the existing pipeline unchanged. Since the vast majority of posts in real-world applications is non-toxic (Borkan et al., 2019), this pipeline approach would only increase the computational load for the relatively few posts classified as toxic. Using only toxic posts in this study was also a way to simplify this first approach to toxic spans detection, assuming an oracle system achieved the first step (deciding which posts are toxic). However, we note that future work could study adding non-toxic posts to our dataset and requiring systems to first detect toxic posts, then extract toxic spans for toxic posts.\nA direct comparison (in terms of size) of TOXICSPANS with other existing toxicity datasets is only possible if one focuses on the toxic class, typically the minority one, since our dataset contains only toxic posts. By adding non-toxic posts, much larger versions of our dataset can be compiled, of sizes similar to those of existing previous datasets (that provide post-level annotations only). Hence, our TOXICSPANS dataset will be accessible with the following versions. First, only toxic posts included (11,006 posts), which is the version we discuss in this work. Second, the previous version will be augmented with the same number of randomly selected non-toxic Civil Comments posts. Third, a version similar to the previous one, but where the ratio of toxic to non-toxic posts will be 1:40 to be closer to that of real-world datasets (325,499 posts).\nAs shown in Section 7, the TOXICSPANS dataset and toxic span detectors can also help study and evaluate explicit toxicity removal when rephrasing toxic posts to be civil. In this case, toxic spans can be used to get a better understanding of how toxic-to-civil models operate, by showing the toxic spans and their context, along with their rephrases."
    }, {
      "heading" : "9 Intended use and misuse potential",
      "text" : "The toxic span detection systems we consider are trained (the sequence-labeling ones) and tested (all systems) on posts with binary ground-truth character offset labels (toxic or not), reflecting the majority opinion of the annotators (Section 3). This runs the risk of ignoring the opinions of minorities, who may also be minorities among crowd-annotators. To address this issue, we also release the toxic spans of all the annotators and the pseudonymous\nrater identities, not just the spans that reflect the majority opinion, to allow different label binarisation strategies and further studies.\nToxic span detection systems are intended to assist the decision making of moderators, not to replace moderators. When they operate correctly, systems of this kind are expected to ease decision making (reject/accept a post). Incorrect results could be of two types; toxic spans that were not highlighted and non-toxic spans that were highlighted. Mistakes of both types, especially of the first one, may mislead a moderator working under pressure.\nAs with other content filtering systems (e.g., spam filters, phishing detectors), toxic span detectors may trigger an adversarial reaction of malicious users, who may study which types of toxic expressions evade the detectors (esp. publicly available ones) and may gradually start using more implicit toxic language (e.g., irony, false claims), which may be more difficult to detect. However, this is a danger that concerns any toxicity detection system, including systems that classify user content at the post level (without detecting toxic spans)."
    }, {
      "heading" : "10 Conclusions and future work",
      "text" : "We studied toxicity detection, which aims to identify the spans of a user post that make it toxic. Our work is the first of this kind in general toxicity detection. We constructed and release a dataset for the new task, along with baselines and models. Finetuning the SPAN-BERT sequence labelling model of Joshi et al. (2020), yielded the best results. A post-level BILSTM toxicity classifier that was combined with an attention-based attribution method, not trained on annotations at the span level, performed well for the task. By leveraging the dataset of posts annotated as toxic or non-toxic (without spans), we showed that this method can reach the performance of a BILSTM sequence labelling approach that was trained on the more costly toxic spans annotations. This result is particularly interesting for future work aiming to perform toxic spans detection by using only datasets with wholepost toxicity annotations. In a final experiment, we examined toxic-to-civil transfer, showing how toxic spans can help shed more light on this task too, by helping assess how well systems and humans address explicit toxicity. All our code and data will be publicly available. In future work we plan to study toxic span detection in multiple languages and in context-dependent toxic posts."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Exploratory analysis of TOXICSPANS\nFigure 1 shows the distribution of the percentage of character offsets of each post that are included in toxic spans. Figure 2 illustrates the distribution of dense toxic spans per post. Figure 3 shows the most frequent toxic spans in the dataset (after lowercasing each post) and their frequencies. Figure 4 shows the most frequent multi-word toxic spans (again after lower-casing). Figure 5 illustrates the distribution of the size (in words) of those posts whose ground truth covers the whole post. Figure 6 shows the frequencies of the countries of origin of the TOXICSPANS crowd-annotators.\nA.2 Error analysis of SPAN-BERT-SEQ\nWe performed an error analysis on our best toxic spans detector (SPAN-BERT-SEQ). We analyzed its predictions on the first fold of the Monte Carlo Cross-Validation, which comprises 10% of the dataset or 1001 posts. We identified three main types of errors. The first, which is the most frequent one occurring in 235 out of 1001 posts (23.5%), comprises posts for which SPAN-BERT-SEQ failed to find all toxic spans. This type of error can be\ndivided in two sub-types: the first sub-type comprises posts for which SPAN-BERT-SEQ predicted no spans at all (Table 7), while the second sub-type comprises posts for which SPAN-BERT-SEQ predicted some, but not all of the gold spans (Table 8). The first sub-type occurs more often, with 217 out of the 235 total occurrences of the first error type, while the second sub-type occurs only a few times (18 out of 235). The second type of error, which is the second most frequent one, occurred in 173 out of the 1001 posts (17.3%). It occurs when the ground truth of a post is empty, but SPAN-BERTSEQ predicts at least one toxic span (Table 9). The last type of error occurs rarely (only 10 out of 1001 posts) when the ground truth of a post is not empty, and SPAN-BERT-SEQ predicts more (or larger) toxic spans than it should (Table 10).\nA.3 Experimental Settings\nSequence labelling BILSTM-SEQ was implemented in KEARS, version 2.7.0.11 We used word embeddings of size 200 and hidden states of size 128; mean squared error (MSE) loss; the Adam optimiser; learning rate 0.001; post padding; maxlen and batch size 128; training for max. 100 epochs. We used early stopping with 5 epoch patience, monitoring the validation loss. The classification threshold was set to 0.5. CNN-SEQ was trained for 30 epochs; we used 0.5 recurrent dropout; progressively increasing batch size from 4 to 32 with step 1. All the other hyper-parameters were set to their default values. BERT-SEQ was implemented using the huggingface transformers library.12 We used the bert-base-cased model, binary cross entropy loss; the Adam optimiser; learning rate 2 · 10−5; maxlen 128; batch size 32; training for max. 100 epochs; early stop-\n11https://keras.io/ 12https://huggingface.co/transformers/\nping with 5 epoch patience, monitoring validation loss. The classification threshold was 0.5.\nSPAN-BERT base (cased) was fine-tuned in the same way that Joshi et al. (2020) fine-tunes it on SQUAD 2.0 (Rajpurkar et al., 2018) with the format mapping presented in Table 11. At training time, we ignore posts with more than one dense toxic span, since the SQUAD 2.0 format allows for only one dense answer span in the context. We trained with a learning rate 2 · 10−5, for 4 epochs with training batches of size 32.\nPost-level classifiers with attribution\nBILSTM+ARE was implemented in KERAS, like BILSTM-SEQ. We used maxlen of 128; post padding; early stopping with patience 5 epoch, monitoring the validation loss; Adam optimizer with 0.001 learning rate; MSE loss. The text classification threshold was 0.5. BERT+ARE was implemented with Huggingface Transformers similarly to BERT-SEQ. We used maxlen of 128; post padding; early stopping with patience 5 epoch,\nmonitoring the validation loss; Adam optimizer with 2 · 10−5 learning rate; binary cross-entropy loss. The text classification threshold was 0.5. In both models, the attention threshold (above which a token is considered toxic) was fine-tuned on the development set of each Monte Carlo C-V fold.\nFurther implementation details can be found in our code repository, which will be made publicly available in the camera-ready version of this paper.\nA.4 Improving BILSTM+ARE with more training of the underlying BILSTM\nFigure 7 shows the improvement in the F1 score of BILSTM+ARE when increasing the training set of the underlying BILSTM with 5k, 10k, 20k, 40k, 80k more posts (always balanced toxic and nontoxic) with post-level annotations only (no toxic span annotations). The dashed lines represent the sequence labeling methods, which cannot benefit directly from training data without toxic span annotations. Similarly, Fig. 8 shows the corresponding improvement in the ROC AUC score of the underlying BILSTM in the toxic/non-toxic text classification task.\nA.5 Toxicity scores of posts with and without explicit toxicity\nWe applied the BERT-based text toxicity classifier (Laugier et al., 2021), which we also used in Section 7, to the 2,778 posts of the P dataset, dividing them in two sets: posts that comprised at least\none toxic span detected by SPAN-BERT-SEQ (1,354 posts with explicit toxicity) and the rest (implicit toxicity). The BERT-based toxicity classifier considered more toxic (higher average toxicity score) the 1,354 posts of the first set compared to the second one, i.e., it was more confident that the posts of the first set (explicit toxicity) were toxic, as one might expect. By resampling 1,000 subsets (of 50 posts each) from the two sets, we confirmed that this is a statistically significant difference (P = 0.001). The difference of the average predicted toxicity score between the two sets is 14% (from 0.94 down to 0.80)."
    } ],
    "references" : [ {
      "title" : "Nuanced metrics for measuring unintended bias with real data for text classification",
      "author" : [ "Daniel Borkan", "Lucas Dixon", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "WWW, pages 491–500, San Francisco, USA.",
      "citeRegEx" : "Borkan et al\\.,? 2019",
      "shortCiteRegEx" : "Borkan et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-grained analysis of propaganda in news article",
      "author" : [ "Giovanni Da San Martino", "Seunghak Yu", "Alberto Barrón-Cedeno", "Rostislav Petrov", "Preslav Nakov." ],
      "venue" : "EMNLP-IJCNLP, pages 5640–5650.",
      "citeRegEx" : "Martino et al\\.,? 2019",
      "shortCiteRegEx" : "Martino et al\\.",
      "year" : 2019
    }, {
      "title" : "Crowdsourcing of parallel corpora: the case of style transfer for detoxification",
      "author" : [ "Daryna Dementieva", "Sergey Ustyantsev", "David Dale", "Olga Kozlova", "Nikita Semenov", "Alexander Panchenko", "Varvara Logacheva." ],
      "venue" : "Proceedings of the 2nd Crowd",
      "citeRegEx" : "Dementieva et al\\.,? 2021",
      "shortCiteRegEx" : "Dementieva et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL, pages 4171–4186, Minneapolis, Minnesota.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards non-toxic landscapes: Automatic toxic comment detection using DNN",
      "author" : [ "Ashwin Geet D’Sa", "Irina Illina", "Dominique Fohr" ],
      "venue" : "In Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying,",
      "citeRegEx" : "D.Sa et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "D.Sa et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention weights in transformer NMT fail aligning words between sequences but largely explain model predictions",
      "author" : [ "Javier Ferrando", "Marta R. Costa-jussà." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 434–443,",
      "citeRegEx" : "Ferrando and Costa.jussà.,? 2021",
      "shortCiteRegEx" : "Ferrando and Costa.jussà.",
      "year" : 2021
    }, {
      "title" : "Style transfer in text: Exploration and evaluation",
      "author" : [ "Zhenxin Fu", "Xiaoye Tan", "Nanyun Peng", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Fu et al\\.,? 2018",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2018
    }, {
      "title" : "Fortifying toxic speech detectors against veiled toxicity",
      "author" : [ "Xiaochuang Han", "Yulia Tsvetkov." ],
      "venue" : "EMNLP, pages 7732–7739, Online.",
      "citeRegEx" : "Han and Tsvetkov.,? 2020",
      "shortCiteRegEx" : "Han and Tsvetkov.",
      "year" : 2020
    }, {
      "title" : "spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani." ],
      "venue" : "To appear.",
      "citeRegEx" : "Honnibal and Montani.,? 2017",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Deceiving google’s perspective api built for detecting toxic comments",
      "author" : [ "Hossein Hosseini", "Sreeram Kannan", "Baosen Zhang", "Radha Poovendran." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Hosseini et al\\.,? 2017",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2017
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "TACL, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Preemptive toxic language detection in Wikipedia comments using thread-level context",
      "author" : [ "Mladen Karan", "Jan Šnajder." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 129– 134, Florence, Italy.",
      "citeRegEx" : "Karan and Šnajder.,? 2019",
      "shortCiteRegEx" : "Karan and Šnajder.",
      "year" : 2019
    }, {
      "title" : "Attention is not only a weight: Analyzing transformers with vector norms",
      "author" : [ "Goro Kobayashi", "Tatsuki Kuribayashi", "Sho Yokoi", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kobayashi et al\\.,? 2020",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2020
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Multiple-attribute text rewriting",
      "author" : [ "Guillaume Lample", "Sandeep Subramanian", "Eric Smith", "Ludovic Denoyer", "Marc’Aurelio Ranzato", "Y-Lan Boureau" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Lample et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2019
    }, {
      "title" : "Civil rephrases of toxic texts with self-supervised transformers",
      "author" : [ "Léo Laugier", "John Pavlopoulos", "Jeffrey Sorensen", "Lucas Dixon." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
      "citeRegEx" : "Laugier et al\\.,? 2021",
      "shortCiteRegEx" : "Laugier et al\\.",
      "year" : 2021
    }, {
      "title" : "Understanding neural networks through representation erasure",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1612.08220.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey on computational propaganda detection",
      "author" : [ "Giovanni Da San Martino", "Stefano Cresci", "Alberto Barrón-Cedeño", "Seunghak Yu", "Roberto Di Pietro", "Preslav Nakov." ],
      "venue" : "IJCAI, pages 4826–4832.",
      "citeRegEx" : "Martino et al\\.,? 2020",
      "shortCiteRegEx" : "Martino et al\\.",
      "year" : 2020
    }, {
      "title" : "Hatexplain: A benchmark dataset for explainable hate speech detection",
      "author" : [ "Binny Mathew", "Punyajoy Saha", "Seid Muhie Yimam", "Chris Biemann", "Pawan Goyal", "Animesh Mukherjee." ],
      "venue" : "Arxiv (accepted at AAAI).",
      "citeRegEx" : "Mathew et al\\.,? 2021",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2021
    }, {
      "title" : "Fighting offensive language on social media with unsupervised text style transfer",
      "author" : [ "Cicero Nogueira dos Santos", "Igor Melnyk", "Inkit Padhi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
      "citeRegEx" : "Santos et al\\.,? 2018",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep learning for user comment moderation",
      "author" : [ "John Pavlopoulos", "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 25–35, Vancouver, BC, Canada. Association for Computa-",
      "citeRegEx" : "Pavlopoulos et al\\.,? 2017a",
      "shortCiteRegEx" : "Pavlopoulos et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep learning for user comment moderation",
      "author" : [ "John Pavlopoulos", "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 1st Workshop on Abusive Language Online, pages 25– 35, Vancouver, Canada.",
      "citeRegEx" : "Pavlopoulos et al\\.,? 2017b",
      "shortCiteRegEx" : "Pavlopoulos et al\\.",
      "year" : 2017
    }, {
      "title" : "Deeper attention to abusive user content moderation",
      "author" : [ "John Pavlopoulos", "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "EMNLP, pages 1125– 1135, Copenghagen, Denmark.",
      "citeRegEx" : "Pavlopoulos et al\\.,? 2017c",
      "shortCiteRegEx" : "Pavlopoulos et al\\.",
      "year" : 2017
    }, {
      "title" : "Toxicity detection: Does context really matter",
      "author" : [ "John Pavlopoulos", "Jeffrey Sorensen", "Lucas Dixon", "Nithum Thain", "Ion Androutsopoulos" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Pavlopoulos et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pavlopoulos et al\\.",
      "year" : 2020
    }, {
      "title" : "Convai at semeval-2019 task 6: Offensive language identification and categorization with perspective and bert",
      "author" : [ "John Pavlopoulos", "Nithum Thain", "Lucas Dixon", "Ion Androutsopoulos." ],
      "venue" : "SemEval, Minneapolis, USA.",
      "citeRegEx" : "Pavlopoulos et al\\.,? 2019",
      "shortCiteRegEx" : "Pavlopoulos et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for SQuAD",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Why Should I Trust You?” Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "SIGKDD, pages 1135–1144, San Francisco, USA.",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "ACL, pages 5477–5490, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on hate speech detection using natural language processing",
      "author" : [ "Anna Schmidt", "Michael Wiegand." ],
      "venue" : "Workshop on Natural Language Processing for Social Media, pages 1–10, Valencia, Spain.",
      "citeRegEx" : "Schmidt and Wiegand.,? 2017",
      "shortCiteRegEx" : "Schmidt and Wiegand.",
      "year" : 2017
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Advances in neural information processing systems, pages 6830–6841.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Challenges for toxic comment classification: An in-depth error analysis",
      "author" : [ "Betty Van Aken", "Julian Risch", "Ralf Krestel", "Alexander Löser." ],
      "venue" : "Proceedings of the 2nd Workshop on Abusive Language Online, pages 33–42, Brussels, Belgium.",
      "citeRegEx" : "Aken et al\\.,? 2018",
      "shortCiteRegEx" : "Aken et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is not not explanation",
      "author" : [ "Sarah Wiegreffe", "Yuval Pinter." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Wiegreffe and Pinter.,? 2019",
      "shortCiteRegEx" : "Wiegreffe and Pinter.",
      "year" : 2019
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "WWW, pages 1391–1399, Perth, Australia.",
      "citeRegEx" : "Wulczyn et al\\.,? 2017",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Semeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "SemEval.",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "Semeval-2020 task 12: Multilingual offensive language identification in social media (offenseval",
      "author" : [ "Marcos Zampieri", "Preslav Nakov", "Sara Rosenthal", "Pepa Atanasova", "Georgi Karadzhov", "Hamdy Mubarak", "Leon Derczynski", "Zeses Pitenis", "Çağrı Çöltekin" ],
      "venue" : null,
      "citeRegEx" : "Zampieri et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Although several toxicity detection datasets (Wulczyn et al., 2017; Borkan et al., 2019) and models (Schmidt and Wiegand, 2017; Pavlopoulos et al.",
      "startOffset" : 45,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "Although several toxicity detection datasets (Wulczyn et al., 2017; Borkan et al., 2019) and models (Schmidt and Wiegand, 2017; Pavlopoulos et al.",
      "startOffset" : 45,
      "endOffset" : 88
    }, {
      "referenceID" : 30,
      "context" : ", 2019) and models (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) exist, most of them classify whole posts, without identifying the specific spans that make a text toxic.",
      "startOffset" : 19,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and models (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) exist, most of them classify whole posts, without identifying the specific spans that make a text toxic.",
      "startOffset" : 19,
      "endOffset" : 96
    }, {
      "referenceID" : 35,
      "context" : ", 2019) and models (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) exist, most of them classify whole posts, without identifying the specific spans that make a text toxic.",
      "startOffset" : 19,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "We then study some characteristics of supervised and self-supervised toxic-to-civil transfer models (Laugier et al., 2021) by comparing them on several datasets, including a recently released parallel toxic-to-civil dataset (Dementieva et al.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : ", 2021) by comparing them on several datasets, including a recently released parallel toxic-to-civil dataset (Dementieva et al., 2021) and",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 30,
      "context" : "Toxicity detection systems (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) are typically trained on datasets annotated at the post level (a text is annotated as toxic or not) (Wulczyn et al.",
      "startOffset" : 27,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "Toxicity detection systems (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) are typically trained on datasets annotated at the post level (a text is annotated as toxic or not) (Wulczyn et al.",
      "startOffset" : 27,
      "endOffset" : 104
    }, {
      "referenceID" : 35,
      "context" : "Toxicity detection systems (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017c; Zampieri et al., 2019) are typically trained on datasets annotated at the post level (a text is annotated as toxic or not) (Wulczyn et al.",
      "startOffset" : 27,
      "endOffset" : 104
    }, {
      "referenceID" : 34,
      "context" : ", 2019) are typically trained on datasets annotated at the post level (a text is annotated as toxic or not) (Wulczyn et al., 2017; Borkan et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : ", 2019) are typically trained on datasets annotated at the post level (a text is annotated as toxic or not) (Wulczyn et al., 2017; Borkan et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "Toxic spans detection can be seen as a case of attribution or rationale extraction (Li et al., 2016; Ribeiro et al., 2016), but specifically for toxic posts, a task that has never been",
      "startOffset" : 83,
      "endOffset" : 122
    }, {
      "referenceID" : 28,
      "context" : "Toxic spans detection can be seen as a case of attribution or rationale extraction (Li et al., 2016; Ribeiro et al., 2016), but specifically for toxic posts, a task that has never been",
      "startOffset" : 83,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "Detecting spans, instead of entire posts, was recently also considered in propaganda (Martino et al., 2020) and hate speech detection (Mathew et al.",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : ", 2020) and hate speech detection (Mathew et al., 2021).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Hate speech is a particular type of toxicity (Borkan et al., 2019), which can be tackled by more general toxicity detectors (Van Aken et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "toxic (Nogueira dos Santos et al., 2018; Laugier et al., 2021) is the next step towards healthier online discussions, and can be viewed as a form of style transfer (Shen et al.",
      "startOffset" : 6,
      "endOffset" : 62
    }, {
      "referenceID" : 31,
      "context" : ", 2021) is the next step towards healthier online discussions, and can be viewed as a form of style transfer (Shen et al., 2017; Fu et al., 2018; Lample et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : ", 2021) is the next step towards healthier online discussions, and can be viewed as a form of style transfer (Shen et al., 2017; Fu et al., 2018; Lample et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : ", 2021) is the next step towards healthier online discussions, and can be viewed as a form of style transfer (Shen et al., 2017; Fu et al., 2018; Lample et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "We used posts (comments) from the publicly available Civil Comments dataset (Borkan et al., 2019), which already provides whole-post toxicity annotations.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "This definition of toxicity has been used extensively in previous work (Hosseini et al., 2017; Van Aken et al., 2018; Karan and Šnajder, 2019; Han and Tsvetkov, 2020; Pavlopoulos et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "This definition of toxicity has been used extensively in previous work (Hosseini et al., 2017; Van Aken et al., 2018; Karan and Šnajder, 2019; Han and Tsvetkov, 2020; Pavlopoulos et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "This definition of toxicity has been used extensively in previous work (Hosseini et al., 2017; Van Aken et al., 2018; Karan and Šnajder, 2019; Han and Tsvetkov, 2020; Pavlopoulos et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 23,
      "context" : "This definition of toxicity has been used extensively in previous work (Hosseini et al., 2017; Van Aken et al., 2018; Karan and Šnajder, 2019; Han and Tsvetkov, 2020; Pavlopoulos et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "in previous work on toxicity detection, classifying posts as toxic or not (Sap et al., 2020; Pavlopoulos et al., 2017a), and in that sense our inter-annotator agreement can be seen as an improvement.",
      "startOffset" : 74,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "in previous work on toxicity detection, classifying posts as toxic or not (Sap et al., 2020; Pavlopoulos et al., 2017a), and in that sense our inter-annotator agreement can be seen as an improvement.",
      "startOffset" : 74,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "We also train a bidirectional LSTM (BILSTM-SEQ),3 and fine-tune BERT (Devlin et al., 2019) and SPAN-",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "BERT (Joshi et al., 2020) for toxic spans (BERTSEQ, SPAN-BERT-SEQ).",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 21,
      "context" : "Interestingly, the BILSTM binary toxicity classifier with the attention-based toxic span detection mechanism (Pavlopoulos et al., 2017b) is close in performance with BILSTM-SEQ, despite the fact that the latter is directly trained on toxic span anno-",
      "startOffset" : 109,
      "endOffset" : 136
    }, {
      "referenceID" : 24,
      "context" : "Several large datasets with post-level toxicity annotations are publicly available (Pavlopoulos et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Average lengths are reported by counting SentencePiece (Kudo and Richardson, 2018) tokens.",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "We first compare the performance of two toxic-tocivil transfer models, CAE-T5 and SED-T5, both based on the T5 transformer encoder-decoder architecture (Raffel et al., 2019); they both fine-tune the weights of the same pre-trained model, namely T5-large.",
      "startOffset" : 152,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "CAE-T5 (Laugier et al., 2021) is a selfsupervised Conditional Auto-Encoder, fine-tuned on a large non-parallel (NP) dataset based on preprocessed posts from the Civil Comments (CC) dataset, the dataset (with post level annotations) that TOXICSPANS was also based on.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 25,
      "context" : "is used here as a measure of fluency and it is computed with GPT-2 (Radford et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "leads to more frequent hallucinations of content that was not present in the original post (Laugier et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 36,
      "context" : "This may be particularly useful in low-resourced languages with limited resources for text toxicity (Zampieri et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "Since the vast majority of posts in real-world applications is non-toxic (Borkan et al., 2019), this pipeline ap-",
      "startOffset" : 73,
      "endOffset" : 94
    } ],
    "year" : 0,
    "abstractText" : "We study the task of toxic spans detection, which concerns the detection of the spans that make a text toxic, when detecting such spans is possible. We introduce a dataset for this task, TOXICSPANS, which we release publicly. By experimenting with several methods, we show that sequence labeling models perform best, but methods that add generic rationale extraction mechanisms on top of classifiers trained to predict if a post is toxic or not are also surprisingly promising. Finally, we use TOXICSPANS and systems trained on it, to provide further analysis of state-of-the-art toxic to non-toxic transfer systems, as well as human performance on that latter task. Our work highlights challenges in finer toxicity detection and mitigation.",
    "creator" : null
  }
}
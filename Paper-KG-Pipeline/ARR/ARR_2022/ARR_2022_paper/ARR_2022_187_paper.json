{
  "name" : "ARR_2022_187_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Neural Pairwise Ranking Model for Readability Assessment",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatic Readability Assessment is the task of assigning a reading level for a given text. It is useful in many applications such as selecting age appropriate texts in classrooms (Sheehan et al., 2014), assessment of patient education materials (Sare et al., 2020) and clinical informed consent forms (Perni et al., 2019), measuring the readability of financial disclosures (Loughran and McDonald, 2014), and so on. Contemporary NLP approaches treat it primarily as a classification problem. This approach makes it non-transferable to situations where the reading level scale in the test data doesn’t match the one in the training set. Applying learning to rank methods has been seen as a potential solution to this problem in the past. Ranking texts by readability is also useful in a range of application scenarios, from ranking search results based on readability (Kim et al., 2012; Fourney et al., 2018)\nto controlling the reading level of machine translation output (Agrawal and Carpuat, 2019; Marchisio et al., 2019). However, exploration of ranking methods has not been a prominent direction for ARA research. Further, recent developments in neural ranking approaches haven’t been explored for this task yet, to our knowledge.\nARA typically relies on the presence of large amounts of data labeled by reading level. Such datasets are not readily available for many languages. Further, although linguistic features are common in ARA research, it is challenging to calculate them for several languages, due to lack of available software support. Though there is a lot of recent interest in neural network based crosslingual transfer learning approaches for various NLP tasks, there hasn’t been much research in this direction for ARA yet. In this background, we propose a new neural pairwise ranking model for ARA in this paper, and evaluate its transferability to other datasets and languages.\nIn short, we address two research questions:\n1. Is neural, pairwise ranking a better approach than classification or regression for ARA, to achieve cross-corpus compatibility?\n2. Is zero-shot, cross-lingual transfer possible for ARA models through such a ranking approach?\nThe main contributions of this paper are as follows:\n1. This paper proposes new neural pairwise ranking model and shows its application to automatic readability assessment.\n2. The feasibility of pairwise ranking as a means of achieving cross-corpus compatibility for monolingual (English) ARA is evaluated.\n3. Zero shot, neural cross-lingual transfer is assessed for two languages - Spanish and\nFrench, based on a model trained on English data. To our knowledge, this is the first such experiment on ARA.\n4. We created a new dataset, with parallel, topic controlled texts between English and French.\nThe rest of this paper is organized as follows: Section 2 gives an overview of related research. Section 3 describes the proposed neural pairwise ranking model. Section 4 describes our experimental setup and Section 5 discusses the results of our experiments. Section 6 concludes the paper by summarizing our findings and discussing the limitations."
    }, {
      "heading" : "2 Related Work",
      "text" : "Readability Assessment has been an active area in educational research for almost a century. Early research on this topic focused on the creation of readability \"formulae\", which relied on easy to calculate measures such as word and sentence length, and presence of words from some standard word list (Lively and Pressey, 1923; Flesch, 1948; Stenner, 1996). More than 200 such formulae were proposed in the past few decades. DuBay (2007) provides a detailed summary of such formulae. The advent of NLP and machine learning resulted in more data driven research on ARA over the past two decades. Starting from statistical language models (Si and Callan, 2001), a range of lexical and syntactic features (Heilman et al., 2007; Petersen and Ostendorf, 2009; Ambati et al., 2016) as well as inter-sentential features (Pitler and Nenkova, 2008; Todirascu et al., 2013; Xia et al., 2016) were explored in machine learning based ARA research in the past. Features motivated by related disciplines such as psycholinguistics (Howcroft and Demberg, 2017), and cognitive science (Feng et al., 2009) were also explored.\nIn the past few years, ARA research has been primarily focused on textual embeddings and deep learning based architectures. Word embeddings in combination with other attributes such as domain knowledge or language modeling (Cha et al., 2017; Jiang et al., 2018) were used for ARA. A range of neural architectures, from multi attentive RNN (Azpiazu and Pera, 2019) to deep reinforcement learning (Mohammadi and Khasteh, 2019) were proposed. Recent research explored combining transformers with linguistic features (Deutsch et al., 2020; Meng et al., 2020; Lee et al., 2021;\nImperial, 2021). While Deutsch et al. (2020) did not report any improvements with linguistic features, the others reported that combining linguistic features with deep learning approaches result in better ARA models.\nAlthough a lot of this research evolved on English, the past decade saw ARA research in other languages such as German (Hancke et al., 2012), French (François and Fairon, 2012), Italian (Dell’Orletta et al., 2011), Bangla (Sinha et al., 2012) etc., which employed language specific feature sets. While most ARA research modeled one language at a time, some research created language agnostic feature sets and architectures and experimented with 2 to 7 languages (Shen et al., 2013; Azpiazu and Pera, 2019; Madrazo Azpiazu and Pera, 2020a,b; Martinc et al., 2021; Weiss et al., 2021). Although only one language is considered per model in all this research, there are two important exceptions. Madrazo Azpiazu and Pera (2020b) explored whether combining texts from related languages during training improves ARA performance for low resource languages. Weiss et al. (2021) used a model trained on English texts on German, based on a common, broad set of handcrafted linguistic features. However, to our knowledge, zero-shot cross lingual transfer of text embeddings, without any handcrafted features, was not explored for this task in the past.\nARA is traditionally treated as a classification problem in NLP research, although there are some exceptions. Heilman et al. (2008) compared linear, ordinal, and logistic regression and concluded that ordinal with a combination of lexical and grammatical features worked the best for ARA, although subsequent research was still dominated by classification approaches. There is some past work that considered ARA as a pairwise ranking problem, using SVM/SVMrank and hand crafted linguistic features (Pitler and Nenkova, 2008; Tanaka-Ishii et al., 2010; Ma et al., 2012; Mesgar and Strube, 2015; Ambati et al., 2016; Howcroft and Demberg, 2017). While Tanaka-Ishii et al. (2010) and Ma et al. (2012) showed that ranking performs better than traditional features and classification/regression respectively, Xia et al. (2016) did not find ranking to be consistently better across the board. In this background, we take a fresh look at the application of ranking for ARA, by proposing a new neural pairwise ranking model."
    }, {
      "heading" : "3 Neural Pairwise Ranking Model",
      "text" : "The data for our pairwise ranking model takes the form of (document, reading level) pairs. Let X = [(x1, y1), ..., (xn, yn)] be n such pairs, where xi is the vector representation for document i and yi is the corresponding reading level. We then construct m pairwise permutations from X to form X ′. The members of X ′ are constructed as follows: if a pair of documents and reading levels (xi, yi) and (xj , yj) are chosen, then both permutations ((xi, xj), (yi, yj)) and ((xj , xi), (yj , yi)) are added to X ′.\nThe neural pairwise ranking model (NPRM ) aims to maximize\nP (yi > yj |xi, xj)\nFormally, this is parametrized as\nP (yi > yj |xi, xj) , NPRM(xi, xj) = softmax(ψ(f(xi, xj)))\n= [sij1, sij2]\nwhere f is a neural model, ψ is a flexible function, sij1 represents the predicted score of P (yi > yj |xi, xj) and sij2 represents the predicted score of 1 - P (yi > yj |xi, xj). Training labels are created as\ny′ij = { [1, 0] if yi ≥ yj [0, 1] if yi < yj\nWe then calculate the loss function as\nL = −y′ij1 · log(sijk1)− y′ij2 · log(sijk2)\nand back-propagate the errors with stochastic gradient descent. This loss function is known as the Pairwise Logistic Loss (Han et al., 2020).\nImplementation Our neural pairwise ranking model (NPRM ) consists of a BERT (Devlin et al., 2018) model as f and a fully connected layer as ψ. We evaluate the performance of the pairwise ranking approach as follows: for a list of texts to be ranked of size S and each text xa within the list, 1 ≤ a ≤ S we compute\nScore(xa) = ∑ b6=a NPRM(xa, xb)\nWe then rank each text xa by Score(xa) in descending order.\nThis pairwise ranking framework allows for NPRM to model relative reading difficulties between texts. While other neural methods have been proposed with more sophisticated learning objectives for ranking problems in the past (Wang et al., 2018; Ai et al., 2019), these methods require fixedsize inputs to rank. The NRPM only needs a minimum of two texts to form a ranking for each, and the aggregation process of scores between pairwise permutations of texts can easily produce rankings for an arbitrary list size larger than two. The aggregation process also produces a bounded (by the list size), but continuous score for each document, which results in a ranking with no ties, as long as the input documents are different. Additionally, the choice of f in the NPRM framework can allow for multi-lingual predictability, improved performance, or improved efficiency. Due to the flexible modeling structure that the NPRM maintains, we hypothesize that with the aid of a multilingual language model, zero-shot cross lingual ARA assessment may also be possible with NPRM . We demonstrate this possibility later in the paper."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "We describe our experimental setup in terms of the datasets used, modeling and evaluation procedures, in this section."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We experimented with three English, one Spanish and one French datasets, which are described below. All the datasets contain texts in multiple reading level versions (in a given language). We call such grouping of a given text in multiple reading levels a slug. We used the first two English datasets for training and testing our models, and the remaining three datasets only as test sets.\nNewsEla-English (NewsEla-En): NewsEla.com1 provides leveled reading content, which is aligned with the common core educational standards (Porter et al., 2011), and contains texts covering grade 2 to grade 12. It follows the Lexile (Stenner, 1996) framework to create such leveled texts. It was first used in NLP research by Xu et al. (2015) and has been a commonly used corpus for ARA and text simplification in the recent past. The English subset of the NewsEla dataset contains 9565 texts\n1NewsEla corpus can be requested from: https:// NewsEla.com/data/\ndistributed across 1911 slugs. Slugs may or may not contain texts for the full range of reading levels available i.e., each text does not have have all reading level versions.\nOneStopEnglish (OSE): This consists of articles sourced from The Guardian newspaper, rewritten by teachers into three reading levels (beginner, intermediate, advanced) (Vajjala and Lučić, 2018) and has been used as a benchmarking dataset for ARA in the recent past. This dataset contains 189 slugs and 3 reading levels, summing to a total of 567 texts (each slug has one text in three versions).\nNewsEla-Spanish (NewsEla-Es): This is the Spanish subset of the existing NewsEla dataset and contains 1221 texts distributed across 243 slugs and 10 reading levels. Similar to NewsEla-En, each slug does not have all 10 levels in it.\nVikidia-En and Vikidia-Fr: Vikidia.org2 is a children’s encyclopedia, with content targeting 8- 13 year old children, in several Europeapn languages. Our dataset contains 24660 texts distributed across 6165 slugs and 2 reading levels, for English and French respectively i.e., each text in the corpus has four versions: en, en-simple, fr and fr-simple, and there are 6165 slugs in total. Azpiazu and Pera (2019)’s experiments used data from this source. However, the data itself is not publicly available. The uniqueness of the current dataset is that these are parallel, document level aligned texts in four versions - en, en-simple, fr, fr-simple. While we did not create paragraph/sentence level alignments on the corpus, we hope that this will be a useful dataset for future English and French research on ARA and Automatic Text Simplification.3"
    }, {
      "heading" : "4.2 Classification, Regression and Ranking models",
      "text" : "Our primary focus in this paper is on the pairwise ranking model. However, we also compared the performance of other classification, regression, and ranking approaches with our pairwise ranking model to establish strong points of comparison.\nFeature representation: While the use of linguistic features, and more recently, contexual embeddings, have been explored in ARA, noncontextual embeddings were not explored much.\n2https://www.vikidia.org/ 3The Vikidia-En/Fr dataset, along with a brief note on the\ncreation process, is provided in the supplementary material.\nHence, in this paper, we employ three embeddings (GloVe (Pennington et al., 2014), Word2vec (Mikolov et al., 2013a), fastText (Bojanowski et al., 2017)) while training non-BERT classification/regression/ranking models. Document-level embeddings are obtained by aggregating and averaging word-level embeddings for each token in the text. For the rest, we use BERT (Devlin et al., 2018) models to get the feature representation for text.\nClassification The following models were used for formulating baselines and comparisons for treating ARA as a classification task. Reading levels are treated as class labels, and results are obtained through a 5-Fold cross validation approach.\n• Non-contextual embeddings fed into an SVM (Boser et al., 1992) classifier.\n• Non-contextual embeddings fed into a Hierarchical Attention Network (HAN) (Yang et al., 2016). This model was used with and without linguistic features in the past, for reading level classification (Deutsch et al. (2020) and Martinc et al. (2021) respectively).\n• 110-M parameter, 12-layer, BERT model with a fully connected layer and a softmax output. The model is then fine-tuned on the classification task with categorical cross-entropy loss.\nRegression The following models were used for formulating baselines and comparisons for treating ARA as a regression task. Reading levels are treated as continuous outputs, and results are obtained through 5-Fold cross validation.\n• Non-contextual word-level embeddings as input into an Ordinary Linear Regression (OLS) model.\n• 110-M parameter BERT model with a fully connected layer. The model is then fine-tuned on the regression task with the mean squared error loss and will be referred to as regBERT in this paper.\n(non-neural) Pairwise Ranking We employ an SVMRank model with a pairwise ranking framework similar to NPRM , but using the noncontextual word embeddings for feature extraction. Input features for the SVM model are obtained by differencing the obtained embeddings in the following manner: for any text representations xi, xj ,\nwith reading levels yi, yj , form training examples as\nx′i = xi − xj x′j = xj − xi\nand training labels as\ny′i = { 1 yi ≥ yj 0 yi < yj\ny′j = { 1 yj ≥ yi 0 yj < yi\nPredicted scores are aggregated in the same manner as in NPRM to form rankings. Results are obtained through 5-Fold cross validation."
    }, {
      "heading" : "4.3 Pairwise Ranking Training",
      "text" : "To control for the variation in the text introduced by different topical content, the training and prediction process for the SVMRank and NPRM aggregates the text by their slug designations before forming pairwise permutations. As a result, the pairwise permutations are constructed from the text within a slug. For controlling the computation time, we fixed the number of pairwise comparisons per slug (m in NPRM) to 3 levels. i.e., In datasets with more than 3 levels per slug (NewsEla-En and NewsElaEs) we choose texts with the highest and lowest reading levels within a slug, and sample the third text from a reading level in between. Note that this will not affect the ability of the model to rank a list of texts where m is higher than 3. As with all baselines, results from NPRM are obtained through 5-Fold cross validation."
    }, {
      "heading" : "4.4 Evaluation",
      "text" : "Accuracy and F1-score are reported for classification and mean-absolute error and mean-squared error are reported for regression. To evaluate ranking performance, w calculate the Normalized Discounted Cumulative Gain (NDCG), Spearman’s Rank (SRR) Correlation, Kendall’s Tau Correlation Coefficient (KTCC), and the percentage of slugs ranked completely correct, which we denote as Ranking Accuracy (RA). These metrics are largely used for evaluating ranking or information-retrieval tasks in literature.\nWe evaluated classification and regression predictions too through the ranking metrics, in addition to traditional measures. To examine the ranking performance, the texts from each dataset are first grouped by their slugs. Then, ground-truth ranking of the texts within the slugs are compared against the rankings formed from the predicted scores of the models. For NDCG, we used the ground-truth reading levels as the relevance score. We took the model predictions as is, and did not employ specific means to address ties (which can happen in classification). The metrics themselves address ties in different ways. NDCG averages ties in predicted scores, KTCC penalizes ties in ground truth and predicted scores, and SRR calculates the average rank of ties. Ranking accuracy does not handle ties."
    }, {
      "heading" : "4.5 Technical Implementation",
      "text" : "Implementations of the non-neural machine learning models were obtained from the sklearn (Pedregosa et al., 2011) library. The HAN model is a Keras implementation4. Transformers library (Wolf et al., 2020) was used for accessing and fine-tuning BERT and mBERT based models (bertbase-uncased and bert-base-multilingual-uncased models were used). TF-Ranking library 5 (Pasumarthi et al., 2019) was used for accessing the Keras-compatible Pairwise Logistic Loss function. NPRM was implemented in Python.\nThe Word2vec embeddings are pre-trained on the English Google News (Mikolov et al., 2013b). The fasttext embeddings contain 1-million word vectors and are trained on subword information from Wikipedia 2017 (Bojanowski et al., 2017). The GloVe embeddings are trained on the Wikipedia 2014 and Gigaword 5 corpus (Pennington et al., 2014). All three word-embedding models are accessed through the gensim6 library."
    }, {
      "heading" : "5 Results",
      "text" : "We performed within corpus evaluation for classification, and within/cross corpus as well as crosslingual evaluation for regression and ranking."
    }, {
      "heading" : "5.1 Classification",
      "text" : "We trained models using Newsela-En and OSE datasets respectively in a five fold CV setup, for\n4https://github.com/tomcataa/HAN-keras 5https://github.com/tensorflow/ranking 6https://radimrehurek.com/gensim/\nclassification. Table 1 shows the performance of our best performing model in terms of traditional classification metrics, comparing with the state of the art, and Table 2 shows the performance of all models in terms of the ranking metrics.\nIn terms of traditional classification metrics, our approach achieves a lower performance than Martinc et al. (2021) for NewsEla-En corpus, but higher performance on the OSE corpus. A more recent paper by Lee et al. (2021) reported further improvement with OSE, with an extensive set of linguistic features. When evaluating the classification models for ranking performance, we observe that there is less variation among different models for NDCG and Ranking Accuracy, compared to SRR and KTCC metrics. The NewsEla-En + BERT classifier achieves the highest average NDCG, SRR, and KTCC, while the NewsEla + fasttext + SVM combination achieves the highest ranking accuracy. We can conclude that classification approaches we explored do well on the ranking metrics too. However, the numbers seem to be higher for NewsElaEn trained models, compared to those trained on\nOSE dataset. This could potentially be due to the larger dataset size, as well as the fact that NewsElaEn covers a broader reading level scale."
    }, {
      "heading" : "5.2 Regression",
      "text" : "Table 3 shows the performance of all the regression models using standard metrics, and the ranking metrics.\nAlthough there are no other reported results of applying regression models on these datasets to our knowledge, the low MAE/MSE for both datasets indicate that regression models perform well for this problem. However, we notice some differences between classification and regression. In contrast to the classification models, when holding the training data and the regressor constant, models with GloVe embeddings perform worse than models using Word2vec or fasttext in regression specific metrics. But similar to classification, the neural regressor (regBERT) outperforms the OLS regressor in regression metrics. When evaluating on ranking metrics, the regression models generally exhibit higher average NDCG, SRR and KTCC than the classification models."
    }, {
      "heading" : "5.3 Pairwise Ranking",
      "text" : "Table 4 shows the performance of pairwise ranking approaches on both the training datasets. When training on the NewsEla English dataset, we observe that NPRM outperforms at least one wordembedding + SVMRank combination in the ranking metrics, but only achieves the top score in NDCG when compared with word-embedding SVMRank methods. In comparison against the classification and regression models, trained on Newsela-En, NPRM performs better than the regBERT in all ranking metrics but is outperformed by the BERT classification in SRR, KTCC, and RA. When training on the OneStopEnglish dataset, NPRM achieves the top score against the wordembedding + SVMRank combinations, although the GloVe + SVMRank method produces the same score in SRR, KTCC, and RA as well. The OneStopEnglish trained NPRM outperforms the corresponding BERT classification and regression models in all ranking metrics."
    }, {
      "heading" : "5.4 Cross-corpus Pair-wise Ranking",
      "text" : "In this experiment, we evaluated the performance of an ARA model trained with one English dataset, on other English datasets. Since NewsEla-En is the larger dataset with more diverse reading levels, we\nused that for training, and used OSE and VikidiaEn as test sets. Since regression scores can also be used to directly rank predictions (compared to classification, which may result in same category assignments, with no way to resolve such ties), we compared the performance of NPRM with BERT based regression model. Table 5 shows the model performance in terms of the ranking metrics.\nThe results of this evaluation show that the NPRM model, trained on Newsela-En, does well with ranking both OSE and Vikidia-En texts by their reading level, compared to the regBERT model. All measures achieve performance > 0.87 for both the datasets, for NPRM . The regBERT performs comparably on Vikidia-En, but does poorly on OSE. While the results for NPRM are\nslightly lower than within corpus evaluation setups, it has to be noted that this evaluation is done without any additional fine-tuning on the target datasets. This experiment leads us to a conclusion that NPRM can successfully be used to rank documents on a different reading level scale too."
    }, {
      "heading" : "5.5 Zero shot, cross-lingual pair-wise ranking",
      "text" : "Zero-shot cross-lingual scenario aims to evaluate whether a model trained on one language can be effectively used to rank texts from another language correctly. We evaluated NPRM and regBERT models trained with mBERT base for this task. Both the models were trained on Newsela-En dataset and evaluated on Newsela-Es and Vikidia-Fr datasets. Table 6 shows the results of this experiment.\nFrom the zero-shot cross-lingual results, we observe that the NPRM with mBERT either performs comparably or outperforms a regression mBERT model on all metrics, for both the datasets. Specifically, the NPRM has a performance increase of 12.3% in RA for Newsela-Es over Vikidia-Fr. Thus, we can conclude that our pairwise ranking approach performs well even in cross-lingual scenarios, and zero-shot, crosslingual transfer can be useful to setup strong start-\ning models for new languages. However, lower performance on Vikidia-Fr compared to Newsela-Es would require a deeper study. More future experiments, with additional languages, will also lead us towards a better understanding of this approach."
    }, {
      "heading" : "6 Conclusions and Discussion",
      "text" : "In this paper we proposed a neural pairwise ranking model for ARA, which we call NPRM . We performed within corpora and cross corpora evaluations to measure its performance, along with a cross-lingual evaluation. Our results in the context of the research questions we started with (Section 1) are discussed below:\n1. Is neural, pairwise ranking a better approach than classification or regression for ARA, to achieve cross-corpus compatibility? - While regression, classification, and pairwiseranking models all offer high levels of performance for modeling ARA in terms of ranking metrics, pairwise ranking performs better in cross-corpus evaluation scenarios, where the target dataset is not guaranteed to have the same reading-difficulty scale as the training set.\n2. Is zero-shot, cross-lingual transfer possible for ARA models through such a ranking approach? - Our experiments show that zeroshot cross-lingual ARA is possible if given an appropriate language model as a basis for finetuning. NPRM trained with English texts achieved > 80% ranking accuracy on both NewsEla Spanish and Vikidia French datasets.\nLimitations of NPRM: NPRM models the relative reading difficulty level between texts. While this approach has performed well for our generalizability experiments, there is a general lack of interpretability in the rankings produced by NPRM .\nFor example, the NewsEla English dataset contains reading level designations that align with the common core educational standards (Porter et al., 2011), and are interpreted to match the school grades of U.S students from kindergarten to highschool. Since the aggregation process of NPRM sums the predicted scores between pairwise comparisons of an intended ranking, the aggregated score is bounded above by the input list size, which will unlikely correspond to the scale of the original reading levels. Thus, while NPRM is able to evaluate relative reading difficulties between texts, the intended interpretability of the original reading levels becomes lost. Further, NPRM takes as input, a list of texts to be ranked by reading difficulty. The pairwise ranking framework forces the constraint of having at least two texts to be ranked as input. Hence, while NPRM is suitable only for scenarios where ranking by reading level is useful (e.g., search, machine translation etc).\nRanking Metrics: While NDCG is a popular metric for the evaluation of such ranking models, we find SRR, KTCC to be more in-line with the overall ranking accuracy. We observe that the large majority of the methods score close to 1.0 on NDCG. In comparison, the SRR and KTCC, while generally quite high, appear more susceptible to poor ranking performance. However, we notice that RA is lower than SRR and KTCC for OSE (Table 5) and NewsEla-Es (Table 5), but SRR and KTCC lag behind RA for Vikidia-Fr (Table 6). We hypothesize that this could be because of the number of reading levels in the datasets. SRR and KTCC seem more forgiving when number of reading levels are more. Future work in this direction should also focus on the evaluation of the evaluation metrics themselves for this task.\nVikidia-En/Fr Dataset: We created a new, large, bilingual readability dataset in this paper, consisting of 25K texts7. The dataset retains a four way (en, en-simple, fr, fr-simple) alignment at document level. This is the first such dataset in ARA, and perhaps the first readily available French readability dataset. Although we did not work on paragraph/sentence level alignments in this paper, we hope this dataset would be useful for future research.\n7All code and data are provided as supplementary material at: https://www.dropbox. com/sh/kwzbn8unn2mx84m/AABQj0Pe_ I3qDCeczfU7gdtca?dl=0"
    }, {
      "heading" : "A Supplementary Material",
      "text" : "All code and data are provided as supplementary material at: https://www.dropbox. com/sh/kwzbn8unn2mx84m/AABQj0Pe_ I3qDCeczfU7gdtca?dl=0"
    } ],
    "references" : [ {
      "title" : "Controlling text complexity in neural machine translation",
      "author" : [ "Sweta Agrawal", "Marine Carpuat." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Agrawal and Carpuat.,? 2019",
      "shortCiteRegEx" : "Agrawal and Carpuat.",
      "year" : 2019
    }, {
      "title" : "Learning groupwise multivariate scoring functions using deep neural networks",
      "author" : [ "Qingyao Ai", "Xuanhui Wang", "Sebastian Bruch", "Nadav Golbandi", "Michael Bendersky", "Marc Najork." ],
      "venue" : "Proceedings of the 2019 ACM SIGIR International Con-",
      "citeRegEx" : "Ai et al\\.,? 2019",
      "shortCiteRegEx" : "Ai et al\\.",
      "year" : 2019
    }, {
      "title" : "Assessing relative sentence complexity using an incremental ccg parser",
      "author" : [ "Bharat Ram Ambati", "Siva Reddy", "Mark Steedman." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Ambati et al\\.,? 2016",
      "shortCiteRegEx" : "Ambati et al\\.",
      "year" : 2016
    }, {
      "title" : "Multiattentive recurrent neural network architecture for multilingual readability assessment",
      "author" : [ "Ion Madrazo Azpiazu", "Maria Soledad Pera." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:421–436.",
      "citeRegEx" : "Azpiazu and Pera.,? 2019",
      "shortCiteRegEx" : "Azpiazu and Pera.",
      "year" : 2019
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "A training algorithm for optimal margin classifiers",
      "author" : [ "Bernhard E Boser", "Isabelle M Guyon", "Vladimir N Vapnik." ],
      "venue" : "Proceedings of the fifth annual workshop on Computational learning theory, pages 144–152.",
      "citeRegEx" : "Boser et al\\.,? 1992",
      "shortCiteRegEx" : "Boser et al\\.",
      "year" : 1992
    }, {
      "title" : "Language modeling by clustering with word embeddings for text readability assessment",
      "author" : [ "Miriam Cha", "Youngjune Gwon", "HT Kung." ],
      "venue" : "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 2003–2006.",
      "citeRegEx" : "Cha et al\\.,? 2017",
      "shortCiteRegEx" : "Cha et al\\.",
      "year" : 2017
    }, {
      "title" : "Read–it: Assessing readability of italian texts with a view to text simplification. In Proceedings of the second workshop on speech and language processing for assistive technologies, pages",
      "author" : [ "Felice Dell’Orletta", "Simonetta Montemagni", "Giulia Venturi" ],
      "venue" : null,
      "citeRegEx" : "Dell.Orletta et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dell.Orletta et al\\.",
      "year" : 2011
    }, {
      "title" : "Linguistic features for readability assessment",
      "author" : [ "Tovly Deutsch", "Masoud Jasbi", "Stuart Shieber." ],
      "venue" : "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–17, Seattle, WA, USA → Online. Associa-",
      "citeRegEx" : "Deutsch et al\\.,? 2020",
      "shortCiteRegEx" : "Deutsch et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Unlocking language: The classic readability studies",
      "author" : [ "William H DuBay." ],
      "venue" : "Impact Information.",
      "citeRegEx" : "DuBay.,? 2007",
      "shortCiteRegEx" : "DuBay.",
      "year" : 2007
    }, {
      "title" : "Cognitively motivated features for readability assessment",
      "author" : [ "Lijun Feng", "Noémie Elhadad", "Matt Huenerfauth." ],
      "venue" : "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 229–237.",
      "citeRegEx" : "Feng et al\\.,? 2009",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2009
    }, {
      "title" : "A new readability yardstick",
      "author" : [ "Rudolph Flesch." ],
      "venue" : "Journal of applied psychology, 32(3):221.",
      "citeRegEx" : "Flesch.,? 1948",
      "shortCiteRegEx" : "Flesch.",
      "year" : 1948
    }, {
      "title" : "Assessing the readability of web search results for searchers with dyslexia",
      "author" : [ "Adam Fourney", "Meredith Ringel Morris", "Abdullah Ali", "Laura Vonessen." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval,",
      "citeRegEx" : "Fourney et al\\.,? 2018",
      "shortCiteRegEx" : "Fourney et al\\.",
      "year" : 2018
    }, {
      "title" : "An “ai readability” formula for french as a foreign language",
      "author" : [ "Thomas François", "Cédrick Fairon." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "François and Fairon.,? 2012",
      "shortCiteRegEx" : "François and Fairon.",
      "year" : 2012
    }, {
      "title" : "Learning-to-rank with bert in tf-ranking",
      "author" : [ "Shuguang Han", "Xuanhui Wang", "Mike Bendersky", "Marc Najork." ],
      "venue" : "Technical report, arXiv.",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Readability classification for german using lexical, syntactic, and morphological features",
      "author" : [ "Julia Hancke", "Sowmya Vajjala", "Detmar Meurers." ],
      "venue" : "Proceedings of COLING 2012, pages 1063–1080.",
      "citeRegEx" : "Hancke et al\\.,? 2012",
      "shortCiteRegEx" : "Hancke et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining lexical and grammatical features to improve readability measures for first and second language texts",
      "author" : [ "Michael Heilman", "Kevyn Collins-Thompson", "Jamie Callan", "Maxine Eskenazi." ],
      "venue" : "Human Language Technologies 2007: The Conference",
      "citeRegEx" : "Heilman et al\\.,? 2007",
      "shortCiteRegEx" : "Heilman et al\\.",
      "year" : 2007
    }, {
      "title" : "An analysis of statistical models and features for reading difficulty prediction",
      "author" : [ "Michael Heilman", "Kevyn Collins-Thompson", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the third workshop on innovative use of NLP for building educational applications, pages",
      "citeRegEx" : "Heilman et al\\.,? 2008",
      "shortCiteRegEx" : "Heilman et al\\.",
      "year" : 2008
    }, {
      "title" : "Psycholinguistic models of sentence processing improve sentence readability ranking",
      "author" : [ "David M Howcroft", "Vera Demberg." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Vol-",
      "citeRegEx" : "Howcroft and Demberg.,? 2017",
      "shortCiteRegEx" : "Howcroft and Demberg.",
      "year" : 2017
    }, {
      "title" : "Knowledge-rich bert embeddings for readability assessment",
      "author" : [ "Joseph Marvin Imperial." ],
      "venue" : "arXiv preprint arXiv:2106.07935.",
      "citeRegEx" : "Imperial.,? 2021",
      "shortCiteRegEx" : "Imperial.",
      "year" : 2021
    }, {
      "title" : "Enriching word embeddings with domain knowledge for readability assessment",
      "author" : [ "Zhiwei Jiang", "Qing Gu", "Yafeng Yin", "Daoxu Chen." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 366–378.",
      "citeRegEx" : "Jiang et al\\.,? 2018",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2018
    }, {
      "title" : "Characterizing web content, user interests, and search behavior by reading level and topic",
      "author" : [ "Jin Young Kim", "Kevyn Collins-Thompson", "Paul N Bennett", "Susan T Dumais." ],
      "venue" : "Proceedings of the fifth ACM international conference on Web search and",
      "citeRegEx" : "Kim et al\\.,? 2012",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2012
    }, {
      "title" : "Pushing on text readability assessment: A transformer meets handcrafted linguistic features",
      "author" : [ "Bruce W Lee", "Yoo Sung Jang", "Jason Hyung-Jong Lee." ],
      "venue" : "arXiv preprint arXiv:2109.12258.",
      "citeRegEx" : "Lee et al\\.,? 2021",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "A method for measuring the vocabulary burden of textbooks",
      "author" : [ "Bertha A Lively", "Sidney L Pressey." ],
      "venue" : "Educational administration and supervision, 9(7):389–398.",
      "citeRegEx" : "Lively and Pressey.,? 1923",
      "shortCiteRegEx" : "Lively and Pressey.",
      "year" : 1923
    }, {
      "title" : "Measuring readability in financial disclosures",
      "author" : [ "Tim Loughran", "Bill McDonald." ],
      "venue" : "The Journal of Finance, 69(4):1643–1671.",
      "citeRegEx" : "Loughran and McDonald.,? 2014",
      "shortCiteRegEx" : "Loughran and McDonald.",
      "year" : 2014
    }, {
      "title" : "Ranking-based readability assessment for early primary children’s literature",
      "author" : [ "Yi Ma", "Eric Fosler-Lussier", "Robert Lofthus." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Ma et al\\.,? 2012",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2012
    }, {
      "title" : "An analysis of transfer learning methods for multilingual readability assessment",
      "author" : [ "Ion Madrazo Azpiazu", "Maria Soledad Pera." ],
      "venue" : "Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization, pages 95–100.",
      "citeRegEx" : "Azpiazu and Pera.,? 2020a",
      "shortCiteRegEx" : "Azpiazu and Pera.",
      "year" : 2020
    }, {
      "title" : "Is cross-lingual readability assessment possible? Journal of the Association for Information Science and Technology, 71(6):644–656",
      "author" : [ "Ion Madrazo Azpiazu", "Maria Soledad Pera" ],
      "venue" : null,
      "citeRegEx" : "Azpiazu and Pera.,? \\Q2020\\E",
      "shortCiteRegEx" : "Azpiazu and Pera.",
      "year" : 2020
    }, {
      "title" : "Controlling the reading level of machine translation output",
      "author" : [ "Kelly Marchisio", "Jialiang Guo", "Cheng-I Lai", "Philipp Koehn." ],
      "venue" : "Proceedings of Machine Translation Summit XVII Volume 1: Research Track, pages 193–203.",
      "citeRegEx" : "Marchisio et al\\.,? 2019",
      "shortCiteRegEx" : "Marchisio et al\\.",
      "year" : 2019
    }, {
      "title" : "Supervised and unsupervised neural approaches to text readability",
      "author" : [ "Matej Martinc", "Senja Pollak", "Marko RobnikŠikonja." ],
      "venue" : "Computational Linguistics, 47(1):141–179.",
      "citeRegEx" : "Martinc et al\\.,? 2021",
      "shortCiteRegEx" : "Martinc et al\\.",
      "year" : 2021
    }, {
      "title" : "Readnet: A hierarchical transformer framework for web article readability analysis",
      "author" : [ "Changping Meng", "Muhao Chen", "Jie Mao", "Jennifer Neville." ],
      "venue" : "Advances in Information Retrieval, 12035:33.",
      "citeRegEx" : "Meng et al\\.,? 2020",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2020
    }, {
      "title" : "Graphbased coherence modeling for assessing readability",
      "author" : [ "Mohsen Mesgar", "Michael Strube." ],
      "venue" : "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 309– 318, Denver, Colorado. Association for Computa-",
      "citeRegEx" : "Mesgar and Strube.,? 2015",
      "shortCiteRegEx" : "Mesgar and Strube.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Text as environment: A deep reinforcement learning text readability assessment model",
      "author" : [ "Hamid Mohammadi", "Seyed Hossein Khasteh." ],
      "venue" : "arXiv preprint arXiv:1912.05957.",
      "citeRegEx" : "Mohammadi and Khasteh.,? 2019",
      "shortCiteRegEx" : "Mohammadi and Khasteh.",
      "year" : 2019
    }, {
      "title" : "Tf-ranking: Scalable tensorflow library for learning-to-rank",
      "author" : [ "Rama Kumar Pasumarthi", "Sebastian Bruch", "Xuanhui Wang", "Cheng Li", "Michael Bendersky", "Marc Najork", "Jan Pfeifer", "Nadav Golbandi", "Rohan Anil", "Stephan Wolf." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Pasumarthi et al\\.,? 2019",
      "shortCiteRegEx" : "Pasumarthi et al\\.",
      "year" : 2019
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Assessment of use, specificity, and readability of written clinical informed consent forms for patients with cancer",
      "author" : [ "Subha Perni", "Michael K Rooney", "David P Horowitz", "Daniel W Golden", "Anne R McCall", "Andrew J Einstein", "Reshma Jagsi" ],
      "venue" : null,
      "citeRegEx" : "Perni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Perni et al\\.",
      "year" : 2019
    }, {
      "title" : "A machine learning approach to reading level assessment",
      "author" : [ "Sarah E Petersen", "Mari Ostendorf." ],
      "venue" : "Computer speech & language, 23(1):89–106.",
      "citeRegEx" : "Petersen and Ostendorf.,? 2009",
      "shortCiteRegEx" : "Petersen and Ostendorf.",
      "year" : 2009
    }, {
      "title" : "Revisiting readability: A unified framework for predicting text quality",
      "author" : [ "Emily Pitler", "Ani Nenkova." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186–195, Honolulu, Hawaii. Association for",
      "citeRegEx" : "Pitler and Nenkova.,? 2008",
      "shortCiteRegEx" : "Pitler and Nenkova.",
      "year" : 2008
    }, {
      "title" : "Common core standards: The new us intended curriculum",
      "author" : [ "Andrew Porter", "Jennifer McMaken", "Jun Hwang", "Rui Yang." ],
      "venue" : "Educational researcher, 40(3):103–116. 10",
      "citeRegEx" : "Porter et al\\.,? 2011",
      "shortCiteRegEx" : "Porter et al\\.",
      "year" : 2011
    }, {
      "title" : "Readability assessment of internet-based patient education materials related to treatment options for benign prostatic hyperplasia",
      "author" : [ "Antony Sare", "Aesha Patel", "Pankti Kothari", "Abhishek Kumar", "Nitin Patel", "Pratik A Shukla." ],
      "venue" : "Academic Radiology.",
      "citeRegEx" : "Sare et al\\.,? 2020",
      "shortCiteRegEx" : "Sare et al\\.",
      "year" : 2020
    }, {
      "title" : "The textevaluator tool: Helping teachers and test developers select texts for use in instruction and assessment",
      "author" : [ "Kathleen M Sheehan", "Irene Kostin", "Diane Napolitano", "Michael Flor." ],
      "venue" : "The Elementary School Journal, 115(2):184–209.",
      "citeRegEx" : "Sheehan et al\\.,? 2014",
      "shortCiteRegEx" : "Sheehan et al\\.",
      "year" : 2014
    }, {
      "title" : "A language-independent approach to automatic text difficulty assessment for second-language learners",
      "author" : [ "Wade Shen", "Jennifer Williams", "Tamas Marius", "Elizabeth Salesky." ],
      "venue" : "Proceedings of the Second Workshop on Predicting and Improving Text",
      "citeRegEx" : "Shen et al\\.,? 2013",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2013
    }, {
      "title" : "A statistical model for scientific readability",
      "author" : [ "Luo Si", "Jamie Callan." ],
      "venue" : "Proceedings of the tenth international conference on Information and knowledge management, pages 574–576.",
      "citeRegEx" : "Si and Callan.,? 2001",
      "shortCiteRegEx" : "Si and Callan.",
      "year" : 2001
    }, {
      "title" : "New readability measures for bangla and hindi texts",
      "author" : [ "Manjira Sinha", "Sakshi Sharma", "Tirthankar Dasgupta", "Anupam Basu." ],
      "venue" : "Proceedings of COLING 2012: Posters, pages 1141–1150.",
      "citeRegEx" : "Sinha et al\\.,? 2012",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2012
    }, {
      "title" : "Measuring reading comprehension with the lexile framework",
      "author" : [ "A Jackson Stenner" ],
      "venue" : null,
      "citeRegEx" : "Stenner.,? \\Q1996\\E",
      "shortCiteRegEx" : "Stenner.",
      "year" : 1996
    }, {
      "title" : "Sorting texts by readability",
      "author" : [ "Kumiko Tanaka-Ishii", "Satoshi Tezuka", "Hiroshi Terada." ],
      "venue" : "Computational linguistics, 36(2):203–227.",
      "citeRegEx" : "Tanaka.Ishii et al\\.,? 2010",
      "shortCiteRegEx" : "Tanaka.Ishii et al\\.",
      "year" : 2010
    }, {
      "title" : "Coherence and cohesion for the assessment of text readability",
      "author" : [ "Amalia Todirascu", "Thomas François", "Nuria Gala", "Cédric Fairon", "Anne-Laure Ligozat", "Delphine Bernhard." ],
      "venue" : "Natural Language Processing and Cognitive Science, 11:11–19.",
      "citeRegEx" : "Todirascu et al\\.,? 2013",
      "shortCiteRegEx" : "Todirascu et al\\.",
      "year" : 2013
    }, {
      "title" : "Onestopenglish corpus: A new corpus for automatic readability assessment and text simplification",
      "author" : [ "Sowmya Vajjala", "Ivana Lučić." ],
      "venue" : "Proceedings of the thirteenth workshop on innovative use of NLP for building educational applica-",
      "citeRegEx" : "Vajjala and Lučić.,? 2018",
      "shortCiteRegEx" : "Vajjala and Lučić.",
      "year" : 2018
    }, {
      "title" : "The lambdaloss framework for ranking metric optimization",
      "author" : [ "Xuanhui Wang", "Cheng Li", "Nadav Golbandi", "Mike Bendersky", "Marc Najork." ],
      "venue" : "Proceedings of The 27th ACM International Conference on Information and Knowledge Management (CIKM",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Using broad linguistic complexity modeling for cross-lingual readability assessment",
      "author" : [ "Zarah Weiss", "Xiaobin Chen", "Detmar Meurers." ],
      "venue" : "Proceedings of the 10th Workshop on NLP for Computer Assisted Language Learning, pages 38–54.",
      "citeRegEx" : "Weiss et al\\.,? 2021",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformers: State-of-theart natural language processing",
      "author" : [ "Shleifer" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
      "citeRegEx" : "Shleifer,? \\Q2020\\E",
      "shortCiteRegEx" : "Shleifer",
      "year" : 2020
    }, {
      "title" : "Text readability assessment for second language learners",
      "author" : [ "Menglin Xia", "Ekaterina Kochmar", "Ted Briscoe." ],
      "venue" : "Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 12–22.",
      "citeRegEx" : "Xia et al\\.,? 2016",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2016
    }, {
      "title" : "Problems in current text simplification research: New data can help",
      "author" : [ "Wei Xu", "Chris Callison-Burch", "Courtney Napoles." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:283–297.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 conference of the North American chapter of the association for computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 45,
      "context" : "It is useful in many applications such as selecting age appropriate texts in classrooms (Sheehan et al., 2014), assessment of patient education materials (Sare et al.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 44,
      "context" : ", 2014), assessment of patient education materials (Sare et al., 2020) and clinical informed consent forms (Perni et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 40,
      "context" : ", 2020) and clinical informed consent forms (Perni et al., 2019), measuring the readability of financial disclosures (Loughran and McDonald, 2014), and so on.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 26,
      "context" : ", 2019), measuring the readability of financial disclosures (Loughran and McDonald, 2014), and so on.",
      "startOffset" : 60,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : "Ranking texts by readability is also useful in a range of application scenarios, from ranking search results based on readability (Kim et al., 2012; Fourney et al., 2018) to controlling the reading level of machine translation output (Agrawal and Carpuat, 2019; Marchisio et al.",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "Ranking texts by readability is also useful in a range of application scenarios, from ranking search results based on readability (Kim et al., 2012; Fourney et al., 2018) to controlling the reading level of machine translation output (Agrawal and Carpuat, 2019; Marchisio et al.",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : ", 2018) to controlling the reading level of machine translation output (Agrawal and Carpuat, 2019; Marchisio et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : ", 2018) to controlling the reading level of machine translation output (Agrawal and Carpuat, 2019; Marchisio et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : "Early research on this topic focused on the creation of readability \"formulae\", which relied on easy to calculate measures such as word and sentence length, and presence of words from some standard word list (Lively and Pressey, 1923; Flesch, 1948; Stenner, 1996).",
      "startOffset" : 208,
      "endOffset" : 263
    }, {
      "referenceID" : 12,
      "context" : "Early research on this topic focused on the creation of readability \"formulae\", which relied on easy to calculate measures such as word and sentence length, and presence of words from some standard word list (Lively and Pressey, 1923; Flesch, 1948; Stenner, 1996).",
      "startOffset" : 208,
      "endOffset" : 263
    }, {
      "referenceID" : 49,
      "context" : "Early research on this topic focused on the creation of readability \"formulae\", which relied on easy to calculate measures such as word and sentence length, and presence of words from some standard word list (Lively and Pressey, 1923; Flesch, 1948; Stenner, 1996).",
      "startOffset" : 208,
      "endOffset" : 263
    }, {
      "referenceID" : 47,
      "context" : "Starting from statistical language models (Si and Callan, 2001), a range of lexical and syntactic features (Heilman et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "Starting from statistical language models (Si and Callan, 2001), a range of lexical and syntactic features (Heilman et al., 2007; Petersen and Ostendorf, 2009; Ambati et al., 2016) as well as inter-sentential features (Pitler and Nenkova, 2008; Todirascu et al.",
      "startOffset" : 107,
      "endOffset" : 180
    }, {
      "referenceID" : 41,
      "context" : "Starting from statistical language models (Si and Callan, 2001), a range of lexical and syntactic features (Heilman et al., 2007; Petersen and Ostendorf, 2009; Ambati et al., 2016) as well as inter-sentential features (Pitler and Nenkova, 2008; Todirascu et al.",
      "startOffset" : 107,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "Starting from statistical language models (Si and Callan, 2001), a range of lexical and syntactic features (Heilman et al., 2007; Petersen and Ostendorf, 2009; Ambati et al., 2016) as well as inter-sentential features (Pitler and Nenkova, 2008; Todirascu et al.",
      "startOffset" : 107,
      "endOffset" : 180
    }, {
      "referenceID" : 42,
      "context" : ", 2016) as well as inter-sentential features (Pitler and Nenkova, 2008; Todirascu et al., 2013; Xia et al., 2016) were explored in machine learning based ARA research in the past.",
      "startOffset" : 45,
      "endOffset" : 113
    }, {
      "referenceID" : 51,
      "context" : ", 2016) as well as inter-sentential features (Pitler and Nenkova, 2008; Todirascu et al., 2013; Xia et al., 2016) were explored in machine learning based ARA research in the past.",
      "startOffset" : 45,
      "endOffset" : 113
    }, {
      "referenceID" : 56,
      "context" : ", 2016) as well as inter-sentential features (Pitler and Nenkova, 2008; Todirascu et al., 2013; Xia et al., 2016) were explored in machine learning based ARA research in the past.",
      "startOffset" : 45,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "Features motivated by related disciplines such as psycholinguistics (Howcroft and Demberg, 2017), and cognitive science (Feng et al.",
      "startOffset" : 68,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "Features motivated by related disciplines such as psycholinguistics (Howcroft and Demberg, 2017), and cognitive science (Feng et al., 2009) were also explored.",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Word embeddings in combination with other attributes such as domain knowledge or language modeling (Cha et al., 2017; Jiang et al., 2018) were used for ARA.",
      "startOffset" : 99,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "Word embeddings in combination with other attributes such as domain knowledge or language modeling (Cha et al., 2017; Jiang et al., 2018) were used for ARA.",
      "startOffset" : 99,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "A range of neural architectures, from multi attentive RNN (Azpiazu and Pera, 2019) to deep reinforcement learning (Mohammadi and Khasteh, 2019) were proposed.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 36,
      "context" : "A range of neural architectures, from multi attentive RNN (Azpiazu and Pera, 2019) to deep reinforcement learning (Mohammadi and Khasteh, 2019) were proposed.",
      "startOffset" : 114,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "Recent research explored combining transformers with linguistic features (Deutsch et al., 2020; Meng et al., 2020; Lee et al., 2021; Imperial, 2021).",
      "startOffset" : 73,
      "endOffset" : 148
    }, {
      "referenceID" : 32,
      "context" : "Recent research explored combining transformers with linguistic features (Deutsch et al., 2020; Meng et al., 2020; Lee et al., 2021; Imperial, 2021).",
      "startOffset" : 73,
      "endOffset" : 148
    }, {
      "referenceID" : 23,
      "context" : "Recent research explored combining transformers with linguistic features (Deutsch et al., 2020; Meng et al., 2020; Lee et al., 2021; Imperial, 2021).",
      "startOffset" : 73,
      "endOffset" : 148
    }, {
      "referenceID" : 20,
      "context" : "Recent research explored combining transformers with linguistic features (Deutsch et al., 2020; Meng et al., 2020; Lee et al., 2021; Imperial, 2021).",
      "startOffset" : 73,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : "Although a lot of this research evolved on English, the past decade saw ARA research in other languages such as German (Hancke et al., 2012), French (François and Fairon, 2012), Italian (Dell’Orletta et al.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : ", 2012), French (François and Fairon, 2012), Italian (Dell’Orletta et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : ", 2012), French (François and Fairon, 2012), Italian (Dell’Orletta et al., 2011), Bangla (Sinha et al.",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 46,
      "context" : "While most ARA research modeled one language at a time, some research created language agnostic feature sets and architectures and experimented with 2 to 7 languages (Shen et al., 2013; Azpiazu and Pera, 2019; Madrazo Azpiazu and Pera, 2020a,b; Martinc et al., 2021; Weiss et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 286
    }, {
      "referenceID" : 3,
      "context" : "While most ARA research modeled one language at a time, some research created language agnostic feature sets and architectures and experimented with 2 to 7 languages (Shen et al., 2013; Azpiazu and Pera, 2019; Madrazo Azpiazu and Pera, 2020a,b; Martinc et al., 2021; Weiss et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 286
    }, {
      "referenceID" : 31,
      "context" : "While most ARA research modeled one language at a time, some research created language agnostic feature sets and architectures and experimented with 2 to 7 languages (Shen et al., 2013; Azpiazu and Pera, 2019; Madrazo Azpiazu and Pera, 2020a,b; Martinc et al., 2021; Weiss et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 286
    }, {
      "referenceID" : 54,
      "context" : "While most ARA research modeled one language at a time, some research created language agnostic feature sets and architectures and experimented with 2 to 7 languages (Shen et al., 2013; Azpiazu and Pera, 2019; Madrazo Azpiazu and Pera, 2020a,b; Martinc et al., 2021; Weiss et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 286
    }, {
      "referenceID" : 42,
      "context" : "There is some past work that considered ARA as a pairwise ranking problem, using SVM/SVMrank and hand crafted linguistic features (Pitler and Nenkova, 2008; Tanaka-Ishii et al., 2010; Ma et al., 2012; Mesgar and Strube, 2015; Ambati et al., 2016; Howcroft and Demberg, 2017).",
      "startOffset" : 130,
      "endOffset" : 274
    }, {
      "referenceID" : 50,
      "context" : "There is some past work that considered ARA as a pairwise ranking problem, using SVM/SVMrank and hand crafted linguistic features (Pitler and Nenkova, 2008; Tanaka-Ishii et al., 2010; Ma et al., 2012; Mesgar and Strube, 2015; Ambati et al., 2016; Howcroft and Demberg, 2017).",
      "startOffset" : 130,
      "endOffset" : 274
    }, {
      "referenceID" : 27,
      "context" : "There is some past work that considered ARA as a pairwise ranking problem, using SVM/SVMrank and hand crafted linguistic features (Pitler and Nenkova, 2008; Tanaka-Ishii et al., 2010; Ma et al., 2012; Mesgar and Strube, 2015; Ambati et al., 2016; Howcroft and Demberg, 2017).",
      "startOffset" : 130,
      "endOffset" : 274
    }, {
      "referenceID" : 33,
      "context" : "There is some past work that considered ARA as a pairwise ranking problem, using SVM/SVMrank and hand crafted linguistic features (Pitler and Nenkova, 2008; Tanaka-Ishii et al., 2010; Ma et al., 2012; Mesgar and Strube, 2015; Ambati et al., 2016; Howcroft and Demberg, 2017).",
      "startOffset" : 130,
      "endOffset" : 274
    }, {
      "referenceID" : 2,
      "context" : "There is some past work that considered ARA as a pairwise ranking problem, using SVM/SVMrank and hand crafted linguistic features (Pitler and Nenkova, 2008; Tanaka-Ishii et al., 2010; Ma et al., 2012; Mesgar and Strube, 2015; Ambati et al., 2016; Howcroft and Demberg, 2017).",
      "startOffset" : 130,
      "endOffset" : 274
    }, {
      "referenceID" : 19,
      "context" : "There is some past work that considered ARA as a pairwise ranking problem, using SVM/SVMrank and hand crafted linguistic features (Pitler and Nenkova, 2008; Tanaka-Ishii et al., 2010; Ma et al., 2012; Mesgar and Strube, 2015; Ambati et al., 2016; Howcroft and Demberg, 2017).",
      "startOffset" : 130,
      "endOffset" : 274
    }, {
      "referenceID" : 15,
      "context" : "This loss function is known as the Pairwise Logistic Loss (Han et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "Implementation Our neural pairwise ranking model (NPRM ) consists of a BERT (Devlin et al., 2018) model as f and a fully connected layer as ψ.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 53,
      "context" : "While other neural methods have been proposed with more sophisticated learning objectives for ranking problems in the past (Wang et al., 2018; Ai et al., 2019), these methods require fixedsize inputs to rank.",
      "startOffset" : 123,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "While other neural methods have been proposed with more sophisticated learning objectives for ranking problems in the past (Wang et al., 2018; Ai et al., 2019), these methods require fixedsize inputs to rank.",
      "startOffset" : 123,
      "endOffset" : 159
    }, {
      "referenceID" : 43,
      "context" : "com1 provides leveled reading content, which is aligned with the common core educational standards (Porter et al., 2011), and contains texts covering grade 2 to grade 12.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 49,
      "context" : "It follows the Lexile (Stenner, 1996) framework to create such leveled texts.",
      "startOffset" : 22,
      "endOffset" : 37
    }, {
      "referenceID" : 52,
      "context" : "OneStopEnglish (OSE): This consists of articles sourced from The Guardian newspaper, rewritten by teachers into three reading levels (beginner, intermediate, advanced) (Vajjala and Lučić, 2018) and has been used as a benchmarking dataset for ARA in the recent past.",
      "startOffset" : 168,
      "endOffset" : 193
    }, {
      "referenceID" : 39,
      "context" : "Hence, in this paper, we employ three embeddings (GloVe (Pennington et al., 2014), Word2vec (Mikolov et al.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 34,
      "context" : ", 2014), Word2vec (Mikolov et al., 2013a), fastText (Bojanowski et al.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : ", 2013a), fastText (Bojanowski et al., 2017)) while training non-BERT classification/regression/ranking models.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "For the rest, we use BERT (Devlin et al., 2018) models to get the feature representation for text.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "• Non-contextual embeddings fed into an SVM (Boser et al., 1992) classifier.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 58,
      "context" : "• Non-contextual embeddings fed into a Hierarchical Attention Network (HAN) (Yang et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 38,
      "context" : "Implementations of the non-neural machine learning models were obtained from the sklearn (Pedregosa et al., 2011) library.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 37,
      "context" : "TF-Ranking library 5 (Pasumarthi et al., 2019) was used for accessing the Keras-compatible Pairwise Logistic Loss function.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 35,
      "context" : "The Word2vec embeddings are pre-trained on the English Google News (Mikolov et al., 2013b).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "The fasttext embeddings contain 1-million word vectors and are trained on subword information from Wikipedia 2017 (Bojanowski et al., 2017).",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 39,
      "context" : "The GloVe embeddings are trained on the Wikipedia 2014 and Gigaword 5 corpus (Pennington et al., 2014).",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "93 BART (Lewis et al., 2020)+Linguistic features (Lee et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 43,
      "context" : "For example, the NewsEla English dataset contains reading level designations that align with the common core educational standards (Porter et al., 2011), and are interpreted to match the school grades of U.",
      "startOffset" : 131,
      "endOffset" : 152
    } ],
    "year" : 0,
    "abstractText" : "Automatic Readability Assessment (ARA), the task of assigning a reading level to a text, is traditionally treated as a classification problem in NLP research. In this paper, we propose the first neural, pairwise ranking approach to ARA and compare it with existing classification, regression, and (non-neural) ranking methods. We establish the performance of our approach by conducting experiments with three English, one French and one Spanish datasets. We demonstrate that our approach performs well in monolingual single/cross corpus testing scenarios and achieves a zero-shot cross-lingual ranking accuracy of over 80% for both French and Spanish when trained on English data. Additionally, we also release a new parallel bilingual readability dataset, that could be useful for future research. To our knowledge, this paper proposes the first neural pairwise ranking model for ARA, and shows the first results of cross-lingual, zero-shot evaluation of ARA with neural models.",
    "creator" : null
  }
}
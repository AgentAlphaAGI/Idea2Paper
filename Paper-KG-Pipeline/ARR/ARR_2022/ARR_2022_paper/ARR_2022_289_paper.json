{
  "name" : "ARR_2022_289_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Transfer Prompts for Text Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In natural language processing (NLP), text generation is an important research topic that aims to automatically produce understandable text in human language from input data (Yu et al., 2020). In recent decades, various approaches have been widely applied for a variety of text generation tasks (KoncelKedziorski et al., 2019; Gehring et al., 2017), especially the emerging of pretrained language models (PLMs) (Qiu et al., 2020). By involving largescale parameters pretrained on massive general corpus, PLMs such as GPT-3 (Brown et al., 2020) have achieved substantial progress in text generation. Through the fine-tuning paradigm, PLMs can adapt to various text generation tasks by directly\nadjusting the pretrained parameters with labelled datasets.\nHowever, in real-world scenarios, we are inevitably confronted with tasks (e.g., new domains) having only limited labelled data. It is usually difficult to fine-tune text generation models in a datascarce situation (Chen et al., 2020; Chang et al., 2021). Although the input and output formats are different for various text generation tasks, these tasks typically adopt similar learning or generation mechanism in essence (e.g., Seq2Seq (Sutskever et al., 2014)). Furthermore, the success of PLMs sheds light on the possibility of developing general or transferable text generation models. For example, Radford et al. (2019) framed generation tasks as language modeling by predicting the next token given previous tokens. Based on these studies, we aim to devise a general and lightweight text generation approach that can quickly adapt to various new tasks and datasets, based on PLMs.\nTo fulfill this purpose, the recently proposed prompt-based learning offers a potential technical solution (Liu et al., 2021b). In this paradigm, a text generation task can be solved with the help of a prompt containing task-specific information. For example, T5 (Raffel et al., 2020) framed summarization and question answering into a text-to-text format by utilizing prompts “summarize:” and “answer the question:”. Based on learned or manually designed prompts, PLMs can be leveraged to perform existing or new generation tasks without being tuned (Brown et al., 2020; Li and Liang, 2021), which provides a unified approach to utilizing PLMs for various generation tasks. Furthermore, to quickly adapt PLMs to new NLU tasks, several works directly used a soft prompt learned from source NLU tasks to initialize the prompt for a target NLU task (Vu et al., 2021; Su et al., 2021). Inspired by these studies, we aim to apply promptbased methods to data-scarce text generation tasks in a transferable setting.\nDespite promising, there are still two major challenges for transferring prompts in text generation. Firstly, it has been found that prompts are highly task-specific (Gao et al., 2020), and it is difficult to effectively transfer or reuse existing prompts for new tasks. Second, for a single task, even a welllearned prompt may not be suitable for all the data instances from a large population (Scao and Rush, 2021), and hence it is non-trivial to design effective transferring strategy considering both task- and instance-level characteristics.\nTo address the above issues, we propose PTG: Prompt Transfer for Text Generation, a novel prompt-based transfer learning approach for text generation. PTG is developed under a transfer learning setting, and we learn source prompts from a number of representative source generation tasks and then transfer these prompts as target prompts to perform target generation tasks. The core idea is that these learned source prompts serve as representation bases (i.e., value vectors in self-attention mechanism), and then for each data instance from a new task, we learn a specific target prompt by attending to highly relevant source prompts. To support such an approach, we construct a multi-key memory network storing both source prompts and prompt clusters for key-value prompt finding, and then design an adaptive attention mechanism considering both task- and instance-level information for constructing the target prompt. Instead of using a fixed prompt for a new task, our approach is able to effectively learn the most suitable prompt representation from source prompts for a specific data instance. Such an adaptive mechanism considers the specific instance-level characteristics, making our approach more flexible to transfer to any new text generation tasks.\nTo the best of our knowledge, we are the first to introduce the idea of prompting in transfer learning to address text generation tasks. For evaluation, we test PTG on 14 datasets from three sets of text generation tasks: i) compression to express salient information in concise text such as summarization; ii) transduction to transform text while preserving content precisely such as style transfer; and iii) creation to produce new content from input context such as story generation. In both fully-supervised and few-shot experiments, PTG yields competitive or better results than fine-tuning PLMs.\nBesides performance benefits, more importantly, we will release our source prompts which can serve\nas an open-source prompt library. Researchers can train new prompts added to our library, and reuse these learned prompts to improve unseen generation tasks. Our library can further act as an analysis tool, such as analyzing what factors influence prompts’ transferability across generation tasks and interpreting the task similarity by measuring the corresponding prompt similarity."
    }, {
      "heading" : "2 Related Work",
      "text" : "Prompt-based Language Models. Prompt-based learning is a way of leveraging PLMs by prepending task-specific instructions to the task input when feeding into PLMs. Early approaches mainly utilized hand-crafted prompts to adapt to different generation tasks (Brown et al., 2020; Raffel et al., 2020; Zou et al., 2021). However, manually designed prompts are not flexible and cannot be applied to more kinds of tasks. Thus, recent works have focused on automating the learning of discrete prompts (Shin et al., 2020; Gao et al., 2020). However, learning prompts over discrete space is hard to optimize and likely to be sub-optimal. To address these problems, many works proposed to optimize continuous prompts (Liu et al., 2021c; Li and Liang, 2021), which are more flexible to any kinds of tasks. Among these studies, prefix-tuning (Li and Liang, 2021) prepended a sequence of vectors to the input for text generation tasks. By contrast, we utilize soft prompts to investigate transfer learning for text generation and demonstrate that generation tasks can often help each other via prompt transfer.\nTransferability of Natural Language Processing. We are also closely related to existing works on transferability of NLP tasks (Jeong et al., 2020; Wiese et al., 2017; Liu et al., 2019). Prior studies have shown that cross-task transfer can address the data scarcity issue (Wiese et al., 2017), enhance the ability to complex reasoning and inference (Jeong et al., 2020), or learn effective word representations (Liu et al., 2019). Efforts to transfer prompts for addressing NLU tasks have also been developed (Vu et al., 2021; Su et al., 2021). As an representative work, Vu et al. (2021) used the learned prompt to directly initialize the prompt for a target task while not considered the specific input. Our work focuses on the challenging text generation tasks by utilizing prompts to extract implicit taskrelated knowledge and considering specific model inputs for the most useful knowledge transfer."
    }, {
      "heading" : "3 Preliminary",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "Generally, the objective of text generation is to model the conditional probability Pr(y|x), where x = ⟨x1, . . . , xn⟩ and y = ⟨y1, . . . , ym⟩ represent the input text and output text respectively and consist of sequences of tokens from a vocabulary V .\nPrompting is the approach of adding extra task information for PLMs as condition during the generation of output text (Brown et al., 2020). Typically, prompting is conducted by prepending a series of tokens (discrete prompts) or continuous vectors (continuous prompts) to the input x. In our paper, we adopt continuous prompts. Specifically, given a series of n input tokens, x = {x1, . . . , xn}, we first utilize PLM to embed the tokens, forming a matrix Ex ∈ Rn×e, where e is the dimension of the embedding space. Then, our continuous prompt p is represented as a parameter matrix Ep ∈ Rl×e, where l is the number of prompt vectors. The prompt p is then concatenated to the embedded input forming a single matrix [Ep;Ex] ∈ R(l+n)×e which is encoded by PLMs as an ordinary sequence, such that the model maximizes the likelihood of the ground-truth y, i.e., Pr(y|[p;x])."
    }, {
      "heading" : "3.2 Prompt-based Transfer Learning",
      "text" : "In a general transfer learning framework, we define a set of source generation tasks S = {S1, . . . ,ST }, where the t-th task St = {(xti, yti)} nt i=1 contains tuples of the input text xti ∈ X and its corresponding output text yti ∈ Y . Given a target generation task T , the goal of transfer learning is to help improve the performance of learned model fθ in the target task T using the previously learned task-specific knowledge in the source tasks S .\nIn this paper, we consider a new transfer learning setting based on prompting. Specifically, the parameters of the underlying PLM are frozen, and the text generation tasks have to be fulfilled by prepending prompts (continuous vectors) to input as described in Section 3.1. Formally, we will learn an independent source prompt pt for each source generation task St based on a shared frozen PLM by maximizing the likelihood Pr(yti |[pt;xti]). Our core idea is to transfer these learned source prompts to a new (target) text generation task, such that the target generation task can be performed in zero or few shot settings."
    }, {
      "heading" : "4 Approach",
      "text" : "Our proposed method, Prompt Transfer for Text Generation (PTG), is depicted in Figure 1. Our approach first learns a small number of representative prompts for source generation tasks, and then derive the prompt for the target generation task with a novel adaptive attention mechanism. Next we will describe each part in detail."
    }, {
      "heading" : "4.1 Learning Transferable Source Prompts",
      "text" : "To extract task-related knowledge from source generation tasks, we learn a set of source prompts and store them in a source prompt pool. The motivations of introducing the prompt pool are twofold. First, we expect to identify the similarity between source generation tasks. Second, the pool stores task-specific prompts for every source task, which can be shared by all target tasks.\nConstructing Source Prompt Pool. For each source generation task St, we aim to learn a source prompt pt given its training data {(xti, yti)} nt i=1. Following the learning steps in Section 3.1, the underlying PLM learns an independent source prompt pt for the source task St, stored in a prompt pool P = {p1, . . . , pt, . . . , pT }, where T is the total number of source text generation tasks.\nTo construct the source prompt pool, a key point lies in the selection of source text generation tasks. According to the literature (Deng et al., 2021), text generation tasks can be categorized as performing compression, transduction, or creation based on changes in conveyed information from input to output. Moreover, recent studies have shown that few\nbut diverse source tasks/domains also lead to remarkable transfer learning performance (Friedman et al., 2021; Zhuang et al., 2021). Therefore, we select six text generation tasks (including 14 public datasets) within the three types of generation tasks for learning their corresponding source prompts.\nClustering Source Prompts. As described above, the source tasks are diverse in the prompt pool. It is challenging for PLMs to effectively transfer or reuse existing prompts for new tasks. Thus, to identify the similarity between source tasks (prompts), we construct source prompt pool for more effective cross-task knowledge transfer. In particular, via spectral clustering algorithm (Ding et al., 2001), we group these source prompts into several prompt clusters. Under this algorithm, each prompt pt is regarded as a node in a weighted undirected graph G. The similarity degree (weight) between node (prompt) pi and pj is computed via the positionagnostic Euclidean distances (Su et al., 2021):\nwi,j = 1 1 + 1 l2 ∑l k1=1 ∑l k2=1 ||pi,k1 − pj,k2 || ,\n(1) where pi,k1 , pj,k2 denote the k1-th and k2-th vector of prompt pi and pj , respectively. We then adopt the min-max cut strategy (Ding et al., 2001) to partition the graph G into several subgraphs representing different prompt clusters C = {C1, . . . , Cm}, where m is the total number of clusters. When transferring the source prompts, it will be better to identify the suitable prompt cluster and select the most similar source prompt within it. By contrast, previous works (Vu et al., 2021; Su et al., 2021) considered each source prompt equally and ignore the differences between different tasks.\nMulti-Key Memory Network. With source prompts encoding task-related knowledge, the second motivation is to share them with every target generation task. To facilitate the prompt transfer from source tasks to target tasks, we build a multi-key memory network to store these clustered prompts. Specifically, for a source prompt pt belonging to the prompt cluster Cz , i.e., pt ∈ Cz , it is associated with a learnable cluster key kcz and a learnable prompt key kpt , as follows:\nP̃ = {Cz : ⟨kcz,k p t , pt⟩}mz=1, (2)\nwhere kcz,k p t ∈ Rd, and d is the key embedding size. In our memory network, these learned source prompts serve as representation bases, i.e., value\nvectors, which can be transferred to target generation tasks through key-value prompt finding."
    }, {
      "heading" : "4.2 Transferring Instance Adaptive Prompts",
      "text" : "Previous works (Li and Liang, 2021; Vu et al., 2021) usually consider only the task information but ignore the specific input data when deriving prompts. However, for a single task, even a welllearned prompt may not be suitable for all the data instances (Scao and Rush, 2021), and thus it is non-trivial to design effective transferring strategy considering both task- and instance-level characteristics. In our model, we design an adaptive attention mechanism to incorporate the instance feature for constructing the target prompt.\nAdaptive Attention Mechanism. Specifically, for an instance (x, y) of the target task T , we use both task-level and instance-level queries to adaptively lookup and select the source prompts for transferring the previously learned task-related knowledge. The task-level query aims to select the overall information related to the specific target task, which is defined as a learnable task query vector qtask ∈ Rd. However, the source prompts in pool are diverse but limited, thus the task-level prompt may not well adapt to all the data instances of the target generation task. Therefore, we design the instance-level query to learn the target prompt by attending to the highly relevant source prompt to help improve the model performance in specific instances. The instance-level query is computed as the input encoding qins ∈ Rd through a frozen PLM such as BERT (Devlin et al., 2019):\nqins = Average(BERT(x)), (3)\nwhere we average the top-layer representations of every input tokens encoded by BERT.\nFor a source prompt pt ∈ Cz , we use qtask and qins to lookup its corresponding cluster key and source key respectively, following multi-head attention (Vaswani et al., 2017). Thus, the final matching score between the instance x and prompt pt is calculated as:\nst = softmax(λ · qtask⊤kcz + (1− λ) · qins⊤k p t ), (4) where λ is a hyper-parameter. Finally, according to the weight score, the selected source prompt is computed as: p̃ = ∑T t=1 st · pt.\nCompared to other prompt-based transfer learning methods that used only a fixed prompts for a\nnew task (Vu et al., 2021; Li and Liang, 2021), our adaptive attention mechanism is able to effectively learn the most suitable prompt representation from source prompts for a specific data instance. Such a mechanism makes our model more flexible to transfer to any new text generation tasks.\nPrompt-based Text Generation. Based on the above adaptive attention mechanism, we retrieve the prompt p̃ encoding the most useful and relevant knowledge to help the model perform the specific generation instances. Similar to Section 3.1, we prepend the prompt p̃ to the input embedding of x, which then flows through a generative PLM such as BART (Lewis et al., 2020) for generating text. The generative PLM is optimized via maximum likelihhod estimation (MLE) as:\nLMLE(θ) = E(x,y)∼(X ,Y) log Pr(y|[p̃;x]). (5)\nDuring the learning process of the target task, the retrieved prompt p̃ is adaptive to different instances and is frozen because it encodes the previously learned task-related knowledge."
    }, {
      "heading" : "4.3 Model Discussion",
      "text" : "For prompt-based transfer learning in text generation, the key point lies in how to effectively transfer or reuse existing prompts (encoding task-specific knowledge) for new generation tasks considering both task- and instance-level characteristics.\nTo achieve this goal, we first learn a set of source prompts encoding task-specific knowledge from a number of representative source text generation tasks. These source prompts serve as representation bases, i.e., value vectors in the multi-key memory network. Moreover, we design an adaptive attention mechanism considering both taskand instance-level information for constructing the target prompt. Each data instance from a new generation task can learn a specific prompt by attending to the most highly relevant source prompts.\nCompared with typical transfer learning methods, our model utilizes a lightweight technique, i.e., prompting, to learn task-specific knowledge from source tasks. The quality of pretrained source prompts has a great impact on performing more effective and useful knowledge transfer."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we first set up the experiments, and then report the results and analysis."
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "Datasets. We select 14 public datasets divided into three types of text generation tasks: i) compression to express salient information in concise text including summarization (CNN/Daily Mail (See et al., 2017), XSum (Narayan et al., 2018), MSNews (Liu et al., 2021a), Multi-News (Fabbri et al., 2019), NEWSROOM (Grusky et al., 2018)) and question generation (SQuAD (Rajpurkar et al., 2016)); ii) transduction to transform text while preserving content precisely including style transfer (Wiki Neutrality (Pant et al., 2020)) and text paraphrase (Quora (Wang et al., 2017)); and iii) creation to produce new content from input context including dialog (PersonaChat (Zhang et al., 2018), TopicalChat (Gopalakrishnan et al., 2019), DailyDialog (Li et al., 2017), DSTC7-AVSD (Alamri et al., 2019), MultiWOZ (Budzianowski et al., 2018)) and story generation (WritingPrompts (Fan et al., 2018)). Dataset statistics are in Appendix A.\nBaselines. We compare our proposed PTG to the following baselines:\n• GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020): The three representative PLMs for text generation, where all of the pretrained parameters are fine-tuned on each target task dataset separately. We adopt the LARGE version of these PLMs.\n• PREFIXTUNING (Li and Liang, 2021): The recent state-of-the-art prompt-based PLM for text generation by concatenating a sequence of vectors and the input, which keeps PLM parameters frozen but optimizes a set of continuous prefix vectors.\n• SPOT (Vu et al., 2021): The recent promptbased transfer learning method which first trains a prompt on source tasks and then uses the resulting prompt to initialize the prompt for a target task.\n• MULTI-TASK MODELTUNING: A multi-task baseline that first fine-tunes BART on the same source tasks used for PTG and then fine-tunes on each target task dataset individually.\nWe conduct all methods in the same setting to obtain their results without special tricks such as label smoothing. Compared with other baselines, our model is extremely lightweight, i.e., when solving target generation tasks, we freeze the transferred target prompt and parameters of the backbone PLM but only tune the multi-head attention parameters in adaptive attention mechanism (Eq. 4). The training and model details can be found in Appendix B.\nEvaluation Metrics. For performance comparison, we adopt three automatic evaluation metrics widely used by previous works, i.e., BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and Distinct (Li et al., 2016). Specifically, BLEU-n measures the ratios of the co-occurrences of n-grams between the generated and real text; ROUGE-n measures the text quality by counting the overlapping n-grams between the generated and real text; and Distinct-n measures the degree of diversity by calculating the number of distinct n-grams in generated text."
    }, {
      "heading" : "5.2 Fully-Supervised Setting",
      "text" : "Table 1 and 2 present the fully-supervised results of cross-task and cross-dataset transferability, respectively, for our model and baselines.\nFor the cross-task experiment, we consider two pairs of source and target tasks transfer: 1) the target task is summarization (CNN/Daily Mail), and the source tasks is the mixture of other five tasks;\nand 2) the target task is dialog (PersonChat), and the source tasks is other five tasks. For the crossdataset experiment, we consider datasets within summarization and dialog. In summarization, the target dataset is CNN/Daily Mail or XSum, and the source dataset is the mixture of other four summarization datasets. In dialog, the target dataset is PersonaChat or DailyDialog, and the source dataset is other four dialog datasets.\nFirst, by transferring prompts from source tasks to target tasks, PTG outperforms MODELTUNING and PREFIXTUNING. The results suggest that prompt transfer in PTG provides an effective means of improving the performance of typical fine-tuning and prompt methods since our method utilizes the knowledge learned from source tasks.\nSecond, PTG performs better than the promptbased transfer method, SPOT. While transferring prompts, SPOT considers each source task equally\nand ignored the specific instance information. By constrast, PTG clusters diverse source prompts and uses an adaptive attention mechanism considering both task- and instance-level characteristics.\nFinally, PTG produces competitive performance or even exceed the strong MULTI-TASK MODELTUNING. Different from most NLU tasks sharing some common knowledge to understand the semantics and syntax of surface words, text generation tasks need to generate diverse text based on different input data, thus having large task boundaries. Thus, in cross-task transfer, simply tuning PLMs on a mixture of tasks but not considering the task similarity leads to a performance decrease. While, our prompt-based transfer learning approach can still achieve the best performance, showing that PTG improves stability across tasks and datasets."
    }, {
      "heading" : "5.3 Few-Shot Setting",
      "text" : "To explore the performance of PTG in few-shot setting systematically, we subsample the target task dataset to obtain small datasets of size {50, 100, 200, 500}. For each size, we sample and 5 different datasets and average over 2 training random seeds.\nThus, we average over 10 models for each low-data setting. In low-data setting, we adopt the same cross-task and cross-dataset experiments with the full-data setting in Section 5.2. Table 3 and 4 shows the few-shot results of our model and baselines.\nWe can clearly see that PTG achieves competitive (underline fonts) or better performance (bold fonts) than the strong baseline (i.e., MULTI-TASK MODELTUNING) in most low-data regimes, but the gap narrows as the training dataset size increases. In addition, our model outperforms most of vanilla PLMs in most cases. The reason behind might be that large PLMs can easily suffer from overfitting during few-shot learning due to their massive parameters (Gao et al., 2020). While, in our framework, we adopt a lightweight technique, i.e., prompting, to learn source prompts, which can provide the previously learned knowledge in source tasks to PLMs and serve as a better starting point when solving the target tasks."
    }, {
      "heading" : "5.4 Effectiveness of Core Designs",
      "text" : "We further conduct ablation studies to demonstrate the effectiveness of the core designs of PTG.\nSource Prompt Pool. To confirm the importance of the prompt pool, we design a counterpart of our method with only training a sharing prompt for all source tasks. From Table 5 (row 1), we can see that PTG significantly outperforms its counterpart with a single prompt, suggesting that the prompt pool encodes task-specific knowledge well.\nSource Prompt Cluster. We remove the step of grouping source prompts into different clusters and directly lookup source prompts based on queries (see in Table 5 row 2). The decrease in performance demonstrates that when tasks are diverse, clustering prompts of tasks can indeed identify the similarity between source tasks and thus promoting effective knowledge transfer.\nMulti-Key Memory Network. We remove the learnable key vector associated with prompts and directly transfer mean of the source prompts to the target task. From Table 5 (row 4), we can see this results in a significant drop, demonstrating the importance of introducing learnable keys to dynamically select prompts through query-key matching.\nInstance-level Query. This technique is used in adaptive attention mechanism. When we remove it (Table 5 row 3), we only use task-level query to choose source prompts. The declined performance demonstrates that incorporating the instance feature can indeed help to transfer the most useful knowledge to the specific instances in target tasks."
    }, {
      "heading" : "5.5 Task Similarity Analysis",
      "text" : "Figure 2 shows a clustered heatmap of cosine similarities between the source prompts of the 14 public datasets within our six text generation tasks we study using the position-agnostic Euclidean distances defined by Eq. 1. We can clearly observe that our learned 14 source prompts are roughly grouped into three clusters. Similar tasks and datasets are grouped together into clusters in this heatmap and these clusters capture many intuitive task relation-\nships. Specifically, these three clusters mainly focus on compression, transduction, and creation tasks respectively. For example, story generation (WritingPrompts) and dialog (PersonaChat) are grouped together into the third cluster. This observation provides a visual evidence to our findings that text generation tasks can help each other within our approach by learning task-specific prompts and then transferring them into the target task. The results also suggest that our method can serve as an effective means of predicting task transferability."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presented a prompt-based transfer learning approach for text generation. We learn a set of prompts from a number of representative source generation tasks and then transfer these prompts to perform the target generation tasks. In our model, we design an adaptive attention mechanism considering both task- and instance-level information to construct the target prompt. Experiments on fullysupervised and few-shot settings demonstrate the effectiveness of our prompt-based transfer learning model. As future work, we will consider incorporating more kinds of text generation tasks."
    }, {
      "heading" : "7 Ethical Concerns",
      "text" : "Text generation techniques has been applied to a wide range of meaningful applications for society, such as game narrative generation, news report generation, and answering questions. However, this technique may be potentially utilized for harmful applications. Our work improves the quality of generated text compared with traditional methods. Thus, the high-quality text generated by our work makes it difficult to distinguish synthetic text from human-written text, such as fake news and reviews. Here we mainly focus on two potential ethical issues: the potential for deliberate misuse of our model and the issue of bias.\nFirst, it is somewhat challenging to anticipate the malicious usages of our method since they often involve repurposing our model in a very different environment or for an unexpected purpose than we intended. To alleviate this issue, we can ask for help from traditional security risk assessment frameworks such as identifying threats. Second, biases existing in training data may result in our model generating stereotyped or prejudiced content. This issue is concerning, since the model bias could harm some people in relevant groups in unexpected ways. In order to prevent bias, it may be helpful to build a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for our model."
    }, {
      "heading" : "A Statistics of Datasets",
      "text" : "The detailed information of the dataset for each task is listed in Table 6, including summarization (CNN/Daily Mail, XSum, MSNews, Multi-News and NEWSROOM), question generation (SQuAD), style transfer (Wiki Neutrality), text paraphrase (Quora), dialog (PersonaChat, TopicalChat, DailyDialog, DSTC7-AVSD and MultiWOZ) and story generation (WritingPrompts). These datasets are utilized under MIT license."
    }, {
      "heading" : "B Configuration of Models",
      "text" : "For our model, we adopt frozen BART-LARGE to learn a set of source prompts. The length of prompt is set to 200 and the learning rate is set to 1 × 10−3. For the target generation task, we utilize BART-LARGE as the generation backbone and frozen BERT-LARGE to obtain the instancelevel query qins. The dimension d is set to 1024, which is the same as the embedding size e of the BERT/BART-LARGE. The multi-head attention in adaptive attention mechanism has 16 heads. During fine-tuning, the learning rate of BART is set to 3 × 10−5 and the learning rate of cluster key kc, prompt key kp, task key qtask and multi-head attention is set to 1× 10−3.\nThe learning rate of other baselines is set to 3×10−5, which is the same as our backbone BART. The other settings of baselines and our model are set the same for fair comparison. And we do not utilize special tricks such as label smoothing, warmup learning rate and length penalty. We apply the Adam optimizer and set β1 = 0.9, β2 = 0.98, ϵ = 1× 10−6. We set the accumulated batch size of each model to 96 using accumulated gradients. Furthermore, we use the model with the best performance on validation set for generation. During inference, we apply the beam search method with a beam size of 5 and a no repeat ngram size of 3. We train our models using NVIDIA A100 GPUs and PyTorch 1.9.0 upon Ubuntu 20.04.2 LTS."
    } ],
    "references" : [ {
      "title" : "Audio-visual scene-aware dialog",
      "author" : [ "Huda Alamri", "Vincent Cartillier", "Abhishek Das", "Jue Wang", "Stefan Lee", "Peter Anderson", "Irfan Essa", "Devi Parikh", "Dhruv Batra", "Anoop Cherian", "Tim K. Marks", "Chiori Hori." ],
      "venue" : "Proceedings of the IEEE Conference on",
      "citeRegEx" : "Alamri et al\\.,? 2019",
      "shortCiteRegEx" : "Alamri et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Multiwoz - a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling",
      "author" : [ "Paweł Budzianowski", "Tsung-Hsien Wen", "Bo-Hsiang Tseng", "Iñigo Casanueva", "Ultes Stefan", "Ramadan Osman", "Milica Gašić." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Budzianowski et al\\.,? 2018",
      "shortCiteRegEx" : "Budzianowski et al\\.",
      "year" : 2018
    }, {
      "title" : "On training instance selection for few-shot neural text generation",
      "author" : [ "Ernie Chang", "Xiaoyu Shen", "Hui-Syuan Yeh", "Vera Demberg." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Chang et al\\.,? 2021",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2021
    }, {
      "title" : "Few-shot NLG with pre-trained language model",
      "author" : [ "Zhiyu Chen", "Harini Eavani", "Wenhu Chen", "Yinyin Liu", "William Yang Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Compression, transduction, and creation: A unified framework for evaluating natural language generation",
      "author" : [ "Mingkai Deng", "Bowen Tan", "Zhengzhong Liu", "Eric P. Xing", "Zhiting Hu." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in",
      "citeRegEx" : "Deng et al\\.,? 2021",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A min-max cut algorithm for graph partitioning and data clustering",
      "author" : [ "Chris H.Q. Ding", "Xiaofeng He", "Hongyuan Zha", "Ming Gu", "Horst D. Simon." ],
      "venue" : "Proceedings of the 2001 IEEE International Conference on Data Mining, 29 November - 2 December",
      "citeRegEx" : "Ding et al\\.,? 2001",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2001
    }, {
      "title" : "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander R. Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir R. Radev." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Lin-",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann N. Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers,",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Single-dataset experts for multi-dataset question answering",
      "author" : [ "Dan Friedman", "Ben Dodge", "Danqi Chen." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 9",
      "citeRegEx" : "Friedman et al\\.,? 2021",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2021
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2012.15723.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin." ],
      "venue" : "ICML.",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations",
      "author" : [ "Karthik Gopalakrishnan", "Behnam Hedayatnia", "Qinlang Chen", "Anna Gottardi", "Sanjeev Kwatra", "Anu Venkatesh", "Raefer Gabriel", "Dilek Hakkani-Tür." ],
      "venue" : "Proc. Interspeech",
      "citeRegEx" : "Gopalakrishnan et al\\.,? 2019",
      "shortCiteRegEx" : "Gopalakrishnan et al\\.",
      "year" : 2019
    }, {
      "title" : "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
      "author" : [ "Max Grusky", "Mor Naaman", "Yoav Artzi" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Grusky et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grusky et al\\.",
      "year" : 2018
    }, {
      "title" : "Transferability of natural language inference to biomedical question answering",
      "author" : [ "Minbyul Jeong", "Mujeen Sung", "Gangwoo Kim", "Donghyeon Kim", "Wonjin Yoon", "Jaehyo Yoo", "Jaewoo Kang." ],
      "venue" : "CLEF.",
      "citeRegEx" : "Jeong et al\\.,? 2020",
      "shortCiteRegEx" : "Jeong et al\\.",
      "year" : 2020
    }, {
      "title" : "Text generation from knowledge graphs with graph transformers",
      "author" : [ "Rik Koncel-Kedziorski", "Dhanush Bekal", "Yi Luan", "Mirella Lapata", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Koncel.Kedziorski et al\\.,? 2019",
      "shortCiteRegEx" : "Koncel.Kedziorski et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Prefix-tuning: Optimizing continuous prompts for generation",
      "author" : [ "Xiang Lisa Li", "Percy Liang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
      "citeRegEx" : "Li and Liang.,? 2021",
      "shortCiteRegEx" : "Li and Liang.",
      "year" : 2021
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Tai-",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "GLGE: A new",
      "author" : [ "Dayiheng Liu", "Yu Yan", "Yeyun Gong", "Weizhen Qi", "Hang Zhang", "Jian Jiao", "Weizhu Chen", "Jie Fu", "Linjun Shou", "Ming Gong", "Pengcheng Wang", "Jiusheng Chen", "Daxin Jiang", "Jiancheng Lv", "Ruofei Zhang", "Winnie Wu", "Ming Zhou", "Nan Duan" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "author" : [ "Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig." ],
      "venue" : "CoRR, abs/2107.13586.",
      "citeRegEx" : "Liu et al\\.,? 2021b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Gpt understands, too",
      "author" : [ "Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang." ],
      "venue" : "arXiv preprint arXiv:2103.10385.",
      "citeRegEx" : "Liu et al\\.,? 2021c",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards detection of subjective bias using contextualized word embeddings",
      "author" : [ "Kartikey Pant", "Tanvi Dadu", "Radhika Mamidi." ],
      "venue" : "Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, pages 75–76. ACM / IW3C2.",
      "citeRegEx" : "Pant et al\\.,? 2020",
      "shortCiteRegEx" : "Pant et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "Xipeng Qiu", "Tianxiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang." ],
      "venue" : "Science China Technological Sciences, pages 1–26.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "J. Mach. Learn. Res., 21:140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "How many data points is a prompt worth",
      "author" : [ "Teven Le Scao", "Alexander M. Rush" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Scao and Rush.,? \\Q2021\\E",
      "shortCiteRegEx" : "Scao and Rush.",
      "year" : 2021
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "On transferability of prompt tuning for natural language understanding",
      "author" : [ "YuSheng Su", "Xiaozhi Wang", "Yujia Qin", "Chi-Min Chan", "Yankai Lin", "Zhiyuan Liu", "Peng Li", "Juanzi Li", "Lei Hou", "Maosong Sun", "Jie Zhou." ],
      "venue" : "CoRR, abs/2111.06719.",
      "citeRegEx" : "Su et al\\.,? 2021",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2021
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Spot: Better frozen model adaptation through soft prompt transfer",
      "author" : [ "Tu Vu", "Brian Lester", "Noah Constant", "Rami Al-Rfou", "Daniel Cer." ],
      "venue" : "CoRR, abs/2110.07904.",
      "citeRegEx" : "Vu et al\\.,? 2021",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2021
    }, {
      "title" : "Bilateral multi-perspective matching for natural language sentences",
      "author" : [ "Zhiguo Wang", "Wael Hamza", "Radu Florian." ],
      "venue" : "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural domain adaptation for biomedical question answering",
      "author" : [ "Georg Wiese", "Dirk Weissenborn", "Mariana L. Neves." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Wiese et al\\.,? 2017",
      "shortCiteRegEx" : "Wiese et al\\.",
      "year" : 2017
    }, {
      "title" : "A survey of knowledge-enhanced text generation",
      "author" : [ "Wenhao Yu", "Chenguang Zhu", "Zaitang Li", "Zhiting Hu", "Qingyun Wang", "Heng Ji", "Meng Jiang." ],
      "venue" : "CoRR, abs/2010.04389.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "A comprehensive survey on transfer learning",
      "author" : [ "Fuzhen Zhuang", "Zhiyuan Qi", "Keyu Duan", "Dongbo Xi", "Yongchun Zhu", "Hengshu Zhu", "Hui Xiong", "Qing He." ],
      "venue" : "Proc. IEEE, 109(1):43–76.",
      "citeRegEx" : "Zhuang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2021
    }, {
      "title" : "Controllable generation from pre-trained language models via inverse prompting",
      "author" : [ "Xu Zou", "Da Yin", "Qingyang Zhong", "Hongxia Yang", "Zhilin Yang", "Jie Tang." ],
      "venue" : "arXiv preprint arXiv:2103.10685.",
      "citeRegEx" : "Zou et al\\.,? 2021",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 42,
      "context" : "In natural language processing (NLP), text generation is an important research topic that aims to automatically produce understandable text in human language from input data (Yu et al., 2020).",
      "startOffset" : 174,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "In recent decades, various approaches have been widely applied for a variety of text generation tasks (KoncelKedziorski et al., 2019; Gehring et al., 2017), especially the emerging of pretrained language models (PLMs) (Qiu et al.",
      "startOffset" : 102,
      "endOffset" : 155
    }, {
      "referenceID" : 29,
      "context" : ", 2017), especially the emerging of pretrained language models (PLMs) (Qiu et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "By involving largescale parameters pretrained on massive general corpus, PLMs such as GPT-3 (Brown et al., 2020) have achieved substantial progress in text generation.",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "It is usually difficult to fine-tune text generation models in a datascarce situation (Chen et al., 2020; Chang et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "It is usually difficult to fine-tune text generation models in a datascarce situation (Chen et al., 2020; Chang et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "To fulfill this purpose, the recently proposed prompt-based learning offers a potential technical solution (Liu et al., 2021b).",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 31,
      "context" : "For example, T5 (Raffel et al., 2020) framed summarization and question answering into a text-to-text format by utilizing prompts “summarize:” and “answer the question:”.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "Based on learned or manually designed prompts, PLMs can be leveraged to perform existing or new generation tasks without being tuned (Brown et al., 2020; Li and Liang, 2021), which provides a unified approach to utilizing PLMs for various generation tasks.",
      "startOffset" : 133,
      "endOffset" : 173
    }, {
      "referenceID" : 19,
      "context" : "Based on learned or manually designed prompts, PLMs can be leveraged to perform existing or new generation tasks without being tuned (Brown et al., 2020; Li and Liang, 2021), which provides a unified approach to utilizing PLMs for various generation tasks.",
      "startOffset" : 133,
      "endOffset" : 173
    }, {
      "referenceID" : 39,
      "context" : "Furthermore, to quickly adapt PLMs to new NLU tasks, several works directly used a soft prompt learned from source NLU tasks to initialize the prompt for a target NLU task (Vu et al., 2021; Su et al., 2021).",
      "startOffset" : 172,
      "endOffset" : 206
    }, {
      "referenceID" : 36,
      "context" : "Furthermore, to quickly adapt PLMs to new NLU tasks, several works directly used a soft prompt learned from source NLU tasks to initialize the prompt for a target NLU task (Vu et al., 2021; Su et al., 2021).",
      "startOffset" : 172,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "Firstly, it has been found that prompts are highly task-specific (Gao et al., 2020), and it is difficult to effectively transfer or reuse existing prompts for new tasks.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 33,
      "context" : "instances from a large population (Scao and Rush, 2021), and hence it is non-trivial to design effective transferring strategy considering both task- and instance-level characteristics.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "Early approaches mainly utilized hand-crafted prompts to adapt to different generation tasks (Brown et al., 2020; Raffel et al., 2020; Zou et al., 2021).",
      "startOffset" : 93,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : "Early approaches mainly utilized hand-crafted prompts to adapt to different generation tasks (Brown et al., 2020; Raffel et al., 2020; Zou et al., 2021).",
      "startOffset" : 93,
      "endOffset" : 152
    }, {
      "referenceID" : 45,
      "context" : "Early approaches mainly utilized hand-crafted prompts to adapt to different generation tasks (Brown et al., 2020; Raffel et al., 2020; Zou et al., 2021).",
      "startOffset" : 93,
      "endOffset" : 152
    }, {
      "referenceID" : 35,
      "context" : "Thus, recent works have focused on automating the learning of discrete prompts (Shin et al., 2020; Gao et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "Thus, recent works have focused on automating the learning of discrete prompts (Shin et al., 2020; Gao et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : "these problems, many works proposed to optimize continuous prompts (Liu et al., 2021c; Li and Liang, 2021), which are more flexible to any kinds of tasks.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "these problems, many works proposed to optimize continuous prompts (Liu et al., 2021c; Li and Liang, 2021), which are more flexible to any kinds of tasks.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "Among these studies, prefix-tuning (Li and Liang, 2021) prepended a sequence of vectors to the input",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "We are also closely related to existing works on transferability of NLP tasks (Jeong et al., 2020; Wiese et al., 2017; Liu et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "We are also closely related to existing works on transferability of NLP tasks (Jeong et al., 2020; Wiese et al., 2017; Liu et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 136
    }, {
      "referenceID" : 23,
      "context" : "We are also closely related to existing works on transferability of NLP tasks (Jeong et al., 2020; Wiese et al., 2017; Liu et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "Prior studies have shown that cross-task transfer can address the data scarcity issue (Wiese et al., 2017), enhance the ability to complex reasoning and inference (Jeong et al.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : ", 2017), enhance the ability to complex reasoning and inference (Jeong et al., 2020), or learn effective word representations (Liu et al.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : ", 2020), or learn effective word representations (Liu et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 39,
      "context" : "Efforts to transfer prompts for addressing NLU tasks have also been developed (Vu et al., 2021; Su et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 36,
      "context" : "Efforts to transfer prompts for addressing NLU tasks have also been developed (Vu et al., 2021; Su et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "Prompting is the approach of adding extra task information for PLMs as condition during the generation of output text (Brown et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "According to the literature (Deng et al., 2021), text generation tasks can be categorized as performing compression, transduction, or creation based on changes in conveyed information from input to output.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "but diverse source tasks/domains also lead to remarkable transfer learning performance (Friedman et al., 2021; Zhuang et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 44,
      "context" : "but diverse source tasks/domains also lead to remarkable transfer learning performance (Friedman et al., 2021; Zhuang et al., 2021).",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "In particular, via spectral clustering algorithm (Ding et al., 2001),",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 36,
      "context" : "The similarity degree (weight) between node (prompt) pi and pj is computed via the positionagnostic Euclidean distances (Su et al., 2021):",
      "startOffset" : 120,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "We then adopt the min-max cut strategy (Ding et al., 2001) to partition the graph G into several subgraphs representing different prompt clusters C = {C1, .",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 39,
      "context" : "By contrast, previous works (Vu et al., 2021; Su et al., 2021) considered each source prompt equally and ignore the differences between different tasks.",
      "startOffset" : 28,
      "endOffset" : 62
    }, {
      "referenceID" : 36,
      "context" : "By contrast, previous works (Vu et al., 2021; Su et al., 2021) considered each source prompt equally and ignore the differences between different tasks.",
      "startOffset" : 28,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "Previous works (Li and Liang, 2021; Vu et al., 2021) usually consider only the task information but ignore the specific input data when deriving prompts.",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 39,
      "context" : "Previous works (Li and Liang, 2021; Vu et al., 2021) usually consider only the task information but ignore the specific input data when deriving prompts.",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 33,
      "context" : "However, for a single task, even a welllearned prompt may not be suitable for all the data instances (Scao and Rush, 2021), and thus it is non-trivial to design effective transferring strategy considering both task- and instance-level characteristics.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "The instance-level query is computed as the input encoding qins ∈ Rd through a frozen PLM such as BERT (Devlin et al., 2019):",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 38,
      "context" : "For a source prompt pt ∈ Cz , we use qtask and qins to lookup its corresponding cluster key and source key respectively, following multi-head attention (Vaswani et al., 2017).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 39,
      "context" : "new task (Vu et al., 2021; Li and Liang, 2021), our adaptive attention mechanism is able to effectively learn the most suitable prompt representation from source prompts for a specific data instance.",
      "startOffset" : 9,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "new task (Vu et al., 2021; Li and Liang, 2021), our adaptive attention mechanism is able to effectively learn the most suitable prompt representation from source prompts for a specific data instance.",
      "startOffset" : 9,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "1, we prepend the prompt p̃ to the input embedding of x, which then flows through a generative PLM such as BART (Lewis et al., 2020) for generating text.",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 34,
      "context" : "We select 14 public datasets divided into three types of text generation tasks: i) compression to express salient information in concise text including summarization (CNN/Daily Mail (See et al., 2017), XSum (Narayan et al.",
      "startOffset" : 182,
      "endOffset" : 200
    }, {
      "referenceID" : 8,
      "context" : ", 2021a), Multi-News (Fabbri et al., 2019), NEWSROOM (Grusky et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : ", 2019), NEWSROOM (Grusky et al., 2018)) and question generation (SQuAD (Rajpurkar et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : ", 2018)) and question generation (SQuAD (Rajpurkar et al., 2016)); ii) transduction to transform text while preserving content precisely including style transfer (Wiki Neutrality (Pant et al.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : ", 2016)); ii) transduction to transform text while preserving content precisely including style transfer (Wiki Neutrality (Pant et al., 2020)) and text paraphrase (Quora (Wang et al.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 40,
      "context" : ", 2020)) and text paraphrase (Quora (Wang et al., 2017)); and iii) creation to produce new content from input context including dialog (PersonaChat (Zhang et al.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 43,
      "context" : ", 2017)); and iii) creation to produce new content from input context including dialog (PersonaChat (Zhang et al., 2018), TopicalChat (Gopalakrishnan et al.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : ", 2017), DSTC7-AVSD (Alamri et al., 2019), MultiWOZ (Budzianowski et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : ", 2019), MultiWOZ (Budzianowski et al., 2018)) and story generation (WritingPrompts (Fan et al.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : ", 2018)) and story generation (WritingPrompts (Fan et al., 2018)).",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : ", 2019), BART (Lewis et al., 2020), and T5 (Raffel et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 31,
      "context" : ", 2020), and T5 (Raffel et al., 2020): The three representative PLMs for text generation, where all of the pretrained parameters are fine-tuned on each",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "• PREFIXTUNING (Li and Liang, 2021): The recent state-of-the-art prompt-based PLM for text generation by concatenating a sequence of vectors and the input, which keeps PLM parameters frozen but optimizes a set of continuous prefix vectors.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "• SPOT (Vu et al., 2021): The recent promptbased transfer learning method which first trains a prompt on source tasks and then uses the resulting prompt to initialize the prompt for a target task.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 28,
      "context" : ", BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and Distinct (Li et al.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : ", 2002), ROUGE (Lin, 2004) and Distinct (Li et al.",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : ", 2002), ROUGE (Lin, 2004) and Distinct (Li et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "The reason behind might be that large PLMs can easily suffer from overfitting during few-shot learning due to their massive parameters (Gao et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 153
    } ],
    "year" : 0,
    "abstractText" : "Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. However, it is difficult to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. Prompt-based learning offers a potential solution. There are two major challenges for applying prompt-based methods to data-scarce text generation tasks in a transferable setting. First, it is difficult to effectively transfer prompts for new tasks. Second, it is important to design effective transferring strategy considering both taskand instance-level information. To address these issues, we propose a novel prompt-based transfer learning approach for text generation called PTG. PTG learns a set of source prompts for various source generation tasks and then transfers these prompts to perform target generation tasks through an adaptive attention mechanism considering both taskand instance-level information. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We will release our source prompts as an open-source library, which can be added or reused to improve new generation tasks for future researches.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_292_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A Math Word Problem (MWP) is described as a natural language narrative with a math question. The MWP solver is required to generate a solution equation, which can be calculated to get the numerical answer, by understanding the contextual problem description.\nIn teaching, students are encouraged to recognize that mathematics is really about patterns and not merely about numbers (Council, 1989). Mathematically excellent students explore patterns, not just memorize procedures (Schoenfeld,\n1The code will be available at anonymous URL for blind review.\nProb: Eq:\nProb: Eq:\nProb: Eq:\n\uD835\uDC5B1 + \uD835\uDC5B2 \uD835\uDC5B1 − \uD835\uDC5B2 (\uD835\uDC5B1 + \uD835\uDC5B2) ÷ \uD835\uDC5B3(\uD835\uDC5B1 + \uD835\uDC5B2) × \uD835\uDC5B3 \uD835\uDC5B1 × \uD835\uDC5B2 \uD835\uDC5B1 ÷ \uD835\uDC5B2\n1992). However, while using deep learning to solve MWPs, existing methods (Xie and Sun, 2019; Zhang et al., 2020) get stuck in memorizing procedures. Patel et al. (2021) provide evidence that these methods rely on shallow heuristics to generate equations. We look at this issue and think it is because they focus on text understanding or equation generation for one problem. The same quantitative relationship corresponds to many problems of different themes and scenarios, but previous methods overlook the outlining and distinction of MWP patterns.\nIn this work, we first investigate how a neural network understands MWP patterns only from semantics. We adopt the widely used encoder-decoder model structure. BERT (Devlin et al., 2019) is\nemployed as the semantic encoder, and we probe its problem representations. The visualization by T-SNE (van der Maaten and Hinton, 2008) in Figure 1 shows that, through the semantic encoder, most representations of problems with the same prototype equation are pulled closer, even if their narratives are semantically different. We also analyze the representations in different BERT layers, and the results show the semantics affects the problem-solving in lower layers. Besides, for each prototype equation, those problem representations far away from its center representation tend to produce incorrect solutions.\nInspired by it, we propose a contrastive learning approach that seeks similar prototypes to support model to better understand patterns and perceive the divergence of patterns. When collecting contrastive examples, we follow Xie and Sun (2019) to convert the prototype equation to a tree. Given an equation tree, the positive examples are retrieved if their trees or subtrees have the same structure, and the negative examples are collected from the rest in terms of the operator types and the size of the tree. The solving model is first jointly optimized by an equation generation loss and a contrastive learning loss on the collected examples, and then, is further trained on the original dataset. While the generation loss empowers the model to memorize procedures from the semantics, the contrastive learning loss brings similar patterns closer and disperses the different patterns apart.\nWe conduct experiments on the Chinese dataset Math23k (Wang et al., 2017) and the English dataset MathQA (Amini et al., 2019) in monolingual and multilingual settings. To support constructing multilingual contrastive examples, we follow Tan et al. (2021) to adapt MathQA as the counterpart of Math23k. Experimental results show that our method achieves consistent gains in monolingual and multilingual settings. In particular, our method allows the model to improve the performance in one language using data in another language, which suggests that MWP patterns are language-independent. Furthermore, we verify that, through our contrastive learning, the representations that previously generate wrong solutions get closer to their centers, and several problems are solved well.\nTo summarize, the contributions of this paper include: i) An analysis of the MWP model showing that the semantic encoder understands semantics in\nlower layers and gathers the prototype equations in higher layers. ii) A contrastive learning approach helping the model to better understand MWP patterns and perceive the divergence of patterns. iii) Applications in the multilingual setting suggesting that we can further improve the model performance using data in different languages."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Math Word Problem Solving",
      "text" : "Given a natural language narrative with a mathematical question, the task is to generate a solution equation to answer the question. The methods can be divided into four categories: rule-based methods (Fletcher, 1985; Bakman, 2007), statistical machine learning methods (Kushman et al., 2014; Hosseini et al., 2014), semantic parsing methods (Shi et al., 2015; Koncel-Kedziorski et al., 2015) and deep learning methods (Wang et al., 2017; Huang et al., 2018; Xie and Sun, 2019; Zhang et al., 2020).\nRecently, deep learning methods have achieved significant improvement on MWP solving. Wang et al. (2017) first attempt to use recurrent neural networks to build a seq2seq solving model. Xie and Sun (2019) propose a tree-structured decoder to generate an equation tree. Syntactically correct equations can be generated through traversing the equation tree. Zhang et al. (2020) apply graph convolutional networks to extract relationships of quantities in math problems. Recent works (Kim et al., 2020; Tan et al., 2021) based on pretrained language models, such as BERT, enhance the ability of problem understanding."
    }, {
      "heading" : "2.2 Contrastive Learning",
      "text" : "Contrastive learning is a method of representation learning, which is first designed by Hadsell et al. (2006). By pulling semantically similar embeddings together and pushing semantic different ones apart, contrastive learning can provide more effective representations. In NLP, similar approaches have been explored in many fields. Bose et al. (2018) develop a sampler to find harder negative examples, which forces the model to learn better word and graph embeddings. Yang et al. (2019) use contrastive learning to reduce word omission errors in neural machine translation. Clark et al. (2020) train a discriminative model on contrastive examples to obtain more informative language representation. Gao et al. (2021) advance the performance\nof sentence embeddings by using contrastive learning in supervised and unsupervised settings.\nTo the best of our knowledge, this is the first work to adopt contrastive learning to MWP solving. With the supervision of contrastive learning, we seek similar MWP patterns to pull them closer, and collect confusing patterns to push them apart."
    }, {
      "heading" : "3 Semantic Encoder Gathers Prototypes",
      "text" : "In this section, we explore how a neural network understands patterns from semantics. We adopt the encoder-decoder model structure to solve problems, and perform analyses on the problem representations. The observation is that the semantic encoder understands semantics at lower layers and gathers the prototype equations at higher layers."
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1.1 Datasets",
      "text" : "We perform analyses on two widely used datasets Math23k (Wang et al., 2017) and MathQA (Amini et al., 2019). The Math23k dataset is composed of 23k MWPs in elementary education, and the MathQA has 37k MWPs with multiple choices and equations."
    }, {
      "heading" : "3.1.2 Model Architecture",
      "text" : "Semantic Encoder The pre-trained language model BERT (Devlin et al., 2019) is employed as the semantic encoder. The unsupervised pretraining on large corpora renders the model to learn linguistic knowledge, which provides rich textual representations.\nEquation Decoder A tree decoder (Xie and Sun, 2019) is adopted to generate solution equations. We use the BERT-encoded representation of [CLS] token to initialize the root node when decoding. Recursively, the decoder generates the embedding of each node, and predicts the probabilities of number and operator candidates.\nFor brevity, we denote our model as BERT-TD. The model takes the textual problem description as the input and is optimized by minimizing the negative log-likelihoods of node probabilities for predicting the ground-truth equation tree."
    }, {
      "heading" : "3.2 Shifts of Problem Representation",
      "text" : "To explore how the neural model learns MWP patterns during training, we first extract BERTencoded representations in different epochs and different layers. Then we perform the T-SNE visualization (van der Maaten and Hinton, 2008) shown\n0 0.2\n0.4\n0.6\n0.8\n1\n[0.5,0.6) [0.6,0.7) [0.7,0.8) [0.8,0.9) [0.9,1.0)\nmBERT-TD w/o CL\nmBERT-TD w CL\nIntervals of cosine distance\nR a te\no f\nco rr\ne ct\np re\nd ic\nti o\nn s\n2504\n10870\nmBERT w/o CL\nmBERT w CL\n65\n189\nmBERT w/o CL\nmBERT w CL\nC a li n\nsk i-\nH a ra\nb as\nz in\nd e x Train set Test set\nin Figure 2. As the training goes on, the representations with the same prototype equation are gathering. Besides, with the increase of the depth of encoder layers, the gathering tendency becomes more and more obvious.\nIntuitively, the prototype equation exhibits the essential relationship between the quantities in MWP. These results also verify that the patterns learned by the neural model are directly associated with the prototype equations."
    }, {
      "heading" : "3.3 Semantics and Prototype Equation",
      "text" : "From the visualizations, we can not see how semantics affects problem-solving. To this end, we collect 20 problem pairs with similar semantics but exactly different prototypes, and 20 problem pairs with the same prototype but in different themes or scenarios. The cosine similarities of representations are calculated for these pairs in different BERT layers.\nThe averaged results are shown in Figure 3. The semantically similar problems obtain higher values in lower layers but the similarity gradually decreases as the model deepens. Meanwhile, with the increase of the model depth, although in different semantics, the problems with the same prototype equation achieve higher similarity. This demonstrates that semantics affects problem-solving at lower layers, and the model further extracts prototypes from semantics at higher layers."
    }, {
      "heading" : "3.4 Clustering and Solving Ability",
      "text" : "With the above observation, we attempt to discover the relationship between prototype clustering and model performance. For each prototype equation, we first average the representations of the corre-\n0\n0.2\n0.4\n0.6\n0.8\n1\n[0.5,0.6) [0.6,0.7) [0.7,0.8) [0.8,0.9) [0.9,1.0)\nmBERT-TD w/o CL mBERT-TD w CL\nIntervals of cosine distance\nR at\ne of\nc or\nre ct\np re\ndi ct\nio ns\n2504\n10870\nm BE\nRT w\n/o …\nm BE\nRT\n…\n65\n189\nm BE\nRT w\n/o …\nm BE\nRT\n…\nC al\nin sk\niH\nar ab\nas z\nin de\nx Train set Test set\nsponding problems to obtain its center point, and then calculate the cosine distances between representations and its centers. A higher cosine distance means the representation is closer to its center. We split the cosine distance into several intervals and compute the proportion of correct predictions for each interval. The results are shown in Figure 4, which suggests that the representations apart from centers tend to produce wrong solutions."
    }, {
      "heading" : "4 Contrastive Learning",
      "text" : "In this section, we propose a contrastive learning approach to help the model to perceive the divergence of MWP patterns. One drawback of existing deep learning methods is that they overlook the outlining and distinction of MWP patterns. In contrast, we seek similar prototype equations from various problems to support model to understand patterns, and collect easily confused patterns for model to distinguish."
    }, {
      "heading" : "4.1 Data Collection",
      "text" : "We construct contrastive MWP triples (p, p+, p−) containing a basic problem p and its positive and negative examples {p+, p−}.\nPositive Example One direct way is to collect problems whose prototype equation is completely the same as the given problem p. However, the same quantitative relationship in p also exists in other problems. As shown in Table 1, for the second problem, before answering \"How many games could he buy?\", another hidden question is \"How much money does he have?\" whose solving equation is in the same prototype as the first problem. Thus, we parse the prototype equation to tree struc-\nture by following Xie and Sun (2019) and consider its sub-equations and subtrees. The problem p+ is taken as a positive example if its tree or subtree has the same structure as p, such as \"tree\" and the subtree of \"tree+\" in Figure 5.\nNegative Example Bose et al. (2018) and Kalantidis et al. (2020) stress the importance of hard negative examples in contrastive learning. If we choose p− whose prototype is totally different from p, the original MWP model can easily distinguish them apart. Thus, in this work, the problem p− is chosen as a hard negative example if its tree has the same number of nodes but different operator node types, such as \"tree\" and \"tree−\" in Figure 5. With the training on hard negative examples, our model can distinguish more subtle differences from various prototypes, and further grasp the inner pattern of MWP."
    }, {
      "heading" : "4.2 Training Procedure",
      "text" : "We train the model on our contrastive problem triples. As shown in Figure 5, the problems are first encoded by BERT, and then the tree decoder predicts the nodes of the equation tree.\nDuring contrastive learning, the triple z = (p, p+, p−) are input to the model together to predict equation trees. Owing to the decoding manner of Xie and Sun (2019), each node embedding represents the whole subtree information rooted in it. The root node embeddings of the problem p and its negative problem p− are picked for model to distinguish. For its positive problem p+, we find the root node of the tree or subtree containing the same structure as p, and pull its embedding closer to that of p. For brevity, we denote these node embeddings as (e, e+, e−) and the contrastive learning\nloss becomes: Lcl = ∑ z max(0, η + sim(e, e−)\n−sim(e, e+)), (1)\nwhere sim(·) is the cosine similarity, and the η is a margin hyper-parameter.\nThe basics of a MWP solving model is to generate a solution equation to answer the math question. We transform the target equation y into Polish notation as [y1, y2, ..., ym], where m is the equation length. The tree decoder generates k-node token yk recursively, and the loss of generating equation is computed as:\nP(y|p) = m∏ k=1 P(yk|p) (2)\nLeq = ∑ p − logP(y|p) (3)\nThe final training objective is to minimize the equation loss and contrastive loss as follows:\nL = Leq + α · Lcl (4)\nwhere α is a hyper-parameter that represents the importance of the contrastive learning.\nHowever, not all problems have positive examples, such as those problems whose solution is one value without any operator. With this in mind, we develop the two-stage training strategy. The MWP solver is first trained on our contrastive triples at stage I, and then further trained on the original dataset at stage II."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate our method on two widely used datasets (Wang et al., 2017; Amini et al., 2019), and demonstrate its effectiveness in monolingual and multilingual settings."
    }, {
      "heading" : "5.1 Configuration",
      "text" : "Data and Metrics We collect problems from the Chinese dataset Math23k (Wang et al., 2017) and the English dataset MathQA (Amini et al., 2019). As the formula formats of the two datasets are different, we follow Tan et al. (2021) to adapt MathQA as a counterpart of Math23k. Table 2 shows data statistics. We report the accuracy of equation generation, namely as \"Acc (eq)\", that the problem is solved well if the generated equation is equal to the annotated formula. Considering several equations satisfy the problem solution, we report the accuracy of answer value, namely as \"Acc (ans)\", to see whether the value calculated by the generated equation is equal to the target value.\nImplementation We conduct our contrastive learning in the monolingual and multilingual perspectives. In the monolingual setting, we construct contrastive triples inside each dataset. In the multilingual setting, for each problem, the positive and negative examples are from different sources. Specifically, given a Chinese MWP in Math23k, we collect positive examples from MathQA and negative examples from Math23k. We adopt BERTbase (Devlin et al., 2019) as the problem encoder, and follow Xie and Sun (2019) to build the treedecoder for solution generation. The hidden size of the decoder is set to 768. Multilingual BERT is used in the multilingual setting. The max input length is set to 120 and the max output length is set to 45. The loss margin η is set to 0.2. The weight α of contrastive learning loss is set to 5. We use AdamW (Loshchilov and Hutter, 2017) as our optimizer, and perform grid search over the sets\nof the learning rate as {5e-5, 1e-4} and the number of epochs as {30, 50} for each training stage. The batch size is fixed to 16 to reduce the search space, and we evaluate models for every epoch. We use the dropout of 0.5 to prevent over-fitting and perform a 3-beam search for better generations."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "To verify the effectiveness of the proposed method, we directly train our model on original datasets without contrastive learning. In particular, the multilingual baseline model is trained by mixing Math23k and the adapted MathQA. In addition to comparing with BERT, we also investigate the following approaches:\nGroupAttention2 (Li et al., 2019) develop an attention mechanism to capture the quantity-related and question-related information.\nGTS3 (Xie and Sun, 2019) generate equation trees through a tree structure decoder in a goaldriven mannner.\nGraph2Tree4 (Zhang et al., 2020) design a graph-based encoder for representing the relationships and order information among the quantities."
    }, {
      "heading" : "5.3 Main Results",
      "text" : "Experimental results are shown in Table 3. Training the MWP solver with our proposed contrastive learning outperforms the baseline models on all datasets.\nMonolingual Results Compared to previous methods, the pretrained linguistic knowledge in BERT can help the MWP solver improve performance greatly. With our proposed contrastive learning method, our model achieves consistent gains on Math23k and the adapted MathQA. This suggests that seeking patterns with supervision benefits the model to solve MWPs.\nMultilingual Results We adapt our model to the multilingual setting by using multilingual BERT and mixing two train sets. The contrastive learning improves Math23k answer accuracy to 83.9 (3.4 absolute improvements) and MathQA answer accuracy to 76.3 (2.8 absolute improvements), which are competitive with the monolingual results. This\n2https://github.com/lijierui/ group-attention\n3https://github.com/ShichaoSun/math_ seq2tree\n4https://github.com/2003pro/Graph2Tree\ndemonstrates that the model can learn similar patterns in different languages."
    }, {
      "heading" : "5.4 Analysis",
      "text" : "We conduct ablations to better understand the contributions of different components in our contrastive learning method."
    }, {
      "heading" : "5.4.1 Effects of Data Collection",
      "text" : "The contrastive examples consist of positive examples with similar patterns and negative examples with exactly different patterns. In this work, we investigate different strategies of collecting positive and negative examples. As well as our strategy, we attempt to collect MWPs containing the same prototype equation to be the positive examples, and randomly select negative examples from the rest.\nTable 4 shows that our strategy achieves better performance on all datasets. In addition to the problems with the same prototype equations, our collected examples include more problems having the same equation subtree structures. It can be seen\nthat the model can benefit from these examples. For the negative examples, we take the problems with the same number of operators but different operator types. If performing random selection, the model performance drops, which suggests that our collected examples can support the model to disperse the different patterns. No matter which strategy we use, compared to the baseline without contrastive learning, our method advances MWP solving and gives one way to improve the performance by using data in different languages."
    }, {
      "heading" : "5.4.2 Effects of Hyperparameters",
      "text" : "We train the \"mBERT-TD\" model with several loss margins (0.05, 0.1, 0.15, 0.2 and 0.3) to disperse the different patterns. As shown in Table 5, the margin 0.2 can help the model achieve the best\nmBERT w/o CL mBERT w CL\nFigure 6: T-SNE visualization of the problem representation with and without our contrastive learning.\n0\n0.2\n0.4\n0.6\n0.8\n1\n[0.5,0.6) [0.6,0.7) [0.7,0.8) [0.8,0.9) [0.9,1.0)\nmBERT-TD w/o CL\nmBERT-TD w CL\nIntervals of cosine distance\nR a te\no f\nco rr\ne ct\np re\nd ic\nti o\nn s\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 1 2 3 4 5 6 7 8 9 10 11 12\nSimilar semantics\nSame Paradigm\nLayer index\nC o\nsi n\ne si\nm il a ri\nty\n2504\n10870\nmBERT w/o CL\nmBERT w CL\n65\n189\nmBERT w/o CL\nmBERT w CL\nC a li n\nsk i-\nH a ra\nb as\nz in\nd e x Train set Test set\n0\n0.2\n0.4\n0.6\n0.8\n1 2 3 4 5 6 7 8 9 10\nInterval index\nR a te\no f\nco rr\ne ct\np re\nd ic\nti o\nn s\nFigure 7: Calinski-Harabasz index on the train/test set with and without our contrastive learning.\nperformance but lower margins 0.1 and 0.15 also perform well.\nAs introduced in Section 4.2, we train our model in two stages and the loss weight α represents the importance of the contrastive learning. Table 6 shows the results of using different weights in each stage. It can be seen that the higher weight achieves better performance, and at stage II, training on all examples further improves the performance."
    }, {
      "heading" : "5.4.3 Visualization and Statistics",
      "text" : "We perform the T-SNE visualization shown in Figure 6. The problem representations with the same prototype equation are more gathered through our contrastive learning. To measure this variation, we calculate the Calinski-Harabasz index (Caliński and Harabasz, 1974). Figure 7 shows that our method supports the model to gain higher clustering scores.\nThe above results illustrate that, for each prototype equation, the representations are pulled closer to its centers. We re-compute the proportion of correct predictions as described in Section 3.4. The results are shown in Figure 8. We observe the accuracy increases in most intervals, which also verifies the effectiveness of contrastive learning. In particular, our model also performs well in lower\n0 0.2 0.4 0.6 0.8 1\n0 1 2 3 4 5 6 7 8 9 10 11 12\nSimilar semantics Same Paradigm\nLayer index\nC o si n e si m il a ri ty\n0 0.2 0.4 0.6 0.8\n1 2 3 4 5 6 7 8 9 10\nInterval index\nR a te o f co rr e ct p re d ic ti o n s\nintervals such as [0.6,0.7) and [0.7,0.8), which indicates those problems a little far away from their centers are not easily confused with other problems of different patterns, and our model disperses different patterns apart indeed.\nBesides, we show few examples in Table 7. It can be seen that the contrastive learning method helps the model capture the quantitative relationships exactly."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we find the neural network generates incorrect solutions due to the non-distinction of MWP patterns. To this end, we propose a contrastive learning approach to support the model to perceive divergence of patterns. We seek similar patterns in terms of the equation tree structure and collect easily confused patterns for our model to distinguish. Our method outperforms previous baselines on Math23k and MathQA in monolingual and multilingual settings."
    } ],
    "references" : [ {
      "title" : "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
      "author" : [ "Aida Amini", "Saadia Gabriel", "Shanchuan Lin", "Rik Koncel-Kedziorski", "Yejin Choi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference",
      "citeRegEx" : "Amini et al\\.,? 2019",
      "shortCiteRegEx" : "Amini et al\\.",
      "year" : 2019
    }, {
      "title" : "Robust understanding of word problems with extraneous information",
      "author" : [ "Yefim Bakman." ],
      "venue" : "arXiv preprint math/0701393.",
      "citeRegEx" : "Bakman.,? 2007",
      "shortCiteRegEx" : "Bakman.",
      "year" : 2007
    }, {
      "title" : "Adversarial contrastive estimation",
      "author" : [ "Avishek Joey Bose", "Huan Ling", "Yanshuai Cao." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1021–1032, Melbourne, Australia.",
      "citeRegEx" : "Bose et al\\.,? 2018",
      "shortCiteRegEx" : "Bose et al\\.",
      "year" : 2018
    }, {
      "title" : "A dendrite method for cluster analysis",
      "author" : [ "T. Caliński", "J Harabasz." ],
      "venue" : "Communications in Statistics, 3(1):1–27.",
      "citeRegEx" : "Caliński and Harabasz.,? 1974",
      "shortCiteRegEx" : "Caliński and Harabasz.",
      "year" : 1974
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "CoRR, abs/2003.10555.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Everybody Counts: A Report to the Nation on the Future of Mathematics Education",
      "author" : [ "National Research Council." ],
      "venue" : "The National Academies Press, Washington, DC.",
      "citeRegEx" : "Council.,? 1989",
      "shortCiteRegEx" : "Council.",
      "year" : 1989
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding and solving arithmetic word problems: A computer simulation",
      "author" : [ "Charles R Fletcher." ],
      "venue" : "Behavior Research Methods, Instruments, & Computers, 17(5):565–571.",
      "citeRegEx" : "Fletcher.,? 1985",
      "shortCiteRegEx" : "Fletcher.",
      "year" : 1985
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2104.08821.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Dimensionality reduction by learning an invariant mapping",
      "author" : [ "Raia Hadsell", "Sumit Chopra", "Yann LeCun." ],
      "venue" : "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 2, pages 1735–1742. IEEE.",
      "citeRegEx" : "Hadsell et al\\.,? 2006",
      "shortCiteRegEx" : "Hadsell et al\\.",
      "year" : 2006
    }, {
      "title" : "Neural math word problem solver with reinforcement learning",
      "author" : [ "Danqing Huang", "Jing Liu", "Chin-Yew Lin", "Jian Yin." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 213–223, Santa Fe, New Mexico, USA. Asso-",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Hard negative mixing for contrastive learning",
      "author" : [ "Yannis Kalantidis", "Mert Bulent Sariyildiz", "Noe Pion", "Philippe Weinzaepfel", "Diane Larlus." ],
      "venue" : "arXiv preprint arXiv:2010.01028.",
      "citeRegEx" : "Kalantidis et al\\.,? 2020",
      "shortCiteRegEx" : "Kalantidis et al\\.",
      "year" : 2020
    }, {
      "title" : "Point to the expression: Solving algebraic word problems using the expressionpointer transformer model",
      "author" : [ "Bugeun Kim", "Kyung Seo Ki", "Donggeon Lee", "Gahgene Gweon." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Parsing algebraic word problems into equations",
      "author" : [ "Rik Koncel-Kedziorski", "Hannaneh Hajishirzi", "Ashish Sabharwal", "Oren Etzioni", "Siena Dumas Ang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:585–597.",
      "citeRegEx" : "Koncel.Kedziorski et al\\.,? 2015",
      "shortCiteRegEx" : "Koncel.Kedziorski et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to automatically solve algebra word problems",
      "author" : [ "Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Kushman et al\\.,? 2014",
      "shortCiteRegEx" : "Kushman et al\\.",
      "year" : 2014
    }, {
      "title" : "Modeling intra-relation in math word problems with different functional multi-head attentions",
      "author" : [ "Jierui Li", "Lei Wang", "Jipeng Zhang", "Yan Wang", "Bing Tian Dai", "Dongxiang Zhang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Fixing weight decay regularization in adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "CoRR, abs/1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Are NLP models really able to solve simple math word problems",
      "author" : [ "Arkil Patel", "Satwik Bhattamishra", "Navin Goyal" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Patel et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Patel et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to think mathematically: Problem solving, metacognition, and sense making in mathematics (reprint)",
      "author" : [ "A. Schoenfeld." ],
      "venue" : "Journal of Education, 196:1 – 38.",
      "citeRegEx" : "Schoenfeld.,? 1992",
      "shortCiteRegEx" : "Schoenfeld.",
      "year" : 1992
    }, {
      "title" : "Investigating math word problems using pretrained multilingual language models",
      "author" : [ "Minghuan Tan", "Lei Wang", "Lingxiao Jiang", "Jing Jiang" ],
      "venue" : null,
      "citeRegEx" : "Tan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2021
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9:2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Deep neural solver for math word problems",
      "author" : [ "Yan Wang", "Xiaojiang Liu", "Shuming Shi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845– 854, Copenhagen, Denmark. Association for Com-",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "A goal-driven tree-structured neural model for math word problems",
      "author" : [ "Zhipeng Xie", "Shichao Sun." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 5299–5305. International",
      "citeRegEx" : "Xie and Sun.,? 2019",
      "shortCiteRegEx" : "Xie and Sun.",
      "year" : 2019
    }, {
      "title" : "Reducing word omission errors in neural machine translation: A contrastive learning approach",
      "author" : [ "Zonghan Yang", "Yong Cheng", "Yang Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph-totree learning for solving math word problems",
      "author" : [ "Jipeng Zhang", "Lei Wang", "Roy Ka-Wei Lee", "Yi Bin", "Yan Wang", "Jie Shao", "Ee-Peng Lim." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3928–",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In teaching, students are encouraged to recognize that mathematics is really about patterns and not merely about numbers (Council, 1989).",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 22,
      "context" : "However, while using deep learning to solve MWPs, existing methods (Xie and Sun, 2019; Zhang et al., 2020) get stuck in memorizing procedures.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "However, while using deep learning to solve MWPs, existing methods (Xie and Sun, 2019; Zhang et al., 2020) get stuck in memorizing procedures.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "We conduct experiments on the Chinese dataset Math23k (Wang et al., 2017) and the English dataset MathQA (Amini et al.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : ", 2017) and the English dataset MathQA (Amini et al., 2019) in monolingual and multilingual settings.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "The methods can be divided into four categories: rule-based methods (Fletcher, 1985; Bakman, 2007), statistical machine learning methods (Kushman et al.",
      "startOffset" : 68,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "The methods can be divided into four categories: rule-based methods (Fletcher, 1985; Bakman, 2007), statistical machine learning methods (Kushman et al.",
      "startOffset" : 68,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "The methods can be divided into four categories: rule-based methods (Fletcher, 1985; Bakman, 2007), statistical machine learning methods (Kushman et al., 2014; Hosseini et al., 2014), semantic parsing methods (Shi et al.",
      "startOffset" : 137,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : ", 2014), semantic parsing methods (Shi et al., 2015; Koncel-Kedziorski et al., 2015) and deep learning methods (Wang et al.",
      "startOffset" : 34,
      "endOffset" : 84
    }, {
      "referenceID" : 21,
      "context" : ", 2015) and deep learning methods (Wang et al., 2017; Huang et al., 2018; Xie and Sun, 2019; Zhang et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : ", 2015) and deep learning methods (Wang et al., 2017; Huang et al., 2018; Xie and Sun, 2019; Zhang et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : ", 2015) and deep learning methods (Wang et al., 2017; Huang et al., 2018; Xie and Sun, 2019; Zhang et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : ", 2015) and deep learning methods (Wang et al., 2017; Huang et al., 2018; Xie and Sun, 2019; Zhang et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "Recent works (Kim et al., 2020; Tan et al., 2021) based on pretrained language models, such as BERT, enhance the ability of problem understanding.",
      "startOffset" : 13,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "Recent works (Kim et al., 2020; Tan et al., 2021) based on pretrained language models, such as BERT, enhance the ability of problem understanding.",
      "startOffset" : 13,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "We perform analyses on two widely used datasets Math23k (Wang et al., 2017) and MathQA (Amini et al.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "2 Model Architecture Semantic Encoder The pre-trained language model BERT (Devlin et al., 2019) is employed as the semantic encoder.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 22,
      "context" : "Equation Decoder A tree decoder (Xie and Sun, 2019) is adopted to generate solution equations.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "We evaluate our method on two widely used datasets (Wang et al., 2017; Amini et al., 2019), and demonstrate its effectiveness in monolingual and multilingual settings.",
      "startOffset" : 51,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "We evaluate our method on two widely used datasets (Wang et al., 2017; Amini et al., 2019), and demonstrate its effectiveness in monolingual and multilingual settings.",
      "startOffset" : 51,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "1 Configuration Data and Metrics We collect problems from the Chinese dataset Math23k (Wang et al., 2017) and the English dataset MathQA (Amini et al.",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : ", 2017) and the English dataset MathQA (Amini et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "We adopt BERTbase (Devlin et al., 2019) as the problem encoder, and follow Xie and Sun (2019) to build the treedecoder for solution generation.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "We use AdamW (Loshchilov and Hutter, 2017) as our optimizer, and perform grid search over the sets of the learning rate as {5e-5, 1e-4} and the number of epochs as {30, 50} for each training stage.",
      "startOffset" : 13,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "In addition to comparing with BERT, we also investigate the following approaches: GroupAttention2 (Li et al., 2019) develop an attention mechanism to capture the quantity-related and question-related information.",
      "startOffset" : 98,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "GTS3 (Xie and Sun, 2019) generate equation trees through a tree structure decoder in a goaldriven mannner.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "Graph2Tree4 (Zhang et al., 2020) design a graph-based encoder for representing the relationships and order information among the quantities.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "Monolingual Setting GroupAttention (Li et al., 2019) 69.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "To measure this variation, we calculate the Calinski-Harabasz index (Caliński and Harabasz, 1974).",
      "startOffset" : 68,
      "endOffset" : 97
    } ],
    "year" : 0,
    "abstractText" : "Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives. Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs. In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns. We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations like n1 + n2 are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions. Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns. We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures. The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer. We conduct experiments1 on the Chinese dataset Math23k and the English dataset MathQA. Our method greatly improves the performance in monolingual and multilingual settings.",
    "creator" : null
  }
}
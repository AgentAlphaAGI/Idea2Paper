{
  "name" : "ARR_2022_37_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition(NER) aims to identify and classify entity mentions from unstructured text, e.g., extracting location mention \"Berlin\" from sentence \"Berlin is wonderful in the winter\". NER is a key component in information retrieval (Tan et al., 2021), question answering (Min et al., 2021), dialog systems (Wang et al., 2020), etc. Traditional NER models are feature-engineering and machine learning based (Zhou and Su, 2002; Takeuchi and Collier, 2002; Agerri and Rigau, 2016). Benefiting from the development of deep learning, neuralnetwork-based NER models have achieved stateof-the-art results on several public benchmarks (Lample et al., 2016; Peters et al., 2018; Devlin et al., 2018; Yamada et al., 2020; Yan et al., 2021).\nRecent studies (Lin et al., 2020; Agarwal et al., 2021) show that, context does influence predictions of NER models, but the main factor driving high performance is learning the named tokens themselves. Consequently, NER models underperform\nwhen predicting entities that have not been seen during training (Fu et al., 2020; Lin et al., 2020), which is referred to as an Out-of-Vocabulary(OOV) problem.\nThere are three classical strategies to alleviate the OOV problem: external knowledge, OOV word embedding, and contextualized embedding. The first one is to introduce additional features, e.g., entity lexicons (Zhang and Yang, 2018), part-ofspeech tags (Li et al., 2018), which alleviates the model’s dependence on word embeddings. However, the external knowledge is not always easy to obtain. The second strategy is to get a better OOV word embedding (Peng et al., 2019; Fukuda et al., 2020). The strategy is learning a static OOV embedding representation, but not directly utilize the context. Last one is fine-tune pre-trained models, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), which provide contextualized word representations. Unfortunately, Yan et al. (2021) shows that the higher performance of pretrained models could be the results of learning the subword structure better.\nHow do we make the model focus on contextual information to tackle the OOV problem? Motivated by the information bottleneck principle (Tishby et al., 2000), we propose a novel learning framework - Mutual Information based Named Entity\nRecognition (MINER). The proposed method provides an information-theoretic perspective to the OOV problem by training an encoder to minimize task-irrelevant nuisances while keeping predictive information.\nSpecifically, MINER contains two mutual information based learning objectives: i) generalizing information maximization, which aims to maximize the mutual information between representations and well-generalizing features, i.e., context and entity surface forms; ii) superfluous information minimization, which prevents the model from rote memorizing the entity names or exploiting biased cus via eliminating entity name information.\nOur main contributions are summarized as follows:\n1. We propose a novel learning framework, i.e., MINER, from an information theory perspective, aiming to improve the robustness of entity changes by eliminating entity-specific and maximize wellgeneralizing information.\n2. We show its effectiveness on several settings and benchmarks, and suggest that MINER is a reliable approach to better OOV entity recognition."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we highlight the information bottleneck principle. Subsequently, the analysis of possible issues when applying it to OOV entity recognition was provided. Furthermore, we review related techniques in deriving our framework.\nInformation Bottleneck (IB) principle originated in information theory, and provides a theoretical framework for analyzing deep neural networks. It formulates the goal of representation learning as an information trade-off between representation compression and predictive power. Given the input dataset (X,Y), it seeks to learn the internal representation Z of some intermediate layers by:\nLIB = −I(Z;Y ) + β ∗ I(Z;X),\nwhere I represents the mutual information(MI), a measure of the mutual dependence between the two variables. The trade-off between the two MI terms is controlled by a Lagrange multiplier β. A low loss indicates that representation Z does not keep too much information from X while still retaining enough information to predict Y.\nSection 5 suggests that directly applying IB to NER can not bring obvious improvement. We\nargue that IB cannot guarantee well-generalizing representation.\nOn the one hand, it has been shown that it is challenging to find a trade-off between high compression and high predictive power (Tishby et al., 2000; Wang et al., 2019; Piran et al., 2020). When compressing task-irrelevant nuisances, however, useful information will inevitably be left out. On the other hand, it is unclear for the IB principle which parts of features are well-generalizing and which are not, as we usually train a classifier to solely maximize accuracy. Consequently, neural networks tend to use any accessible signal to do so (Ilyas et al., 2019), which is referred to as a shotcut learning problem (Geirhos et al., 2020). For training sets with limited size, it may be easier for neural networks to memorize entity names rather than to classify them by context and common entity features (Agarwal et al., 2021). In Section 4, we demonstrate how we extend BN to the NER task and address these issues."
    }, {
      "heading" : "3 Model Architecture",
      "text" : "In recent years, NER systems have undergone a paradigm shift from sequence labeling, which formulates NER as a token-level tagging task (Chiu and Nichols, 2016; Akbik et al., 2018; Yan et al., 2019), to span prediction (SpanNER), which regards NER as a span-level classification task (Mengge et al., 2020; Yamada et al., 2020; Fu et al., 2021). We choose SpanNER as base architecture for two reasons:\n1) SpanNER can yield the whole span representation, which can be directly used for optimize information. 2) compared with sequence labeling, SpanNER does better in sentences with more OOV words (Fu et al., 2021).\nOverall, SpanNER consists of three major modules: token representation layer, span representation layer, and span classification layer. Besides, our method inserts a bottleneck layer to the architecture for information optimization."
    }, {
      "heading" : "3.1 Token Representation Layer",
      "text" : "Let X = {x1, x2, · · · , xn} represents the input sentence, thus, the token representation hi is as follows:\nu1, · · · , un = Embedding(x1, · · · , xn) (1)\nh1, · · · , hn = Encoder(u1, · · · , un) (2)\nwhere Embedding() is the non-contextualized word embeddings, e.g., Glove (Pennington et al., 2014) or contextualized word embeddings, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2018). Encoder() can be any network structures with context encoding function, e.g., LSTM (Hochreiter and Schmidhuber, 1997), CNN (LeCun et al., 1995), transformer (Vaswani et al., 2017), and so on."
    }, {
      "heading" : "3.2 Span Representation Layer",
      "text" : "For all possible spans S = {s1, s2, · · · , sm} of sentence X , we re-assign a label y ∈ Y for each span. Take \"Berlin is wonderful\" as an example, its possible spans and labels are {(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)} and {LOC,O,O,O,O,O}, respectively.\nGiven the start index bi and end index ei, the representation of span si can be calculated by two parts: boundary embedding and span length embedding.\nBoundary embedding: This part is calculated by concatenating the start and end tokens’ representation tbi = [hbi ;hei ].\nSpan length embedding: In order to introduce the length feature, we additionally provide the length embedding tli, which can be obtained by a learnable look-up table.\nFinally, the span representation can be obtained as: ti = [tbi ; t l i]."
    }, {
      "heading" : "3.3 Information Bottleneck Layer",
      "text" : "In order to optimize the information in the span representation, our method additionally adds an information bottleneck layer of the form:\nN ( z | fµe (t), fΣe (x) ) (3)\nwhere fe is an MLP which outputs both the Kdimensional mean µ of z as well as the K ∗ K covariance matrix Σ."
    }, {
      "heading" : "3.3.1 Span Classification Layer",
      "text" : "Once the information bottleneck layer is finished, zi is fed into the classifier to obtain the probability of its label yi. Based on the probability, the basic loss function can be calculated as follows:\nLbase = − score(zi, yi)∑\ny′∈Y score(zi, y ′) , (4)\nwhere score() is a function that measures the compatibility between a specified label and a span representation:\nscore(zi, y k) = exp(zTi y k), (5)\nwhere yk is a learnable representation of class k. Heuristic Decoding A heuristic decoding solution for the flat NER is provided to avoid the prediction of over-lapped spans. For those overlapped spans, we keep the span with the highest prediction probability and drop the others.\nIt’s worth noting that our method is flexible and can be used with any other NER model based\non span classification. In next section, we will introduce two additional objectives to tackle the OOV problem of NER."
    }, {
      "heading" : "4 MI-based objectives",
      "text" : "Motivated by IB (Tishby et al., 2000; Federici et al., 2020), we can subdividing I(X;Z) into two components by using the chain rule of mutual information(MI):\nI(X;Z) = I(Y ;Z)︸ ︷︷ ︸ predictive + I(X;Z|Y )︸ ︷︷ ︸ superfluous , (6)\nThe first term determines how much information about Y is accessible from Z. While the second term, conditional mutual information term I(X;Z|Y ), denotes the information in Z that is not predictive of Y .\nFor NER, which parts of the information retrieved from input are useful and which are redundant?\nFrom human intuition, text context should be the main predictive information for NER. For example, \"The CEO of X resigned\", the type of X in each of these contexts should always be \"ORG\". Besides, entity mentions also provide much information for entity recognition. For example, nearly all person names capitalize the first letter and follow the \"firstName lastName\" or \"lastName firstName\" patterns. However, entity name is not a well-generalizing features. By simply memorizing the fact which span is an entity, it may be possible for it to fit the training set, but it is impossible to predict entities that have never been seen before.\nWe convert the targets of Eq. (6) into a form that is easier to solve via a contrastive strategy. Specifically, consider x1 and x2 are two contrastive samples of similar context, and contains different entity mentions of the same entity category, i.e., s1 and s2, respectively. Assuming both x1 and x2 are both sufficient for inferring label y. The mutual information between x1 and z1 can be factorized to two parts.\nI(x1; z1) = I(z1;x2)︸ ︷︷ ︸ consistent + I(x1; z1|x2)︸ ︷︷ ︸ specific , (7)\nwhere z1 and z2 are span representations of s1 and s2, respectively, I(z1;x2) denotes the information that isn’t entity-specific. And I(x1; z1|x2) represents the information in z1 which is unique to x1\nbut is not predictable by sentence x2, i.e., entityspecific information.\nThus any representation z containing all information shared from both sentences would also contain the necessary label information, and sentencespecific information is superfluous. So Eq. (6) can be approximated by Eq. (7) by:\nmaximize I(z1; y) ∼ I(z1;x2), (8)\nminimize I(x1; z1|y) ∼ I(x1; z1|x2), (9)\nThe target of Eq. (8) is defined as generalizing information maximization. We proved that I(z1; z2) is a lower bound of I(z1;x2)(proof could be found in appendix 7). InfoNCE (Oord et al., 2018) was used as a lower bound on MI and can be used to approximate I(z1; z2). Subsequently, it can be optimized by:\nLgi = −Ep [ gw(z1, z2)− Ep′ log\n∑ z′ exp gw(z1, z ′)\n] ,\n(10) where gw(·, ·) is a compatible score function approximated by a neural network, z2 are the positive entity representations from the joint distribution p of original sample and corresponding generated sample, z′ are the negative entity representations drawn from the joint distribution of original sample and other original sample.\nThe target of Eq. (9) is defined as superfluous information minimization. To restrict this term, we can minimize an upper bound of I(x1; z1|x2) (proofs could be found in appendix 7) as follows:\nLsi = Ex1,x2Ez1,z2 [DJS [Pz1 ||Pz2 ]] , (11)\nwhere DJS represents Jensen-Shannon divergence. In practice, Eq. (11) encourage z to be invariant to entity changes."
    }, {
      "heading" : "4.1 Contrastive sample generation",
      "text" : "It is difficult to obtain samples with similar contexts but different entity words. We generate contrastive samples by the mention replacement mechanism(Dai and Adel, 2020). For each mention in the sentence, we replace it by another mention from the original training set, which has the same entity type. The corresponding span label can be changed accordingly. For example, \"LOC\" mention \"Berlin\" in sentence \"Berlin is wonderful in the winter\" is replaced by \"Iceland\"."
    }, {
      "heading" : "4.2 Training",
      "text" : "Combine Eq. (4), (10), and (11), we can get the following objective function, which try to minimize:\nL = Lbase + γ ∗ Lgi + β ∗ Lsi, (12)\nwhere γ and β are the weights of the generalizing information loss and superfluous information loss, respectively."
    }, {
      "heading" : "5 Experiment",
      "text" : "In this section, we verified the performance of the proposed method on five OOV datasets, and compared it with other methods. In addition, We tested the universality of the proposed method in various pre-trained models."
    }, {
      "heading" : "5.1 Datasets and Metrics",
      "text" : "Datasets We performed experiments on:\n1. WNUT2017 (Derczynski et al., 2017), a dataset focus on unusual, previous-unseen entities in training data, and is collected from social media.\n2. TwitterNER (Zhang et al., 2018), an English NER dataset created from Tweets.\n3. BioNER (Kim et al., 2004), the JNLPBA 2004 Bio-NER dataset focus on technical terms in the biology domain.\n4. Conll03-Typos (Wang et al., 2021), which is generated from Conll2003 (Sang and De Meulder, 2003). The entities in the test set is replaced by typos version(character modify, insert, and delete operation).\n5. Conll03-OOV (Wang et al., 2021), which is generated from Conll2003 (Sang and De Meulder, 2003). The entities in the test set is replaced by another out-of-vocabulary entity in test set.\nTable 2 reports the static results of the OOV problem on the test sets of each dataset. As shown in the table, the test set of these data sets comprises a substantial amount of OOV entities.\nMetrics We measured the entity-level micro average F1 score on the test set to compare the results of different models."
    }, {
      "heading" : "5.2 Baseline methods",
      "text" : "Li et al. (2020) share the same intuition, enrich word representations with contextual, with us. However, the work is neither open source nor reported on the same data set, so this method is not compared with MINER. We compare our method with baselines as follows:\n• SpanNER (Fu et al., 2021), which is trained by original SpanNER framework, means without any constraint and extra data processing.\n• Vanilla information bottleneck(VaniIB), this method employs the original information bottleneck constraint to the SpanNER, which is optimized based on Alemi et al. (2016). Compared with our method, it directly compresses all the information from the input.\n• Dai and Adel (2020) (DataAug) , which trains model with data augmentation strategy, while keeps the same model architecture of SpanNER. This model is trained by 1:1 original training set and entity replacement training set, which keeps the same input as the proposed method.\n• Shahzad et al. (2021) (InferNER), the method focus on word-, character-, and sentence-level information for NER in short-text, without recurring to external sources. In addition, it is able to incorporate visual information and introduce an attention component which computes attention weight probabilities over textual and text-relevant visual contexts separately.\n• Li et al. (2021) (MIN), which utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task.\n• Fukuda et al. (2020) (CoFEE), which refer to pre-trained word embeddings for known\nwords with similar surfaces to target OOV words.\n• Nie et al. (2020) (SA-NER), which utilize semantic enhancement methods to reduce the negative impact of data sparsity problems. Specifically, the method obtains the augmented semantic information from a largescale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively.\nTo verify the universality of our method, we measured its performance in various pre-trained models, i.e., Bert (Devlin et al., 2018), Roberta (Liu et al., 2019), Albert (Lan et al., 2019)."
    }, {
      "heading" : "5.3 Implementation Details",
      "text" : "Bert-large released by Devlin et al. (2018) is selected as our base encoder. The learning rate is set to 5e-5, and the dropout is set to 0.2. The output dim of information bottleneck layer is 50. In order to make a trade-off for the performance and efficiency, on the one hand, we truncate the part of the sentence whose tokens exceeds 128. On the other hand, we count the length distribution of entity length in different datasets, and finally chose 4 as the maximum enumerated entity length. The values of β and γ are different for different data sets. Empirically, 1e-5 for β and 0.01 for γ can get promised results. The model is trained in a NVIDIA GeForce RTX 2080Ti GPU. Checkpoints\nwith top-3 performance are finally evaluated on the test set to report averaged results."
    }, {
      "heading" : "5.4 Main Results",
      "text" : "We demonstrate the effectiveness of MINER against other state-of-the-art models. As shown in table 3, we have the following observations and analysis:\n1) Our baseline model, i.e., SpanNER, does a good job at predicting OOV entities. Compared to sequence labeling, the the span classification could model the relation of entity tokens directly;2) The performance of SpanNER is further boosted with our proposed approach, which proved the effectiveness of our method. As shown in table, we almost beats all other SOTA methods without any external resource;3) Compared to Typos data transformation, it is more difficult for model to predict OOV words. To pre-trained model, typos word may not appear in training set, but they share most subwords with the original token. MOreover, the subword of OOV entity may be rare; 4) It seems that the traditional information bottleneck will not greatly improve the OOV prediction ability of the model. We argue that the traditional information bottlenecks will indiscriminately compress the information in the representation, leading to underfitting; 5Our model has significantly improved the performance of the model on the entity perturbed methods of typos and OOV, proving that our method can not only improve the generalization ability of OOV words in the field, but also significantly improve the robustness in the face of noise; 6) It is clearly\nthat our proposed method is universal and can further improve OOV prediction performance for different embedding models. As we get stably improvements on Bert, Roberta, and Albert."
    }, {
      "heading" : "5.5 Ablation Study",
      "text" : "We also perform ablation studies to validate the effectiveness of each part in MINER. Table 4 demonstrates the results of different settings for the proposed training strategy equipped with BERT. After only adding the Lgi loss for enhance context and entity surface form information, we find that the results are better than the original PLMs. Similar phenomenon occured in Lsi, too. It reflects that both Lgi and Lsi are beneficial to improve the generalizing ability on OOV entities. Moreover, the results on three dataset are significantly improved by add both Lgi and Lsi learning objectives. It means Lgi and Lsi can boost each over, which proves that our method enhances representation via deep understanding of context and entity surface forms and discourages representation from rotate\nmemorizing entity names or exploiting biased cues in data."
    }, {
      "heading" : "5.6 Sensitivity Analysis of β and γ",
      "text" : "To show the different influence of our proposed training objectives Lgi and Lsi, we conduct sensitivity analysis of the coefficient β and γ. Figure 2 shows the performance change under different settings of the two coefficients. The yellow line denotes ablation results without the corresponding loss functions (with β=0 or γ=0). From Figure 2 we can observe that the performance is significantly enhanced with a small rate of β or γ, where the best performance is achieved when β=1e-3 and γ=1e-4, respectively. It probes the effectiveness of our proposed training objectives that enhances representation via deep understanding of context and entity surface forms and discourages representation from rotate memorizing entity names or exploiting biased cues in data. When the coefficient rate increases continuously, the performance shows a decline trend, which means the over-constraint of Lgi or Lsi will hurt the generalizing ability of predicting the OOV entities."
    }, {
      "heading" : "5.7 Interpretable Analysis",
      "text" : "The above experiments show the promising performance of MINER on predicting the unseen entities. To further investigate which part of the sentence MINER focuses on, we visualize the attention weights over entities and contexts. We demonstrate an example in Figure 4 , where is selected from TwitterNER. The attention score is calculated by averaging the attention weight of the 0th layer of BERT. Take the attention weights of entity \"State Street\" as a example, it is obvious that baseline model, i.e., SpanNER, focus on entity\nwords themselves. While the scores of our model is more average, means that our method concern more context information."
    }, {
      "heading" : "6 Related Work",
      "text" : ""
    }, {
      "heading" : "6.1 External Knowledge",
      "text" : "This group of methods makes it easier to predict OOV entities using external knowledge. Zhang and Yang (2018) Use a dictionary to list numerous entity mentions. It is possible to get stronger \"lookup\" models by integrating dictionary information, but there is no guarantee that entities outside the training set and vocabulary will be correctly identified. To diminish the model’s dependency on OOV embedding, Li et al. (2018) introduces partof-speech tags. External resources are not always available, which is a limitation of this strategy."
    }, {
      "heading" : "6.2 OOV word Embedding",
      "text" : "The OOV problem can be alleviated by improving the OOV word embedding. The character ngram of each word is used by Bojanowski et al. (2017) to represent the OOV word embedding. Pinter et al. (2017) captures morphological features using character-level RNN. Another technique is to first match the OOV words with the words that have been seen in training, then replace the OOV words’ embedding with the seen words’ embedding. Peng et al. (2019) trains a student network to predict the closest word representation to the OOV term. Fukuda et al. (2020) referring to pre-trained word embeddings for known words with similar surfaces to target OOV words. This kind of method is learning a static OOV embedding representation, and does not directly utilize the context."
    }, {
      "heading" : "6.3 Contextualized Embedding",
      "text" : "Contextual information is used to enhance the representation of OOV words in this strategy. (Hu et al., 2019) formulate the OOV problem as a Kshot regression problem and learns to predict the OOV embedding by aggregating only K contexts and morphological features. Pre-trained models\ncontextualized word embbeddings via pretraining on large background corpora. Furthermore, contextualized word embeddings can be provided by the pre-trained models which are pre-trained on large background corpora (Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019). Yan et al. (2021) shows that BERT are not always better at capturing context as compared to Gloe-based BiLSTM-CRFs. Their higher performance could be the results of learning the subword structure better."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Based on the recent studies of NER, we analyzed how to improve the OOV entity recognition. In this work, we propose a novel and flexible learning framework - MINER, to tackle OOV entities recognition issue from an information-theoretic perspective. On the one hand, this method can enhance the context information of the output of the encoder. On the other hand, it can safely eliminate task-irrelevant nuisances and prevents the model from rote memorizing the entities. Specifically, the proposed approach contains two mutual information based training objectives: generalizing information maximization, and superfluous information minimization. Experiments on various datasets demonstrate that MINER achieves much better performance in predicting out-of-vocabulary entities."
    }, {
      "heading" : "A Appendix",
      "text" : "This section provides the proof of generalizing information maximization, i.e., Eq. (8). Consider x1 and x2 are two contrastive samples of similar context, and contains different entity mentions of the same entity category, i.e., s1 and s2, respectively.\nI(z1;x2) =I(z1;x2z2)− I(z1; z2|x2) =I(z1;x2z2)\n=I(z1; z2) + I(z1;x2|z2) ≥I(z1; z2)\n(13)"
    }, {
      "heading" : "B Appendix",
      "text" : "This section provides the proof of superfluous information minimization, i.e. Eq. (9).\nI(x1; z1|x2)\n= Ex1,x2∼p(x1,x2)Ez∼p(z1|v1) log p(x1,z1|x2)\np(x1|x2)p(z1|x2)\n= Ex1,x2∼p(x1,x2)Ez∼p(z1|v1) log p(z1|x1)p(x1|x2) p(x1|x2)p(z1|x2)\n= Ex1,x2∼p(x1,x2)Ez∼p(z1|v1) log p(z1|x1) p(z1|x2)\n= Ex1,x2∼p(x1,x2)Ez∼p(z1|v1) log p(z1|x1)p(z2|x2) p(z2|x2)p(z1|x2)\n= DKL(p(z1|x1)||p(z2|x2))\n−DKL(p(z1|x2)||p(z2|x2))\n≤ DKL(p(z1|x1)||p(z2|x2))(14)"
    } ],
    "references" : [ {
      "title" : "Interpretability analysis for named entity recognition to understand system predictions and how they can improve",
      "author" : [ "Oshin Agarwal", "Yinfei Yang", "Byron C Wallace", "Ani Nenkova." ],
      "venue" : "Computational Linguistics, 47(1):117–140.",
      "citeRegEx" : "Agarwal et al\\.,? 2021",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2021
    }, {
      "title" : "Robust multilingual named entity recognition with shallow semisupervised features",
      "author" : [ "Rodrigo Agerri", "German Rigau." ],
      "venue" : "Artificial Intelligence, 238:63–",
      "citeRegEx" : "Agerri and Rigau.,? 2016",
      "shortCiteRegEx" : "Agerri and Rigau.",
      "year" : 2016
    }, {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th international conference on computational linguistics, pages 1638–1649.",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep variational information bottleneck",
      "author" : [ "Alexander A Alemi", "Ian Fischer", "Joshua V Dillon", "Kevin Murphy." ],
      "venue" : "arXiv preprint arXiv:1612.00410.",
      "citeRegEx" : "Alemi et al\\.,? 2016",
      "shortCiteRegEx" : "Alemi et al\\.",
      "year" : 2016
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason PC Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "An analysis of simple data augmentation for named entity recognition",
      "author" : [ "Xiang Dai", "Heike Adel." ],
      "venue" : "arXiv preprint arXiv:2010.11683.",
      "citeRegEx" : "Dai and Adel.,? 2020",
      "shortCiteRegEx" : "Dai and Adel.",
      "year" : 2020
    }, {
      "title" : "Results of the wnut2017 shared task on novel and emerging entity recognition",
      "author" : [ "Leon Derczynski", "Eric Nichols", "Marieke van Erp", "Nut Limsopatham." ],
      "venue" : "Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 140–147.",
      "citeRegEx" : "Derczynski et al\\.,? 2017",
      "shortCiteRegEx" : "Derczynski et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning robust representations via multi-view information bottleneck",
      "author" : [ "Marco Federici", "Anjan Dutta", "Patrick Forré", "Nate Kushman", "Zeynep Akata." ],
      "venue" : "arXiv preprint arXiv:2002.07017.",
      "citeRegEx" : "Federici et al\\.,? 2020",
      "shortCiteRegEx" : "Federici et al\\.",
      "year" : 2020
    }, {
      "title" : "SpanNER: Named entity re-/recognition as span prediction",
      "author" : [ "Jinlan Fu", "Xuanjing Huang", "Pengfei Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
      "citeRegEx" : "Fu et al\\.,? 2021",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2021
    }, {
      "title" : "Rethinking generalization of neural models: A named entity recognition case study",
      "author" : [ "Jinlan Fu", "Pengfei Liu", "Qi Zhang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7732–7739.",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Robust Backed-off Estimation of Out-of-Vocabulary Embeddings",
      "author" : [ "Nobukazu Fukuda", "Naoki Yoshinaga", "Masaru Kitsuregawa." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4827–4838, Online. Association for",
      "citeRegEx" : "Fukuda et al\\.,? 2020",
      "shortCiteRegEx" : "Fukuda et al\\.",
      "year" : 2020
    }, {
      "title" : "Shortcut learning in deep neural networks",
      "author" : [ "Robert Geirhos", "Jörn-Henrik Jacobsen", "Claudio Michaelis", "Richard Zemel", "Wieland Brendel", "Matthias Bethge", "Felix A Wichmann." ],
      "venue" : "Nature Machine Intelligence, 2(11):665–673.",
      "citeRegEx" : "Geirhos et al\\.,? 2020",
      "shortCiteRegEx" : "Geirhos et al\\.",
      "year" : 2020
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Few-shot representation learning for outof-vocabulary words",
      "author" : [ "Ziniu Hu", "Ting Chen", "Kai-Wei Chang", "Yizhou Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4102–4112, Florence, Italy. Asso-",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial examples are not bugs, they are features",
      "author" : [ "Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Logan Engstrom", "Brandon Tran", "Aleksander Madry." ],
      "venue" : "arXiv preprint arXiv:1905.02175.",
      "citeRegEx" : "Ilyas et al\\.,? 2019",
      "shortCiteRegEx" : "Ilyas et al\\.",
      "year" : 2019
    }, {
      "title" : "Introduction to the bio-entity recognition task at jnlpba",
      "author" : [ "Jin-Dong Kim", "Tomoko Ohta", "Yoshimasa Tsuruoka", "Yuka Tateisi", "Nigel Collier." ],
      "venue" : "Proceedings of the international joint workshop on natural language processing in biomedicine and its",
      "citeRegEx" : "Kim et al\\.,? 2004",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2004
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Convolutional networks for images, speech, and time series",
      "author" : [ "Yann LeCun", "Yoshua Bengio" ],
      "venue" : "The handbook of brain theory and neural networks,",
      "citeRegEx" : "LeCun and Bengio,? \\Q1995\\E",
      "shortCiteRegEx" : "LeCun and Bengio",
      "year" : 1995
    }, {
      "title" : "A selfattentive model with gate mechanism for spoken language understanding",
      "author" : [ "Changliang Li", "Liang Li", "Ji Qi." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3824–3833, Brussels,",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Modularized interaction network for named entity recognition",
      "author" : [ "Fei Li", "Zheng Wang", "Siu Cheung Hui", "Lejian Liao", "Dandan Song", "Jing Xu", "Guoxiu He", "Meihuizi Jia." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Handling rare entities for neural sequence labeling",
      "author" : [ "Yangming Li", "Han Li", "Kaisheng Yao", "Xiaolong Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6441–6451, Online. Association for Computational",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Coarse-to-Fine Pretraining for Named Entity Recognition",
      "author" : [ "Xue Mengge", "Bowen Yu", "Zhenyu Zhang", "Tingwen Liu", "Yue Zhang", "Bin Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Mengge et al\\.,? 2020",
      "shortCiteRegEx" : "Mengge et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint passage ranking for diverse multi-answer retrieval",
      "author" : [ "Sewon Min", "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Min et al\\.,? 2021",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2021
    }, {
      "title" : "Named entity recognition for social media texts with semantic augmentation",
      "author" : [ "Yuyang Nie", "Yuanhe Tian", "Xiang Wan", "Yan Song", "Bo Dai." ],
      "venue" : "arXiv preprint arXiv:2010.15458.",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning taskspecific representation for novel words in sequence labeling",
      "author" : [ "Minlong Peng", "Qi Zhang", "Xiaoyu Xing", "Tao Gui", "Jinlan Fu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1905.12277.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Mimicking word embeddings using subword RNNs",
      "author" : [ "Yuval Pinter", "Robert Guthrie", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 102–112, Copenhagen, Denmark. Association",
      "citeRegEx" : "Pinter et al\\.,? 2017",
      "shortCiteRegEx" : "Pinter et al\\.",
      "year" : 2017
    }, {
      "title" : "The dual information bottleneck",
      "author" : [ "Zoe Piran", "Ravid Shwartz-Ziv", "Naftali Tishby." ],
      "venue" : "arXiv preprint arXiv:2006.04641.",
      "citeRegEx" : "Piran et al\\.,? 2020",
      "shortCiteRegEx" : "Piran et al\\.",
      "year" : 2020
    }, {
      "title" : "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Inferner: an attentive model leveraging the sentence-level information for named entity recognition in microblogs",
      "author" : [ "Moemmur Shahzad", "Ayesha Amin", "Diego Esteves", "Axel-Cyrille Ngonga Ngomo." ],
      "venue" : "The International FLAIRS Conference Proceed-",
      "citeRegEx" : "Shahzad et al\\.,? 2021",
      "shortCiteRegEx" : "Shahzad et al\\.",
      "year" : 2021
    }, {
      "title" : "Use of support vector machines in extended named entity recognition",
      "author" : [ "Koichi Takeuchi", "Nigel Collier." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Takeuchi and Collier.,? 2002",
      "shortCiteRegEx" : "Takeuchi and Collier.",
      "year" : 2002
    }, {
      "title" : "Extracting event temporal relations via hyperbolic geometry",
      "author" : [ "Xingwei Tan", "Gabriele Pergola", "Yulan He." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8065–8077, Online and Punta Cana,",
      "citeRegEx" : "Tan et al\\.,? 2021",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2021
    }, {
      "title" : "The information bottleneck method",
      "author" : [ "Naftali Tishby", "Fernando C Pereira", "William Bialek." ],
      "venue" : "arXiv preprint physics/0004057.",
      "citeRegEx" : "Tishby et al\\.,? 2000",
      "shortCiteRegEx" : "Tishby et al\\.",
      "year" : 2000
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-domain dialogue acts and response co-generation",
      "author" : [ "Kai Wang", "Junfeng Tian", "Rui Wang", "Xiaojun Quan", "Jianxing Yu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7125–7134, Online. Association",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep multi-view information bottleneck",
      "author" : [ "Qi Wang", "Claire Boudreau", "Qixing Luo", "Pang-Ning Tan", "Jiayu Zhou." ],
      "venue" : "Proceedings of the 2019 SIAM International Conference on Data Mining, pages 37–45. SIAM.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Textflint: Unified multilingual robustness evaluation toolkit for natural language processing",
      "author" : [ "Xiao Wang", "Qin Liu", "Tao Gui", "Qi Zhang", "Yicheng Zou", "Xin Zhou", "Jiacheng Ye", "Yongxin Zhang", "Rui Zheng", "Zexiong Pang" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Tener: adapting transformer encoder for named entity recognition",
      "author" : [ "Hang Yan", "Bocao Deng", "Xiaonan Li", "Xipeng Qiu." ],
      "venue" : "arXiv preprint arXiv:1911.04474.",
      "citeRegEx" : "Yan et al\\.,? 2019",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2019
    }, {
      "title" : "A unified generative framework for various NER subtasks",
      "author" : [ "Hang Yan", "Tao Gui", "Junqi Dai", "Qipeng Guo", "Zheng Zhang", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "Adaptive co-attention network for named entity recognition in tweets",
      "author" : [ "Qi Zhang", "Jinlan Fu", "Xiaoyu Liu", "Xuanjing Huang." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Chinese NER using lattice LSTM",
      "author" : [ "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554–1564, Melbourne, Australia. Association for Computational",
      "citeRegEx" : "Zhang and Yang.,? 2018",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2018
    }, {
      "title" : "Named entity recognition using an hmm-based chunk tagger",
      "author" : [ "GuoDong Zhou", "Jian Su." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 473–480.",
      "citeRegEx" : "Zhou and Su.,? 2002",
      "shortCiteRegEx" : "Zhou and Su.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "NER is a key component in information retrieval (Tan et al., 2021), question answering (Min et al.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : ", 2021), question answering (Min et al., 2021), dialog systems (Wang et al.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 47,
      "context" : "Traditional NER models are feature-engineering and machine learning based (Zhou and Su, 2002; Takeuchi and Collier, 2002; Agerri and Rigau, 2016).",
      "startOffset" : 74,
      "endOffset" : 145
    }, {
      "referenceID" : 36,
      "context" : "Traditional NER models are feature-engineering and machine learning based (Zhou and Su, 2002; Takeuchi and Collier, 2002; Agerri and Rigau, 2016).",
      "startOffset" : 74,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "Traditional NER models are feature-engineering and machine learning based (Zhou and Su, 2002; Takeuchi and Collier, 2002; Agerri and Rigau, 2016).",
      "startOffset" : 74,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "Benefiting from the development of deep learning, neuralnetwork-based NER models have achieved stateof-the-art results on several public benchmarks (Lample et al., 2016; Peters et al., 2018; Devlin et al., 2018; Yamada et al., 2020; Yan et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 250
    }, {
      "referenceID" : 31,
      "context" : "Benefiting from the development of deep learning, neuralnetwork-based NER models have achieved stateof-the-art results on several public benchmarks (Lample et al., 2016; Peters et al., 2018; Devlin et al., 2018; Yamada et al., 2020; Yan et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 250
    }, {
      "referenceID" : 8,
      "context" : "Benefiting from the development of deep learning, neuralnetwork-based NER models have achieved stateof-the-art results on several public benchmarks (Lample et al., 2016; Peters et al., 2018; Devlin et al., 2018; Yamada et al., 2020; Yan et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 250
    }, {
      "referenceID" : 44,
      "context" : "Benefiting from the development of deep learning, neuralnetwork-based NER models have achieved stateof-the-art results on several public benchmarks (Lample et al., 2016; Peters et al., 2018; Devlin et al., 2018; Yamada et al., 2020; Yan et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 250
    }, {
      "referenceID" : 0,
      "context" : "Recent studies (Lin et al., 2020; Agarwal et al., 2021) show that, context does influence predictions of NER models, but the main factor driving high performance is learning the named tokens themselves.",
      "startOffset" : 15,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "when predicting entities that have not been seen during training (Fu et al., 2020; Lin et al., 2020), which is referred to as an Out-of-Vocabulary(OOV) problem.",
      "startOffset" : 65,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : ", entity lexicons (Zhang and Yang, 2018), part-ofspeech tags (Li et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : ", entity lexicons (Zhang and Yang, 2018), part-ofspeech tags (Li et al., 2018), which alleviates the model’s dependence on word embeddings.",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "The second strategy is to get a better OOV word embedding (Peng et al., 2019; Fukuda et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "The second strategy is to get a better OOV word embedding (Peng et al., 2019; Fukuda et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : ", 2018), BERT (Devlin et al., 2018), which provide contextualized word representations.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 38,
      "context" : "How do we make the model focus on contextual information to tackle the OOV problem? Motivated by the information bottleneck principle (Tishby et al., 2000), we propose a novel learning framework - Mutual Information based Named Entity",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 38,
      "context" : "On the one hand, it has been shown that it is challenging to find a trade-off between high compression and high predictive power (Tishby et al., 2000; Wang et al., 2019; Piran et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 189
    }, {
      "referenceID" : 41,
      "context" : "On the one hand, it has been shown that it is challenging to find a trade-off between high compression and high predictive power (Tishby et al., 2000; Wang et al., 2019; Piran et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 189
    }, {
      "referenceID" : 33,
      "context" : "On the one hand, it has been shown that it is challenging to find a trade-off between high compression and high predictive power (Tishby et al., 2000; Wang et al., 2019; Piran et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 189
    }, {
      "referenceID" : 16,
      "context" : "Consequently, neural networks tend to use any accessible signal to do so (Ilyas et al., 2019), which is referred to as a shotcut learning problem (Geirhos et al.",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : ", 2019), which is referred to as a shotcut learning problem (Geirhos et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "For training sets with limited size, it may be easier for neural networks to memorize entity names rather than to classify them by context and common entity features (Agarwal et al., 2021).",
      "startOffset" : 166,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "In recent years, NER systems have undergone a paradigm shift from sequence labeling, which formulates NER as a token-level tagging task (Chiu and Nichols, 2016; Akbik et al., 2018; Yan et al., 2019), to span prediction (SpanNER), which regards NER as a span-level classification task (Mengge et al.",
      "startOffset" : 136,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : "In recent years, NER systems have undergone a paradigm shift from sequence labeling, which formulates NER as a token-level tagging task (Chiu and Nichols, 2016; Akbik et al., 2018; Yan et al., 2019), to span prediction (SpanNER), which regards NER as a span-level classification task (Mengge et al.",
      "startOffset" : 136,
      "endOffset" : 198
    }, {
      "referenceID" : 43,
      "context" : "In recent years, NER systems have undergone a paradigm shift from sequence labeling, which formulates NER as a token-level tagging task (Chiu and Nichols, 2016; Akbik et al., 2018; Yan et al., 2019), to span prediction (SpanNER), which regards NER as a span-level classification task (Mengge et al.",
      "startOffset" : 136,
      "endOffset" : 198
    }, {
      "referenceID" : 25,
      "context" : ", 2019), to span prediction (SpanNER), which regards NER as a span-level classification task (Mengge et al., 2020; Yamada et al., 2020; Fu et al., 2021).",
      "startOffset" : 93,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : ", 2019), to span prediction (SpanNER), which regards NER as a span-level classification task (Mengge et al., 2020; Yamada et al., 2020; Fu et al., 2021).",
      "startOffset" : 93,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "2) compared with sequence labeling, SpanNER does better in sentences with more OOV words (Fu et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : ", Glove (Pennington et al., 2014) or contextualized word embeddings, e.",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 38,
      "context" : "Motivated by IB (Tishby et al., 2000; Federici et al., 2020), we can subdividing I(X;Z) into two components by using the chain rule of mutual information(MI):",
      "startOffset" : 16,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Motivated by IB (Tishby et al., 2000; Federici et al., 2020), we can subdividing I(X;Z) into two components by using the chain rule of mutual information(MI):",
      "startOffset" : 16,
      "endOffset" : 60
    }, {
      "referenceID" : 28,
      "context" : "InfoNCE (Oord et al., 2018) was used as a lower bound on MI and can be used to approximate I(z1; z2).",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "We generate contrastive samples by the mention replacement mechanism(Dai and Adel, 2020).",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "WNUT2017 (Derczynski et al., 2017), a dataset focus on unusual, previous-unseen entities in training data, and is collected from social media.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 45,
      "context" : "TwitterNER (Zhang et al., 2018), an English NER dataset created from Tweets.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "BioNER (Kim et al., 2004), the JNLPBA 2004 Bio-NER dataset focus on technical terms in the biology domain.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 42,
      "context" : "Conll03-Typos (Wang et al., 2021), which is generated from Conll2003 (Sang and De Meulder, 2003).",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 42,
      "context" : "Conll03-OOV (Wang et al., 2021), which is generated from Conll2003 (Sang and De Meulder, 2003).",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "• SpanNER (Fu et al., 2021), which is trained by original SpanNER framework, means without any constraint and extra data processing.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 24,
      "context" : ", 2018), Roberta (Liu et al., 2019), Albert (Lan et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "(Hu et al., 2019) formulate the OOV problem as a Kshot regression problem and learns to predict the OOV embedding by aggregating only K contexts and morphological features.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 31,
      "context" : "Furthermore, contextualized word embeddings can be provided by the pre-trained models which are pre-trained on large background corpora (Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 196
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, contextualized word embeddings can be provided by the pre-trained models which are pre-trained on large background corpora (Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 196
    }, {
      "referenceID" : 24,
      "context" : "Furthermore, contextualized word embeddings can be provided by the pre-trained models which are pre-trained on large background corpora (Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 196
    } ],
    "year" : 0,
    "abstractText" : "NER model has achieved promising performance on standard NER benchmarks. However, recent studies show that previous approaches may over-rely on entity mention information, resulting in poor performance on out-of-vocabulary(OOV) entity recognition. In this work, we propose MINER, a novel NER learning framework, to remedy this issue from an information-theoretic perspective. The proposed approach contains two mutual information based training objectives: i) generalizing information maximization, which enhances representation via deep understanding of context and entity surface forms; ii) superfluous information minimization, which discourages representation from rotate memorizing entity names or exploiting biased cues in data. Experiments on various settings and datasets demonstrate that it achieves better performance in predicting OOV entities.",
    "creator" : null
  }
}
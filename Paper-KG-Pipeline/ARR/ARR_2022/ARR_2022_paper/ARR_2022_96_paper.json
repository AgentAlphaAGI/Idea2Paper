{
  "name" : "ARR_2022_96_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Early Stopping Based on Unlabeled Samples",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Early stopping, a form of regularization, is a widely used technique to prevent a model from over-fitting (Yao et al., 2007; Zhang et al., 2017). It is generally based on a separate validation set (Goodfellow et al., 2016). While monitoring the validation performance during training, the training process stops when the validation error starts to increase. Validation-based early stopping is advantageous because it is easy to implement and can be interpreted directly (Prechelt, 1998).\nIn a scenario where sufficient labeled data are available, the use of a validation set is generally preferred (Goodfellow et al., 2016). However, when only a few labeled data exist, a tradeoff problem\nis encountered (Kann et al., 2019; Choi and Lee, 2021). For example, although the usage of a relatively large validation set enables more reliable estimation, the number of samples for training becomes insufficient. Conversely, if small fractions of the samples are assigned to the validation set, the stopping point becomes ambiguous because the small validation set is not representative enough.\nEarly stopping is more important in a low resource setting because the prediction accuracy fluctuates highly during training. Such high fluctuations render it challenging when to stop the model. One way to mitigate these fluctuations is to use sufficient training data. In this context, training all the available samples would be more effective, and for this purpose, an appropriate stopping point should be determined without validation split. However, this has not been extensively studied. Duvenaud et al. (2016) and Mahsereci et al. (2017) proposed gradient-based stop-methods and applied statistical inference on the training samples. Lee and Chung (2021) suggested the usage of local intrinsic dimensionality (LID) for early stopping. In addition, some studies treat the stopping epoch as a hyperparameter: the stopping epoch is obtained by grid-search or averaging in cross validation (Choi and Lee, 2021). These methods allow the training of all the labeled samples. However, they do not consider the task-related performance metrics (e.g., accuracy) during training, and the LID and gradient-based stop criteria have not been commonly used in natural language processing (NLP). Furthermore, gradient-based stop-criteria depend on the training samples, the size of which may still be small to be representative.\nIn this study, we propose an early stopping method based on unlabeled samples (BUS-stop). We are motivated by the following two considerations: (i) The probabilities of the predicted class label (i.e., the distribution of the prediction confidence) can serve as an indicator for over-fitting\nor under-fitting. (ii) In a better model, the output class distribution is more likely to be closer to the class distribution of the true labels. To incorporate these two assumptions, two stop criteria are proposed, and combined in the BUS-stop method. Our method monitors the prediction results of unlabeled samples during training and utilizes them for determining the stop-criteria. The first proposed stop-criterion is based on confidence similarity (conf-sim). The model stops when the distribution of the prediction confidence of the unlabeled samples is most similar to the reference distribution, which is precalculated on the labeled set with cross-validation. Conf-sim can reflect the long-term trend of the loss curve, and thereby assist in preventing over-training. The second stop criterion is based on the class distribution similarity (class-sim). This criterion stops the model when the predicted class distribution on the unlabeled set is most similar to the pre-estimated distribution. To this end, we present a novel estimation method for the true class distribution, which calibrates the predicted distribution by extrapolation such that it is closer to the true distribution. Class-sim can reflect the short-term trend of the accuracy. Our method requires several retraining steps to obtain the reference distribution for conf-sim and the estimated class distribution for class-sim. The BUSstop method that combines class-sim and conf-sim includes the advantages of both, and thereby performs with better accuracy and loss compared to each (class-sim and conf-sim).\nThe following characteristics of our method contribute to performance improvement. Our method does not require a separate validation set; hence, all the labeled samples can be trained. Training can stop at a more generalized model, using a large unlabeled set. The proposed stop-criteria, conf-sim and class-sim, consider two performance metrics, namely, the loss and accuracy.\nOur contributions are summarized as follows:\n• We propose BUS-stop, an early stopping method, based on unlabeled samples. BUSstop can stop the training at a more generalized model, and the performance is better even than using an additional validation set.\n• Furthermore, we present a calibration method to better estimate the class distribution. This method calibrates the output class distribution to render it closer to the true distribution, improving the class-sim performance.\n• Extensive experiments are conducted on five popular text classification datasets in English. Comparison with several stop-methods demonstrates that the proposed method outperforms these existing stop-methods in both balanced and imbalanced data settings."
    }, {
      "heading" : "2 Related Work",
      "text" : "Prechelt (1998) experimented on 14 different validation-based stop criteria. Prechelt (1998) focused on an issue that the validation error during training may represent many local minima prior to a global optimum.\nExisting non-validation stop-criteria are generally based on statistical inference. Duvenaud et al. (2016) interpreted stochastic gradient descent in terms of the variational inference and proposed an estimation method for the marginal likelihood of the posterior, which was applied as an early stopping criterion. However, this method requires considerable computation for the Hessian, which is not practical in large models. Mahsereci et al. (2017) also proposed a gradient-related stopping method referred to as evidence-based stopping (EB). The EB-criterion is based on the fast-to-compute local statistics of the computed gradients. The criterion represents whether the gradients of the training samples lie within the expected range. Intrinsic dimensionality (ID), which refers to the minimum number of parameters required to represent a dataset, has been used for analyzing the training or redundancy of neural networks (Amsaleg et al., 2015). LID is a version of ID that estimates the subspace dimensions of the local regions. Lee and Chung (2021) found that LID works well as a stopping-criterion in several few-shot image classification datasets. Moreover, LID can be applied to unlabeled samples. Another method involves the pre-estimation of the the number of training epochs by training the model multiple times, such as cross validation (Choi and Lee, 2021); the model can stop at the pre-estimated (PE) stop-epoch when training all the labeled samples.\nHowever, these methods have not been commonly studied for NLP tasks and do not consider the performance metrics during training. Furthermore, comparisons among the non-validation stopmethods have not been reported. In this study, we compare our method with the EB, LID, PE, and validation-based stopping methods on five text classification datasets. The method proposed by\nAlgorithm 1 Preliminary stage for BUS-stop Input: Labeled set Dl, Unlabeled set Du Output: Sorted output probabilities ~Pl,\nCalibrated class distribution ~Cu Let Count[1 · · ·nl] = 0 Let Pl[1 · · ·nl] = 0 for t ∈ {1, · · · , T} do\nInitialize a model, M Split Dl into Dtrain and Dval at a ratio of r Train the M with (Dtrain, Dval) M ← load the M that was the best on Dval for i ∈ Dval do\npi ←M(xi) Pl[i] = Pl[i] + pi Count[i] = Count[i] + 1\nend for Ĉu ←M(Du) Ĉval, Accval ←M(Dval) ~Ctu = Calibration(Ĉu, Ĉval, Accval)\nend for for i ∈ Dl do\nPl[i] = Pl[i]/Count[i] end for ~Pl ← sort Pl in ascending (or descending) order ~Cu = ∑T t=1 ~Ctu/T return ~Pl, ~Cu\nDuvenaud et al. (2016) was not compared because it involves considerable computational cost."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we describe the proposed method in detail. The main notations used are as follows: Dl = {(xi, yi)}nli=1 and Du = {(xi)} nu i=1 denote the labeled and unlabeled sets, respectively. xi and yi are the i-th sample and its true label, respectively, and nl and nu are the numbers of labeled and unlabeled samples, respectively. pij denotes the prediction probability of the j-th class on the ith sample. LetC be the true class distribution of the samples. The output probability (i.e., confidence) pi associated with the predicted label on sample xi and the predicted (i.e., output) class distribution Ĉ of the samples are defined as follows:\npi =max j (pij)\nĈ[j] = ndata∑ i=1 pij/ndata\nwhere ∀j∈{1,· · ·, nc}; nc is the number of classes."
    }, {
      "heading" : "3.1 Preliminary Stage",
      "text" : "The pseudocode for the preliminary stage is summarized in Alg. 1. In the preliminary stage, the distribution of the prediction confidence ~Pl of the labeled setDl and the estimated class distribution ~Cu of the unlabeled set Du are calculated. Using Dl, the model is reinitialized-and-retrained T -times using a resampling method such as cross-validation. In low-resource settings, such retraining enables more reliable predictions by averaging the results. Each sample in ~Pl is evaluated when the validation loss is the lowest. Each sample in Dl should be validated at least once; the prediction confidence is averaged for each sample. When retraining T - times, the output class distribution of the unlabeled set Du is obtained and calibrated (this calibration is defined in Section 3.3). Then, the T calibrated class distributions are averaged, resulting in ~Cu. After this stage, ~Pl and ~Cu are used to calculate the distribution similarities for the two stop criteria, conf-sim and class-sim, respectively."
    }, {
      "heading" : "3.2 Main Stage Applying BUS-stop",
      "text" : "After the preliminary stage, we train all the labeled samples and refer to this stage as the main stage for convenience. The combined BUS-stop method applied in the main stage is summarized in Alg. 2. The unlabeled set is predicted at every epoch during training.\nConf-sim The first proposed stop criterion confsim Sconf represents the similarity of the distribution of the prediction confidence ~Pu on the unlabeled set with the reference distribution ~Pl. To calculate the similarity between these two distributions, their dimensions must be the same. We sample ~Pu at regular intervals such that it is the same size as ~Pl and denoted it as ... P u. We use the Euclidean distance to calculate the similarity, resulting in Sconf . Then, the first stop criterion for training is when Sconf has the lowest value, i.e.,... P u is most similar to ~Pl. There is a natural concern that ... P u is likely to produce higher (thus dissimilar) confidence than ~Pl because ... P u is obtained by training all the labeled samples, unlike ~Pl. However, the fact that the confidence for each sample in ~Pl is obtained when the validation error is the lowest can alleviate this concern. Thereby, Sconf can be a rough criterion for avoiding under- and over-fitting, and can reflect the trend of the loss because it is based on the reference distribution that shows the best validation loss in the preliminary stage.\nAlgorithm 2 BUS-stop in main stage\nInput: Dl, Du, ~Pl, ~Cu Output: Expected best model Mbest\nLet Queue[1 · · ·nque] = 0 Let Bconf = inf , and npat = 0 Initialize a model, M for epoch ∈ {1, 2, 3, · · · } do\nTrain the M one epoch on Dl Pu, Ĉu ←M(Du) ~Pu ← sortPu in ascending (or descending) order... P u ← sampling ~Pu at regular intervals Sconf = Euclidia-distance( ... P u, ~Pl) Sclass = Cosine-similarity(Ĉu, ~Cu) if Sconf < Bconf then\nnpat = 0 Bconf = Sconf\nelse npat = npat + 1 end if if npat < nque then\nif Sclass > max(Queue) then Mbest ← save the current M end if Queue\ndequeue &←−−−−−− enqueue Sclass\nelse End training\nend if end for return Mbest\nClass-sim The second proposed stop criterion is class-sim, Sclass. The predicted class distribution Ĉu on the unlabeled set is compared with the estimated class distribution ~Cu from the preliminary stage. The assumption is that a well-trained model can also predict the class distribution more accurately. Therefore, estimation of the true class distribution is crucial. A calibration method that facilitates better estimation of the class distribution is presented in Section 3.3. We use the cosine similarity to calculate the similarity between Ĉu and ~Cu, and obtain Sclass. The second stop criterion is when Sclass has the highest value, i.e., Ĉu is most similar to ~Cu. Thereby, Sclass can reflect the short-term trend of the accuracy because it is more likely that the outputs of a higher accuracy model are closer to the true class distribution.\nBUS-stop Finally, we combine the two stopcriteria, conf-sim and class-sim, to form the BUSstop method, as depicted in Alg. 2. A simple\nproduct of the two stop criteria can be an ineffective stop criterion because the sizes of Sconf and Sclass are relative. Our combined stop-criterion is to save the model with the highest Sclass among of the epochs from the lowest Sconf to the subsequent (nque−1)-th epoch. This technique enables fine-stopping by considering both Sconf and Sclass, which reflect the long-term and short-term performances, respectively. It is to be noted that early stopping methods should be operated as an ongoing process, and not as a type of post-hoc method. To this end, we use a fixed-size queue Queue, and its size nque as a hyperparameter, as shown in Alg. 2."
    }, {
      "heading" : "3.3 Calibration of Class Distribution",
      "text" : "In this section, we describe the calibration of the predicted class distribution. The calibration method aims to better estimate the true class distribution of the unlabeled set, thereby improving the performance of class-sim, particularly for imbalanced classification.\nTrained neural networks often involve sampling biases. For example, in binary classification, the prediction results of a model trained with a class ratio a:b tend to follow the distribution of a:b. Thus, when the class distributions are different in the test and training sets, the model performance can deteriorate. Let us suppose the following somewhat ideal and naive situations. Let Cu be the true class distribution of the unlabeled set. If the model is perfectly trained with an accuracy of 1.0, the output class distribution will be equal to Cu. On the other hand, if the model fails to learn any inference knowledge from training, the model will output the predictions only by sampling bias; i.e., when the accuracy is the same as the random expectation (e.g., 0.5 in binary classification), the output class distribution will be equal to the sampling bias B. Thus, the model accuracy can reflect whether the output class distribution is closer to the sampling bias or the true distribution. In the preliminary stage, we obtained the models’ proxy accuracy and output class distribution as Accval and Ĉu, respectively.\nAssuming that there is an approximate linear relationship, we can define a proportional expression as follows:\n(1−Accmin) : (Accval −Accmin) ≈ (Cu −B) : (Ĉu −B) (1)\nWe rearrange the above expression in terms of Cu:\nCu ≈ B + (1−Accmin)\n(Accval −Accmin) (Ĉu −B) (2)\nThen, we denote the approximation of Cu as ~Cu. Considering the class distribution as a vector, Eq. (2) is a type of extrapolation. B can be defined as the class distribution of Dtrain or the predicted distribution in the validation set, Ĉval, of the preliminary stage. In addition, theAcc can be replaced with F1-score. Fig. 1 illustrates an example of our calibration method."
    }, {
      "heading" : "4 Experimental",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conducted extensive experiments using five text classification datasets. The statistics are summarized in Table 1. These datasets have been extensively used in NLP research, and are publicly available. The SST-2 (Socher et al., 2013), IMDB (Maas et al., 2011), and Elec (McAuley and Leskovec, 2013) datasets are used for sentiment analysis. SST-2 and IMDB include movie reviews, and Elec includes reviews on Amazon electronics. AG-news (Zhang et al., 2015) and DBpedia (Zhang et al., 2015) are topic classification tasks for Wikipedia and news articles, respectively. For each dataset, we sampled K labeled samples per class from the training set. K was set to 50 for low-resource settings; we also experimented by varying K ∈ {50, 100, 200, 400, 800, 1600}. We used the test samples as the unlabeled set for each dataset, which is referred to as transductive setting in few-shot classification (Liu et al., 2019)."
    }, {
      "heading" : "4.2 Methods for Comparison",
      "text" : "In this section, we describe the various stop-criteria for comparison with our method.\nEB The EB (Mahsereci et al., 2017) is a criterion based on gradients of training samples. The EBcriterion stops when the following condition is met:\n1− |S| D D∑ k=1 [ (∇LS,k)2 Σ̂k ] > 0 (3)\nwhere S represents a sample set, D is the number of parameters, ∇L is the gradients of loss, and subscript k indicates the k-th weight of the total parameters. Σ̂ is the variance estimator, which is calculated as follows:\nΣ̂k = 1 (|S| − 1) ∑ x∈S (∇lk(x)−∇LS,k)2 (4)\nwhere∇l(x) is the loss gradient on sample x. Note that LS = 1|S| ∑ x∈S l(x). For further details, refer Mahsereci et al. (2017). LID Lee and Chung (2021) approximated LID as follows:\nLID = − ∑ x∈Du [ 1 m m∑ i=1 ln di(~z(x)) dm(~z(x)) ]−1 (5)\nwhere ~z(x) is the representation vector of sample x, and di is the Euclidean distance of ~z(x) and its i-th nearest neighbor. m is a hyperparameter, which denotes the number of nearest neighbors. The lowest LID is the stop criterion.\nVal-stopsplit(x) and Val-stopadd(x) Val-stop denotes validation-based stopping. Val-stopsplit(x) indicates that x validation samples per class are taken from the labeled set. Therefore, K−x samples are trained and x samples are validated for each class. Val-stopadd(x) indicates that x additional samples per class are used for validation; i.e., Val-stopadd(x) uses a total of K+x labeled samples per class. Valstopadd(x) has an unfair advantage because it uses additional labeled samples.\nPE-stop-epoch The stopping epoch is preestimated with cross-validation, as described in Section 2. We use four-fold cross-validation.\nConf-sim and class-sim can also be used as a single stop criterion, as previously mentioned. We compare the single stop criteria described above with the combined BUS-stop criterion. Conf-sim stops when Sconf is the lowest, and class-sim stops when Sclass is the highest."
    }, {
      "heading" : "4.3 Implementation",
      "text" : "BERT-base (Devlin et al., 2019) was adopted as our text encoder. The Adam optimizer (Kingma and Ba, 2015) was applied for categorical crossentropy loss, and its learning rate was set to 3e-5. The dropout (Srivastava et al., 2014) was set to 0.2, and the batch size was 16. All the stop-criteria were evaluated simultaneously for each run to reduce the variance of the estimation. We averaged 10 results in all the experiments. In EB, 64 random training samples were used for S in Eq. (3). In LID, the final vector of the [CLS] token in the BERT model was assigned to ~z(x) in Eq. (5), and the best m was selected from {5, 10, 20, 50, 100}. In BUSstop, nque in Alg. 2 was set to five. Note that K is the number of training samples per class. When K was set to 50, T and r in the preliminary stage (see Alg. 1) were set to 5 and 1:1, respectively. When K was set above 50, T and r were set to 4 and 3:1, respectively. In our calibration method, we used B as Ĉval and Accval as the F1-score (macro)."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Balanced Classification",
      "text" : "Table 2 shows the results when K=50 for training. It is noted that the original test sets have a balanced class distribution. We also report the loss measure as well as accuracy because loss can imply over-training. As shown in Table 2, our BUSstop method exhibits the best performance on an average, and the accuracy is better even than Valstopadd(25), which uses a larger numbers of labeled samples. Note that Val-stopadd(25) uses a total of 75 labeled samples per class. The performance of Val-stopsplit(25) indicates that splitting data for validation can result in poor performance in lowresource settings. LID underperforms compared to the PE-stop-epoch that does not require unlabeled samples. Conf-sim shows the second-best loss on an average. Class-sim underperforms as a stop criterion by itself. However, the BUS-stop method, which combines these two methods, shows better performance than each one on an average. Figure 2 displays the results of conf-sim and class-sim over the epochs. More examples are presented in Appendix A. In Fig. 2, the conf-sim curve is similar to the long-term trend of the loss; however, it does\nnot accurately reflect the short-term fluctuation of the performance from epochs 7–16. On the other hand, class-sim is well responsive to the short-term fluctuation of the accuracy, but does not reflect the long-term trend. BUS-stop, which is a combination of these two methods, takes advantage of the short- as well as long-term methods, and thereby facilitates fine stopping. The EB-criterion results in the second-best accuracy but has the secondhighest loss on an average. The accuracy and loss show conflicting results. That was due to overconfidence on the misclassified samples, caused by over-training. Note that Loss = − ∑ yi log pi. Overconfidence on the wrong label makes pi close to zero on its true label yi. Thus, excessively low pi can increase the loss drastically. Table 3 lists the over-confidence error (OE); the equation for OE is presented in Thulasidasan et al. (2019). This confidence error can be detrimental in various applications, as described by Guo et al. (2017)."
    }, {
      "heading" : "5.2 Imbalanced Classification",
      "text" : "We experimented with an imbalanced setting in binary classification tasks. For testing, we sampled 1,000 instances in the SST-2 test set, and 10,000 instances each in the IMDB and Elec test sets, with a class distribution of 2:8 (negative:positive). Table 4 shows the results when K was set to 50 for training. In most cases, BUS-stop exhibits the best performance with respect to the accuracy as well as loss. In addition, it is noted that BUS-stop outperforms the other methods with a greater margin in an imbalanced setting than in a balanced one (Table 2). Class-sim shows the best or second-best accuracy among the datasets. It is observed that the output class distribution can be an important indicator for a better model.\nTable 5 shows the results in various imbalanced settings of the SST-2 (both the training and test sets are imbalanced). The number of training samples was fixed to 100 for the different class-distribution settings. In general, when the class distributions of the training and test sets are similar, the results shows better performance for all the three methods, EB, BUS-stop, and Val-stopadd(25). In most cases, BUS-stop consistently outperforms Val-stopadd(25) and EB, and the margin is greater when the class distributions are more different between the training and test sets. This result indicates that BUSstop is robust to imbalanced classification."
    }, {
      "heading" : "6 Discussion",
      "text" : "Impact of the training size Figure 3 indicates the accuracy curve with respect to the training size, using the IMDB dataset. The x values of Valstopadd(x) and Val-stopsplit(x) were set to 25, 25, 50, 100, 200, and 400, according to the increase in K. It can be observed that the performance\nof BUS-stop is good in the sufficient-data regime as well. However, the performances of the three stop-criteria converge almost similarly with the increase in the training size. The impact of splitting the samples for validation does not deteriorate the performance when K is greater than 400. Rather, Val-stopsplit(x) performs slightly better when K is 1600. This result suggests that when sufficient labeled data are available, validation-based stopping can be a better choice.\nCalibration performance In the BUS-stop method, accurate estimation of the class distribution plays a crucial role. The cosine similarity between the class distribution of the test set and the estimated distributions by various estimators are shown in Table 6, where the uncalibrated output distributions (Ĉu) and the estimated distributions by the calibration methods, based on the Acc-score (CaliAcc) and macro F1-score (CaliF1), were compared. When the class distributions are similar between the test and training sets, the performance of Ĉu is slightly better than those of the other estimators. However, the estimation by calibration\nbased on the F1-score (CaliF1) is better on an average, and particularly when the class distributions of the test and training sets are different. Figure 4 indicates the BUS-stop accuracies when each model stops based on the estimated class distribution in Table 6 (the same color corresponds to one cell in Table 6). For example, the yellow colors correspond to the settings in which the class distribution is 2:8 and 8:2 in the training and test sets, respectively. As shown in Fig. 4, the better the class distribution is estimated, the higher is the accuracy of BUS-stop. Such high correlation indicates the importance of the class distribution estimator. This result is consistent with our assumption that the output class distribution of better models will be closer to the true distribution."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Validation-based early stopping can be detrimental in low-resource settings because the reduction in the number of samples by validation split may result in insufficient samples for training. In this study, we proposed an early stopping method called BUS-stop, based on unlabeled samples. Moreover, we proposed a calibration method to better estimate the true class distribution, which was used in the BUS-stop method to improve the performance. We conducted experiments on five popular text classification datasets. The results indicated that BUS-stop outperformed the existing stop-criteria in both balanced and imbalanced settings. In particular, BUSstop showed robustness to imbalanced classification. The proposed BUS-stop method enables the training of all the available samples and presents a better stopping point using large unlabeled samples. In future, we plan to better exploit the unlabeled samples in self-training schemes."
    }, {
      "heading" : "A Appendix",
      "text" : "Fig. 5 provides several examples of the learning curves and the stop-criteria measurements over the epochs.\ndenotes conf-sim and class-sim, respectively; and denotes the test loss and accuracy, respectively. The red vertical line denotes the best model selected by the BUS-stop method. The balanced and imbalanced settings are the same as the settings in Section 5.1 and 5.2, respectively. The loss and conf-sim were scaled between 0.25- 0.75 for easy comparison. The BUS-stop enables fine-stopping. As shown in these figures, our method skillfully avoids the points where the performance is decreased by fluctuations."
    } ],
    "references" : [ {
      "title" : "Estimating local intrinsic dimensionality",
      "author" : [ "Laurent Amsaleg", "Oussama Chelly", "Teddy Furon", "Stéphane Girard", "Michael E. Houle", "Ken-ichi Kawarabayashi", "Michael Nett." ],
      "venue" : "Proceedings of the 21th ACM SIGKDD International Conference on",
      "citeRegEx" : "Amsaleg et al\\.,? 2015",
      "shortCiteRegEx" : "Amsaleg et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting all samples in low-resource sentence classification: early stopping and initialization parameters",
      "author" : [ "HongSeok Choi", "Hyunju Lee." ],
      "venue" : "arXiv preprint arXiv:2111.06971.",
      "citeRegEx" : "Choi and Lee.,? 2021",
      "shortCiteRegEx" : "Choi and Lee.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Early stopping as nonparametric variational inference",
      "author" : [ "David Duvenaud", "Dougal Maclaurin", "Ryan Adams." ],
      "venue" : "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Duvenaud et al\\.,? 2016",
      "shortCiteRegEx" : "Duvenaud et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep learning",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Goodfellow et al\\.,? 2016",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2016
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pages 1321––1330. JMLR.org.",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards realistic practices in lowresource natural language processing: The development set",
      "author" : [ "Katharina Kann", "Kyunghyun Cho", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Kann et al\\.,? 2019",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Unsupervised embedding adaptation via early-stage feature reconstruction for few-shot classification",
      "author" : [ "Dong Hoon Lee", "Sae-Young Chung." ],
      "venue" : "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings",
      "citeRegEx" : "Lee and Chung.,? 2021",
      "shortCiteRegEx" : "Lee and Chung.",
      "year" : 2021
    }, {
      "title" : "Learning to propagate labels: Transductive propagation network for few-shot learning",
      "author" : [ "Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sungju Hwang", "Yi Yang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Early stopping without a validation set",
      "author" : [ "Maren Mahsereci", "Lukas Balles", "Christoph Lassner", "Philipp Hennig." ],
      "venue" : "arXiv preprint arXiv:1703.09580.",
      "citeRegEx" : "Mahsereci et al\\.,? 2017",
      "shortCiteRegEx" : "Mahsereci et al\\.",
      "year" : 2017
    }, {
      "title" : "Hidden factors and hidden topics: Understanding rating dimensions with review text",
      "author" : [ "Julian McAuley", "Jure Leskovec." ],
      "venue" : "Proceedings of the 7th ACM Conference on Recommender Systems, RecSys ’13, page 165–172, New York, NY, USA. Associa-",
      "citeRegEx" : "McAuley and Leskovec.,? 2013",
      "shortCiteRegEx" : "McAuley and Leskovec.",
      "year" : 2013
    }, {
      "title" : "Early Stopping - But When?, pages 55–69",
      "author" : [ "Lutz Prechelt." ],
      "venue" : "Springer Berlin Heidelberg, Berlin, Heidelberg.",
      "citeRegEx" : "Prechelt.,? 1998",
      "shortCiteRegEx" : "Prechelt.",
      "year" : 1998
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The journal of machine learning research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "On mixup training: Improved calibration and predictive uncertainty for deep neural networks",
      "author" : [ "Sunil Thulasidasan", "Gopinath Chennupati", "Jeff A Bilmes", "Tanmoy Bhattacharya", "Sarah Michalak." ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Thulasidasan et al\\.,? 2019",
      "shortCiteRegEx" : "Thulasidasan et al\\.",
      "year" : 2019
    }, {
      "title" : "On early stopping in gradient descent learning",
      "author" : [ "Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto." ],
      "venue" : "Constructive Approximation, 26(2):289–315.",
      "citeRegEx" : "Yao et al\\.,? 2007",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2007
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Early stopping, a form of regularization, is a widely used technique to prevent a model from over-fitting (Yao et al., 2007; Zhang et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "Early stopping, a form of regularization, is a widely used technique to prevent a model from over-fitting (Yao et al., 2007; Zhang et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "It is generally based on a separate validation set (Goodfellow et al., 2016).",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "Validation-based early stopping is advantageous because it is easy to implement and can be interpreted directly (Prechelt, 1998).",
      "startOffset" : 112,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "In a scenario where sufficient labeled data are available, the use of a validation set is generally preferred (Goodfellow et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "However, when only a few labeled data exist, a tradeoff problem is encountered (Kann et al., 2019; Choi and Lee, 2021).",
      "startOffset" : 79,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "However, when only a few labeled data exist, a tradeoff problem is encountered (Kann et al., 2019; Choi and Lee, 2021).",
      "startOffset" : 79,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "hyperparameter: the stopping epoch is obtained by grid-search or averaging in cross validation (Choi and Lee, 2021).",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "ing or redundancy of neural networks (Amsaleg et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "Another method involves the pre-estimation of the the number of training epochs by training the model multiple times, such as cross validation (Choi and Lee, 2021); the model can stop at the pre-estimated (PE) stop-epoch when training all the labeled samples.",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : ", 2013), IMDB (Maas et al., 2011), and Elec (McAuley and Leskovec, 2013) datasets are used for sentiment analysis.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : ", 2011), and Elec (McAuley and Leskovec, 2013) datasets are used for sentiment analysis.",
      "startOffset" : 18,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : ", 2015) and DBpedia (Zhang et al., 2015) are topic classification tasks for Wikipedia and news articles, respectively.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "We used the test samples as the unlabeled set for each dataset, which is referred to as transductive setting in few-shot classification (Liu et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : "BERT-base (Devlin et al., 2019) was adopted as our text encoder.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "The Adam optimizer (Kingma and Ba, 2015) was applied for categorical crossentropy loss, and its learning rate was set to 3e-5.",
      "startOffset" : 19,
      "endOffset" : 40
    } ],
    "year" : 0,
    "abstractText" : "Early stopping, which is widely used to prevent overfitting, is generally based on a separate validation set. However, in low resource settings, validation-based stopping can be risky because a small validation set may not be sufficiently representative, and the reduction in the number of samples by validation split may result in insufficient samples for training. In this study, we propose an early stopping method that uses unlabeled samples. The proposed method is based on confidence and class distribution similarities. To further improve the performance, we present a calibration method to better estimate the class distribution of the unlabeled samples. The proposed method is advantageous because it does not require a separate validation set and provides a better stopping point by using a large unlabeled set. Extensive experiments are conducted on five text classification datasets and several stop-methods are compared. Experimental results show that the proposed model even performs better than using an additional validation set as well as the existing stopmethods, in both balanced and imbalanced data settings. Our code is available at https: //activated/after/accept.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_33_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The PubMed1 database is a resource that provides access to the MEDLINE bibliographic database of references and abstracts together with the full text articles of some of these citations which are available in the PubMed Central2 (PMC) repository. MEDLINE3 contains more than 28 million references (as of Feb. 2021) to journal articles in the biomedical, health, and related disciplines. Journal articles in MEDLINE are indexed according to Medical Subject Headings (MeSH)4, an hierarchically organized vocabulary that has been developed and maintained by the National Library of Medicine (NLM)5. Currently, there are 29,369 main MeSH headings, and each MEDLINE citation\n1https://pubmed.ncbi.nlm.nih.gov/about/ 2https://en.wikipedia.org/wiki/PubMed_Central 3https://www.nlm.nih.gov/medline/medline_overview.\nhtml 4https://www.nlm.nih.gov/mesh/meshhome.html 5https://www.nlm.nih.gov\nhas 13 MeSH indices, on average. MeSH terms are distinctive features of MEDLINE and can be used in many applications in biomedical text mining and information retrieval (Lu et al., 2008; Huang et al., 2011; Gu et al., 2013), being recognized as important tools for research (e.g., knowledge discovery and hypothesis generation).\nCurrently, MeSH indexing is done by human annotators who examine full articles and assign MeSH terms to each article according to rules set by NLM6. Human annotation is time consuming and costly – the average cost of annotating one article in MEDLINE is about $9.40 (Mork et al., 2013). Nearly 1 million citations were added to MEDLINE in 2020 (approximately 2,600 on a daily basis)7. The rate of articles being added to the MEDLINE database is constantly increasing, so there is a huge financial and time-consuming cost for the status quo. Therefore, it is imperative to develop an automatic annotation system that can assist MeSH indexing of large-scale biomedical articles efficiently and accurately.\nAutomatic MeSH indexing can be regarded as an extreme multi-label text classification (XMC) problem, where each article can be labeled with multiple MeSH terms. Compared with standard multilabel problems, XMC finds relevant labels from an enormous set of candidate labels. The challenge of large-scale MeSH indexing comes from both the label and article sides. Currently, there are more than 29,000 distinct MeSH terms, and new MeSH terms are updated to the vocabulary every year. The frequency of different MeSH terms appearing in documents are quite imbalanced. For instance, the most frequent MeSH term, ‘humans’, appears in more than 8 million citations; ‘Pandanaceae’, on the other hand, appears in only 31 documents (Zhai\n6https://www.nlm.nih.gov/bsd/indexing/training/TIP_ 010.html\n7https : / / www. nlm . nih . gov / bsd / medline _ pubmed _ production_stats.html\net al., 2015). In addition, the MeSH terms that have been assigned to each article varies greatly, ranging from more than 30 to fewer than 5. Furthermore, semantic features of the biomedical literature are complicated to capture, as they contain many domain-specific concepts, phrases, and abbreviations. The aforementioned difficulties make the task more complicated to generate an effective and efficient prediction model for MeSH indexing.\nIn this work, inspired by the rapid development of deep learning, we propose a novel neural architecture called KenMeSH (Knowledge-enhanced MeSH labelling) which is suitable for handling XMC problems where the labels are arrayed hierarchically and could capture useful information as a directed graph. Our method uses a dynamic knowledge-enhanced mask attention mechanism and incorporates document features together with label features to index biomedical articles. Our major contributions are:\n1. We design a multi-channel document representation module to extract document features from the title and the abstract using a bidirectional LSTM. We use multi-level dilated convolution to capture semantic units in the abstract channel. This module combines a hybrid of information, at the levels of words and latent semantics, to capture local correlations and long-term dependencies from text. 2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate MeSH hierarchical information to map label representations. 3. We propose a novel dynamic knowledgeenhanced mask attention mechanism which incorporates external journal-MeSH cooccurrence information and document similarity in the PubMed database to constrain the large universe of possible labels in the MeSH indexing task. 4. We evaluate our model on a corpus of PMC articles. Our proposed method consistently achieves superior performance over previous approaches on a number of measures."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Automatic MeSH Indexing",
      "text" : "To address the MeSH indexing task mentioned in above section, the National Library of Medicine developed Medical Text Indexer (MTI) – software\nthat automatically recommends MeSH terms to each MEDLINE article using the abstract and title as input (Aronson et al., 2004). It first generates the candidate MeSH terms for given articles, and then ranks the candidates to provide the final predictions. There are two modules in MTI – MetaMap Indexing (MMI) and PubMed-Related Citations (PRC) (Lin and Wilbur, 2007; Aronson and Lang, 2010). MetaMap is NLM-developed software which extracts the biomedical concepts in the documents and maps them to Unified Medical Language System concepts. MMI recommends MeSH terms using the biomedical concepts discovered by MetaMap. PRC uses k-nearest neighbours to find the MeSH annotations of similar citations in MEDLINE. The two mentioned sets of MeSH terms combine the final MeSH recommendations from MTI.\nBioASQ8, an EU-funded project, has organized challenges on automatic MeSH indexing since 2013, which provides opportunities to involve more participants in continuing to the development of MeSH indexing systems. Many effective MeSH indexing systems have been developed since then, such as MeSHLabeler (Liu et al., 2015), DeepMeSH (Peng et al., 2016), AttentionMeSH (Jin et al., 2018), and MeSHProbeNet (Xun et al., 2019). MeSHLabeler introduced a Learning-toRank (LTR) framework, which is a two-step strategy, first predicting the candidate MeSH terms and then ranking them to obtain the final suggestions. MeSHLabeler first trained an independent binary classifier for each MeSH term and then used various evidence, including similar publications and term frequencies, to rank candidate MeSH terms. DeepMeSH is an improved version of MeSHLabeler, which also uses the LTR strategy. It first generates MeSH predictions by incorporating deep semantics in the word embedding space, and then ranks the candidates. AttentionMeSH and MeSHProbeNet are based on bidirectional recurrent neural networks (RNNs) and attention mechanisms. The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of biomedical information from the input articles.\nStudies in MeSH indexing with full texts are very limited because of restrictions on full text ac-\n8http://bioasq.org\ncess. Jimeno-Yepes et al. (2013) randomly selected 1413 articles from the PMC Open Access Subset and used automatically-generated summaries from these full texts as input to MTI for MeSH indexing. Demner-Fushman and Mork (2015) collected 14,828 full text articles from PMC Open Access Subset and developed a rule-based string-matching algorithm to extract a subject of MeSH terms called ‘check tags’ that are used to describe the characteristics of the subjects. Wang and Mercer (2019) randomly selected 257,590 full text articles from PMC Open Access Subset and developed a multichannel model using CNN-based feature selection to extract important information from different sections of the articles. HGCN4MeSH (Yu et al., 2020) used the PMC dataset generated by Wang and Mercer (2019) and employed graph convolutional neural network to learn the co-occurrences between MeSH terms. FullMeSH (Dai et al., 2019) and BERTMeSH (You et al., 2020) used all available full text articles in PMC Open Access Subset. FullMeSH applied an attention-based CNN to predict the MeSH terms and LTR to get the final MeSH candidates; BERTMeSH incorporated pre-trained BERT and an attention mechanism to improve the performance of MeSH indexing."
    }, {
      "heading" : "2.2 Graph Convolutional Network in Text Classification",
      "text" : "Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention recently. Some text classification systems introduce GCN by formulating their problems as graph-structural tasks. For instance, TextGCN (Yao et al., 2019) built a single text graph for a corpus based on word co-occurrence and document word relations to infer labels. Zhang et al. (2019a) built a GCN-based dependency tree of a sentence to exploit syntactical information and word dependencies for sentiment analysis. Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multilabel text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels."
    }, {
      "heading" : "3 Proposed Model",
      "text" : "MeSH indexing can be regarded as a multi-label text classification problem in which, given a set of biomedical documents X = {x1, x2, ..., xn} and a set of MeSH labels Y = {y1, y2, ..., yL},\nmulti-label classification learns the function f : X → [0, 1]Y using the training set D = (xi, Yi), i = 1, ..., n, where n is the number of documents in the set.\nFigure 1 illustrates our overall architecture. Our model is composed of a multi-channel document representation module, a label features learning module, a dynamic semantic mask attention module, and a classifier."
    }, {
      "heading" : "3.1 Multi-channel Document Representation Module",
      "text" : "The multi-channel document representation module has two input channels – the title channel and the abstract channel, for each type of text. These two texts are represented by two embedding matrices, namely Etitle ∈ Rd, the word embedding matrix for the title, and Eabstract ∈ Rd, the word embedding matrix for the abstract. We first apply a bidirectional Long Short-Term Memory (biLSTM) network (Hochreiter and Schmidhuber, 1997) in both channels to encode the two types of text and to generate the hidden representations ht for each word at time step t. The computations of\n−→ ht and←− ht are illustrated below:\n−→ ht = LSTM(xt, −−→ ht−1, ct−1) ←− ht = LSTM(xt, ←−− ht−1, ct−1)\n(1)\nWe then obtain the final representation for each word by concatenating the hidden states from both directions, namely ht = [ −→ ht : ←− ht ] and ht ∈ Rl×2dh , where l is the number of words in the text and dh is the hidden dimensions. The biLSTM returns context-aware representations Htitle and Habstract for the title and abstract channels, respectively:\nHtitle = biLSTM(Etitle)\nHabstract = biLSTM(Eabstract) (2)\nIn order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel. The concept of dilated convolution was originally developed for wavelet decomposition (Holschneider et al., 1990), and has been applied to NLP tasks such as neural machine translation (Kalchbrenner et al., 2017) and text classification (Lin et al., 2018). The main idea of DCNN is to insert ‘holes’ in convolutional kernels, which extract the longer-term dependencies and generate higher-level representations, such as phases and\nsentences. Following Lin et al. (2018), we apply a multi-level DCNN with different dilation rates on top of the hidden representations generated by the biLSTM on the abstract channel. Small dilation rates capture phrase-level information, and large ones capture sentence-level information. The DCNN returns the semantic features of the abstract channel Dabstract ∈ R(l−s+1)×2dh , where s is the width of the convolution kernels."
    }, {
      "heading" : "3.2 Label Features Learning Module",
      "text" : "As the MeSH hierarchy is important to our task, we use a two-layer GCN to incorporate the hierarchical parent and child information among labels. We first use the MeSH descriptors to generate a label feature vector for each MeSH term. Each label vector is calculated by averaging the word embedding of each word in its descriptors:\nvi = 1\nN ∑ j∈N wj , i = 1, 2, ..., L, (3)\nwhere vi ∈ Rd, N is the number of words in its descriptor, and L is the number of labels. In the graph structure, we formulate each node as a MeSH label, and edges are implement MeSH hierarchies.\nThe edge types of a node include edges from its parent, from its children, and from itself. At each GCN layer, the node feature is aggregated by its parent and children to form the new label feature for the next layer:\nhl+1 = σ(A · hl ·W l), (4)\nwhere hl and hl+1 ∈ RL×d indicate the node presentation of the lth and (l + 1)th layers, σ(·) denotes an activation function, A is the adjacency matrix of the MeSH hierarchical graph, and W l is a layer-specific trainable weight matrix. We then concatenate the label feature vectors from descriptors in Equation 3 with GCN label vectors to form:\nHlabel = v ‖ hl+1, (5)\nwhere Hlabel ∈ RL×2d is the final label vector."
    }, {
      "heading" : "3.3 Dynamic Knowledge-enhanced Mask Attention Module",
      "text" : "In the dynamic knowledge-enhanced mask attention module, we integrate external knowledge from outside sources to generate a unique mask for each article dynamically. We only consider a subset of the full MeSH list and employ a masked label-wise\nattention that computes the element-wise multiplication of a mask matrix and an attention matrix for the following two reasons. First, the MeSH terms are numerous and have widely varying occurrence frequencies. Therefore, for each MeSH label, there are far more negative examples than positive ones. For each article, selecting a subset of MeSH labels, namely a MeSH mask, achieves down-sampling of the negative examples, which forces the classifier to concentrate on the candidate labels. Second, the issue with the original attention mechanism (Bahdanau et al., 2015) is that the classifier focuses on spotting relevant information for all predicted labels, which is a lack of pertinence. Using a masked label-wise attention allows the classifier to find relevant information for each label inside the MeSH mask.\nThe dynamic ensures that the module generates a unique MeSH mask for each article, specifically. To generate the MeSH masks, we consider two external knowledge sources: journal information and document similarity. The journal information refers to the name of the journal in which an article was published, which usually defines a specific research domain. We expect that articles published in the same journal tend to be indexed with MeSH terms that are relevant to the journal’s research focus. We build a co-occurrence matrix between journals and MeSH labels using conditional probabilities, i.e., P (Li | Jj), which denotes the probability of occurrence of label Li when journal Jj appears.\nP (Li | Jj) = CL∩J CJ , (6)\nwhere CL∩J denotes the co-occurring times of Li and Jj , CJ denotes the number of occurrences of Li in the training set. To avoid the noise of rare co-occurrences, we set a threshold τ to filter noisy correlations.\nMj = {Lk|P (Lk|Jj) > τ, k = 1, ..., L}, (7)\nwhere Mj denotes the MeSH mask for journal j. We then use k-nearest neighbors (KNN) to choose a subset of specific MeSH terms for each article by referring to document similarity. We represent each article by the IDF-weighted sum of word embeddings in the abstract:\nDidf = ∑n i=1 IDFi × ei∑n\ni=1 IDFi , (8)\nwhere ei is the word embedding, and IDFi is the inverse document frequency of the word. Next, we\ncalculate the cosine similarity between abstracts and use KNN to find k nearest neighbours for each article. After that, we collect MeSH terms from neighbours and form as Mn.\nM =Mj ∪Mn, (9)\nwhere M ∈ RL, Mi ∈ [0, 1] is the MeSH mask. We calculate the similarity between MeSH terms and the texts in two channels by applying masked label-wise attention.\nHmasked = Hlabel M αtitle = Softmax(Htitle ·Hmasked)\nαabstract = Softmax(Dabstract ·Hmasked), (10)\nwhere denotes element-wise multiplication, Hmasked denotes the masked label features, and αtitle and αabstract measure how informative each text fragment is for each label in the title and abstract channels, respectively. We then generate the label-specific title and abstract representations, respectively:\nctitle = α T title ·Htitle\ncabstract = α T abstract ·Dabstract\n(11)\nsuch that ctitle ∈ RL×2d, and cabstract ∈ RL×2d. We sum up the representations in the title and abstract channels to form the document vector for each article:\nD = ctitle + cabstract (12)"
    }, {
      "heading" : "3.4 Classifier",
      "text" : "We gain scores for each MeSH term i:\nŷi = σ(D Hlabel), i = 1, 2, ..., L, (13)\nwhere σ(·) represents the sigmoid function. We train our model using the multi-label binary crossentropy loss (Nam et al., 2014): L = L∑ i=1 [−yi · log(ŷi)− (1− yi) · log(1− ŷi))], (14) where yi ∈ [0, 1] is the ground truth of label i, and ŷi ∈ [0, 1] denotes the prediction of label i obtained from the proposed model."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We follow Dai et al. (2019) and You et al. (2020) by using the PMC FTP service9 (Comeau et al., 2019)\n9https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/ BioC-PMC\nand downloading PMC Open Access Subset (as of Sep. 2021), totalling 3,601,092 citations. We also download the entire MEDLINE collection based on the PubMed Annual Baseline Repository (as of Dec. 2020) and obtain 31,850,051 citations with titles and abstracts. In order to reduce bias, we only focus on articles that are annotated by human curators (not annotated by a ‘curated’ or ‘auto’ modes in MEDLINE). We then match PMC articles with the citations in PubMed to PMID and obtain a set of 1,284,308 citations. Out of these PMC articles, we use the latest 20,000 articles as the test set, the next latest 200,000 articles as the validation data set, and the remaining 1.24M articles as the training set. In total, 28,415 distinct MeSH terms are covered in the training dataset."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We implement our model in PyTorch (Paszke et al., 2019). For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words are lowercased. Titles longer than 100 char-\nacters and abstracts longer than 400 characters are truncated. We use pre-trained biomedical word embeddings (BioWordVec) (Zhang et al., 2019b), and the embedding dimension is 200. To avoid overfitting, we use dropout directly after the embedding layer with a rate of 0.2. The number of units in hidden layers are 200 in all three modules. We use a three-level dilated convolution with dilation rate [1, 2, 3] and select 1000 nearest documents to generate MeSH masks for each article. We use Adam optimizer (Kingma and Ba, 2015) and early stopping strategies. The learning rate is initialized to 0.0003, and the decay rate is 0.9 in every epoch. The gradient clip is applied to the maximum norm of 5. The batch size is 32 and the model is trained on a single NVIDIA V100 GPU."
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "We use three main evaluation metrics to test the performance of MeSH indexing systems: Microaverage measure (MiM), example-based measure (EBM), and ranking-based measure (RBM), where MiM and EBM are commonly used in MeSH indexing tasks and RBM is commonly used in evaluating multi-label classification. Micro-average Fmeasure (MiF) aggregate the global contributions of all MeSH labels and then calculate the harmonic mean of micro-average precision (MiP) and microaverage recall (MiR), which are heavily influenced by frequent MeSH terms. Example-based measures are computed per data point, which computes the harmonic mean of standard precision (EBP) and recall (EBR) for each data point. In the ranking-based measure, precision at k (P@k) shows the number of relevant MeSH terms that are suggested in the top-k recommendations of the MeSH indexing system, and recall at k (R@k) indicates the proportion of relevant items that are suggested in the top-k recommendations. The detailed computations of evaluation metrics can be found in Appendix A.\nThe threshold has a large influence on MiF and EBF. We select final MeSH labels whose predicted probability is larger than a tuned threshold ti:\nMeSHi = { ŷi ≥ ti, 1 ŷi < ti, 0\n(15)\nwhere ti is the threshold for MeSH term i. We compute optimal threshold for each MeSH term on the validation set following Pillai et al. (2013) that tunes ti by maximizing MiF:\nti = argmax T\nMiF(T), (16)\nwhere T denotes all possible threshold values for label i."
    }, {
      "heading" : "5 Results and Ablation Studies",
      "text" : "We evaluate our proposed model with five state-of-the-art models: MTI, DeepMeSH, FullMeSH, BERTMeSH and HGCN4MeSH. Among these, MTI, DeepMeSH, BERTMeSH, and HGCN4MeSH are trained with abstracts and titles only; FullMeSH (Full) and BERTMeSH (Full) are trained with full PMC articles. Our proposed model is trained on titles and abstracts, and is tested using 20,000 of the latest articles. We mainly focus on MiF, which is the main evaluation metric in MeSH indexing task.\nWe compare our model against previous related systems on micro-average measure and examplebases measure in Table 1. Each row in the table shows all evaluation metrics on a specific method, where the best score for each metric is indicated. As reported, our model achieves the best performance on most evaluation metrics, expect MiR and EBR, on which BERTMeSH (Full) achieves the best performance. This is because that BERTMeSH (Full) is trained on full text articles, which uses much more content information in the articles than ours. Our model outperforms the subset of systems that were trained only on the abstract and the title – MTI, HGCN4MeSH, DeepMeSH and BERTMeSH in all metrics. Most importantly, there is improvement in precision without a decrease in recall. Comparing with systems trained on full articles indicates that our model achieves the best MiF, and is only slightly below BERTMeSH (Full) on MiR (0.4 percentage points). Although our model is trained only on the abstract and title (which may suggest that it can capture less complex semantics) it performs very well against more complex systems. Furthermore, we compared the performance of our model with HGCN4MeSH on ranking-based measures that do not require a specific threshold, the results are summarized in Table 2. As reported, we see that our model always performs better than HGCN4MeSH with up to almost 18% improvement.\nAs the frequency of different MeSH terms are imbalanced, we are interested in examining the efficiency of our model on infrequent MeSH terms. We divide MeSH terms into four groups based on the number of occurrences in the training set: (0, 100), [100, 1000), [1000, 5000), and [5000, ). Figure 2a\nshows the distribution of MeSH terms and percent of occurrence among the four divided groups in the training set, which indicates that the distribution of MeSH frequency is highly biased and it falls into a long-tail distribution. Figure 2b and 2c show the performance of our model comparing to MTI baseline in the four MeSH groups on MiF and EBF respectively. Our model obtains substantial improvements among frequent and infrequent labels on both MiF and EBF.\nWe are interested in studying how the effectiveness and robustness of our model are due to the various modules, such as the multi-channel mechanism, the dilated CNN, the label graph, and masked attention. To further understand the impacts of these factors, we conduct controlled experiments with four different settings: (a) examining a single channel architecture by concatenating the title and abstract as input into the abstract channel; (b) removing the dilated CNN; (c) replacing the label feature learning module with a fully connected layer; and (d) removing the masked attention module. The influence of each of these modules can then be evaluated individually. The results are summarized in Table 3.\nImpacts on Multi-channel Settings As shown in Table 3, the multi-channel setting outperforms the single channel one. The reason for this could be that the single channel model misses some important features in titles and abstracts in the LSTM layer. LSTM has the capability to learn and remember over long sequences of inputs, but it can be challenging to use when facing very long input sequences. Concatenating the title and abstract into one longer sequence may hurt the performance of LSTM. To be more explicit, the single channel model may be remembering insignificant features in the LSTM layer when dealing with longer sequences. Therefore, extracting information from the title and the abstract separately is better than directly concatenating the information.\nImpacts on Dilated Semantic Feature Extractions As reported in Table 3, the performance drops when removing the dilated CNN layer. The reason for this seems to be that multi-level dilated CNNs can extract high-level semantic information from the semantic units that are often wrapped in phrases or sentences, and then capture local correlation together with longer-term dependencies from\nthe text. Compared with word-level information extracted from the biLSTM layer, high-level information extracted from the semantic units seems to provide better understanding of the text, at least for the purposes of labelling.\nImpacts on Learning Label Features As shown in Table 3, not learning the label features has the largest negative impacts on performance especially for recall (and subsequently F-measure). By removing the label features, the model pays more attention to the frequent MeSH terms and misclassifies infrequent labels as negative. This indicates that label features learned through GCN can capture the hierarchical information between MeSH terms, and MeSH indexing for infrequent terms can benefit from this hierarchical information.\nImpacts on Dynamic Knowledge-enhanced Mask Attention Table 3 shows a performance drop when removing the masked attention layer, suggesting that the attention mechanism has positive impacts on performance. This result further suggest that the masked attention takes advantage of incorporating external knowledge to alleviate the extremely large pool of possible labels. To select the proper mask for each article, two hyperparame-\nters are used: threshold τ for journal-MeSH occurrence and the number of nearest articles K. With τ = 0.5 and K = 1000, all of the gold-standard MeSH labels are guaranteed to be in the mask."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose a novel end-to-end model integrating document features and label hierarchical features for MeSH indexing. We use a novel dynamic knowledge-enhanced mask attention mechanism to handle the large universe of candidate MeSH terms and employ GCN in extracting label correlations. Experimental results demonstrate that our proposed model significantly outperforms the baseline models and provides especially large improvements on infrequent MeSH labels.\nIn the future, we believe two important research directions will lead to further improvements. First, we plan to explore full text articles, which contain more information, to see whether our model takes advantage of the full text to improve the performance of large-scale MeSH indexing. Second, we are interested in integrating knowledge from the Unified Medical Language System (UMLS) (Bodenreider, 2004), a comprehensive ontology of biomedical concepts, in our model."
    }, {
      "heading" : "A Evaluation Metrics",
      "text" : "Micro F-measure (MiF) computes the harmonic mean of micro-average precision (MiF) and microaverage recall (MiR):\nMiF = 2×MiR×MiP\nMiR + MiP , (17)\nwhere\nMiP = ∑L j=1 TPj∑L\nj=1 TPj + ∑L j=1 FPj , (18)\nMiR = ∑L j=1 TPj∑L\nj=1 TPj + ∑L j=1 FNj , (19)\nwhere TPj , FPj and FNj as true positives, false positives, and false negatives respectively for each label lj in the set of total labels L.\nEBF can be computed as the harmonic mean of standard precision (EBP) and recall (EBR):\nEBF = 2× EBR× EBP\nEBR + EBP , (20)\nwhere\nEBP = 1\nN N∑ i=1 |yi ∩ ŷi| |ŷi| , (21)\nEBR = 1\nN N∑ i=1 |yi ∩ ŷi| |yi| , (22)\nwhere yi is the true label set and ŷi is the predicted label set for instance i, N represents the total number of instance.\nRanking-based evaluation, including precision at k (P@k), and recall at k (R@k).The metrics are defined as follows:\nP@k = 1\nk ∑ l∈rk(ŷ) yl, (23)\nR@k = 1 |yi| ∑\nl∈rk(ŷ)\nyl, (24)\nwhere rk returns the top-k recommended items."
    } ],
    "references" : [ {
      "title" : "The NLM Indexing Initiative’s Medical Text Indexer",
      "author" : [ "A. Aronson", "James G. Mork", "Clifford W. Gay", "S. Humphrey", "Willie J. Rogers." ],
      "venue" : "Studies in health technology and informatics, 107 Pt 1:268–",
      "citeRegEx" : "Aronson et al\\.,? 2004",
      "shortCiteRegEx" : "Aronson et al\\.",
      "year" : 2004
    }, {
      "title" : "An overview of MetaMap: Historical perspective and recent advances",
      "author" : [ "Alan R Aronson", "François-Michel Lang." ],
      "venue" : "Journal of the American Medical Informatics Association, 17(3):229–236.",
      "citeRegEx" : "Aronson and Lang.,? 2010",
      "shortCiteRegEx" : "Aronson and Lang.",
      "year" : 2010
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR, abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "The unified medical language system (umls): integrating biomedical terminology",
      "author" : [ "Olivier Bodenreider." ],
      "venue" : "Nucleic acids research, 32 Database issue:D267–70.",
      "citeRegEx" : "Bodenreider.,? 2004",
      "shortCiteRegEx" : "Bodenreider.",
      "year" : 2004
    }, {
      "title" : "PMC text mining subset in BioC: about three million full-text articles and growing",
      "author" : [ "Donald C. Comeau", "Chih-Hsuan Wei", "R. Dogan", "Zhiyong Lu." ],
      "venue" : "Bioinformatics.",
      "citeRegEx" : "Comeau et al\\.,? 2019",
      "shortCiteRegEx" : "Comeau et al\\.",
      "year" : 2019
    }, {
      "title" : "FullMeSH: improving large-scale MeSH indexing with full text",
      "author" : [ "Suyang Dai", "Ronghui You", "Zhiyong Lu", "Xiaodi Huang", "Hiroshi Mamitsuka", "Shanfeng Zhu." ],
      "venue" : "Bioinformatics, 36(5):1533– 1541.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Extracting characteristics of the study subjects from full-text articles",
      "author" : [ "Dina Demner-Fushman", "James G. Mork." ],
      "venue" : "Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium, pages 484–491.",
      "citeRegEx" : "Demner.Fushman and Mork.,? 2015",
      "shortCiteRegEx" : "Demner.Fushman and Mork.",
      "year" : 2015
    }, {
      "title" : "Efficient Semisupervised MEDLINE Document Clustering With MeSHSemantic and Global-Content Constraints",
      "author" : [ "Jun Gu", "Wei Feng", "Jia Zeng", "Hiroshi Mamitsuka", "Shanfeng Zhu." ],
      "venue" : "IEEE Transactions on Cybernetics, 43(4):1265–1276.",
      "citeRegEx" : "Gu et al\\.,? 2013",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9:1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A real-time algorithm for signal analysis with the help of the wavelet transform",
      "author" : [ "M. Holschneider", "R. Kronland-Martinet", "J. Morlet", "Ph. Tchamitchian." ],
      "venue" : "Wavelets, pages 286–297, Berlin, Heidelberg. Springer Berlin Heidelberg.",
      "citeRegEx" : "Holschneider et al\\.,? 1990",
      "shortCiteRegEx" : "Holschneider et al\\.",
      "year" : 1990
    }, {
      "title" : "Recommending MeSH terms for annotating biomedical articles",
      "author" : [ "Minlie Huang", "Aurélie Névéol", "Zhiyong Lu." ],
      "venue" : "Journal of the American Medical Informatics Association : JAMIA, 18:660 – 667.",
      "citeRegEx" : "Huang et al\\.,? 2011",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2011
    }, {
      "title" : "Comparison and combination of several MeSH indexing approaches",
      "author" : [ "Antonio Jimeno-Yepes", "James G. Mork", "Dina DemnerFushman", "Alan R. Aronson." ],
      "venue" : "Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium,",
      "citeRegEx" : "Jimeno.Yepes et al\\.,? 2013",
      "shortCiteRegEx" : "Jimeno.Yepes et al\\.",
      "year" : 2013
    }, {
      "title" : "AttentionMeSH: Simple, effective and interpretable automatic MeSH indexer",
      "author" : [ "Qiao Jin", "Bhuwan Dhingra", "William W. Cohen." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BioASQ: Largescale Biomedical Semantic Indexing and Question",
      "citeRegEx" : "Jin et al\\.,? 2018",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation in linear time",
      "author" : [ "Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas Kipf", "Max Welling." ],
      "venue" : "ArXiv, abs/1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "PubMed related articles: A probabilistic topic-based model for content similarity",
      "author" : [ "Jimmy Lin", "W. John Wilbur." ],
      "venue" : "BMC Bioinformatics, 8(1):423.",
      "citeRegEx" : "Lin and Wilbur.,? 2007",
      "shortCiteRegEx" : "Lin and Wilbur.",
      "year" : 2007
    }, {
      "title" : "Semantic-unit-based dilated convolution for multi-label text classification",
      "author" : [ "Junyang Lin", "Qi Su", "Pengcheng Yang", "Shuming Ma", "Xu Sun." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4554–",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "MeSHLabeler: Improving the accuracy of largescale mesh indexing by integrating diverse evidence",
      "author" : [ "Ke Liu", "Shengwen Peng", "Junqiu Wu", "ChengXiang Zhai", "Hiroshi Mamitsuka", "Shanfeng Zhu." ],
      "venue" : "Bioinformatics, 31(12):i339–i347.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluation of query expansion using MeSH in PubMed",
      "author" : [ "Zhiyong Lu", "W. Kim", "W. Wilbur." ],
      "venue" : "Information Retrieval, 12:69–80.",
      "citeRegEx" : "Lu et al\\.,? 2008",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2008
    }, {
      "title" : "The NLM Medical Text Indexer system for indexing biomedical literature",
      "author" : [ "James G. Mork", "Antonio Jimeno-Yepes", "Alan R. Aronson." ],
      "venue" : "Proceedings of the first Workshop on Bio-Medical Semantic Indexing and Question Answering (BioASQ).",
      "citeRegEx" : "Mork et al\\.,? 2013",
      "shortCiteRegEx" : "Mork et al\\.",
      "year" : 2013
    }, {
      "title" : "Largescale multi-label text classification - revisiting neural networks",
      "author" : [ "Jinseok Nam", "Jungi Kim", "Eneldo Loza Mencía", "Iryna Gurevych", "Johannes Fürnkranz." ],
      "venue" : "ArXiv, abs/1312.5419.",
      "citeRegEx" : "Nam et al\\.,? 2014",
      "shortCiteRegEx" : "Nam et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-label text classification using attention-based graph neural network",
      "author" : [ "Ankit Pal", "M. Selvakumar", "Malaikannan Sankarasubbu." ],
      "venue" : "ICAART.",
      "citeRegEx" : "Pal et al\\.,? 2020",
      "shortCiteRegEx" : "Pal et al\\.",
      "year" : 2020
    }, {
      "title" : "PyTorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : null,
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "DeepMeSH: deep semantic representation for improving large-scale MeSH indexing",
      "author" : [ "Shengwen Peng", "Ronghui You", "Hongning Wang", "ChengXiang Zhai", "Hiroshi Mamitsuka", "Shanfeng Zhu." ],
      "venue" : "Bioinformatics, 32(12):i70–i79.",
      "citeRegEx" : "Peng et al\\.,? 2016",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2016
    }, {
      "title" : "Threshold optimisation for multi-label classifiers",
      "author" : [ "Ignazio Pillai", "Giorgio Fumera", "Fabio Roli." ],
      "venue" : "Pattern Recognition, 46(7):2055–2065.",
      "citeRegEx" : "Pillai et al\\.,? 2013",
      "shortCiteRegEx" : "Pillai et al\\.",
      "year" : 2013
    }, {
      "title" : "Incorporating Figure Captions and Descriptive Text in MeSH Term Indexing",
      "author" : [ "Xindi Wang", "Robert E. Mercer." ],
      "venue" : "BioNLP@ACL.",
      "citeRegEx" : "Wang and Mercer.,? 2019",
      "shortCiteRegEx" : "Wang and Mercer.",
      "year" : 2019
    }, {
      "title" : "MeSHProbeNet: A selfattentive probe net for MeSH indexing",
      "author" : [ "Guangxu Xun", "Kishlay Jha", "Ye Yuan", "Yaqing Wang", "Aidong Zhang." ],
      "venue" : "Bioinformatics.",
      "citeRegEx" : "Xun et al\\.,? 2019",
      "shortCiteRegEx" : "Xun et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph convolutional networks for text classification",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Bertmesh: Deep contextual representation learning for large-scale high-performance mesh indexing with full text",
      "author" : [ "R. You", "Yuxuan Liu", "Hiroshi Mamitsuka", "Shanfeng Zhu." ],
      "venue" : "Bioinformatics.",
      "citeRegEx" : "You et al\\.,? 2020",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2020
    }, {
      "title" : "HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing",
      "author" : [ "Miaomiao Yu", "Yujiu Yang", "Chenhui Li." ],
      "venue" : "ACL.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "MeSHLabeler: Improving the accuracy of largescale MeSH indexing by integrating diverse evidence",
      "author" : [ "Chengxiang Zhai", "Hiroshi Mamitsuka", "Junqiu Wu", "Ke Liu", "Shanfeng Zhu", "Shengwen Peng." ],
      "venue" : "Bioinformatics, 31(12):i339–i347.",
      "citeRegEx" : "Zhai et al\\.,? 2015",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2015
    }, {
      "title" : "Aspect-based sentiment classification with aspectspecific graph convolutional networks",
      "author" : [ "Chen Zhang", "Qiuchi Li", "Dawei Song." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "BioWordVec, improving biomedical word embeddings with subword information and MeSH",
      "author" : [ "Yijia Zhang", "Qingyu Chen", "Zhihao Yang", "Hongfei Lin", "Zhiyong Lu." ],
      "venue" : "Scientific Data, 6.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "MeSH terms are distinctive features of MEDLINE and can be used in many applications in biomedical text mining and information retrieval (Lu et al., 2008; Huang et al., 2011; Gu et al., 2013), being recognized as important tools for research (e.",
      "startOffset" : 136,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : "MeSH terms are distinctive features of MEDLINE and can be used in many applications in biomedical text mining and information retrieval (Lu et al., 2008; Huang et al., 2011; Gu et al., 2013), being recognized as important tools for research (e.",
      "startOffset" : 136,
      "endOffset" : 190
    }, {
      "referenceID" : 7,
      "context" : "MeSH terms are distinctive features of MEDLINE and can be used in many applications in biomedical text mining and information retrieval (Lu et al., 2008; Huang et al., 2011; Gu et al., 2013), being recognized as important tools for research (e.",
      "startOffset" : 136,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "To address the MeSH indexing task mentioned in above section, the National Library of Medicine developed Medical Text Indexer (MTI) – software that automatically recommends MeSH terms to each MEDLINE article using the abstract and title as input (Aronson et al., 2004).",
      "startOffset" : 246,
      "endOffset" : 268
    }, {
      "referenceID" : 16,
      "context" : "There are two modules in MTI – MetaMap Indexing (MMI) and PubMed-Related Citations (PRC) (Lin and Wilbur, 2007; Aronson and Lang, 2010).",
      "startOffset" : 89,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "There are two modules in MTI – MetaMap Indexing (MMI) and PubMed-Related Citations (PRC) (Lin and Wilbur, 2007; Aronson and Lang, 2010).",
      "startOffset" : 89,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "Many effective MeSH indexing systems have been developed since then, such as MeSHLabeler (Liu et al., 2015),",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : "DeepMeSH (Peng et al., 2016), AttentionMeSH (Jin et al.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : ", 2016), AttentionMeSH (Jin et al., 2018), and MeSHProbeNet (Xun et al.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "HGCN4MeSH (Yu et al., 2020) used the PMC dataset generated by Wang and Mercer (2019) and employed graph convolutional neural network to learn the co-occurrences between MeSH terms.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 29,
      "context" : ", 2019) and BERTMeSH (You et al., 2020) used all available full text articles in PMC Open Access Subset.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : "Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention recently.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "For instance, TextGCN (Yao et al., 2019) built a single text graph for a corpus based on word co-occurrence and document word relations to infer labels.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : ", MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "We first apply a bidirectional Long Short-Term Memory (biLSTM) network (Hochreiter and Schmidhuber, 1997) in both channels to encode the two types of text and to generate the hidden representations ht for each word at time step t.",
      "startOffset" : 71,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "The concept of dilated convolution was originally developed for wavelet decomposition (Holschneider et al., 1990), and has been applied to NLP tasks such as neural machine translation (Kalchbrenner et al.",
      "startOffset" : 86,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : ", 1990), and has been applied to NLP tasks such as neural machine translation (Kalchbrenner et al., 2017) and text classification (Lin et al.",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "Second, the issue with the original attention mechanism (Bahdanau et al., 2015) is that the classifier focuses on spotting relevant information for all predicted labels, which is a lack of pertinence.",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "We train our model using the multi-label binary crossentropy loss (Nam et al., 2014):",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "(2020) by using the PMC FTP service9 (Comeau et al., 2019)",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 33,
      "context" : "We use pre-trained biomedical word embeddings (BioWordVec) (Zhang et al., 2019b), and the embedding dimension is 200.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "We use Adam optimizer (Kingma and Ba, 2015) and early stopping strategies.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "Second, we are interested in integrating knowledge from the Unified Medical Language System (UMLS) (Bodenreider, 2004), a comprehensive ontology of biomedical concepts, in our model.",
      "startOffset" : 99,
      "endOffset" : 118
    } ],
    "year" : 0,
    "abstractText" : "Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information. With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important. MeSH indexing is a challenging task for machine learning, as it needs to assign multiple labels to each article from an extremely large hierachically organized collection. To address this challenge, we propose KenMeSH, an end-to-end model that combines new text features and a dynamic Knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.",
    "creator" : null
  }
}
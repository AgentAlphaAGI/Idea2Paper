{
  "name" : "ARR_2022_150_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Table-based Fact Verification with Self-adaptive Mixture of Experts",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kryscinski et al., 2020). While most existing research focuses on verification based on unstructured text, a new trend is extending the scope to structured evidence (e.g., tables), which is informative and ubiquitous in our daily lives. Table-based verification is more challenging than unstructured-text-based due to the\ncomplexity of the requirements, including sophisticated textual, numerical, and logical reasoning across evidence tables; even for some statements, multiple types of reasoning are indispensable to complete the verification. An example is presented in Figure 1.\nTo tackle the challenges above, previous work established two kinds of methods: (1) programenhanced methods (Chen et al., 2020; Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) and (2) table-based pre-trained models (Eisenschlos et al., 2020; Liu et al., 2021). The program-enhanced methods mainly leverage programs generated by the semantic parser. Specifically, statements are parsed into executable programs to extract the logical/numerical semantics, which is further be leveraged together with contextual semantics learned by a language model (e.g., BERT) in inference. However, the semantic parsers that generate semanticconsistent programs must be trained in a weak supervision setting, which brings difficulties in training. Furthermore, generalizing this method to other datasets is almost impossible without the API set modification according to the reasoning requirements on the new datasets.\nThe table-based pre-trained models leverage elaborate model structure (Herzig et al., 2020) and pre-training tasks (Eisenschlos et al., 2020; Liu et al., 2021) to enhance the reasoning skills on structured data. Nevertheless, two significant shortcomings remain. Firstly, the process is demanding due\nto the tremendous computing resources required by pre-training. Moreover, the effectiveness of pretraining to its downstream tasks mainly depends on the adaptability between these two tasks. Therefore, implementing pre-training tasks may fail to meet the requirements when facing the unseen reasoning types demanded by new datasets.\nIn this paper, we introduce an innovative framework, Self-adaptive Mixture of Experts (SaMoE), to address the previously mentioned problems. The entire framework is illustrated in Figure 2. SaMoE consists of 3 components: feature extractor, experts, and management module, which is the combination of manager and supervisor networks. Each expert initially takes the same feature as input and then learns to deal with different parts of the reasoning types (e.g., contextual/logical/numerical) required by table-based verification. A management module is designed to guide the training of experts and combine experts’ verification results effectively. The manager network in this module assigns each expert a unique attention score, allowing each individual to focus on different kinds of reasoning types and summarizes experts’ entire outputs as the final verification result. However, managers failed to allocate the highest attention score to the expert who performs best on the current reasoning type in most circumstances. To alleviate this problem, we introduce a supervisor network to adjust the attention score given by the manager. The supervisor network is trained selfadaptively (i.e., it learns directly from experts’ performance on the train set) without prior knowledge of the task or dataset. Extensive experiments are conducted to show that our proposed framework,\nimplemented with a general pre-trained language model RoBERTa (Liu et al., 2019), outperforms previous state-of-the-art methods, including tablebased pre-trained models. The main contributions of this work are as follows:\n• We innovatively implement mixture-ofexperts for table-based verification, aiming to arrange each expert to different types of reasoning. This method can also be easily generalized to other datasets.\n• We investigate a self-adaptive method to adjust suitable attention score to each expert according to its performance on different reasoning types, achieving more efficient cooperation across experts.\n• Our framework achieves better performance on the TABFACT dataset without the assistance of table-based pre-trained models."
    }, {
      "heading" : "2 Task Formulation",
      "text" : "The table-based verification task expects one to determine whether a statement S is entailed or refuted by an evidence table T . The process above can be regarded as a binary classification task and thus denoted as f(S, T ) = ŷ, where f is the verification model and ŷ ∈ {0, 1} its prediction."
    }, {
      "heading" : "3 Methods",
      "text" : "We present the proposed framework (SaMoE), which leverages a set of experts to deal with different parts of the reasoning types involved in tablebased verification. This section is organized as follows. Sec.3.1 introduces the feature extractor that\nextracts the joint semantics of the table-statement pair. Sec.3.2 describes experts that verify the statements separately based on the same extracted semantics. Sec.3.3 describes the management module that guides the experts’ training and combines their verification results effectively; two components of this module, the manager and the supervisor, are introduced in this section individually."
    }, {
      "heading" : "3.1 Feature Extractor",
      "text" : "Feature extractor parses the statement-table pair and learns the joint table-statement semantics. Tables are initially pruned and serialized into a sequence. Subsequently, the serialized tables are transmitted into the language model together with the statements for joint representation learning. These two processes will be further interpreted in the following subsections."
    }, {
      "heading" : "3.1.1 Table Pre-processing",
      "text" : "As for Tables, the pre-processing (pruning and serializing) before the joint representation learning provides convenience for subsequent processing of the existing language model.\nTable Pruning Table pruning discards some parts of the table that do not participate in the verification, according to the input size limit of the language model. We take advantage of the tablepruning algorithm proposed in Chen et al. (2020) and further enhance its performance. The original algorithm matches the entities in statements with cells in tables by a heuristic method and selects the columns that include matched cells to form the pruned table. Noticed that the algorithm always fails to select the critical columns of verification while there is still room left for the input sequence of the language model, we further add a greedy strategy on the algorithm that keeps adding columns that are not selected to the pruned table until reaching the maximum input size of the downstream model to make the best use of its capacity.\nTable Serializing Tables are further serialized to a 1-D sequence after pruning to be compatible with the input format of the language model. We follow the serializing method used in TABLE-BERT (Chen et al., 2020) that paraphrases tables with a natural language template. Specifically, a table with m rows and n columns is paraphrased as “row 1 is: h1 is T11; ... ; hn is T1n. row 2 is: ... row m is: h1 is Tm1; ... ; hn is Tmn.\", where hi refers the ith\nheader and Tij the value in the (i, j)− th cell of table T . We find that such template-serialized tables are more suitable for language models pre-trained on unstructured text to process."
    }, {
      "heading" : "3.1.2 Joint Representation Learning",
      "text" : "After the table pre-processing, the serialized table and the statement are further passed to a language model to learn the joint contextual representation of each token. The learned representation vectors are then transmitted to the experts and the management module for inference and management. Specifically, the serialized table and the statement are initially tokenized into two token sequences T̃ and S. Then the joint token sequence X is formed as X = [⟨s⟩, S, ⟨/s⟩, T̃, ⟨/s⟩], where ⟨s⟩ and ⟨/s⟩ are the separators that identify the beginning and the end of each token sequence. The token sequence X will be padded or truncated to fit the maximum input length of the language model. Finally, a transformer model is applied to learn the contextual representation of X :\nH = fLM (X) (1)\nwhere H ∈ Rn×d refers to the learned joint representation, n is the maximum input length and d the dimension of the representation vector. fLM denotes the contextual representation learning process of the language model. In this paper, we implement it with transformer (Vaswani et al., 2017), the most popular contextual representation model in recent years."
    }, {
      "heading" : "3.2 Experts",
      "text" : "A group of experts is applied to verify the statements separately based on the same statement-table joint semantics extracted by the feature extractor module. Experts share the same model structure, while the parameter learning strategy of SaMoE gives expert differentiation. Specifically, each expert is implemented with a stack of transformer encoding layers. An MLP classifier that calculates the probability of the statement is entailed by the evidence table based on the encoded semantics. We implement experts with the same general structure rather than different structures specially designed for certain reasoning types since we anticipate that the proposed framework can be smoothly generalized to other datasets. The process above can be formulated as follows:\nhi = fEnci(H) (2)\npi = softmax(tanh(hiW i 1)W i 2) (3)\nwhere hi ∈ Rd is the token ⟨s⟩’s final representation vector encoded by the ith expert’s encoder Enci. It implies the ith expert’s whole understanding to the statement-table pair. Wi1 ∈ Rd×d and Wi2 ∈ Rd×2 are the trainable parameters of ith expert’s classifier, which projects hi to the probabilities pi ∈ R2 that the statement is entailed/refuted by the table. tanh and softmax are activation functions. ne refers to the number of experts."
    }, {
      "heading" : "3.3 Management Module",
      "text" : "Learning the joint semantics parsed in Sec.3.1, the management module intends to generate attention scores to bias experts’ training and combine experts’ results efficiently. The module consists of two components: manager and supervisor, both of them are implemented based on transformer model. The manager is mainly designed to guide experts’ training, while the supervisor is applied to combine experts’ results efficiently.\nManager The manager guides the training of experts and forms a preliminary assumption to the expert combination. It encodes the joint representation matrix and generates attention scores aM to guide the experts’ training process:\nhM = fEncM (H) (4)\neM = tanh(hMWM1 )W M 2 (5)\naM = softmax(eM ) (6)\nwhere EncM denotes the manager’s encoder, WM1 ∈ Rd×d and WM2 ∈ Rd×ne are trainable parameters. The network structures of the manager and experts are basically the same, only different in the layers of the encoder and the output dimension.\nAfter preceding calculation, the normalized attention scores aM are used to guide the training of experts by a specially designed verification loss, which will be introduced in Sec.4.1.1.\nSupervisor The supervisor adjusts the attention scores submitted by the manager to improve the cooperative efficiency among experts (i.e. assigning higher weights to experts who perform better on the current input pair). The network predicts the deviation between the preliminary assumption (i.e., the attention) and the ideal combination weights\nbased on the knowledge encoded in the joint representation matrix H:\nhS = fEncS (H) (7)\neS = tanh(hSWS1 )W S 2 (8)\naS = softmax(eM + eS) (9)\nwhere WS1 ∈ Rd×d, WS2 ∈ Rd×ne are trainable parameters and EncS refers to the encoder of the supervisor. Parameters of the supervisor are optimized self-adaptively based on experts’ performance on the train set. More details of this learning strategy will be presented in Sec.4.2."
    }, {
      "heading" : "4 Parameter Learning",
      "text" : "Parameters in SaMoE are learned in two consecutive stages: 1) Supervised learning: parameters in the feature extractor, experts and the manager are end-to-end optimized under the supervision of labels; 2) Self-adaptive learning: parameters in the supervisor are self-optimized by observing experts’ performance on the train set (other parameters are fixed simultaneously). A weighted sum of two losses is minimized in the first stage to achieve diverse and balanced training of experts. For the second stage, we minimize a self-adaptive loss calculated based on the experts’ classification loss. Subsequent sections introduce these two learning stages in detail."
    }, {
      "heading" : "4.1 Supervised Learning",
      "text" : "Supervised learning guides each expert on dealing with different reasoning types while maintaining balanced training across experts. To achieve the goals above, we develop two loss functions: 1) verification loss LV that measures the weighted sum of each expert’s classification loss, differentiating experts’ learning with different attention scores assigned by the manager; 2) manager assumption loss LM that is applied to prevent the occurrence of imbalanced training across experts. The overall loss of this state is calculated by a weighted sum of these two terms: L1 = LV + λLM , where λ is a hyperparameter that controls the ratio of LM . Subsequent sections provide detailed introduction to these two terms."
    }, {
      "heading" : "4.1.1 Verification Loss",
      "text" : "The verification loss LV is designed based on the loss function proposed in Jacobs et al. (1991). It\nis calculated by the weighted sum of each expert’s cross-entropy:\nLV = ne∑ i=1 (aM )i · CE(pi, l) (10)\nwhere (aM )i is the ith element of the attention scores aM , l ∈ {0, 1} is the label of the statementtable pair and CE(·, ·) the cross-entropy function. Note that it is necessary to calculate each expert’s cross-entropy independently. We want each expert to behave like an independent expert (i.e., complete the verification without the help of other experts). The attention vector aM acts as a \"training scheduler\" in this loss function: experts that are assigned with larger attention scores receive a larger gradient than other experts on the current input, resulting in diverse experts’ performance."
    }, {
      "heading" : "4.1.2 Manager Assumption Loss",
      "text" : "We have trained the MoE with only the verification loss LV and observe a severe \"imbalanced experts\" phenomenon that only one expert is well-trained (i.e., the expert performance is improved by training) and the manager keeps assigning a close-to-1 attention score to this expert, which is also reported in previous research (Eigen et al., 2013; Shazeer et al., 2017). To avoid this problem, we develop another loss function that forces the manager to assign reasonable attention scores to experts:\nLM = D(aP ||aM ) (11)\nwhere D(·||·) denotes the Kullback–Leibler divergence and aP a prior assumption that is generated with a simple heuristic algorithm (to be introduced in the next paragraph) which requires limited prior knowledge of the reasoning types. By minimizing LM , the manager learns to assign each expert with a reasonable attention score, resulting in a balanced training across experts.\nPrior Assumption Generation The prior assumption aP is generated to represent the probabilities that the statement involves different reasoning types that we are interested in. Specifically, we develop a trigger-word-based heuristic algorithm to form the prior assumption for each statement automatically:\n1. Initialize the prior assumption with e0 ∈ Rne , which is empirically set as (0.1, 0.1, ..., 0.6)T . The (e0)ne represents the probability that the\nstatement does not involve any predefined reasoning types and thus is set higher than other values in advance.\n2. Traverse the trigger-word set of each reasoning type (ne − 1 types in total). If a trigger word/pattern w that belongs to ith reasoning type is detected in the statement, the trigger’s weight sw (set empirically) is accumulated to the ith dimension of a zero-initialized bias vector δ ∈ Rne : δi ← δi + sw.\n3. Add the bias vector δ to the prior assumption e0 and normalize to get the prior assumption: aP = softmax(e0 + δ).\nFigure 3 presents an example of this process. Learning to imitate the prior assumption, the manager guides each expert to focus on different reasoning types and thus achieves diverse experts. We implement a relatively small trigger-word pool in experiments and find the method works effectively, indicating that the method can be smoothly generalized to other datasets with little modification to the predefined reasoning types and trigger-word pool."
    }, {
      "heading" : "4.2 Self-adaptive Learning",
      "text" : "Self-adaptive learning aims to enhance further the expert combining efficiency with only the knowledge of the expert’s performance on the train set. Specifically, an “expert ability\" vector aE ∈ Rne is calculated based on the “expert loss\" vector m ∈ Rne , where mi = CE(pi, l) is the crossentropy loss of the ith expert. Note that the crossentropy of the expert is negatively correlated with its performance. Then the expert ability vector aE is calculated as follows:\naE = softmax(−α ·m) (12) where α = √\nβ/V ar(m) is a variancenormalizing coefficient and β is a hyperparameter\nthat decides the variance of the expert ability vector before the activation (i.e., V ar(−α·m) = β). Such normalization is designed based on the observation that m tends to have a extreme small variance and softmax(−m) often generates a close-to-uniform distribution. Note that the generated aE is positively correlated with the experts’ performance (e.g., if the ith expert outperforms the jth expert on the input pair then we have (aE)i > (aE)j).\nBased on aE , we develop the loss function that has the same form with LM in Sec 4.1.2:\nLS = D(aE ||aS) (13)\nBy minimizing the loss above, the higher attention scores are assigned to the best-performed experts after the supervisor’s adjustment, resulting in more efficient cooperation across experts."
    }, {
      "heading" : "5 Experiment Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Data and Metric",
      "text" : "We conduct the experiments on TABFACT, a large scale benchmark dataset of the table-based fact verification task1. TABFACT contains a total of 117k statements and 16k Wikipedia tables. The test set is further divided into a simple and complex subset based on verification difficulty, for verifying some statements on TABFACT requires more logical/numerical reasoning skills. We choose accuracy as the evaluation metric following the existing work to make our experiment results comparable. More details of TABFACT are presented in Appendix A."
    }, {
      "heading" : "5.2 Implementation Details",
      "text" : "Training Details We set ne = 5 expert networks in our implementation of SaMoE. The transformer layers are 12 for encoders in the feature extractor and experts and 2 for encoders in the manager and supervisor. The hidden states’ dimension d, the maximum input length n, the λ in Sec.4.1, and the β in Sec.4.2 are set to 1024, 512, 0.1 and 0.1 respectively. We applied RoBERTa-Large (Liu et al., 2019) to initialize the feature extractor and experts in our framework. The details of parameter initialization can be found in Appendix B.\nWe apply Adam optimizer (Kingma and Ba, 2015) in training with learning rate 2e-5, dropout rate 0.1, warmup step 17,304, and batch size 32.\n1We did not conduct experiments on other datasets such as SEM-TAB-FACTS (Wang et al., 2021) and InfoTabs (Gupta et al., 2020), since there is little work and comparisons have been made on these datasets.\nSaMoE is first trained in the supervised learning stage for 57,680 steps (20 epochs). Then the supervisor is trained in the self-adaptive learning stage for another 5,000 steps, while the best parameters of other parts in the framework are loaded and fixed. The model is evaluated every 1000 steps, and the model that achieves the highest performance on the development set is saved. All the codes are implemented with Pytorch (Paszke et al., 2019) and the transformers package (Wolf et al., 2020).\nSettings of Prior Assumption We choose the top 4 types of reasoning types that appear most frequently in TABFACT2 (count, comparative, superlative, negation). We apply a small trigger-word pool containing only 26 trigger words, injecting limited prior knowledge of the dataset. More details of this part are presented in Appendix C."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "We compared our proposed framework with different kinds of baselines on TABFACT: (1) Programenhanced methods: LPA (Chen et al., 2020), LogicalFactChecker (Zhong et al., 2020), HeterTFV (Shi et al., 2020), ProgVGAT (Yang et al., 2020) and Decomp (Yang and Zhu, 2021); (2) Tablebased pre-trained models: TAPAS (Eisenschlos et al., 2020) and TAPEX (Liu et al., 2021); (3) Other methods: Table-BERT (Chen et al., 2020) and SAT (Zhang et al., 2020)."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Overall Performance",
      "text" : "We compare the proposed SaMoE with different kinds of baselines, and the results are listed in Table 1. Baselines are presented with the best performance reported in the corresponding papers. SaMoE obtains an accuracy of 85.1% on the test set, achieving a new state-of-the-art on the dataset. Results show that our method consistently outperforms all the program-enhanced methods with a significant 2.4% improvement compared with the Decomp method (the best performed programenhanced method). Note that SaMoE performs similar with Decomp-LARGE on the simple subset of the test set (93.6% vs. 93.6%) while outperforms Decomp-LARGE with a remarkable 3.5% on the complex subset (80.9% vs. 77.4%). Such analysis indicates that the performance improvement is mainly derived from successfully verifying\n2We follow the statistics in Chen et al. (2020) for the frequency of different reasoning types.\ncomplex statements, which required more sophisticated reasoning than statements in the simple set. SaMoE even shows comparable performance with the previous SOTA TAPEX that is pre-trained to execute SQL queries on tables. Our method outperforms TAPEX with a 0.9% improvement on the test set and a further 1.3% improvement on the complex subset, indicating that SaMoE, based on a text-based pre-trained model, performs even better than table-based pre-trained models on a variety of complex reasoning types demanded by the tablebased verification."
    }, {
      "heading" : "6.2 Ablation Study",
      "text" : "We further investigate the effectiveness of the MoE structure and self-adaptive learning with an ablation study. We conduct two experiments: one reduces the number of experts to 1 to disable the contribution from the MoE structure (SaMoE w/o Sa (ne = 1)); the other trains the proposed framework with only the supervised learning stage (SaMoE w/o Sa). Results are presented in Table 2. The MoE structure achieves a 0.7% improvement on the test set (84.7% vs. 84.0%), and self-adaptive learning further improves the performance slightly\n(85.1% vs. 84.7%). Note that the slight improvement of self-adaptive learning is expected since the experts and the feature extractor are fixed in this stage. The results demonstrate the effectiveness of both the MoE structure and the self-adaptive learning."
    }, {
      "heading" : "6.3 Effectiveness Analysis",
      "text" : "We show in this section that the effectiveness of the proposed framework is derived from two aspects: the differentiation of experts (each expert outperforms others on a specific part of reasoning types) and the effective attention assignment by the management module (the best-performed experts are assigned with higher attention scores)."
    }, {
      "heading" : "6.3.1 Expert Differentiation",
      "text" : "We first investigate the proposed manager assumption loss LM and find that it achieves balanced training across experts, which is the premise of expert differentiation. Figure 4 compares the two models trained with and without LM , with the performance curves of different experts on the development set presented in each sub-figure. Once LM is applied, four experts that fail to be trained (the\nperformance stays around 50% as training steps increase) achieve comparable performance with the rest expert (expert 5 in sub-figure (a)). The result indicates that the proposed LM leads balanced training across experts.\nWe further show that the proposed framework achieves differentiation across experts. Figure 5 presents the proportion of statements in the test set that are verified correctly by at least k experts (k varies from 1 to 5). Note that the proportion increases rapidly as k decreases (76.2% to 90.7% for k from 5 to 1), which illustrates that experts behave differently on a large proportion of statements. The results indicate that SaMoE successfully achieves expert differentiation, which expands the original performance upper bound considerably (90.7%)."
    }, {
      "heading" : "6.3.2 Effective Attention Assignment",
      "text" : "We conduct a detailed analysis to investigate whether the management module assigns higher attention scores to experts with the best performance after self-adaptive learning. To achieve this goal, we regard the management module as a ne-class classifier and calculate the top-k accuracy of predicting the best-performed expert (the one with the smallest cross-entropy) on the test set where k is chosen in [1, 2, 3]. The results of the analysis are presented in Table 3. The top-k accuracy is improved significantly after self-adaptive learning\n(+6.6%, +14.2%, +8.4% respectively), indicating that the management module successfully assigns higher attention scores to the best-performed experts by self-adaptive learning.\nBased on the significant performance upper bound expanded by the expert differentiation, the effective attention assignment achieves more efficient cooperation across these diverse experts, thus improving the verification performance."
    }, {
      "heading" : "7 Related Works",
      "text" : "Table-Based Fact Verification Most of the current models utilize programs to improve the model’s ability to handle various types of numerical and logical reasoning (Chen et al., 2020; Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020; Yang and Zhu, 2021), while Eisenschlos et al. (2020); Liu et al. (2021) leverage table-based pre-trained models to parse the structural and numerical semantics of tables better. Unlike previous works, we use a novel mixture-of-experts framework to handle different logical and numerical semantics without semantic parsing and table-based pre-training.\nMixture of Experts Mixture of experts is a special model combining method. Jacobs et al. (1991) first introduces this method and proposes a loss that encourages competitive learning across expert models. We develop a self-adapted mixture-ofexperts framework that achieves a more effective combination of experts by learning from the experts’ performance on the train set."
    }, {
      "heading" : "8 Conclusion",
      "text" : "This paper proposes a framework that leverages the mixture of experts to recognize and execute different types of reasoning required for table-based fact verification. We propose an MoE model guided with limited prior knowledge to handle different parts of the reasoning types required by table-based verification with diverse experts. Moreover, we design a novel supervisor network to adjust the imprecise attention score and achieve a more efficient combination across experts. A self-adaptive learning strategy is further applied to train the proposed supervisor network without prior knowledge of the task or dataset. The experiments show that the proposed model achieves a new state-of-the-art performance of 85.1% accuracy on the benchmark dataset TABFACT. The ablation studies and analysis further indicate the effectiveness of the proposed MoE structure and self-adaptive learning strategy."
    }, {
      "heading" : "A Statistics of TABFACT",
      "text" : "Table 4 shows the basic statistics of TABFACT. As the table shows, the whole dataset is randomly divided into three subsets with the ratio be 8:1:1. The average numbers of rows and columns in tables keep approximately the same across three subsets, which reflects the consistency of data distribution."
    }, {
      "heading" : "B Parameter Initialization",
      "text" : "For parameter initialization, We leverage RoBERTa-Large, a pre-trained language model that has 24 transformer encoding layers. We initial parameters of the feature extractor with the embedding layer and the bottom 12 encoding layers of RoBERTa-Large and each expert with the upper 12 encoding layers of RoBERTa-Large, respectively. We use PyTorch to initialize other parameters randomly."
    }, {
      "heading" : "C Specific Setting of Prior Assumption Generation",
      "text" : "We choose four reasoning types that appear most frequently in TABFACT: count, comparative, superlative, and negation. The detailed definitions of four reasoning types chosen in our implementation are listed below:\n1. Count: counting the number of specific rows in the table, such as “xxx be listed a total of 3 times\", “xxx win only 1 time in ...\", etc.\n2. Comparative: comparing two values in the statement or cells, such as “xxx play in more than 1 game during ...\", “xxx has a larger yyy than zzz\", etc.\n3. Superlative: finding the highest/lowest value of the specific column, such as “the longest xxx be yyy\", “the lowest score at xxx be yyy\", etc.\n4. Negation: negating the original semantics of the statement, such as “xxx has never lost a game in ...\", “xxx never score 0 points\", etc.\nA small trigger-word pool that contains only 26 trigger words/patterns is applied for the prior assumption generation: 11 triggers for the \"count\" type, 15 for \"negation\"; and for the rest types (i.e., \"comparative\" and \"superlative\" types), the NLTK package is employed to recognize the comparative and superlative words automatically. Such a small trigger-word pool injects limit prior knowledge of the dataset, indicating that the proposed method can be generalized to other datasets by simply modifying the pool of trigger words. Table 5 presents some words/patterns in the trigger-word pool applied in our experiments. x+[number] denotes a combination of a word and a number that is served as a trigger (e.g., for the statement “xxx win 3 times in ...\", we match the phrase “3 times\" with the trigger “[number]+times\")."
    } ],
    "references" : [ {
      "title" : "Tabfact: A large-scale dataset for table-based fact verification",
      "author" : [ "Wenhu Chen", "Hongmin Wang", "Jianshu Chen", "Yunkai Zhang", "Hong Wang", "Shiyang Li", "Xiyou Zhou", "William Yang Wang." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314",
      "author" : [ "David Eigen", "Marc’Aurelio Ranzato", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Eigen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Eigen et al\\.",
      "year" : 2013
    }, {
      "title" : "Understanding tables with intermediate pre-training",
      "author" : [ "Julian Eisenschlos", "Syrine Krichene", "Thomas Müller." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 281–296, Online. Association for Computational Lin-",
      "citeRegEx" : "Eisenschlos et al\\.,? 2020",
      "shortCiteRegEx" : "Eisenschlos et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing the factual accuracy of generated text",
      "author" : [ "Ben Goodrich", "Vinay Rao", "Peter J Liu", "Mohammad Saleh." ],
      "venue" : "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 166–175.",
      "citeRegEx" : "Goodrich et al\\.,? 2019",
      "shortCiteRegEx" : "Goodrich et al\\.",
      "year" : 2019
    }, {
      "title" : "INFOTABS: Inference on tables as semi-structured data",
      "author" : [ "Vivek Gupta", "Maitrey Mehta", "Pegah Nokhiz", "Vivek Srikumar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2309–2324, Online. Association",
      "citeRegEx" : "Gupta et al\\.,? 2020",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "TaPas: Weakly supervised table parsing via pre-training",
      "author" : [ "Jonathan Herzig", "Pawel Krzysztof Nowak", "Thomas Müller", "Francesco Piccinno", "Julian Eisenschlos." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Herzig et al\\.,? 2020",
      "shortCiteRegEx" : "Herzig et al\\.",
      "year" : 2020
    }, {
      "title" : "Adaptive mixtures of local experts",
      "author" : [ "Robert A Jacobs", "Michael I Jordan", "Steven J Nowlan", "Geoffrey E Hinton." ],
      "venue" : "Neural computation, 3(1):79–87.",
      "citeRegEx" : "Jacobs et al\\.,? 1991",
      "shortCiteRegEx" : "Jacobs et al\\.",
      "year" : 1991
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kryscinski et al\\.,? 2020",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2020
    }, {
      "title" : "Tapex: Table pre-training via learning a neural sql executor",
      "author" : [ "Qian Liu", "Bei Chen", "Jiaqi Guo", "Zeqi Lin", "Jianguang Lou." ],
      "venue" : "arXiv preprint arXiv:2107.07653.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Truth of varying shades: Analyzing language in fake news and political fact-checking",
      "author" : [ "Hannah Rashkin", "Eunsol Choi", "Jin Yea Jang", "Svitlana Volkova", "Yejin Choi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Rashkin et al\\.,? 2017",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2017
    }, {
      "title" : "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "author" : [ "Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc V. Le", "Geoffrey E. Hinton", "Jeff Dean." ],
      "venue" : "5th International Conference on Learning Representa-",
      "citeRegEx" : "Shazeer et al\\.,? 2017",
      "shortCiteRegEx" : "Shazeer et al\\.",
      "year" : 2017
    }, {
      "title" : "Learn to combine linguistic and symbolic information for table-based fact verification",
      "author" : [ "Qi Shi", "Yu Zhang", "Qingyu Yin", "Ting Liu." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 5335–5346, Barcelona,",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and VERification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "Do sentence interactions matter? leveraging sentence level representations for fake news classification",
      "author" : [ "Vaibhav Vaibhav", "Raghuram Mandyam", "Eduard Hovy." ],
      "venue" : "Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language",
      "citeRegEx" : "Vaibhav et al\\.,? 2019",
      "shortCiteRegEx" : "Vaibhav et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "SemEval2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TABFACTS)",
      "author" : [ "Nancy X.R. Wang", "Diwakar Mahajan", "Marina Danilevsky", "Sara Rosenthal." ],
      "venue" : "Proceedings of the 15th International",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Program enhanced fact verification with verbalization and graph attention network",
      "author" : [ "Xiaoyu Yang", "Feng Nie", "Yufei Feng", "Quan Liu", "Zhigang Chen", "Xiaodan Zhu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring decomposition for table-based fact verification",
      "author" : [ "Xiaoyu Yang", "Xiaodan Zhu." ],
      "venue" : "arXiv preprint arXiv:2109.11020.",
      "citeRegEx" : "Yang and Zhu.,? 2021",
      "shortCiteRegEx" : "Yang and Zhu.",
      "year" : 2021
    }, {
      "title" : "Table fact verification with structure-aware transformer",
      "author" : [ "Hongzhi Zhang", "Yingyao Wang", "Sirui Wang", "Xuezhi Cao", "Fuzheng Zhang", "Zhongyuan Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "LogicalFactChecker: Leveraging logical operations for fact checking with graph module network",
      "author" : [ "Wanjun Zhong", "Duyu Tang", "Zhangyin Feng", "Nan Duan", "Ming Zhou", "Ming Gong", "Linjun Shou", "Daxin Jiang", "Jiahai Wang", "Jian Yin." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kryscinski et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 303
    }, {
      "referenceID" : 15,
      "context" : "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kryscinski et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 303
    }, {
      "referenceID" : 3,
      "context" : "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kryscinski et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 303
    }, {
      "referenceID" : 16,
      "context" : "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kryscinski et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 303
    }, {
      "referenceID" : 8,
      "context" : "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kryscinski et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 303
    }, {
      "referenceID" : 0,
      "context" : "enhanced methods (Chen et al., 2020; Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) and (2) table-based pre-trained models (Eisenschlos et al.",
      "startOffset" : 17,
      "endOffset" : 93
    }, {
      "referenceID" : 23,
      "context" : "enhanced methods (Chen et al., 2020; Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) and (2) table-based pre-trained models (Eisenschlos et al.",
      "startOffset" : 17,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "enhanced methods (Chen et al., 2020; Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) and (2) table-based pre-trained models (Eisenschlos et al.",
      "startOffset" : 17,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "enhanced methods (Chen et al., 2020; Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) and (2) table-based pre-trained models (Eisenschlos et al.",
      "startOffset" : 17,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : ", 2020) and (2) table-based pre-trained models (Eisenschlos et al., 2020; Liu et al., 2021).",
      "startOffset" : 47,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : ", 2020) and (2) table-based pre-trained models (Eisenschlos et al., 2020; Liu et al., 2021).",
      "startOffset" : 47,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "The table-based pre-trained models leverage elaborate model structure (Herzig et al., 2020) and pre-training tasks (Eisenschlos et al.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : ", 2020) and pre-training tasks (Eisenschlos et al., 2020; Liu et al., 2021) to enhance the reasoning skills on structured data.",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : ", 2020) and pre-training tasks (Eisenschlos et al., 2020; Liu et al., 2021) to enhance the reasoning skills on structured data.",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Extensive experiments are conducted to show that our proposed framework, implemented with a general pre-trained language model RoBERTa (Liu et al., 2019), outperforms previous state-of-the-art methods, including table-",
      "startOffset" : 135,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "We follow the serializing method used in TABLE-BERT (Chen et al., 2020) that paraphrases tables with a natural language template.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "In this paper, we implement it with transformer (Vaswani et al., 2017), the most popular contextual representation model in recent years.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "ing) and the manager keeps assigning a close-to-1 attention score to this expert, which is also reported in previous research (Eigen et al., 2013; Shazeer et al., 2017).",
      "startOffset" : 126,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "ing) and the manager keeps assigning a close-to-1 attention score to this expert, which is also reported in previous research (Eigen et al., 2013; Shazeer et al., 2017).",
      "startOffset" : 126,
      "endOffset" : 168
    }, {
      "referenceID" : 10,
      "context" : "We applied RoBERTa-Large (Liu et al., 2019) to initialize the feature extractor and experts in our framework.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "We apply Adam optimizer (Kingma and Ba, 2015) in training with learning rate 2e-5, dropout rate 0.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : "We did not conduct experiments on other datasets such as SEM-TAB-FACTS (Wang et al., 2021) and InfoTabs (Gupta et al.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : ", 2021) and InfoTabs (Gupta et al., 2020), since there is little work and comparisons have been made on these datasets.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "We compared our proposed framework with different kinds of baselines on TABFACT: (1) Programenhanced methods: LPA (Chen et al., 2020), LogicalFactChecker (Zhong et al.",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : ", 2020), LogicalFactChecker (Zhong et al., 2020), HeterTFV",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : ", 2020), ProgVGAT (Yang et al., 2020) and Decomp (Yang and Zhu, 2021); (2) Tablebased pre-trained models: TAPAS (Eisenschlos et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : ", 2020) and Decomp (Yang and Zhu, 2021); (2) Tablebased pre-trained models: TAPAS (Eisenschlos et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : ", 2020) and Decomp (Yang and Zhu, 2021); (2) Tablebased pre-trained models: TAPAS (Eisenschlos et al., 2020) and TAPEX (Liu et al.",
      "startOffset" : 82,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : ", 2020) and TAPEX (Liu et al., 2021); (3) Other methods: Table-BERT (Chen et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : ", 2021); (3) Other methods: Table-BERT (Chen et al., 2020)",
      "startOffset" : 39,
      "endOffset" : 58
    } ],
    "year" : 0,
    "abstractText" : "The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). In this paper, we present a Self-adaptive Mixture-of-Experts Network (SaMoE), a novel framework built on this fundamental property. Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning—the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result. A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge. The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TABFACT, comparable with the previous state-ofthe-art models. We hope our framework can serve as a new baseline for table-based verification. Our code will be available at (URL to be released here).",
    "creator" : null
  }
}
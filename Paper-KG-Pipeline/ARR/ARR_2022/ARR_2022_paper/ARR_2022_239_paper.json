{
  "name" : "ARR_2022_239_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Low Resource Style Transfer via Domain Adaptive Meta Learning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Text style transfer (TST) aims to change the style of the input text and keep its content unchanged, which has been applied successfully to text formalization (Jain et al., 2019) , text rewriting (Nikolov and Hahnloser, 2018) , personalized dialogue generation (Niu and Bansal, 2018) and other stylized text generation tasks (Gao et al., 2019; Cao et al., 2020; Syed et al., 2020).\nText style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017; Wang et al., 2020b; Pryzant et al., 2020). However, parallel datasets are difficult to obtain due to expensive manual annotation. The recent surge of deep generative methods (Hu et al., 2017a; Zhao et al., 2017; Li et al., 2018)\nhas spurred progress in text style transfer without parallel data. However, these methods typically require large amounts of nonparallel data and not perform well in low-resource domain scenarios.\nOne typical method is to resort to massive data from different domains, which has been studied as an effective solution to address the above data insufficiency issue (Glorot et al., 2011; Wang et al., 2017). However, directly leveraging large amounts of data from other domains for the TST task is problematic due to the differences in data distribution over different domains, as different domains usually use their domain-specific lexica (Li et al., 2019a). For instance, fine-tuning a TST model trained on a high-resource movie-related domain to a low-resource restaurant-related domain can get us unreasonable sentences like \"the food is dramatic.\" The sentiment word \"dramatic\" is weird to comment on the food but suitable for a movie.\nIn this work, we tackle the problem of domain adaptation in the scenarios where the target domain data is scarce and misaligned with the distribution in the source domain. Recently, model-agnostic meta-learning (MAML) has received resurgence in the context of few-shot learning scenario (Lin et al., 2019; Gu et al., 2018; Nooralahzadeh et al., 2020). Inspired by the essence of MAML (Finn et al., 2017), we propose a new meta-learning training strategy named domain adaptive meta-learning (DAML). Unlike MAML, DAML adopts a domain adaptive approach to construct meta tasks that would be more suitable to learn a robust and generalized initialization for low-resource TST domain adaption.\nWith the DAML strategy, we design a TST model for each domain. Usually, if a TST model tries to decouple style information from the semantics of a text, it tends to produce content loss during style transfer (Hu et al., 2017b; Dai et al., 2019; Carlson et al., 2018). Thus, we propose a new style transfer model ATM, which is composed of a\nsequence-to-sequence pre-trained language model combined with adversarial style training for style transfer. In this way, ATM can better preserve the content information without disentangling content and style in the latent space.\nCombining DAML and ATM, in this paper, we propose the method named DAML-ATM, which extends traditional meta-learning to a domain adaptive method combined with a sequence-to-sequence style transfer model. DAML contains two alternating phases. During the meta-training phase, a series of meta-tasks are constructed from a large pool of source domains for balanced absorption of general knowledge, resulting in a domain-specific temporary model. In the meta validation stage, the temporary model is evaluated on the meta validation set to minimize domain differences and realize meta knowledge transfer across different domains. In ATM, a pre-training language model based TST model is used to improve text content retention. Moreover, we propose a two-stage training algorithm to combine the DAML training method and ATM model better.\nIn summary, the main contributions in this paper are three-fold: (i) We propose a new unsupervised TST model, which achieves SOTA performance without disentangling content and style latent representations compared to other models. (ii) We extend the traditional meta-learning strategy to the domain adaptive meta transfer method, effectively alleviating the domain adaption problem in TST. (iii) We propose a two-stage training algorithm to train DAML-ATM, achieving state-of-the-art performance against multiple strong baselines."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Text Style Transfer",
      "text" : "Text style transfer based on deep learning has been extensively studied in recent years. A typical pattern is first to separate the latent space as content and style features, then adjust the style-related features and generate stylistic sentences through the decoder. (Hu et al., 2017a; Fu et al., 2017; Li et al., 2019a)assume that appropriate style regularization can achieve the separation. Style regularization may be implemented as an adversarial discriminator or style classifier in an automatic encoding process. However, these style transfer paradigms use large amounts of annotation data to train models for specific tasks. If we already have a model for a similar task, it is unreasonable to need many\ndata still to train the model from scratch. On the other hand, some of the previous work learned to do TST without manipulating the style of the generated sentence based on this learned latent space. (Dai et al., 2019)use the transformer architecture language model to introduce attention mechanism, but they do not make full use of the prior knowledge of sequence to sequence pre-trained language model, such as Bart (Lewis et al., 2019) and T5 (Raffel et al., 2019), which have made significant progress in text generation tasks. In this paper, we proposed the DAML training method to solve the domain shift problem in TST and proposed a new TST model architecture named ATM, which makes no assumption about the latent representation of source sentence and takes the proven sequence-to-sequence pre-trained language model."
    }, {
      "heading" : "2.2 Domain adaptation",
      "text" : "Domain adaptation has been studied in various natural language processing tasks (Glorot et al., 2011; Qian and Yu, 2019; Wang et al., 2017). However, there is no recent work about domain adaptation for a TST, except DAST (Li et al., 2019a). DAST is a semi-supervised learning method that adapts domain vectors to adapt models learned from multiple source domains to a new target domain via domain discriminator. Different From DAST, we propose to combine meta-learning and adversarial networks to achieve similar domain adaption ability, and our model exceeds the performance of DAST without domain discriminator. Although there are some methods perform well in few shot data transfer (Riley et al., 2021; Krishna et al., 2021), these methods discuss completely new text style transfer, while we focus on the domain adaptation issue."
    }, {
      "heading" : "2.3 Model-Agnostic Meta-Learning",
      "text" : "Model-agnostic meta-learning (MAML) (Finn et al., 2017) provides a general method to adapt to parameters in different domains. MAML solves few-shot learning problems by learning a good parameter initialization. During testing, such initialization can be fine-tuned through a few gradient steps, using a limited number of training examples in the target domain. Although there have been some researches (Qian and Yu, 2019; Li et al., 2020; Wu et al., 2020) on MAML in natural language processing, it is still scarce compared to computer vision. Unlike the above research on classification under few-shot learning, our research focuses on text style transfer based on text generation. In this\npaper, we seek a new meta-learning strategy combined with adversarial networks, which is more suitable for encouraging robust domain representation. As far as we know, we are the first to adopt meta-learning in the domain adaptation problem of text style transfer tasks."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we first define the problem of domain adaptive learning for TST. Then we describe our approach, DAML-ATM, in detail."
    }, {
      "heading" : "3.1 Task Definition",
      "text" : "Let DS = {D1, ..., DN} be N source domains in the training phase, where Dn(1 ≤ n ≤ N) is the n-th source domain containing style-labelled non-parallel data Dn = {(Xi, li)}Lni=1, where Ln is the total number of sentences, Xi denotes the ith source sentence, and li denotes the corresponding style label, which belongs to a source style label set: li ∈ LS (e.g., positive/negative). Likewise, there are K target domains DT = {D1, ..., DK} which are unseen in DS . Our task is to transfer a sentence Xi with style li in the target domain to another sentence Y\n′ i sharing the same content\nwhile having a different style l̃i from li and domainspecific characteristics of the target domain.\nWe propose a two-stage algorithm for domain adaptation in TST: pre-training learning strategy and domain adaptive meta-learning strategy. In pre-training learning, our objective is to make the model more able to preserve content information and distinguish between different text styles. In domain adaptive meta-learning, our objective is to learn a meta-knowledge learner for the sequenceto-sequence model by leveraging sufficient source data Ds. Given a new unseen domain from Dnew, the new learning task of TST can be solved by finetuning the learned sequence-to-sequence model (domain-invariant parameters) with only a small number of training samples."
    }, {
      "heading" : "3.2 DAML-ATM Approach",
      "text" : ""
    }, {
      "heading" : "3.2.1 Overview of DAML",
      "text" : "Model-agnostic meta-learning can utilize a few training samples to train a model with good generalization ability. However, since it is based on the assumption that the meta tasks are from the same distribution (Figure 1, left), simply feeding all the sources data into it might get sub-optimal results (Chen and Zhu, 2020). Therefore, we propose\na modified way to construct meta tasks (Figure 1, right). Different from MAML, for DAML, in one batch, the data in each meta task comes from the same source domain, and each meta task comes from a different domain. In this way, we can guarantee that DAML can learn generic representations from different domains in a balanced way. During each iteration, we randomly split all source domains into a meta-training set Dtr and a metavalidation set Dval, where DS = Dtr ∪Dval and Dtr∩Dval = ∅. A meta-training task Ti is sampled from Dtr and is composed of n instances from a specific domain. Likewise, a meta-validation task Tj is sampled from Dval. The validation errors on Dval should be considered to improve the robustness of the model. In short, with DAML, the parameters learned by the model in the parameter space are not biassed towards any one particular domain s with as little data as possible during model updating as shown in Figure 1(right).\nIn the final evaluation phase, the metaknowledge learned by the sequence-to-sequence model can be applied to new domains. Given a new unseen domainDnew = (Ttr, Tte), the learned sequence-to-sequence model and the discriminator are fine-tuned on Ttr and finally tested on Tte."
    }, {
      "heading" : "3.2.2 ATM Model",
      "text" : "In this section, we give a brief introduction to our proposed model: ATM, which combines sequenceto-sequence pre-trained model with adversarial training. (1) For the content preservation, we train the sequence-to-sequence model θ to reconstruct the original input sentenceX with the original style label l. (2) For the style controlling, we train a discriminator network γ to assist the sequence-tosequence model network in better controlling the\nstyle of the generated sentence. The structure of the model is shown in Figure 2.\nS2S-model To ease the explanation, we start with the sequence-to-sequence (S2S) model here. Explicitly, for an input sentence X = (x1, x2, ..., xn) of length n, X ∈ D, the S2S encoder Enc(X; θE) maps inputs to a sequence of continuous hidden representations H = (h1, h2, ..., hn). Then, the S2S decoder Dec(H; θD) estimates the conditional probability for the output sentence Y = (y1, y2, ..., yn) by auto-regressively factorized its as:\npθ(Y |X) = n∏ t=1 pθ(yt|H, y1, ..., yt−1) (1)\nAt each time step t, the probability of the next token is computed by a softmax classifier:\npθ(yt|H, y1, ...., yt−1)) = softmax(ot) (2)\nwhere ot is logit vector outputted by decoder network. The standard S2S model without discriminator makes the output sequence Y the same as the input sequence X .\nDiscriminator Model By teacher forcing, S2S tends to ignore the style labels and collapses to a reconstruction model, which might copy the input sentence, hence failing to transfer the style. Therefore, to make the model learn meaningful style information, we apply a style discriminator γ for the style regularization. In summary, we use a style discriminator to provide the direction (gradient) for TST to conform to the target style. Our discriminator is a multi-layer perceptron with a sigmoid\nAlgorithm 1 ATM Pre-traing Learning Input: sequence-to-sequence model fθ ,discriminator γ,and a dataset Di with style li belong to Ls Output: well-trained parameter θ, γ 1: Sample a batch of m sentences X1, X2, ...Xm from Di. 2: while in first stage and not convergence do 3: Use fθ to generate new sentence 4: Yi = fθ(Xi, li) 5: Compute Lcls(γ) for Yi by Eq. (4) ; 6: Compute Lrec(θ) for Yi by Eq. (3) ;\nactivation function to predict style labels or guide the direction of style transfer. Our model training involves a pre-training learning strategy and a domain adaptive meta-learning strategy."
    }, {
      "heading" : "3.2.3 First Stage: Pre-training Learning",
      "text" : "In the first stage, we train the discriminator model to distinguish different text styles. In this stage, the discriminator models are equivalent to a text classifier. Inspired by (Lewis et al., 2019), we feed the hidden states from the last layer of the decoder into the classifier instead of the gumblesoftmax trick (Jang et al., 2017) for gradient backpropagation, which is more stable and better than gumble-softmax(See Table 5). The loss function for the discriminator is simply the cross-entropy loss of the classification problem:\nLcls(γ) = − E Xi∼DS [logP (li|Xi, li; θ, γ)] (3)\nFor the S2S model, we pre-train the S2S model to allow the generation model to learn to copy an input sentence X using teacher forcing. The loss function of the sequence-to-sequence model minimizes the negative log-likelihood of the training data:\nLrec(θ) = − E Xi∼DS [logP (Yi|Xi; θ)] (4)\nIn summary, we train the sequence model and the style classification model separately on the source domain to learn content preservation and style discrimination in the first stage. The first stage training procedure of the ATM is summarized in Algorithm 1."
    }, {
      "heading" : "3.2.4 Second Stage: Domain Adaptive Meta Learning with Adversarial Training",
      "text" : "After the first stage of training, the style classifier has learned how to distinguish between different text styles. For style controlling, we adopt a method\nof adversarial training to avoid disentangling the content and style in the latent space. The discriminator model aims to minimize the negative loglikelihood of opposite style l̃i when feeding to the sequence model sentence Xi with the style label li. In the second stage, we freeze the parameters of the discriminator. Therefore, style loss only works on the S2S model θ, which forces the S2S model θ to generate opposite styles of sentences:\nLstyle(θ) = − E Xi∼D [logP (l̃i|Xi, li; θ, γ)] (5)\nIn the second stage, we use the DAML algorithm for domain adaptive TST, so the text reconstruction loss and the style discriminator loss are calculated over the meta-training samples in task Ti from Dtr. These two losses can be written as\nLrecTi (θ) = − E Xi∼Ti [logP (Yi|Xi; θ)]\nLstyleTi (θ) = − EXi∼Ti [logP (l̃i|Xi, li; θ, γ))\n(6)\nWe add different prefixes to the input in the second stage, which allows the S2S model to perceive different TST tasks. The second stage of the algorithm is called domain adaptive meta-strategy, which consists of two core phases: a meta-training phase and a meta-validation phase, as shown in Figure 3.\nDomain Adaptive Meta-Training. In the meta-training phase, our objective is to learn different domain-specific temporary models for each domain that are capable of learning the general knowledge of each domain. Inspired by feature-critic networks (Li et al., 2019b), we use a similar manner to adapt the parameters of the domain-specific temporary model:\nθoldi = θi−1 − α∇θi−1LrecTi (θi−1, γi−1)\nθnewi = θ old i−1 − α∇θi−1L style Ti\n(θi−1, γi−1) (7)\nwhere i is the adaptation step in the inner loop, and α is the learning rate of the internal optimization. At each adaptation step, the gradients are calculated with respect to the parameters from the previous step. For each domain of Dtr, it has different θold and θnew . The base model parameters θ0 should not be changed in the inner loop.\nAlgorithm 2 The training procedure of DAMLATM Input: D = {D1, ...,DK}, α, β Output: optimal meta-learned model θ 1: Initialize the base sequence-to-sequence model θ and dis-\ncriminator model γ by algorithm 1 2: while not converge do 3: Randomly splitD = Dtr∪Dval andDtr∩Dval = ∅ 4: Meta-training: 5: for j in meta batches do //Outer loop 6: Sample a task Tj from Dval 7: for i in adaptation steps do //Inner loop 8: Sample a task Ti from Dtr 9: Compute meta-training rec loss LrecTi 10: Compute meta-training style loss LstyleTj 11: Compute adapted parameters with gradient\ndescent for θi−1 12: θoldi = θi−1 − α∇θi−1LtrTi(θi−1, γi−1) 13: θnewi = θ old i−1 − α∇θi−1LstyleTi (θi−1, γi−1) 14: Meta-validation: 15: Compute meta-validation loss on Tj : LvalTj 16: Meta-optimization: 17: Perform gradient step w.r.t. θ 18: θ0 = θ0 − β∇θ0ETjLvalTj (θ old i , θ new i , γ)\nDomain Adaptive Meta-Validation After meta-training phase, DAML-ATM has already learned a temporary model(θoldi , θ new i ) in the meta-training domains Dtr. The meta-validation phase tries to minimize the distribution divergence between the source domains Dtr and simulated target domains Dval using the learned temporary model. In the meta-validation phase, each temporary model is calculated on the meta-validation domain Dval to get meta validation losses.\nLvalTj = L rec Tj (θ old i , γ0) + L style Tj (θnewi , γ0) (8)\nThus, the base model θ is updated by gradient descent\nθ0 = θ0 − β∇θ0LvalTj (9)\nwhere β is the meta-learning rate. Unlike the ordinary gradient descent process, the update mechanism of Eq. (9) involves updating one gradient by another gradient (w.r.t. the parameters of the temporary model). This process requires a second-order optimization partial derivative."
    }, {
      "heading" : "3.2.5 Final Evaluation Phase of DAML-ATM",
      "text" : "In the final evaluation phase, we first initialize the model with the parameters learned during the above algorithm 2. Then, the model takes input as a new adaptation task T , which consists of a small indomain data Str for fine-tuning the model and a test set Ste for testing. The procedure is summarized in Algorithm 3. (Note that the discriminator is not needed for inference.)\nAlgorithm 3 The Final Evaluation Procedure of DAML-ATM Input: θ, γ learned from Algorithm 2, low resource training set Str and test set Ste of an unseen domain Dnew Output: Performance on Ste 1: while not convergence do 2: Serialize a task Ttr from the unseen domain Str 3: Update θ = θ − β∇θ ∑ Ttr (LrecTtr (θ) + L style Ttr (θ))\n4: return optimal θ∗ for Ste 5: Style accuracy, bleu, domain accuracy = fTte(θ)"
    }, {
      "heading" : "4 Experiment",
      "text" : "In this section, we first detail the experimental setups. Then, we present our experimental results over multiple target domains."
    }, {
      "heading" : "4.1 Datasets and Experimental Setups",
      "text" : "In this experiment, we use the following four datasets from different domains: (i) IMDB movie review corpus (Diao et al., 2014). (ii) Yelp restaurant review dataset (Li et al., 2018). (iii) Amazon product review dataset (Li et al., 2018). (iv)\nYAHOO! Answers dataset (Li et al., 2019a), the amazon and yelp test sets each have 1k human annotations.The statistics of these corpora are summarized in Table 1.\nFor the S2S model, we take the T5 base model (Raffel et al., 2019) (220MB) for our experiments. For style discriminator, we use 4-layer fully connected neural networks. We train our framework using the Adam optimizer (Kingma and Ba, 2014)with the initial learning rate 1e-5. The epoch is set to 50 for both stage 1 and stage 2. The inner learning rate α is 0.0001, and the outer learning rate β is 0.001. Following (Shankar et al., 2018; Li et al., 2020), we use the leave-one-out evaluation method by picking a domain as the target domain Dnew for the final evaluation. For each iteration of the training phase, two source domains are randomly selected as the meta-training domain Dtr and the remaining domains as the meta-validation domain Dval.\nIn order to evaluate the model performance, we use three famous and widely adopted automatic metrics following previous work (Li et al., 2019a; Fu et al., 2017; Hu et al., 2017a) and a human metric. BLEU verifies whether the generated sentences retain the original content (Papineni et al., 2002). While IMDB and Amazon have no manual references, we compute the BLEU scores w.r.t the input sentences. Style Control (S-Acc) measures the style accuracy of the transferred sentences with a style classifier that is pre-trained on the datasets. Domain Control (D-Acc) verifies whether the generated sentences have the characteristics of the target domain with a pre-trained domain classifier to measure the percentage of generated sentences belonging to the target domain. Human Evaluation Following (Madotto et al., 2019), We randomly sampled 100 sentences generated on the target domain and distributed a questionnaire at Amazon Mechanical Turk asking each worker to rank the content retention (0 to 5), style transfer(0 to 5 ) and fluency(0 to 5): human score = Average( ∑ scorestyle + ∑ scorecontent +∑\nscorefluency), human score ∈ [0, 100] . Five workers were recruited for human evaluation. The results of the other metrics are shown in the appendix."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "In our experiments, for ATM model, we adopt five state-of-the-art TST models for comparison:\nCrossAlign (Shen et al., 2017), ControlGen (Hu et al., 2017a), DAST (Li et al., 2019a), CatGen (Wang et al., 2020a) and FGIM (Wang et al., 2019). They are jointly trained on the source domains and fine-tuned on the target domain.\nTo well analyze our training method DAML, following (Li et al., 2020), we also use five simple and effective domain adaptation settings with ControlGen (Hu et al., 2017a) structure as DAML: (1) In-Domain method is trained on the training set of the target domain; (2) Joint-Training method combines all the training sets of the source and\ntarget domains and performs a joint-training on these datasets; (3) Fine-Tuning method is trained on the training sets of the source domains and then fine-tuned on the training set of the target domain; (4) D-Shift This is trained on the combination of training sets from all source domains. Then, the evaluation is conducted on the test set of a target domain using the direct domain shift strategy; (5) MAML method uses classical model agnostic meta-learning algorithm (Finn et al., 2017)."
    }, {
      "heading" : "4.3 Results and Analysis",
      "text" : "For DAML-ATM, we first choose restaurant as the target domain and the other three as the source domains for observation. Table 2 reports the results\nof different methods and models under both the full-data and few-shot settings. From this table, we can see that DAML-ATM outperforms all baselines in terms of S-Acc, BLEU, D-Acc and human evaluation. We attribute this to the fact that DAMLATM explicitly simulates the domain shift during training via DAML, which helps adapt to the new target domain. We can also see that in the case of a few-shot setting, the results of Fine-tuning and Joint training are even worse than In-domain and DAML. The reason may be that the data size of the source domain is much larger than the target domain so that the model tends to remember the characteristics of the source domain. MAML achieves good performance in most metrics. However, it does not balance meta-tasks across different source domains, performing poorly on D-acc.\nFurther, to verify the robustness of our method under the low-resource setting, we separately select the other three domains as the target domain. As shown in Table 4, our approach has achieved good performance on different target domains.\nWe also provide some examples in Table 3. From the example, we can see intuitively that D-shift and Fine-tuning will lead to the misuse of domainspecific words due to lack of target domain information. In addition, compared with Joint-training, the sentences generated by DAML-ATM are more consistent with the human reference. Compared to MAML, DAML generates sentences that are more diverse and vivid due to the more balanced absorp-\ntion of information from multiple domains. Figure 4 shows the system performance positively correlates with the amount of training data available in the target domain. To visualize how well DAMLATM performs on the new unseen domain, we use t-SNE (Van der Maaten and Hinton, 2008) plots to analyze the degree of separation between the source domain sentences and the generated target domain sentences. Figure 5 shows that as the training epoch increases, the sentences generated by DAML-ATM in the target domain are completely separated from the source domain in the latent space."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "To study the impact of different components on the overall performance, we further did an ablation study for our model, and the results are shown in Table 5. After we disabled the reconstruction loss, our model failed to learn meaningful outputs and only learned to generate a word for any combination of input sentences and styles. Then, when the discriminator loss is not used, the model degrades rapidly, simply copying the original sentence without any style modification. After not using the pre-training language model weights, the model’s performance is reduced in the metric of content preservation. When using gumble-softmax instead of hidden states for gradient descent, the model performs poorly in style accuracy because of the instability of gumble-softmax. In summary, each factor plays an essential role in the DAML-ATM training stage."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose DAML-ATM, a novel training strategy combined with a new TST model for domain adaptation, which can be easily adapted to new domains with few shot data. On four popular TST benchmarks, we found significant improvements against multiple baselines, verifying the effectiveness of our method. We explore extending this approach for other low resource NLP tasks in future work."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 More Details on Experiment Setups\nOur model is initialized from T5 and Bart (Liu et al., 2020; Raffel et al., 2019). Specifically, the encoder and decoder are all 12-layer transformers with 16 attention heads, hidden size 1,024 and feedforward filter size 4,096, which amounts to 406M trainable parameters. We train our framework using the Adam optimizer (Kingma and Ba, 2014)with the initial learning rate 1e-5, and we employ a linear schedule for the learning rate, all models are trained on 8 RTX 3090 GPUs.\nA.2 Details on Human Evaluation\nFor the results generated by each method, following (Krishna et al., 2020), we randomly selected 100 sentences to be placed in the Amazon Mechanical Turk1 questionnaire. We pay our workers 5 cents per sentence. As shown in Figure 6, the questionnaire asked to judge the generated sentences on three dimensions: strength of style transfer, degree of content retention, and text fluency. To minimize the impact of spamming, we require each worker to be a native English speaker with a 95% or higher approval rate and a minimum of 1,000 hits.\n1https://www.mturk.com/\nA.3 More Ablation Study and Metrics\nTo verify that the general S2S models work well with our algorithm, we use bart (Lewis et al., 2019) as the S2S base model. For the robustness of the experiment, we add a new metric J-(a,c,f) (Krishna et al., 2020) to measure our results, which is a sentence-level aggregation strategy evaluate style transfer models.\nAs can be seen from Table 6, our approach can be combined with other general pre-trained language models and performs well, proving our method’s generality. Furthermore, as we can visually see from Table 7, our model also performs well on the J-(a,c,f) metric, which indicates that our model generates sentences in a specific style while having the right target style, preserving content, and being fluent.\nA.4 Shakespeare Style Generation\nTo demonstrate the robustness of our algorithm, we trained our algorithm on the Shakespeare dataset (Xu et al., 2012). We choose three of these plays, Hamlet, Macbeth, and Othello, as different domains. The results are shown in Table 8 and\nTable 11. As shown in Table 8, our approach achieves good results on the more difficult Shakespearean style transfer, which indicates that our algorithm generalizes nicely to other complex style transfer tasks.\nA.5 More Generation Examples To demonstrate more examples of generation to verify the effectiveness of the model, we selected 10 generated sentences from amazon and yelp each, as shown in Table 9 and Table 10."
    } ],
    "references" : [ {
      "title" : "Expertise style transfer: A new task towards better communication between experts and laymen",
      "author" : [ "Yixin Cao", "Ruihao Shui", "Liangming Pan", "Min-Yen Kan", "Zhiyuan Liu", "Tat-Seng Chua." ],
      "venue" : "arXiv preprint arXiv:2005.00701.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating prose style transfer with the bible",
      "author" : [ "Keith Carlson", "Allen Riddell", "Daniel Rockmore." ],
      "venue" : "Royal Society open science, 5(10):171920.",
      "citeRegEx" : "Carlson et al\\.,? 2018",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2018
    }, {
      "title" : "StΘ2: Smalldata text style transfer via multi-task meta-learning",
      "author" : [ "Xiwen Chen", "Kenny Q Zhu." ],
      "venue" : "arXiv preprint arXiv:2004.11742.",
      "citeRegEx" : "Chen and Zhu.,? 2020",
      "shortCiteRegEx" : "Chen and Zhu.",
      "year" : 2020
    }, {
      "title" : "Style transformer: Unpaired text style transfer without disentangled latent representation",
      "author" : [ "Ning Dai", "Jianze Liang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1905.05621.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)",
      "author" : [ "Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J Smola", "Jing Jiang", "Chong Wang." ],
      "venue" : "Proceedings of the 20th ACM SIGKDD international conference on",
      "citeRegEx" : "Diao et al\\.,? 2014",
      "shortCiteRegEx" : "Diao et al\\.",
      "year" : 2014
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "arXiv preprint arXiv:1703.03400.",
      "citeRegEx" : "Finn et al\\.,? 2017",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Style transfer in text: Exploration and evaluation",
      "author" : [ "Zhenxin Fu", "Xiaoye Tan", "Nanyun Peng", "Dongyan Zhao", "Rui Yan" ],
      "venue" : null,
      "citeRegEx" : "Fu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2017
    }, {
      "title" : "Structuring latent spaces for stylized response generation",
      "author" : [ "Xiang Gao", "Yizhe Zhang", "Sungjin Lee", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1909.05361.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio." ],
      "venue" : "ICML.",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Meta-learning for lowresource neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Yun Chen", "Kyunghyun Cho", "Victor O.K. Li" ],
      "venue" : null,
      "citeRegEx" : "Gu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P. Xing" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing." ],
      "venue" : "International Conference on Machine Learning, pages 1587–1596. PMLR.",
      "citeRegEx" : "Hu et al\\.,? 2017b",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised controllable text formalization",
      "author" : [ "Parag Jain", "Abhijit Mishra", "Amar Prakash Azad", "Karthik Sankaranarayanan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6554–6561.",
      "citeRegEx" : "Jain et al\\.,? 2019",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2019
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole" ],
      "venue" : null,
      "citeRegEx" : "Jang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Shakespearizing modern language using copy-enriched sequence-to-sequence models",
      "author" : [ "Harsh Jhamtani", "Varun Gangal", "Eduard Hovy", "Eric Nyberg." ],
      "venue" : "arXiv preprint arXiv:1707.01161.",
      "citeRegEx" : "Jhamtani et al\\.,? 2017",
      "shortCiteRegEx" : "Jhamtani et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Fewshot controllable style transfer for low-resource settings: A study in indian languages",
      "author" : [ "Kalpesh Krishna", "Deepak Nathani", "Xavier Garcia", "Bidisha Samanta", "Partha Talukdar" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2021
    }, {
      "title" : "Reformulating unsupervised style transfer as paraphrase generation",
      "author" : [ "Kalpesh Krishna", "John Wieting", "Mohit Iyyer" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2020
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain adaptive text style transfer",
      "author" : [ "Dianqi Li", "Yizhe Zhang", "Zhe Gan", "Yu Cheng", "Chris Brockett", "Ming-Ting Sun", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1908.09395.",
      "citeRegEx" : "Li et al\\.,? 2019a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Metaner: Named entity recognition with meta-learning",
      "author" : [ "Jing Li", "Shuo Shang", "Ling Shao." ],
      "venue" : "Proceedings of The Web Conference 2020, pages 429–440.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1804.06437.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "2019b. Feature-critic networks for heterogeneous domain generalization",
      "author" : [ "Yiying Li", "Yongxin Yang", "Wei Zhou", "Timothy M. Hospedales" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Personalizing dialogue agents via meta-learning",
      "author" : [ "Zhaojiang Lin", "Andrea Madotto", "Chien-Sheng Wu", "Pascale Fung" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Personalizing dialogue agents via meta-learning",
      "author" : [ "Andrea Madotto", "Zhaojiang Lin", "Chien-Sheng Wu", "Pascale Fung." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5454–5459.",
      "citeRegEx" : "Madotto et al\\.,? 2019",
      "shortCiteRegEx" : "Madotto et al\\.",
      "year" : 2019
    }, {
      "title" : "Large-scale hierarchical alignment for data-driven text rewriting",
      "author" : [ "Nikola I Nikolov", "Richard HR Hahnloser." ],
      "venue" : "arXiv preprint arXiv:1810.08237.",
      "citeRegEx" : "Nikolov and Hahnloser.,? 2018",
      "shortCiteRegEx" : "Nikolov and Hahnloser.",
      "year" : 2018
    }, {
      "title" : "Polite dialogue generation without parallel data",
      "author" : [ "Tong Niu", "Mohit Bansal." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:373–389.",
      "citeRegEx" : "Niu and Bansal.,? 2018",
      "shortCiteRegEx" : "Niu and Bansal.",
      "year" : 2018
    }, {
      "title" : "Zero-shot cross-lingual transfer with meta learning",
      "author" : [ "Farhad Nooralahzadeh", "Giannis Bekoulis", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "arXiv preprint arXiv:2003.02739.",
      "citeRegEx" : "Nooralahzadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Nooralahzadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Automatically neutralizing subjective bias in text",
      "author" : [ "Reid Pryzant", "Richard Diehl Martinez", "Nathan Dass", "Sadao Kurohashi", "Dan Jurafsky", "Diyi Yang." ],
      "venue" : "Proceedings of the aaai conference on artificial intelligence, volume 34, pages 480–489.",
      "citeRegEx" : "Pryzant et al\\.,? 2020",
      "shortCiteRegEx" : "Pryzant et al\\.",
      "year" : 2020
    }, {
      "title" : "Domain adaptive dialog generation via meta learning",
      "author" : [ "Kun Qian", "Zhou Yu." ],
      "venue" : "arXiv preprint arXiv:1906.03520.",
      "citeRegEx" : "Qian and Yu.,? 2019",
      "shortCiteRegEx" : "Qian and Yu.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Textsettr: Few-shot text style extraction and tunable targeted restyling",
      "author" : [ "Parker Riley", "Noah Constant", "Mandy Guo", "Girish Kumar", "David Uthus", "Zarana Parekh" ],
      "venue" : null,
      "citeRegEx" : "Riley et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Riley et al\\.",
      "year" : 2021
    }, {
      "title" : "Generalizing across domains via cross-gradient training",
      "author" : [ "Shiv Shankar", "Vihari Piratla", "Soumen Chakrabarti", "Siddhartha Chaudhuri", "Preethi Jyothi", "Sunita Sarawagi." ],
      "venue" : "arXiv preprint arXiv:1804.10745.",
      "citeRegEx" : "Shankar et al\\.,? 2018",
      "shortCiteRegEx" : "Shankar et al\\.",
      "year" : 2018
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Advances in neural information processing systems, pages 6830–6841.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Adapting language models for non-parallel author-stylized rewriting",
      "author" : [ "Bakhtiyar Syed", "Gaurav Verma", "Balaji Vasan Srinivasan", "Anandhavelu Natarajan", "Vasudeva Varma." ],
      "venue" : "AAAI, pages 9008– 9015.",
      "citeRegEx" : "Syed et al\\.,? 2020",
      "shortCiteRegEx" : "Syed et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(11).",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Controllable unsupervised text attribute transfer via editing entangled latent representation",
      "author" : [ "Ke Wang", "Hang Hua", "Xiaojun Wan" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Instance weighting for neural machine translation domain adaptation",
      "author" : [ "Rui Wang", "Masao Utiyama", "Lemao Liu", "Kehai Chen", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Cat-gen: Improving robustness in nlp models via controlled adversarial text generation",
      "author" : [ "Tianlu Wang", "Xuezhi Wang", "Yao Qin", "Ben Packer", "Kang Li", "Jilin Chen", "Alex Beutel", "Ed Chi." ],
      "venue" : "arXiv preprint arXiv:2010.02338.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Formality style transfer with shared latent space",
      "author" : [ "Yunli Wang", "Yu Wu", "Lili Mou", "Zhoujun Li", "Wenhan Chao." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2236–2249.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhanced meta-learning for cross-lingual named entity recognition with minimal resources",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Guoxin Wang", "Hui Chen", "Börje F Karlsson", "Biqing Huang", "Chin-Yew Lin." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial In-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Paraphrasing for style",
      "author" : [ "Wei Xu", "Alan Ritter", "Bill Dolan", "Ralph Grishman", "Colin Cherry." ],
      "venue" : "COLING, pages 2899–2914.",
      "citeRegEx" : "Xu et al\\.,? 2012",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Adversarially regularized autoencoders",
      "author" : [ "Jake Zhao", "Yoon Kim", "Kelly Zhang", "Alexander M. Rush", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Text style transfer (TST) aims to change the style of the input text and keep its content unchanged, which has been applied successfully to text formalization (Jain et al., 2019) , text rewriting (Nikolov and Hahnloser, 2018) , personalized dialogue generation (Niu and Bansal, 2018) and other stylized text generation tasks (Gao et al.",
      "startOffset" : 159,
      "endOffset" : 178
    }, {
      "referenceID" : 26,
      "context" : ", 2019) , text rewriting (Nikolov and Hahnloser, 2018) , personalized dialogue generation (Niu and Bansal, 2018) and other stylized text generation tasks (Gao et al.",
      "startOffset" : 25,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : ", 2019) , text rewriting (Nikolov and Hahnloser, 2018) , personalized dialogue generation (Niu and Bansal, 2018) and other stylized text generation tasks (Gao et al.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : ", 2019) , text rewriting (Nikolov and Hahnloser, 2018) , personalized dialogue generation (Niu and Bansal, 2018) and other stylized text generation tasks (Gao et al., 2019; Cao et al., 2020; Syed et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 209
    }, {
      "referenceID" : 0,
      "context" : ", 2019) , text rewriting (Nikolov and Hahnloser, 2018) , personalized dialogue generation (Niu and Bansal, 2018) and other stylized text generation tasks (Gao et al., 2019; Cao et al., 2020; Syed et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 209
    }, {
      "referenceID" : 36,
      "context" : ", 2019) , text rewriting (Nikolov and Hahnloser, 2018) , personalized dialogue generation (Niu and Bansal, 2018) and other stylized text generation tasks (Gao et al., 2019; Cao et al., 2020; Syed et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017; Wang et al., 2020b; Pryzant et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 167
    }, {
      "referenceID" : 41,
      "context" : "Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017; Wang et al., 2020b; Pryzant et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : "Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017; Wang et al., 2020b; Pryzant et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 167
    }, {
      "referenceID" : 44,
      "context" : "The recent surge of deep generative methods (Hu et al., 2017a; Zhao et al., 2017; Li et al., 2018) has spurred progress in text style transfer without parallel data.",
      "startOffset" : 44,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "The recent surge of deep generative methods (Hu et al., 2017a; Zhao et al., 2017; Li et al., 2018) has spurred progress in text style transfer without parallel data.",
      "startOffset" : 44,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "as an effective solution to address the above data insufficiency issue (Glorot et al., 2011; Wang et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 111
    }, {
      "referenceID" : 39,
      "context" : "as an effective solution to address the above data insufficiency issue (Glorot et al., 2011; Wang et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : "However, directly leveraging large amounts of data from other domains for the TST task is problematic due to the differences in data distribution over different domains, as different domains usually use their domain-specific lexica (Li et al., 2019a).",
      "startOffset" : 232,
      "endOffset" : 250
    }, {
      "referenceID" : 23,
      "context" : "Recently, model-agnostic meta-learning (MAML) has received resurgence in the context of few-shot learning scenario (Lin et al., 2019; Gu et al., 2018; Nooralahzadeh et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "Recently, model-agnostic meta-learning (MAML) has received resurgence in the context of few-shot learning scenario (Lin et al., 2019; Gu et al., 2018; Nooralahzadeh et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 178
    }, {
      "referenceID" : 28,
      "context" : "Recently, model-agnostic meta-learning (MAML) has received resurgence in the context of few-shot learning scenario (Lin et al., 2019; Gu et al., 2018; Nooralahzadeh et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 178
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the essence of MAML (Finn et al., 2017), we propose a new meta-learning training strategy named domain adaptive meta-learning (DAML).",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Usually, if a TST model tries to decouple style information from the semantics of a text, it tends to produce content loss during style transfer (Hu et al., 2017b; Dai et al., 2019; Carlson et al., 2018).",
      "startOffset" : 145,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "Usually, if a TST model tries to decouple style information from the semantics of a text, it tends to produce content loss during style transfer (Hu et al., 2017b; Dai et al., 2019; Carlson et al., 2018).",
      "startOffset" : 145,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "Usually, if a TST model tries to decouple style information from the semantics of a text, it tends to produce content loss during style transfer (Hu et al., 2017b; Dai et al., 2019; Carlson et al., 2018).",
      "startOffset" : 145,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "(Hu et al., 2017a; Fu et al., 2017; Li et al., 2019a)assume that appropriate style regularization can achieve the separation.",
      "startOffset" : 0,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : "(Hu et al., 2017a; Fu et al., 2017; Li et al., 2019a)assume that appropriate style regularization can achieve the separation.",
      "startOffset" : 0,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "(Dai et al., 2019)use the transformer architecture language model to introduce attention mechanism, but they do not make full use of the prior",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "knowledge of sequence to sequence pre-trained language model, such as Bart (Lewis et al., 2019) and T5 (Raffel et al.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 32,
      "context" : ", 2019) and T5 (Raffel et al., 2019), which have made significant progress in text generation tasks.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "Domain adaptation has been studied in various natural language processing tasks (Glorot et al., 2011; Qian and Yu, 2019; Wang et al., 2017).",
      "startOffset" : 80,
      "endOffset" : 139
    }, {
      "referenceID" : 31,
      "context" : "Domain adaptation has been studied in various natural language processing tasks (Glorot et al., 2011; Qian and Yu, 2019; Wang et al., 2017).",
      "startOffset" : 80,
      "endOffset" : 139
    }, {
      "referenceID" : 39,
      "context" : "Domain adaptation has been studied in various natural language processing tasks (Glorot et al., 2011; Qian and Yu, 2019; Wang et al., 2017).",
      "startOffset" : 80,
      "endOffset" : 139
    }, {
      "referenceID" : 19,
      "context" : "However, there is no recent work about domain adaptation for a TST, except DAST (Li et al., 2019a).",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 33,
      "context" : "Although there are some methods perform well in few shot data transfer (Riley et al., 2021; Krishna et al., 2021), these methods discuss completely new text style transfer, while we focus on the domain adaptation issue.",
      "startOffset" : 71,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "Although there are some methods perform well in few shot data transfer (Riley et al., 2021; Krishna et al., 2021), these methods discuss completely new text style transfer, while we focus on the domain adaptation issue.",
      "startOffset" : 71,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : "Model-agnostic meta-learning (MAML) (Finn et al., 2017) provides a general method to adapt to parameters in different domains.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 31,
      "context" : "Although there have been some researches (Qian and Yu, 2019; Li et al., 2020; Wu et al., 2020) on MAML in natural language processing, it is still scarce compared to computer vision.",
      "startOffset" : 41,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "Although there have been some researches (Qian and Yu, 2019; Li et al., 2020; Wu et al., 2020) on MAML in natural language processing, it is still scarce compared to computer vision.",
      "startOffset" : 41,
      "endOffset" : 94
    }, {
      "referenceID" : 42,
      "context" : "Although there have been some researches (Qian and Yu, 2019; Li et al., 2020; Wu et al., 2020) on MAML in natural language processing, it is still scarce compared to computer vision.",
      "startOffset" : 41,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "However, since it is based on the assumption that the meta tasks are from the same distribution (Figure 1, left), simply feeding all the sources data into it might get sub-optimal results (Chen and Zhu, 2020).",
      "startOffset" : 188,
      "endOffset" : 208
    }, {
      "referenceID" : 18,
      "context" : "Inspired by (Lewis et al., 2019), we feed the hidden states from the last layer of the decoder into the classifier instead of the gumblesoftmax trick (Jang et al.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : ", 2019), we feed the hidden states from the last layer of the decoder into the classifier instead of the gumblesoftmax trick (Jang et al., 2017) for gradient backpropagation, which is more stable and better than gumble-softmax(See Table 5).",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "In this experiment, we use the following four datasets from different domains: (i) IMDB movie review corpus (Diao et al., 2014).",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "(ii) Yelp restaurant review dataset (Li et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "(iii) Amazon product review dataset (Li et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : "(iv) YAHOO! Answers dataset (Li et al., 2019a), the amazon and yelp test sets each have 1k human annotations.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 32,
      "context" : "For the S2S model, we take the T5 base model (Raffel et al., 2019) (220MB) for our experiments.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "We train our framework using the Adam optimizer (Kingma and Ba, 2014)with the initial learning rate 1e-5.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 34,
      "context" : "Following (Shankar et al., 2018; Li et al., 2020), we use the leave-one-out evaluation method by picking a domain as the target domain Dnew for the final evaluation.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Following (Shankar et al., 2018; Li et al., 2020), we use the leave-one-out evaluation method by picking a domain as the target domain Dnew for the final evaluation.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "In order to evaluate the model performance, we use three famous and widely adopted automatic metrics following previous work (Li et al., 2019a; Fu et al., 2017; Hu et al., 2017a) and a human metric.",
      "startOffset" : 125,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : "In order to evaluate the model performance, we use three famous and widely adopted automatic metrics following previous work (Li et al., 2019a; Fu et al., 2017; Hu et al., 2017a) and a human metric.",
      "startOffset" : 125,
      "endOffset" : 178
    }, {
      "referenceID" : 29,
      "context" : "BLEU verifies whether the generated sentences retain the original content (Papineni et al., 2002).",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 25,
      "context" : "Human Evaluation Following (Madotto et al., 2019), We randomly sampled 100 sentences generated on the target domain and distributed a questionnaire at Amazon Mechanical Turk asking each worker to rank the content retention (0 to 5), style transfer(0 to 5 ) and fluency(0 to 5): human score =",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : ", 2017a), DAST (Li et al., 2019a), CatGen (Wang et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 40,
      "context" : ", 2019a), CatGen (Wang et al., 2020a) and FGIM (Wang et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "To well analyze our training method DAML, following (Li et al., 2020), we also use five simple and effective domain adaptation settings with ControlGen (Hu et al.",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "get domain using the direct domain shift strategy; (5) MAML method uses classical model agnostic meta-learning algorithm (Finn et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 140
    } ],
    "year" : 0,
    "abstractText" : "Text style transfer (TST) without parallel data has achieved some practical success. However, most of the existing unsupervised text style transfer methods suffer from (i) requiring massive amounts of nonparallel data to guide transferring different text styles. (ii) colossal performance degradation when fine-tuning the model in new domains. In this work, we propose DAML-ATM(Domain Adaptive MetaLearning with Adversarial Transfer Model), which consists of two parts, DAML and ATM. DAML is a domain adaptive metalearning approach to refine general knowledge in multi-heterogeneous source domains, capable of adapting to new unseen domains with a small amount of data. Moreover, we propose a new unsupervised TST approach Adversarial Transfer Model (ATM), composed of a sequence-to-sequence pre-trained language model and uses adversarial style training for better content preservation and style transfer. Results on multi-domain datasets demonstrate that our approach generalizes well on unseen low-resource domains, achieving state-of-theart results against ten-strong baselines.",
    "creator" : null
  }
}
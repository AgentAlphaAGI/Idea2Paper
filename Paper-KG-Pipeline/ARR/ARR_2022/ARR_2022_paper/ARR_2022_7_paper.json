{
  "name" : "ARR_2022_7_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dense retrieval uses dense vectors to represent documents and retrieve documents by similarity scores between query vectors and document vectors. Different from cross-encoders (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020) or late-interaction models (Khattab and Zaharia, 2020; Gao et al., 2021a), which predict a match score for each query-document pair thus are computationally costly, dense retrieval can be run in milliseconds, with the help of an approximate nearest neighbor (ANN) retrieval library, e.g., FAISS (Johnson et al., 2021).\nAs a drawback, dense retrieval models often require large supervised datasets like MS-\n1We will make all code and model weights publicly available upon acceptance.\nMARCO (Nguyen et al., 2016) (533k training examples) or NQ (Kwiatkowski et al., 2019) (133k training examples) for training. Unfortunately, Thakur et al. (2021) empirically show that models trained on one dataset suffer from an out-of-domain (OOD) problem when transferring to another. This hinders the applications of dense retrieval systems. On the other hand, creating a large supervised training dataset for dense retrieval is time-consuming and expensive. For many low-resource languages, there is even no existing supervised dataset for retrieval and it can be extremely difficult to construct one.\nThe recently proposed BEIR benchmark (Thakur et al., 2021) highlights the generalization ability of text retrieval systems. The benchmark features a setting where models are trained on a large supervised dataset MS-MARCO (Nguyen et al., 2016) and then tested on 18 heterogeneous datasets of 9 tasks. In this paper, we propose Large-scale Pretrained Dense Zero-shot Retriever (LaPraDoR), a fully unsupervised pretrained retriever for zero-shot text retrieval. While existing dense retrievers need large supervised data and struggle to compete with a lexical matching approach like BM25 (Robertson and Zaragoza, 2009) for zero-shot retrieval, we take a different approach by complementing lexical matching with semantic matching. Without any supervised data, LaPraDoR outperforms all dense retrievers on BEIR. LaPraDoR achieves state-of-the-art performance on BEIR with a further fine-tuning, outperforming re-ranking, despite being 22.5× and 42× faster on GPU and CPU, respectively.\nTraining LaPraDoR faces two challenges: (1) Training Efficiency. For large-scale pretraining, training efficiency can be important. In contrastive learning, more negative instances often lead to better performance (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b). However, traditional inbatch negative sampling is bottlenecked by limited\nGPU memory. To alleviate this problem, we propose Iterative Contrastive Learning (ICoL), which iteratively trains the query and document encoders with a cache mechanism. Compared to existing solutions MoCo (He et al., 2020) and xMoCo (Yang et al., 2021), ICoL does not introduce extra encoders and can solve the mismatching between representation spaces, thus demonstrating superior performance. (2) Versatility. There are different types of downstream tasks from various domains in both BEIR and real-world applications. We use a large-scale multi-domain corpus, C4 (Raffel et al., 2020), to train our LaPraDoR model. To make LaPraDoR versatile, besides conventional querydocument retrieval, we also incorporate documentquery, query-query, and document-document retrieval into the pretraining objective. We further share the weights between the query and document encoders and obtain an all-around encoder that fits all retrieval tasks.\nTo summarize, our contribution is three-fold: (1) We train LaPraDoR, an all-around unsupervised pretrained dense retriever that achieves state-ofthe-art performance on the BEIR benchmark. (2) We propose Iterative Contrastive Learning (ICoL) for training a retrieval model effectively. (3) We propose Lexicon-Enhanced Dense Retrieval as an efficient way for combining BM25 with a dense retriever, compared to the widely-used re-ranking paradigm."
    }, {
      "heading" : "2 Related Work",
      "text" : "Dense Retrieval DPR (Karpukhin et al., 2020) initializes a bi-encoder model with BERT (Devlin et al., 2019) and achieves better results than earlier dense retrieval methods. RocketQA (Qu et al., 2021) exploits a trained retriever to mine hard negatives and then re-train a retriever with the mined negatives. ANCE (Xiong et al., 2021) dynamically mines hard negatives throughout training but requires periodic encoding of the entire corpus. TASB (Hofstätter et al., 2021) is a bi-encoder trained with balanced topic-aware sampling and knowledge distillation from a cross-encoder and a ColBERT model (Khattab and Zaharia, 2020), in addition to in-batch negatives. xMoCo (Yang et al., 2021) adapt MoCo (He et al., 2020), a contrastive learning algorithm that is originally proposed for image representation, to text retrieval by doubling its fast and slow encoders. Although these dense retrieval systems demonstrate effectiveness on some\n!\ndatasets, the BEIR benchmark (Thakur et al., 2021) highlights a main drawback of these dense retrieval systems - failure to generalize to out-of-domain data. This motivates pretraining as a solution for better domain generalization (Gururangan et al., 2020).\nPretraining for Retrieval Lee et al. (2019) first propose to pretrain a bi-encoder retriever with an Inverse Cloze Task (ICT), which constructs a training pair by randomly selecting a sentence from a passage as the query and leaving the rest as the document. Chang et al. (2020) propose two pretraining tasks for Wikipedia and attempt to combine them with ICT and masked language modeling (MLM). Guu et al. (2020) pretrain a retriever and a reader together for end-to-end question answering (QA). Very recently, DPR-PAQ (Oğuz et al., 2021) highlight the importance of domain matching by using both synthetic and crawled QA data to pretrain and then fine-tune the model on downstream datasets for dialogue retrieval. Condenser (Gao and Callan, 2021a) is a new Transformer variant for MLM pretraining. It exploits an information bottleneck to facilitate learning for information aggregation. On top of that, coCondenser (Gao and Callan, 2021b) adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Different from these works, LaPraDoR is the first pretrained retriever that does not require fine-tuning on a downstream dataset and can perform zero-shot retrieval."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Dual-Tower Architecture",
      "text" : "Two Encoders The dual-tower architecture, as illustrated in Figure 1, is widely used in dense retrieval systems (Lee et al., 2019; Karpukhin et al., 2020; Xiong et al., 2021). The dual-tower archi-\ntecture has a query encoder EQ and a document encoderED, which in our work are both BERT-like bidirectional text encoders (Devlin et al., 2019). Compared with cross-attention models (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020), the dual-tower architecture enables pre-indexing and fast approximate nearest neighbor search (to be detailed shortly), thus is popular in production.\nDense Representation Given an input document (query) x = {[CLS], w1, . . . , wl,[SEP]}, we use a document (query) encoder ED (EQ) to encode the input sequence into hidden states h = {v[CLS], v1, . . . , vl, v[SEP]}, where wi is the i-th token; [CLS] and [SEP] are special tokens that mark the start and end of a sentence, respectively. To obtain a dense representation, we use mean pooling over hidden states h as the representation hx of the input x. Some prior works (Lee et al., 2019; Chang et al., 2020; Karpukhin et al., 2020) use v[CLS] as the representation for the input x, but Huang et al. (2021) empirically find that applying mean pooling to hidden states h outperforms taking v[CLS] as the representation.\nSimilarity Function After obtaining the representation for both the query q and the document d, we use the cosine function as a similarity function to measure the similarity between them:\nsim(q, d) = EQ(q) · ED(d) ‖EQ(q)‖‖ED(d)‖\n(1)\nApproximate Nearest Neighbor In practice, for the dual-tower architecture, the documents are encoded offline and their dense representations can be pre-indexed by a fast vector similarity search library (e.g., FAISS, Johnson et al., 2021). The library can utilize GPU acceleration to perform approximate nearest neighbor (ANN) search in sublinear time with almost no loss in recall. Thus, compared to a cross-encoder (i.e., an encoder that accepts the concatenation of the query and every candidate document), a pre-indexed ANN-based retrieval system is at least 10 times faster (to be detailed in Section 4.2)."
    }, {
      "heading" : "3.2 Constructing Positive Instances",
      "text" : "In this section, we first introduce how we build the positive instances with two self-supervised tasks, namely Inverse Cloze Task (ICT) and Dropout as Positive Instance (DaPI).\nInverse Cloze Task (ICT) First introduced in Lee et al. (2019), ICT is an effective way to pretrain a text retrieval model (Chang et al., 2020). Given a passage p consisting of sentences p = {s1, . . . , sn}, we randomly select a sentence sk as query q and treat its context as document d = {s1, . . . , sk−1, sk+1, . . . , sn}. ICT is designed to mimic a text retrieval task where a short query is used to retrieve a longer document which is semantically relevant. Also, unlike some pretraining tasks, e.g., Wiki Link Prediction or Body First Selection (Chang et al., 2020), ICT is fast and does not rely on a specific corpus format (e.g., Wikipedia) thus can be scaled to a large multi-source corpus (e.g., C4, Raffel et al., 2020).\nDropout as Positive Instance (DaPI) DaPI is originally proposed in SimCSE (Gao et al., 2021c) as a simple strategy for perturbing intermediate representations and thus can serve as data augmentation.2 We apply a dropout rate of 0.1 to the fullyconnected layers and attention probabilities in the Transformer encoders, as in BERT (Devlin et al., 2019). The same input is fed to the encoder twice to obtain two representations, of which one is used as the positive instance of the other. Gao et al. (2021c) conduct experiments and conclude that the dropout strategy outperforms all commonly-used discrete perturbation techniques including cropping, word deletion, masked language modeling and synonym replacement. Note that different from SimCSE (Gao et al., 2021c), in our training, only one of the two passes is with gradient. This choice significantly decreases the memory use, allowing us to increase the number of in-batch negative instances, which is important for contrastive learning."
    }, {
      "heading" : "3.3 Iterative Contrastive Learning",
      "text" : "Previous studies (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b) show that the number of negative instances is critical to the performance of the model. Since the batch size on a single GPU is limited, we propose Iterative Contrastive Learning (ICoL) to mitigate the insufficient memory on a single GPU and allow more negative instances for better performance. We illustrate LaPraDoR training in Figure 2.\nIterative Training We iteratively train the query encoder and document encoder. To be specific, we\n2To avoid confusion with the SimCSE model, we address the dropout strategy as DaPI here.\n!\n!\nand document encoder while freezing the other (marked with an ice cube icon ). For Lqd and Ldq , we obtain additional negative instances from the cache queue. For each batch of data, we enqueue the representation encoded by the frozen encoder into the cache queue as future negative instances. The cache queue is cleared when switching the encoder to train from one to the other.\nfirst arbitrarily select an encoder to start training. Here we assume to start with the query encoder EQ. The training loss consists of two terms. First, we calculate the loss for query-query retrieval with DaPI to optimize the negative log likelihood of the positive instance:\nLqq(qi, {q+i , q−i,1, . . . , q−i,n}) =− log e sim(qi,q+i )\nesim(qi,q + i ) + ∑n j=1 e sim(qi,q−i,j)\n(2)\nwhere qi and q+i are the same query that are encoded by EQ with different dropout masks; {q−i,1, ..., q−i,n} is a set of randomly sampled negative instances; sim(·, ·) is the cosine similarity function defined in Equation 1.\nThe second term is to retrieve the corresponding document d+i with the query qi, where qi and d + i are a pair constructed with ICT. Similarly, we optimize the negative log likelihood of the positive instance by:\nLqd(qi, {d+i , d−i,1, . . . , d−i,n, d−Q,1, . . . , d−Q,|Q|})\n=− log e sim(qi,d+i )\nesim(qi,d + i ) + ∑n j=1 e sim(qi,d−i,j)\n+ ∑|Q|\nk=1 e sim(qi,d−Q,k)\n(3) where {d−i,1, ..., d−i,n} is a set of freshly sampled documents that are encoded at the current step i; {d−Q,1, ..., d−Q,|Q|} is a set of representations that are currently stored in the cache queue Q. Then, we optimize the sum of the two losses with a weight coefficient λ:\nLq = Lqd + λLqq (4)\nNote that the query qi only needs to be encoded once and can be used for calculation of both Lqd and Lqq.\nAfter a predefined number of steps, the EQ becomes frozen as the training for ED starts. Similarly, for di, a document encoded by ED, we have the training objective:\nLdd(di, {d+i , d−i,1, . . . , d−i,n}) =− log e sim(di,d+i )\nesim(di,d + i ) + ∑n j=1 e sim(di,d−i,j) (5)\nLdq(di, {q+i , q−i,1, . . . , q−i,n, q−Q,1, . . . , q−Q,|Q|})\n=− log e sim(di,q+i )\nesim(di,q + i ) + ∑n j=1 e sim(di,q−i,j)\n+ ∑|Q|\nk=1 e sim(di,q−Q,k)\n(6)\nLd = Ldq + λLdd (7) where d+i and q + i are positive instances constructed by DaPI and ICT, respectively; {d−i,1, . . . , d−i,n} is a set of randomly sampled query negatives; {q−i,1, . . . , q−i,n} is a set of freshly sampled documents encoded at step i; {q−Q,1, . . . , q−Q,|Q|} are the cached query representations. To speed up training, we apply the in-batch negatives technique (Yih et al., 2011; Henderson et al., 2017; Gillick et al., 2019) that can reuse computation and train b queries/documents in a mini-batch simultaneously.\nCache Mechanism To enlarge the size of negative instances, we maintain a cache queue Q that\nstores previously encoded representations that can serve as negative instances for the current step, extending an earlier study (Wu et al., 2018). Our cache queue is implemented as first-in-first-out (FIFO) with a maximum capacity m, which is a hyperparameter set based on the GPU memory size. When training with multiple GPUs, Q can be shared across GPUs. Since the representations in the queue are encoded with a frozen encoder and thus do not require gradients, m can be set large to supplement the numbers of negative instances. When Q is full, the earliest cached representations will be dequeued. When we switch the training from one encoder to the other, the queue will be cleared to ensure that all representations in Q lie in the same hidden space and are encoded with the currently frozen encoder.\nICoL vs. MoCo Previously, similar to our method, MoCo (He et al., 2020) exploits a queue for storing encoded representations. Specifically, MoCo consists of a slow encoder and a fast encoder to encode queries and documents, respectively. The slow encoder is updated as a slow moving average of the fast encoder to reduce inconsistency of encoded document representations between training steps. A queue is maintained to allow the encoded document representations to be reused in later steps as negative instances.\nHowever, we argue there are a two limitations that make MoCo not ideal for training a text retrieval model: (1) As pointed out by Yang et al. (2021), unlike the image matching task in the original paper of MoCo, in text retrieval, the queries and documents are distinct from each other thus not interchangeable. Yang et al. (2021) propose xMoCo, which incorporates two sets of slow and fast encoders, as a simple fix for this flaw. (2) The cached representations are in different hidden spaces. Although the fast encoders in both MoCo and xMoCo are updated with momentum, the already-encoded representations in the queue will never be updated. This creates a semantic mismatch between newly encoded and cached old representations and creates noise during training. In ICoL, all representations used for contrastive learning are aligned in the same hidden space. Besides, ICoL is more flexible than xMoCo since it does not introduce additional fast encoders and even the weights of its query encoder and document encoder can be shared. We conduct experiments to compare ICoL with MoCo and xMoCo in Section 4.2.1."
    }, {
      "heading" : "3.4 Lexicon-Enhanced Dense Retrieval",
      "text" : "Although dense retrieval achieves state-of-the-art performance, its performance significantly degenerates on out-of-domain data (Thakur et al., 2021). On the other hand, BM25 (Robertson and Zaragoza, 2009) demonstrates good performance without training. Early attempts at combining lexical match with dense retrieval often formulate it to a reranking task (Nguyen et al., 2016). First, BM25 is used to recall the top-k documents from the corpus. Then, a cross-encoder is applied to further re-rank candidate documents. Recently, COIL (Gao et al., 2021a) highlights the importance of lexical match and incorporates exact lexical matching into dense retrieval. Different from these works, we propose a fast and effective way, namely Lexicon-Enhanced Dense Retrieval (LEDR) to enhance dense retrieval with BM25. The similarity score of BM25 is defined as:\nBM25(q, d) = ∑ t∈q∩d IDF(t)hq(q, t)hd(d, t)\nhq(q, t) = TFt,q (1 + k2)\nTFt,q +k2\nhd(d, t) = TFt,d (1 + k1)\nTFt,d+k1 ( 1− b+ b |d|avgdl ) (8)\nwhere TFt,d and TFt,q refer to term frequency of term t in document d and query q, respectively; IDF(t) is the inverse document frequency; b, k1 and k2 are hyperparameters. For inference, we simply multiply the BM25 score with the similarity score for dense retrieval:\nscore(q, d) = sim(q, d)× BM25(q, d) (9) In this way, we consider both lexical and semantic matching. This combination makes LaPraDoR more robust on unseen data in zero-shot learning."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setting",
      "text" : "Benchmark We use BEIR (Thakur et al., 2021), a recently released benchmark for zero-shot evaluation of information retrieval models. BEIR includes 18 heterogeneous datasets, focusing on evaluating a retrieval system that works across different domains (bio-medical, scientific, news, social media, etc.). The evaluation metric is Normalized Discounted Cumulative Gain (NDCG@10). We include details of the BEIR benchmark in Appendix A.\nModel Settings In our preliminary experiments on Wikipedia (see Table 2), we find that sharing weights between the query encoder EQ and document encoder ED has no negative effect on downstream performance. For weight sharing between EQ and ED, we simply copy the weights of EQ to ED when switching to training of ED, vice versa. This design eliminates nearly half of the parameters. An additional benefit is that weight sharing makes the encoder versatile to handle not only query-document retrieval, but also query-query and document-document retrieval.\nIn our preliminary experiments on Wikipedia, we observed a diminishing return when increasing the model size from 6 layers to 12 layers, or 24 layers. Thus, we initialize our encoder with the 6-layer DistilBERT (Sanh et al., 2019), which has ∼67M parameters. For BM25, we use the implementation and default settings of Elastic Search3. BM25 scores after the top 1,000 retrieved text are set to 0 to save computation.\nTraining Details For pretraining, we optimize the model with the AdamW optimizer with a learning rate of 2e-4. The model is trained with 16\n3https://github.com/elastic/ elasticsearch\nNvidia V100 32GB GPUs with FP16 mixed precision training. The batch size for each GPU is set to 4,096. Training switches between EQ and ED every 100 steps. The cache queue has a maximum capacity m of 100k. The loss weight hyperparameter λ is fixed to 1. For our main results, we train LaPraDoR on C4 (Raffel et al., 2020) for 1M steps, which takes about 400 hours. For the ablation study, since training on C4 is very costly, we train LaPraDoR on Wikipedia4 for 100k steps. When calculating the loss, we apply a re-scaling trick of multiplying the cosine similarity score by 20 for better optimization (Thakur et al., 2021).\nWe test LaPraDoR under two settings: (1) No supervised data at all. We directly use the pretrained model for zero-shot retrieval on BEIR. (2) Finetuning on MS-MARCO (Nguyen et al., 2016) and zero-shot transfer to the other datasets. This is the original setting for BEIR. We use BEIR’s official script5 to fine-tune LaPraDoR. The batch size is set to 75 per GPU and the learning rate is 2e-5.\n4https://huggingface.co/datasets/ wikipedia\n5https://github.com/UKPLab/beir/blob/ main/examples/retrieval/training/train_ msmarco_v3.py\nBaselines For dense retrieval, we compare our model to the dual-tower models: DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021), TASB (Hofstätter et al., 2021) and GenQ (Thakur et al., 2021). For lexical matching, we use the BM25 results reported in Thakur et al. (2021). We also consider a late interaction baseline ColBERT (Khattab and Zaharia, 2020). The model computes multiple contextualized embeddings for each token of queries and documents, and then maximizes a similarity function to retrieve relevant documents. For re-ranking, we use the BM25+CE baseline implemented in Thakur et al. (2021) that uses BM25 to retrieve top-100 documents and a 6-layer MiniLM (Wang et al., 2020) model to further re-rank the recalled documents. As shown in Table 1, the latency for both lexical and dense retrieval is low whereas re-ranking introduces significantly higher latency, with late-interaction inbetween. Details of the baselines can be found in Appendix B."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "We list the results of LaPraDoR on the BEIR benchmark in Table 1. Our model achieves state-of-theart performance on BEIR to date (November 15, 2021). Without any supervised data, LaPraDoR outperforms the previous state-of-the-art for zeroshot dense retrieval, TAS-B (Hofstätter et al., 2021), on 13 tasks (out of 18) of BEIR with an average advantage of 0.042, though TAS-B applies additional query clustering and knowledge distillation. When further fine-tuned on MS-MARCO, LaPraDoR can\noutperform all baselines, including late interaction and re-ranking, whose latency on GPU is 17.5× and 22.5× higher than our method. Compared to dense retrieval, we only add 0.4 GB of BM25 indices and almost no additional latency."
    }, {
      "heading" : "4.2.1 Effect of Iterative Contrastive Learning",
      "text" : "We set a baseline that only uses in-batch negatives and compare our proposed Iterative Contrastive Learning (ICoL) to MoCo (He et al., 2020) and xMoCo (Yang et al., 2021) for training LaPraDoR on Wikipedia in Table 2. The aforementioned two flaws of MoCo hinder its performance and lead to a performance drop instead of an improvement. In contrast, our ICoL approach outperforms the inbatch baseline on all datasets. It also beats the competitive MoCo variant for text retrieval, xMoCo, on 15 out of 18 tasks. ICoL only uses two encoders (which can be further shared) which can alleviate the GPU memory problem and thus can fit more in-batch negatives. Meanwhile, MoCo uses two encoders and xMoCo uses four (two sets of MoCo’s encoders). Moreover, we observe no performance drop on average if we share the encoder between query and document (as we do when training LaPraDoR on C4). Thus, we can eliminate half of the parameters in our model by simply sharing the encoder."
    }, {
      "heading" : "4.2.2 Effect of Pretraining and Lexicon-Enhanced Dense Retrieval",
      "text" : "We conduct an ablation study for both pretraining and Lexicon-Enhanced Dense Retrieval to verify\nthe effectiveness of these designs. As shown in Table 3, Lexicon-Enhanced Dense Retrieval (LEDR) improves performance of dense retrieval on most tasks for both fully unsupervised and fine-tuned LaPraDoR. Furthermore, as illustrated in Table 4, we test the effectiveness of the two components in our loss function. We can see that both ICT and DaPI contribute to the performance of our model. DaPI helps LaPraDoR get higher scores on 13 out of 18 tasks. Also, we observe a significant performance drop when removing ICT from the training loss, suggesting the importance of retrieval pretraining."
    }, {
      "heading" : "4.3 Case Study",
      "text" : "We conduct a case study to intuitively demonstrate the effectiveness of LaPraDoR. As shown in Figure 3, for Q1, the lexical method (i.e., BM25) can successfully find the corresponding document in its top-2 retrieved results. However, due to lower lexical overlap, the score of the ground truth is lower than that of the first document. Although the phrase “prepare for his departure” in the first document indicates that Aeneas has not left Carchage yet and provides strong evidence that this document is incorrect, BM25 fails to correctly rank the ground truth due to its lack of ability in semantic matching. By incorporating both lexical and semantic matching, LaPraDoR can successfully retrieve the ground truth.\nFor Q2, with the powerful semantic matching,\nLaPraDoR successfully retrieves the ground truth whereas BM25 fails to distinguish among the documents that contain both the keywords Mars and Sun. On the other hand, after removing lexical matching, LaPraDoR without LEDR suffers from noise: the key entity Sun does not appear in its top-1 retrieved document. LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we introduce LaPraDoR, an unsupervised pretrained dense retriever that achieves state-of-the-art performance on the zero-shot text retrieval benchmark BEIR. We propose Iterative Contrastive Learning (ICoL) for efficiently training LaPraDoR and Lexicon-Enhanced Dense Retrieval (LEDR) to combine lexical matching with LaPraDoR. Our experiments verify the effectiveness of both ICoL and LEDR, shedding light on a new paradigm for unsupervised text retrieval. For future work, we plan to extend unsupervised LaPraDoR to multilingual and multi-modal retrieval.\nBroader Impact\nEthical Concerns LaPraDoR is trained with web-crawled data, which may contain inappropriate content. However, due to the nature of text retrieval, our retriever has lower ethical risk compared to a generative auto-regressive language model (Bender et al., 2021). Meanwhile, our unsupervised retrieval model enables high-performance text retrieval for low-resource languages where there is no supervised query-document dataset. This contributes to equity and diversity of language technology.\nCarbon Footprint To conduct all experiments in this paper, we estimate to have consumed 3,840 kWh of electricity and emitted 1,420.8 kg (3,132.3 lbs) of CO2. All emitted carbon dioxide has already been offset."
    }, {
      "heading" : "A The BEIR Benchmark",
      "text" : "We list the statistics of the BEIR benchmark in Table 5. The 18 English zero-shot evaluation datasets come from 9 heterogeneous retrieval tasks, including bio-medical information retrieval, question answering, tweet retrieval, news retrieval, argument retrieval, duplicate question retrieval, citation prediction, and fact checking."
    }, {
      "heading" : "B Baselines",
      "text" : "We use the baselines from the current BEIR leaderboard (Thakur et al., 2021). These baselines can be divided into four groups: dense retrieval, lexical retrieval, late interaction and re-ranking.\nDense Retrieval For dense retrieval, the baselines are the same dual-tower model as ours. We consider DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021), TAS-B (Hofstätter et al., 2021) and GenQ (Thakur et al., 2021) in this paper.\n• DPR uses a single BM25 retrieval example and in-batch examples as hard negative examples to train the model. Following Thakur et al. (2021), we use Multi-DPR as the baseline. The model is a BERT-base model and is trained on four QA datasets, including NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), WebQuestions (Berant et al., 2013) and CuratedTREC (Baudis and Sedivý, 2015).\n• ANCE constructs hard negative examples from an ANN index of the corpus. The hard negative training instances are updated in parallel during fine-tuning of the model. The model is a RoBERTa (Liu et al., 2019) model trained on MS-MARCO for 600k steps.\n• TAS-B is trained with Balanced Topic Aware Sampling using dual supervision from a crossencoder and a ColBERT model (Khattab and Zaharia, 2020). The model is trained with a combination of a pairwise Margin-MSE (Hofstätter et al., 2021) loss and an in-batch negative loss function.\n• GenQ fine-tunes a T5-base (Raffel et al., 2020) model on MS MARCO for 2 epochs and generate 5 queries for each document as additional training data to continue to finetune the TAS-B model.\nLexical Retrieval Lexical retrieval is a score function for token matching calculated between two high-dimensional sparse vectors with token\nweights. BM25 (Robertson and Zaragoza, 2009) is the most commonly used lexical retrieval function. We use the BM25 results reported in Thakur et al. (2021) for comparison.\nLate Interaction We also consider a late interaction baseline, namely ColBERT (Khattab and Zaharia, 2020). The model computes multiple contextualized embeddings for each token of queries and documents, and then uses a maximum similarity function to retrieve relevant documents. This type of matching requires significantly more disk space for indexes and has a higher latency.\nRe-ranking Re-ranking based approaches use the output of a first-stage retrieval system (e.g., BM25), and then re-rank the retrieved documents using a cross-encoder (Nogueira and Cho, 2020). In this paper, we use the BM25+CE baseline implemented in Thakur et al. (2021) that uses BM25 to retrieve top-100 documents and a 6-layer MiniLM (Wang et al., 2020) model to further rerank the recalled documents."
    } ],
    "references" : [ {
      "title" : "Overview of the trec 2004 robust retrieval track",
      "author" : [ "J Allan." ],
      "venue" : "TREC, volume 13.",
      "citeRegEx" : "Allan.,? 2004",
      "shortCiteRegEx" : "Allan.",
      "year" : 2004
    }, {
      "title" : "Modeling of the question answering task in the yodaqa system",
      "author" : [ "Petr Baudis", "Jan Sedivý." ],
      "venue" : "CLEF, volume 9283 of Lecture Notes in Computer Science, pages 222–228. Springer.",
      "citeRegEx" : "Baudis and Sedivý.,? 2015",
      "shortCiteRegEx" : "Baudis and Sedivý.",
      "year" : 2015
    }, {
      "title" : "On the dangers of stochastic parrots: Can language models be too big? In FAccT, pages 610–623",
      "author" : [ "Emily M. Bender", "Timnit Gebru", "Angelina McMillanMajor", "Shmargaret Shmitchell." ],
      "venue" : "ACM.",
      "citeRegEx" : "Bender et al\\.,? 2021",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "EMNLP, pages 1533– 1544. ACL.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Overview of Touché 2020",
      "author" : [ "Alexander Bondarenko", "Maik Fröbe", "Meriem Beloucif", "Lukas Gienapp", "Yamen Ajjour", "Alexander Panchenko", "Chris Biemann", "Benno Stein", "Henning Wachsmuth", "Martin Potthast", "Matthias Hagen" ],
      "venue" : null,
      "citeRegEx" : "Bondarenko et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bondarenko et al\\.",
      "year" : 2020
    }, {
      "title" : "A full-text learning to rank dataset for medical information retrieval",
      "author" : [ "Vera Boteva", "Demian Gholipour Ghalandari", "Artem Sokolov", "Stefan Riezler." ],
      "venue" : "ECIR, volume 9626 of Lecture Notes in Computer Science, pages 716–722. Springer.",
      "citeRegEx" : "Boteva et al\\.,? 2016",
      "shortCiteRegEx" : "Boteva et al\\.",
      "year" : 2016
    }, {
      "title" : "Pre-training tasks for embedding-based large-scale retrieval",
      "author" : [ "Wei-Cheng Chang", "Felix X. Yu", "Yin-Wen Chang", "Yiming Yang", "Sanjiv Kumar." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Chang et al\\.,? 2020",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2020
    }, {
      "title" : "SPECTER: document-level representation learning using citation-informed transformers",
      "author" : [ "Arman Cohan", "Sergey Feldman", "Iz Beltagy", "Doug Downey", "Daniel S. Weld." ],
      "venue" : "ACL, pages 2270–2282. Association for Computational Linguis-",
      "citeRegEx" : "Cohan et al\\.,? 2020",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186. Association for Computational Linguistics.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Climate-fever: A dataset for verification of real-world climate claims",
      "author" : [ "Thomas Diggelmann", "Jordan Boyd-Graber", "Jannis Bulian", "Massimiliano Ciaramita", "Markus Leippold" ],
      "venue" : null,
      "citeRegEx" : "Diggelmann et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Diggelmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Condenser: a pre-training architecture for dense retrieval",
      "author" : [ "Luyu Gao", "Jamie Callan." ],
      "venue" : "arXiv preprint arXiv:2104.08253.",
      "citeRegEx" : "Gao and Callan.,? 2021a",
      "shortCiteRegEx" : "Gao and Callan.",
      "year" : 2021
    }, {
      "title" : "Unsupervised corpus aware language model pre-training for dense passage retrieval",
      "author" : [ "Luyu Gao", "Jamie Callan." ],
      "venue" : "arXiv preprint arXiv:2108.05540.",
      "citeRegEx" : "Gao and Callan.,? 2021b",
      "shortCiteRegEx" : "Gao and Callan.",
      "year" : 2021
    }, {
      "title" : "Modularized transfomer-based ranking framework",
      "author" : [ "Luyu Gao", "Zhuyun Dai", "Jamie Callan." ],
      "venue" : "EMNLP, pages 4180–4190. Association for Computational Linguistics.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "COIL: revisit exact lexical match in information retrieval with contextualized inverted list",
      "author" : [ "Luyu Gao", "Zhuyun Dai", "Jamie Callan." ],
      "venue" : "NAACLHLT, pages 3030–3042. Association for Computational Linguistics.",
      "citeRegEx" : "Gao et al\\.,? 2021a",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Scaling deep contrastive learning batch size under memory limited setup",
      "author" : [ "Luyu Gao", "Yunyi Zhang", "Jiawei Han", "Jamie Callan." ],
      "venue" : "The 6th Workshop on Representation Learning for NLP (RepL4NLP), pages 316–321.",
      "citeRegEx" : "Gao et al\\.,? 2021b",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2104.08821.",
      "citeRegEx" : "Gao et al\\.,? 2021c",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning dense representations for entity retrieval",
      "author" : [ "Daniel Gillick", "Sayali Kulkarni", "Larry Lansing", "Alessandro Presta", "Jason Baldridge", "Eugene Ie", "Diego García-Olano." ],
      "venue" : "CoNLL, pages 528– 537. Association for Computational Linguistics.",
      "citeRegEx" : "Gillick et al\\.,? 2019",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2019
    }, {
      "title" : "Declutr: Deep contrastive learning for unsupervised textual representations",
      "author" : [ "John M. Giorgi", "Osvald Nitski", "Bo Wang", "Gary D. Bader." ],
      "venue" : "ACLIJCNLP, pages 879–895. Association for Computational Linguistics.",
      "citeRegEx" : "Giorgi et al\\.,? 2021",
      "shortCiteRegEx" : "Giorgi et al\\.",
      "year" : 2021
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasovic", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "ACL, pages 8342–8360. Association for",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Realm: Retrievalaugmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:2002.08909.",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dbpedia-entity v2: A test collection for entity search",
      "author" : [ "Faegheh Hasibi", "Fedor Nikolaev", "Chenyan Xiong", "Krisztian Balog", "Svein Erik Bratsberg", "Alexander Kotov", "Jamie Callan." ],
      "venue" : "SIGIR, pages 1265–1268. ACM.",
      "citeRegEx" : "Hasibi et al\\.,? 2017",
      "shortCiteRegEx" : "Hasibi et al\\.",
      "year" : 2017
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross B. Girshick." ],
      "venue" : "CVPR, pages 9726–9735. Computer Vision Foundation / IEEE.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient natural language response suggestion for smart reply",
      "author" : [ "Matthew Henderson", "Rami Al-Rfou", "Brian Strope", "YunHsuan Sung", "László Lukács", "Ruiqi Guo", "Sanjiv Kumar", "Balint Miklos", "Ray Kurzweil." ],
      "venue" : "arXiv preprint arXiv:1705.00652.",
      "citeRegEx" : "Henderson et al\\.,? 2017",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2017
    }, {
      "title" : "Efficiently teaching an effective dense retriever with balanced topic aware sampling",
      "author" : [ "Sebastian Hofstätter", "Sheng-Chieh Lin", "Jheng-Hong Yang", "Jimmy Lin", "Allan Hanbury." ],
      "venue" : "SIGIR, pages 113– 122. ACM.",
      "citeRegEx" : "Hofstätter et al\\.,? 2021",
      "shortCiteRegEx" : "Hofstätter et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving efficient neural ranking models with cross-architecture knowledge distillation",
      "author" : [ "Sebastian Hofstätter", "Sophia Althammer", "Michael Schröder", "Mete Sertkan", "Allan Hanbury" ],
      "venue" : null,
      "citeRegEx" : "Hofstätter et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hofstätter et al\\.",
      "year" : 2021
    }, {
      "title" : "Cqadupstack: A benchmark data set for community question-answering research",
      "author" : [ "Doris Hoogeveen", "Karin M Verspoor", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 20th Australasian document computing symposium, pages 1–8.",
      "citeRegEx" : "Hoogeveen et al\\.,? 2015",
      "shortCiteRegEx" : "Hoogeveen et al\\.",
      "year" : 2015
    }, {
      "title" : "Whiteningbert: An easy unsupervised sentence embedding approach",
      "author" : [ "Junjie Huang", "Duyu Tang", "Wanjun Zhong", "Shuai Lu", "Linjun Shou", "Ming Gong", "Daxin Jiang", "Nan Duan." ],
      "venue" : "arXiv preprint arXiv:2104.01767.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Billion-scale similarity search with gpus",
      "author" : [ "Jeff Johnson", "Matthijs Douze", "Hervé Jégou." ],
      "venue" : "IEEE Trans. Big Data, 7(3):535–547.",
      "citeRegEx" : "Johnson et al\\.,? 2021",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2021
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer." ],
      "venue" : "ACL, pages 1601–1611. Association for Computational Linguistics.",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick S.H. Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "EMNLP, pages 6769–6781. Association for Compu-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Colbert: Efficient and effective passage search via contextualized late interaction over BERT",
      "author" : [ "Omar Khattab", "Matei Zaharia." ],
      "venue" : "SIGIR, pages 39–48. ACM.",
      "citeRegEx" : "Khattab and Zaharia.,? 2020",
      "shortCiteRegEx" : "Khattab and Zaharia.",
      "year" : 2020
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association of Computational Linguistics.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "ACL, pages 6086– 6096. Association for Computational Linguistics.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient document re-ranking for transformers by precomputing term representations",
      "author" : [ "Sean MacAvaney", "Franco Maria Nardini", "Raffaele Perego", "Nicola Tonellotto", "Nazli Goharian", "Ophir Frieder." ],
      "venue" : "SIGIR, pages 49–58. ACM.",
      "citeRegEx" : "MacAvaney et al\\.,? 2020",
      "shortCiteRegEx" : "MacAvaney et al\\.",
      "year" : 2020
    }, {
      "title" : "WWW’18 open challenge: Financial opinion mining and question answering",
      "author" : [ "Macedo Maia", "Siegfried Handschuh", "André Freitas", "Brian Davis", "Ross McDermott", "Manel Zarrouk", "Alexandra Balahur." ],
      "venue" : "WWW (Companion Volume), pages",
      "citeRegEx" : "Maia et al\\.,? 2018",
      "shortCiteRegEx" : "Maia et al\\.",
      "year" : 2018
    }, {
      "title" : "MS MARCO: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "CoCo@NIPS, volume 1773 of CEUR Workshop Proceedings.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Passage Re-ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1901.04085.",
      "citeRegEx" : "Nogueira and Cho.,? 2020",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2020
    }, {
      "title" : "Domain-matched pretraining tasks for dense retrieval",
      "author" : [ "Barlas Oğuz", "Kushal Lakhotia", "Anchit Gupta", "Patrick Lewis", "Vladimir Karpukhin", "Aleksandra Piktus", "Xilun Chen", "Sebastian Riedel", "Wen-tau Yih", "Sonal Gupta" ],
      "venue" : null,
      "citeRegEx" : "Oğuz et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Oğuz et al\\.",
      "year" : 2021
    }, {
      "title" : "Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering",
      "author" : [ "Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Wayne Xin Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang." ],
      "venue" : "NAACL-HLT, pages",
      "citeRegEx" : "Qu et al\\.,? 2021",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "J. Mach. Learn. Res., 21:140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "EMNLP-IJCNLP, pages 3980–3990. Association for Computational Linguistics.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Trec-covid: rationale and structure of an information retrieval shared task for covid-19",
      "author" : [ "Kirk Roberts", "Tasmeer Alam", "Steven Bedrick", "Dina Demner-Fushman", "Kyle Lo", "Ian Soboroff", "Ellen Voorhees", "Lucy Lu Wang", "William R Hersh." ],
      "venue" : "Jour-",
      "citeRegEx" : "Roberts et al\\.,? 2020",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "The probabilistic relevance framework: BM25 and beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "Now Publishers Inc.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Trec 2019 news track overview",
      "author" : [ "Ian Soboroff", "Shudong Huang", "Donna Harman." ],
      "venue" : "TREC.",
      "citeRegEx" : "Soboroff et al\\.,? 2019",
      "shortCiteRegEx" : "Soboroff et al\\.",
      "year" : 2019
    }, {
      "title" : "A data collection for evaluating the retrieval of related tweets to news articles",
      "author" : [ "Axel Suarez", "Dyaa Albakour", "David P.A. Corney", "Miguel Martinez-Alvarez", "José Esquivel." ],
      "venue" : "ECIR, volume 10772 of Lecture Notes in Computer Science, pages",
      "citeRegEx" : "Suarez et al\\.,? 2018",
      "shortCiteRegEx" : "Suarez et al\\.",
      "year" : 2018
    }, {
      "title" : "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
      "author" : [ "Nandan Thakur", "Nils Reimers", "Andreas Rücklé", "Abhishek Srivastava", "Iryna Gurevych." ],
      "venue" : "Thirty-fifth Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Thakur et al\\.,? 2021",
      "shortCiteRegEx" : "Thakur et al\\.",
      "year" : 2021
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and verification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "NAACL-HLT, pages 809–819. Association for Computational Linguistics.",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "An overview of the bioasq",
      "author" : [ "George Tsatsaronis", "Georgios Balikas", "Prodromos Malakasiotis", "Ioannis Partalas", "Matthias Zschunke", "Michael R Alvers", "Dirk Weissenborn", "Anastasia Krithara", "Sergios Petridis", "Dimitris Polychronopoulos" ],
      "venue" : null,
      "citeRegEx" : "Tsatsaronis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsatsaronis et al\\.",
      "year" : 2015
    }, {
      "title" : "Retrieval of the best counterargument without prior topic knowledge",
      "author" : [ "Henning Wachsmuth", "Shahbaz Syed", "Benno Stein." ],
      "venue" : "ACL, pages 241–251. Association for Computational Linguistics.",
      "citeRegEx" : "Wachsmuth et al\\.,? 2018",
      "shortCiteRegEx" : "Wachsmuth et al\\.",
      "year" : 2018
    }, {
      "title" : "Fact or fiction: Verifying scientific claims",
      "author" : [ "David Wadden", "Shanchuan Lin", "Kyle Lo", "Lucy Lu Wang", "Madeleine van Zuylen", "Arman Cohan", "Hannaneh Hajishirzi." ],
      "venue" : "EMNLP, pages 7534–7550. Association for Computational Linguistics.",
      "citeRegEx" : "Wadden et al\\.,? 2020",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2020
    }, {
      "title" : "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
      "author" : [ "Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised feature learning via nonparametric instance discrimination",
      "author" : [ "Zhirong Wu", "Yuanjun Xiong", "Stella X. Yu", "Dahua Lin." ],
      "venue" : "CVPR, pages 3733–3742. Computer Vision Foundation / IEEE Computer Society.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Clear: Contrastive learning for sentence representation",
      "author" : [ "Zhuofeng Wu", "Sinong Wang", "Jiatao Gu", "Madian Khabsa", "Fei Sun", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2012.15466.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
      "author" : [ "Lee Xiong", "Chenyan Xiong", "Ye Li", "Kwok-Fung Tang", "Jialin Liu", "Paul N. Bennett", "Junaid Ahmed", "Arnold Overwijk." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Xiong et al\\.,? 2021",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2021
    }, {
      "title" : "xmoco: Cross momentum contrastive learning for open-domain question answering",
      "author" : [ "Nan Yang", "Furu Wei", "Binxing Jiao", "Daxing Jiang", "Linjun Yang." ],
      "venue" : "ACL-IJCNLP, pages 6120–6129. Association for Computational Linguistics.",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 2369–2380. Association for",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning discriminative projections for text similarity measures",
      "author" : [ "Wen-tau Yih", "Kristina Toutanova", "John C. Platt", "Christopher Meek." ],
      "venue" : "CoNLL, pages 247–256. ACL.",
      "citeRegEx" : "Yih et al\\.,? 2011",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2011
    }, {
      "title" : "The model is trained with a combination of a pairwise Margin-MSE (Hofstätter et al., 2021) loss and an in-batch negative loss function",
      "author" : [ "Zaharia" ],
      "venue" : null,
      "citeRegEx" : "Zaharia and 2020..,? \\Q2021\\E",
      "shortCiteRegEx" : "Zaharia and 2020..",
      "year" : 2021
    }, {
      "title" : "In this paper, we use the BM25+CE baseline implemented in Thakur et al. (2021) that uses BM25 to retrieve top-100 documents and a 6-layer MiniLM",
      "author" : [ "cross-encoder (Nogueira", "Cho" ],
      "venue" : null,
      "citeRegEx" : ".Nogueira and Cho,? \\Q2020\\E",
      "shortCiteRegEx" : ".Nogueira and Cho",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 41,
      "context" : "Different from cross-encoders (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020) or late-interaction models (Khattab and Zaharia, 2020; Gao et al.",
      "startOffset" : 30,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "Different from cross-encoders (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020) or late-interaction models (Khattab and Zaharia, 2020; Gao et al.",
      "startOffset" : 30,
      "endOffset" : 100
    }, {
      "referenceID" : 34,
      "context" : "Different from cross-encoders (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020) or late-interaction models (Khattab and Zaharia, 2020; Gao et al.",
      "startOffset" : 30,
      "endOffset" : 100
    }, {
      "referenceID" : 30,
      "context" : ", 2020) or late-interaction models (Khattab and Zaharia, 2020; Gao et al., 2021a), which predict a match score for each query-document pair thus are computationally costly, dense retrieval can be run in milliseconds, with the help of an approximate nearest neighbor (ANN) retrieval library, e.",
      "startOffset" : 35,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : ", 2020) or late-interaction models (Khattab and Zaharia, 2020; Gao et al., 2021a), which predict a match score for each query-document pair thus are computationally costly, dense retrieval can be run in milliseconds, with the help of an approximate nearest neighbor (ANN) retrieval library, e.",
      "startOffset" : 35,
      "endOffset" : 81
    }, {
      "referenceID" : 36,
      "context" : "MARCO (Nguyen et al., 2016) (533k training examples) or NQ (Kwiatkowski et al.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 47,
      "context" : "The recently proposed BEIR benchmark (Thakur et al., 2021) highlights the generalization ability of text retrieval systems.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 36,
      "context" : "The benchmark features a setting where models are trained on a large supervised dataset MS-MARCO (Nguyen et al., 2016) and then tested on 18 heterogeneous datasets of 9 tasks.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 43,
      "context" : "While existing dense retrievers need large supervised data and struggle to compete with a lexical matching approach like BM25 (Robertson and Zaragoza, 2009) for zero-shot retrieval, we take a different approach by complementing lexical matching with semantic matching.",
      "startOffset" : 126,
      "endOffset" : 156
    }, {
      "referenceID" : 17,
      "context" : "In contrastive learning, more negative instances often lead to better performance (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b).",
      "startOffset" : 82,
      "endOffset" : 139
    }, {
      "referenceID" : 54,
      "context" : "In contrastive learning, more negative instances often lead to better performance (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b).",
      "startOffset" : 82,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "In contrastive learning, more negative instances often lead to better performance (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b).",
      "startOffset" : 82,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "Compared to existing solutions MoCo (He et al., 2020) and xMoCo (Yang et al.",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 56,
      "context" : ", 2020) and xMoCo (Yang et al., 2021), ICoL does not introduce extra encoders and can solve the mismatching between",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 40,
      "context" : "We use a large-scale multi-domain corpus, C4 (Raffel et al., 2020), to train our LaPraDoR model.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : "Dense Retrieval DPR (Karpukhin et al., 2020) initializes a bi-encoder model with BERT (Devlin et al.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : ", 2020) initializes a bi-encoder model with BERT (Devlin et al., 2019) and achieves better results than earlier dense retrieval methods.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 39,
      "context" : "RocketQA (Qu et al., 2021) exploits a trained retriever to mine hard negatives and then re-train a retriever with the mined negatives.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 55,
      "context" : "ANCE (Xiong et al., 2021) dynamically mines hard negatives throughout training but requires periodic encoding of the entire corpus.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "TASB (Hofstätter et al., 2021) is a bi-encoder trained with balanced topic-aware sampling and knowledge distillation from a cross-encoder and a ColBERT model (Khattab and Zaharia, 2020), in addition to in-batch negatives.",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 30,
      "context" : ", 2021) is a bi-encoder trained with balanced topic-aware sampling and knowledge distillation from a cross-encoder and a ColBERT model (Khattab and Zaharia, 2020), in addition to in-batch negatives.",
      "startOffset" : 135,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : ", 2021) adapt MoCo (He et al., 2020), a contrastive learning algorithm that is originally proposed for image representation, to text retrieval by doubling its fast and slow encoders.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 47,
      "context" : "datasets, the BEIR benchmark (Thakur et al., 2021) highlights a main drawback of these dense retrieval systems - failure to generalize to out-of-domain data.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : "This motivates pretraining as a solution for better domain generalization (Gururangan et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 38,
      "context" : "Very recently, DPR-PAQ (Oğuz et al., 2021) highlight the importance of domain matching by using both synthetic and crawled QA data to pretrain and then fine-tune the model on downstream datasets for dialogue retrieval.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "Condenser (Gao and Callan, 2021a) is a new Transformer variant for MLM pretraining.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "On top of that, coCondenser (Gao and Callan, 2021b) adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 32,
      "context" : "Two Encoders The dual-tower architecture, as illustrated in Figure 1, is widely used in dense retrieval systems (Lee et al., 2019; Karpukhin et al., 2020; Xiong et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 29,
      "context" : "Two Encoders The dual-tower architecture, as illustrated in Figure 1, is widely used in dense retrieval systems (Lee et al., 2019; Karpukhin et al., 2020; Xiong et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 55,
      "context" : "Two Encoders The dual-tower architecture, as illustrated in Figure 1, is widely used in dense retrieval systems (Lee et al., 2019; Karpukhin et al., 2020; Xiong et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "tecture has a query encoder EQ and a document encoderED, which in our work are both BERT-like bidirectional text encoders (Devlin et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 41,
      "context" : "Compared with cross-attention models (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020), the dual-tower architecture enables pre-indexing and fast approximate nearest neighbor",
      "startOffset" : 37,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "Compared with cross-attention models (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020), the dual-tower architecture enables pre-indexing and fast approximate nearest neighbor",
      "startOffset" : 37,
      "endOffset" : 107
    }, {
      "referenceID" : 34,
      "context" : "Compared with cross-attention models (Reimers and Gurevych, 2019; Gao et al., 2020; MacAvaney et al., 2020), the dual-tower architecture enables pre-indexing and fast approximate nearest neighbor",
      "startOffset" : 37,
      "endOffset" : 107
    }, {
      "referenceID" : 32,
      "context" : "Some prior works (Lee et al., 2019; Chang et al., 2020; Karpukhin et al., 2020) use v[CLS] as the representation for the input x, but Huang et al.",
      "startOffset" : 17,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Some prior works (Lee et al., 2019; Chang et al., 2020; Karpukhin et al., 2020) use v[CLS] as the representation for the input x, but Huang et al.",
      "startOffset" : 17,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "Some prior works (Lee et al., 2019; Chang et al., 2020; Karpukhin et al., 2020) use v[CLS] as the representation for the input x, but Huang et al.",
      "startOffset" : 17,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "(2019), ICT is an effective way to pretrain a text retrieval model (Chang et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : ", Wiki Link Prediction or Body First Selection (Chang et al., 2020), ICT is fast and does not rely on a specific corpus format (e.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Dropout as Positive Instance (DaPI) DaPI is originally proposed in SimCSE (Gao et al., 2021c) as a simple strategy for perturbing intermediate representations and thus can serve as data augmentation.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "1 to the fullyconnected layers and attention probabilities in the Transformer encoders, as in BERT (Devlin et al., 2019).",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "Note that different from SimCSE (Gao et al., 2021c), in our training, only one of the two passes is with gradient.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "Previous studies (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b) show that the number of negative instances is critical to the performance of the model.",
      "startOffset" : 17,
      "endOffset" : 74
    }, {
      "referenceID" : 54,
      "context" : "Previous studies (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b) show that the number of negative instances is critical to the performance of the model.",
      "startOffset" : 17,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Previous studies (Giorgi et al., 2021; Wu et al., 2020; Gao et al., 2021b) show that the number of negative instances is critical to the performance of the model.",
      "startOffset" : 17,
      "endOffset" : 74
    }, {
      "referenceID" : 58,
      "context" : "To speed up training, we apply the in-batch negatives technique (Yih et al., 2011; Henderson et al., 2017; Gillick et al., 2019) that can reuse computation and train b queries/documents in a mini-batch simultaneously.",
      "startOffset" : 64,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "To speed up training, we apply the in-batch negatives technique (Yih et al., 2011; Henderson et al., 2017; Gillick et al., 2019) that can reuse computation and train b queries/documents in a mini-batch simultaneously.",
      "startOffset" : 64,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "To speed up training, we apply the in-batch negatives technique (Yih et al., 2011; Henderson et al., 2017; Gillick et al., 2019) that can reuse computation and train b queries/documents in a mini-batch simultaneously.",
      "startOffset" : 64,
      "endOffset" : 128
    }, {
      "referenceID" : 53,
      "context" : "stores previously encoded representations that can serve as negative instances for the current step, extending an earlier study (Wu et al., 2018).",
      "startOffset" : 128,
      "endOffset" : 145
    }, {
      "referenceID" : 21,
      "context" : "MoCo Previously, similar to our method, MoCo (He et al., 2020) exploits a queue for storing encoded representations.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 47,
      "context" : "Although dense retrieval achieves state-of-the-art performance, its performance significantly degenerates on out-of-domain data (Thakur et al., 2021).",
      "startOffset" : 128,
      "endOffset" : 149
    }, {
      "referenceID" : 43,
      "context" : "On the other hand, BM25 (Robertson and Zaragoza, 2009) demonstrates good performance without training.",
      "startOffset" : 24,
      "endOffset" : 54
    }, {
      "referenceID" : 36,
      "context" : "Early attempts at combining lexical match with dense retrieval often formulate it to a reranking task (Nguyen et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "Recently, COIL (Gao et al., 2021a) highlights the importance of lexical match and incorporates exact lexical matching into dense retrieval.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 47,
      "context" : "Benchmark We use BEIR (Thakur et al., 2021), a recently released benchmark for zero-shot evaluation of information retrieval models.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 47,
      "context" : "Table 1: Experimental results on the BEIR benchmark (Thakur et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 44,
      "context" : "Thus, we initialize our encoder with the 6-layer DistilBERT (Sanh et al., 2019), which has ∼67M parameters.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 40,
      "context" : "For our main results, we train LaPraDoR on C4 (Raffel et al., 2020) for 1M steps, which takes about 400 hours.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 47,
      "context" : "When calculating the loss, we apply a re-scaling trick of multiplying the cosine similarity score by 20 for better optimization (Thakur et al., 2021).",
      "startOffset" : 128,
      "endOffset" : 149
    }, {
      "referenceID" : 36,
      "context" : "(2) Finetuning on MS-MARCO (Nguyen et al., 2016) and zero-shot transfer to the other datasets.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 55,
      "context" : ", 2020), ANCE (Xiong et al., 2021), TASB (Hofstätter et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : ", 2021), TASB (Hofstätter et al., 2021) and GenQ (Thakur et al.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "We also consider a late interaction baseline ColBERT (Khattab and Zaharia, 2020).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 52,
      "context" : "(2021) that uses BM25 to retrieve top-100 documents and a 6-layer MiniLM (Wang et al., 2020) model to further re-rank the recalled documents.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : "Without any supervised data, LaPraDoR outperforms the previous state-of-the-art for zeroshot dense retrieval, TAS-B (Hofstätter et al., 2021), on 13 tasks (out of 18) of BEIR with an average advantage of 0.",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 44,
      "context" : "The results of “w/o PT” directly use DistilBERT (Sanh et al., 2019) for fine-tuning, which is also used to initialize our model.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "We set a baseline that only uses in-batch negatives and compare our proposed Iterative Contrastive Learning (ICoL) to MoCo (He et al., 2020) and xMoCo (Yang et al.",
      "startOffset" : 123,
      "endOffset" : 140
    }, {
      "referenceID" : 56,
      "context" : ", 2020) and xMoCo (Yang et al., 2021) for training LaPraDoR on Wikipedia in Table 2.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "The “w/o ICT” variant is equal to the original SimCSE approach (Gao et al., 2021c).",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "However, due to the nature of text retrieval, our retriever has lower ethical risk compared to a generative auto-regressive language model (Bender et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 160
    } ],
    "year" : 0,
    "abstractText" : "In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. The cache mechanism not only enlarges the numbers of negative instances but also keeps representations of cache examples in the same hidden space. We then use Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zeroshot text retrieval tasks. Experimental results show that LaPraDoR achieves state-ofthe-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5× faster) while achieving superior performance.1",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_264_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evaluating Extreme Hierarchical Multi-label Classification",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many natural language processing (NLP) problems involve classification, such as sentiment analysis, entity linking, etc. However, the adequacy of evaluation metrics is still an open problem. Different metrics such as Accuracy, F-measure or Macro Average Accuracy (MAAC) may differ substantially, seriously affecting the system optimization process. For instance, assigning all items to the majority class can be highly effective according to Accuracy and obtains a low score according to MAAC.\nIn addition, in many scenarios such as tagging in social networks (Coope et al., 2018) or topic identification (Yu et al., 2019), the classifier must assign several labels to each item (multi-label classification). This greatly complicates the evaluation problem since, in addition to the class specificity (frequency), other variables appears such as the distribution of labels per item in the gold standard, the excess or absence of labels in the system output,\netc. The evaluation problem becomes even more complicated if we consider hierarchical category structures, which are very common in NLP. For example, toxic messages are divided into different types of toxicity (Fortuna et al., 2019), named entities could be organized in nested categories (Sekine and Nobata, 2004), etc. In these scenarios, the category proximity in the hierarchical structure is an additional variable.\nEven, the problem can be further complicated. Extreme Classification scenarios address with thousands of highly unbalanced categories (Gupta et al., 2019), where a few categories are very frequent and others completely infrequent (Almagro et al., 2020). In addition, some items have no category at all and some have many. An example scenario that we will use as a case study in this article is the labelling of adverse events in medical documents.\nIn this paper, we analyse the state of the art on metrics for multi-label, hierarchical and extreme classification problems. We characterize existing metrics by means of a set of formal properties. The analysis shows that different metric families satisfy different properties, and that satisfying all of them at the same time is not straightforward.\nThen, propose an information-theoretic based metric inspired by the Information Contrast Model similarity measure (ICM), which can be particularized to simpler scenarios (e.g. flat, single labeled) while keeping its formal properties. Later, we define a set of five tests on synthetic data to compare empirically ICM against existing metrics. Finally, we explore a case study with real data which shows the suitability of ICM for such extreme scenarios. The paper ends with some conclusions and future work."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we analyze the literature on the two main evaluation problems tackled in this paper: multi-labeling and class hierarchies, keeping the focus on extreme scenarios (numerous and unbalanced classes)."
    }, {
      "heading" : "2.1 Multi-Label Classification",
      "text" : "There are three main ways of generalizing effectiveness metrics to the multi-label scenario (Zhang and Zhou, 2014). The first one consists in modeling the problem as a ranking task, i.e. the system returns an ordered label list for each item according to their suitability. Some specific ranking metrics applied in multi-label classification displayed in (Wu and Zhou, 2017) are: Ranking Loss, which is a ordinal correlation measure, one-error which is based on Precision at 1, or Average Precision. Although this approach is very common, it does not take into account the specificity of (unbalanced) classes. Jain et al. proposed the propensity versions of ranking metrics (Precision@k, nDCG) in order to weight classes according to their frequency in the data set (Jain et al., 2016). These metric variants are specially appropriate in extreme classification scenarios. However, they require system output to be in ranking format. In this paper, we focus on classification outputs, so ranking based metrics are out of our scope.\nApart from ranking metrics, multi-label effectiveness metrics have been categorized into labeland example-based metrics (Tsoumakas et al., 2010; Zhang and Zhou, 2014). Label-based evaluation measures assess and average the predictive performance for each category as a binary classification problem, where the negative category corresponds with the other categories. The most popular are the label-based Accuracy (LB-ACC) and F-measure (LB-F). In the single label scenario, the label-based F-measure converges to the traditional F and the label-based accuracy is proportional to the traditional ACC. The label-based metrics have some drawbacks. First, they do not consider the distribution of labels per item. Hits are rewarded independently of how many labels are associated to the item. Second, while items are supposed to be random samples, classes are not, so the concept of averaging results across classes is not always consistent. That is, the metric scores can vary substantially depending on how the category space is configured. Finally, if there are a large number\nof possible categories (extreme classification), the score contribution of any label has an upper limit of 1 |C| , being C the set of categories. This limit can be problematic, specially when labels are unbalanced and numerous.\nOn the other hand, the example-based metrics compute for each object, the proximity between predicted and true label sets (s(d) = {cs1, .., csn} and g(d) = {cg1, .., c g n}). Some popular ways to match category sets in multi-label classification evaluation are the Jaccard similarity (EB-JACC) which is computed as |s(d)∩g(d)||s(d)∪g(d)| (Godbole and\nSarawagi, 2004), or the precision ( |s(d)∩g(d)| |s(d)| ) , re-\ncall ( |s(d)∩g(d)| |g(d)| ) and their F combination (EB-F). Another example-based metric is the Hamming Loss (EB-HAMM) (Zhang et al., 2006) which matching function is defined as: |s(d)XOR g(d)||Cg | where Cg represents the set of categories annotated in the gold standard. Subset Accuracy (EBSUBACC) (Ghamrawi and McCallum, 2005) is a more strict measure due to it requires exact matching between both category sets. Notice that all example-based multi-label metrics converge to Accuracy in the single-label scenario. On the other hand, there are some situations in which these metrics are undefined. If both the gold standard and the system output label sets are empty, the maximum score is usually assigned to the item.\nThe drawback of these approaches is that they do not take into account the specificity of classes (i.e. unbalanced classes in extreme classification). The label propensity applied over precision and recall for single items can solve this lack. Each accurate class in the intersection is weighted according to the class propensity pc (Jain et al., 2016):\nPropP (i) =\n∑ c∈s(i)∩g(i) 1 pc |s(i)| , PropR(i) = ∑ c∈s(i)∩g(i) 1 pc |g(i)|\nThe propensity factor pc for each class is computed as: pc = 11+Ce−A log2(Nc+B) where Nc is the number of data points annotated with label c in the observed ground truth data set of size N and A, B are application specific parameters and C = (logN −1)(B+1)A. In our experiments, we set the recommended parameter values A = 0.55 and B = 1.5.\nHowever, propensity precision and recall values are not upper bounded as 1pc tends to infinite when pc tends to zero. In order to solve this issue, we replace the normalization factors |s(i)| and |g(i)|\nwith the accumulation of inverse propensities in the system output or the gold standard. We also add the empty class c∅ in both the system output and the gold standard in order to capture the specificity of classes in the mono-label scenario:\nPropP (i) =\n∑ c∈s′(i)∩g′(i)\n1 pc∑\nc∈s′(i) 1 pc\nPropR(i) =\n∑ c∈s′(i)∩g′(i)\n1 pc∑\nc∈g′(i) 1 pc\nwhere s′(i) = s(i)∪ {c∅} and g′(i) = g(i)∪ {c∅}. Propensity F-measure (PROP-F) is computed as the harmonic mean of these values."
    }, {
      "heading" : "2.2 Hierarchical Classification",
      "text" : "There are different taxonomies of hierarchical classification metrics (Costa et al., 2007; Kosmopoulos et al., 2013). Kosmopoulos et al. distinguish between pair and set-based metrics. Pair-based metrics weight hits or misses according to the distance between categories in the hierarchy. This distance depends on the number of intermediate nodes (Wang et al., 1999; Sun and Lim, 2001), with the disadvantage that the specificity of the categories is not taken into account. Depth-based distance metrics include the class depth in the metric (Blockeel et al., 2002). However, depending on their frequency, leaf nodes at the first levels may be more specific than leaf nodes at deeper levels.\nWe can compare the predicted and true single labels by means of standard ontological similarity measures such as Leackock and Chodorow (pathbased) (Leacock and Chodorow, 1998), Wu and Palmer (Wu and Palmer, 1994), Resnik (depthbased) (Resnik, 1999), Jiang and Conrath (Jiang and Conrath, 1997) or Lin (Lin, 1998) similarities. The last two are based on the notion of Information Content (IC) or category specificity, i.e., the amount of items belonging to the category or any of its descendants.\nHowever, extending pair-based hierarchical metrics to the multi-label scenario is not straightforward. Sun and Lim extended Accuracy, Precision and Recall measures for ontological distance based metrics (Sun and Lim, 2001). This method has two drawbacks. First, it requires defining a neutral hierarchical distance, i.e., an acceptable distance threshold for range normalization purposes. The second drawback is that it inherits the weaknesses of label-based metrics (see previous section). Blockeel et al. proposed computing a kernel and thus\ndefine a Euclidean distance metric between sums of class values (Blockeel et al., 2002). The drawback is that they assume a previously defined distance metric between categories and the origin and between different categories. Information based ontological similarity measures such as Jiang and Conrath or Lin’s similarity do not fit in this framework since they are not upper-bounded.\nOn the other hand, set-based metrics (also called hierarchical-based) consider the ancestor overlap (Kiritchenko et al., 2004; Costa et al., 2007). More concretely, hierarchical precision and recall are computed as the intersection of ancestor divided by the amount of ancestors of the system output category and of the gold standard respectively1. Their combination is the Hierarchical Fmeasure (HF). Since these metrics are based on category set overlap, they can be applied as example based multi-label classification by joining ancestors and computing the F measure. Their drawback is that the specificity of categories is not strictly captured since they assume a correspondence between specificity and hierarchical deepness. However, this correspondence is not necessarily true. Categories in first levels can be infrequent whereas leaf categories can be very common in the data set.\nIn this paper, we propose an information theoretic similarity measure called Information Contrast Model (ICM). ICM is an example-based metric as it is computed per item. Just like HF, ICM is a set-based multi-label metric as it computes the similarity between category sets. Unlike HF, ICM takes into account the statistical specificity of categories."
    }, {
      "heading" : "3 Formal Properties",
      "text" : "In order to define the set of desirable properties, we formalize both the gold standard g and the system output s as sets of item/category assignments (i, c) ∈ I × C, where I and C represent the set of items and categories respectively. We will denote as P (cj) the probability of items to be classified as cj in the gold standard (P ((i, cj) ∈ g|i ∈ I)). We also assume that the categories in the hierarchical structure are subsumed. For instance, items in a PERSON_NAMED_ENTITY category are implicitly labeled with the parent category NAMED_ENTITY. The common ancestor with\n1In our experiments, when computing the ancestor overlap we consider the common empty label (root class) in order to avoid undefined situations\nmaximum depth is denoted as lso(c1, c2) and the descendant categories are denoted as Desc(c) including itself.\nThe first property is related to hits in flat or hierarchical classification:\nProperty 1 [Strict Monotonicity] A hit increases effectiveness. Given a flat single label category structure, if (i, c) ∈ g\\s, then2 Eff(s∪{(i, c)}) > Eff(s)\nThe next two properties state that the specificity of both the predicted and the true category affects the metric score. For instance, an error or a hit in an infrequent category should have more effect than in the majority category.\nProperty 2 [True Category Specificity] Given a flat single label category distribution, if P (c1) < P (c2) and (i, c1), (i, c2) ∈ g \\ s, then Eff(s ∪ {(i, c1)}) > Eff(s ∪ {(i, c2)}).\nProperty 3 [Wrong Category Specificity] Given a flat single label category distribution, if P (c1) < P (c2) and (i, c1), (i, c2) /∈ g ∪ s, then Eff(s ∪ {(i, c1)}) < Eff(s ∪ {(i, c2)}).\nThe hierarchical proximity is captured by the following property.\nProperty 4 [Hierarchical Proximity] Under equiprobable categories (P (c1) = P (c2) = P (c3)), the deepness of the common ancestor affects similarity. Given a single label hierarchical category structure, if s(i) = ∅, g(i) = c1 and lso(c1, c2) ∈ Desc(lso(c1, c3)) then Eff(s ∪ {(i, c2)}) > Eff(s ∪ {(i, c1)}). The last two properties are related with the multilabeling problem. Property 5 rewards the amount of predicted categories per item. Property 6 rewards hits on multiple items regarding a single item with multiple categories.\nProperty 5 [Multi-label Monotonicity] The amount of predicted categories increases effectiveness. Given a flat multi-label category structure, if (i, c) ∈ g \\ s, then Eff(s ∪ {(i, c)}) > Eff(s)\nProperty 6 [Label vs. Item Quantity] n hits on different items are more beneficial than n labels assigned to one item. Given a flat multi-label category distribution, if ∀j = 1..n((j, cj) ∈ g \\ s) and ∀j = 1..n, i > n((i, cj) ∈ g \\ s) then Eff(s ∪ {(1, c1), .., (n, cn)}) > Eff(s ∪ {(i, c1), .., (i, cn)}).\n2Notice that x ∈ X \\ Y ≡ x ∈ X ∧ x /∈ Y"
    }, {
      "heading" : "4 Formal Analysis of the Metrics",
      "text" : "In this section, we analyze existing metrics on the basis of the proposed formal properties (Table 1). Most of metrics satisfy Strict Monotonicity in single label scenarios. The label-based metric LB-F captures the true and wrong category specificity via the recall component. The example-based metric PROP-F (modified as described in Section 2) captures these properties via the propensity factor. Notice that the original propensity F-measure does not capture the wrong category specificity (Property 3) given that the pc factor is applied only to hits. In addition, both kind of metrics do not capture hierarchical structures. The contribution of example regarding label-based metrics is that, as label-based metrics are computed item by item, the property Label vs. Item Quantity is satisfied (Property 6). The exception is EB-HAMM which does not normalize the results with respect to the amount of labels assigned to the item.\nUnlike previous metrics, the set based F-measure (HF) captures the hierarchical structure (Property 4). However, it does not capture the category specificity (properties 2 and 3). Some information-based ontological similarity measures, (Lin and Jiang & Conrath) capture both the category specificity and the hierarchical structure. However, they are not defined for multi-label classification (properties 5 and 6). In sum, different metric families satisfy different properties, and that satisfying all of them at the same time is not straightforward. The properties of ICM are described in the next section."
    }, {
      "heading" : "5 Information Contrast Model",
      "text" : "The Information Contrast Model (ICM) is a similarity measure that unifies measures based on both feature sets and Information Theory (Amigó et al., 2020). Given two feature sets A and B, ICM is computed as:\nICM(A,B) = α1IC(A)+α2IC(B)−βIC(A∪B)\nWhere IC(A) represents the information content (−log(P (A)). The intuition is that the more the feature sets are unlikely to occur simultaneously (large IC(A∪B)), the less they are similar. Given a fixed joint IC, the more the feature sets are specific (IC(A) and IC(B)), the more they are similar. ICM is grounded on similarity axioms supported by the literature in both information access and cognitive sciences. In addition, it generalizes the\nPointwise Mutual Information and the Tversky’s linear contrast model (Amigó et al., 2020)."
    }, {
      "heading" : "5.1 Computing Information Content",
      "text" : "The IC of a single category corresponds with the probability of items to appear in the category or any of its descendant. It can be estimated as follows:\nIC(c) = −log2(P (c)) ' −log2\n(∣∣⋃ c′∈{c}∪Desc(c) Ic′ ∣∣∣∣⋃ c′∈C Ic′ ∣∣ )\nwhere Ic′ represent the set of items assigned to the category c′ and Desc(c) represents the set of descendant categories. In order to estimate the IC of category set, we state the following considerations. The first one is that, given two categories A and B the common ancestor represents their intersection in terms of feature sets:\n{ci} ∩ {cj} = lso(ci, cj) (1)\nThe second consideration is that we assume Information Additivity, i.e. the IC of the union of two sets is the sum of their IC’s minus the IC of its intersection:\nIC({ci}∪{cj}) = IC(ci)+IC(cj)−I({ci}∩{cj}) (2) Equations 1 and 2 are enough to compute ICM in the single label scenario. Generalizing for category\nsets:\nIC({c1, c2, .., cn}) = IC (⋃ i {ci} ) = IC(c1) + IC({c2, .., cn})− IC({c1} ∩ {c2, .., cn})\nwhere, according to the transitivity property; {c1} ∩ {c2, .., cn} = ⋃\ni=2..n\n({c1} ∩ {ci})\nand according to Equation 1, it is equivalent to⋃ i=2..n{lso(c1, ci)}. Then, we finally obtain a recursive function to compute the IC of a category set:\nIC({c1, c2, .., cn}) =\nIC(c1) + IC ( ⋃ i=2..n {ci} ) − IC ( ⋃ i=2..n {lso(c1, ci)} )\nIn the case of ICM, it is possible the need for estimating the IC of classes that do not appear in the gold standard. Therefore, we have not evidence about its frequency or probability. We apply a smoothing approach by considering the minimum probability 1|I| ."
    }, {
      "heading" : "5.2 Parameterization and Formal Properties",
      "text" : "On the basis of five general similarity axioms, in (Amigó et al., 2020) it is stated that the ICM parameters should satisfy α1, α2 < β < α1 + α2.\nWe propose the parameter values α1 = α2 = 2 an β = 3. This parameterization leads to the following instantiations for each particular classification scenario. In the hierarchical mono-label scenario, it becomes into (equations 1 and 2):\nICM(c1, c2) = −IC(c1)−IC(c2)+IC(lso(c1, c2)) (3) which is similar to the Jiang and Conrath ontological similarity measure. In the flat multi-label scenario, it becomes into:\nICM(C,C ′) = ∑\nc∈C∩C′ IC(c)− ∑ c∈C\\C′ ∪C′\\C IC(c)\n(4) which is an information additive example-based metric. That is, the information content of the common categories minus the differences. Finally, in the traditional flat mono-label scenario, it becomes into:\nICM(c1, c2) ' { IC(c1) if c1 = c2 −IC(c1)− IC(c2) i.o.c. (5) which corresponds with Accuracy weighted according to the information content of categories.\nAccording to the flat mono-label instantiation (Equation 5) ICMα1=α2=2,β=3 satisfies the properties 1 2 and 3. According to the single label hierarchical instantiation (Equation 3) Property 4 is satisfied. According to the flat multi-label instantiation (Equation 4), Property 5 is satisfied. Unfortunately, the label vs item quantity property is not strictly satisfied given that the gain per hit is additive in non hierarchical scenarios (Property 6). However, in the experiments we will see that the hit gain on items with many categories is smoothed out if the categories are related to each other by a hierarchical structure."
    }, {
      "heading" : "6 Experiments on Synthetic Data",
      "text" : "Different evaluation aspects such as error rate, category specificity, hierarchical structures, etc., may have more or less weight depending on the scenario. These aspects correspond to the formal properties defined in the previous section. We perform a set of tests in order to quantify the suitability of metrics with respect to each property or evaluation aspect.\nFirst, we generate the following synthetic data set. Given a hierarchical structure structure of 700 categories exposed in Figure 1, and 1000 items, we generate assignments for each pair\nitem/category (i, c) with a probability of pi · pc where pi = max ( 51−i 2225 , 1 2225 ) with i = 1..1000 and pc = max( 512c ,1)\n1713 where c = 1..700. We repeat this 1000 times. The result is a distribution (300, 150, 40, .., 0.6, 0.6) items per category and (22.5, 22, 21.6, 21.1, ..., 0.5, 0.5) labels per item. The purpose is to ensure unbalanced assignments across items and classes. We generate 1000 gold standards by reordering the category identifiers c each time in the pc computation in order to alter the distribution of items in the hierarchical structure.\nWe consider in this experiment the metrics labelbased Accuracy and F-measure (LB-ACC and LB-F), the example-based metrics Hamming (EBHAMM), Jaccard (EB-JACC), Subset Accuracy (EB-SUBACC), F-measure (EB-F) and Propensity F-measure (PROP-F), the Hierarchical F-measure (HF) and ICM. The ontological similarity metrics are discarded given that they are not defined for the multi-label case. Ranking based metrics are discarded as the synthetic data set does not include graded assignments.\nAfter this, we perform the following tests by comparing two noisy versions of the gold standard. The test result is the percentage of cases in which the hypothetically worse noised output is outscored by the best noised output (Table 2). Ties count 0.5.\nIn the first experiment referred in Table 2 as Sensitivity to Error Rate, we randomly remove selected (i, c) assignments with a probability of 0.09 and 0.1 for the best and worst outputs respectively. For all metrics the best output outperforms the worst output in more that 50% of cases. LBACC and EB-HAMM seems to be specially sensitive to the error rate. This is due to the fact that they\ndo not consider other aspects such as the category specificity or the hierarchical proximity. Surprisingly, ICM achieves a relatively high error rate sensitivity although it also consider other aspects. We do not have a clear explanation for this.\nThe second experiment is the True Category Specificity test. With an error rate of 0.05, for the best output, we remove a randomly selected single label assignment. For the worst output, the first select randomly a category and we remove an assignment to a single labeled item. The result is that the best output tends to concentrate the gaps in frequent categories to a greater extent than the worst output. At the table shows, the metrics that satisfy the corresponding property achieve high scores (LB-F, PROP-F and ICM).\nThe third experiment is the Wrong Category Specificity test. With an error rate of 0.05, we select an assignment (i, c) randomly from items with a single label. For the best output we replace c with the most frequent class different than c. For the worst output, we replace c with a randomly selected category different than c. We obtain the same result than in the previous experiment.\nThe fourth experiment is the Hierarchical Similarity test. Again, with an error rate of 0.05, we select an assignment (i, c) randomly from single labeled items with leaf categories. For the best output we replace c with a sister wrong category. For the worst output, we replace cwith a randomly selected wrong category. Again, the metrics that satisfy the corresponding property achieve high scores.\nThe last test is Item Specificity. For the best output, we randomly select an assignment (i, c) (with the same error rate 0.05). For the worst output, we randomly select an item i, and we take one if\nits assignments (i, c). In both cases, the category is replaced with a randomly selected wrong label. The effect is that the best output concentrates errors in items with many labels. Again, those metrics that satisfy the corresponding metric achieve high performance. The label-based F-measure tends to reward the worst output. The reason is that items with many labels tend to concentrate diverse labels. Therefore, the label-based F measure penalizes the best output. As discussed in the previous section, although ICM does not satisfy the property, the hit gain on items with many categories is smoothed out if the categories are related to each other by a hierarchical structure."
    }, {
      "heading" : "7 A Case Study",
      "text" : "The problem addressed is the automatic encoding of discharge reports (Dermouche et al., 2016; Bampa and Dalianis, 2020) from a Spanish hospital to detect adverse events (AEs) from CIE-10-ES3, the Spanish version of the tenth revision of the International Classification of Diseases (ICD-10).\nAEs detection fits to the scenario tackled in this article due to the following reasons: (i) Extreme: CIE-10-ES contains 4816 codes related to AEs, which probability follows a power-law distribution since most of them rarely appear in health records or even they do not appear; (ii) Hierarchical: CIE10-ES is a hierarchy with six levels: an empty root (c∅ such that IC(c∅) = 0), and then a level composed by three-character-codes categories which can be divided into successive nested subcategories adding characters until seven-character-codes at most; and (iii) Multi-label classification: Each\n3https://eciemaps.mscbs.gob.es/ecieMaps/\ndischarge report could have associated with several AEs codes.\nWe have used a corpus composed of 36264 real anonymized discharge reports (Almagro et al., 2020) annotated with AEs codes by experts. The corpus has been divided into three data sets, training, development and test, following the proportion 50%-30%-20% respectively. The corpus includes only 671 AEs codes of 4816 and 84% of the discharge reports have no AEs, so the data is highly biased and unbalanced.\nWe have applied five simple baselines in order to analyze the behaviour of the metrics: (i) ALL NONE does not assign any code to each item; (ii) MOST FREQ. assigns the most frequent AE code in the training data set (T45.1X5A) to each item, which just appears in 68 items of 7253; (iii) MATCH 75% divides each item into sentences and assigns a code if a sentence contains 75% of the words of the code description avoiding stop-words; (iv) SVM DESCR. creates a binary classifier for each AE code in the training set using the presence of words of the AEs codes descriptions in the items as features, excepting stop-words; (v) SVM CODES: similar to the previous one but using as features the annotated non-AEs codes in order to check if AEs codes are related to non-AEs codes. Note that MATCH 75% is able to assign any AE, but the SVM baselines are only able to assign AEs appearing in the training data set.\nTable 3 shows the metrics results obtained by each baseline and these values normalized. LBACC, LB-F and EB-HAMM reward the absence of most of the labels in the corpus, so they are not suitable in this scenario. The rest of the metrics sort systems in the same way. The particularity of ICM is that, as shows the normalized results, the\nbaseline MATCH 75% is penalized with respect to ALL NONE to a greater extent than in other metrics, since MATCH 75% assigns many codes incorrectly, whereas ALL NONE does not provide any information. Another slight particularity of ICM is that the system SVM CODES is rewarded against the rest of baselines to a greater extent. Notice that SVM CODES achieves 269 hits while SVM DESCR achieves 77 hits."
    }, {
      "heading" : "8 Conclusions and Future Work",
      "text" : "The definition of evaluation metrics is an open problem for extreme hierarchical multi-label classification scenarios due to the role of several variables, for instance, a huge number of labels, unbalanced and biased label and item distributions, proximity between classes into the hierarchy, etc. Our formal analysis shows that metrics from different families (label, example, set-based, ontological similarity measures etc.) satisfy different properties and capture different evaluation aspects. The information-theoretic metric ICM proposed in this paper, combines strengths from different families. Just like example-based multi-label metrics, it computes scores by items. Just like set-based metrics, it compares hierarchical category sets. Just like some ontological similarity measures (Lin or Jiang and Conrath), it considers the specificity of categories in terms of Information Content. Our experiments using synthetic and real data show the suitability of ICM with respect to existing metrics.\nICM does not strictly hold the label vs. item quantity property. We propose to adapt ICM in order to guarantee all the formal properties as future work."
    } ],
    "references" : [ {
      "title" : "ICD-10 coding of spanish electronic discharge summaries: An extreme classification problem",
      "author" : [ "Mario Almagro", "Raquel Martínez", "Víctor Fresno", "Soto Montalvo." ],
      "venue" : "IEEE Access, 8:100073–100083.",
      "citeRegEx" : "Almagro et al\\.,? 2020",
      "shortCiteRegEx" : "Almagro et al\\.",
      "year" : 2020
    }, {
      "title" : "On the foundations of similarity in information access",
      "author" : [ "Enrique Amigó", "Fernando Giner", "Julio Gonzalo", "Felisa Verdejo." ],
      "venue" : "Inf. Retr. J., 23(3):216– 254.",
      "citeRegEx" : "Amigó et al\\.,? 2020",
      "shortCiteRegEx" : "Amigó et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting adverse drug events from Swedish electronic health records using text mining",
      "author" : [ "Maria Bampa", "Hercules Dalianis." ],
      "venue" : "Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020), pages 1–",
      "citeRegEx" : "Bampa and Dalianis.,? 2020",
      "shortCiteRegEx" : "Bampa and Dalianis.",
      "year" : 2020
    }, {
      "title" : "Hierarchical multi-classification",
      "author" : [ "Hendrik Blockeel", "Maurice Bruynooghe", "Saso Dzeroski", "Jan Ramon", "Jan Struyf." ],
      "venue" : "Workshop Notes of the KDD’02 Workshop on Multi-Relational Data Mining, pages 21–35.",
      "citeRegEx" : "Blockeel et al\\.,? 2002",
      "shortCiteRegEx" : "Blockeel et al\\.",
      "year" : 2002
    }, {
      "title" : "A neural architecture for multi-label text classification",
      "author" : [ "Sam Coope", "Yoram Bachrach", "Andrej Zukov Gregoric", "José Rodríguez", "Bogdan Maksak", "Conan McMurtie", "Mahyar Bordbar." ],
      "venue" : "Intelligent Systems and Applications - Proceedings of the 2018 In-",
      "citeRegEx" : "Coope et al\\.,? 2018",
      "shortCiteRegEx" : "Coope et al\\.",
      "year" : 2018
    }, {
      "title" : "A review of performance evaluation measures for hierarchical classifiers",
      "author" : [ "Eduardo P. Costa", "Ana C. Lorena", "Andre C.P.L.F. Carvalho", "Alex A. Freitas." ],
      "venue" : "AAAI Workshop - Technical Report.",
      "citeRegEx" : "Costa et al\\.,? 2007",
      "shortCiteRegEx" : "Costa et al\\.",
      "year" : 2007
    }, {
      "title" : "Supervised topic models for diagnosis code assignment to discharge summaries",
      "author" : [ "Mohamed Dermouche", "Julien Velcin", "Rémi Flicoteaux", "Sylvie Chevret", "Namik Taright." ],
      "venue" : "Computational Linguistics and Intelligent Text Processing - 17th International",
      "citeRegEx" : "Dermouche et al\\.,? 2016",
      "shortCiteRegEx" : "Dermouche et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchically-labeled Portuguese hate speech dataset",
      "author" : [ "Paula Fortuna", "João Rocha da Silva", "Juan SolerCompany", "Leo Wanner", "Sérgio Nunes." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 94–104, Florence,",
      "citeRegEx" : "Fortuna et al\\.,? 2019",
      "shortCiteRegEx" : "Fortuna et al\\.",
      "year" : 2019
    }, {
      "title" : "Collective multi-label classification",
      "author" : [ "Nadia Ghamrawi", "Andrew McCallum." ],
      "venue" : "Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05, page 195–200, New York, NY, USA. Association for",
      "citeRegEx" : "Ghamrawi and McCallum.,? 2005",
      "shortCiteRegEx" : "Ghamrawi and McCallum.",
      "year" : 2005
    }, {
      "title" : "Discriminative methods for multi-labeled classification",
      "author" : [ "Shantanu Godbole", "Sunita Sarawagi." ],
      "venue" : "Advances in Knowledge Discovery and Data Mining, pages 22–30, Berlin, Heidelberg. Springer Berlin Heidelberg.",
      "citeRegEx" : "Godbole and Sarawagi.,? 2004",
      "shortCiteRegEx" : "Godbole and Sarawagi.",
      "year" : 2004
    }, {
      "title" : "Distributional semantics meets multi-label learning",
      "author" : [ "Vivek Gupta", "Rahul Wadbude", "Nagarajan Natarajan", "Harish Karnick", "Prateek Jain", "Piyush Rai." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innova-",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications",
      "author" : [ "Himanshu Jain", "Yashoteja Prabhu", "Manik Varma." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowl-",
      "citeRegEx" : "Jain et al\\.,? 2016",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic similarity based on corpus statistics and lexical taxonomy",
      "author" : [ "Jay J. Jiang", "David W. Conrath." ],
      "venue" : "Proc. of the Int’l. Conf. on Research in Computational Linguistics, pages 19–33.",
      "citeRegEx" : "Jiang and Conrath.,? 1997",
      "shortCiteRegEx" : "Jiang and Conrath.",
      "year" : 1997
    }, {
      "title" : "Hierarchical text categorization as a tool of associating genes with gene ontology codes",
      "author" : [ "Svetlana Kiritchenko", "Stan Matwin", "Fazel Famili." ],
      "venue" : "Proceedings of the 2nd European Workshop on Data Mining and Text Mining in Bioinformatics.",
      "citeRegEx" : "Kiritchenko et al\\.,? 2004",
      "shortCiteRegEx" : "Kiritchenko et al\\.",
      "year" : 2004
    }, {
      "title" : "Evaluation measures for hierarchical classification: a unified view and novel approaches",
      "author" : [ "Aris Kosmopoulos", "Ioannis Partalas", "Eric Gaussier", "Georgios Paliouras", "Ion Androutsopoulos." ],
      "venue" : "Data Mining and Knowledge Discovery, 29.",
      "citeRegEx" : "Kosmopoulos et al\\.,? 2013",
      "shortCiteRegEx" : "Kosmopoulos et al\\.",
      "year" : 2013
    }, {
      "title" : "Combining local context and wordnet similarity for word sense identification",
      "author" : [ "Claudia Leacock", "Martin Chodorow." ],
      "venue" : "WordNet: An electronic lexical database, 49(2):265–283.",
      "citeRegEx" : "Leacock and Chodorow.,? 1998",
      "shortCiteRegEx" : "Leacock and Chodorow.",
      "year" : 1998
    }, {
      "title" : "An information-theoretic definition of similarity",
      "author" : [ "Dekang Lin." ],
      "venue" : "Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998), Madison, Wisconsin, USA, July 24-27, 1998, pages 296–304. Morgan Kaufmann.",
      "citeRegEx" : "Lin.,? 1998",
      "shortCiteRegEx" : "Lin.",
      "year" : 1998
    }, {
      "title" : "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language",
      "author" : [ "Philip Resnik." ],
      "venue" : "J. Artif. Intell. Res., 11:95–130.",
      "citeRegEx" : "Resnik.,? 1999",
      "shortCiteRegEx" : "Resnik.",
      "year" : 1999
    }, {
      "title" : "Definition, dictionaries and tagger for extended named entity hierarchy",
      "author" : [ "Satoshi Sekine", "Chikashi Nobata." ],
      "venue" : "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04), Lisbon, Portugal. European Language",
      "citeRegEx" : "Sekine and Nobata.,? 2004",
      "shortCiteRegEx" : "Sekine and Nobata.",
      "year" : 2004
    }, {
      "title" : "Hierarchical text classification and evaluation",
      "author" : [ "Aixin Sun", "Ee-Peng Lim." ],
      "venue" : "Proceedings of the 2001 IEEE International Conference on Data Mining, 29 November - 2 December 2001, San Jose, California, USA, pages 521–528. IEEE Computer Soci-",
      "citeRegEx" : "Sun and Lim.,? 2001",
      "shortCiteRegEx" : "Sun and Lim.",
      "year" : 2001
    }, {
      "title" : "Mining multi-label data",
      "author" : [ "Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis P. Vlahavas." ],
      "venue" : "Data Mining and Knowledge Discovery Handbook, 2nd ed, pages 667–685. Springer.",
      "citeRegEx" : "Tsoumakas et al\\.,? 2010",
      "shortCiteRegEx" : "Tsoumakas et al\\.",
      "year" : 2010
    }, {
      "title" : "Building hierarchical classifiers using class proximity",
      "author" : [ "Ke Wang", "Senqiang Zhou", "Shiang Chen Liew." ],
      "venue" : "VLDB’99, Proceedings of 25th International Conference on Very Large Data Bases, September 7-10, 1999, Edinburgh, Scotland, UK,",
      "citeRegEx" : "Wang et al\\.,? 1999",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 1999
    }, {
      "title" : "A unified view of multi-label performance measures",
      "author" : [ "Xi-Zhu Wu", "Zhi-Hua Zhou." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, page 3780–3788. JMLR.org.",
      "citeRegEx" : "Wu and Zhou.,? 2017",
      "shortCiteRegEx" : "Wu and Zhou.",
      "year" : 2017
    }, {
      "title" : "Verb semantics and lexical selection",
      "author" : [ "Zhibiao Wu", "Martha S. Palmer." ],
      "venue" : "32nd Annual Meeting of the Association for Computational Linguistics, 27-30 June 1994, New Mexico State University, Las Cruces, New Mexico, USA, Proceedings, pages 133–",
      "citeRegEx" : "Wu and Palmer.,? 1994",
      "shortCiteRegEx" : "Wu and Palmer.",
      "year" : 1994
    }, {
      "title" : "Hierarchical topic modeling of twitter data for online analytical processing",
      "author" : [ "Dongjin Yu", "Dengwei Xu", "Dongjing Wang", "Zhiyong Ni." ],
      "venue" : "IEEE Access, 7:12373–12385.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "A review on multi-label learning algorithms",
      "author" : [ "Min-Ling Zhang", "Zhi-Hua Zhou." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 26(8):1819–1837.",
      "citeRegEx" : "Zhang and Zhou.,? 2014",
      "shortCiteRegEx" : "Zhang and Zhou.",
      "year" : 2014
    }, {
      "title" : "Ensemble pruning via semi-definite programming",
      "author" : [ "Yi Zhang", "Samuel Burer", "W. Nick Street." ],
      "venue" : "Journal of Machine Learning Research, 7:1315–1338.",
      "citeRegEx" : "Zhang et al\\.,? 2006",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In addition, in many scenarios such as tagging in social networks (Coope et al., 2018) or topic identification (Yu et al.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : ", 2018) or topic identification (Yu et al., 2019), the classifier must assign several labels to each item (multi-label classification).",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "For example, toxic messages are divided into different types of toxicity (Fortuna et al., 2019), named entities could be organized in nested categories (Sekine and Nobata, 2004), etc.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : ", 2019), named entities could be organized in nested categories (Sekine and Nobata, 2004), etc.",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Extreme Classification scenarios address with thousands of highly unbalanced categories (Gupta et al., 2019), where a few categories are very frequent and others completely infrequent (Almagro et al.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "There are three main ways of generalizing effectiveness metrics to the multi-label scenario (Zhang and Zhou, 2014).",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "applied in multi-label classification displayed in (Wu and Zhou, 2017) are: Ranking Loss, which is a ordinal correlation measure, one-error which is based on Precision at 1, or Average Precision.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "proposed the propensity versions of ranking metrics (Precision@k, nDCG) in order to weight classes according to their frequency in the data set (Jain et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "Apart from ranking metrics, multi-label effectiveness metrics have been categorized into labeland example-based metrics (Tsoumakas et al., 2010; Zhang and Zhou, 2014).",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 25,
      "context" : "Apart from ranking metrics, multi-label effectiveness metrics have been categorized into labeland example-based metrics (Tsoumakas et al., 2010; Zhang and Zhou, 2014).",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 26,
      "context" : "Another example-based metric is the Hamming Loss (EB-HAMM) (Zhang et al., 2006) which matching function is defined as: |s(d)XOR g(d)| |Cg | where Cg represents the set of categories annotated in the gold standard.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Each accurate class in the intersection is weighted according to the class propensity pc (Jain et al., 2016):",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "There are different taxonomies of hierarchical classification metrics (Costa et al., 2007; Kosmopoulos et al., 2013).",
      "startOffset" : 70,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "There are different taxonomies of hierarchical classification metrics (Costa et al., 2007; Kosmopoulos et al., 2013).",
      "startOffset" : 70,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "This distance depends on the number of intermediate nodes (Wang et al., 1999; Sun and Lim, 2001),",
      "startOffset" : 58,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "This distance depends on the number of intermediate nodes (Wang et al., 1999; Sun and Lim, 2001),",
      "startOffset" : 58,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "Depth-based distance metrics include the class depth in the metric (Blockeel et al., 2002).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "We can compare the predicted and true single labels by means of standard ontological similarity measures such as Leackock and Chodorow (pathbased) (Leacock and Chodorow, 1998), Wu and Palmer (Wu and Palmer, 1994), Resnik (depthbased) (Resnik, 1999), Jiang and Conrath (Jiang and Conrath, 1997) or Lin (Lin, 1998) similarities.",
      "startOffset" : 147,
      "endOffset" : 175
    }, {
      "referenceID" : 23,
      "context" : "We can compare the predicted and true single labels by means of standard ontological similarity measures such as Leackock and Chodorow (pathbased) (Leacock and Chodorow, 1998), Wu and Palmer (Wu and Palmer, 1994), Resnik (depthbased) (Resnik, 1999), Jiang and Conrath (Jiang and Conrath, 1997) or Lin (Lin, 1998) similarities.",
      "startOffset" : 191,
      "endOffset" : 212
    }, {
      "referenceID" : 17,
      "context" : "We can compare the predicted and true single labels by means of standard ontological similarity measures such as Leackock and Chodorow (pathbased) (Leacock and Chodorow, 1998), Wu and Palmer (Wu and Palmer, 1994), Resnik (depthbased) (Resnik, 1999), Jiang and Conrath (Jiang and Conrath, 1997) or Lin (Lin, 1998) similarities.",
      "startOffset" : 234,
      "endOffset" : 248
    }, {
      "referenceID" : 12,
      "context" : "We can compare the predicted and true single labels by means of standard ontological similarity measures such as Leackock and Chodorow (pathbased) (Leacock and Chodorow, 1998), Wu and Palmer (Wu and Palmer, 1994), Resnik (depthbased) (Resnik, 1999), Jiang and Conrath (Jiang and Conrath, 1997) or Lin (Lin, 1998) similarities.",
      "startOffset" : 268,
      "endOffset" : 293
    }, {
      "referenceID" : 16,
      "context" : "We can compare the predicted and true single labels by means of standard ontological similarity measures such as Leackock and Chodorow (pathbased) (Leacock and Chodorow, 1998), Wu and Palmer (Wu and Palmer, 1994), Resnik (depthbased) (Resnik, 1999), Jiang and Conrath (Jiang and Conrath, 1997) or Lin (Lin, 1998) similarities.",
      "startOffset" : 301,
      "endOffset" : 312
    }, {
      "referenceID" : 19,
      "context" : "Sun and Lim extended Accuracy, Precision and Recall measures for ontological distance based metrics (Sun and Lim, 2001).",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "proposed computing a kernel and thus define a Euclidean distance metric between sums of class values (Blockeel et al., 2002).",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, set-based metrics (also called hierarchical-based) consider the ancestor overlap (Kiritchenko et al., 2004; Costa et al., 2007).",
      "startOffset" : 100,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "On the other hand, set-based metrics (also called hierarchical-based) consider the ancestor overlap (Kiritchenko et al., 2004; Costa et al., 2007).",
      "startOffset" : 100,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "The Information Contrast Model (ICM) is a similarity measure that unifies measures based on both feature sets and Information Theory (Amigó et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "Pointwise Mutual Information and the Tversky’s linear contrast model (Amigó et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "On the basis of five general similarity axioms, in (Amigó et al., 2020) it is stated that the ICM parameters should satisfy α1, α2 < β < α1 + α2.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "The problem addressed is the automatic encoding of discharge reports (Dermouche et al., 2016; Bampa and Dalianis, 2020) from a Spanish hospital to detect adverse events (AEs) from CIE-10-ES3, the Spanish version of the tenth revision of the International Classification of Diseases (ICD-10).",
      "startOffset" : 69,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "The problem addressed is the automatic encoding of discharge reports (Dermouche et al., 2016; Bampa and Dalianis, 2020) from a Spanish hospital to detect adverse events (AEs) from CIE-10-ES3, the Spanish version of the tenth revision of the International Classification of Diseases (ICD-10).",
      "startOffset" : 69,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "real anonymized discharge reports (Almagro et al., 2020) annotated with AEs codes by experts.",
      "startOffset" : 34,
      "endOffset" : 56
    } ],
    "year" : 0,
    "abstractText" : "Several natural language processing (NLP) tasks are defined as a classification problem in its most complex form: Multi-label Hierarchical Extreme classification, in which items may be associated with multiple classes from a set of thousands of possible classes organized in a hierarchy and with a highly unbalanced distribution both in terms of class frequency and the number of labels per item. We analyze the state of the art of evaluation metrics based on a set of formal properties and we define an information theoretic based metric inspired by the Information Contrast Model (ICM). Experiments on synthetic data and a case study on real data show the suitability of the ICM for such scenarios.",
    "creator" : null
  }
}
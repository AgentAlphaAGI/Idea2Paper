{
  "name" : "ARR_2022_62_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Tables store rich numerical data, so a wide range of tasks require numerical reasoning over (semi)structured tabular context, such as question answering over tables (Cheng et al., 2021; Chen et al., 2021b; Zhu et al., 2021), table-to-text (Suadaa et al., 2021; Moosavi et al., 2021; Cheng et al., 2021), spreadsheet formula prediction (Chen et al., 2021a), and table structure understanding (Koci et al., 2019). Take Table#2 in Figure 1 as an example, both suggesting the formula (C4-B4)/B4 for cell D4 and answering “0.61%” to the question require numerical reasoning capabilities of (1) understanding the contextual meaning of individual numerical cells, e.g., “11.49” at B4 and “11.56” at C4 are “population”s of “Belgium” in “2019” and “2020”; (2) inferring calculational relationships of numerical cells, e.g., percentage change from “11.49” to\n“11.56”. As Figure 1 shows, same capabilities also benefit table structure recognition and table-to-text. So it’s a fundamental need to empower table modeling with stronger numerical reasoning capabilities.\nHowever, it is challenging to endow a tabular model with robust numerical reasoning capabilities. First, understanding a local numerical cell needs dimension inference (Chambers and Erwig, 2008), unit inference (Shbita et al., 2019), and index inference (Dong et al., 2019a), e.g., “population” (dimension), “million” (unit), “2020” (index), and “Belgium” (index) jointly describe “11.56” in Figure 1. It is non-trivial concerning the great flexibility of table semantic structures (Wang et al., 2021b). Second, calculational relationships among two or more numerical cells are various and often compositional, e.g., “F1 Score = 2× (Recall× Precision) / (Recall + Precision)” in machine learning papers and “Profit Margin = Net Income / Sales” in financial reports. To make matters more challenging, human labeling for numerical reasoning\nin relevant tasks (Chen et al., 2020; Suadaa et al., 2021; Koci et al., 2019) is labor-intensive and errorprone, largely restricting the generalization ability of large models that are rather data-hungry.\nRecently, table pretraining on large amount of unlabeled tables shows promising results on table understanding and reasoning. Self-supervised objectives are derived from tables and text such as Masked Language Models (MLM) (Herzig et al., 2020), masked column prediction (Yin et al., 2020), masked entity recovery (Deng et al., 2020b), cell cloze and corrupt detection (Wang et al., 2021b; Tang et al., 2020; Iida et al., 2021), table-text matching and alignment (Wang et al., 2021a,b; Deng et al., 2020a). However, numerical and calculational relationships of cells lack sufficient attention. Then (Yoran et al., 2021) and (Liu et al., 2021; Yu et al., 2020) synthesize questions and SQL queries, respectively, as training corpus for reasoning purpose, but SQL is only applicable to database-like relational tables, and importantly, it’s challenging to ensure synthesized questions and SQLs be realistic, meaningful, and diverse.\nGladly, tens of millions of real spreadsheet formulas are publicly available on the web and can be valuable for numerical reasoning in tables. The spreadsheet formula is an expressive yet simple language consisting of operators (e.g., +,/,%), functions (e.g., SUM,MAX,COUNT), referenced cells (e.g., B4), and constant values (e.g., 100) (Aivaloglou et al., 2015). Since writing the formula does not require formal programming education, it’s widely used by non-programmers such as business professionals or other kinds of domain specialists whose jobs involve computational tasks. So spreadsheet formulas cover real numerical calculations in a great variety of domains.\nTo this end, we propose FORmula-driven TAble Pretraining (FORTAP ) for numerical reasoning. One should master two basic concepts to use the formula language: cells as variables and operators/functions as relationships between variables. So we explicitly decompose information in formulas into numerical reference and numerical calculation and devise two complementary tasks. Given a table as well as a formula cell in it, we mask the formula and then (1) the model classifies whether “header A references header B” (we consider that “header A references header B” if the formula cell belonging to header A references a numerical cell belonging to header B, as illustrated in Figure 2);\n(2) the model predicts the operator/function of a group of referenced numerical cells. Furthermore, to better encode and represent formulas, we also apply MLM to the token sequence of formulas.\nConsidering the flexibility of table structures in spreadsheets, we base FORTAP on TUTA (Wang et al., 2021b), the first transformer-based method for spreadsheet tables with carefully-designed textual, numerical, positional, and formatting embedding layers. Importantly, its tree-based position encoding and attention are highly effective in representing generally structured tables. TUTA is pretrained with MLM, cell cloze, and table-text matching.\nExperiment results on three tasks demonstrate that the significance of leveraging formulas for table pretraining. For formula prediction, FORTAP achieves 55.8% top-1 accuracy, significantly surpassing TUTA (48.5%) and SpreadsheetCoder (40.4%) on Enron. For table question answering, TUTA achieves comparable accuracy with SOTA on HiTab. After pretraining with formulas, FORTAP delivers a huge improvement of 6.3% over SOTA. For cell type classification, FORTAP improves TUTA by 6.6% on the “derived” class and 3.2% on overall Macro-F1 on DeEx."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 TUTA as Encoder",
      "text" : "TUTA (Wang et al., 2021b) is the first pretraining architecture for spreadsheet tables. It is effective in capturing table semantic structures, achieving SOTA results on cell type and table type classification. As mentioned in Section 1, understanding table semantic structures is critical to numerical reasoning, so we choose TUTA to be the encoder of FORTAP . Since our pretraining tasks are generic for encoders of tables, future works can also explore other encoders such as (Herzig et al., 2020).\nHeader Recognition. Headers usually provide short yet informative descriptions of table contents in Natural Language (NL), so TUTA leverages the detected header regions and hierarchies, as presented in Section 2.2. (Chen et al., 2021a) also shows that using headers (even without considering hierarchies) greatly helps formula prediction. FORTAP follows to place detected headers in inputs.\nArchitecture. TUTA bases on BERT (Devlin et al., 2019) with several enhancements: (1) a positional encoding layer based on a unified bi-dimensional coordinate tree to describe both the spatial and hi-\nerarchical information of cells; (2) a number encoding layer to encode magnitude, precision, the first digit, and the last digit; (3) a tree-based attention mechanism that enables local cells to aggregate their structurally neighbouring contexts within a tree-based distance threshold.\nModel Input/Output. The input consists of a table (T ) and NL texts (C). By traversing the cell matrix of a table from left to right and from top to bottom, the input is linearized to “[CLS], C0, ..., CK−1, [SEP], T(0,0), [SEP], T(0,1), ..., [SEP], T(M−1,N−1)”, where K is the token length of NL texts, and M and N are the numbers of rows and columns of the table, respectively. Note that T(i,j) refers to the token sequence of the cell string in the (i + 1)th row and (j + 1)th column, and each token has token, number, position, and format input embeddings. The output of the encoder contains token-level, cell-level, and table-level embeddings. FORTAP follows these input/output settings except when inputting formula token sequence."
    }, {
      "heading" : "2.2 Pretraining Corpus",
      "text" : "Spreadsheet Source and Preprocessing. We use the same spreadsheet table corpus as TUTA: (1) 13.5 million public spreadsheet files are crawled from 1.75 million websites; (2) table ranges and headers are detected using TableSense (Dong et al., 2019b,a); (3) header hierarchies are extracted with effective heuristics; (4) extreme size tables are filtered out; (5) duplicated tables are discarded. In the end, 4.5 million spreadsheet tables are left.\nFormula Preprocessing. Spreadsheet Formula is a widely-used end-user language for table organization and calculation. A formula consists of four types of formula tokens: operator (e.g., +,/,%), functions (e.g., SUM), referenced cells (e.g., B4) and constant values (e.g., 100), which we denote as OP, FUNC, CELL and CONST in the rest part of the paper. We use XLParser (Aivaloglou et al., 2015), a highly-compatible formula parser with compact grammar, to analyze formula. In this way, we derive the AST of each formula (an example AST in Figure 2) and the type of each formula token. Since we focus on single table setting, we discard the cross-table, cross-sheet, and cross-file formulas. Formulas with Array or User-Defined-Function are also discarded. The absolute reference sign “$” is deleted from formula strings, without changing their meanings. We only keep the first five occurrences of formulas in the same row/column because\nsome spreadsheets contain hundreds of duplicated or dragged formulas in one row/column, which are inefficient for training. Formulas are linearized as formula token sequences in prefix representation of AST following SpreadsheetCoder (Chen et al., 2021a). Finally, 10.8 million formulas are derived."
    }, {
      "heading" : "3 Pretraining Tasks",
      "text" : "As mentioned in Section 1, empowering table modeling with stronger numerical reasoning capabilities is a fundamental need. Spreadsheet formulas naturally contain information of numerical references (CELL) and calculations (OP/FUNC), motivating us to devise effective tasks to leverage them for numerical-reasoning-aware pretraining.\nBased on information parsed from the formula expression, we carefully devise two complementary objectives, Numerical Reference Prediction (NRP) and Numerical Calculation Prediction (NCP), to exploit the reasoning process behind referencing local cells (as operands) and applying calculations (on operands), respectively. Meanwhile, to get better representations of the spreadsheet formula, which could be further used in downstream applications like formula error detection (Cheung et al., 2016), we extend MLM (Devlin et al., 2019) from NL contexts to formulas. Figure 2 gives an illustration of these tasks.\nNumerical Reference Predication (NRP) We consider “header A references header B” in a table if: in a formula, the formula cell (cell with formula) belonging to header A references a cell belonging to header B. Take the table in Figure 2 as an example, the header “%Increase” references headers “2016” and “2021” since E3 in column “%Increase” references C3 and D3 in columns “2016” and “2021”. We let the model learn header reference relationship since a cell belonging to a referenced header is more likely to be involved in the calculation. It is important but usually unknown as a priori, especially when tables are from diverse or unfamiliar domains. Note that we use header cells instead of data cells in this task since headers provide high-level descriptions of the data (Chen et al., 2021a) and thus header reference relationships have more generic semantics across tables.\nGiven extracted header regions and hierarchies in corpus preprocessing, we first formulate NRP as a binary classification task over header pairs: given a formula cell tf and its referenced cells {t(i)p }, we first find their non-shared headers hf\n(for tf ) and {h (i) p } (for {t(i)p }), then we group them as positive pairs {(hf , h (i) p )}. Usually a formula cell shares a header with referenced cells in the same row/column (e.g., in Figure 2, “Onion” is the shared header for E3, C3, D3). As it does not reflect header reference relationships, we exclude the shared header in this task. The negative pairs {(hf , h (i) n )} are sampled among those unreferenced headers on the same direction (either on top or left headers) of hf . Number of negative samples is at most 3:1 to positive ones to balance samples. The binary classification probability of the ith pair p(i) = f(hf ,h (i) p/n), where h is the header cell embedding derived by the encoder and f(·) is a two-layer binary classification module.\nTo inject table-text joint reasoning skills into FORTAP , which TUTA does not excel at, we further extend NRP task to table-text setting. Given a table with a formula cell, we first construct a formula-based prompt as context by picking 1 to\n10 tokens randomly from the vocabulary as a noisy sentence and then inserting the row and column header of formula cell into it at random positions. Next, we jointly input the formula-based prompt and the table, and the task is to classify (1) reference header cell(s), (2) formula header cell(s), (3) formula cell, (4) other cells from the table. Identifying reference headers requires knowledge of numerical reference since the model can only see formula headers in the prompt. In addition, identifying formula headers and formula cells can improve the capability of table-text alignment and cell selection respectively.\nThe NRP loss Lnr is calculated as the sum of binary cross entropy loss and multi-class cross entropy loss under table-only and table-text setting.\nNumerical Calculation Prediction (NCP) Given data cells as operands, a model then needs to find out which operators/functions should be applied. For example, in Figure 2, subtraction and division are applied on C3 and D3 in the formula. We hope the model can speculate the target operator/function based on the semantics, numeracy, and positions of given operands (data cells). Thus, we design the task to predict the operator/function for a group of data cells with their contextual cell embeddings produced by the encoder.\nWe formulate it as a multi-class classification task: given a formula and its AST parsed in prerpocessing, we select the operators/functions {o(i)} satisfying that all direct children nodes {d(j)}(i) on the formula AST of o(i) are in CELL type with integer or float data. The probability of predicting the operator/function of these data cells is p(i) = f(POOL({d(j)}(i))), where d is the output cell embedding by the encoder, f(·) is a twolayer classification module, and POOL is a meanpooling layer. Note that we only include the operator/function o whose all direct children nodes are in CELL type in this task, because otherwise some descendant data cells will firstly be calculated via other operators/functions and thus have indirect connections with o (e.g., in Figure 2, “/” is not a target operator since its left child is an operator “−”). We include 17 common calculation operators/functions (see Appendix A) covered in spreadsheet formulas in this task. The NCP objective Lnc is the multi-class cross entropy loss. Formula MLM To encode the formula, we first expand 41 tokens in the vocabulary for all four formula token types, covering 99.1% formulas in\ncorpus. The added tokens are listed in Appendix A. Note that a special case is the CELL type, like D4, because it references another cell. Since referenced cells theoretically can be anywhere in a large table, it is infeasible to explicitly insert all cell positions into the vocabulary. Thus, for CELL type token in formula, we use a [RANGE] tag as input token and copy all cell-level embeddings (position, format, numeric, ...) from the referenced cell to this CELL type token.\nWe then apply MLM to formula tokens. Masking and recovering operators/functions is straightforward. When masking or recovering a referenced cell in a formula, we need to avoid label leakage from embeddings of the referenced cell. Thus, to mask a referenced cell, besides using the [MASK] token embedding, the number embedding is set to default to mask the number, and the position and format embeddings are set to the same as the formula cell. To recover a masked referenced cell tr, the cell t(i) in input sequence with the highest probability p(i) = Softmax(f(tr, t(i))) is selected as the predicted cell, where t is output cell embedding of the encoder and f(·) is a two-layer classification module. The objective Lfmlm is calculated as the sum of cross entropy loss over operator/function recovery and referenced cell recovery.\nFinally, the total pretraining objective is\nL = Lnr + Lnc + Lfmlm (1)"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we describe the pretraining details and validate the effectiveness of FORTAP on three downstream tasks: formula prediction, question answering, and cell type classification. The statistics of datasets we use are listed in Table 1."
    }, {
      "heading" : "4.1 Pretrain Implementation",
      "text" : "We initialize FORTAP with parameters of the pretrained TUTA. The input is linearized following TUTA by concatenating the text (the prompt built in NRP pretraining task) and the flattened table traversed in row order. Due to memory limit, we only place (1) header cells, (2) data cells on the same row/column of the formula cell, into the input sequence and skip the other cells. Our input pattern is reasonable as a tradeoff between performance and memory since we find that more than 89% formulas only reference cells on the same row/column. To match different downstream tasks, for the cell with formula, we input its formula token sequence (e.g.\n(C4-B4)/B4) with 40% probability, formula tag [FORMULA] with 30% (the number embedding is set to default) and cell literal value with 30% (e.g. number 42.1). In experiments, we find it is more effective in Formula MLM to mask either all operators/functions or all referenced cells, so we implement it this way. We first pretrain 400K steps on sequence length 256 with batch size 32, and 250K steps on sequence length 512 with batch size 8. The whole pretraining phase takes about 4 days on 4 Tesla V100 GPUs."
    }, {
      "heading" : "4.2 Formula Prediction",
      "text" : "Formula prediction (Chen et al., 2021a) facilitates spreadsheet end-users by recommending formulas since writing formulas could be time-consuming and error-prone. Given a table and a target cell in table, the task is to predict a formula for the target cell. Formula prediction requires complex in-table numerical reasoning capabilities to predict both referenced cells and involved calculations.\nDatasets. Enron (Hermans and Murphy-Hill) is a massive database of public Excel Spreadsheet, containing over 17K spreadsheets with rich table structures and formula types. We exclude Enron from our pretraining corpus to prevent data leakage. Tables and formulas are preprocessed in the same way as the pretraining corpus. We divide Enron by sheet and the final dataset contains 100.3K/12.3K/12.9K table-formula pairs for train/dev/test. The formula cell in table is regarded as the target cell and the formula is seen as the ground truth in formula prediction task. We follow the evaluation metrics in SpreadsheetCoder (Chen et al., 2021a): (1) Formula Accuracy, (2) Sketch Accuracy, (3) Range Accuracy measuring the percentage of correctly predicted formulas, formula sketches (formula using placeholder [RANGE] as referenced cells), and formula ranges (only the referenced cells of formula).\nPrevious to our work, SpreadsheetCoder evaluates formula prediction on collected Google Sheets and Enron. However, we do not directly use its\ndatasets for three reasons: (1) The Google Sheet corpus is not released, and for Enron, SpreadsheetCoder only adopts formulas referencing cells within a limited rectangular neighborhood region (21 × 20) of the formula cell, while we argue in real tables the referenced cells can be easily beyond this region. (2) A large proportion of table headers are not properly detected (mentioned in its paper), while we adopt ranges and headers detected by TableSense (Dong et al., 2019b) and extract table header hierarchies. (3) Despite the inconsistencies above, we try to backtrack the original file to align with SpreadsheetCoder and apply our preprocessing. However, the document IDs of tables in SpreadsheetCoder are mostly empty. Thus, we build our dataset based on Enron and evaluate SpreadsheetCoder on it for a fair comparison.\nBaselines. We adopt SpreadsheetCoder (Chen et al., 2021a) and TUTA as our baselines. SpreadsheetCoder is a SOTA method based on BERT, and it incorporates headers and contextual information of rows and columns of the target cell to assist formula prediction.\nFine-tune. FORTAP consumes all header cells in the table and data cells lying on the same row/column of the target cell just like the manner in pretraining, with a max sequence length, 512. The [FORMULA] tag is placed at the target cell position in input, whose number embedding is set to default. A two-stage LSTM formula decoder (Dong and Lapata, 2018; Chen et al., 2021a) accepts the formula cell embedding as input, and generates the formula by first generating formula sketches and then selecting referenced cells. All models in experiments are fine-tuned 800K steps on Enron. The beam size is 5 for generating formula. Since SpreadsheetCoder only published part of its code, we re-implement it in PyTorch (Paszke et al., 2019) based on its paper. More details about SpreadsheetCoder are shown in Appendix B.\nResults. Table 2 summarizes the results of formula prediction on the test set. As shown, FORTAP delivers a big improvement over SpreadsheetCoder by 15.4% on formula accuracy with formula pretraining and utilization of header structures. FORTAP also outperforms TUTA by 7.3%, showing formula pretraining effectively assists formula prediction. As stated in (Chen et al., 2021a), SpreadsheetCoder only reported 29.8% top-1 accuracy on Enron because the spreadsheet format difference between Google Sheets and Excel caused low header"
    }, {
      "heading" : "20% Train Set",
      "text" : ""
    }, {
      "heading" : "100% Train Set",
      "text" : "detection accuracy. On our processed dataset for Excel tables with well detected tables and headers, its top-1 accuracy achieves 40.4%. We also experiment on a low-resource setting (20% training data), and the improvements of FORTAP are more significant, surpassing TUTA by 10.2%. Since Enron is not included in our pretraining corpus, this result well indicates formula pretraining can largely benefit formula prediction after seeing large numbers of real formulas. Moreover, we conjecture that formula pretraining potentially improves numerical reasoning capabilities of the model, because the two-stage prediction of formula sketches and ranges relies on numerical calculation and numerical reference capabilities, respectively."
    }, {
      "heading" : "4.3 Table Question Answering",
      "text" : "Table QA (Pasupat and Liang, 2015; Cheng et al., 2021) contains a table and an NL question over the table as the model input. Its output can be cell value(s) or number(s) calculated over numerical cell value(s). Table QA calls for both in-table numerical reasoning and table-text joint reasoning.\nDatasets. There are several datasets (Pasupat and Liang, 2015; Cheng et al., 2021; Zhu et al., 2021; Chen et al., 2021b) focusing on Table QA or Table-text hybrid QA. We choose to evaluate on HiTab (Cheng et al., 2021), a hierarchical table dataset for question answering and data-to-text. First, tables in HiTAB contain rich table structures (98.1% tables are hierarchical) from 29 domains, posing a challenge to numerical reasoning. Second, a large proportion of questions (∼ 40%) from Statistical Reports demands complex numerical inference over table and text. Moreover, questions in HiTAB are revised from sentences written by professional analysts to ensure naturalness and meaningfulness. The QA evaluation metric is Execution Accuracy measuring the percentage of correctly predicted answers.\nBaselines. We employ TaPas (Herzig et al., 2020),\nHiTAB model (Cheng et al., 2021), and TUTA as our baselines. TaPas is an end-to-end table parsing model without generating logical forms, which enjoys pretraining on the large-scale table-text corpus from Wikipedia. HiTAB devises a hierarchy-aware logical form for hierarchical tables, and predicts the answer using a weakly supervised semantic parser MAPO (Liang et al., 2018), which is a reinforcement learning framework to systematically explore and generate programs. The question and table are encoded by BERT and the logical forms are generated by an LSTM decoder.\nFine-tune. We replace the BERT encoder of HiTAB model with FORTAP , and follow the finetuning settings of HiTAB. We find it is unnecessary to freeze encoder parameters at the first 5000 steps, so we train the encoder-decoder model together.\nResults. Table 3 summarizes QA results on HiTab. FORTAP achieves a state-of-the-art accuracy 47.0% using MAPO as the semantic parser, surpassing HiTAB best system with 6.3%. Meanwhile, replacing BERT with TUTA does not see a significant performance gain. We conjecture one of the reasons is that TUTA may be not skilled at table-text joint reasoning, and FORTAP enhances this skill by the table-text setting of the NRP task."
    }, {
      "heading" : "4.4 Cell Type Classification",
      "text" : "Cell type classification (CTC) (Koci et al., 2019; Gol et al., 2019; Gonsior et al., 2020) aims to interpret tabular data layouts automatically via classifying table cells by their roles in data layouts (e.g.,\ntop attribute, data, derived). It requires understanding of table semantics, structures, and numerical relationships considering diverse table layouts.\nDatasets. DeEx (Koci et al., 2019) is a widelystudied CTC dataset with tables of various structures and semantics. DeEx includes tables from various domains by mixing three public corpora: Enron (Hermans and Murphy-Hill), Euses (Fisher and Rothermel, 2005), and Fuse(Barik et al., 2015). Cells in DeEx are categorized into six finegrained types: metadata, notes, data, left attribute, top attribute, and derived. The evaluation metric is the Macro-F1 score over all cell types.\nBaselines. We compare FORTAP with two learning-based methods CNNBERT(Dong et al., 2019a) and Bi-LSTM (Gol et al., 2019), and three table-pretraining methods TaBERT (Yin et al., 2020), TaPas (Herzig et al., 2020), and TUTA.\nFine-tune. Since DeEx needs dense classification on all cells, to handle large tables in DeEx, we split tables into chunks with a max input sequence length (512) and distribute headers to each chunk. For cells with formulas, [FORMULA] tags are used as input tokens. We fine-tune 100 epochs on five folds and report the average scores. All these settings are the same as TUTA.\nResults. Table 4 lists the CTC results on DeEx. FORTAP achieves a SOTA Macro-F1 of 79.6%. Specifically, FORTAP largely improves the performance on type derived and notes, surpassing TUTA by 6.6% and 7.5%. The improvement on derived indicates formula pretraining helps identifying cells derived by calculations over some other cells. Note that derived in DeEx not only includes cells with explicit formulas, but also those cells with hidden (missing) formulas (Koci et al., 2019), which poses a great challenge to existing methods since it requires discovery of numerical relationships between cells. Thus, this is a strong signal that formula pretraining endows the model with better numerical reasoning capabilities. We think that the improvement on notes mainly benefits from the NRP pretraining task with formula-based prompts as the context, enhancing FORTAP ’s capability on table-text joint modeling."
    }, {
      "heading" : "4.5 Analysis",
      "text" : "In this section, we analyze our method in terms of (1) the effects of different pretraining tasks, (2) whether and to what extent our model learns nu-\nmerical reasoning skills.\nEffects of pretraining tasks. We conduct ablation studies on different pretraining tasks on the formula prediction task. Here we pretrain TUTA with each pretraining task and fine-tune on Enron dataset, as summarized in Table 5. We can see that combining all pretraining tasks brings the most gain on formula accuracy. NRP and NCP improve more on range accuracy and sketch accuracy, respectively. This aligns with our design motivation that NRP targets on how to reference and NCP learns how to calculate. To our surprise, Formula MLM alone also largely benefits formula prediction. We deduce the reason is that both MLM and formula prediction requires encoding and recovering/generating capabilities of the formula token sequence.\nNumerical reasoning skills. We have shown our model learns numerical reasoning skills by two facts: (1) NRP and NCP improve more on the range and sketch accuracy on the formula prediction task, respectively; (2) our model boosts the performance of derived cell type on cell type classification. Here we further decompose QA accuracy of different operations on HiTAB. The comparison between previous SOTA system BERT(MAPO) and our FORTAP (MAPO) is shown in Table 6. As shown, our model improves most on complex cell selection (cell indexed by ≥ 3 headers) and arithmetic (e.g., difference, sum) problems. Note that complex cell selection not only requires table-text alignment, but also the references between headers considering that mentions of headers in question could be implicit or missing. Meanwhile, our model also handles superlative (e.g., argmax) and comparative (e.g., less than) problems better than BERT, despite these types are relatively infrequent in our formula pretraining corpus. To summarize, our model mainly improves numerical skills regarding cell reference and arithmetic, as well as other aspects like comparing and ranking."
    }, {
      "heading" : "5 Related Works",
      "text" : "Table Pretraining. Table pretraining has been widely studied in recent years. Some works\nmine large-scale table-text pairs as pretraining corpus (Deng et al., 2020b; Yin et al., 2020; Herzig et al., 2020; Wang et al., 2021b), some leverage annotated table-text datasets (Deng et al., 2021; Yu et al., 2020), and some synthesize a table-text corpus by templates (Yu et al., 2020; Eisenschlos et al., 2020). Regarding pretraining tasks, they either train the model to recover masked tokens/column/cell/entity (Yin et al., 2020; Herzig et al., 2020; Wang et al., 2021b; Deng et al., 2020b), or explicitly learn table-text alignments (Deng et al., 2021; Yu et al., 2020). Recently, TaPEx (Liu et al., 2021) adopts BART (Lewis et al.) as a neural executor for synthesized SQLs to improve table reasoning. Whereas, our method explores to use real spreadsheet formulas to guide table pretraining.\nNumerical reasoning over Natural Language. Numerical reasoning is important in NL domain (Dua et al., 2019). Numbers even account for 6.15% of all unique tokens in English Wikipedia (Thawani et al., 2021). Various works target improving numerical reasoning skills on NL (Andor et al., 2019; Geva et al., 2020; Jin et al., 2021). Except using pure NL, MathBERT (Peng et al., 2021) pretrains NL documents with mathematical formulas. In this paper, we target numerical reasoning over (semi-) structured tables."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we present FORTAP , a numericalreasoning-aware table pretraining model that learns numerical reasoning capabilities from spreadsheet formulas. Specifically, we design two pretraining tasks to capture numerical reasoning capabilities by explicitly predicting cell reference and calculation relations. Experiments show that FORTAP achieves new SOTA on formula prediction, question answering, and cell type classification. Further analyses indicate that formula pretraining indeed improves numerical reasoning skills of the model. In the future, we plan to exploit spreadsheet formulas beyond numerical reasoning, e.g., using logic functions like VLOOKUP and text functions like LEN to guide complex logic and text reasoning, which is not yet the main focus of this paper."
    }, {
      "heading" : "7 Ethical Considerations",
      "text" : "In this work, we present a table pretraining method leveraging spreadsheet formulas. Dataset. Our pretraing corpus is built upon public English spreadsheet files crawled from webs via the search engine (Wang et al., 2021b). All datasets used for evaluation are public datasets, e.g., for formula prediction, Enron (Hermans and MurphyHill) is a public spreadsheet dataset consisting of over 17K spreadsheet files, and we re-purpose it for formula prediction following (Chen et al., 2021a). Application. Our model shows its effectiveness in three fundamental table-related tasks. Formula prediction helps spreadsheet end-users to write formulas which could be tedious and error-prone. Table QA enables users to query on the table without the need of domain background knowledge. Cell type classification assists interpreting fine-grained table semantic structures, which help users to better understand table structures and contents."
    }, {
      "heading" : "C Error Analysis of Formula Prediction",
      "text" : "Figure 3 presents the proportion and accuracy regarding different formula sketch lengths in prefix order (parentheses excluded). As shown, sketch length 3 and 4 account for two-thirds of formulas, since length 3 is typical for binary operations like C4-B4, and length 4 is a common pattern for aggregation functions like SUM(B4:C5). Thus, the accuracy of length 3/4 is higher than shorter sketch\nlength 1/2 since more samples in its length are seen in training. And for longer formulas (>6), a significant performance drop occurs because complex nested references and calculations may be involved when the sketch gets longer.\nTo further analyze the errors in formula prediction, we randomly pick 100 false generation results in dev set and divide these errors into three groups: (i) sketch failure (54%): a wrong sketch is generated, which occurs more frequently when the formula gets longer and nested. A typical case is the formula with function IF, involving multiple arguments and nested calculations; (ii) reference unreachable (27%): referenced cells are not in the sequence since we only consider the cells on the same row/column of the target cell as input; (iii) reference failure (19%): wrong referenced cells are selected, which often occurs at the start or end of a cell range. Future works may improve formula prediction in these directions: handling long nested formulas, inputting more cells of table matrix as reference candidates conquering memory issues, and designing a module to match generated sketch with input table cells more accurately."
    }, {
      "heading" : "D Real examples of spreadsheet tables with formulas",
      "text" : "Here we show several real examples for spreadsheet tables."
    } ],
    "references" : [ {
      "title" : "A grammar for spreadsheet formulas evaluated on two large datasets",
      "author" : [ "Efthimia Aivaloglou", "David Hoepelman", "Felienne Hermans" ],
      "venue" : "In 2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM),",
      "citeRegEx" : "Aivaloglou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Aivaloglou et al\\.",
      "year" : 2015
    }, {
      "title" : "Giving bert a calculator: Finding operations and arguments with reading comprehension",
      "author" : [ "Daniel Andor", "Luheng He", "Kenton Lee", "Emily Pitler" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Andor et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2019
    }, {
      "title" : "Fuse: a reproducible, extendable, internet-scale corpus of spreadsheets",
      "author" : [ "Titus Barik", "Kevin Lubick", "Justin Smith", "John Slankas", "Emerson Murphy-Hill" ],
      "venue" : "In 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,",
      "citeRegEx" : "Barik et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barik et al\\.",
      "year" : 2015
    }, {
      "title" : "Dimension inference in spreadsheets",
      "author" : [ "Chris Chambers", "Martin Erwig" ],
      "venue" : "IEEE Symposium on Visual Languages and Human-Centric Computing,",
      "citeRegEx" : "Chambers and Erwig.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chambers and Erwig.",
      "year" : 2008
    }, {
      "title" : "Open question answering over tables and text",
      "author" : [ "Wenhu Chen", "Ming-Wei Chang", "Eva Schlinger", "William Wang", "William W Cohen" ],
      "venue" : "arXiv preprint arXiv:2010.10439,",
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Hitab: A hierarchical table dataset for question answering and natural language generation",
      "author" : [ "Zhoujun Cheng", "Haoyu Dong", "Zhiruo Wang", "Ran Jia", "Jiaqi Guo", "Yan Gao", "Shi Han", "Jian-Guang Lou", "Dongmei Zhang" ],
      "venue" : "arXiv preprint arXiv:2108.06712,",
      "citeRegEx" : "Cheng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Structure-grounded pretraining for textto-sql",
      "author" : [ "Xiang Deng", "Ahmed Hassan Awadallah", "Christopher Meek", "Oleksandr Polozov", "Huan Sun", "Matthew Richardson" ],
      "venue" : "arXiv preprint arXiv:2010.12773,",
      "citeRegEx" : "Deng et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2020
    }, {
      "title" : "Turl: Table understanding through representation learning",
      "author" : [ "Xiang Deng", "Huan Sun", "Alyssa Lees", "You Wu", "Cong Yu" ],
      "venue" : "arXiv preprint arXiv:2006.14806,",
      "citeRegEx" : "Deng et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2020
    }, {
      "title" : "Structure-grounded pretraining for text-to-sql",
      "author" : [ "Xiang Deng", "Ahmed Hassan", "Christopher Meek", "Oleksandr Polozov", "Huan Sun", "Matthew Richardson" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Deng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2021
    }, {
      "title" : "Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova. Bert" ],
      "venue" : "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Coarse-to-fine decoding for neural semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "author" : [ "Li Dong", "Mirella Lapata" ],
      "venue" : null,
      "citeRegEx" : "Dong and Lapata.,? \\Q2018\\E",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2018
    }, {
      "title" : "Semantic structure extraction for spreadsheet tables with a multi-task learning architecture",
      "author" : [ "Haoyu Dong", "Shijie Liu", "Zhouyu Fu", "Shi Han", "Dongmei Zhang" ],
      "venue" : "In Workshop on Document Intelligence at NeurIPS",
      "citeRegEx" : "Dong et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Tablesense: Spreadsheet table detection with convolutional neural networks",
      "author" : [ "Haoyu Dong", "Shijie Liu", "Shi Han", "Zhouyu Fu", "Dongmei Zhang" ],
      "venue" : "In Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Dong et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner" ],
      "venue" : "In Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Dua et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding tables with intermediate pre-training",
      "author" : [ "Julian Martin Eisenschlos", "Syrine Krichene", "Thomas Müller" ],
      "venue" : "arXiv preprint arXiv:2010.00571,",
      "citeRegEx" : "Eisenschlos et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Eisenschlos et al\\.",
      "year" : 2020
    }, {
      "title" : "The euses spreadsheet corpus: a shared resource for supporting experimentation with spreadsheet dependability mechanisms",
      "author" : [ "Marc Fisher", "Gregg Rothermel" ],
      "venue" : "In Proceedings of the first workshop on Enduser software engineering,",
      "citeRegEx" : "Fisher and Rothermel.,? \\Q2005\\E",
      "shortCiteRegEx" : "Fisher and Rothermel.",
      "year" : 2005
    }, {
      "title" : "Injecting numerical reasoning skills into language models",
      "author" : [ "Mor Geva", "Ankit Gupta", "Jonathan Berant" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Geva et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2020
    }, {
      "title" : "Tabular cell classification using pre-trained cell embeddings",
      "author" : [ "Majid Ghasemi Gol", "Jay Pujara", "Pedro Szekely" ],
      "venue" : "IEEE International Conference on Data Mining (ICDM),",
      "citeRegEx" : "Gol et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gol et al\\.",
      "year" : 2019
    }, {
      "title" : "Active learning for spreadsheet cell classification",
      "author" : [ "Julius Gonsior", "Josephine Rehak", "Maik Thiele", "Elvis Koci", "Michael Günther", "Wolfgang Lehner" ],
      "venue" : "In EDBT/ICDT Workshops,",
      "citeRegEx" : "Gonsior et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gonsior et al\\.",
      "year" : 2020
    }, {
      "title" : "Enron’s spreadsheets and related emails: A dataset and analysis",
      "author" : [ "Felienne Hermans", "Emerson Murphy-Hill" ],
      "venue" : "In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering",
      "citeRegEx" : "Hermans and Murphy.Hill.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermans and Murphy.Hill.",
      "year" : 2015
    }, {
      "title" : "Tabbie: Pretrained representations of tabular data",
      "author" : [ "Hiroshi Iida", "Dung Thai", "Varun Manjunatha", "Mohit Iyyer" ],
      "venue" : "arXiv preprint arXiv:2105.02584,",
      "citeRegEx" : "Iida et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Iida et al\\.",
      "year" : 2021
    }, {
      "title" : "Numgpt: Improving numeracy ability of generative pre-trained models",
      "author" : [ "Zhihua Jin", "Xin Jiang", "Xingbo Wang", "Qun Liu", "Yong Wang", "Xiaozhe Ren", "Huamin Qu" ],
      "venue" : "arXiv preprint arXiv:2109.03137,",
      "citeRegEx" : "Jin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2021
    }, {
      "title" : "Deco: A dataset of annotated spreadsheets for layout and table recognition",
      "author" : [ "Elvis Koci", "Maik Thiele", "Josephine Rehak", "Oscar Romero", "Wolfgang Lehner" ],
      "venue" : "In 2019 International Conference on Document Analysis and Recognition (ICDAR),",
      "citeRegEx" : "Koci et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Koci et al\\.",
      "year" : 2019
    }, {
      "title" : "Memory augmented policy optimization for program synthesis and semantic parsing",
      "author" : [ "Chen Liang", "Mohammad Norouzi", "Jonathan Berant", "Quoc V Le", "Ni Lao" ],
      "venue" : "NeurIPS,",
      "citeRegEx" : "Liang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2018
    }, {
      "title" : "Tapex: Table pre-training via learning a neural sql executor",
      "author" : [ "Qian Liu", "Bei Chen", "Jiaqi Guo", "Zeqi Lin", "Jian-guang Lou" ],
      "venue" : "arXiv preprint arXiv:2107.07653,",
      "citeRegEx" : "Liu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to reason for text generation from scientific tables",
      "author" : [ "Nafise Sadat Moosavi", "Andreas Rücklé", "Dan Roth", "Iryna Gurevych" ],
      "venue" : "arXiv preprint arXiv:2104.08296,",
      "citeRegEx" : "Moosavi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Moosavi et al\\.",
      "year" : 2021
    }, {
      "title" : "Mathbert: A pre-trained model for mathematical formula understanding",
      "author" : [ "Shuai Peng", "Ke Yuan", "Liangcai Gao", "Zhi Tang" ],
      "venue" : "arXiv preprint arXiv:2105.00377,",
      "citeRegEx" : "Peng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2021
    }, {
      "title" : "Parsing, representing and transforming units of measure",
      "author" : [ "Basel Shbita", "Arunkumar Rajendran", "Jay Pujara", "Craig A Knoblock" ],
      "venue" : "Modeling the World’s Systems,",
      "citeRegEx" : "Shbita et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Shbita et al\\.",
      "year" : 2019
    }, {
      "title" : "Rpt: Relational pre-trained transformer is almost all you need towards democratizing data preparation",
      "author" : [ "Nan Tang", "Ju Fan", "Fangyi Li", "Jianhong Tu", "Xiaoyong Du", "Guoliang Li", "Sam Madden", "Mourad Ouzzani" ],
      "venue" : "arXiv preprint arXiv:2012.02469,",
      "citeRegEx" : "Tang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Numeracy enhances the literacy of language models",
      "author" : [ "Avijit Thawani", "Jay Pujara", "Filip Ilievski" ],
      "venue" : "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Thawani et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Thawani et al\\.",
      "year" : 2021
    }, {
      "title" : "Retrieving complex tables with multi-granular graph representation learning",
      "author" : [ "Fei Wang", "Kexuan Sun", "Muhao Chen", "Jay Pujara", "Pedro Szekely" ],
      "venue" : "arXiv preprint arXiv:2105.01736,",
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Tabert: Pretraining for joint understanding of textual and tabular data",
      "author" : [ "Pengcheng Yin", "Graham Neubig", "Wen-tau Yih", "Sebastian Riedel" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Yin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Turning tables: Generating examples from semi-structured tables for endowing language models with reasoning skills",
      "author" : [ "Ori Yoran", "Alon Talmor", "Jonathan Berant" ],
      "venue" : "arXiv preprint arXiv:2107.07261,",
      "citeRegEx" : "Yoran et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yoran et al\\.",
      "year" : 2021
    }, {
      "title" : "Grappa: Grammar-augmented pre-training for table semantic parsing",
      "author" : [ "Tao Yu", "Chien-Sheng Wu", "Xi Victoria Lin", "Bailin Wang", "Yi Chern Tan", "Xinyi Yang", "Dragomir Radev", "Richard Socher", "Caiming Xiong" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2009
    }, {
      "title" : "Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance",
      "author" : [ "Fengbin Zhu", "Wenqiang Lei", "Youcheng Huang", "Chao Wang", "Shuo Zhang", "Jiancheng Lv", "Fuli Feng", "Tat-Seng Chua" ],
      "venue" : "arXiv preprint arXiv:2105.07624,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Tables store rich numerical data, so a wide range of tasks require numerical reasoning over (semi)structured tabular context, such as question answering over tables (Cheng et al., 2021; Chen et al., 2021b; Zhu et al., 2021), table-to-text (Suadaa et al.",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 34,
      "context" : "Tables store rich numerical data, so a wide range of tasks require numerical reasoning over (semi)structured tabular context, such as question answering over tables (Cheng et al., 2021; Chen et al., 2021b; Zhu et al., 2021), table-to-text (Suadaa et al.",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 25,
      "context" : ", 2021), table-to-text (Suadaa et al., 2021; Moosavi et al., 2021; Cheng et al., 2021), spreadsheet formula prediction (Chen et al.",
      "startOffset" : 23,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : ", 2021), table-to-text (Suadaa et al., 2021; Moosavi et al., 2021; Cheng et al., 2021), spreadsheet formula prediction (Chen et al.",
      "startOffset" : 23,
      "endOffset" : 86
    }, {
      "referenceID" : 22,
      "context" : ", 2021a), and table structure understanding (Koci et al., 2019).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "First, understanding a local numerical cell needs dimension inference (Chambers and Erwig, 2008), unit inference (Shbita et al.",
      "startOffset" : 70,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "First, understanding a local numerical cell needs dimension inference (Chambers and Erwig, 2008), unit inference (Shbita et al., 2019), and index inference (Dong et al.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "in relevant tasks (Chen et al., 2020; Suadaa et al., 2021; Koci et al., 2019) is labor-intensive and errorprone, largely restricting the generalization ability of large models that are rather data-hungry.",
      "startOffset" : 18,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "in relevant tasks (Chen et al., 2020; Suadaa et al., 2021; Koci et al., 2019) is labor-intensive and errorprone, largely restricting the generalization ability of large models that are rather data-hungry.",
      "startOffset" : 18,
      "endOffset" : 77
    }, {
      "referenceID" : 31,
      "context" : ", 2020), masked column prediction (Yin et al., 2020), masked entity recovery (Deng et al.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 24,
      "context" : ", 2021) and (Liu et al., 2021; Yu et al., 2020) synthesize questions and SQL queries, respectively, as training corpus for reasoning purpose, but SQL is only applicable to database-like relational tables, and importantly, it’s challenging to ensure synthesized questions and SQLs be realistic, meaningful, and diverse.",
      "startOffset" : 12,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "TUTA bases on BERT (Devlin et al., 2019) with several enhancements: (1) a positional encoding layer based on a unified bi-dimensional coordinate tree to describe both the spatial and hi-",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "We use XLParser (Aivaloglou et al., 2015), a highly-compatible formula parser with compact grammar, to analyze formula.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : ", 2016), we extend MLM (Devlin et al., 2019) from NL contexts to formulas.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "A two-stage LSTM formula decoder (Dong and Lapata, 2018; Chen et al., 2021a) accepts the formula cell embedding as input, and generates the formula by first generating formula sketches and then selecting referenced cells.",
      "startOffset" : 33,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "Table QA (Pasupat and Liang, 2015; Cheng et al., 2021) contains a table and an NL question over the table as the model input.",
      "startOffset" : 9,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "There are several datasets (Pasupat and Liang, 2015; Cheng et al., 2021; Zhu et al., 2021; Chen et al., 2021b) focusing on Table QA or Table-text hybrid QA.",
      "startOffset" : 27,
      "endOffset" : 110
    }, {
      "referenceID" : 34,
      "context" : "There are several datasets (Pasupat and Liang, 2015; Cheng et al., 2021; Zhu et al., 2021; Chen et al., 2021b) focusing on Table QA or Table-text hybrid QA.",
      "startOffset" : 27,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "We choose to evaluate on HiTab (Cheng et al., 2021), a hierarchical table dataset for question answering and data-to-text.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "HiTAB model (Cheng et al., 2021), and TUTA as our baselines.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "HiTAB devises a hierarchy-aware logical form for hierarchical tables, and predicts the answer using a weakly supervised semantic parser MAPO (Liang et al., 2018), which is a reinforcement learning framework to systematically explore and generate programs.",
      "startOffset" : 141,
      "endOffset" : 161
    }, {
      "referenceID" : 22,
      "context" : "Cell type classification (CTC) (Koci et al., 2019; Gol et al., 2019; Gonsior et al., 2020) aims to interpret tabular data layouts automatically via classifying table cells by their roles in data layouts (e.",
      "startOffset" : 31,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "Cell type classification (CTC) (Koci et al., 2019; Gol et al., 2019; Gonsior et al., 2020) aims to interpret tabular data layouts automatically via classifying table cells by their roles in data layouts (e.",
      "startOffset" : 31,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Cell type classification (CTC) (Koci et al., 2019; Gol et al., 2019; Gonsior et al., 2020) aims to interpret tabular data layouts automatically via classifying table cells by their roles in data layouts (e.",
      "startOffset" : 31,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "DeEx (Koci et al., 2019) is a widelystudied CTC dataset with tables of various structures and semantics.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "DeEx includes tables from various domains by mixing three public corpora: Enron (Hermans and Murphy-Hill), Euses (Fisher and Rothermel, 2005), and Fuse(Barik et al.",
      "startOffset" : 113,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "DeEx includes tables from various domains by mixing three public corpora: Enron (Hermans and Murphy-Hill), Euses (Fisher and Rothermel, 2005), and Fuse(Barik et al., 2015).",
      "startOffset" : 151,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : ", 2019a) and Bi-LSTM (Gol et al., 2019), and three table-pretraining methods TaBERT (Yin et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 31,
      "context" : ", 2019), and three table-pretraining methods TaBERT (Yin et al., 2020), TaPas (Herzig et al.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "Note that derived in DeEx not only includes cells with explicit formulas, but also those cells with hidden (missing) formulas (Koci et al., 2019), which poses a great challenge to existing methods since it requires discovery of numerical relationships between cells.",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 31,
      "context" : "mine large-scale table-text pairs as pretraining corpus (Deng et al., 2020b; Yin et al., 2020; Herzig et al., 2020; Wang et al., 2021b), some leverage annotated table-text datasets (Deng et al.",
      "startOffset" : 56,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : ", 2021b), some leverage annotated table-text datasets (Deng et al., 2021; Yu et al., 2020), and some synthesize a table-text corpus by templates (Yu et al.",
      "startOffset" : 54,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : ", 2020), and some synthesize a table-text corpus by templates (Yu et al., 2020; Eisenschlos et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 31,
      "context" : "Regarding pretraining tasks, they either train the model to recover masked tokens/column/cell/entity (Yin et al., 2020; Herzig et al., 2020; Wang et al., 2021b; Deng et al., 2020b), or explicitly learn table-text alignments (Deng et al.",
      "startOffset" : 101,
      "endOffset" : 180
    }, {
      "referenceID" : 8,
      "context" : ", 2020b), or explicitly learn table-text alignments (Deng et al., 2021; Yu et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "Recently, TaPEx (Liu et al., 2021) adopts BART (Lewis et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : "15% of all unique tokens in English Wikipedia (Thawani et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "Except using pure NL, MathBERT (Peng et al., 2021) pretrains NL documents with mathematical formulas.",
      "startOffset" : 31,
      "endOffset" : 50
    } ],
    "year" : 0,
    "abstractText" : "Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is valuable supervision for numerical reasoning in tables. Considering large amounts of spreadsheets available on the web, we propose FORTAP , the first exploration to leverage spreadsheet formulas for table pretraining. Two novel self-supervised pretraining objectives are derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP). While our proposed objectives are generic for encoders, to better capture spreadsheet table layouts and structures, we built FORTAP upon TUTA, the first transformer-based method for spreadsheet table pretraining with tree attention. FORTAP outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction, question answering, and cell type classification, showing the great potential of leveraging formulas for table pretraining.",
    "creator" : null
  }
}
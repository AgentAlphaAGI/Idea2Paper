{
  "name" : "ARR_2022_274_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early Exiting",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Since BERT (Devlin et al., 2018), the pre-trained language models (PLMs) become the default stateof-the-art (SOTA) models for natural language processing (NLP). The recent years have witnessed the rise of many PLMs, such as GPT (Radford et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2020), and so forth. These BERTstyle models achieved considerable improvements in many Natural Language Processing (NLP) tasks by pre-training on the unlabeled corpus and finetuning on labeled tasks, such as text classification, natural language inference (NLI), sequence labeling. Despite their excellent performances, there are two issues for PLMs.\nFirst, previous studies show that PLMs such as BERT suffer from the over-thinking problem. (Zhou et al., 2020; Zhu et al., 2021) shows that in the sentence classification task, BERT’s last few layers may be too deep for some samples. For a sentence classification task, if we insert a classifier on a certain intermediate layer and drop the deeper layers, these intermediate layers may outperform the last layer.\nThe second drawback of PLMs is their high latency. Sentence classification (CLS) tasks play a central role in many application scenarios, such as dialogue systems, document analysis, content recommendation, etc. However, these applications are time-sensitive. For example, if a task-oriented dialogue (TOD) system takes a lot of time to respond, users will have no doubt stop using this system. User experience studies show that a response has to be made in between 0-100 ms. Thus, a CLS module should be efficient and accurate. In addition, a special feature of consumer queries is that there are times when the number of queries is extremely high. For example, during the flu season, online medical consultation will be used much often than usual. Thus, it is important for deployed models\nto adjust their latency dynamically. During peak hours, it switches to a low-latency mode to deal with more queries. And in other hours, it makes the best of itself to provide accurate answers. So how can we make model inference dynamically? The answer is adaptive inference.\nThere exists a branch of literature focusing on making PLMs’ inference more efficient via network pruning (Zhu and Gupta, 2018; Xu et al., 2020; Fan et al., 2020; Michel et al., 2019), knowledge distillation (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2020a), weight quantization (Zhang et al., 2020; Bai et al., 2020; Kim et al., 2021) and adaptive inference (Zhou et al., 2020; Xin et al., 2020; Liu et al., 2020). The adaptive inference has drawn much attention. The idea of adaptive inference is to deal with simple examples with only shallow layers of BERT and process more difficult queries with deeper layers, thus significantly speeding up the inference time on average while maintaining high accuracy. The speed-up ratio can be easily controlled with certain hyper-parameters to process significant changes in query traffic without re-deploying the model services or maintaining a group of models.\nEarly exiting is one of the most important adaptive inference methods (Bolukbasi et al., 2017). As depicted in Figure 2(b), it implements adaptive inference by installing an early exit, i.e., an intermediate prediction layer, at each layer of BERT and early exiting \"easy\" samples to speed up inference. At the training stage, all the exits are jointly optimized with BERT’s parameters. At the inference stage, there are two different settings. First, in budgeted exiting mode, the model makes a pre-\ndiction with a fixed exit for all queries. This mode deals with heavy traffic by assigning a shallower exit for prediction. The other one is dynamic exiting mode. That is, some strategies for early exiting are designed to decide whether to exit at each layer given the currently obtained predictions (from previous and current layers) (Teerapittayanon et al., 2016; Kaya et al., 2019; Xin et al., 2020; Zhou et al., 2020). In this mode, different samples can exit at different depths.\nThere are mainly three early exiting strategies for BERT dynamic exiting. The first one is scorebased early exiting. BranchyNet (Teerapittayanon et al., 2016), FastBERT (Liu et al., 2020), and DeeBERT (Xin et al., 2020) calculated the entropy of the prediction probability distribution as an estimation for the confidence of exiting classifiers to enable dynamic early exiting. Shallow-Deep Nets (Kaya et al., 2019) and RightTool (Schwartz et al., 2020a) leveraged the maximum of the predicted distribution as the exiting signal. The second type is the learned exiting (Elbayad et al., 2020). In this type of work, an early exiting signal is generated by a learnable module in the neural network. For example, BERxiT (Xin et al., 2021) install a fully connected layer right after each transformer block of BERT to output a score that is used to decide whether the BERT should stop inference and exit early. The third type is patience-based early exiting, which relies on cross-layer comparison to formulate the exiting signal. PABEE (Zhou et al., 2020) propose a dynamic exiting strategy analogous to early stopping model training. That is, if the exits’ predictions remain unchanged for a pre-defined number of times (patience), the model will stop\ninference and exit early. PABEE achieves SOTAs results for BERT early exiting.\nDespite its state-of-the-art performances during early exiting, PABEE is inflexible in adjusting the speedup ratios. On a given task, once the multiexit BERT is fine-tuned and the patience parameter is fixed, PABEE can only achieve a fixed average speedup ratio. Thus, PABEE can not achieve speedup ratios of certain values. This drawback makes PABEE inconvenient to use in real industrial scenarios. Thus, it is of great importance to come up with a method that can flexibly adjust its speedup ratios and performs comparable to or better than PABEE.\nIn this work, we propose Patiently Confidently Early Exiting BERT (PCEE-BERT), a novel early exiting method that combines the advantage of score-based methods and the patience based early exiting method. A multi-exit BERT is adopted as the backbone model, and an intermediate classifier (i.e., an exit) is installed right after each transformer black. PCEE-BERT will early exit if there are enough numbers (i.e., the patience parameter) of consecutive exits being confident for their predicted distributions. We mainly use entropy as the confidence measure. Intuitively, our method requires patience and confidence. It will not rush into an early exiting if we only see a couple of intermediate layers being confident. In addition, it allows the next layer to modify the predictions. In this way, our PCEE-BERT can exit with higher accuracy while maintaining flexibility.\nExtensive experiments are conducted on the GLUE benchmark (Wang et al., 2018). The results show that our method outperforms the previous SOTA early exiting methods, especially in cases where the speedup ratio is large. In addition, one can adjust the patience and confidence threshold so that PCEE-BERT can arrive at different speedup ratios. A series of ablation studies are conducted, resulting in the following observations: (a) PCEE-BERT can work with different confidence measures; (b) our method performs consistently well on different PLMs, and can work alongside model compression methods to further speed up the BERT’s inference; (c) our PCEE-BERT can also be applied to computer vision tasks.\nThe rest of the paper is organized as follows. First, we introduce the preliminaries for multi-exit BERT and early exiting. Second, we elaborate on our PCEE-BERT method. Third, we conduct\nexperiments on the GLUE benchmark and conduct a series of ablations studies. Finally, we conclude with possible future works."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this section, we introduce the necessary background for BERT early exiting. Throughout this work, we consider the case of multi-class classification with samples {(x, y), x ∈ X , y ∈ Y, i = 1, 2, ..., N}, e.g., sentences, and the number of classes is K."
    }, {
      "heading" : "2.1 Backbone models",
      "text" : "In this work, we adopt BERT as the backbone model. BERT is a multi-layer Transformer (Vaswani et al., 2017) network, which is pre-trained in a self-supervised manner on a large corpus. The number of transformer layers of our backbone is denoted as M , and the hidden dimension is d."
    }, {
      "heading" : "2.2 Early-exiting Architecture",
      "text" : "As depicted in Figure 2, early exiting architectures are networks with exits at each transformer layer. With M exits, M classifiers f (m)(x; θ(m)) : X → ∆K (m = 1, 2, ...,M ) are designated at M layers of BERT, each of which maps its input to p(m)(x; θ(m)), a probability distribution over the K classes. All the parameters of the transformer layers and exits are denoted as Θ."
    }, {
      "heading" : "2.2.1 Training",
      "text" : "At the training stage, all the exits are jointly optimized with a summed loss function. Following Huang et al. (2017) and Zhou et al. (2020), the loss function is the weighted average of the crossentropy (CE) losses given by\nL = ∑M\nm=1m ∗ L(m)∑M m=1m , (1)\nwhere L(m) = CE(y, p(m)(x; θ(m))) denotes the cross-entropy loss of the m-th exit. Note that the weight m corresponds to the relative inference cost of exit m."
    }, {
      "heading" : "2.2.2 Inference",
      "text" : "During inference, the multi-exit BERT can exit early in two different modes, depending on whether the computational budget to classify an example is known or not.\nBudgeted Exiting. If the computational budget is known, we can directly appoint a suitable exit m∗ of BERT, f (m ∗)(x; θ(m ∗)), to predict all queries.\nDynamic Exiting. Under this mode, after receiving a query input x, the model starts to predict on the classifiers f (1)(x; θ(1)), f (2)(x; θ(2)), ..., in turn in a forward pass, reusing computation where possible. It will continue to do so until it receives a signal to stop early at an exit m∗ < M , or arrives at the last exit M . At this point, it will output the final predictions based on the current and previous predictions. Note that under this early exit setting, different samples might exit at different layers."
    }, {
      "heading" : "3 PCEE-BERT",
      "text" : ""
    }, {
      "heading" : "3.1 Motivation",
      "text" : "PABEE achieves the SOTA performances for BERT early exiting by applying an early exiting decisionmaking process that mimics the early stopping of model training. However, one drawback of PABEE is that it can not flexibly adjust the average inference layers (i.e., speed-ups) for a given dataset once its patience parameter is set. Table 1 shows PABEE can not achieve certain values for average inference layers, such as around 4.0, 6.0, or 9.0 on RTE. This drawback may limit the industrial usage of early exiting techniques. Thus, it is of great importance to develop a new method that performs comparably with PABEE and is more flexible than PABEE."
    }, {
      "heading" : "3.2 PCEE-BERT: a novel dynamic exiting method",
      "text" : "The inference process of PCEE-BERT is illustrated in Figure 2(b). Assume the feed forward process for predicting sample x has gone through layers 1, ..., m− 1, and we are now at layer m. After going through the transformer layer m, the intermediate classifier f (m)(x; θ(m)) predicts a class label distribution p(m)(x; θ(m)). The confidence level of layer m is measured by the entropy value of distribution\np(m)(x; θ(m)):\nC(m) =\n∑K k=1 p (m) k log p (m) k\nlog(1/K) , (2)\nwhere p(m)k is the probability mass for k-th class label. If C(m) is smaller than a pre-defined threshold τ , the predictions of layer m is considered confident. Otherwise, it is considered in-confident.\nWe use a patience counter pct to store the number of times that the predictions remain confident in consecutive layers. Formally, at layer m, pct(m) is calculated as\npct(m) =\n{ pct(m−1) + 1, if C(m) < τ,\n0, otherwise. (3)\nWe stop inference early at layer m when pct(m) reaches a predefined integer number t (the patience parameter). If this condition is never fulfilled, we use the final classifier M for prediction. In this way, the model can make an early exit without passing through all layers to make a prediction.\nOur method draws advantages from the previous score-based early exiting method (Teerapittayanon et al., 2016) and patience-based method (Zhou et al., 2020) and overcomes their shortcomings. First, the score-based early exiting method relies on the confidence score from only the current layer. However, as revealed by Szegedy et al. (2014); Jiang et al. (2018), prediction of probability distributions (i.e., softmax scores) suffers from being over-confident to one class, making it an unreliable metric to represent confidence. In our method, early exiting occurs when a group of consecutive layers is confident, thus making the early exiting decision more reliable. Second, with a patiencebased early exiting method like PABEE, when a deeper layer tries to correct the predictions, the patience count resets to zero. As a result, PABEE is less efficient than our PCEE-BERT. Third, since our method is a combination of PABEE and the score-based method, one can conveniently adjust the threshold and patience parameters to control the speed-up ratios, which makes our method more flexible than PABEE."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our proposed approach to the classification tasks on the GLUE benchmark. We only\nexclude the STS-B task since it is a regression task, and we exclude the WNLI task following previous work (Devlin et al., 2018; Xu et al., 2020)."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "We compare our approaches with three groups of baselines.\nBackbone models: We mainly choose the BERT-base model open-sourced by Devlin et al. (2019) as the backbone model. We also investigate whether our method is applicable across different backbones, so we also run ablation experiments with ALBERT base (Lan et al., 2020) and TinyBERT6 (Jiao et al., 2020b).\nBudgeted exiting: In the section 2.2 we have introduced how to train a multi-exit BERT. Once the multi-exit BERT, we can conduct budgeted early exiting, that is, asking a designated intermediate layer to encode and predict all the samples. Budgeted exiting is a direct way to speed up BERT’s inference, but it is instance adaptive. Some of the samples may not need to go through many of the BERT’s layers, and the others may be more difficult and require deeper feature encoding from the deeper layers of BERT.\nDynamic exiting: In this part, we compare our methods with a series of strong baselines, including BranchyNet (Teerapittayanon et al., 2016), Shallow-Deep (Kaya et al., 2019), BERxiT (Xin et al., 2021), and PABEE (Zhou et al., 2020). Note that PABEE can not flexibly adjust the average inference layers on a task once the patience parameter is set. So we will adjust the thresholds in the other baselines and our PCEE-BERT so that all methods’ number of average inference layers are close."
    }, {
      "heading" : "4.3 Evaluation of early exiting method",
      "text" : "In this work, we strictly follow the GLUE benchmark to report the performances metrics on each task. Note that this work focuses on investigating the early exiting of PLMs. Thus we have to consider the trade-offs between performance and efficiency. Following PABEE (Zhou et al., 2020), we mainly report the speedup ratio as the efficiency metric. Assume the PLM backbone has N layers in total. For each test sample xi (i ∈ {0, 1, ..., N}), the early exiting layer is mi, then the average speedup ratio on the test set is calculated by\nSpeedup = 1− ∑N\n1 mi∑N 1 M . (4)\nWe choose this efficiency metric for the following reason: (1) it is linear w.r.t. the actual amount of computation; (2) according to our experiments, it is proportional to actual wall-clock runtime and is also more stable across different runs compared with actual runtime due to randomness by other processes on the same machine."
    }, {
      "heading" : "4.4 Experimental settings",
      "text" : "Training We add a linear output layer after each intermediate layer of the pre-trained BERT or other backbone models as the internal classifiers. We perform grid search over batch sizes of 16, 32, 128, and learning rates of 1e-5, 2e-5, 3e-5, 5e-5 with an Adam optimizer. The hyper-parameters are selected via the 5-fold cross validation on the train set of GLUE tasks. We implement PCEEBERT on the base of Hugging Face’s Transformers (Wolf et al., 2020). Experiments are conducted on a single Nvidia V100 16GB GPU.\nInference Following prior work on inputadaptive inference (Teerapittayanon et al., 2016; Kaya et al., 2019), inference is on a per-instance basis, i.e., the batch size for inference is set to 1. This is a common scenario in the industry where individual requests from different users (Schwartz et al., 2020b) come at different time points. We report the median performance over five runs with different random seeds."
    }, {
      "heading" : "4.5 Main results",
      "text" : "In Table 2, we report the performance comparisons of each method on the GLUE benchmark under three different speedup settings. The three speedup settings are: (1) 74% to 82% speedup; (2) 46% to 54% speedup; (3) 23% to 28% speedup. Since PABEE can not flexibly adjust the speedup ratios for a given patience parameter and a given task, we adjust the hyper-parameters (such as entropy threshold) of our PCEE-BERT and the other baselines to achieve similar speedups with PABEE. The results in table 2 clearly show that our PCEE-BERT method outperforms the baseline methods under different speedup ratios. Table 2 also shows that the PABEE method is the best performing baseline. Thus, in order to further analyze and better visualize the results, we draw the score-speedup curves (in Figure 3) for budgeted early exiting, PABEE and PCEE-BERT, on the QNLI and MRPC tasks. 1 With Table 2 and Figure 3, we can make the\n1The score-speedup curves for the other five GLUE tasks can be found in the appendix.\nfollowing observations:\n• Although it is clear that PABEE performs better than the other baselines when the speedup ratio is around 50% or 25%, its advantages over the other baselines with the 75% speedup ratio is relatively small. With the 75% speedup ratio for seven GLUE tasks, it performs better than the score-based methods only on three tasks. This observation motivates us to improve PABEE by combining its patience-based early exiting mechanism with the score-based ones.\n• Our PCEE-BERT consistently performs better than the baseline methods, especially when the speedup ratio is large. Note that our PCEEBERT also consistently outperforms the budgeted exiting speedup ratios, which the other\nbaselines do not achieve. Figure 3(a) and 3(b) show that score-speedup curve for PABEE is interleaving with that of the budgeted exiting. However, the score-speedup curve for PCEEBERT distances itself from the others for most of the GLUE tasks.\n• The overthinking problem is prevailing in the GLUE benchmark, and our PCEE-BERT early exiting can effectively take advantage of this phenomenon. For 6 of the GLUE tasks, PCEEBERT can outperform BERT-base with a 25% (or more than) speedup ratio. And for 2 of the GLUE tasks, PCEE-BERT can outperform BERT-base with a 50% (or more than) speedup ratio.\nPutting performance comparisons aside, one benefit of PCEE-BERT is that it is flexible since by\nadjusting the threshold and the patience parameter, it can easily control the average inference layers and cover (or achieve values close to) any speedup ratios.2"
    }, {
      "heading" : "4.6 Ablation studies",
      "text" : ""
    }, {
      "heading" : "4.6.1 Ablation on the confidence measures",
      "text" : "Note that our PCEE-BERT is a novel combination of PABEE and BranchyNet. Thus PCEE-BERT mainly uses the entropy of predicted distributions as the confidence measure of an intermediate layer. However, can PCEE-BERT work with the other confidence measures, such as Shallow-Deep? We switch the entropy-based confidence level C(M) (Equation 2) with that from Shallow-Deep (Kaya et al., 2019):\nC(M) = Argmaxkp (m) k , (5)\nand we will call this version of PCEE-BERT as PCEE-BERT-v1. Note that PCEE-BERT-v1 does not require a newly fine-tuned model.\nWith BERxiT, we can come up with PCEEBERT-v2. Following BERxiT, PCEE-BERT-v2 fine-tunes the multi-exit BERT with a fully connected layer right after each transformer block designated to evaluate the confidence score C(M) for early exiting at that layer. C(M) is learned along with the training of intermediate classifiers. Note that PCEE-BERT-v2 can not reuse the fine-tuned checkpoints used in PCEE-BERT and requires one to fine-tune the BERT backbones on the task at hand.\nWe conduct the experiments on the QNLI tasks, and the results are reported in Figure 4. We can see that PCEE-BERT-v1 and PCEE-BERT-v2 perform comparably to PCEE-BERT. The results show that the proposed PCEE-BERT early exiting mechanism is off-the-shelf, and the reason for the success of our PCEE-BERT is its early exiting mechanism, that is, early exit if a group of consecutive exits is confident for their predictions."
    }, {
      "heading" : "4.6.2 Ablation of PLM backbones",
      "text" : "In the main experiments, we use BERT as the pretrained backbone model. However, PCEE-BERT can also work with the other types of pre-trained backbones, such as ALBERT base (Lan et al., 2020) and TinyBERT6 (Jiao et al., 2020b). We conduct the experiments on the QNLI task with these two\n2See the Appendix for demonstration on MRPC.\nbackbone models, and results are presented in Figure 5(a) and 5(b). We can see that when using the other pre-trained backbones, PCEE-BERT also performs better than the baseline methods.\nThe results for PCEE-BERT on the TinyBERT also convey an important message: as an inference speedup method, our PCEE method can work alongside the model compression methods to further reduce the latency of BERT."
    }, {
      "heading" : "4.6.3 Ablation of cross-layer ensemble",
      "text" : "Since we have a prediction module at each layer of BERT, we can conduct model ensemble across layers that the forward pass has gone through already. In Figure 6, we conduct the ablation studies on the RTE and QNLI tasks. According to Figure 6, cross-layer ensemble leads to performance degradation when the speedup ratio is large, while when the average inference layers is close to the number of BERT’s transformer blocks M , cross-layer ensemble results in slight improvements. In conclusion, the cross-layer ensemble does not result in consistent performance improvements.\nA possible application of the above results is to apply the cross-layer ensemble when a low speedup ratio is applied. And when we ask the model to exit early in the shallow layers, the cross-layer ensemble is not used.\n4.6.4 PCEE-BERT are effective for image classification\nOur main experiments are conducted on BERT, a pre-trained language model, and the GLUE benchmark, a series of natural language understanding tasks. However, our PCEE-BERT method is a plugand-play early exiting and can be applied to models and tasks of different modalities. To demon-\nstrate the effectiveness of PCEE-BERT on the image classification task, we follow the experimental settings in PABEE (Zhou et al., 2020). We conduct experiments on two image classification datasets, CIFAR-10 and CIFAR-100 (Krizhevsky, 2009). The ResNet-56 model (He et al., 2016) serves as the backbone, and we compare PCEEBERT with PABEE. We place an exiting classifier at every two convolutional layers. We set the batch size to 128 and use an SGD optimizer with a learning rate of 0.1.\nTable 3 reports the results. PCEE-BERT outperforms PABEE when early exiting at different speedup ratios. In addition, the performance advantages of PCEE-BERT are larger when the speedup ratio is large, which is also observed in the NLP tasks. And PCEE-BERT outperforms the original ResNet-56 on both tasks even when it provides around 25% speedup."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we propose PCEE-BERT, a novel efficient inference method that can yield a better performance-speed trade-off than the existing early exiting methods. PCEE-BERT adopts BERT as the backbone model and makes the exiting decision if there are enough intermediate layers to make confident predictions. The confidence level is measured by the entropy of the predicted distributions. Experiments on the GLUE benchmark demonstrate that our method outperforms the previous SOTA early exiting methods, especially when the speedup ratio is large. In addition, PCEE-BERT can achieve different speedup ratios by adjusting the patience parameter and the confidence threshold, which makes it more flexible in industrial usage. Ablation studies show that: (a) our PCEE-BERT can adopt different confidence measures, such as maximum probability mass; (b) our method performs consistently well on different PLMs and can work together with model compression methods to speed up the BERT’s inference; (c) our PCEE-BERT also performs well on computer vision tasks."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Quality–efficiency trade-offs on GLUE benchmark tasks.\nIn the main content, we present the quality–efficiency trade-offs curves for 2 GLUE tasks. And here we put the results of the other five tasks in Figure 7.\nA.2 Demonstrating PCEE-BERT can cover (or achieve values close to) any speedup ratios\nPCEE-BERT’s speedup ratio can be conveniently adjusted by setting different values for the patience parameter and the confidence threshold. To validate our claim, we alternate the threshold among 100 points between 0.0 to 1.0 when the patience parameter takes the value of 1, 2, 3, 6. The average numbers of inference layers are reported in the scatter plot (Figure 8). We can see that by adjusting the threshold and the patience parameter, one can easily control the average inference layers and cover (or achieve values close to) any speedup ratios."
    } ],
    "references" : [ {
      "title" : "Binarybert: Pushing the limit of bert quantization",
      "author" : [ "Haoli Bai", "Wei Zhang", "L. Hou", "L. Shang", "Jing Jin", "X. Jiang", "Qun Liu", "Michael R. Lyu", "Irwin King." ],
      "venue" : "ArXiv, abs/2012.15701.",
      "citeRegEx" : "Bai et al\\.,? 2020",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "Adaptive neural networks for efficient inference",
      "author" : [ "Tolga Bolukbasi", "J. Wang", "O. Dekel", "Venkatesh Saligrama." ],
      "venue" : "ICML.",
      "citeRegEx" : "Bolukbasi et al\\.,? 2017",
      "shortCiteRegEx" : "Bolukbasi et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Depth-adaptive transformer",
      "author" : [ "Maha Elbayad", "Jiatao Gu", "Edouard Grave", "Michael Auli." ],
      "venue" : "ArXiv, abs/1910.10073.",
      "citeRegEx" : "Elbayad et al\\.,? 2020",
      "shortCiteRegEx" : "Elbayad et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing transformer depth on demand with structured dropout",
      "author" : [ "Angela Fan", "E. Grave", "Armand Joulin." ],
      "venue" : "ArXiv, abs/1909.11556.",
      "citeRegEx" : "Fan et al\\.,? 2020",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "X. Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-scale dense convolutional networks for efficient prediction",
      "author" : [ "Gao Huang", "Danlu Chen", "T. Li", "Felix Wu", "L.V.D. Maaten", "Kilian Q. Weinberger." ],
      "venue" : "ArXiv, abs/1703.09844.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "To trust or not to trust a classifier",
      "author" : [ "Heinrich Jiang", "Been Kim", "Maya R. Gupta." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Jiang et al\\.,? 2018",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2018
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Y. Yin", "L. Shang", "Xin Jiang", "X. Chen", "Linlin Li", "F. Wang", "Qun Liu." ],
      "venue" : "ArXiv, abs/1909.10351.",
      "citeRegEx" : "Jiao et al\\.,? 2020a",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "TinyBERT: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–",
      "citeRegEx" : "Jiao et al\\.,? 2020b",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Shallow-deep networks: Understanding and mitigating network overthinking",
      "author" : [ "Y. Kaya", "Sanghyun Hong", "T. Dumitras." ],
      "venue" : "ICML.",
      "citeRegEx" : "Kaya et al\\.,? 2019",
      "shortCiteRegEx" : "Kaya et al\\.",
      "year" : 2019
    }, {
      "title" : "I-bert: Integer-only bert quantization",
      "author" : [ "Se-Hoon Kim", "Amir Gholami", "Zhewei Yao", "M.W. Mahoney", "K. Keutzer." ],
      "venue" : "ArXiv, abs/2101.01321.",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ArXiv, abs/1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Fastbert: a self-distilling bert with adaptive inference time",
      "author" : [ "Weijie Liu", "P. Zhou", "Zhe Zhao", "Zhiruo Wang", "Haotang Deng", "Q. Ju." ],
      "venue" : "ArXiv, abs/2004.02178.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Are sixteen heads really better than one? In NeurIPS",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Michel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "Xipeng Qiu", "Tianxiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang." ],
      "venue" : "ArXiv, abs/2003.08271.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "ArXiv, abs/1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "The right tool for the job: Matching model and instance complexities",
      "author" : [ "Roy Schwartz", "Gabi Stanovsky", "Swabha Swayamdipta", "Jesse Dodge", "N.A. Smith." ],
      "venue" : "ACL.",
      "citeRegEx" : "Schwartz et al\\.,? 2020a",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "The right tool for the job: Matching model and instance complexities",
      "author" : [ "Roy Schwartz", "Gabriel Stanovsky", "Swabha Swayamdipta", "Jesse Dodge", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Schwartz et al\\.,? 2020b",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "S. Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "D. Erhan", "Ian J. Goodfellow", "Rob Fergus." ],
      "venue" : "CoRR, abs/1312.6199.",
      "citeRegEx" : "Szegedy et al\\.,? 2014",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Branchynet: Fast inference via early exiting from deep neural networks",
      "author" : [ "Surat Teerapittayanon", "Bradley McDanel", "H.T. Kung." ],
      "venue" : "2016 23rd International Conference on Pattern Recognition (ICPR), pages 2464–2469.",
      "citeRegEx" : "Teerapittayanon et al\\.,? 2016",
      "shortCiteRegEx" : "Teerapittayanon et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "L. Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "BlackboxNLP@EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Deebert: Dynamic early exiting for accelerating bert inference",
      "author" : [ "J. Xin", "Raphael Tang", "J. Lee", "Y. Yu", "Jimmy Lin." ],
      "venue" : "ArXiv, abs/2004.12993.",
      "citeRegEx" : "Xin et al\\.,? 2020",
      "shortCiteRegEx" : "Xin et al\\.",
      "year" : 2020
    }, {
      "title" : "BERxiT: Early exiting for BERT with better fine-tuning and extension to regression",
      "author" : [ "Ji Xin", "Raphael Tang", "Yaoliang Yu", "Jimmy Lin." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Xin et al\\.,? 2021",
      "shortCiteRegEx" : "Xin et al\\.",
      "year" : 2021
    }, {
      "title" : "Bert-of-theseus: Compressing bert by progressive module replacing",
      "author" : [ "Canwen Xu", "Wangchunshu Zhou", "Tao Ge", "Furu Wei", "M. Zhou." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Z. Yang", "Zihang Dai", "Yiming Yang", "J. Carbonell", "R. Salakhutdinov", "Quoc V. Le." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Ternarybert: Distillation-aware ultra-low bit bert",
      "author" : [ "W. Zhang", "L. Hou", "Y. Yin", "L. Shang", "X. Chen", "X. Jiang", "Qun Liu." ],
      "venue" : "ArXiv, abs/2009.12812.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert loses patience: Fast and robust inference with early exit",
      "author" : [ "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian McAuley", "Ke Xu", "Furu Wei." ],
      "venue" : "ArXiv, abs/2006.04152.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "To prune, or not to prune: exploring the efficacy of pruning for model compression",
      "author" : [ "M. Zhu", "S. Gupta." ],
      "venue" : "ArXiv, abs/1710.01878.",
      "citeRegEx" : "Zhu and Gupta.,? 2018",
      "shortCiteRegEx" : "Zhu and Gupta.",
      "year" : 2018
    }, {
      "title" : "GAML-BERT: Improving BERT early exiting by gradient aligned mutual learning",
      "author" : [ "Wei Zhu", "Xiaoling Wang", "Yuan Ni", "Guotong Xie." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3033–3044, Online",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (Qiu et al., 2020), the significant latency during inference forbids more widely industrial usage.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "Since BERT (Devlin et al., 2018), the pre-trained language models (PLMs) become the default stateof-the-art (SOTA) models for natural language processing (NLP).",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "The recent years have witnessed the rise of many PLMs, such as GPT (Radford et al., 2019), XLNet (Yang et al.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : ", 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : ", 2019), and ALBERT (Lan et al., 2020), and so forth.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 33,
      "context" : "(Zhou et al., 2020; Zhu et al., 2021) shows that in the sentence classification task, BERT’s last few layers may be too deep for some samples.",
      "startOffset" : 0,
      "endOffset" : 37
    }, {
      "referenceID" : 35,
      "context" : "(Zhou et al., 2020; Zhu et al., 2021) shows that in the sentence classification task, BERT’s last few layers may be too deep for some samples.",
      "startOffset" : 0,
      "endOffset" : 37
    }, {
      "referenceID" : 33,
      "context" : "Figure 2: Comparison between PABEE (Zhou et al., 2020) and our PCEE-BERT, a novel early exiting method that combines the score-based early exiting with the patience-based early exiting.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : "There exists a branch of literature focusing on making PLMs’ inference more efficient via network pruning (Zhu and Gupta, 2018; Xu et al., 2020; Fan et al., 2020; Michel et al., 2019), knowl-",
      "startOffset" : 106,
      "endOffset" : 183
    }, {
      "referenceID" : 30,
      "context" : "There exists a branch of literature focusing on making PLMs’ inference more efficient via network pruning (Zhu and Gupta, 2018; Xu et al., 2020; Fan et al., 2020; Michel et al., 2019), knowl-",
      "startOffset" : 106,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "There exists a branch of literature focusing on making PLMs’ inference more efficient via network pruning (Zhu and Gupta, 2018; Xu et al., 2020; Fan et al., 2020; Michel et al., 2019), knowl-",
      "startOffset" : 106,
      "endOffset" : 183
    }, {
      "referenceID" : 16,
      "context" : "There exists a branch of literature focusing on making PLMs’ inference more efficient via network pruning (Zhu and Gupta, 2018; Xu et al., 2020; Fan et al., 2020; Michel et al., 2019), knowl-",
      "startOffset" : 106,
      "endOffset" : 183
    }, {
      "referenceID" : 22,
      "context" : "edge distillation (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2020a), weight quantization (Zhang et al.",
      "startOffset" : 18,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "edge distillation (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2020a), weight quantization (Zhang et al.",
      "startOffset" : 18,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "edge distillation (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2020a), weight quantization (Zhang et al.",
      "startOffset" : 18,
      "endOffset" : 75
    }, {
      "referenceID" : 32,
      "context" : ", 2020a), weight quantization (Zhang et al., 2020; Bai et al., 2020; Kim et al., 2021) and adaptive inference (Zhou et al.",
      "startOffset" : 30,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : ", 2020a), weight quantization (Zhang et al., 2020; Bai et al., 2020; Kim et al., 2021) and adaptive inference (Zhou et al.",
      "startOffset" : 30,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : ", 2020a), weight quantization (Zhang et al., 2020; Bai et al., 2020; Kim et al., 2021) and adaptive inference (Zhou et al.",
      "startOffset" : 30,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "Early exiting is one of the most important adaptive inference methods (Bolukbasi et al., 2017).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : ", 2016), FastBERT (Liu et al., 2020), and DeeBERT (Xin et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 28,
      "context" : ", 2020), and DeeBERT (Xin et al., 2020) calculated the entropy of the prediction probability distribution as an estimation for the confidence of exiting classifiers to enable dynamic early exiting.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "Shallow-Deep Nets (Kaya et al., 2019) and RightTool (Schwartz et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : ", 2019) and RightTool (Schwartz et al., 2020a) leveraged the maximum of the predicted distribution as the exiting signal.",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "The second type is the learned exiting (Elbayad et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : "For example, BERxiT (Xin et al., 2021) install a fully connected layer right after each transformer block of BERT to output a score that is used to decide whether the BERT should stop inference and exit early.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 33,
      "context" : "PABEE (Zhou et al., 2020) propose a dynamic exiting strategy analogous to early stopping model training.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 26,
      "context" : "Extensive experiments are conducted on the GLUE benchmark (Wang et al., 2018).",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "BERT is a multi-layer Transformer (Vaswani et al., 2017) network, which is pre-trained in a self-supervised manner on a large corpus.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 33,
      "context" : ", 2016) and patience-based method (Zhou et al., 2020) and overcomes their shortcomings.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "exclude the STS-B task since it is a regression task, and we exclude the WNLI task following previous work (Devlin et al., 2018; Xu et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "exclude the STS-B task since it is a regression task, and we exclude the WNLI task following previous work (Devlin et al., 2018; Xu et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "We also investigate whether our method is applicable across different backbones, so we also run ablation experiments with ALBERT base (Lan et al., 2020) and TinyBERT6 (Jiao et al.",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 24,
      "context" : "Dynamic exiting: In this part, we compare our methods with a series of strong baselines, including BranchyNet (Teerapittayanon et al., 2016), Shallow-Deep (Kaya et al.",
      "startOffset" : 110,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : ", 2016), Shallow-Deep (Kaya et al., 2019), BERxiT (Xin et al.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 29,
      "context" : ", 2019), BERxiT (Xin et al., 2021), and PABEE (Zhou et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 33,
      "context" : "Following PABEE (Zhou et al., 2020), we mainly report the speedup ratio as the efficiency metric.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : "adaptive inference (Teerapittayanon et al., 2016; Kaya et al., 2019), inference is on a per-instance basis, i.",
      "startOffset" : 19,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "adaptive inference (Teerapittayanon et al., 2016; Kaya et al., 2019), inference is on a per-instance basis, i.",
      "startOffset" : 19,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "This is a common scenario in the industry where individual requests from different users (Schwartz et al., 2020b) come at different time points.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "However, can PCEE-BERT work with the other confidence measures, such as Shallow-Deep? We switch the entropy-based confidence level C(M) (Equation 2) with that from Shallow-Deep (Kaya et al., 2019):",
      "startOffset" : 177,
      "endOffset" : 196
    }, {
      "referenceID" : 14,
      "context" : "However, PCEE-BERT can also work with the other types of pre-trained backbones, such as ALBERT base (Lan et al., 2020) and TinyBERT6 (Jiao et al.",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : "strate the effectiveness of PCEE-BERT on the image classification task, we follow the experimental settings in PABEE (Zhou et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "We conduct experiments on two image classification datasets, CIFAR-10 and CIFAR-100 (Krizhevsky, 2009).",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "The ResNet-56 model (He et al., 2016) serves as the backbone, and we compare PCEEBERT with PABEE.",
      "startOffset" : 20,
      "endOffset" : 37
    } ],
    "year" : 0,
    "abstractText" : "BERT and other pre-trained language models (PLMs) are ubiquitous in the modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (Qiu et al., 2020), the significant latency during inference forbids more widely industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-theshelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer’s prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can make achieve different speedup ratios by adjusting the patience parameter and the confidence threshold.",
    "creator" : null
  }
}
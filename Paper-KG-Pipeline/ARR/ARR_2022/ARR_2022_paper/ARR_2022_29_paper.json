{
  "name" : "ARR_2022_29_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Transformer models such as BERT (Devlin et al., 2019), and other variants (Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020) have achieved state-of-the-art results on many challenging NLP tasks. Moreover, recent work in longinput transformers (Ainslie et al., 2020; Zaheer et al., 2020b; Beltagy et al., 2020; Tay et al., 2021) has shown that increasing the input length a Transformer is able to process results in further performance gains. Additionally, it is also known that increasing model size also leads to performance gains in many tasks (Kaplan et al., 2020).\nIn this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. To achieve this, we integrate long-input transformer attention and pre-training ideas into the scalable T5 (Raffel et al., 2019a) model architecture. The resulting model, as shown in Figure 1, achieves state-of-the-art performance on several tasks which require handling long sequence inputs.\nRegarding attention, we design a new attention mechanism, which we call Transient Global (TGlobal), that mimics ETC’s local/global mechanism (Ainslie et al., 2020). Importantly, TGlobal attention removes the need for the additional side inputs in ETC, in order to fit within the T5 architecture. The main idea of ETC’s local/global mechanism is to introduce local sparsity in the attention mechanism to reduce the quadratic cost when scaling to long inputs. Specifically, ETC only allows tokens in the input (called the long input) to attend to a local neighborhood, and adds a secondary input called the global memory, through which tokens in the long input can attend to each other indirectly. One disadvantage of this mechanism is that it requires designing this secondary global input for each new problem. In order to adapt it to T5, our new TGlobal mechanism synthesizes these global tokens on the fly (as aggregations of groups of tokens in the input), at each attention layer. Our experiments show that this mechanism results in only a small degradation in performance with respect to full attention in the same input length but allows the model to scale to much larger input lengths,\nresulting in significant performance gains. Regarding pre-training, we adopt the pretraining strategy in the PEGASUS (Zhang et al., 2019a) model. This pre-training strategy was originally designed for abstractive summarization, but in our experiments, we found it also improves model performance for other tasks, such as question answering, and hence we adopted it in LongT5. The key idea is to mask out key (principle) sentences from a document and ask the model to reproduce them as a single string, as if it was a summary.\nWe evaluate LongT5 on several summarization and question answering tasks (see Sections 4.2.1 and 4.3.1 for detailed descriptions of these datasets). Thanks to the scaling of both input length and model size, we achieve state-of-the-art results on many of them.\nThe main contributions of this work are:\n• A new Transformer architecture, LongT5, that allows for scaling both input length and model scale at the same time1.\n• A new attention mechanism (TGlobal), which mimics ETC’s local/global mechanism but is a drop-in replacement to regular attention for existing Transformer architectures like T5.\n• An analysis of model performance when varying both input length and model size of vanilla T5 and LongT5 models (pushing both models up to the maximum lengths they can handle before encountering memory issues), to understand the trade-offs in both performance and computation cost.\n• State-of-the-art results on the arXiv, PubMed, BigPatent, MediaSum, and TriviaQA datasets. For Natural Questions, we used a slightly different formulation than the original tasks, and hence we do not make state-of-the-art claims.\n2 T5\nT5 (Raffel et al., 2019a) is a transformer based textto-text pre-trained language model that is gaining popularity for its unified framework that converts all text-based language problems into a text-to-text format, and its ease to scale up in number of parameters (from 60M to 11B parameters) with model parallelism. With full attention transformer, T5 has been successfully applied to many NLP tasks, but\n1We will open source our codes and pre-trained models in case of acceptance.\nthe tasks only require shorter input sequences. This is due to the limitation of quadratic computation growth with respect to input sequence length, resulting in larger memory consumption and longer training time. Recently, Press et al. (2021) explored scaling up T5 style models at inference time to longer sequences than seen during training, but how to scale up T5 style models in the input sequence length during training remains underexplored."
    }, {
      "heading" : "3 LongT5",
      "text" : ""
    }, {
      "heading" : "3.1 Architecture",
      "text" : "We extend the original T5 encoder with globallocal attention sparsity patterns (Ainslie et al., 2020; Zaheer et al., 2020a) to handle long inputs. For the work reported in this paper, we used a standard T5 decoder since all of the tasks we considered require relatively short output sequence lengths.\nArchitecturally, the main difference between T5 and LongT5 lies in the attention mechanism. We experiment with two attention mechanism variations for LongT5, illustrated in Figure 2: (1) Local Attention and (2) Transient Global Attention (TGlobal). Both variations preserve several properties of T5: relative position representations, support for example packing, and compatibility with T5 checkpoints."
    }, {
      "heading" : "3.1.1 Local Attention",
      "text" : "For Local Attention, we simply replace the encoder self-attention operation in T5 with a sparse slidingwindow local attention operation following the implementation in ETC (Ainslie et al., 2020). Specifically, for a given local radius r, this formulation only allows each token to attend r tokens to the left and right of it (see Figure 2.a). We found r = 127 to be sufficient in practice, where r is the number of neighboring tokens to the left and to the right.\nLocal Attention does not introduce any new parameters and easily accommodates the attention masking required for example packing2. For a given choice of r, complexity is linear in input sequence length l: O(l × r)."
    }, {
      "heading" : "3.1.2 Transient Global Attention (TGlobal)",
      "text" : "To allow input tokens to interact with each other in each layer of the encoder at a longer range than Lo-\n2Example packing refers to packing more than one short example in the same input sequence to increase training efficiency. This is specially useful in LongT5, since with the large input lengths used in our model, if many examples are short, most of the input sequence would be dedicated to padding, wasting significant computation.\nAttention keys\nA tte\nnt io\nn qu\ner ie\ns\nAttention keys\nA tte\nnt io\nn qu\ner ie\ns\nAttention keys\nA tte\nnt io\nn qu\ner ie\ns\nx1 … xk xk+1 … xl g1 … gm\n+\n+\n…\nInput Tokens Global Tokens\nEach global token is the result of averaging k input tokens.\nEach input token can attend to its neighborhood (like in local attention), plus to all global tokens.\nEach input token can attend to its\nneighborhood: r tokens to the left, and r tokens to the right.\nrr\na) LongT5 Local Attention b) LongT5 Transient Global (TGlobal) Attention\nLayerNorm …\nFigure 2: Illustration of the two attention mechanisms we experimented with in LongT5.\ncal Attention’s local radius, we introduce Transient Global Attention as a modification of ETC’s globallocal attention in a “fixed blocks” pattern. Namely, we divide the input sequence into blocks of k tokens, and for each block we compute a global token by summing (and then normalizing) the embeddings of every token in the block (see Figure 2.b). Now when computing attention, we allow each input token to attend not only to nearby tokens like in Local Attention, but also to every global token. We call these global tokens transient because in contrast to ETC-like global-local attention patterns, these tokens are dynamically constructed (and subsequently discarded) within each attention operation, removing any requirement for deciding which input tokens should be treated as “global”.\nTGlobal attention only introduces a couple new parameters3: (1) T5-style relative position biases representing the distance from an input token’s block to the block of each global token it’s attending to, and (2) T5-style layer normalization parameters for normalizing each global token’s embedding. The rest of the parameters are identical to T5, and we accommodate sequence packing by additionally masking attention from input tokens to global tokens of other examples. We found block size k = 16 to be sufficient in practice. Notice thus, that TGlobal attention introduces a block of l ∗ l/k additional attention key-value pairs to calculate on top of Local Attention (l input tokens, attending to l/k global tokens; represented by the right most rectangle in Figure 2.b), hence for input sequence length l, complexity is O(l(r + l/k)).\n3For base models, we introduced 10k additional parameters, 25k for large, and 50k for xl."
    }, {
      "heading" : "3.2 PEGASUS Principle Sentences Generation Pre-training",
      "text" : "T5 is pre-trained with a span corruption objective, where consecutive spans of input tokens are replaced with a mask token and the model is trained to reconstruct the masked-out tokens. While it is effective, recent work on masked language modeling (MLM) (Liu et al., 2019; Zhang et al., 2019b) shows that carefully selecting the prediction objective could lead to significantly better performance. One argument is that predicting more informative tokens from the text could force the model to learn better semantics of the text. Motivated by that, we explore masking and generating the principle sentences from the text. In particular, we adopt the Gap Sentences Generation with Principle IndUniq strategy from Zhang et al. (2019a), which was used for summarization pre-training.\nFollowing Zhang et al. (2019a), we select top-m scored (Principle) sentences based on ROUGE-F1 score (Lin, 2004) using si = rouge(xi, D \\ {xi},∀i), where i is the sentence index, D is the collection of sentences in the document. Each sentence is scored independently (Ind), and each n-gram is only counted once (Uniq)."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Configurations",
      "text" : "LongT5 is implemented using JAX4 and the Flaxformer5 library. Following the same setup as T5.1.16, we consider models of 3 sizes: base\n4https://github.com/google/jax 5https://github.com/google/flaxformer 6https://github.com/google-research/text-to-text-transfer-\ntransformer/blob/main/released_checkpoints.md#t511\n(∼220M), large (∼770M), and xl (∼3B), and use the same cased English SentencePiece vocab model used by T5.1.1, which contains 32000 sentence pieces. We use batch size of 128 and Adafactor as the optimizer in all experiments."
    }, {
      "heading" : "4.1.1 Pre-training",
      "text" : "We pre-train LongT5 models for 1M steps on 4096 input sequence length and 910 output sequence length. We use the same inverse squareroot learning rate schedule as T5, with learning rate set to 1/ √ max(step, warm_up steps), where warm_up steps is set to 1000. The same as T5.1.1, we pre-train LongT5 only on the C4 dataset (Raffel et al., 2019b), and we do not apply dropout during pre-training. As described in section 3.2, we use the PEGASUS Principle Sentences Generation objective as our pre-training objective. The configuration is similar to what was described by Zhang et al. (2019a) for their larger models, except for the masked sentence ratio in which we use a value of 0.2 instead of 0.457. In section 5.3, we will show our ablation study between Principle Sentences Generation and Span Corruption."
    }, {
      "heading" : "4.1.2 Fine-tuning",
      "text" : "For fine-tuning, we use a constant learning rate of 0.001 and dropout rate of 0.1 for all tasks. For summarization tasks, we experiment with values of 4096, 8192, and 16384 for input lengths and 512 for output lengths. For QA tasks, we experiment with values starting at 512 and scale up to 36864 for input lengths and 128 for output lengths."
    }, {
      "heading" : "4.2 Evaluation on Summarization Tasks",
      "text" : "We choose to benchmark our models on summarization tasks that cover various context lengths, because of their long context understanding and generative nature."
    }, {
      "heading" : "4.2.1 Datasets",
      "text" : "LongT5 was benchmarked on the following six datasets.\nCNN / Daily Mail (Nallapati et al., 2016) News from CNN and Daily Mail are used as input and the article’s summary bullets are the target summary.\nPubMed (Cohan et al., 2018) Scientific documents were collected from PubMed, with a document’s content used as input and its corresponding abstract as the target summary.\n7We briefly experimented with other values, but found 0.2 to work best with the downstream tasks of interest.\narXiv (Cohan et al., 2018) Similar to PubMed, but with documents taken from arXiv.\nBigPatent (Sharma et al., 2019) U.S. patent documents, with the patent’s details used as input and the patent’s abstract as the target summary.\nMediaSum (Zhu et al., 2021) Interview transcripts from CNN and NPR were used as input and their corresponding topic and overviews used as the target summary.\nMulti-News (Fabbri et al., 2019) The task involves summarizing multiple news documents about a topic into a human-written summary.\nTable 1 provides statistics for the number of examples in train, validation, and test splits, and the average, median, max, and 90th percentile input sequence length. As can be seen, these datasets are long in input length, and would benefit from models that can model lengthier inputs. We included the CNN / Daily Mail dataset to benchmark on a common task, especially to see how using TGlobal attention impacts the model, despite the length of the inputs being smaller than the other datasets."
    }, {
      "heading" : "4.2.2 Results",
      "text" : "We compare LongT5 with various top approaches: BigBird-PEGASUS (Zaheer et al., 2020b), HATBART (Rohde et al., 2021), DANCER PEGASUS (Gidiotis and Tsoumakas, 2020), PRIMER (Xiao et al., 2021), TG-MultiSum (Cui and Hu, 2021), LED (Beltagy et al., 2020), and an application of BART by Zhu et al. (2021). For these comparisons, we use common evaluation metrics of ROUGE-1, ROUGE-2, and ROUGE-L.\nAs can be seen in Table 2, LongT5 is able to achieve state-of-the-art rouge scores for arXiv, PubMed, BigPatent, and MediaSum. For arXiv and PubMed, which are composed of longer inputs, being able to scale up to 16k input length helps LongT5 achieve strong results.\nOne dataset where LongT5 is not able to achieve state-of-the-art results is with Multi-News. LongT5 is the 2nd best model, slightly worth than PRIMER. This is understandable as the PRIMER model was pre-trained on a large corpus of documents related to news events, thus exposing the model to a similar corpus as that seen in Multi-News.\nWhen looking at CNN / Daily Mail, we can see that LongT5 was comparable with HAT-BART, despite not having full attention. LongT5 did at least get stronger scores in the ROUGE-2 metric."
    }, {
      "heading" : "4.3 Evaluation on QA Tasks",
      "text" : "For the evaluation on QA tasks, we choose two popular benchmarks, Natural Questions and TriviaQA, that require long context understanding."
    }, {
      "heading" : "4.3.1 Datasets",
      "text" : "NaturalQuestions (NQ) Questions are real queries issued by multiple users to Google search that retrieve a Wikipedia page in the top five search results. Answer text is drawn from the search results (Kwiatkowski et al., 2019).\nThe original NQ dataset asks models to predict a short answer (including no-answer or yes/no) and a long answer. We framed the task as a seq2seq task and ignored the long answer. Hence, our results focus only on short answer. Moreover, since our models predict answer texts instead of answer spans, our evaluation method differs slightly from the leader boards, and our results are not directly comparable to other existing approaches: (1) Since only the train and dev sets are publicly available, we use 90% of the official train set for training while using 10% as hold-out dev set to fine-tune the hyperparameters and training epoch, and use the official dev set as our test set. (2) We benchmark LongT5 against the corresponding T5.1.1 models instead of directly comparing to the leader boards.\nTriviaQA Trivia enthusiasts authored questionanswer pairs. Answers are drawn from Wikipedia and Bing web search results, excluding trivia websites (Joshi et al., 2017).\nWe use the official train/validation splits for training and fine-tuning the hyperparameters and training epoch, then re-train that model combining both train and validation sets to evaluate on the Wikipedia domain on the leader board 8.\nTable 3 shows the dataset statistics for the number of examples in train and validation splits, and the average, median, max, and 90th percentile input sequence length.\n8https://competitions.codalab.org/competitions/17208"
    }, {
      "heading" : "4.3.2 Results",
      "text" : "Table 4 shows a summary of the results for the NQ and TriviaQA datasets (see Appendix B for full results). For each dataset, we show two metrics: EM (Exact Match) and F1 score (evaluating precision and recall of individual words in the answer compared to the ground truth, ignoring stop words).\nFor NQ, we compare T5.1.1 and LongT5 with TGlobal attention. We decided to run T5.1.1 (1) with the default 512 input sequence length9 and (2) with the largest input sequence length that can fit into device memory10, and use those as baselines. Since we are comparing against T5.1.1, for LongT5 experiments we report results at 512 input length for base and large, and the largest input length allowed by each model before running out of memory on the same hardware configuration used in our T5.1.1 experiments.\nAs the table shows, increasing input length generally results in significant benefits in NQ, with models with larger input lengths significantly outperforming those with smaller input lengths in most cases. Some times, models with the largest input lengths underperform those with 4k length, but we believe those to be due to noise in the experiments, as results are the output of just one repetition of each experiment due to resource constraints. Moreover, LongT5 with TGlobal attention significantly outperforms T5.1.1. For example, considering the large size models, T5.1.1 was able to scale up to an input length of 3k tokens, while the TGlobal model was able to reach 6k tokens, outperforming T5.1.1 at 4k token length (there was a dip at 6k token length, but we hypothesize this is just due to variance, as we only did one run for each configuration).\nFor TriviaQA, we compare LongT5 with various top approaches on the leader board: BigBird-ETC (Zaheer et al., 2020a), Fusion-in-Decoder (Izacard and Grave, 2021), and ReadTwice (Zemlyanskiy et al., 2021). As shown in Table 3, TriviaQA inputs\n9For base and large models. 10For base and large models, we used 4x8 TPUv3 and no model partitioning; for xl model, we used 8x16 TPUv3 and 8 partitions.\nare quite long, therefore being able to scale up both in model size and to 16k input length helps LongT5 achieve state-of-the-art."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Input Length vs Speed",
      "text" : "In order to evaluate the training speed and memory consumption of LongT5, compared to T5.1.1, we performed a series of training runs in the NQ data set starting at input length 512, and increasing the input length steadily until models ran out of memory on a 4x8 TPUv3 slice. Results are shown in Figure 3, which compares 6 different model configurations: T5.1.1 base, T5.1.1 large, LongT5 (base Local), LongT5 (large Local), LongT5 (base TGlobal), and LongT5 (large TGlobal). For each model configuration, we show a curve plotting the number of sequences per second processed during\ntraining (speed, in the vertical axis) for each input length (horizontal axis). Both axes are shown in logarithmic scale.\nWe can see that at shorter lengths (512), T5.1.1, LongT5 Local, LongT5 TGlobal have similar speeds, but as we increase the sequence length, LongT5 becomes significantly faster. For example at sequence length 2048, T5.1.1 base can only process 479 sequences per second, while LongT5 (base TGlobal) can process 765 and LongT5 (base Local) can process 860. The differences grow even larger as sequence length increases.\nAnother important fact that Figure 3 shows is that T5.1.1 models reach their out of memory point much earlier. For example, we could only scale up to 6k tokens for T5.1.1 base. On the other hand, LongT5 (base Local) can go up to 36k tokens in length, and LongT5 (base TGlobal) up to 12k. Large models show a similar picture with T5.1.1 large going only up to 3k, but the LongT5 variants going to 10k (large Local) and 6k (large TGlobal)."
    }, {
      "heading" : "5.2 Input Length vs Performance",
      "text" : "This section presents a similar analysis, but where we plotted model speed versus performance in NQ (F1 score). Results are shown in Figure 4 for models with large size. Each point in the curves is annotated with the corresponding sequence length.\nAs Figure 4 shows, performance increases sig-\nnificantly as input length increases, highlighting the benefits of LongT5. Moreover, input length by itself is not enough to achieve good performance in all datasets, and in particular, in the NQ dataset (used in this figure), using Local Attention significantly hurts performance when compared with TGlocal or with T5.1.1. So, even at very long input lengths, LongT5 with Local Attention just matches T5.1.1 with input length of 3k in NQ. However, LongT5 with TGlobal attention outperforms T5.1.1. Moreover, note that although the plot shows a few irregularities (such as 8k length for LongT5 with Local Attention, or 6k length with TGlobal Attention), that is because the plot shows only the results of a single run, and hence there is some noise. However, trends can clearly be seen."
    }, {
      "heading" : "5.3 Principle Sentences Generation vs. Span Corruption",
      "text" : "As mentioned in section 3.2, we use PEGASUS Principle Sentences Generation instead of default Span Corruption used in T5 as our pre-training objective. Table 5 shows our ablation study with the default Span Corruption pre-training objective, compared to Principle Sentences Generation for both NQ and arXiv. The comparison is done on the dev set of the tasks, and with TGlobal base models. Fine-tuning is done with input sequence length 4096. The table shows, even though Principle Sentences Generation was developed by Zhang et al. (2019a) as a pre-training strategy for summarization, it benefits both summarization and QA tasks."
    }, {
      "heading" : "6 Related Work",
      "text" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020). BERT (Devlin et al., 2019) introduced Mask Language Model (MLM), where a model predicts masked tokens given a sequence of text input. Fine-tuning a pre-trained BERT model has led to improved performance on various NLP tasks. However, MLM predictions are not made auto-regressively, which limits the capability of the BERT family for generation tasks. Raffel et al. (2019a) introduced the span corruption task in T5 as the pre-training objective, where a model predicts the masked token span using an autoregressive model. It can handle the generation tasks as the pretraining is done in a generative way. BART (Lewis et al., 2020) is similar to T5 but used a slightly different pre-training objective, in which spans are masked from the input but the complete output is predicted. However, none of these works tried to investigate pre-training for very long sequence inputs. They often use a transformer (Vaswani et al., 2017) architecture as backbone, the complexity of which is quadratic to the input length, making them impractical to model very long sequence input.\nLong text modeling An extensive amount of work has also been done for modeling long text like documents. The work from Roy et al. (2016); Chen (2017); Wu et al. (2018) obtained document embeddings from word-level embeddings. Another line of research tries to model long documents through hierarchical training. The work from Yang et al. (2016); Miculicich et al. (2018) employed Hierarchical Attention Networks for document classification and neural machine translation, and Guo\net al. (2019) proposed using a hierarchy network to build document embeddings on top of sentence embeddings for parallel document mining.\nMore recent research has been focusing on improving the memory and computation efficiency of transformer models (Tay et al., 2020b, 2021) for handling long input. One type of such approaches is using non-full attention patterns to restrict the attention field range, so that it reduces the attention complexity from O(n2) to O(nlogn) or O(n), including Sinkhorn (Tay et al., 2020a), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020a). Another type of approaches is leveraging the low-rank approximation of the attention matrix, such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2021), Random Feature Attention (Peng et al., 2021), and LUNA (Ma et al., 2021)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper presented a new Transformer-based neural model called LongT5, with which we have explored the effects of scaling both input length and model size at the same time. Specifically, the main differences of LongT5 with respect to T5.1.1 are (1) a new scalable attention mechanism called Transient Global attention, which is a drop-in replacement to the standard T5 attention mechanism, and hence can be used without needing additional sideinputs to the model or modifications to the model inputs; and (2) using a PEGASUS-style Principle Sentences Generation pre-training objective.\nVia experimentation in several challenging summarization and question answering datasets, we have explored the performance gains that can be achieved by scaling both input length and model size, resulting in state-of-the-art results on several datasets: arXiv, PubMed, BigPatent, MediaSum, and TriviaQA.\nAs part of our future work, we would like to pursue several directions such as studying efficient attention mechanisms in the decoder and decoder-toencoder attention pieces of the model (both Local Attention and TGlobal attention are only applied to the encoder in LongT5 for now). Additionally, we would like to incorporate additional long-input transformer ideas into the LongT5 architecture, that could further improve model efficiency."
    }, {
      "heading" : "A Summarization Results",
      "text" : "Table 7 shows the full set of results on the summarization datasets used in this paper. This includes both standard T5 model (using version T5.1.1) and LongT5 model.\nAs can be seen, scaling up the input size for both models helps achieve better performance metrics. T5 models though struggle when scaling up to 4k for input, as the fine-tuning task can take many days even when using a large topology of TPUv3.\nLongT5, despite having a reduced attention from using TGlobal attention, is able to get strong performance results due to both scaling up to larger inputs and leveraging the Gap Sentences Generation pre-training strategy."
    }, {
      "heading" : "B QA Results",
      "text" : "Table 6 shows the full set of results comparing T5.1.1 and LongT5 models on the QA datasets used in this paper. For both NQ and TriviaQA in this comparison study, we use 90% of the official training set for training while using 10% as holdout dev set to fine-tune the hyperparameters and training epoch, and use the official dev set to report the numbers in this table. We run each model to the largest input length allowed before running out of memory on specific hardware configuration - base/large models on 4x8 TPUv3 with no model partitioning, and xl models on 8x16 TPUv3 with 8 partitions."
    } ],
    "references" : [ {
      "title" : "ETC: encoding long and structured data in transformers",
      "author" : [ "Joshua Ainslie", "Santiago Ontañón", "Chris Alberti", "Philip Pham", "Anirudh Ravula", "Sumit Sanghai." ],
      "venue" : "CoRR, abs/2004.08483.",
      "citeRegEx" : "Ainslie et al\\.,? 2020",
      "shortCiteRegEx" : "Ainslie et al\\.",
      "year" : 2020
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient vector representation for documents through corruption",
      "author" : [ "Minmin Chen." ],
      "venue" : "5th International Conference on Learning Representations.",
      "citeRegEx" : "Chen.,? 2017",
      "shortCiteRegEx" : "Chen.",
      "year" : 2017
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of the 2018 Conference of",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "Topic-guided abstractive multi-document summarization",
      "author" : [ "Peng Cui", "Le Hu" ],
      "venue" : null,
      "citeRegEx" : "Cui and Hu.,? \\Q2021\\E",
      "shortCiteRegEx" : "Cui and Hu.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-News: A large-scale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir Radev." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "A divide-and-conquer approach to the summarization of long documents",
      "author" : [ "Alexios Gidiotis", "Grigorios Tsoumakas." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:3029– 3040.",
      "citeRegEx" : "Gidiotis and Tsoumakas.,? 2020",
      "shortCiteRegEx" : "Gidiotis and Tsoumakas.",
      "year" : 2020
    }, {
      "title" : "Hierarchical document encoder for parallel corpus mining",
      "author" : [ "Mandy Guo", "Yinfei Yang", "Keith Stevens", "Daniel Cer", "Heming Ge", "Yun-hsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of the Fourth",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Leveraging passage retrieval with generative models for open domain question answering",
      "author" : [ "Gautier Izacard", "Edouard Grave" ],
      "venue" : null,
      "citeRegEx" : "Izacard and Grave.,? \\Q2021\\E",
      "shortCiteRegEx" : "Izacard and Grave.",
      "year" : 2021
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Van-",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Scaling laws for neural language models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei." ],
      "venue" : "arXiv preprint arXiv:2001.08361.",
      "citeRegEx" : "Kaplan et al\\.,? 2020",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association of Computational Linguistics.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Luna: Linear unified nested attention",
      "author" : [ "Xuezhe Ma", "Xiang Kong", "Sinong Wang", "Chunting Zhou", "Jonathan May", "Hao Ma", "Luke Zettlemoyer." ],
      "venue" : "Thirty-Fifth Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Ma et al\\.,? 2021",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2021
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Çağlar Gu̇lçehre", "Bing Xiang" ],
      "venue" : "In Proceedings of The 20th SIGNLL Conference on Computational Natural Lan-",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Random feature attention",
      "author" : [ "Hao Peng", "Nikolaos Pappas", "Dani Yogatama", "Roy Schwartz", "Noah Smith", "Lingpeng Kong." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Peng et al\\.,? 2021",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2021
    }, {
      "title" : "Train short, test long: Attention with linear biases enables input length extrapolation",
      "author" : [ "Ofir Press", "Noah A. Smith", "Mike Lewis" ],
      "venue" : null,
      "citeRegEx" : "Press et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Press et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "CoRR, abs/1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019a",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "CoRR, abs/1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019b",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical learning for generation with long source sequences",
      "author" : [ "Tobias Rohde", "Xiaoxia Wu", "Yinhan Liu" ],
      "venue" : null,
      "citeRegEx" : "Rohde et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Rohde et al\\.",
      "year" : 2021
    }, {
      "title" : "Representing documents and queries as sets of word embedded vectors for information retrieval",
      "author" : [ "Dwaipayan Roy", "Debasis Ganguly", "Mandar Mitra", "Gareth J.F. Jones." ],
      "venue" : "CoRR, abs/1606.07869.",
      "citeRegEx" : "Roy et al\\.,? 2016",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2016
    }, {
      "title" : "BIGPATENT: A large-scale dataset for abstractive and coherent summarization",
      "author" : [ "Eva Sharma", "Chen Li", "Lu Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204–2213, Florence, Italy.",
      "citeRegEx" : "Sharma et al\\.,? 2019",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2019
    }, {
      "title" : "Sparse sinkhorn attention",
      "author" : [ "Yi Tay", "Dara Bahri", "Liu Yang", "Donald Metzler", "DaCheng Juan." ],
      "venue" : "ICML.",
      "citeRegEx" : "Tay et al\\.,? 2020a",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Long range arena : A benchmark for efficient transformers",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Samira Abnar", "Yikang Shen", "Dara Bahri", "Philip Pham", "Jinfeng Rao", "Liu Yang", "Sebastian Ruder", "Donald Metzler." ],
      "venue" : "International Conference on Learning",
      "citeRegEx" : "Tay et al\\.,? 2021",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient transformers: A survey",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler." ],
      "venue" : "ArXiv, abs/2009.06732.",
      "citeRegEx" : "Tay et al\\.,? 2020b",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Linformer: Self-attention with linear complexity",
      "author" : [ "Sinong Wang", "Belinda Z. Li", "Madian Khabsa", "Han Fang", "Hao Ma" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Word mover’s embedding: From word2vec to document embedding",
      "author" : [ "Lingfei Wu", "Ian En-Hsu Yen", "Kun Xu", "Fangli Xu", "Avinash Balakrishnan", "Pin-Yu Chen", "Pradeep Ravikumar", "Michael J. Witbrock." ],
      "venue" : "Proceedings of the 2018 Confer-",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "PRIMER: Pyramid-based masked sentence pre-training for multi-document summarization",
      "author" : [ "Wen Xiao", "Iz Beltagy", "Giuseppe Carenini", "Arman Cohan" ],
      "venue" : null,
      "citeRegEx" : "Xiao et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2021
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Big bird: Transformers for longer sequences",
      "author" : [ "Manzil Zaheer", "Guru Guruganesh", "Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontañón", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed." ],
      "venue" : "CoRR, abs/2007.14062.",
      "citeRegEx" : "Zaheer et al\\.,? 2020a",
      "shortCiteRegEx" : "Zaheer et al\\.",
      "year" : 2020
    }, {
      "title" : "Big Bird: Transformers for longer sequences",
      "author" : [ "Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontanon", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed." ],
      "venue" : "Advances in",
      "citeRegEx" : "Zaheer et al\\.,? 2020b",
      "shortCiteRegEx" : "Zaheer et al\\.",
      "year" : 2020
    }, {
      "title" : "Readtwice: Reading very large documents with memories",
      "author" : [ "Yury Zemlyanskiy", "Joshua Ainslie", "Michiel de Jong", "Philip Pham", "Ilya Eckstein", "Fei Sha" ],
      "venue" : null,
      "citeRegEx" : "Zemlyanskiy et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zemlyanskiy et al\\.",
      "year" : 2021
    }, {
      "title" : "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu." ],
      "venue" : "CoRR, abs/1912.08777.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "ERNIE: Enhanced language representation with informative entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "MediaSum: A large-scale media interview dataset for dialogue summarization",
      "author" : [ "Chenguang Zhu", "Yang Liu", "Jie Mei", "Michael Zeng." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Transformer models such as BERT (Devlin et al., 2019), and other variants (Liu et al.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : ", 2019), and other variants (Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020) have achieved state-of-the-art results on many challenging NLP tasks.",
      "startOffset" : 28,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : ", 2019), and other variants (Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020) have achieved state-of-the-art results on many challenging NLP tasks.",
      "startOffset" : 28,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : ", 2019), and other variants (Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020) have achieved state-of-the-art results on many challenging NLP tasks.",
      "startOffset" : 28,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : ", 2019), and other variants (Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020) have achieved state-of-the-art results on many challenging NLP tasks.",
      "startOffset" : 28,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Moreover, recent work in longinput transformers (Ainslie et al., 2020; Zaheer et al., 2020b; Beltagy et al., 2020; Tay et al., 2021) has shown that increasing the input length a Transformer is able to process results in further performance gains.",
      "startOffset" : 48,
      "endOffset" : 132
    }, {
      "referenceID" : 37,
      "context" : "Moreover, recent work in longinput transformers (Ainslie et al., 2020; Zaheer et al., 2020b; Beltagy et al., 2020; Tay et al., 2021) has shown that increasing the input length a Transformer is able to process results in further performance gains.",
      "startOffset" : 48,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "Moreover, recent work in longinput transformers (Ainslie et al., 2020; Zaheer et al., 2020b; Beltagy et al., 2020; Tay et al., 2021) has shown that increasing the input length a Transformer is able to process results in further performance gains.",
      "startOffset" : 48,
      "endOffset" : 132
    }, {
      "referenceID" : 29,
      "context" : "Moreover, recent work in longinput transformers (Ainslie et al., 2020; Zaheer et al., 2020b; Beltagy et al., 2020; Tay et al., 2021) has shown that increasing the input length a Transformer is able to process results in further performance gains.",
      "startOffset" : 48,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "Additionally, it is also known that increasing model size also leads to performance gains in many tasks (Kaplan et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "To achieve this, we integrate long-input transformer attention and pre-training ideas into the scalable T5 (Raffel et al., 2019a) model architecture.",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Figure 1: The average ROUGE score ((R-1 + R-2 + R-L)/3) of LongT5 and baseline models on arXiv and PubMed summarization tasks (Cohan et al., 2018) with different input length (x axis).",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 25,
      "context" : "Baseline models: HATBART (Rohde et al., 2021), BigBird-PEGASUS (Zaheer et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 37,
      "context" : ", 2021), BigBird-PEGASUS (Zaheer et al., 2020b), PRIMER (Xiao et al.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 34,
      "context" : ", 2020b), PRIMER (Xiao et al., 2021), LED (Beltagy et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Regarding attention, we design a new attention mechanism, which we call Transient Global (TGlobal), that mimics ETC’s local/global mechanism (Ainslie et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 163
    }, {
      "referenceID" : 39,
      "context" : "Regarding pre-training, we adopt the pretraining strategy in the PEGASUS (Zhang et al., 2019a) model.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "T5 (Raffel et al., 2019a) is a transformer based textto-text pre-trained language model that is gaining popularity for its unified framework that converts all text-based language problems into a text-to-text format, and its ease to scale up in number of parameters (from 60M to 11B parameters) with model parallelism.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "We extend the original T5 encoder with globallocal attention sparsity patterns (Ainslie et al., 2020; Zaheer et al., 2020a) to handle long inputs.",
      "startOffset" : 79,
      "endOffset" : 123
    }, {
      "referenceID" : 36,
      "context" : "We extend the original T5 encoder with globallocal attention sparsity patterns (Ainslie et al., 2020; Zaheer et al., 2020a) to handle long inputs.",
      "startOffset" : 79,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "window local attention operation following the implementation in ETC (Ainslie et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "While it is effective, recent work on masked language modeling (MLM) (Liu et al., 2019; Zhang et al., 2019b) shows that carefully selecting the prediction objective could lead to significantly better performance.",
      "startOffset" : 69,
      "endOffset" : 108
    }, {
      "referenceID" : 40,
      "context" : "While it is effective, recent work on masked language modeling (MLM) (Liu et al., 2019; Zhang et al., 2019b) shows that carefully selecting the prediction objective could lead to significantly better performance.",
      "startOffset" : 69,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "(2019a), we select top-m scored (Principle) sentences based on ROUGE-F1 score (Lin, 2004) using si = rouge(xi, D \\ {xi},∀i), where i is the sentence index, D is the collection of sentences in the document.",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "1, we pre-train LongT5 only on the C4 dataset (Raffel et al., 2019b), and we do not apply dropout during pre-training.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 19,
      "context" : "CNN / Daily Mail (Nallapati et al., 2016) News from CNN and Daily Mail are used as input and the article’s summary bullets are the target summary.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "PubMed (Cohan et al., 2018) Scientific documents were collected from PubMed, with a document’s content used as input and its corresponding abstract as the target summary.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "arXiv (Cohan et al., 2018) Similar to PubMed, but with documents taken from arXiv.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 41,
      "context" : "MediaSum (Zhu et al., 2021) Interview transcripts from CNN and NPR were used as input and their corresponding topic and overviews used as the target summary.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "Multi-News (Fabbri et al., 2019) The task involves summarizing multiple news documents about a topic into a human-written summary.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 37,
      "context" : "We compare LongT5 with various top approaches: BigBird-PEGASUS (Zaheer et al., 2020b), HATBART (Rohde et al.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : ", 2020b), HATBART (Rohde et al., 2021), DANCER PEGASUS (Gidiotis and Tsoumakas, 2020), PRIMER (Xiao et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : ", 2021), DANCER PEGASUS (Gidiotis and Tsoumakas, 2020), PRIMER (Xiao et al.",
      "startOffset" : 24,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : ", 2021), DANCER PEGASUS (Gidiotis and Tsoumakas, 2020), PRIMER (Xiao et al., 2021), TG-MultiSum (Cui and Hu, 2021), LED (Beltagy et al.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : ", 2021), TG-MultiSum (Cui and Hu, 2021), LED (Beltagy et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : ", 2021), TG-MultiSum (Cui and Hu, 2021), LED (Beltagy et al., 2020), and an application of BART by Zhu et al.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "Answers are drawn from Wikipedia and Bing web search results, excluding trivia websites (Joshi et al., 2017).",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 36,
      "context" : "For TriviaQA, we compare LongT5 with various top approaches on the leader board: BigBird-ETC (Zaheer et al., 2020a), Fusion-in-Decoder (Izacard and Grave, 2021), and ReadTwice (Zemlyanskiy et al.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : ", 2020a), Fusion-in-Decoder (Izacard and Grave, 2021), and ReadTwice (Zemlyanskiy et al.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 38,
      "context" : ", 2020a), Fusion-in-Decoder (Izacard and Grave, 2021), and ReadTwice (Zemlyanskiy et al., 2021).",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 266
    }, {
      "referenceID" : 16,
      "context" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 266
    }, {
      "referenceID" : 40,
      "context" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 266
    }, {
      "referenceID" : 22,
      "context" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 266
    }, {
      "referenceID" : 23,
      "context" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 266
    }, {
      "referenceID" : 14,
      "context" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 266
    }, {
      "referenceID" : 10,
      "context" : "Language model pre-training followed by task specific fine-tuning has proven to be a powerful tool for numerous NLP tasks (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 266
    }, {
      "referenceID" : 5,
      "context" : "BERT (Devlin et al., 2019) introduced Mask Language Model (MLM), where a model predicts masked tokens given a sequence of text input.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "BART (Lewis et al., 2020) is similar to T5 but used a slightly different pre-training objective, in which spans are masked from the input but the complete output is predicted.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 31,
      "context" : "They often use a transformer (Vaswani et al., 2017) architecture as backbone, the complexity of which is quadratic to the input length, making them impractical to model very long sequence input.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 28,
      "context" : "One type of such approaches is using non-full attention patterns to restrict the attention field range, so that it reduces the attention complexity from O(n2) to O(nlogn) or O(n), including Sinkhorn (Tay et al., 2020a), Longformer (Beltagy et al.",
      "startOffset" : 199,
      "endOffset" : 218
    }, {
      "referenceID" : 1,
      "context" : ", 2020a), Longformer (Beltagy et al., 2020), ETC (Ainslie et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : ", 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 32,
      "context" : "Another type of approaches is leveraging the low-rank approximation of the attention matrix, such as Linformer (Wang et al., 2020), Performer (Choromanski et al.",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : ", 2021), Random Feature Attention (Peng et al., 2021), and LUNA (Ma et al.",
      "startOffset" : 34,
      "endOffset" : 53
    } ],
    "year" : 0,
    "abstractText" : "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pretraining strategies from summarization pretraining (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC’s local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-ofthe-art results on several summarization and question answering tasks, as well as outperform the original T5 models on these tasks.",
    "creator" : null
  }
}
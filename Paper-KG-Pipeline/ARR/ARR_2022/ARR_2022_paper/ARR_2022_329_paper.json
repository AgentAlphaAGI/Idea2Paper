{
  "name" : "ARR_2022_329_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MUKAYESE: Turkish NLP Strikes Back",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Although some human languages, such as Turkish, are not classified as under-resourced languages, only a few research communities are working on them (Joshi et al., 2020). As a result, they are left behind in terms of developing state-of-the-art systems due to a lack of organized benchmarks and baselines. In this study, we aim to fill this gap for the Turkish language with MUKAYESE (Turkish for Benchmarking), an extensive set of datasets and benchmarks for several Turkish NLP tasks.\nHaving a sufficient amount of resources in a language lifts it from the under-resourced class, but it does not necessarily stop it from being underresearched. We survey several tasks in Turkish NLP and observe an absence of organized benchmarks and research. We demonstrate how the lack of benchmarks affects languages like Turkish and how it can keep the state of research lag behind the state-of-the-art of NLP. We accomplish this by presenting state-of-the-art baselines that outperform previous work significantly.\nIn our work on MUKAYESE, we survey seven NLP tasks in the Turkish language, and evaluate\navailable datasets in Turkish for these tasks, and describe the process of creating four new datasets for tasks that do not have public or accessible datasets. Furthermore, we provide at least two baseline models/methods per task besides evaluating existing methods. More details are enlisted in Table 1.\nOur overall contribution for Turkish NLP can be summarized as the following: (a) Set of seven organized benchmarks for NLP. (b) Four new datasets in Turkish for language modeling, sentence segmentation, and spellchecking and correction. (c) Dataset splits for fair benchmarking. (d) Several replicable baselines for each task. (e) Benchmarking state-of-the-art methods on Turkish.\nThe rest of the paper is organized as follows: We give a background on the importance of benchmarking and review similar efforts on NLP in Section 2. We explain the approach we follow for each task in Section 3. We provide dataset statistics, evaluation details, and explain the baselines for each task in 4."
    }, {
      "heading" : "2 Benchmarks and NLP",
      "text" : "Following the research on NLP over the years, it can be observed how datasets and benchmarks are very important in measuring the progress of NLP.\nFor instance, we can observe the progress of 1image credits: paperswithcode.com\nlanguage modeling for English by looking at the WikiText-103 benchmarking dataset (Merity et al., 2017); See Figure 1.\nLikewise, the SQuAD dataset (Rajpurkar et al., 2016) can be used to observe the progress of English Question Answering, and GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019) benchmarks for English Language Understanding.\nSuch progress has been enabled by the existence of benchmarks, which allowed for fair and meaningful comparison, and showed if there is a room for improvement. In addition, organized benchmarks and datasets allow for the research community to make progress with minimal amount of domain knowledge. This is especially important when it comes to languages with less number of speakers.\nThis is essential if we want to include other communities in the development of under-resourced and under-researched languages. Since research communities are more likely to contribute when such organized tasks are presented (MartínezPlumed et al., 2021)."
    }, {
      "heading" : "3 Methodology",
      "text" : "We focus in MUKAYESE on under-researched tasks of NLP in the Turkish language. We define the required elements for each benchmark as a triplet of (Datasets, Evaluation, Baseline).\nAfter defining the task and assessing its importance, we define its inputs and outputs and construct the following three key elements:\nDatasets are the first element to consider when it comes to a benchmark. We define the minimum requirements of a benchmark dataset as follows: accessible with reasonable size and quality and a shareable format.\nUnless used in a few-shot setting, benchmarks with small datasets will lack generalizability, and models trained on them might suffer from overfitting. On the other hand, training models on huge datasets might be costly and inefficient.\nAnother feature to assess is the quality of the dataset. A manually annotated dataset with a low Interannotator Agreement (IAA) rate is not suitable for benchmarking. Moreover, to build a generalizable benchmark, we need to consider using a dataset representing the general domain, e.g., sentence segmentation methods of editorial texts do not work on user-generated content such as social media posts as we show in Subsection 4.4.\nEvaluation is the second element of benchmarks. We need to define one or more metrics to evaluate and compare methodologies. We have to answer the following questions before deciding\non one metric or more for a benchmark: (a) Is what this metric measures what our task aims to do? (b) How well does it correlate with human judgment? (c) Are there any issues to consider in these metrics? (Using accuracy to measure performance on an unbalanced set does not give a representative idea of model performance).\nBaselines are the final element of benchmarking. In order to build a good performance representation of different methodologies, it is better to diversify our baselines as much as possible. For instance, we can compare pretrained vs. non-pretrained approaches, rule-based systems vs. trained systems, or unsupervised vs. supervised models."
    }, {
      "heading" : "4 Tasks",
      "text" : "We provide benchmarks in form of (dataset, evaluation, baseline) triplets for each the following NLP tasks."
    }, {
      "heading" : "4.1 Language Modeling",
      "text" : "Language modeling is a generative process, which focuses on modeling the probability P (X) of a text sequence of n tokens, where X = (x1, x2, ..., xn), and P (X) = ∏n i=1 P (xi|x<i). This type of language modeling is known as Auto-regressive (AR) or causal language modeling. The main objective of the model is to learn to estimate the probability of a given text sequence or a corpus. In our work, we focus on neural approaches for this task (Bengio et al., 2003), where we present two new benchmarking datasets for AR language modeling, and report the results of two different baseline models.\nDatasets We present two different datasets for AR modeling, namely TRNEWS-64 and TRWIKI67, along with their train/validation/test splits (See Table 2). These datasets are presented in a similar fashion to enwik8 (Hutter, 2006) and WikiText (Merity et al., 2017) English datasets.\nTRWIKI-67 is a language modeling dataset that contains 67 million words of raw Turkish Wikipedia articles. We extracted this dataset from a recent Turkish Wikipedia dump2 using WikiExtractor (Attardi, 2015). Additionally, further preprocessing was applied to get rid of the redundant text. Only the articles’ raw text and titles were kept and presented in their cased format (with no\n2https://dumps.wikimedia.org/trwiki/20210720/: accessed on 20 July 2021.\nupper/lower case transformations). For the tokenization process, we train a sentencepiece unigram model (Kudo, 2018) with a vocabulary size of 32K, only using the training split of the dataset. Although we advise using the tokenized version of this dataset to encourage reproducibility, we provide a raw version of this dataset that can be utilized as a benchmark for language modeling tasks on character, subword, or word level.\nTRNEWS-64 is a character language modeling dataset that contain 64 million words of news columns and articles that was retrieved from TS Timeline Corpus (Sezer, 2017). It can be utilized as a benchmark for modeling long-range dependencies in the Turkish language, as it contains relatively long documents (See Table 2). This dataset consists of a mix of news articles collected from different journals about various domains and topics. Since trnews-64 is intended for language modeling on character level, articles were lightly preprocessed, and no further tokenization was applied.\nEvaluation Language models are trained on minimizing the negative log-likelihood (NLL) of the training set, and their performance is measured based on how well they can generalize on the test set:\nNLL(Xtest) = − 1\nn n∑ i=1 log pθ(xi|xtest< i) (1)\nWord or sub-word level language models are evaluated using the word perplexity (PPL) metric, a derivative of NLL. On the other hand, character language models are evaluated using entropy-based Bits-per-character (BPC) metric, which is also an-\nother derivative of NLL (Huyen, 2019). We consider PPL for the evaluation of models on TRWIKI67, and BPC for TRNEWS-64. Note that lower is better for both metrics.\nWe note that PPL needs to be computed with the same count of tokens, otherwise it needs to be normalized in case different tokenization methods are preferred. If not normalized, the metrics would not be comparable (Shoeybi et al., 2019).\nBaselines We consider two baseline models of different families. The first one is Single Headed Attention - RNN (SHA-RNN) (Merity, 2019), which is a Recurrent Neural Networkbased language model, and the second is Adaptive Transformer (ADAP.TRANS) (Sukhbaatar et al., 2019), which is based on Transformer architecture (Vaswani et al., 2017). In Table 3, we provide the results of these models, which we train separately on TRWIKI-67 and TRNEWS-64 datasets. It is notable that unlike the case for the English language (Merity, 2019), SHA-RNN performed better than Adaptive Transformer for both of the presented Turkish datasets."
    }, {
      "heading" : "4.2 Machine Translation",
      "text" : "Machine translation is the problem of translating a piece of text from one language to another. Over the years, neural machine translation models have become dominant, especially in low resource settings, benefiting from transfer learning (Zoph et al., 2016). In this work, we focus on evaluating neural machine translation models for translation between English and Turkish languages. We provide the results of three different baselines on two datasets.\nDatasets The first dataset we evaluate is the Turkish-English subset of WMT16. This dataset was presented at the first Conference of Machine Translation (WMT)3, it consists of manually translated Turkish-English sentence pairs. The second\n3http://www.statmt.org/wmt16/\none is the Turkish-English subset of Multilingual Speech Translation Corpus (MUST-C) (Di Gangi et al., 2019). This corpus was extracted from movies and TV shows subtitles. We present the statistics of both datasets in Table 4.\n#Sentences #Words\nTurkish MUST-C 236K / 1.3K / 2K 3.4M / 19K / 33K WMT-16 205K / 1K / 3K 3.6M / 14K / 44K\nEnglish MUST-C 236K / 1K/ 2K 4.6M / 26K / 45K WMT-16 205K / 1K / 3K 4.4M / 19K / 58K\nTable 4: Results of machine translation baselines. Each cell represents the (Train / Validation / Test) values of the datasets in the corresponding row. WMT-16 and MUST-C refer to Turkish-English subsets.\nEvaluation We evaluate our models on the relevant test sets for translation in both directions. We utilize BLEU Score (Papineni et al., 2002) for the assessment of translation quality.\nBaselines In this task, we train three different models. First, we train a TRANSFORMER (Vaswani et al., 2017) with the same settings for the encoder and the decoder parts. Where we use 6 layers, with 4 attention heads each, and hidden size of 512. Second, we utilize the Convolutional sequence-to-sequence CONVS2S model (Gehring et al., 2017) following the same settings. The last model is mBART 50 (Tang et al., 2020), a multilingual model pre-trained on 50 different languages, which we fine-tune for each dataset separately.\nIn Table 5 we present BLEU score of the models on each translation dataset in both directions. The benefit of pre-training can be seen in the case of MBART50, where it outperforms the counterparts that we train from scratch."
    }, {
      "heading" : "4.3 Named-Entity Recognition (NER)",
      "text" : "We include the Named-Entity Recognition (NER) task in our set of benchmarks, as it has an essential role in NLP applications. In this task, words representing named-entities are detected in the text input and assigned one of the predefined namedentity classes such as Person or Location (Chinchor and Robinson, 1998). We benchmark three different models on two NER datasets for Turkish and compare our work with previous work.\nDatasets The first dataset we use is MILLIYETNER (Tür et al., 2003), which is a set of manually, annotated news articles from the Turkish Milliyet news resource4. The second is the Turkish subset of the semi-automatically annotated Cross-lingual NER dataset WIKIANN or (PAN-X) (Pan et al., 2017), which consists of Turkish Wikipedia articles. Both datasets have three entity classes as shown in Table 6.\nEvaluation NER systems are evaluated based on their capability of finding named entities with their correct boundaries and classes. Following previous work on Turkish NER (Yeniterzi, 2011; Şeker and Eryiğit, 2012), we report the CoNLL Fmeasure metric (Tjong Kim Sang, 2002) to assess our NER baselines.\nBaselines We train three different baseline models for this task. One with no pre-trained embeddings, which utilizes bi-directional Long Short Term Memory with Conditional Random Fields (BILSTM-CRF) (Panchendrarajan and Amaresan, 2018). The remaining two models employ pretrained representations from BERT (Devlin et al.,\n4https://www.milliyet.com.tr/\n2019). In one of the models, we investigate the benefit of adding a CRF layer on top of BERT. As for the pre-trained BERT model, we use BERTURK base, which is pre-trained on a large Turkish corpus (Schweter, 2020).\nIn Table 7, we provide the evaluation results (CoNLL F-measure) for the three baselines on both datasets’ test sets. Additionally, we compare our results with previous work of (Yeniterzi, 2011; Şeker and Eryiğit, 2012; Güngör et al., 2018) on MILLIYET-NER dataset."
    }, {
      "heading" : "4.4 Sentence Segmentation",
      "text" : "Sentence segmentation is the task of detecting sentence boundaries in a given article. Despite its fundamental place in the NLP pipelines, sentence segmentation attracts little interest. Common approaches are rule-based systems that rely on cues such as punctuation marks and capital letters (Jurafsky and Martin, 2018).\nDatasets We present TRSEG-41, a new sentence segmentation dataset for Turkish. This dataset consists of 300 scientific abstracts from (Özturk et al., 2014), 300 curated news articles from TRNEWS-64, and a set of 10K tweets. For the scientific abstracts, our sampling rationale is to maximize the number of abbreviations that reduce the accuracy of the rule-based approaches. As for the news set, we maximize the length of documents and the number of proper name. In the Twitter dataset, we balance the number of multi/single sentence tweets, and preprocess the tweets by replacing all URLs with http://some.url, and all user mentions with @user.\nWe manually annotate the sentence boundaries of these articles and present two dataset splits, one for training and development and one for testing and benchmarking. The statistics of the splits can be found in Table 8.\nApplying sentence segmentation to usergenerated content such as social media posts or\ncomments can be quite challenging. To simulate such difficult cases and expose the weaknesses of rule-based methods, we create another version of TRSEG-41 where we artificially corrupt the boundaries of sentences. This is done by randomly converting them to lowercase or uppercase with 50% probability, or by removing all punctuation marks with 50% probability.\nEvaluation Our evaluation procedure is based on the metrics F1 score, Precision, Recall for each segment. Unlike (Wicks and Post, 2021), we evaluate our models with the entire test set, without removing sentences with ambiguous boundaries. Furthermore, in order to highlight the gap in performance, we cross-evaluate our systems on the original and corrupted set.\nF1-SCORE PRECISION RECALL\nSPACY 0.74 / 0.37 0.76 / 0.48 0.72 / 0.30\nBaselines For this task, we employ three methods as baseline models. ERSATZ, a context-based approach that relies on supervised training (Wicks and Post, 2021), the unsupervised PUNKT tokenizer (Kiss and Strunk, 2006), and SPACY Sentencizer tool (Montani et al., 2021). While ERSATZ utilizes Transformer (Vaswani et al., 2017) architecture, spaCy Sentencizer is a rule-based sentence boundary detector, whereas Punkt Tokenizer relies on an unsupervised training approach.\nWe experiment with these models on four different training and testing set combinations, where we train using the original and corrupted training sets\nseparately and test on both test sets. Results are presented in Table 9.In all settings, SPACY SENTENCIZER is outperformed by its trained counterparts. Among the baselines, ERSATZ performed the best. Our experiments show that deep learning models are more robust to corruption in the data."
    }, {
      "heading" : "4.5 Spellchecking and Correction",
      "text" : "Spellcheckers are among the most widely used NLP tools. The basic task is to check for misspellings in an input and suggest a set of corrections. Different methods can be employed for error correction, such as looking up words that minimize the edit distance from a dictionary or utilizing probabilistic models with N-grams to suggest the most likely correct word based on the context (Jurafsky and Martin, 2018). In this work, we focus on contextless (single word) spellchecking and correction. We present a new benchmarking dataset for this type of spellcheckers and an efficient dictionary for Turkish.\nDatasets We present TRSPELL-10, a dataset of 10K words, for benchmarking spellchecking and correction. The dataset consists of tuples of input and correct (gold) words.\nTo create this dataset, we randomly sample 8500 Turkish words from the TS Corpus Word List (Sezer, 2013, 2017). We create artificial misspellings by applying random insertions, deletions, and substitutions on 65% of the words, where we apply at most two operations on the same word. The remaining 35% of the words are unchanged. Moreover, we add 1K random foreign words, and 500 randomly generated word-like character sequences.\nAs a quality check of these artificial misspellings, given a list of corrupted words, we ask our annotators to provide us a list of suggestions up to 10 suggestions per word. Their suggestion lists had the gold output 91% of the time.\nEvaluation We evaluate spellcheckers’ ability to detect misspellings using the macro-averaged F1-Score metric. Additionally, we evaluate their spell correction accuracy (SCA) based on the suggestions provided for misspelled words.\nBaselines We take advantage of the agglutinative nature of the Turkish language by developing a Hunspell-based (Trón et al., 2005) dictionary for Turkish. Using a list of 4M words we filter from\nWeb crawls and Turkish corpora, we optimize the splits that minimize the size of the root dictionary and the affix list.\nWe compare this dictionary to hunspell-tr (Zafer, 2017), another Hunspell-based Turkish dictionary, and to Zemberek spellchecker (Akın and Akın, 2007), which is designed based on morphological features of the Turkish language. As shown in Table 10, our dictionary surpasses other baselines in terms of error detection. However, Zemberek’s correction accuracy is higher compared to the Hunspell based methods."
    }, {
      "heading" : "4.6 Summarization",
      "text" : "Abstractive text summarization is the task of generating a short description (summary) of an article (longer text). Formally, given a sequence of tokens (input article) X = (x1, x2, ..., xn) and its summary Y = (y1, y2, ..., ym), the main task is to model the conditional probability: P (Y |X) =∏m\ni=1 P (yi|y<i, X). For this task, we work on the Multi-lingual Summarization (MLSUM) dataset (Scialom et al., 2020) and present state-of-the-art summarization results for Turkish.\nDatasets MLSUM is a multi-lingual dataset for abstractive summarization. This dataset consists of a large set of crawled news articles with their abstracts in multiple languages. We focus on the Turkish subset of MLSUM.\nWe found 4378 duplicated instances and 12 overlapping instances among the splits while assessing\nthe dataset’s quality. We remove these instances from the dataset for a more accurate evaluation and evaluate our models on both the original and the cleaned sets. In Table 11, we provide some statistics about both sets, before and after the deduplication.\nEvaluation To assess the quality of the generated summaries, we use the N-gram co-occurrencebased ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) metrics. We report two different results for each model, one on the original test set and another on the cleaned set.\nBaselines As a baseline model for summarization, we present TRBART, a Seq2Seq Transformer (Vaswani et al., 2017) trained following the configuration of BART Base (Lewis et al., 2020), which is a state-of-the-art model for abstractive summarization in English.\nMoreover, we fine-tune two different pre-trained models. The first model is Multilingual BART (MBART50) (Tang et al., 2020), which is pretrained on data from 50 different languages. The second model is Multilingual Text to Text Transformer (MT5-BASE) (Xue et al., 2021). As shown in Table 12, all models performs better than the best proposed baseline model (Scialom et al., 2020), which follows UniLM architecture (Dong et al., 2019)."
    }, {
      "heading" : "4.7 Text Classification",
      "text" : "Text classification can be utilized in several applications such sentiment analysis or topic identification. In this task we take a sequence of text as an input, and output a probability distribution over arbitrary number of classes. In our work on Turkish we benchmark three models on two datasets from different domains.\nDatasets We work on the news categorization (NEWS-CAT) dataset (Amasyalı and Yıldırım, 2004). In this dataset, news articles are labeled\nwith one of the following five categories health, sports, economy, politics, magazine. There is no splits provided in the original work for NEW-CAT dataset. Hence we construct our own splits.\nThe second dataset is the corpus of Offensive Speech Identification in Social media (OFFENSEVAL) (Çöltekin, 2020). This dataset was collected from Twitter, where the tweets are annotated for offensive speech with offensive, or non-offensive labels. We choose these datasets for benchmarking since they vary in domain, average article length.\nEvaluation We evaluate our baseline models using the F1 score. We use the macro averaged variant to account for the imbalance in classes within the datasets.\nBaselines We measure the performance of three deep learning models—one with pre-training and two with none. The pre-trained model is the BERT (Devlin et al., 2019) based Turkish pretrained (BERTURK) model (Schweter, 2020). The remaining two models employ randomly initialized embeddings of size 256. In one of them we use two layers of Bidirectional LSTM (BILSTM) (Hochreiter and Schmidhuber, 1997) with a hidden size of 256. In the other model (CNN-TEXT), we use Convolutional Neural Networks for Sentence Classification (Kim, 2014) with 32 filters instead of 2.\nLooking at F1 scores of the models In Table 14, we can observe the clear advantage of the pre-trained BERTURK model over BILSTM and CNN-TEXT."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We believe that while some human languages such as Turkish do not fall under the definition of underresourced languages, they are shown little interest as a result of the lack of organized benchmarks and baselines. To address this problem, we presented MUKAYESE, a comprehensive set of benchmarks along with corresponding baselines for seven different tasks: Language Modeling, Machine Translation, Named Entity Recognition, Sentence Segmentation, Spell Checking and Correction, Summarization, and Text Classification, as well as four new benchmarking datasets in Turkish for Language Modeling, Sentence Segmentation, and Spell Checking and Correction. For future work, the same methodology can be followed to include more NLP tasks such as Dependency Parsing, Morphological Analysis and other tasks.\nWe hope that MUKAYESE sets an example and leads to an increase in efforts on under-researched languages."
    } ],
    "references" : [ {
      "title" : "Zemberek, an open source nlp framework for turkic languages",
      "author" : [ "Ahmet Afsin Akın", "Mehmet Dündar Akın." ],
      "venue" : "10:1–5.",
      "citeRegEx" : "Akın and Akın.,? 2007",
      "shortCiteRegEx" : "Akın and Akın.",
      "year" : 2007
    }, {
      "title" : "Otomatik haber metinleri sınıflandırma",
      "author" : [ "MF Amasyalı", "T Yıldırım." ],
      "venue" : "2004 12th Signal Processing and Communications Applications Conference (SIU), pages 224–226. IEEE.",
      "citeRegEx" : "Amasyalı and Yıldırım.,? 2004",
      "shortCiteRegEx" : "Amasyalı and Yıldırım.",
      "year" : 2004
    }, {
      "title" : "Wikiextractor",
      "author" : [ "Giusepppe Attardi." ],
      "venue" : "https:// github.com/attardi/wikiextractor.",
      "citeRegEx" : "Attardi.,? 2015",
      "shortCiteRegEx" : "Attardi.",
      "year" : 2015
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "J. Mach. Learn. Res., 3(null):1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Appendix E: MUC7 named entity task definition (version 3.5)",
      "author" : [ "N. Chinchor", "P. Robinson" ],
      "venue" : "In Seventh Message Understanding Conference",
      "citeRegEx" : "Chinchor and Robinson.,? \\Q1998\\E",
      "shortCiteRegEx" : "Chinchor and Robinson.",
      "year" : 1998
    }, {
      "title" : "A corpus of Turkish offensive language on social media",
      "author" : [ "Çağrı Çöltekin." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 6174–6184, Marseille, France. European Language Resources Association.",
      "citeRegEx" : "Çöltekin.,? 2020",
      "shortCiteRegEx" : "Çöltekin.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "One-to-many multilingual end-to-end speech translation",
      "author" : [ "Mattia A Di Gangi", "Matteo Negri", "Marco Turchi." ],
      "venue" : "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 585–592. IEEE.",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Information Pro-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, page 1243–1252. JMLR.org.",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Recurrent neural networks for turkish named entity recognition",
      "author" : [ "Onur Güngör", "Suzan Üsküdarlı", "Tunga Güngör." ],
      "venue" : "2018 26th Signal Processing and Communications Applications Conference (SIU), pages 1–4.",
      "citeRegEx" : "Güngör et al\\.,? 2018",
      "shortCiteRegEx" : "Güngör et al\\.",
      "year" : 2018
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "The human knowledge compression prize",
      "author" : [ "Marcus Hutter" ],
      "venue" : null,
      "citeRegEx" : "Hutter.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2006
    }, {
      "title" : "Evaluation metrics for language modeling",
      "author" : [ "Chip Huyen." ],
      "venue" : "The Gradient.",
      "citeRegEx" : "Huyen.,? 2019",
      "shortCiteRegEx" : "Huyen.",
      "year" : 2019
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Speech and language processing (draft)",
      "author" : [ "Daniel Jurafsky", "James H Martin." ],
      "venue" : "https://web. stanford. edu/ ̃ jurafsky/slp3.",
      "citeRegEx" : "Jurafsky and Martin.,? 2018",
      "shortCiteRegEx" : "Jurafsky and Martin.",
      "year" : 2018
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Linguis-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Unsupervised multilingual sentence boundary detection",
      "author" : [ "Tibor Kiss", "Jan Strunk." ],
      "venue" : "Computational Linguistics, 32(4):485–525.",
      "citeRegEx" : "Kiss and Strunk.,? 2006",
      "shortCiteRegEx" : "Kiss and Strunk.",
      "year" : 2006
    }, {
      "title" : "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "author" : [ "Taku Kudo." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66–75,",
      "citeRegEx" : "Kudo.,? 2018",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2018
    }, {
      "title" : "BART: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Research community dynamics behind popular ai benchmarks",
      "author" : [ "Fernando Martínez-Plumed", "Pablo Barredo", "Seán Ó hÉigeartaigh", "José Hernández-Orallo." ],
      "venue" : "Nature Machine Intelligence, 3(7):581–589.",
      "citeRegEx" : "Martínez.Plumed et al\\.,? 2021",
      "shortCiteRegEx" : "Martínez.Plumed et al\\.",
      "year" : 2021
    }, {
      "title" : "Single headed attention rnn: Stop thinking with your head",
      "author" : [ "Stephen Merity." ],
      "venue" : "Computing Research Repository, arXiv:1911.11423. Version 2.",
      "citeRegEx" : "Merity.,? 2019",
      "shortCiteRegEx" : "Merity.",
      "year" : 2019
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Merity et al\\.,? 2017",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2017
    }, {
      "title" : "Mazaev, and GregDubbin",
      "author" : [ "Bot", "Leander Fiedler", "Grégory Howard", "Wannaphong Phatthiyaphaibun", "Yohei Tamura", "Sam Bozek", "murat", "Mark Amery", "Björn Böing", "Pradeep Kumar Tippa", "Leif Uwe Vogelsang", "Bram Vanroy", "Ramanan Balakrishnan", "Vadim" ],
      "venue" : null,
      "citeRegEx" : "Bot et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bot et al\\.",
      "year" : 2021
    }, {
      "title" : "Turkish labeled text corpus",
      "author" : [ "Seçil Özturk", "Bülent Sankur", "Tunga Gungör", "Mustafa Berkay Yilmaz", "Bilge Köroǧlu", "Onur Aǧin", "Mustafa İşbilen", "Çaǧdaş Ulaş", "Mehmet Ahat." ],
      "venue" : "2014 9",
      "citeRegEx" : "Özturk et al\\.,? 2014",
      "shortCiteRegEx" : "Özturk et al\\.",
      "year" : 2014
    }, {
      "title" : "Cross-lingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Bidirectional LSTM-CRF for named entity recognition",
      "author" : [ "Rrubaa Panchendrarajan", "Aravindh Amaresan." ],
      "venue" : "Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation, Hong Kong. Association for Computational Linguis-",
      "citeRegEx" : "Panchendrarajan and Amaresan.,? 2018",
      "shortCiteRegEx" : "Panchendrarajan and Amaresan.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Berturk - bert models for turkish",
      "author" : [ "Stefan Schweter" ],
      "venue" : null,
      "citeRegEx" : "Schweter.,? \\Q2020\\E",
      "shortCiteRegEx" : "Schweter.",
      "year" : 2020
    }, {
      "title" : "MLSUM: The multilingual summarization corpus",
      "author" : [ "Thomas Scialom", "Paul-Alexis Dray", "Sylvain Lamprier", "Benjamin Piwowarski", "Jacopo Staiano." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Scialom et al\\.,? 2020",
      "shortCiteRegEx" : "Scialom et al\\.",
      "year" : 2020
    }, {
      "title" : "Initial explorations on using CRFs for Turkish named entity recognition",
      "author" : [ "Gökhan Akın Şeker", "Gülşen Eryiğit." ],
      "venue" : "Proceedings of COLING 2012, pages 2459–2474, Mumbai, India. The COLING 2012 Organizing Committee.",
      "citeRegEx" : "Şeker and Eryiğit.,? 2012",
      "shortCiteRegEx" : "Şeker and Eryiğit.",
      "year" : 2012
    }, {
      "title" : "Ts corpus: Herkes İçin türkçe derlem",
      "author" : [ "Taner Sezer." ],
      "venue" : "Proceedings 27th National Linguistics Conference, pages 217–225.",
      "citeRegEx" : "Sezer.,? 2013",
      "shortCiteRegEx" : "Sezer.",
      "year" : 2013
    }, {
      "title" : "Ts corpus project: An online turkish dictionary and ts diy corpus",
      "author" : [ "Taner Sezer." ],
      "venue" : "European Journal of Language and Literature, 9(1):18–24.",
      "citeRegEx" : "Sezer.,? 2017",
      "shortCiteRegEx" : "Sezer.",
      "year" : 2017
    }, {
      "title" : "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "author" : [ "Mohammad Shoeybi", "Mostofa Patwary", "Raul Puri", "Patrick LeGresley", "Jared Casper", "Bryan Catanzaro." ],
      "venue" : "Computing Research Repository, arXiv:1909.08053.",
      "citeRegEx" : "Shoeybi et al\\.,? 2019",
      "shortCiteRegEx" : "Shoeybi et al\\.",
      "year" : 2019
    }, {
      "title" : "Adaptive attention span in transformers",
      "author" : [ "Sainbayar Sukhbaatar", "Edouard Grave", "Piotr Bojanowski", "Armand Joulin." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335, Florence, Italy.",
      "citeRegEx" : "Sukhbaatar et al\\.,? 2019",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual translation with extensible multilingual pretraining and finetuning",
      "author" : [ "Yuqing Tang", "Chau Tran", "Xian Li", "Peng-Jen Chen", "Naman Goyal", "Vishrav Chaudhary", "Jiatao Gu", "Angela Fan." ],
      "venue" : "Computing Research Repository, arXiv:2008.00401.",
      "citeRegEx" : "Tang et al\\.,? 2020",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Hunmorph: Open source word analysis",
      "author" : [ "Viktor Trón", "Gyögy Gyepesi", "Péter Halácsky", "András Kornai", "László Németh", "Dániel Varga." ],
      "venue" : "Proceedings of Workshop on Software, pages 77–85, Ann Arbor, Michigan. Association for Computational Lin-",
      "citeRegEx" : "Trón et al\\.,? 2005",
      "shortCiteRegEx" : "Trón et al\\.",
      "year" : 2005
    }, {
      "title" : "A statistical information extraction system for turkish",
      "author" : [ "Gökhan Tür", "Dilek Hakkani-TüR", "Kemal Oflazer." ],
      "venue" : "Natural Language Engineering, 9(2):181–210.",
      "citeRegEx" : "Tür et al\\.,? 2003",
      "shortCiteRegEx" : "Tür et al\\.",
      "year" : 2003
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, page 6000–6010.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Advances in Neural Information",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "A unified approach to sentence segmentation of punctuated text in many languages",
      "author" : [ "Rachel Wicks", "Matt Post." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
      "citeRegEx" : "Wicks and Post.,? 2021",
      "shortCiteRegEx" : "Wicks and Post.",
      "year" : 2021
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483–498, On-",
      "citeRegEx" : "Raffel.,? 2021",
      "shortCiteRegEx" : "Raffel.",
      "year" : 2021
    }, {
      "title" : "Exploiting morphology in Turkish named entity recognition system",
      "author" : [ "Reyyan Yeniterzi." ],
      "venue" : "Proceedings of the ACL 2011 Student Session, pages 105–110, Portland, OR, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Yeniterzi.,? 2011",
      "shortCiteRegEx" : "Yeniterzi.",
      "year" : 2011
    }, {
      "title" : "hunspell-tr",
      "author" : [ "Harun Reşit Zafer." ],
      "venue" : "https:// github.com/hrzafer/hunspell-tr.",
      "citeRegEx" : "Zafer.,? 2017",
      "shortCiteRegEx" : "Zafer.",
      "year" : 2017
    }, {
      "title" : "Transfer learning for low-resource neural machine translation",
      "author" : [ "Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1568–1575, Austin, Texas.",
      "citeRegEx" : "Zoph et al\\.,? 2016",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Although some human languages, such as Turkish, are not classified as under-resourced languages, only a few research communities are working on them (Joshi et al., 2020).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 24,
      "context" : "language modeling for English by looking at the WikiText-103 benchmarking dataset (Merity et al., 2017); See Figure 1.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 30,
      "context" : "Likewise, the SQuAD dataset (Rajpurkar et al., 2016) can be used to observe the progress of English Question Answering, and GLUE (Wang et al.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 44,
      "context" : ", 2016) can be used to observe the progress of English Question Answering, and GLUE (Wang et al., 2018), SuperGLUE (Wang et al.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 43,
      "context" : ", 2018), SuperGLUE (Wang et al., 2019) benchmarks for English Language Understanding.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "In our work, we focus on neural approaches for this task (Bengio et al., 2003), where we present two new benchmarking datasets for AR language modeling, and report the results of two different baseline models.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "These datasets are presented in a similar fashion to enwik8 (Hutter, 2006) and WikiText (Merity et al.",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "These datasets are presented in a similar fashion to enwik8 (Hutter, 2006) and WikiText (Merity et al., 2017) English datasets.",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "We extracted this dataset from a recent Turkish Wikipedia dump2 using WikiExtractor (Attardi, 2015).",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "For the tokenization process, we train a sentencepiece unigram model (Kudo, 2018) with a vocabulary size of 32K, only using the training split of the dataset.",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 35,
      "context" : "TRNEWS-64 is a character language modeling dataset that contain 64 million words of news columns and articles that was retrieved from TS Timeline Corpus (Sezer, 2017).",
      "startOffset" : 153,
      "endOffset" : 166
    }, {
      "referenceID" : 36,
      "context" : "If not normalized, the metrics would not be comparable (Shoeybi et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 23,
      "context" : "The first one is Single Headed Attention - RNN (SHA-RNN) (Merity, 2019), which is a Recurrent Neural Networkbased language model, and the second is Adaptive Transformer (ADAP.",
      "startOffset" : 57,
      "endOffset" : 71
    }, {
      "referenceID" : 37,
      "context" : "TRANS) (Sukhbaatar et al., 2019), which is based on Transformer architecture (Vaswani et al.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 42,
      "context" : ", 2019), which is based on Transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "It is notable that unlike the case for the English language (Merity, 2019), SHA-RNN performed better than Adaptive Transformer for both of the presented Turkish datasets.",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 49,
      "context" : "Over the years, neural machine translation models have become dominant, especially in low resource settings, benefiting from transfer learning (Zoph et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 162
    }, {
      "referenceID" : 29,
      "context" : "We utilize BLEU Score (Papineni et al., 2002) for the assessment of translation quality.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 42,
      "context" : "First, we train a TRANSFORMER (Vaswani et al., 2017) with the same settings for the encoder and the decoder parts.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Second, we utilize the Convolutional sequence-to-sequence CONVS2S model (Gehring et al., 2017) following the same settings.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 38,
      "context" : "The last model is mBART 50 (Tang et al., 2020), a multilingual model pre-trained on 50 different languages, which we fine-tune for each dataset separately.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "In this task, words representing named-entities are detected in the text input and assigned one of the predefined namedentity classes such as Person or Location (Chinchor and Robinson, 1998).",
      "startOffset" : 161,
      "endOffset" : 190
    }, {
      "referenceID" : 41,
      "context" : "Datasets The first dataset we use is MILLIYETNER (Tür et al., 2003), which is a set of manually, annotated news articles from the Turkish Milliyet news resource4.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "The second is the Turkish subset of the semi-automatically annotated Cross-lingual NER dataset WIKIANN or (PAN-X) (Pan et al., 2017), which consists of Turkish Wikipedia articles.",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 47,
      "context" : "Following previous work on Turkish NER (Yeniterzi, 2011; Şeker and Eryiğit, 2012), we report the CoNLL Fmeasure metric (Tjong Kim Sang, 2002) to assess our NER baselines.",
      "startOffset" : 39,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : "Following previous work on Turkish NER (Yeniterzi, 2011; Şeker and Eryiğit, 2012), we report the CoNLL Fmeasure metric (Tjong Kim Sang, 2002) to assess our NER baselines.",
      "startOffset" : 39,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : "One with no pre-trained embeddings, which utilizes bi-directional Long Short Term Memory with Conditional Random Fields (BILSTM-CRF) (Panchendrarajan and Amaresan, 2018).",
      "startOffset" : 133,
      "endOffset" : 169
    }, {
      "referenceID" : 31,
      "context" : "As for the pre-trained BERT model, we use BERTURK base, which is pre-trained on a large Turkish corpus (Schweter, 2020).",
      "startOffset" : 103,
      "endOffset" : 119
    }, {
      "referenceID" : 47,
      "context" : "Additionally, we compare our results with previous work of (Yeniterzi, 2011; Şeker and Eryiğit, 2012; Güngör et al., 2018) on MILLIYET-NER dataset.",
      "startOffset" : 59,
      "endOffset" : 122
    }, {
      "referenceID" : 33,
      "context" : "Additionally, we compare our results with previous work of (Yeniterzi, 2011; Şeker and Eryiğit, 2012; Güngör et al., 2018) on MILLIYET-NER dataset.",
      "startOffset" : 59,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "Additionally, we compare our results with previous work of (Yeniterzi, 2011; Şeker and Eryiğit, 2012; Güngör et al., 2018) on MILLIYET-NER dataset.",
      "startOffset" : 59,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "Common approaches are rule-based systems that rely on cues such as punctuation marks and capital letters (Jurafsky and Martin, 2018).",
      "startOffset" : 105,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "This dataset consists of 300 scientific abstracts from (Özturk et al., 2014), 300 curated news articles from TRNEWS-64, and a set of 10K tweets.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 45,
      "context" : "Unlike (Wicks and Post, 2021), we evaluate our models with the entire test set, without removing sentences with ambiguous boundaries.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 45,
      "context" : "ERSATZ, a context-based approach that relies on supervised training (Wicks and Post, 2021), the unsupervised PUNKT tokenizer (Kiss and Strunk, 2006), and SPACY Sentencizer tool (Montani et al.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "ERSATZ, a context-based approach that relies on supervised training (Wicks and Post, 2021), the unsupervised PUNKT tokenizer (Kiss and Strunk, 2006), and SPACY Sentencizer tool (Montani et al.",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 42,
      "context" : "While ERSATZ utilizes Transformer (Vaswani et al., 2017) architecture, spaCy Sentencizer is a rule-based sentence boundary detector, whereas Punkt Tokenizer relies on an unsupervised training approach.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 16,
      "context" : "Different methods can be employed for error correction, such as looking up words that minimize the edit distance from a dictionary or utilizing probabilistic models with N-grams to suggest the most likely correct word based on the context (Jurafsky and Martin, 2018).",
      "startOffset" : 239,
      "endOffset" : 266
    }, {
      "referenceID" : 40,
      "context" : "Baselines We take advantage of the agglutinative nature of the Turkish language by developing a Hunspell-based (Trón et al., 2005) dictionary for Turkish.",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 48,
      "context" : "We compare this dictionary to hunspell-tr (Zafer, 2017), another Hunspell-based Turkish dictionary, and to Zemberek spellchecker (Akın and Akın, 2007), which is designed based on morphological features of the Turkish language.",
      "startOffset" : 42,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "We compare this dictionary to hunspell-tr (Zafer, 2017), another Hunspell-based Turkish dictionary, and to Zemberek spellchecker (Akın and Akın, 2007), which is designed based on morphological features of the Turkish language.",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 32,
      "context" : "For this task, we work on the Multi-lingual Summarization (MLSUM) dataset (Scialom et al., 2020) and present state-of-the-art summarization results for Turkish.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "Evaluation To assess the quality of the generated summaries, we use the N-gram co-occurrencebased ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) metrics.",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "Evaluation To assess the quality of the generated summaries, we use the N-gram co-occurrencebased ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) metrics.",
      "startOffset" : 129,
      "endOffset" : 155
    }, {
      "referenceID" : 42,
      "context" : "Baselines As a baseline model for summarization, we present TRBART, a Seq2Seq Transformer (Vaswani et al., 2017) trained following the configuration of BART Base (Lewis et al.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 20,
      "context" : ", 2017) trained following the configuration of BART Base (Lewis et al., 2020), which is a state-of-the-art model for abstractive summarization in English.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 38,
      "context" : "The first model is Multilingual BART (MBART50) (Tang et al., 2020), which is pretrained on data from 50 different languages.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 32,
      "context" : "As shown in Table 12, all models performs better than the best proposed baseline model (Scialom et al., 2020), which follows UniLM architecture (Dong et al.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : ", 2020), which follows UniLM architecture (Dong et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "Datasets We work on the news categorization (NEWS-CAT) dataset (Amasyalı and Yıldırım, 2004).",
      "startOffset" : 63,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "The second dataset is the corpus of Offensive Speech Identification in Social media (OFFENSEVAL) (Çöltekin, 2020).",
      "startOffset" : 97,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "The pre-trained model is the BERT (Devlin et al., 2019) based Turkish pretrained (BERTURK) model (Schweter, 2020).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 31,
      "context" : ", 2019) based Turkish pretrained (BERTURK) model (Schweter, 2020).",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "In one of them we use two layers of Bidirectional LSTM (BILSTM) (Hochreiter and Schmidhuber, 1997) with a hidden size of 256.",
      "startOffset" : 64,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "In the other model (CNN-TEXT), we use Convolutional Neural Networks for Sentence Classification (Kim, 2014) with 32 filters instead of 2.",
      "startOffset" : 96,
      "endOffset" : 107
    } ],
    "year" : 0,
    "abstractText" : "Having sufficient resources for a language X lifts it from the under-resourced languages class, but does not necessarily lift it from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the State-of-the-Art in NLP applications. As a solution, we present MUKAYESE, a set of NLP benchmarks for the Turkish language that contains several NLP tasks. For each benchmark, we work on one or more datasets and present two or more baselines. Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spellchecking and correction.",
    "creator" : null
  }
}
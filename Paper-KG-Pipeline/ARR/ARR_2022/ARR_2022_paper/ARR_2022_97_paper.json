{
  "name" : "ARR_2022_97_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hierarchical Recurrent Aggregative Generation for Few-Shot NLG",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The recent development of large pretrained language models (PLMs; i.e. BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020)) has caused a shift of interest in the research community towards domain adaptation and transfer learning. For the task of concept-totext natural language generation (NLG), wherein the aim is to generate a natural language text that describes the semantic content of an abstract structured machine-readable input (Meaning Representation; MR), transfer learning from PLMs has become a popular and high performing approach with 13 out of the 15 participating teams in the latest\n1Code will be made public on acceptance of the paper.\nWebNLG+ Shared Task (Ferreira et al., 2020) employing a fine-tuned pretrained model as their main submitted system. Specifically, T5-based systems achieved a human evaluation ranking on par with the ground truth in terms of fluency and adequacy. Transfer learning from PLMs also enables training on few-shot and zero-shot settings, i.e. when sufficient in-domain data are unavailable. Prominent and relevant examples include machine translation (Zoph et al., 2016; Brown et al., 2020) and NLG for task-oriented dialogues (Peng et al., 2020).\nThis paper focuses on concept-to-text NLG, where recent machine learning and in extension transfer learning approaches adopt an end-to-end architecture (Peng et al., 2020) that inputs the full meaning representation and produces the full out-\nput text. In such end-to-end models, the traditional sub-tasks (Reiter and Dale, 2000) involved in language generation (i.e. planning, lexicalisation, aggregation, referring expression generation, and surface realisation) are performed implicitly. However, we posit that some of these sub-tasks, specifically lexicalisation (i.e. choice of vocabulary) and aggregation (i.e. process of combining simpler sentence structures to form complex ones), exhibit varying potential for exploiting transfer learning as the former is more domain-specific than the latter. For example, it is more difficult to exploit transfer learning for lexicalisation since if certain words are not already associated with a particular MR input, fewshot learning may not be able to create a strong association through the limited data. This is further exacerbated in zero-shot learning. On the other hand, the knowledge required to form complicated sentence structures and apply aggregation strategies is more commonly shared between domains and would benefit more from transfer learning.\nWe aim to exploit these differing potentials for transfer learning in few-shot and zero-shot generation, via a new hierarchical approach to conceptto-text NLG. Specifically, we propose Hierarchical Recurrent Aggregative Generation (HRAG), a three-moduled architecture where the first module is in charge of independently lexicalising each unit of information in the input as a sub-phrase (e.g. a phrase expressing that unit of information alone), the second module is responsible for recurrently aggregating these sub-units to generate a unified text, and the third module rephrases it to produce a coherent and fluent output; see Figure 1. These are jointly trained via a loss that combines their discrete objectives. Concept-to-text is ideal for HRAG as MRs can be split into attribute-value pairs that vaguely correspond to output sub-phrases.\nIn this paper, we (i) present Hierarchical Re-\ncurrent Aggregative Generation and experimentally demonstrate the benefits of separately applying transfer learning to language generation subtasks; (ii) facilitate the model’s training by inferring module-specific training signal from the available output targets; (iii) provide extensive empirical analysis and ablation studies on few-shot and zero-shot settings across 4 datasets, one of which we adapt ourselves for few-shot learning; (iv) perform human evaluation comparing our proposed approach to previous work on few-shot generation. Our automatic and human evaluation results show that our hierarchical approach achieves state-of-theart results when compared against previous work."
    }, {
      "heading" : "2 Method",
      "text" : "Figure 1 shows the overall structure of the proposed hierarchical model HRAG. Its three modules are in charge of lexicalisation, aggregation and postedit, and are inspired by traditional NLG stages and their specific potential for transfer learning in a few-shot setting. Figure 2 shows an example of how the outputs of each stage are formed."
    }, {
      "heading" : "2.1 Input segmentation",
      "text" : "In a pre-processing step, the input MR is divided into individual attribute value pairs sxvx each corresponding to one distinct fact (i.e. unit of information). Concept-to-text generation is particularly fitted to our approach as the input MR is usually straightforwardly divisible into distinct facts. To elaborate, a typical input MR consists of one or more predicates that denote the communicative goal of the sentence, followed by a set of attributevalue pairs that correspond to the information that should be expressed in the final text.\nFor example, in Figure 2 the input MR describes that the text should offer/suggest to the user a stylist named “Atelier Salon Willow Glen” that is in the\ncity of “San Jose’, and also inform them that it has found “10” salons that match their criteria. We assume that each attribute-value pair corresponds to one distinct fact which is expressed as a sub-phrase of the final output, e.g. CITY = SAN JOSE loosely corresponds to the sub-phrase “in San Jose”."
    }, {
      "heading" : "2.2 Lexicalisation",
      "text" : "The next stage is lexicalisation, i.e. the process of selecting the required vocabulary to express the input. HRAG’s respective module achieves this by independently generating a corresponding phrase wx1 . . . w x len_x for each input fact sxvx, e.g. “located in San Jose” should be generated from input CITY = SAN JOSE in Figure 2. We opt to generate from single facts, disconnected from their MR context, as it makes it easier for the model to associate them with their relevant vocabulary. This might lead to the loss of informative context, but HRAG reintroduces context in a later stage. Additionally, having a single fact input facilitates transfer learning in the few-shot setting since any previous context may be irrelevant to new domains. A final benefit is that such input is more robust to unseen facts, as any unknown attributes will only affect the corresponding sub-phrase and will not interfere with the generation from other facts.\nIn contrast, due to considering the whole input at once, previous end-to-end models need to be exposed to a lot of different combinations and orderings of attribute-value slots, to sufficiently associate complex input MRs with the output text. In few-shot settings, this becomes an issue as available MR combinations during training are limited."
    }, {
      "heading" : "2.3 Recurrent aggregation",
      "text" : "In this stage, the generated sub-phrases of the lexicalisation module are ordered based on the input’s original order, and input into the aggregation layer one at a time in a recurrent fashion. At the first step, the first two sub-phrases w11 . . . w 1 len_1 and w21 . . . w 2 len_2, and the correspondent attributevalue pairs s1v1 s2v2, are input into the aggregation layer to produce the combined sub-phrase w\n[1,2] 1 . . . w [1,2] len_[1,2] (see Figure 1). For example, the sub-phrases “it is called Atelier Salon Willow Glen” and “located in San Jose” are combined to form “there is a nice salon called Atelier Salon Willow Glen located in San Jose” as shown in Figure 2.\nAt each subsequent step r the input of the aggregation module consists of the concatenation of the previously aggregated sub-phrases\nw [1,r−1] 1 . . . w [1,r−1] len_[1,r−1], the current sub-phrase wr1 . . . w r len_r, and the correspondent attributevalue pairs s1v1 s2v2 . . . srvr, to produce the combined sub-phrase w[1,r−1]1 . . . w [1,r] len_[1,r]. The aggregation module is called recurrently until all the subphrases generated by the lexicalisation module are combined into a single output w[1,n]1 . . . w [1,n] len_[1,n].\nEach distinct aggregation layer has the advantage of being able to disassociate (to some extent) from the specific semantics of the input and direct its attention on how to combine (and copy over) the sub-phrases of the lexicalisation module. This is further enhanced by the recurrent structure of the proposed aggregation layer which permits the model to focus on a limited amount of operations at a time, converging into a final unified output."
    }, {
      "heading" : "2.4 Post-editing",
      "text" : "The aggregation layer models are trained to combine sub-phrases into larger sub-phrases and do not necessarily produce a fluent and coherent text complete with appropriate punctuation and devoid of errors. In order to rewrite the aggregated subphrases, fix any errors and finalise the text, the post-edit module takes the fully aggregated subphrases w[1,n]1 . . . w [1,n] len_[1,n] and produces the output w′1 . . . w ′ l, as seen in the top stage of Figure 2. Being largely domain-agnostic, aggregation and post-edit benefit the most from transfer learning."
    }, {
      "heading" : "2.5 Training, reranking and selection",
      "text" : "Each module is built on top of a PLM; these PLMs have separate shared weights per stage and are specifically fine-tuned for that stage. For training, the modules’ losses are combined as in Eq. 1:\nLoss = 1n ∑ n Losslex + 1 n−1 ∑ n−1 Lossaggr + Losspe\n(1) where cross entropy is used for Losslex, Lossaggr and Losspe , and n the number of units in the MR.\nTo mitigate any data sparsity issues, we employ language agnostic delexicalisation (Zhou and Lampouras, 2021) for the lexicalisation and aggregation modules, with relexicalisation performed before post-edit. Briefly, any input value that is determined to occur in the text (via embedding similarity) is delexicalized. In addition, to minimise the error propagated between layers, each module generates multiple hypotheses per input and forward the hypothesis with the least slot error rate to\nthe next iteration/module, where slot error rate is defined as the percentage of values in the input that are missing, repeated or hallucinated in the output."
    }, {
      "heading" : "2.6 Inferring labels",
      "text" : "Ideally, the PLMs that are used in HRAG’s different modules would be fine-tuned on stage-specific parallel input and target data. However, while the postedit module can be trained against the dataset’s final output target, such direct annotations for the first two modules are not readily available. To overcome this, we adopt a distant supervision approach to automatically extract stage-appropriate training signals from the existing data.\nFor the lexicalization stage, we extract subphrase targets from the output target that weakly correspond to the individual facts; this process is depicted in Figure 3. Given an MR, we first determine occurrences of its values in the output target via language agnostic delexicalisation. If the value is not matched, we repeat the process using the attribute instead; this is useful for some boolean attributes (e.g. “accepts credit cards = yes”). If a match is still not found, we assume that the fact is not present in the output target, and we ignore that attribute-value pair from the input during training.\nFor each fact sxvx, the corresponding target subphrase is set to include the matched value of vx and all words preceding and following it until either a punctuation mark or another matched value is reached. This will cause some overlap between the inferred sub-phrase targets but ensures that all the relevant vocabulary is included in each fact’s target. While using this noisy training signal may encourage some hallucinations of irrelevant input, in preliminary experiments this strategy worked better than alternatives; the aggregation layer proved robust enough to ignore irrelevant or repeated words\nthat were output from the lexicalisation layer. Using the aforementioned value matching, we can similarly infer targets for the aggregation layers. However, to facilitate the process, the order in which lexicalisation sub-phrases are aggregated (see Section 2.3) needs to be fixed to the appearance order of the corresponding matched values in the output target. Given the example of Figure 3, the order would be INFORM (COUNT = 10) > OFFER (CITY = SAN JOSE) > OFFER (STYLIST NAME = ATELIER SALON WILLOW GLEN).\nThe aggregation targets are then inferred as such: for every aggregation group s1v1 s2v2 . . . srvr, the target consists of a subphrase of the output target, from its beginning, including the words of the last matched value vr, and until either a punctuation or another matched value is reached after that point. Again following the example of Figure 3, the aggregation target for INFORM (COUNT = 10) + OFFER (CITY = SAN JOSE) will be “I found 10 salons you may like. There is a nice salon in San Jose called”.\nWe note that this order of lexicalisation subphrases is only imposed during training since we are limited by the output target. During testing, as we mentioned in Section 2.5, the generated subphrases of the lexicalisation module follow the original input’s order. This results in an important discrepancy between the order of sub-phrases that HARG is exposed to during training and inference, which we leave for future work."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We perform experiments on four datasets: SchemaGuided Dialogue (Rastogi et al., 2020, SGD) with the few-shot splits provided by (Kale and Rastogi, 2020, FewShotSGD), MultiWoZ 2.2 (Zang et al., 2020), FewShotWoZ (Peng et al., 2020) and WebNLG 3.0 (Ferreira et al., 2020). The first three are task-oriented dialogue datasets, that have been adapted to different extents for few-shot learning by previous work. For our experiments, dialogue MRs are linearised as lists of “INTENT ( ATTRIBUTE = VALUE)”, similar to what is depicted in Figure 2, while utterances are tokenised and lower-cased.\nIn contrast to the other datasets, WebNLG 3.0 (Ferreira et al., 2020) does not contain dialogues but describes entities from a variety of domains, and consists of sets of RDF triples and corresponding texts in English and Russian; here we use only the English portion. The dataset is organised in\nsubsets based on the number of RDF triples in the input, ranging from 1 to 7. To create appropriate splits for few-shot learning, for each length-specific subset, we identified all unique combinations of RDF properties in the input and limited the dataset to a single (where available) instance per combination. In other words, we kept only 1 instance per property for the 1-triple subset, 1 instance per pair of properties for the 2-triple subset, and so forth. Our splits essentially constitute a 1-shot learning dataset, which we will refer to as FewShotWeb dataset. More details regarding the FewShotWeb splits can be found in appendix A. Preprocessing of the RDF triples and target text were performed similarly to Zhou and Lampouras (2021)."
    }, {
      "heading" : "3.2 Automatic metrics",
      "text" : "Following related work, to estimate the fluency of the output, we provide results for BLEU-4 (computed with SacreBLEU) (Papineni et al., 2002; Post, 2018), and BLEURT (Sellam et al., 2020) (specifically the bleurt-base-128 version). We calculate BLEU score over multiple references to mitigate the unreliability of single reference evaluation.\nTo estimate adequacy, we use Missing Slot Error (MER), computed as the macro-averaged percentage of values in the MR that are missing (i.e. do not appear verbatim) from the output utterance. We should note that MER is an imperfect approximation compared to slot error rate, as it does not account for hallucinations, boolean or no-value attributes. These types of slot errors are difficult to detect in non-delexicalized output, which all systems in our experiments produce. Evaluation is performed consistently across all datasets.2"
    }, {
      "heading" : "3.3 Systems",
      "text" : "We compare HRAG against a fine-tuned end-toend T5 model (E2E T5), equivalent to the “Naive\" model shown by Kale and Rastogi (2020), which achieved state-of-the-art on the MultiWoZ dataset as well as in the recent WebNLG Challenge 2020 (Castro Ferreira et al., 2020). We employ t5-small for the underlying PLMs of both HRAG and E2E T5, to be consistent with Kale and Rastogi (2020)."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Ablation Study",
      "text" : "First, we present an ablation study of HRAG on the 5-shot SGD dataset aimed to analyse the impact of\n2Evaluation scripts will be released along with the code.\nits components; the results are presented in Table 1. To examine the output of the lexicalisation module without aggregation, we simply concatenate the independently generated sub-phrases to form a unified text. As is to be expected, such a concatenation achieves low BLEU and BLEURT scores, clearly indicating the need for more sophisticated aggregation. Nevertheless, the lexicalisation module achieves 0% missing slot error thanks to its focus on individual units of information.\nFor the aggregation module, we examine the output of its final iteration. Its performance is on par with lexicalisation output, seemingly suggesting that aggregation offers little improvement. However, based on output analysis, the low BLEU and BLEURT are misleading and do not reflect the output quality. We attribute the lack of automatic score improvements to the module’s tendency to overgenerate at the end of the output in anticipation of the next sub-phrase (as shown in the example in Figure 2). Other errors emerge from no-value attributes and due to sub-optimal training targets. MER increases the most during aggregation, as its recurrent nature is prone to error propagation. We should also note that using a single aggregation layer to aggregate all sub-phrases at the same time had comparable BLEU and BLEURT performance but underperformed by 8.18 points in MER.\nThe role of the post-edit module is to obviate errors propagated from the lexicalisation and aggregation modules, and it greatly improves performance by 6.4 and 0.1 points in BLEU and BLEURT respectively. Specifically, this stage fixes the lexicalisation of no-value attributes, removes overgenerated tokens, improves fluency, and adds or removes values that have been missed or repeated. Nonetheless, as this is an extra generation step, it occasionally removes some required values, as indicated by the almost constant slot error.\nDue to these frequent imperfections in the post-\nedit layer’s output, the final output of HRAG is selected between the output of the last aggregation iteration and the output of the post-edit module according to which one has the lower MER. This process leads to the highest BLEU and BLEURT scores and a MER close to 0%.\nFinally, we examine delexicalisation’s impact on E2E T5, applying it similarly to how it is applied to HRAG. While HRAG benefits from delexicalisation as it improves its generalisation ability and helps reduce MER, we observe marginal improvements (a MER decrease of 0.03%) when applied over a strong end-to-end model like E2E T5."
    }, {
      "heading" : "4.2 Few-Shot Evaluation",
      "text" : "Tables 2 show automatic evaluation results for E2E T5 and HRAG systems trained on an increasing amount of data on FewShotSGD, FewShotWeb and MultiWoZ datasets. Overall the behaviour of the two systems is consistent across the three datasets.\nAs discussed in Section 4.1, HRAG manages to preserve the input MR values throughout the generation, and as such outperforms E2E T5 in MER across all dataset splits by a significant margin, especially when trained on smaller splits. E2E T5 only overperforms on 0.1% MultiWoZ, but a closer examination of the outputs reveals that the 4.85% MER is achieved at great expense to fluency as the system simply copies all input MRs instead of generating utterances. Although HRAG was not completely unaffected by such behaviour, it was still able to generate relevant outputs thanks to its ability to independently lexicalise smaller and simpler sub-phrases which lead to improvements of 10.79 BLEU and 0.55 BLEURT scores over E2E T5 despite the higher MER on 0.1% MultiWoZ.\nResults in Table 2 demonstrate the effectiveness of HRAG in extremely low-resourced conditions with differences in BLEU and BLEURT scores of 2.89 and 0.3 for FewShotSGD and 6.54 and 0.12 for FewShotWeb on their respective smaller splits. Improvements over the end-to-end systems converge as the number of training examples increases, but HRAG consistently performs best in MER across all datasets and training pool sizes. In terms of BLEU/BLEURT, HRAG is able to maintain an edge over E2E T5 on all FewShotSGD and FewShotWeb splits, while on MultiWoZ, E2E T5 appears to be the best performing system, especially in terms of BLEURT score with a difference up to 0.9. By looking at the system’s outputs, how-\never, HRAG appears to perform comparably or even outperform E2E T5 despite lower BLEURT scores; examples are provided in Appendix C.\nTable 3 shows results on FewShotWoZ; models are trained and tested separately on each domain. Similarly to the results shown in Table 2, HRAG excels in terms of MER, missing on average 5% fewer values compared to E2E T5. In BLEU and BLEURT scores, while on average E2E T5 outperforms HRAG, there is no consistently better system. Unfortunately, only one reference per MR is provided making multi-reference scoring impossible and in extension BLEU more unreliable. In Section 4.4, we perform human evaluation to better assess\nsystems performance on FewShotWoZ."
    }, {
      "heading" : "4.3 Zero-Shot Evaluation",
      "text" : "We perform zero-shot analysis on SGD and WebNLG testsets; Figure 4 shows the results of the systems presented in Section 4.2 with reported performances split into domains seen and unseen during training according to the original datasets.3\nIn both datasets, HRAG achieves MER in unseen cases lower than even E2E T5’s seen scores, further validating the generalisation ability of HRAG when little to no resources are available. Overall, HRAG achieves higher BLEU and BLUERT scores than E2E T5 as well, with the exception of BLEURT scores for FewShotSGD. However, similarly to what has been found in Section 4.2, HRAG’s outputs do not appear to necessarily be more disfluent than E2E T5 outputs.\nInterestingly, HRAG’s MER for unseen FewShotWeb is lower than the corresponding seen one. We observe that HRAG tends to avoid generating complex sentence structures when dealing with unseen inputs, and simply concatenates the lexicalisation sub-phrases (e.g. “liselotte grschebina, born in the german empire, attended the school of applied arts in stuttgart, israel.”). This strategy benefits FewShotWeb as HRAG focuses on copying elements from the input and effectively avoids introducing noise. Such behaviour is not observed for FewShotSGD, but seen/unseen MER are still comparable and in close proximity to 0%."
    }, {
      "heading" : "4.4 Human Evaluation",
      "text" : "To account for the shortcomings of automatic evaluation, we employed the human evaluation framework Direct Assessment (Graham et al., 2017) to set up tasks on the Amazon Mechanical Turk\n3Full results tables are shown in Appendix D.\n0\n10\n20\n30\n40\n50\n1 e 2\n(AMT) platform and assess the fluency and adequacy of various models’ outputs. We created separate tasks to assess the fluency and adequacy of the texts on two distinct subsets, in order to minimise correlation between the criteria. Specifically, we sampled 750 MRs from each test set of 5-shot SGD and FewShotSGD, and collected the corresponding outputs of HRAG, E2E T5, and the ground truth (GOLD); we include the latter to provide context to the evaluation. We picked the 5-shot subset of SGD to observe how the systems behave when exposed to the least amount of in-domain data. The pool of crowd-workers was limited to those residing in English-speaking countries, and who had a high acceptance rate; every text was evaluated by at least 3 crowd-workers on a 1 to 100 Likert scale. After consulting the crowd-workers’ reliability based on"
    }, {
      "heading" : "D GOLD 80.502 0.103 78.690 0.044",
      "text" : "the Direct Assessment platform analysis, we had to filter out 39.5% of the participants.\nTable 4 gathers the raw and mean standardised z-scores of the evaluation. Both models of course are considered worse than the ground truth, but HRAG performs better than E2E T5 in both fluency and adequacy, with the exception of fluency in FewShotWoZ where the systems exhibit no statistically significant difference (according to Wilcoxon rank-sum tests). These results further support the efficacy of HARG for few-shot settings."
    }, {
      "heading" : "5 Related work",
      "text" : "Despite being an important research topic with real-life applications, domain adaptation for lowresource/few-shot concept-to-text NLG has not been extensively researched. Wen et al. (2016) leveraged the scarcity of target in-domain data by augmenting it with synthetic data, Tran and Nguyen (2018) used variational autoencoders in conjunction with text similarity and domain critics to better guide the fine-tuning process, while Mi et al. (2019) tackled the problem by defining domain adaptation as an optimisation meta-learning task. Most recently, Peng et al. (2020) and Kale and Rastogi (2020) have proposed the use of pretrained language models to tackle few-shot and zero-shot learning in concept-to-text NLG, achieving significant gains over strong non-pretrained baselines. Specifically, Peng et al. (2020) proposed SC-GPT, a semantically conditioned GPT-2 model, wherein, prior to few-shot learning, the GPT-2 model is further fine-tuned on a number of task-oriented dialogue datasets in order to mitigate the problem of representation bias. On the other hand, in Kale and Rastogi (2020), a set of human-authored templates are used to generate high-quality sentences\ncorresponding to each unit of information in an MR. These are then concatenated and given as input to a T5 model (T2G2) to form a coherent sentence. In this paper’s evaluation, we opt to compare our approach against the naive T5 baseline introduced by Kale and Rastogi (2020), as it is shown to overly outperform SC-GPT by basically replacing the underlying GPT-2 model for T5, and SC-GPT was outperform all previous non-pretrained baselines. We do not compare against T2G2, as access to human authored templates or other such manually annotated resources, which are by nature very domain-specific and costly to create, are not necessarily guaranteed in low-resource settings. We note that T2G2 is equivalent to the naive T5 when templates are not employed.\nIn our proposed system, the hierarchy emerges from modelling the lexicalisation and aggregation sub-tasks on separate layers. Previous attempts in exploring hierarchical structures for text generation tasks instead focused on modelling different aspects of the input or output. In concept-to-text NLG for task-oriented dialogues, Su et al. (2018) proposed a multi-layered decoding process where each layer was responsible for generating words associated with specific part-of-speech tags. Chen et al. (2019) and Tseng et al. (2019) took advantage of the intrinsically hierarchical structure of dialogue acts to create better input representations and ease domain adaptation. Our approach is also related to coarse-to-fine approaches, which have been explored in story (Fan et al., 2018), review (Li et al., 2019) and keyphrase (Chen et al., 2020) generation tasks. However, in these tasks, the output is not necessarily restricted to be an exact realisation of the input, and can be initially loosely prompted or drafted, and subsequently expanded."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We proposed Hierarchical Recurrent Aggregative Generation, a three-moduled jointly trained architecture, designed to exploit the different extents to which lexicalisation and aggregation can benefit from transfer learning. Due to the lack of explicit training signals for HRAG’s modules, we also show how module-specific targets can be inferred from the available output targets. Extensive automatic metric experiments and analysis across 4 datasets, as well as accompanying human evaluation, shows that HRAG outperforms previous state-of-the-art, especially on missing slot error and adequacy."
    }, {
      "heading" : "A WebNLG few-shot splits",
      "text" : "Table 5 details how many of the total data were kept in our WebNLG 3.0 few-shot splits (FewShotWeb); as the triple length grows, most property combinations are unique which results in a bigger portion of the data being included. Interestingly, the 1-triple subset covers 346 out of 372 occurring properties, which makes it particularly suited for supervised learning of our lexicalisation module."
    }, {
      "heading" : "B Configurations",
      "text" : "Fine-tuning is performed with Adafactor (Shazeer and Stern, 2018) as an optimiser, with learning rate set to 1e−3 and Huggingface (Wolf et al., 2020)’s default parameters; gradient accumulation is used with a batch size of 256 for all the datasets except FewShotWoZ where the batch size is set to 1 as in (Peng et al., 2020); early-stopping is adopted with patience set to 30 and a combined loss between BLEU and slot error rate as the scoring function.\nReranking is performed as described in Section 2.5, with 5 lexicalisation and aggregation hypotheses generated at each time step. However, at training time, for computational reasons, only the lexicalisation outputs are reranked. At inference time, reranking is performed for both the baseline and HRAG’s post-edit module, with 10 hypotheses generated and reranked. Each system is fine-tuned with 5 different seeds. Section 4 reports the average performance of each system."
    }, {
      "heading" : "C Examples",
      "text" : "Table 6 shows examples of outputs produced by systems trained on 20% MultiWoZ where BLEURT score does not correlate.\nTable 7 shows examples from FewShotWoZ where E2E T5 suffers from hallucinations."
    }, {
      "heading" : "D Full results",
      "text" : "Tables 8 and 9 show the full results presented in Section 4.3.\nE Inferred training signal examples\nTables 10, 11, 12, and 13 show examples of inferred training signals for the lexicalization and aggregation modules as discussed in Section 2.6. Note that as described in the aforementioned section, during training the input facts are ordered according to their values’ appearance in the reference to facilitate the proper inference of the training signal for aggregation. Also note that we show the values intact in these examples for clarity, even though throughout our experiments the signal is inferred after a delexicalization pre-processing step.\nMR: offer ( pickup location = santa fe depot ; pickup date = march 2nd ; type = standard ; car name = accord ) Reference: there is an accord , standard , at santa fe depot on march 2nd .\nFact Inferred sub-phrase target\n1: offer ( car name = accord ) there is an accord 2: offer ( type = standard ) standard 3: offer ( pickup location = santa fe depot ) at santa fe depot on 4: offer ( pickup date = march 2nd ) on march 2nd\nFacts to be combined Inferred aggregation target\n1 + 2 there is an accord , standard 1 + 2 + 3 there is an accord , standard , at santa fe depot on 1 + 2 + 3 + 4 there is an accord , standard , at santa fe depot on march 2nd\nPost-edit target\nthere is an accord , standard , at santa fe depot on march 2nd .\nMR: confirm ( restaurant name = jo ’s sushi bar ; location = pleasant hill ; time = 11 am ; number of seats = 2 ; date = march 13th ) Reference: you want a table for 2 at jo ’s sushi bar in pleasant hill at 11 am on march 13th ?\nFact Inferred sub-phrase target\n1: confirm ( number of seats = 2 ) you want a table for 2 at 2: confirm ( restaurant name = jo ’s sushi bar ) at jo ’s sushi bar in 3: confirm ( location = pleasant hill ) in pleasant hill at 4: confirm ( time = 11 am ) at 11 am on 5: confirm ( date = march 13th ) on march 13th\nFacts to be combined Inferred aggregation target\n1 + 2 you want a table for 2 at jo ’s sushi bar in 1 + 2 + 3 you want a table for 2 at jo ’s sushi bar in pleasant hill at 1 + 2 + 3 + 4 you want a table for 2 at jo ’s sushi bar in pleasant hill at 11 am on 1 + 2 + 3 + 4 + 5 you want a table for 2 at jo ’s sushi bar in pleasant hill at 11 am on march 13th\nPost-edit target\nMR: inform_no_match ( goodformeal = breakfast ; near = civic center ) Reference: unfortunately there are no restaurant -s near civic center that are good for breakfast\nFact Inferred sub-phrase target\n1: inform_no_match ( near = civic center ) unfortunately there are no restaurant -s near civic center that are good for 2: inform_no_match ( goodformeal = breakfast ) that are good for breakfast\nFacts to be combined Inferred aggregation target\n1 + 2 unfortunately there are no restaurant -s near civic center that are good for breakfast\nPost-edit target\nMR: hotel_request ( stars ; price ; area ), hotel_inform ( choice = 29 ) Reference: there are 29 hotels that meet your needs . can you narrow it down to area , price range and stars ?\nFact Inferred sub-phrase target\n1: hotel_inform ( choice = 29 ) there are 29 hotels that meet your needs 2: hotel_request ( area ) can you narrow it down to area 3: hotel_request ( price ) price range and 4: hotel_request ( stars ) range and stars\nFacts to be combined Inferred aggregation target\n1 + 2 there are 29 hotels that meet your needs . can you narrow it down to area 1 + 2 + 3 there are 29 hotels that meet your needs . can you narrow it down to area , price range and 1 + 2 + 3 + 4 there are 29 hotels that meet your needs . can you narrow it down to area , price range and stars\nPost-edit target\nthere are 29 hotels that meet your needs . can you narrow it down to area , price range and stars ?\nMR: booking_nobook ( time = 10:00 ; day = saturday ), booking_request ( time ) Reference: i am sorry we could not book you for saturday at 10:00 . would you like to try another time ?\nFact Inferred sub-phrase target\n1: booking_nobook ( day = saturday ) i am sorry we could not book you for saturday at 2: booking_nobook ( time = 10:00 ) at 10:00 3: booking_request ( time ) you like to try another time\nFacts to be combined Inferred aggregation target\n1 + 2 i am sorry we could not book you for saturday at 10:00 1 + 2 + 3 i am sorry we could not book you for saturday at 10:00 . would you like to try another time\nPost-edit target\ni am sorry we could not book you for saturday at 10:00 . would you like to try another time ?\nPost-edit target\namdavad ni gufa is located in ahmedabad , india , where sumitra mahajan is a leader .\nMR: <asterix ( comics character ), creator, rené goscinny> , <asterix ( comics character ), creator, albert uderzo> Reference: the comic strip character asterix was created by albert uderzo and rene goscinny .\nFact Inferred sub-phrase target\n1: <asterix ( comics character ), creator, albert uderzo> the comic strip character asterix was created by albert uderzo and 2: <asterix ( comics character ), creator, rené goscinny> and rene goscinny\nFacts to be combined Inferred aggregation target\n1 + 2 the comic strip character asterix was created by albert uderzo and rene goscinny\nPost-edit target"
    } ],
    "references" : [ {
      "title" : "The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+",
      "author" : [ "Thiago Castro Ferreira", "Claire Gardent", "Nikolai Ilinykh", "Chris van der Lee", "Simon Mille", "Diego Moussallem", "Anastasia Shimorina" ],
      "venue" : null,
      "citeRegEx" : "Ferreira et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2020
    }, {
      "title" : "Exclusive hierarchical decoding for deep keyphrase generation",
      "author" : [ "Wang Chen", "Hou Pong Chan", "Piji Li", "Irwin King." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1095–1105, Online. Asso-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantically conditioned dialog response generation via hierarchical disentangled self-attention",
      "author" : [ "Wenhu Chen", "Jianshu Chen", "Pengda Qin", "Xifeng Yan", "William Yang Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "The 2020 bilingual, bidirectional webnlg+ shared task overview and evaluation results (webnlg+ 2020)",
      "author" : [ "Thiago Ferreira", "Claire Gardent", "Nikolai Ilinykh", "Chris van der Lee", "Simon Mille", "Diego Moussallem", "Anastasia Shimorina." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Ferreira et al\\.,? 2020",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2020
    }, {
      "title" : "Can machine translation systems be evaluated by the crowd alone",
      "author" : [ "Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel." ],
      "venue" : "Natural Language Engineering, 23(1):3–30.",
      "citeRegEx" : "Graham et al\\.,? 2017",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2017
    }, {
      "title" : "Template guided text generation for task-oriented dialogue",
      "author" : [ "Mihir Kale", "Abhinav Rastogi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6505–6520, Online. Association for Computa-",
      "citeRegEx" : "Kale and Rastogi.,? 2020",
      "shortCiteRegEx" : "Kale and Rastogi.",
      "year" : 2020
    }, {
      "title" : "Generating long and informative reviews with aspect-aware coarse-to-fine decoding",
      "author" : [ "Junyi Li", "Wayne Xin Zhao", "Ji-Rong Wen", "Yang Song." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1969–",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Association for Computational Linguistics",
      "author" : [ "Florence", "Italy" ],
      "venue" : null,
      "citeRegEx" : "1979 et al\\.,? \\Q1979\\E",
      "shortCiteRegEx" : "1979 et al\\.",
      "year" : 1979
    }, {
      "title" : "Meta-learning for low-resource natural language generation in task-oriented dialogue systems",
      "author" : [ "Fei Mi", "Minlie Huang", "Jiyong Zhang", "Boi Faltings." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-",
      "citeRegEx" : "Mi et al\\.,? 2019",
      "shortCiteRegEx" : "Mi et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Few-shot natural language generation for task-oriented dialog",
      "author" : [ "Baolin Peng", "Chenguang Zhu", "Chunyuan Li", "Xiujun Li", "Jinchao Li", "Michael Zeng", "Jianfeng Gao." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020,",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
      "author" : [ "Abhinav Rastogi", "Xiaoxue Zang", "Srinivas Sunkara", "Raghav Gupta", "Pranav Khaitan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Rastogi et al\\.,? 2020",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2020
    }, {
      "title" : "Building Natural Language Generation Systems",
      "author" : [ "Ehud Reiter", "Robert Dale." ],
      "venue" : "Studies in Natural Language Processing. Cambridge University Press.",
      "citeRegEx" : "Reiter and Dale.,? 2000",
      "shortCiteRegEx" : "Reiter and Dale.",
      "year" : 2000
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Adafactor: Adaptive learning rates with sublinear memory cost",
      "author" : [ "Noam Shazeer", "Mitchell Stern." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018,",
      "citeRegEx" : "Shazeer and Stern.,? 2018",
      "shortCiteRegEx" : "Shazeer and Stern.",
      "year" : 2018
    }, {
      "title" : "Natural language generation by hierarchical decoding with linguistic patterns",
      "author" : [ "Shang-Yu Su", "Kai-Ling Lo", "Yi-Ting Yeh", "YunNung Chen." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Su et al\\.,? 2018",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial domain adaptation for variational neural language generation in dialogue systems",
      "author" : [ "Van-Khanh Tran", "Le-Minh Nguyen." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1205–1217, Santa Fe,",
      "citeRegEx" : "Tran and Nguyen.,? 2018",
      "shortCiteRegEx" : "Tran and Nguyen.",
      "year" : 2018
    }, {
      "title" : "Tree-structured semantic encoder with knowledge sharing for domain adaptation in natural language generation",
      "author" : [ "Bo-Hsiang Tseng", "PaweĹ‚ Budzianowski", "Yen-chen Wu", "Milica Gasic" ],
      "venue" : "In Proceedings of the 20th Annual SIGdial Meeting on Dis-",
      "citeRegEx" : "Tseng et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tseng et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-domain neural network language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "Lina M. Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young." ],
      "venue" : "Proceedings of the 2016 Con-",
      "citeRegEx" : "Wen et al\\.,? 2016",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines",
      "author" : [ "Xiaoxue Zang", "Abhinav Rastogi", "Srinivas Sunkara", "Raghav Gupta", "Jianguo Zhang", "Jindong Chen" ],
      "venue" : null,
      "citeRegEx" : "Zang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zang et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalising multilingual concept-to-text NLG with language agnostic delexicalisation",
      "author" : [ "Giulio Zhou", "Gerasimos Lampouras." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Zhou and Lampouras.,? 2021",
      "shortCiteRegEx" : "Zhou and Lampouras.",
      "year" : 2021
    }, {
      "title" : "Transfer learning for low-resource neural machine translation",
      "author" : [ "Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1568–1575, Austin,",
      "citeRegEx" : "Zoph et al\\.,? 2016",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : ", 2020), T5 (Raffel et al., 2020)) has caused a shift of interest in the research community towards domain adaptation and transfer learning.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "WebNLG+ Shared Task (Ferreira et al., 2020) employing a fine-tuned pretrained model as their main submitted system.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : "Prominent and relevant examples include machine translation (Zoph et al., 2016; Brown et al., 2020) and NLG for task-oriented dialogues (Peng et al.",
      "startOffset" : 60,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : ", 2020) and NLG for task-oriented dialogues (Peng et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "This paper focuses on concept-to-text NLG, where recent machine learning and in extension transfer learning approaches adopt an end-to-end architecture (Peng et al., 2020) that inputs the full meaning representation and produces the full out-",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "In such end-to-end models, the traditional sub-tasks (Reiter and Dale, 2000) involved in language generation (i.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "To mitigate any data sparsity issues, we employ language agnostic delexicalisation (Zhou and Lampouras, 2021) for the lexicalisation and aggregation modules, with relexicalisation performed before post-edit.",
      "startOffset" : 83,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "0 (Ferreira et al., 2020) does not contain dialogues but describes entities from a variety of domains, and consists of sets of RDF triples and corresponding texts in English and Russian; here we use only the English portion.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "2 Automatic metrics Following related work, to estimate the fluency of the output, we provide results for BLEU-4 (computed with SacreBLEU) (Papineni et al., 2002; Post, 2018), and BLEURT (Sellam et al.",
      "startOffset" : 139,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "2 Automatic metrics Following related work, to estimate the fluency of the output, we provide results for BLEU-4 (computed with SacreBLEU) (Papineni et al., 2002; Post, 2018), and BLEURT (Sellam et al.",
      "startOffset" : 139,
      "endOffset" : 174
    }, {
      "referenceID" : 17,
      "context" : ", 2002; Post, 2018), and BLEURT (Sellam et al., 2020) (specifically the bleurt-base-128 version).",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "To account for the shortcomings of automatic evaluation, we employed the human evaluation framework Direct Assessment (Graham et al., 2017) to set up tasks on the Amazon Mechanical Turk",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "Our approach is also related to coarse-to-fine approaches, which have been explored in story (Fan et al., 2018), review (Li et al.",
      "startOffset" : 93,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : ", 2018), review (Li et al., 2019) and keyphrase (Chen et al.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : ", 2019) and keyphrase (Chen et al., 2020) generation tasks.",
      "startOffset" : 22,
      "endOffset" : 41
    } ],
    "year" : 0,
    "abstractText" : "Large pretrained models enable transfer learning to low-resource domains for language generation tasks. However, previous end-to-end approaches do not account for the fact that some generation sub-tasks, specifically aggregation and lexicalisation, can benefit from transfer learning to different extents. To exploit these varying potentials for transfer learning, we propose a new hierarchical approach for few-shot and zero-shot generation. Our approach consists of a three-moduled jointly trained architecture: the first module independently lexicalises the distinct units of information in the input as sentence sub-units (e.g. phrases), the second module recurrently aggregates these sub-units to generate a unified intermediate output, while the third module subsequently post-edits it to generate a coherent and fluent final text. We perform extensive empirical analysis and ablation studies on fewshot and zero-shot settings across 4 datasets. Automatic and human evaluation shows that the proposed hierarchical approach is consistently capable of achieving state-of-the-art results when compared to previous work.1",
    "creator" : null
  }
}
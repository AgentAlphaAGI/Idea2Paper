{
  "name" : "ARR_2022_279_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Direct parsing to sentiment graphs",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The task of structured sentiment analysis (SSA) is aimed at locating all opinion tuples within a sentence, where a single opinion contains a) a polar expression, b) an optional holder, c) an optional sentiment target, and d) a positive, negative or neutral polarity. An example is provided in Figure 1. While there have been sentiment corpora annotated with this type of information for decades (Wiebe et al., 2005; Toprak et al., 2010), there have so far been few attempts at modeling the full representation, rather focusing on various subcomponents, such as the polar expressions and targets without explicitly expressing the relations (Peng et al., 2019; Xu et al., 2020) or the polarity (Yang and Cardie, 2013; Katiyar and Cardie, 2016).\nDependency parsing approaches have recently shown promising results for SSA (Barnes et al., 2021; Peng et al., 2021). Here we present a novel sentiment parser which, unlike previous attempts, predicts sentiment graphs directly from text without reliance on heuristic lossy conversions to intermediate dependency representations. The model takes inspiration from successful work in meaning representation parsing, and in particular the permutation-invariant graph-based parser of Samuel and Straka (2020) called PERIN.\nExperimenting with several different graph encodings, we evaluate our approach on five datasets from four different languages, and find that it compares favorably to dependency-based baselines across all datasets; most significantly on the more structurally complex ones – NoReC and MPQA."
    }, {
      "heading" : "2 Related work",
      "text" : "Proposing a dependency parsing approach to the full task of SSA, Barnes et al. (2021) show that it leads to strong improvements over state-of-the-art baselines. Peng et al. (2021) propose a sparse fuzzy attention mechanism to deal with the sparseness of dependency arcs in the models from Barnes et al. (2021) and show further improvements. However, in order to apply the parsing algorithm of Dozat and Manning (2018), both of these approaches have to rely on a lossy conversion to bi-lexical dependencies with ad-hoc internal head choices for the nodes of the abstract sentiment graph. This lossy behaviour is caused by nested text spans in the sentiment graphs, as illustrated by Figure 1, which are ambiguous in their bi-lexical dependency encoding (see Section A in the Appendix).\nMore generally, decoding structured graph information from text has sparked a lot of interest in recent years, especially for parsing meaning representation graphs (Oepen et al., 2020). There has been tremendous progress in developing complex transition-based and graph-based parsers (Hershcovich et al., 2017; McDonald and Pereira, 2006; Dozat and Manning, 2018). In this paper, we adopt PERIN (Samuel and Straka, 2020), a state-of-theart graph-based parser capable of modeling a superset of graph features needed for our task."
    }, {
      "heading" : "3 PERIN model",
      "text" : "PERIN is a general permutation-invariant text-tograph parser. We briefly describe our modified SSA version, please consult the original work for more details (Samuel and Straka, 2020)."
    }, {
      "heading" : "3.1 Architecture",
      "text" : "PERIN processes the input text in four steps, illustrated in Figure 2: 1) To encode the input, PERIN uses contextualized embeddings from XLM-R (base size; Conneau et al., 2020) and combines them with learned character-level embeddings; 2) each token is mapped onto latent queries by a linear transformation; 3) a stack of Transformer layers (Vaswani et al., 2017) optionally models the inter-query dependencies; and 4) classification heads select and label queries onto nodes, establish anchoring from nodes to tokens, and predict the node-to-node edges."
    }, {
      "heading" : "3.2 Permutation-invariant query-to-node matching",
      "text" : "Traditional graph-based parsers are trained as autoregressive sequence-to-sequence models. PERIN does not assume any prior ordering of the graph nodes. Instead, it processes all queries in parallel and then dynamically maps them to gold nodes.\nBased on the predicted probabilities of labels and anchors, we create a weighted bipartite graph between all queries and nodes. Our goal is to find the most probable matching, which can be done efficiently in polynomial time by using the Hungarian algorithm. Finally, every node is assigned to a query and we can backpropagate through standard cross-entropy losses to update the model weights."
    }, {
      "heading" : "3.3 Graph encodings",
      "text" : "PERIN defines an overall framework for general graph parsing, it can cater to specific graph encodings by changing the subset of its classification heads. In parsing the abstract sentiment structures, there are several possible lossless graph encodings depending on the positioning of the polarity information and the sentiment node type (see Figure 3):\n1. Node-centric encoding, with labeled nodes and directed unlabeled arcs. Each node corresponds to a target, holder or sentiment expression; edges form their relationships. The parser uses a multi-class node head, an anchor head and a binary edge classification head.\n2. Labeled-edge encoding, with deduplicated unlabeled nodes and labeled arcs. Each node corresponds to a unique text span from some sentiment graph, while edge labels denote their relationships and functions. The model has a binary node classifier, an anchor classifier and a binary and multi-class edge head. 3. Opinion-tuple encoding, which represents the structured sentiment information as a sequence of opinion four-tuples. This encoding is the most restrictive, having the lowest degrees of freedom. The parser utilizes a multiclass node head and three anchor classifiers, it does not need an edge classifier."
    }, {
      "heading" : "4 Experiments",
      "text" : "Following Barnes et al. (2021) we perform experiments on five structured sentiment datasets in four languages, the statistics of which are shown in Table 1. The largest dataset is the NoReCfine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian. EU and CA (Barnes et al., 2018) contain hotel reviews in Basque and Catalan, respectively. MPQA (Wiebe et al., 2005) annotates news wire text in English. Finally, DSU (Toprak et al., 2010) annotates English reviews of online universities. We use the SemEval 2022 releases of MPQA and DSU.1\n1Available from https://competitions. codalab.org/competitions/33556.\nNode-centric representation Labeled-edge representation Opinion-tuple representation"
    }, {
      "heading" : "4.1 Evaluation",
      "text" : "Following Barnes et al. (2021), we evaluate our models using both token-level F1 for extraction of Holders, Targets, and polar Expressions, as well as the graph-level metrics Non-polar Sentiment Graph F1 (NSF1) and Sentiment Graph F1 (SF1), weighing the overlap in predicted and gold spans for each entity, averaged across all three spans. SF1, which also includes polarity, is considered the primary metric for the full SSA task."
    }, {
      "heading" : "4.2 Models",
      "text" : "We compare our models to the head-final dependency graph parsers from Barnes et al. (2021) as well as the second-order Sparse Fuzzy Attention parser of Peng et al. (2021). For all models, we perform 5 runs with 5 different random seeds and report the mean and standard deviation. Results on development splits are provided in Appendix D, training details are in Appendix E."
    }, {
      "heading" : "4.3 Results",
      "text" : "Table 2 shows the main results. Our models outperform both dependency graph models on SF1, although the results are mixed for span extraction. The opinion-tuple encoding gives the best performance on SF1 (an average of 6.2 percentage points (pp.) better than Peng et al. (2021)), followed by the labeled edge encoding (3.0) and finally the node-centric encoding (2.1).\nFor extracting spans, the opinion tuple encoding also achieves the the best results on NoReC, either labeled-edge or node centric on CA and MPQA, while Peng et al. (2021) is best on EU and DSU. This suggests that the main benefit of PERIN is at the structural level, rather than local extraction."
    }, {
      "heading" : "5 Analysis",
      "text" : "There are a number of architectural differences between the dependency parsing approaches compared above. In this section, we aim to isolate the effect of predicting intermediate dependency graphs vs. directly predicting sentiment graphs by creating more comparable dependency2 and PERIN models. We adapt the dependency model from Barnes et al. (2021) by removing the token, lemma, and POS embeddings and replacing mBERT (Devlin et al., 2019) with XLM-R (Conneau et al., 2020). The ‘XLM-R dependency’ model thus has character LSTM embeddings and token-level XLM-R features. Since these are not updated during training, for the opinion-tuple ‘Frozen PERIN’ model, we fix the XLM-R weights to make it comparable.\nAs shown in Table 3, predicting the sentiment graph directly leads to an average gain of 3.7 pp. on the Sentiment Graph F1 metric. For extracting the spans of holder, target, and polar expressions, the\n2We do not use the model from Peng et al. (2021) as the code is not available.\nbenefit is less clear. Here, the PERIN model only outperforms the XLM-R dependency model 5 of 15 times, which seems to confirm that its benefit is at the graph level. This is further supported by the fact that the highest gains are found on the datasets with the most nested sentiment expressions and dependency arcs lost due to overlap, which are difficult\nto encode in bi-lexical graphs (see Appendix A)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Previous work cast the task of structured sentiment analysis (SSA) as dependency parsing, converting the sentiment graphs into lossy dependency graphs. We present a novel sentiment parser which, unlike previous attempts, predicts sentiment graphs directly from text without reliance on lossy dependency representations. We adapted a state-ofthe-art meaning representation parser to SSA and experimentally evaluated three candidate graph encodings of the sentiment structures. The results suggest that our approach to SSA has clear performance benefits, advancing the state of the art on four out of five commonly used benchmarks. Specifically, the most direct opinion-tuple encoding provides the highest performance gains. More detailed analysis of the results shows that the benefits stem from better extraction of global structures, rather than local span prediction. We will release the source code, models and predictions in the camera-ready version of this paper at https://github.com/censored/for-review."
    }, {
      "heading" : "A Problems with dependency encoding",
      "text" : "As briefly mentioned in the main text, previous dependency parsing approaches have relied on a lossy bi-lexical conversion. We use this appendix to describe this problem in more detail. There is an inherent ambiguity in the encoding of two nested text spans with the same head (defined as either the first or the last token in (Barnes et al., 2021)). To be concrete, we can use the running example “I actually enjoyed the bad acting”, which has two opinions with nested targets “the bad acting” and “acting”. As shown in Figure 4, both expressiontarget edges correctly lead to the word “acting” but it is impossible to disambiguate the prefix of both targets in the bi-lexical encoding. For that, we need a more abstract graph encoding, such as the ones suggested in the main text.\nTable 4 shows that the amount of nesting in the SSA datasets is not negligible. This is especially true for NoReC and MPQA, two datasets experiencing significant performance gains from our\nproposed graph encoding. Table 5 further shows the amount of dependency edges lost because of overlap. Finally, Table 6 shows the SF1 score when converting the gold sentiment graphs to bi-lexical dependency graphs and back – an inherent upper bound for any dependency parser."
    }, {
      "heading" : "B Changes to datasets",
      "text" : "We found out that the official data published at https://competitions.codalab.org/ competitions/33556 was slightly changed from the data used in previous related work. Specifically the MPQA and DSU datasets had removed a number of errors resulting from the annotation and from the conversion scripts used to create the sentiment graph representations. We re-run the experiments\nfor the comparable baseline model and show the performance differences in Table 8."
    }, {
      "heading" : "C Bootstrap Significance Testing",
      "text" : "In order to see whether the performance differences for the experiments are significant, we do bootstrap significance testing Berg-Kirkpatrick et al. (2012), combining two variations. First, we resample the test sets with replacement from all 5 runs together, b = 1 000 000 times, setting the threshold at p = 0.05. Additionally, we test each pair out\nof the 5× 5 combinations for all runs, resampling the test set with replacement b = 100 000 times, setting the threshold again at p = 0.5. When one system is significantly better in 15 out of the 25 comparisons, and additionally significantly better in the first joint test, we finally mark it as significantly better."
    }, {
      "heading" : "D Results on development data",
      "text" : "To make any future comparison of our approach easier, we show the development scores of all reported models in Table 7."
    }, {
      "heading" : "E Training details",
      "text" : "Generally, we follow the training regime described in the original PERIN paper (Samuel and Straka, 2020). The trainable parameters are updated with the AdamW optimizer (Loshchilov and Hutter, 2019), and their learning rate is linearly warmed-up for the first 10% of the training to improve stability, and then decayed with a cosine schedule. The XLM-R parameters are updated with a lower learning rate and higher weight decay to improve gener-\nalization; its lower also use an increasingly lower learning rate (Howard and Ruder, 2018). Similarly to PERIN, we freeze the embedding parameters for increased efficiency and regularization. Following the finding by Zhang et al. (2021), we use small learning rates and fine-tune for a rather long time to increase the training stability. Unlike the authors of PERIN, we did not find any benefits from a dynamic scaling of loss weights (Chen et al., 2018), so we simply set all loss weights to constant 1.0.\nWe trained our models on a single Nvidia P100 with 16GB RAM, the runtimes are given in Table 7. We made five runs from different seeds for each reported value to better estimate the expected error. The hyperparameter configurations for all runs follow, please consult the released code for more details and context: https://github.com/ censored/for-review.\nGeneral hyperparameters\nbatch_size = 16 beta_2 = 0.98 char_embedding = True char_embedding_size = 128 decoder_learning_rate = 6.0e-4 decoder_weight_decay = 1.2e-6 dropout_anchor = 0.4 dropout_edge_label = 0.5 dropout_edge_presence = 0.5 dropout_label = 0.85 dropout_transformer = 0.25 dropout_transformer_attention = 0.1 dropout_word = 0.1 encoder = \"xlm-roberta-base\" encoder_freeze_embedding = True encoder_learning_rate = 6.0e-6 encoder_weight_decay = 0.1 epochs = 200 focal = True freeze_bert = False hidden_size_ff = 4 * 768 hidden_size_anchor = 256 hidden_size_edge_label = 256 hidden_size_edge_presence = 256 layerwise_lr_decay = 0.9 n_attention_heads = 8 n_layers = 3 query_length = 1 pre_norm = True\nNoReC node-centric hyperparameters\ngraph_mode = \"node-centric\" query_length = 2\nNoReC labeled-edge hyperparameters\ngraph_mode = \"labeled-edge\" query_length = 2\nNoReC opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\"\nNoReC frozen opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\" freeze_bert = True batch_size = 8 decoder_learning_rate = 1.0e-4 dropout_transformer = 0.5 epochs = 50\nEU node-centric hyperparameters\ngraph_mode = \"node-centric\" query_length = 2 n_layers = 0\nEU labeled-edge hyperparameters\ngraph_mode = \"labeled-edge\" query_length = 2 n_layers = 0\nEU opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\" n_layers = 0\nEU frozen opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\" freeze_bert = True n_layers = 0 epochs = 50\nCA node-centric hyperparameters\ngraph_mode = \"node-centric\" query_length = 2 n_layers = 0\nCA labeled-edge hyperparameters\ngraph_mode = \"labeled-edge\" query_length = 2 n_layers = 0\nCA opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\" n_layers = 0\nCA frozen opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\" freeze_bert = True n_layers = 0 epochs = 50\nMPQA node-centric hyperparameters\ngraph_mode = \"node-centric\" decoder_learning_rate = 1.0e-4 query_length = 2\nMPQA labeled-edge hyperparameters\ngraph_mode = \"labeled-edge\" decoder_learning_rate = 1.0e-4 query_length = 2\nMPQA opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\"\nMPQA frozen opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\" freeze_bert = True batch_size = 8 decoder_learning_rate = 1.0e-4 dropout_transformer = 0.5 epochs = 50\nDSU node-centric hyperparameters\ngraph_mode = \"node-centric\" decoder_learning_rate = 1.0e-4 query_length = 2\nDSU labeled-edge hyperparameters\ngraph_mode = \"labeled-edge\" decoder_learning_rate = 1.0e-4 query_length = 2\nDSU opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\"\nDSU frozen opinion-tuple hyperparameters\ngraph_mode = \"opinion-tuple\" freeze_bert = True batch_size = 8 decoder_learning_rate = 1.0e-4 dropout_transformer = 0.5 epochs = 50"
    } ],
    "references" : [ {
      "title" : "MultiBooked: A corpus of Basque and Catalan hotel reviews annotated for aspect-level sentiment classification",
      "author" : [ "Jeremy Barnes", "Toni Badia", "Patrik Lambert." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Barnes et al\\.,? 2018",
      "shortCiteRegEx" : "Barnes et al\\.",
      "year" : 2018
    }, {
      "title" : "Structured sentiment analysis as dependency graph parsing",
      "author" : [ "Jeremy Barnes", "Robin Kurtz", "Stephan Oepen", "Lilja Øvrelid", "Erik Velldal." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th In-",
      "citeRegEx" : "Barnes et al\\.,? 2021",
      "shortCiteRegEx" : "Barnes et al\\.",
      "year" : 2021
    }, {
      "title" : "An Empirical Investigation of Statistical Significance in NLP",
      "author" : [ "Taylor Berg-Kirkpatrick", "David Burkett", "Dan Klein." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Nat-",
      "citeRegEx" : "Berg.Kirkpatrick et al\\.,? 2012",
      "shortCiteRegEx" : "Berg.Kirkpatrick et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
      "author" : [ "Zhao Chen", "Vijay Badrinarayanan", "Chen-Yu Lee", "Andrew Rabinovich." ],
      "venue" : "International Conference on Machine Learning, pages 794–803.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Simpler but more accurate semantic dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 484–490, Mel-",
      "citeRegEx" : "Dozat and Manning.,? 2018",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2018
    }, {
      "title" : "A transition-based directed acyclic graph",
      "author" : [ "Daniel Hershcovich", "Omri Abend", "Ari Rappoport" ],
      "venue" : null,
      "citeRegEx" : "Hershcovich et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hershcovich et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal language model fine-tuning for text classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339, Melbourne, Australia.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "Investigating LSTMs for joint extraction of opinion entities and relations",
      "author" : [ "Arzoo Katiyar", "Claire Cardie." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 919–929, Berlin,",
      "citeRegEx" : "Katiyar and Cardie.,? 2016",
      "shortCiteRegEx" : "Katiyar and Cardie.",
      "year" : 2016
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Online learning of approximate dependency parsing algorithms",
      "author" : [ "Ryan McDonald", "Fernando Pereira." ],
      "venue" : "11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for Computational",
      "citeRegEx" : "McDonald and Pereira.,? 2006",
      "shortCiteRegEx" : "McDonald and Pereira.",
      "year" : 2006
    }, {
      "title" : "A fine-grained sentiment dataset for Norwegian",
      "author" : [ "Lilja Øvrelid", "Petter Mæhlum", "Jeremy Barnes", "Erik Velldal." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 5025– 5033, Marseille, France. European Language Re-",
      "citeRegEx" : "Øvrelid et al\\.,? 2020",
      "shortCiteRegEx" : "Øvrelid et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
      "author" : [ "Haiyun Peng", "Lu Xu", "Lidong Bing", "Fei Huang", "Wei Lu", "Luo Si" ],
      "venue" : null,
      "citeRegEx" : "Peng et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Sparse fuzzy attention for structured sentiment analysis",
      "author" : [ "Letian Peng", "Zuchao Li", "Hai Zhao" ],
      "venue" : null,
      "citeRegEx" : "Peng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2021
    }, {
      "title" : "ÚFAL at MRP 2020: Permutation-invariant semantic parsing in PERIN",
      "author" : [ "David Samuel", "Milan Straka." ],
      "venue" : "Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 53–64, Online. Association for",
      "citeRegEx" : "Samuel and Straka.,? 2020",
      "shortCiteRegEx" : "Samuel and Straka.",
      "year" : 2020
    }, {
      "title" : "Sentence and expression level annotation of opinions in user-generated discourse",
      "author" : [ "Cigdem Toprak", "Niklas Jakob", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, Up-",
      "citeRegEx" : "Toprak et al\\.,? 2010",
      "shortCiteRegEx" : "Toprak et al\\.",
      "year" : 2010
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Annotating expressions of opinions and emotions in language",
      "author" : [ "Janyce Wiebe", "Theresa Wilson", "Claire Cardie." ],
      "venue" : "Language Resources and Evaluation, 39(2-3):165–210.",
      "citeRegEx" : "Wiebe et al\\.,? 2005",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 2005
    }, {
      "title" : "Position-aware tagging for aspect sentiment triplet extraction",
      "author" : [ "Lu Xu", "Hao Li", "Wei Lu", "Lidong Bing." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2339–2349, Online. Associa-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint inference for fine-grained opinion extraction",
      "author" : [ "Bishan Yang", "Claire Cardie." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1640–1649, Sofia, Bulgaria. Association for",
      "citeRegEx" : "Yang and Cardie.,? 2013",
      "shortCiteRegEx" : "Yang and Cardie.",
      "year" : 2013
    }, {
      "title" : "Revisiting fewsample {bert} fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "While there have been sentiment corpora annotated with this type of information for decades (Wiebe et al., 2005; Toprak et al., 2010), there have so far been few attempts at modeling the full representation, rather focusing on various subcomponents, such as the polar expressions and targets without explicitly expressing the relations (Peng et al.",
      "startOffset" : 92,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "While there have been sentiment corpora annotated with this type of information for decades (Wiebe et al., 2005; Toprak et al., 2010), there have so far been few attempts at modeling the full representation, rather focusing on various subcomponents, such as the polar expressions and targets without explicitly expressing the relations (Peng et al.",
      "startOffset" : 92,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : ", 2010), there have so far been few attempts at modeling the full representation, rather focusing on various subcomponents, such as the polar expressions and targets without explicitly expressing the relations (Peng et al., 2019; Xu et al., 2020) or the polarity (Yang and Cardie, 2013; Katiyar and Cardie, 2016).",
      "startOffset" : 210,
      "endOffset" : 246
    }, {
      "referenceID" : 19,
      "context" : ", 2010), there have so far been few attempts at modeling the full representation, rather focusing on various subcomponents, such as the polar expressions and targets without explicitly expressing the relations (Peng et al., 2019; Xu et al., 2020) or the polarity (Yang and Cardie, 2013; Katiyar and Cardie, 2016).",
      "startOffset" : 210,
      "endOffset" : 246
    }, {
      "referenceID" : 1,
      "context" : "Dependency parsing approaches have recently shown promising results for SSA (Barnes et al., 2021; Peng et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "Dependency parsing approaches have recently shown promising results for SSA (Barnes et al., 2021; Peng et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "There has been tremendous progress in developing complex transition-based and graph-based parsers (Hershcovich et al., 2017; McDonald and Pereira, 2006; Dozat and Manning, 2018).",
      "startOffset" : 98,
      "endOffset" : 177
    }, {
      "referenceID" : 11,
      "context" : "There has been tremendous progress in developing complex transition-based and graph-based parsers (Hershcovich et al., 2017; McDonald and Pereira, 2006; Dozat and Manning, 2018).",
      "startOffset" : 98,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "There has been tremendous progress in developing complex transition-based and graph-based parsers (Hershcovich et al., 2017; McDonald and Pereira, 2006; Dozat and Manning, 2018).",
      "startOffset" : 98,
      "endOffset" : 177
    }, {
      "referenceID" : 15,
      "context" : "In this paper, we adopt PERIN (Samuel and Straka, 2020), a state-of-theart graph-based parser capable of modeling a superset of graph features needed for our task.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "We briefly describe our modified SSA version, please consult the original work for more details (Samuel and Straka, 2020).",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "PERIN processes the input text in four steps, illustrated in Figure 2: 1) To encode the input, PERIN uses contextualized embeddings from XLM-R (base size; Conneau et al., 2020) and combines them with learned character-level embeddings; 2) each token is mapped onto latent queries by a linear transformation; 3) a stack of Transformer layers (Vaswani et al.",
      "startOffset" : 143,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : ", 2020) and combines them with learned character-level embeddings; 2) each token is mapped onto latent queries by a linear transformation; 3) a stack of Transformer layers (Vaswani et al., 2017) optionally models the inter-query dependencies; and 4) classification heads select and label queries onto nodes, establish anchoring from nodes to tokens, and predict the node-to-node edges.",
      "startOffset" : 172,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "The largest dataset is the NoReCfine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "EU and CA (Barnes et al., 2018) contain hotel reviews in Basque and Catalan, respectively.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "MPQA (Wiebe et al., 2005) annotates news wire text in English.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "Finally, DSU (Toprak et al., 2010) annotates English reviews of online universities.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "(2021) by removing the token, lemma, and POS embeddings and replacing mBERT (Devlin et al., 2019) with XLM-R (Conneau et al.",
      "startOffset" : 76,
      "endOffset" : 97
    } ],
    "year" : 0,
    "abstractText" : "This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions with the camera-ready version.",
    "creator" : null
  }
}
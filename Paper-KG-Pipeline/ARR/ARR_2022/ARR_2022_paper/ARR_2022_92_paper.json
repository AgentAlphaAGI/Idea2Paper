{
  "name" : "ARR_2022_92_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DISAPERE: A Dataset for Discourse Structure in Peer Review Discussions",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present DISAPERE, a labeled dataset of 20k sentences contained in 506 review-rebuttal pairs in English, annotated by experts. DISAPERE synthesizes label sets from prior work and extends them to include fine-grained annotation of the rebuttal sentences, characterizing their context in the review and the authors’ stance towards review arguments. Further, we annotate every review and rebuttal sentence.\nWe show that discourse cues from rebuttals can shed light on the quality and interpretation of reviews. Further, an understanding of the argumentative strategies employed by the reviewers and authors provides useful signal for area chairs and other decision makers."
    }, {
      "heading" : "1 Introduction",
      "text" : "Peer review performs the essential role of quality control in the dissemination of scientific knowledge. The recent rapid increase in academic output places an immense burden on decision makers such as area chairs and editors, as their decisions must take into account not only extensive manuscripts, but enormous additional amounts of technical text including reviews, rebuttals, and other discussions.\nOne long term goal of research in peer review is to support decision makers in managing their workload by providing tools to help them efficiently absorb the discussions they must read. While machine learning should not be used to produce condensed accounts of the peer review text due to the risk of amplifying biases (Zhao et al., 2017), ML tools could nevertheless help decision makers manage their information overload by identifying patterns\nin the data, such as argumentative strategies, goals, and intentions.\nAny such research requires an extensive labeled dataset. While the OpenReview platform (Soergel et al., 2013) has made it easy to obtain unlabeled public peer review text, labeling this data for supervised NLP requires highly qualified annotators. Correct interpretation of the discourse structure of the text requires the understanding of the technical content, precluding the use of standard crowdsourcing techniques. Prior work on discourse in peer review has focused this qualified labor force on labeling arguments extracted from the text, which enables the complete annotation of more examples, at the expense of research on non-argumentative behaviors in peer review. While there has been extensive research and deep analysis of different aspects of peer review, the taxonomies used to describe review argumentation are disparate and not directly compatible. Finally, there has been limited research into understanding the discourse relations between rebuttals and reviews (Cheng et al., 2020; Bao et al., 2021), and none so far into the discourse structure of rebuttals.\nThis paper presents DISAPERE (DIscourse Structure in Academic PEer REview), a dataset focusing on the interaction between reviewer and author. We give reviews and rebuttals equal importance, and emphasize the relations between them. To enable the study of behaviors beyond the core arguments, we also annotate every sentence of both the review and rebuttal, and provide fine-grained labels for non-argumentative types. We annotate at the sentence level not only for completeness but also to avoid the propagation of errors from argument detection. We annotate four properties (REVIEW-ACTION, FINE-REVIEWACTION, ASPECT, POLARITY) of each review sentence, where the set of properties and their values were developed by synthesizing taxonomies from prior work. We also annotate each sentence of a\nrebuttal with a fine-grained label indicating the author’s intentions and commitment, and a link to the set of review sentences that form its context. Figure 1 shows the DISAPERE annotation scheme on a minimal, fictional example review-rebuttal pair.\nDISAPERE is intended as a comprehensive and high-quality test collection, along with training data to fine-tune models. Our annotations are carried out by graduate students in computer science who have undergone training and calibration, amounting to over 850 person-hours of annotation work. Much of the test data is double-annotated, and we report inter-annotator agreement on all aspects of the annotation. We describe the performance of state of the art models on the tasks of predicting labels and contexts, showing that interesting ambiguities in the data provide the NLP community with research challenges. We also show an example that demonstrates how decision makers could use models like these to understand trends and inform policies for future conferences (§ 5).\nThe contributions of this paper are as follows: (1) a new labeled training dataset of 506 reviewrebuttal pairs (over 20k sentences) of peer review discussion text in English, where review sentences are annotated with four properties, and rebuttal sentences are annotated with context and labels from a novel scheme to describe discourse structure; (2) a taxonomy of discourse labels synthesizing prior work on discourse in peer review and extending it to add useful subcategories; (3) a summary of the performance of baseline models on this dataset\n(§ 6); (4) examples of analyses on this dataset that could benefit decision makers in peer review (§ 4), and (5) annotation software and extensive annotation guidelines to support future labeling efforts."
    }, {
      "heading" : "2 Related work",
      "text" : "The design of this dataset draws upon extensive, but disparate prior work on this topic. Many works, some addressed below, have taken advantage of the availability of review text hosted on OpenReview.\nArgument-level review labeling Prior work has developed label sets that address different phenomena. Hua et al. (2019) introduced the study of discourse structure in peer review by annotating argumentative propositions in the AMPERE dataset with a set of labels tailored to the peer review domain (EVALUATION, REQUEST, FACT, REFERENCE, and QUOTE). Similarly, Fromm et al. (2020)’s AMSR dataset frames the problem as an argumentation process, in which the stance of each argument towards the paper’s acceptance or rejection is of paramount importance. Both view peer review as argumentation, using argument mining techniques to highlight spans of interest.\nWhile its goal is not to examine discourse structure per se, Yuan et al. (2021) uses polarity labels to indicate each argument’s support or attack of the authors’ bid for acceptance. Besides polarity, these examples follow Chakraborty et al. (2020) by annotating each argument with the aspect of the\npaper it comments on.1 In contrast to Yuan et al. (2021), we do not attempt or recommend generating peer review text, instead focusing on analyzing human-generated text in peer review.\nReview-rebuttal interactions We also expand on work by Cheng et al. (2020), who first annotated discourse relations between sentences in reviews and rebuttals. While Cheng et al. (2020) present new deep learning architectures, in this paper we focus on the creation and comprehensive annotation of a new dataset, illustrated with results from some less specialized baseline models.\nOther research into rebuttals includes Gao et al. (2019). Besides their main finding that reviewers rarely change their rating in response to rebuttals, they find that more specific, convincing and explicit responses are more likely to elicit a score change. Observations from this paper are formalized into rebuttal action labels in DISAPERE.\nComparison of datasets In DISAPERE we attempted to unify these schemas to form a single hierarchical schema for review discourse structure. We then expanded this hierarchical schema to introduce fine-grained classes for implicit and explicit requests made by the reviewers. The details of the correspondence between DISAPERE labels and those from prior work are summarized in Appendix B. In contrast to prior work, DISAPERE labels discourse phenomena at a sentence level rather than an argument level. This enables more thorough coverage of the text while avoiding the propagation of errors from machine learning models earlier in the annotation pipeline. While using manually defined discourse units (above or below the sentence level) may more precisely capture some discourse information, a separate pass of discourse segmentation can hinder the use of discourse datasets, as achieving consistent and replicable annotation of argument units is known to be highly challenging (Trautmann et al., 2020), and also because few works actually tackle unit segmentation (Ajjour et al., 2017)."
    }, {
      "heading" : "3 Dataset",
      "text" : "Each example in DISAPERE consists of a pair of texts: a review and a rebuttal. Labels for reviews and rebuttal sentences are described below. Review sentence labels are summarized in Table 2, and rebuttal sentence labels in Table 3.\n1Aspects are based on the ACL 2018 rubric."
    }, {
      "heading" : "3.1 Review sentence labels",
      "text" : ""
    }, {
      "heading" : "3.1.1 Review actions",
      "text" : "REVIEW-ACTION annotations characterize a sentence’s intended function in the review. Annotators label each sentence with one of six coarsegrained sentence types including evaluative and fact sentences, request sentences (including questions, which are requests for information), as well as non-argument types: social, and structuring for organization of the text."
    }, {
      "heading" : "3.1.2 Fine-grained review actions",
      "text" : "We also extend two of these review actions with subtypes: structuring sentences include headers, quotations, or summarization sentences, and request sentences are subdivided by the nature of the request, distinguishing between clarification of factual information, requests for new experiments,\nrequests for an explanation (e.g. of motivations or claims), requests for edits, and identification of minor typos."
    }, {
      "heading" : "3.1.3 Aspect and polarity",
      "text" : "ASPECT annotations follow the ACL review form (Chakraborty et al., 2020; Yuan et al., 2021). These distinguish clarity, originality, soundness/correctness, replicability, substance, impact/motivation, and meaningful comparison. Following Yuan et al. (2021), arguments with an ASPECT are also annotated for POLARITY. We label positive and negative polarities. ASPECT and POLARITY are applied to sentences whose REVIEW-ACTION value is evaluative or request."
    }, {
      "heading" : "3.2 Rebuttal sentence labels",
      "text" : "We annotate two properties of each rebuttal sentence: a REBUTTAL-ACTION label characterizing its intent, and its CONTEXT in the review in the form of a subset of review sentences."
    }, {
      "heading" : "3.2.1 Rebuttal actions",
      "text" : "The 14 rebuttal actions (Table 3) are divided into three REBUTTAL-STANCE categories (concur, dis-\npute, non-arg) based on the author’s stance towards the reviewer’s comments.\n(1) concur: The author concurs with the premise of the context. This includes answering a question or discussing a requested change that has been made to the manuscript, conceding a criticism in an evaluative sentence. (2) dispute: The author disputes the premise of the context. The rebuttal sentence may reject a criticism or request, disagree with an underlying fact or assertion, or mitigate criticism (accepting a criticism while, e.g., arguing it to be offset by other properties). (3) non-arg: Encompasses rebuttal actions including social actions (such as thanking reviewers), structuring labels, for sentences that organize the review.\nResponses to requests are further annotated: if the author concurs, we record whether the task has been completed by the time of the rebuttal, or promised by the camera ready deadline; if the author disputes, we record whether the task was deemed to be out of scope for the manuscript."
    }, {
      "heading" : "3.2.2 Rebuttal context",
      "text" : "We refer to the set of sentences which a rebuttal sentence is responding to as the context of that\nrebuttal, with special labels for when referring to the entire review (global context) or the empty set (no context). By not mandating a fixed discourse chunking, these annotations may handle situations when some rebuttal sentences respond to large sections of text, and other rebuttal sentences respond to specific sentences within those sections."
    }, {
      "heading" : "3.3 Data Source and Annotation",
      "text" : "DISAPERE uses English text from scientific discussions on OpenReview (Soergel et al., 2013), which makes peer review reports available for research purposes. We draw review-rebuttal pairs from the International Conference on Learning Representations (ICLR) in 2019 and 2020, resulting in text within the domain of machine learning research. Review-rebuttal pairs are split into train, development and test sets in a 3:1:2 ratio such that all texts associated with any manuscript occur in the same subset. Overall statistics for the dataset are summarized in Table 4.\nAuthors are able to respond to each ICLR review by adding a comment. Although rebuttals are not formally named, we consider direct replies by the author to the initial review comment to constitute a rebuttal. While multi-turn interactions are possible, we focus on reviews and initial responses, and leave study of extended discussion for future work. The text is separated into sentences using the spaCy (Honnibal and Montani, 2017) sentence separator.\nAnnotation was accomplished with a custom annotation tool designed for this task (described in\ndetail in Appendix C)2 Annotators annotate each sentence of a review, then examine the rebuttal sentences in order, selecting sets of review sentences to form their context. While this linking between sentences does not explicitly align multisentence chunks as in pipelined approaches to discourse alignment (Cheng et al., 2020), we note that since multiple sentences may be aligned to the same set of sentences in the review, some discourse structure is nevertheless latently implied."
    }, {
      "heading" : "3.4 Agreement",
      "text" : "We report Cohen’s κ (Cohen, 1960) on the IAA of labeling both review and rebuttals, treating each sentence as a labeling unit (Table 5). The annota-\n2This tool will be released upon publication.\ntors for each example are selected randomly from the pool of 10 annotators. Cohen’s κ is calculated for sentences annotated at least twice. Where more than two annotations were produced, we calculate κ between all pairs and normalize by the number of possible pairs. The results show between moderate and substantial chance-corrected agreement between annotators, for both REBUTTAL-ACTION and REBUTTAL-STANCE labels (Appendix E provides details about agreement on context sentences). While these IAA scores do illustrate the noise of the task, note that this is not highly unusual for discourse labeling tasks – e.g. Habernal and Gurevych (2017) and Miller et al. (2019) both report αu between 0.4 and 0.5."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Context types",
      "text" : "We separate the different types of rebuttal contexts in terms of the number and relative position of selected review sentences in Table 6, along with the four cases in which the context cannot be described as a subset of review sentences."
    }, {
      "heading" : "4.2 Alignment",
      "text" : "Since authors often respond to reviewers’ points in order, one would expect alignment between rebuttal and review sentences to be trivial, yet this is not often the case. In Figure 2, we calculate Spearman’s ρ between rebuttal sentence indices and their aligned review sentence indices. Rebuttals responding to each point in order would achieve ρ = 1.0. This is rare; instead, we find that ρ is even negative in some cases. This indicates that while linear inductive bias may be beneficial, the task of determining rebuttal sentences’ contexts is not trivial."
    }, {
      "heading" : "4.3 Author interpretations of criticism",
      "text" : "The REBUTTAL-ACTION labels were designed in the context of REVIEW-ACTION labels, and thus tend to explicitly refer to different responses to criticism (accept, reject, mitigate) or to requests (answer, refute, etc.) However, annotations revealed that authors often interpret a review sentence in ways that support their argumentative goals rather than a general reader’s interpretation, such as treating a negative evaluative statement as a request for an improvement. Figure 3 shows the distribution of contexts for three different rebuttal actions."
    }, {
      "heading" : "5 Application: Agreeability",
      "text" : "Gao et al. (2019) showed that reviewers do not appear to act upon the rebuttals responding their reviews. It is possible that this is due to paucity of time on the reviewers’ part. It is also common practice for area chairs to use review variance across a manuscript’s reviews as a practical heuristic to decide which manuscripts need their attention. We propose that discourse information such as that described by DISAPERE can be used to provide\nheuristics that are data-driven, yet interpretable, and leverage information from the content of reviews rather than just numerical scores, resulting in better decision making.\nOne such measure is agreeability, which we define as the ratio of CONCUR sentences to argumentative sentences, i.e.: agreeability = nconcurnconcur+ndispute . We argue that low agreeability can indicate problematic reviews even in cases where the variance in scores does not reveal an issue, as illustrated in Figure 4. Agreeability is only weakly correlated with rating, with Pearson’s r = 0.347. In Figure 4, 18% (28/159) of manuscripts would not meet the bar for high variance scores (top quartile), although their low agreeability (bottom quartile) indicates that area chairs may want to pay closer attention3."
    }, {
      "heading" : "6 Baselines",
      "text" : "Two types of machine learning tasks can be defined in DISAPERE. First, a sentence-level classification task for each of the four review labels and the two levels of rebuttal labels. Second, an alignment task in which, given a rebuttal sentence, the set of review sentences that form its context are to be predicted.\nThe models described below are not intended to introduce innovations in discourse modeling, rather,\n3Two such examples from DISAPERE can be found here and here.\nwe intend to show the off-the-shelf performance of state-of-the-art models, and indicate through error analysis the phenomena that are yet to be captured."
    }, {
      "heading" : "6.1 Sentence classification",
      "text" : "For the six classification tasks, we use bert-base (Devlin et al., 2019) to produce sentence embeddings for each sentence, then classify the representation of the [CLS] token using a feedforward network.\nWe report macro-averaged F1 scores, shown in Table 7. In general, F1 is lower for tasks with larger label spaces. While the performance is reasonable in most cases, there is still room for improvement. ASPECT achieves a particularly low F1 score, but its κ is within the bounds of moderate agreement, this must be accounted for by the inherent difficulty of the task rather than a deficit in data quality.\nAs one might expect, errors in the classification\nresults largely mirror disagreements in the annotations, which in turn reflect particularly ambiguous utterances. One example is the occurrence of rhetorical questions, such as (1) in Table 8, incorrectly labeled as request instead of evaluative. In fact, for sentences such as (1), additional context would disambiguate its type: the reviewer answers the question in the next sentence, and hence both sentences were labeled evaluative. Similarly, (2) was labeled ‘fact’, but since it is an integral part of a reviewer’s argument against the soundness of the paper, should have been labeled ‘evaluative’. Certain reviewers also use conventions that do not fit the general schema we observed when developing DISAPERE. For example, (3), an opinionated heading, could be considered both structuring and evaluative. Finally, certain lexical cues a model may pick up on can be quite subtle. For example, though they share a prefix, sentences (4) and (5) are clearly evaluative and request respectively."
    }, {
      "heading" : "6.2 Rebuttal context alignment",
      "text" : "We model rebuttal context alignment as a ranking task. Ideally, a model should rank all relevant review sentences higher than non-relevant review sentences. As a baseline, we use an information retrieval (IR) model based on BM25 that, given a rebuttal sentence ranks all the corresponding review sentences. We also report results from a neural sentence alignment model based on a twotower Siamese-BERT (S-BERT) model (Reimers and Gurevych, 2019). We add a NO_MATCH sentence to the review, to which rebuttal sentences without context sets in the review are aligned. Then, each review and rebuttal sentence is encoded independently using a S-BERT encoder and the similarity between two sentences is computed using cosine similarity. We initialize with a model4 pre-trained on various sentence-pair datasets. Alignment is evaluated using mean reciprocal rank (MRR).\nThe neural model outperforms the IR baseline indicating that simple lexical matching models\n4We initialize from a sentence-transformers/all-MiniLML6-v2 model\nare not enough to achieve good performance on this task. However, both the neural and IR model achieve a relatively low MRR, indicating that there is significant scope for improvement; in particular that similarity as encoded by lexical feature or cosine similarity in large language models’ latent space is not a good proxy for relatedness in the sense encoded in the rebuttal context annotations.\nThe neural model predicts a contiguous set of two or more sentences as its top choices only 8.36% of the time, although the prevalence of contiguous sentence contexts is 60% (Table 6). This indicates the need for inductive bias in models for this task, and the shortcomings of modeling each context decision independently. Further, the model predicts ‘no context’ 4.7% of the time, underestimating the prevalence of such sentences."
    }, {
      "heading" : "7 Conclusion",
      "text" : "As the burden of academic peer reviewing grows, it is important for program chairs and editors to act upon data-driven insights rather than heuristics, to make the best possible use of participants’ scarce time. Models trained on data like DISAPERE will allow decision makers to glean deep insights on the interactions occurring during peer review.\nAlmost all publicly available peer review data is from the domain of artificial intelligence, limiting the scope of DISAPERE and any similar project. While this means that models trained on DISAPERE won’t necessarily generalize to all new domains, we hope that with the detailed annotation guidelines and seamless data collection using the software provided with this paper support, users can build on our work, and ensure that their insights are robust to differences over time and across fields."
    }, {
      "heading" : "8 Ethics",
      "text" : "The outcomes of peer review can have outsize effects on the careers of participating scholars. As machine learning models are known to amplify biases, we strongly recommend against using the outputs of any machine learning system to make decisions about individual cases. A dataset like DISAPERE is best used to survey participants’ behavior. Any interventions based on this information should be subjected to studies in order to ensure that they do not introduce or exacerbate bias."
    }, {
      "heading" : "A Example analysis with rating",
      "text" : "Figure 5 shows one possible analysis taking into account the rating of the review. We show the distribution of FINE-REVIEW-ACTION labels of requests with rating. It appears that high-scoring manuscripts are rarely asked to add experiments, and are polished enough to not elicit requests to fix typos. Interestingly, low-scoring manuscripts have the second-lowest occurrence of typo requests, which could possibly be attributed to the preponderance of other requests, but this bears further examination."
    }, {
      "heading" : "B Rationale for taxonomy construction",
      "text" : "Our label sets leverage ideas from and commonalities between existing work in this domain, including AMPERE (Hua et al., 2019), AMSR (Fromm et al., 2020) ASAP-Review (Yuan et al., 2021), and Gao et al. (2019):\n• ASAP-Review’s polarity labels approximately correspond to arg-pos and arg-neg labels in AMSR\n• AMSR and AMPERE each label nonargumentative sentences in a similar manner\n• aspect labels from ASAP-Review apply only to certain types of sentences; namely request and evaluative sentences from AMPERE’s taxonomy.\n• summary is an exception among ASAPReview’s aspects, behaving similarly to AMPERE’s quote. We thus include both of these under a structuring category.\n• Further, in order to gauge the extent to which authors acquiesced to reviewers’ requests, we introduce a fine-grained categorization of the types of requests.\n• Gao et al. (2019) enumerates some features of rebuttals, including expressing gratitude,\npromising revisions, and disagreeing with criticisms. We formalize these observations into our rebuttal label taxonomy."
    }, {
      "heading" : "C Annotation tool",
      "text" : "Two modes of annotation are possible. First, annotators can apply labels on a sentence-by-sentence basis. Multiple labeling schemas can be annotated simulatenously, with the option of adding constraints so that certain values govern possible values for other properties. This annotation mode is shown in Figure 6.\nThe second annotation mode can build on the output of the first annotation mode. Here, sentences of a focus text (the rebuttal) are presented in sequence, and annotators are permitted to select one or more of the sentences in the reference text (the review) which form the context of the sentence of the focus text. Further, a label can be applied to the alignment. This annotation mode is shown in Figure 7 and Figure 8."
    }, {
      "heading" : "D Annotated review-rebuttal pair",
      "text" : "Figure 9 shows a truncated version of a reviewrebuttal pair from the train set of DISAPERE."
    }, {
      "heading" : "E Context overlap analysis",
      "text" : "As a proxy for agreement of rebuttal spans, we show the types of overlap between spans selected by two annotators in Table 10.\nType of context overlap Num. rebuttal sentences % rebuttal sentences"
    }, {
      "heading" : "F Additional Agreement Analysis",
      "text" : "While some of the IAA scores on annotation are low, we note that the labels used in this task attempt\nto characterize relatively complex relationships in text. To give more insight into such disagreements, Figure 10 provides a confusion matrix regarding the REBUTTAL-ACTION labels. Recognizing that there are often situations in which users of a dataset will hope to reduce a label set, we provide some guidance as to which such merges may be acceptable and which are not.\nMany disagreements come from three labels which might be said to exist upon a continuum – ANSWER, MITIGATE CRITICISM and REJECT CRITICISM. We suggest that in the situation of needing to minimize IAA disagreement, one might consider first merging mitigate criticism into reject criticism. The kind of disagreements seen between the two are understandable but nuanced: the difference between saying that the reviewer has a point (but that they disagree on the relevance of that point) and disagreeing with the point itself. Out-of-context rebuttal sentences illustrating this are provided below as examples of this kind of ambiguous situation:\n• We note that such rules are indeed limited to some extent, but they still capture a rather expressive fragment of answer set programs\nwith restricted forms of external computations\n• The use of Cvalp for hyperparameter tuning was incidental and not a central point of our paper.\n• We agree that the measure theoretic approach is not always necessary (indeed for angular actions, it is not needed), but it is necessary for a very common scenario – clipped actions.\nFurthermore, we note that (as illustrated in the confusion matrix) a wide range of disagreements are hard to distinguish from “answer” labels, as authors often attempt to frame disagreements as simple answers to questions."
    } ],
    "references" : [ {
      "title" : "Unit segmentation of argumentative texts",
      "author" : [ "Yamen Ajjour", "Wei-Fan Chen", "Johannes Kiesel", "Henning Wachsmuth", "Benno Stein." ],
      "venue" : "Proceedings of the 4th Workshop on Argument Mining, pages 118– 128, Copenhagen, Denmark. Association for Compu-",
      "citeRegEx" : "Ajjour et al\\.,? 2017",
      "shortCiteRegEx" : "Ajjour et al\\.",
      "year" : 2017
    }, {
      "title" : "Argument pair extraction with mutual guidance and inter-sentence relation graph",
      "author" : [ "Jianzhu Bao", "Bin Liang", "Jingyi Sun", "Yice Zhang", "Min Yang", "Ruifeng Xu." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Bao et al\\.,? 2021",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2021
    }, {
      "title" : "Aspect-based sentiment analysis of scientific reviews",
      "author" : [ "Souvic Chakraborty", "Pawan Goyal", "Animesh Mukherjee." ],
      "venue" : "Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020.",
      "citeRegEx" : "Chakraborty et al\\.,? 2020",
      "shortCiteRegEx" : "Chakraborty et al\\.",
      "year" : 2020
    }, {
      "title" : "APE: Argument pair extraction from peer review and rebuttal via multi-task learning",
      "author" : [ "Liying Cheng", "Lidong Bing", "Qian Yu", "Wei Lu", "Luo Si." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Argument mining driven analysis of peerreviews",
      "author" : [ "Michael Fromm", "Evgeniy Faerman", "Max Berrendorf", "Siddharth Bhargava", "Ruoxia Qi", "Yao Zhang", "Lukas Dennert", "Sophia Selle", "Yang Mao", "Thomas Seidl." ],
      "venue" : "arXiv preprint arXiv:2012.07743.",
      "citeRegEx" : "Fromm et al\\.,? 2020",
      "shortCiteRegEx" : "Fromm et al\\.",
      "year" : 2020
    }, {
      "title" : "Does my rebuttal matter? insights from a major NLP conference",
      "author" : [ "Yang Gao", "Steffen Eger", "Ilia Kuznetsov", "Iryna Gurevych", "Yusuke Miyao." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Argumentation mining in user-generated web discourse",
      "author" : [ "Ivan Habernal", "Iryna Gurevych." ],
      "venue" : "Computational Linguistics, 43(1):125–179.",
      "citeRegEx" : "Habernal and Gurevych.,? 2017",
      "shortCiteRegEx" : "Habernal and Gurevych.",
      "year" : 2017
    }, {
      "title" : "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani." ],
      "venue" : "To appear.",
      "citeRegEx" : "Honnibal and Montani.,? 2017",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Argument mining for understanding peer reviews",
      "author" : [ "Xinyu Hua", "Mitko Nikolov", "Nikhil Badugu", "Lu Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hua et al\\.,? 2019",
      "shortCiteRegEx" : "Hua et al\\.",
      "year" : 2019
    }, {
      "title" : "A streamlined method for sourcing discourselevel argumentation annotations from the crowd",
      "author" : [ "Tristan Miller", "Maria Sukhareva", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Miller et al\\.,? 2019",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Open scholarship and peer review: a time for experimentation",
      "author" : [ "David Soergel", "Adam Saunders", "Andrew McCallum" ],
      "venue" : null,
      "citeRegEx" : "Soergel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Soergel et al\\.",
      "year" : 2013
    }, {
      "title" : "Fine-grained argument unit recognition and classification",
      "author" : [ "Dietrich Trautmann", "Johannes Daxenberger", "Christian Stab", "Hinrich Schütze", "Iryna Gurevych." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9048–9056.",
      "citeRegEx" : "Trautmann et al\\.,? 2020",
      "shortCiteRegEx" : "Trautmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Can we automate scientific reviewing",
      "author" : [ "Weizhe Yuan", "Pengfei Liu", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Yuan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2021
    }, {
      "title" : "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
      "author" : [ "Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    }, {
      "title" : "ASAP-Review’s polarity labels approximately correspond to arg-pos and arg-neg labels in AMSR",
      "author" : [ "Gao" ],
      "venue" : null,
      "citeRegEx" : "Gao,? \\Q2019\\E",
      "shortCiteRegEx" : "Gao",
      "year" : 2019
    }, {
      "title" : "2019) enumerates some features of rebuttals, including expressing gratitude",
      "author" : [ "• Gao" ],
      "venue" : null,
      "citeRegEx" : "Gao,? \\Q2019\\E",
      "shortCiteRegEx" : "Gao",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "While machine learning should not be used to produce condensed accounts of the peer review text due to the risk of amplifying biases (Zhao et al., 2017), ML tools could nevertheless help decision makers manage their information overload by identifying patterns in the data, such as argumentative strategies, goals, and intentions.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "While the OpenReview platform (Soergel et al., 2013) has made it easy to obtain unlabeled public peer review text, labeling this data for supervised NLP requires highly qualified annotators.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "Finally, there has been limited research into understanding the discourse relations between rebuttals and reviews (Cheng et al., 2020; Bao et al., 2021), and none so far into the discourse structure of rebuttals.",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "Finally, there has been limited research into understanding the discourse relations between rebuttals and reviews (Cheng et al., 2020; Bao et al., 2021), and none so far into the discourse structure of rebuttals.",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "While using manually defined discourse units (above or below the sentence level) may more precisely capture some discourse information, a separate pass of discourse segmentation can hinder the use of discourse datasets, as achieving consistent and replicable annotation of argument units is known to be highly challenging (Trautmann et al., 2020), and also because few works actually tackle unit segmentation (Ajjour et al.",
      "startOffset" : 322,
      "endOffset" : 346
    }, {
      "referenceID" : 0,
      "context" : ", 2020), and also because few works actually tackle unit segmentation (Ajjour et al., 2017).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "Table 1: Comparison between our dataset and prior work: AMPERE (Hua et al., 2019), AMSR (Fromm et al.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : ", 2019), AMSR (Fromm et al., 2020), ASAP-Review (Yuan et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : ", 2020), ASAP-Review (Yuan et al., 2021), APE (Cheng et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "ASPECT annotations follow the ACL review form (Chakraborty et al., 2020; Yuan et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "ASPECT annotations follow the ACL review form (Chakraborty et al., 2020; Yuan et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "DISAPERE uses English text from scientific discussions on OpenReview (Soergel et al., 2013), which makes peer review reports available for research purposes.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "The text is separated into sentences using the spaCy (Honnibal and Montani, 2017) sentence separator.",
      "startOffset" : 53,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "While this linking between sentences does not explicitly align multisentence chunks as in pipelined approaches to discourse alignment (Cheng et al., 2020), we note that since multiple sentences may be aligned to the same set of sentences in the review, some discourse structure is nevertheless latently implied.",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "We report Cohen’s κ (Cohen, 1960) on the IAA of labeling both review and rebuttals, treating each sentence as a labeling unit (Table 5).",
      "startOffset" : 20,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "For the six classification tasks, we use bert-base (Devlin et al., 2019) to produce sentence embeddings for each sentence, then classify the representation of the [CLS] token using a feedforward network.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "We also report results from a neural sentence alignment model based on a twotower Siamese-BERT (S-BERT) model (Reimers and Gurevych, 2019).",
      "startOffset" : 110,
      "endOffset" : 138
    } ],
    "year" : 0,
    "abstractText" : "At the foundation of scientific evaluation is the labor-intensive process of peer review. This critical task requires participants to consume vast amounts of highly technical text. Prior work has annotated different aspects of review argumentation, but discourse relations between reviews and rebuttals have yet to be examined. We present DISAPERE, a labeled dataset of 20k sentences contained in 506 review-rebuttal pairs in English, annotated by experts. DISAPERE synthesizes label sets from prior work and extends them to include fine-grained annotation of the rebuttal sentences, characterizing their context in the review and the authors’ stance towards review arguments. Further, we annotate every review and rebuttal sentence. We show that discourse cues from rebuttals can shed light on the quality and interpretation of reviews. Further, an understanding of the argumentative strategies employed by the reviewers and authors provides useful signal for area chairs and other decision makers.",
    "creator" : null
  }
}
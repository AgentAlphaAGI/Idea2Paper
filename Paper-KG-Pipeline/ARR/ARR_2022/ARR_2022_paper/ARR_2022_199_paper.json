{
  "name" : "ARR_2022_199_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "With the widespread adoption of task-oriented dialogue (TOD) systems, these need to support an everincreasing variety of services/APIs. Since many service developers lack the resources to collect labeled data or the requisite ML expertise, zero/fewshot transfer to unseen services becomes critical to the democratization of dialogue agents.\nNew approaches to TOD that can generalize to new services mainly rely on combining two techniques: large language models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), and schema-guided modeling i.e. using natural language descriptions of schema elements (intents and slots) as model inputs to enable inference on unseen services (Rastogi et al., 2020a,b). Models combining the two currently show state-of-the-art results on dialogue state tracking (DST) (Heck et al., 2020; Lee et al., 2021a; Anon, 2021).\nHowever, description-based schema representations have drawbacks: precise natural language descriptions still take manual effort and can be tricky to write, while only constituting indirect supervision for unseen services compared to an example dialogue. Furthermore, Lee et al. (2021b) showed that state-of-the-art schema-guided DST models may not be robust to variation in schema descriptions, causing significant accuracy drops.\nAlternatively, we propose using a single dialogue example (with final state annotations) in place of the service schema representation, similar to oneshot priming (Brown et al., 2020). Rather than telling the model about schema element semantics in natural language, we aim to show the schema through a demonstration, as in Figure 1. Our approach, \"Show, Don’t Tell (SDT),\" when applied to two SotA DST models, offers consistently superior accuracy and generalizes better to new APIs across both the SGD (Rastogi et al., 2020b) and MultiWoZ-Leave-One-Out (Budzianowski et al., 2018; Lin et al., 2021b) benchmarks, while being more data-efficient and robust to schema variations."
    }, {
      "heading" : "2 Show, Don’t Tell",
      "text" : "Following SoTA models, we pose DST as a seq2seq task (Wu et al., 2019; Zhao et al., 2021a), where the input language model (in our case, T5) is finetuned on the training set for a DST dataset. During finetuning and evaluation, the model input consists of a prompt and context, and the target contains ground truth belief states. We consider two models/prompt formats as our baselines:\n• T5-ind (Lee et al., 2021a): Model input comprises of the dialogue history as context concatenated with one slot description as the prompt. The target is the value of that slot in the dialogue state. Inference must be done per slot i.e. values for different slots are independently decoded.\n• T5-seq (Anon, 2021): Model input comprises\nthe descriptions of all slots as the prompt, followed by the dialogue history as the context. The target is the sequence of slot-value pairs in the dialogue state. In other words, the dialogue state is decoded sequentially in a single pass.\nWe modify the above prompt formats to include demonstrations instead of descriptions as follows. The new example-based prompt formats are described below and illustrated in Figure 1.\n• SDT-ind: A prompt Pindi comprises a single labeled slot value pair for slot i formatted as\nPindi = [ex]; d ind i ; [slot]; svi\nwhere dindi is a single user utterance indicating a value for slot i, and svi is the slot value pair. [ex], [slot] are special delimiter tokens.\n• SDT-seq: A prompt Pseq comprises a single labeled dialogue turn formatted as:\nPseq = [ex]; d1; ...; dn; [slot]; sv1; ...; svm\ni.e. the prompt is constructed by concatenating all utterances in the example dialogue followed by all slot-value pairs in the final dialogue state.\nFor all prompt formats (T5-* and SDT-*), we format the values for categorical slots (taking one of a fixed set of values) as a multiple-choice question.\nThe context in both prompt formats is a concatenation of the dialogue history for the current training example. The final model input is formed by concatenating the prompt and the context strings. The target string is unchanged, containing only the value for the specific slot for independent decoding\nand the turn belief state for sequential decoding. More details on the prompt design and its impact on performance are provided in Appendix C. Formulating prompt examples: Given neither SDT prompt format contains slot descriptions, it is imperative that the prompt(s) contain enough semantic information to infer values for all slots in the schema. This is easy for SDT-ind, which uses a separate prompt for each slot. However, for SDTseq, we ensure that the chosen example dialogue contains annotations for all slots in that schema."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "Datasets: We conduct experiments on two DST benchmarks: Schema-guided Dialogue (SGD) (Rastogi et al., 2020b) and MultiWOZ 2.1 (Budzianowski et al., 2018; Eric et al., 2019). For MultiWOZ, we evaluate on the cross-domain transfer setup from Wu et al. (2019); Lin et al. (2021a), where models are trained on all domains but one and evaluated on the holdout domain. For SGD, we created prompt dialogues manually, with 5.9 turns on average, compared to 15.3 average turns in the SGD single-domain dataset. For MultiWoZ, we selected a short dialogue containing all slots from each holdout domain’s training set for the prompt. Implementation: We train SDT models by finetuning pretrained T5 1.1 checkpoints of various sizes. For both datasets, we select one example prompt per service schema (for SDT-seq) or slot (for SDT-ind), and use the same prompt for all examples for that service/slot across training and evaluation. Unless otherwise noted, all T5-based models (T5/SDT-seq/ind) are finetuned on T5-XXL (11B parameters). Appendices A and B have more details on training and baselines respectively."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Results on SGD",
      "text" : "Table 1 contains results on the SGD test set. Since SDT results may depend on the choice of example turn/dialogue provided in the prompt, 5 different versions of prompts are created for each service using different examples. The reported results are obtained by averaging the JGA across these versions. SDT-seq achieves the highest JGA, showing major gains, particularly over unseen services, over its description-based counterpart T5-seq and the next-best model T5-ind. SDT-ind is comparable to its counterpart T5-ind, and better than T5-seq.\nBased on these results, a single dialogue example appears more effective than using natural language descriptions. By its construction, the SDTind prompt format is unable to model phenomena such as coreference, a limitation not faced by SDTseq which can jointly model all slots in a service. Further finetuning T5-seq: To evaluate T5-seq in a scenario where it can access the dialogue examples used for SDT-seq prompts, We try further finetuning T5-seq on this exact set of dialogue examples. This model, therefore, gets slot descriptions as well as the demonstrations to finetune on. The model obtains a JGA of 87.7% on SGD, level with T5-ind but still lower than SDT-seq, indicating dialogue examples are better used as prompts (Le Scao and Rush, 2021). Interestingly, finetuning on more than one dialogue example does not help."
    }, {
      "heading" : "4.2 MultiWOZ Results",
      "text" : "Table 2 summarizes results for the MultiWOZ 2.1 leave-one-out transfer setup. Comparing T5-seq and SDT-seq, both finetuned over T5-XXL, the latter achieves state-of-the-art results on 3 of 5 domains and overall for this task, and the former performs best for the remaining 2 domains."
    }, {
      "heading" : "4.3 Impact of Model Size",
      "text" : "T5-XXL may be too large/slow for a number of settings, so we look SDT’s performance on SGD across more T5 model sizes in Table 3. For the base and large model sizes, SDT variants offer consistently higher JGA than their description-based counterparts. SDT-ind fares better than SDT-seq, possibly due to smaller T5 models being less capable of inferring unseen slots with just a description."
    }, {
      "heading" : "4.4 Data Efficiency",
      "text" : "To examine the data efficiency of SDT models, we train SDT-seq in a low-resource setting with 0.16% (10-shot), 1%, and 10% of the SGD training data and evaluating on the entire test set. For 10-shot, we randomly sample 10 dialogues from every service; for 1% and 10%, we sample uniformly from the full dataset. Results from Table 4 demonstrate far higher training data efficiency for SDT-seq."
    }, {
      "heading" : "4.5 Robustness",
      "text" : "Large LMs are often sensitive to the choice of prompt (Zhao et al., 2021b; Reynolds and McDonell, 2021). To this end, we evaluate SDT-seq on the SGD-X (Lee et al., 2021b) dataset, which includes 5 variant schemas with paraphrased slot names and descriptions. Table 5 shows SDT-seq achieves the highest average JGA (JGAv1−5) and lowest schema sensitivity (SSJGA), indicating it\nis the most robust of the compared models. Note, however, that the JGA drop indicates SDT-seq is still sensitive to slot name variations."
    }, {
      "heading" : "5 Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Writing descriptions vs. demonstrations",
      "text" : "We note that the information provided to SDT is not identical to what is provided to usual schemaguided models, as SDT trades out natural language descriptions in exchange for a demonstration of how to identify slots in a dialogue. However, we argue that from a developer’s point of view, creating a single example is a similar amount of effort as writing descriptions, so we consider the methods to be comparable. For creating the SDT-seq prompts for each service in SGD, an experienced annotator took ∼2 hours, compared to ∼90 minutes for generating descriptions for all slots in all services. SDT-ind prompts are arguably even simpler to write.\nDescriptions, however, have their advantages: they are agnostic to the model architecture and writing them does not require knowledge of dialogue systems, which SDT-seq prompts does. Given the performance gain, however, example-based prompts may be a better choice for many settings, especially for smaller model sizes."
    }, {
      "heading" : "5.2 Error analysis",
      "text" : "Figure 2 contains some common error patterns in predictions from T5-seq and SDT-seq. These indicate that SDT benefits from having a better un-\nderstanding of the context around unseen slots, or when slot descriptions are too similar to one another (#1). However, SDT can be limited by its prompt: for instance, in #3 it has only seen context for the other categorical value for slot event_type ."
    }, {
      "heading" : "6 Related Work",
      "text" : "Prior approaches focused on framing DST as question answering (Ruan et al., 2020; Ma et al., 2020; Zhang et al., 2021). Many MultiWoZ cross-domain models leverage slot names/descriptions (Wu et al., 2019; Lee et al., 2019; Lin et al., 2021a).\nPretrained generative LLMs (Raffel et al., 2020; Brown et al., 2020) have enable framing NLP tasks as seq2seq problems. Some DST papers (Zhao et al., 2021a; Feng et al., 2021) look at settings with no train-test discrepancy. Many studies explore the efficacy of task-specific prompts (Jiang et al., 2020; Liu et al., 2021). Madotto et al. (2020) prime LMs with examples for dialogue tasks, but without finetuning. Wei et al. (2021) fine-tune language models to understand prompts for a different task."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We study the use of demonstrations as LM prompts to convey the semantics of APIs in lieu of natural language descriptions for TOD. Even though they take similar effort to construct, they outperform description-based prompts in our experiments across DST datasets (SGD and MultiWOZ), model sizes, and training data sizes, while being more robust to changes in schemata. This work provides developers of TOD systems with more options for API representations to enable transfer to unseen services. In future work, we would like to explore this representation for other TOD tasks (e.g. dialogue management and response generation)."
    }, {
      "heading" : "8 Ethical Considerations",
      "text" : "We proposed a more efficient way of building TOD systems by leveraging demonstrations in place of descriptions, leading to increased accuracy with minimal/no data preparation overhead. We conduct our experiments on publicly-available TOD datasets in English, covering domains which are popular for building conversational agents for. We are hopeful our work leads to building more accurate TOD systems with similar or less overhead, and encourages further research in the area."
    }, {
      "heading" : "A SDT Model Details",
      "text" : "All T5 checkpoints used are available publicly1. For all experiments, we use a sequence length of 2048, dropout of 10% and a batch size of 16. We used a constant learning rate of 1e − 3 or 1e − 4. All models were trained for 50k steps or until convergence, and each experiment was conducted on either 64 or 128 TPU v3 chips (Jouppi et al., 2017)."
    }, {
      "heading" : "B Baseline Models",
      "text" : "For SGD, we compare against SGP-DST (Ruan et al., 2020), MRC+WD-DST (Ma et al., 2020), T5seq (Anon, 2021) and T5-ind (Lee et al., 2021a).\nFor MultiWOZ, we compare against TRADE (Wu et al., 2019), SUMBT (Lee et al., 2019), TransferQA (Lin et al., 2021a), and T5-seq.\nTransfer QA is based on T5-large, and T5-ind and T5-seq are based on T5-XXL in this paper unless otherwise noted."
    }, {
      "heading" : "C Prompt Design",
      "text" : "We experimented with various formats for the SDT prompt before arriving at the final format. Below, we list alternative designs that we tried and their impact on JGA, as evaluated on the SGD test set.\nC.1 Categorical value strings vs. multiple choice answers\nWe found that JGA dropped -2% when we tasked the model with decoding categorical values instead of multiple choice answers - e.g. payment_method=debit card instead of payment_method=b (where b is linked to the value debit card in the prompt as described in Section 2). We found that when tasking the model to decode categorical values, it would often decode related yet invalid values, which we counted as false in our evaluation. For example, instead\n1https://github.com/google-research/ text-to-text-transfer-transformer/blob/ main/released_checkpoints.md\nof debit card, the model might decode bank balance.\nC.2 Slot IDs vs. slot names When we delexicalized slot names with slot IDs, JGA dropped -5%. One downside of this approach is that the model lost access to valuable semantic information conveyed by the slot name. Another downside is that the model could not distinguish two slots that had the same value in the prompt. For example, if the prompt was \"I would like a petfriendly hotel room with wifi\" and the corresponding slots were 1=True (has_wifi) and 2=True (pets_allowed), it is ambiguous which ID refers to which slot.\nThe potential upside of using slot IDs was to remove dependence on the choice of slot name, but this did not succeed for the reasons above.\nC.3 Decoding active slots vs. all slots We experimented with training the model to only decode active slots rather than all slots with none values when they were inactive. JGA dropped - 0.4%, which we hypothesized could be a result of greater dissimilarity between the slot-value string in the prompt (which contained all slots by construction) and the target, which only contained a subset of slots.\nC.4 In-line annotations vs. dialogue+slots concatenated\nWe hypothesized that bringing the slot annotation in the prompt closer to where it was mentioned in the dialogue might help the model better understand the slot’s semantic meaning. We changed the format as follows:\n• Original: [example] [user] I would like a pet-friendly hotel room with wifi [system] I found ... [slot] has_wifi=True\n• In-line: [example] [user] I would like a pet-friendly hotel room with wifi [has_wifi=True] [system] I found ...\nHowever, this decreased JGA by more than - 20%. We hypothesized that this was likely due to a mismatch between the prompt’s annotations and the target string format, which remained the same."
    } ],
    "references" : [ {
      "title" : "Description-driven task-oriented dialog state tracking",
      "author" : [ "Anon." ],
      "venue" : "ACL Rolling Review, November 2021.",
      "citeRegEx" : "Anon.,? 2021",
      "shortCiteRegEx" : "Anon.",
      "year" : 2021
    }, {
      "title" : "Multiwoz–a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling",
      "author" : [ "Paweł Budzianowski", "Tsung-Hsien Wen", "Bo-Hsiang Tseng", "Inigo Casanueva", "Stefan Ultes", "Osman Ramadan", "Milica Gašić." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Budzianowski et al\\.,? 2018",
      "shortCiteRegEx" : "Budzianowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Zero-shot transfer learning with synthesized data for multi-domain dialogue state tracking",
      "author" : [ "Giovanni Campagna", "Agata Foryciarz", "Mehrad Moradshahi", "Monica Lam." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Campagna et al\\.,? 2020",
      "shortCiteRegEx" : "Campagna et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669",
      "author" : [ "Mihail Eric", "Rahul Goel", "Shachi Paul", "Abhishek Sethi", "Sanchit Agarwal", "Shuyag Gao", "Dilek HakkaniTur" ],
      "venue" : null,
      "citeRegEx" : "Eric et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2019
    }, {
      "title" : "A sequenceto-sequence approach to dialogue state tracking",
      "author" : [ "Yue Feng", "Yang Wang", "Hang Li." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Feng et al\\.,? 2021",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "TripPy: A triple copy strategy for value independent neural dialog state tracking",
      "author" : [ "Michael Heck", "Carel van Niekerk", "Nurul Lubis", "Christian Geishauser", "Hsien-Chin Lin", "Marco Moresi", "Milica Gasic." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the",
      "citeRegEx" : "Heck et al\\.,? 2020",
      "shortCiteRegEx" : "Heck et al\\.",
      "year" : 2020
    }, {
      "title" : "How can we know what language models know",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Indatacenter performance analysis of a tensor pro",
      "author" : [ "Norman P. Jouppi", "Cliff Young", "Nishant Patil", "David Patterson", "Gaurav Agrawal", "Raminder Bajwa", "Sarah Bates", "Suresh Bhatia", "Nan Boden", "Al Borchers", "Rick Boyle", "Pierre-luc" ],
      "venue" : "Cantin",
      "citeRegEx" : "Jouppi et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jouppi et al\\.",
      "year" : 2017
    }, {
      "title" : "Ma-dst: Multi-attention-based scalable dialog state tracking",
      "author" : [ "Adarsh Kumar", "Peter Ku", "Anuj Goyal", "Angeliki Metallinou", "Dilek Hakkani-Tur." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8107–8114.",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "How many data points is a prompt worth",
      "author" : [ "Teven Le Scao", "Alexander M Rush" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Scao and Rush.,? \\Q2021\\E",
      "shortCiteRegEx" : "Scao and Rush.",
      "year" : 2021
    }, {
      "title" : "Dialogue state tracking with a language model using schema-driven prompting",
      "author" : [ "Chia-Hsuan Lee", "Hao Cheng", "Mari Ostendorf." ],
      "venue" : "arXiv preprint arXiv:2109.07506.",
      "citeRegEx" : "Lee et al\\.,? 2021a",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "2021b. Sgd-x: A benchmark for robust generalization in schemaguided dialogue systems",
      "author" : [ "Harrison Lee", "Raghav Gupta", "Abhinav Rastogi", "Yuan Cao", "Bin Zhang", "Yonghui Wu" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "Sumbt: Slot-utterance matching for universal and scalable belief tracking",
      "author" : [ "Hwaran Lee", "Jinsik Lee", "Tae-Yoon Kim." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 5478–5483.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot dialogue state tracking via cross-task transfer",
      "author" : [ "Zhaojiang Lin", "Bing Liu", "Andrea Madotto", "Seungwhan Moon", "Paul Crook", "Zhenpeng Zhou", "Zhiguang Wang", "Zhou Yu", "Eunjoon Cho", "Rajen Subba", "Pascale Fung" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2021
    }, {
      "title" : "Leveraging slot descriptions for zero-shot cross-domain dialogue statetracking",
      "author" : [ "Zhaojiang Lin", "Bing Liu", "Seungwhan Moon", "Paul A Crook", "Zhenpeng Zhou", "Zhiguang Wang", "Zhou Yu", "Andrea Madotto", "Eunjoon Cho", "Rajen Subba." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Lin et al\\.,? 2021b",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2021
    }, {
      "title" : "An end-to-end dialogue state tracking system with machine reading comprehension and wide deep classification",
      "author" : [ "Yue Ma", "Zengfeng Zeng", "Dawei Zhu", "Xuan Li", "Yiying Yang", "Xiaoyuan Yao", "Kaijie Zhou", "Jianping Shen" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models as few-shot learner for task-oriented dialogue systems",
      "author" : [ "Andrea Madotto", "Zihan Liu", "Zhaojiang Lin", "Pascale Fung." ],
      "venue" : "arXiv preprint arXiv:2008.06239.",
      "citeRegEx" : "Madotto et al\\.,? 2020",
      "shortCiteRegEx" : "Madotto et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Raffel et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Schemaguided dialogue state tracking task at dstc8",
      "author" : [ "Abhinav Rastogi", "Xiaoxue Zang", "Srinivas Sunkara", "Raghav Gupta", "Pranav Khaitan." ],
      "venue" : "arXiv preprint arXiv:2002.01359.",
      "citeRegEx" : "Rastogi et al\\.,? 2020a",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
      "author" : [ "Abhinav Rastogi", "Xiaoxue Zang", "Srinivas Sunkara", "Raghav Gupta", "Pranav Khaitan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Rastogi et al\\.,? 2020b",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2020
    }, {
      "title" : "Prompt programming for large language models: Beyond the few-shot paradigm",
      "author" : [ "Laria Reynolds", "Kyle McDonell" ],
      "venue" : null,
      "citeRegEx" : "Reynolds and McDonell.,? \\Q2021\\E",
      "shortCiteRegEx" : "Reynolds and McDonell.",
      "year" : 2021
    }, {
      "title" : "Fine-tuning bert for schema-guided zero-shot dialogue state tracking",
      "author" : [ "Yu-Ping Ruan", "Zhen-Hua Ling", "Jia-Chen Gu", "Quan Liu." ],
      "venue" : "arXiv preprint arXiv:2002.00181.",
      "citeRegEx" : "Ruan et al\\.,? 2020",
      "shortCiteRegEx" : "Ruan et al\\.",
      "year" : 2020
    }, {
      "title" : "Finetuned language models are zero-shot learners",
      "author" : [ "Jason Wei", "Maarten Bosma", "Vincent Y. Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Wei et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    }, {
      "title" : "Transferable multi-domain state generator for task-oriented dialogue systems",
      "author" : [ "Chien-Sheng Wu", "Andrea Madotto", "Ehsan HosseiniAsl", "Caiming Xiong", "Richard Socher", "Pascale Fung" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Sgd-qa: Fast schema-guided dialogue state tracking for unseen services",
      "author" : [ "Yang Zhang", "Vahid Noroozi", "Evelina Bakhturina", "Boris Ginsburg" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Effective sequence-tosequence dialogue state tracking",
      "author" : [ "Jeffrey Zhao", "Mahdis Mahdieh", "Ye Zhang", "Yuan Cao", "Yonghui Wu." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7486–7493.",
      "citeRegEx" : "Zhao et al\\.,? 2021a",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "2021b. Calibrate before use: Improving few-shot performance of language models",
      "author" : [ "Tony Z. Zhao", "Eric Wallace", "Shi Feng", "Dan Klein", "Sameer Singh" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "New approaches to TOD that can generalize to new services mainly rely on combining two techniques: large language models like BERT (Devlin et al., 2019) and T5 (Raffel et al.",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : ", 2019) and T5 (Raffel et al., 2020), and schema-guided modeling i.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Models combining the two currently show state-of-the-art results on dialogue state tracking (DST) (Heck et al., 2020; Lee et al., 2021a; Anon, 2021).",
      "startOffset" : 98,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "Models combining the two currently show state-of-the-art results on dialogue state tracking (DST) (Heck et al., 2020; Lee et al., 2021a; Anon, 2021).",
      "startOffset" : 98,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "Models combining the two currently show state-of-the-art results on dialogue state tracking (DST) (Heck et al., 2020; Lee et al., 2021a; Anon, 2021).",
      "startOffset" : 98,
      "endOffset" : 148
    }, {
      "referenceID" : 20,
      "context" : "Our approach, \"Show, Don’t Tell (SDT),\" when applied to two SotA DST models, offers consistently superior accuracy and generalizes better to new APIs across both the SGD (Rastogi et al., 2020b) and MultiWoZ-Leave-One-Out (Budzianowski et al.",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : ", 2020b) and MultiWoZ-Leave-One-Out (Budzianowski et al., 2018; Lin et al., 2021b) benchmarks, while being more data-efficient and robust to schema variations.",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : ", 2020b) and MultiWoZ-Leave-One-Out (Budzianowski et al., 2018; Lin et al., 2021b) benchmarks, while being more data-efficient and robust to schema variations.",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 24,
      "context" : "Following SoTA models, we pose DST as a seq2seq task (Wu et al., 2019; Zhao et al., 2021a), where the input language model (in our case, T5) is finetuned on the training set for a DST dataset.",
      "startOffset" : 53,
      "endOffset" : 90
    }, {
      "referenceID" : 26,
      "context" : "Following SoTA models, we pose DST as a seq2seq task (Wu et al., 2019; Zhao et al., 2021a), where the input language model (in our case, T5) is finetuned on the training set for a DST dataset.",
      "startOffset" : 53,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "• T5-ind (Lee et al., 2021a): Model input comprises of the dialogue history as context concatenated with one slot description as the prompt.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "Datasets: We conduct experiments on two DST benchmarks: Schema-guided Dialogue (SGD) (Rastogi et al., 2020b) and MultiWOZ 2.",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "Results for TRADE, SUMBT, and TransferQA from (Kumar et al., 2020), (Campagna et al.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 21,
      "context" : "Large LMs are often sensitive to the choice of prompt (Zhao et al., 2021b; Reynolds and McDonell, 2021).",
      "startOffset" : 54,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "Prior approaches focused on framing DST as question answering (Ruan et al., 2020; Ma et al., 2020; Zhang et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "Prior approaches focused on framing DST as question answering (Ruan et al., 2020; Ma et al., 2020; Zhang et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "Prior approaches focused on framing DST as question answering (Ruan et al., 2020; Ma et al., 2020; Zhang et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "Many MultiWoZ cross-domain models leverage slot names/descriptions (Wu et al., 2019; Lee et al., 2019; Lin et al., 2021a).",
      "startOffset" : 67,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Many MultiWoZ cross-domain models leverage slot names/descriptions (Wu et al., 2019; Lee et al., 2019; Lin et al., 2021a).",
      "startOffset" : 67,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "Pretrained generative LLMs (Raffel et al., 2020; Brown et al., 2020) have enable framing NLP tasks as seq2seq problems.",
      "startOffset" : 27,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "Some DST papers (Zhao et al., 2021a; Feng et al., 2021) look at settings with no train-test discrepancy.",
      "startOffset" : 16,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Some DST papers (Zhao et al., 2021a; Feng et al., 2021) look at settings with no train-test discrepancy.",
      "startOffset" : 16,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Many studies explore the efficacy of task-specific prompts (Jiang et al., 2020; Liu et al., 2021).",
      "startOffset" : 59,
      "endOffset" : 97
    } ],
    "year" : 0,
    "abstractText" : "Building universal dialogue systems that can seamlessly operate across multiple domains/APIs and can generalize to new ones with minimal supervision and low maintenance is a critical challenge. Recent works have leveraged natural language descriptions for schema elements to build such systems. However, descriptions only provide indirect supervision for downstream tasks, while still requiring effort to construct. In this work, we propose Show, Don’t Tell, which uses a short labeled example dialogue to show the semantics of a schema rather than telling the model about the schema elements via descriptions. While requiring similar effort from service developers, we show that using short examples as schema representations with large language models results in stronger performance and better generalization on two popular dialogue state tracking benchmarks: the SchemaGuided Dialogue (SGD) dataset and the MultiWoZ leave-one-out benchmark.",
    "creator" : null
  }
}
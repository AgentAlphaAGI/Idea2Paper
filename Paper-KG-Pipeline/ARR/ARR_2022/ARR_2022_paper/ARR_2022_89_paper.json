{
  "name" : "ARR_2022_89_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LITE: Intent-based Task Representation Learning Using Weak Supervision",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Task management tools are widely used to organize tasks and keep track of progress in work and daily lives. Examples include Microsoft To-do, Todoist, Trello, and digital assistants, such as Amazon Alexa and Google Assistant. Machine learning techniques can automate various aspects of task management such as task creation (Mukherjee et al., 2020), organization (Landes and Di Eugenio, 2018), prioritization, and decomposition of complex tasks (Nouri et al., 2020; Zhang et al., 2021).\nThe goal of this work is to develop a single, general-purpose encoding system that converts todo task texts into real-valued vector representations. Using one encoding system for multiple task functionalities (task detection, organization, recommendation, etc.) and applications (email, to-do apps,\ndigital assistants) as opposed to having custom encoders saves time and computational costs.\nRepresentation learning has been extensively studied in natural language processing (CamachoCollados and Pilehvar, 2018). Adapting models pre-trained on massive amounts of raw texts to a target domain or task has become common practice (Qiu et al., 2020), with many publicly available pre-trained models (e.g., BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2018), and sentence encoders (Cer et al., 2018; Reimers and Gurevych, 2019)). Leveraging word context is one of the key strengths of these pre-trained models. However, to-do texts exhibit unique characteristics that make this context-based modeling less effective (§2).\nOur analysis on a dataset of 6.5 million entries shows that to-do texts are short and often lack an action verb. While similar to web search queries, they are not written to be understood by a search engine, but instead are personal notes to the users themselves, and assume rich personal context. On the other hand, some task management applications allow users to organize their to-dos under userdefined lists, which, our analysis shows, can sometimes convey important information about their meaning (e.g., a “grocery” list vs. a “today” list).\nOur hypothesis is that we can effectively fine-tune contextualized representation models for under-specified texts using multiple weaklysupervised prediction/generation tasks that focus on knowledge about to-do actions. We induce supervision signals semi-automatically from existing resources so that to-do items that have similar intents share similar target labels. To this end, we propose LITE,1 a framework for training todo task representation models using the following auxiliary tasks: (1) autocompletion of to-do descriptions, (2) pre-action and goal generation based on COMET (Bosselut et al., 2019; Hwang et al., 2021), and (3) action attribute predictions based on\n1Short for Latent Intent Task Embedding\nFrameNet (Ruppenhofer et al., 2016). We implement LITE on top of existing pretrained language models and evaluate its performance through four downstream tasks: urgent and important to-do detection, actionable to-do classification, co-located and co-timed to-do pair detection, and intent detection. We evaluate its performance using two proprietary and two publicly available datasets (Jauhar et al., 2021; Landes, 2018).\nOverall, we make the following contributions: (1) A neural multi-task learning framework to finetune embeddings of to-do texts based on intents. (2) A methodology to collect weak supervision signals from various resources without costly manual annotations. (3) An empirical comparison of contextual embeddings models on real to-do texts, where LITE outperforms various baseline models including , RoBERTa, Sentence-BERT/RoBERTa, achieving error reduction of 4.8-38.7%."
    }, {
      "heading" : "2 User-generated To-do Data",
      "text" : ""
    }, {
      "heading" : "2.1 Data Collection",
      "text" : "For training and data analysis, we use a dataset based on the now-retired Wunderlist task management app. The app was available on multiple platforms and had more than 13 million users in 2015. The dataset (henceforth WL) contains 6.5 million English to-do texts. Each to-do text includes a description (e.g., “call mum”) and associated list name (e.g, “today”). See Appendix A for more details on anonymization of this dataset.\nWe performed a basic linguistic analysis on the WL data. As observed by Landes and Di Eugenio (2018), general-purpose analyzers often fail to analyze to-do texts correctly due to the writing style and the lack of context words. To alleviate this problem, we used frequency information obtained from a large corpus to correct automatically assigned POS tags, through the following 3-step process. First, we ran the spaCy tagger (Honnibal et al., 2020)2 to assign POS tags. Then, as proposed by Keyaki and Miyazaki (2017), we corrected the POS tags based on frequency information derived from 3 billion sentences from the DepCC corpus (Panchenko et al., 2018).3 Finally, we\n2We used the English model en_core_web_lg v3.0.0 3We extracted the first 100 files from DepCC and re-tagged the sentences using spaCy. We counted the frequencies of 1-3 grams of token-XPOS pairs and replaced tokens that appeared fewer than 100 times with an out-of-vocabulary token. The frequencies were used to score the sequences of the POS tokens obtained in the previous step, and replace them with more\napplied the spaCy dependency parser to the texts with the corrected POS tags and identified main verbs and arguments."
    }, {
      "heading" : "2.2 Observations",
      "text" : "To-do descriptions are very short: The mean number of tokens per to-do description is 2.4, which is similar to that of search engine queries (Taghavi et al., 2012), but with two key differences: (1) many search queries are intended for information seeking (Broder, 2002), while to-dos typically express things to perform or to remember, and (2) people write search queries with the capabilities of a search engine in mind, but to-do descriptions are personal notes to the users themselves. Most to-do descriptions have no action verb: We observed that 87.8% of to-do descriptions do not have action verbs. If an action verb is present, 75.1% and 12.7% have a direct object and a prepositional phrase, respectively. The degree of underspecification depends on a to-do’s list name. An action verb is more frequently used in to-do descriptions that appear in generic lists, such as “inbox”4 (29.7%), “to do” (28.4%), and “today” (22.1%). Where list names already imply a specific action, the action verb is more likely to be omitted such as in the “shopping” (3.3%) and “movies to watch” (4.7%) lists. List names can be indicative of task intents: For example, a to-do text (description = “avocados”, list name = “to buy”) signifies the intent “to buy avocados”, but the same description can appear also in generic lists, such as “to do” or “reminders”. When a list name is generic, a task description needs to be weighted more to accurately capture the intent of a task. Fig. 1 shows this is a nontrivial problem for pre-trained language models like BERT. The figure visualizes the distribution of the embeddings of ‘the ‘buy <grocery>” and “call <person>” to-do texts expressed in two ways5: (1) The descriptions “buy <grocery>” and “call <person>” are paired with generic list names. (2) The descriptions “<grocery>” and “<pairs>” are paired with specific list names (indicating the actions “to buy” and “to call”). We can see that a BERT model\nfrequent ones, if found. One of the authors manually evaluated the 100 frequent to-do descriptions with tags changed by post-processing and found 17/57 errors were corrected.\n4“Inbox” was the default list name in the Wunderlist app. 5<grocery> stands for grocery items, and <person> stands for person names taken from the following web pages: vegetablesfruitsgrains.com and ssa.gov/oact/babynames\ncannot capture the similarity within the buy nor call intent groups even after domain adaptation (DA) to to-do texts (see §4.3 for more details on DA). Our model, LITE, can successfully ignore the generic lists and group similar tasks together."
    }, {
      "heading" : "3 Method: Multi-task Learning (MTL)",
      "text" : "We propose a multi-task learning (MTL) framework to represent to-do descriptions along with their list names (Fig. 2). Our model first encodes text using off-the-shelf encoders (§3.1). The token representations along with information about their types are merged by an intent extractor with multihead attention (§3.2). We train the encoder and extractor on three auxiliary tasks (§3.3,3.4)."
    }, {
      "heading" : "3.1 Off-the-shelf Text Encoder",
      "text" : "We encode input texts using off-the-shelf transformer-based pre-trained language models, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b)6. Our model takes as input the concatenation of two types of texts, descriptions and list names, separated by the token [SEP]: <s> desc. [SEP] list name </s>, where <s> and </s> are beginning-of-sentence and end-of-sentence tokens pre-defined for the encoder. The encoder converts a sequence of N input tokens w1, w2, · · ·wN into real-valued vector representations using multiple layers of attention mechanism and fully-connected networks. We use the last hidden states H = {hi}i=1,2,··· ,N as the contextual token representations of the input."
    }, {
      "heading" : "3.2 Intent Extraction with Attention",
      "text" : "List names are often–but not always–indicative of task intents (§2). For example, a “shopping” list tends to have items that a user wishes to purchase and is useful for identifying intents, but some list names merely express time (e.g., “today”), topics/targets (e.g., “family”), or nothing specific (e.g.,\n6Note that our method is applicable to other encoder types.\n“things to do”). In these cases, the model should “pay more attention” to the to-do description.\nTo handle this, we use a multi-head attention mechanism (Vaswani et al., 2017; Chaudhari et al., 2021) to extract a vector representing the intent of a to-do task, and introduce token type embeddings to explicitly inform a model of text types.\nMulti-head attention: An attention mechanism is suitable to model the variable nature of token importance. We use a multi-head, scaled dot-product attention mechanism (Vaswani et al., 2017)7 and aggregate H based on token importance into the intent embedding z.\nToken type embedding: We introduce token type embeddings, etask, elist, eother ∈ Rd, to inform a model of the source of each token. BERT injects token type embeddings in the lowest layer, the embedding layer, and train them during pre-training (Devlin et al., 2019), but other models do not (Radford et al., 2018; Liu et al., 2019b; Raffel et al., 2020). To avoid breaking the pre-trained parameters of those models, we add type embeddings to H and feed it to the multi-head attention module:\nh′i = tanh (hi) + tanh (etype(i)) (1)\nwhere type(i) is the type of the i-th token."
    }, {
      "heading" : "3.3 Auxiliary Tasks for MTL",
      "text" : "One straightforward way to train the extractor is to directly optimize it to predict the intent of a given to-do task. However, task intents are often obscure and hard to discretize into a fixed number of categories. As a result, manual collection of such categories can be costly and subjective. For example, “buy milk” and “buy a car” are both purchase action, but they differ in many aspects: different locations, different prerequisite events, and different motives.\n7We employ a simplified version of an attention mechanism, where a query vector is a fixed trainable vector.\nInstead, we propose to train the extractor on multiple auxiliary tasks with weak supervision that provide semantic augmentation to under-specified to-do texts. The underlying assumption is that tasks with similar intents have similar target labels/texts in the auxiliary tasks. Below, we present our three auxiliary training tasks."
    }, {
      "heading" : "3.3.1 Autocompletion",
      "text" : "Motivation: Inspired by Lewis et al. (2020), our first task focuses on surface-level information of to-do texts, namely prediction of missing tokens based on context tokens. Specifically, we feed a to-do text (the combination of a description and a list name) to a model, convert it into an intent embedding, and generate the maximal form of a to-do description that is inferable from the input. We call this auxiliary task autocompletion objective. We automatically collect such forms for under-specified to-do descriptions from the WL dataset. Data collection: As previously observed, to-do descriptions under generic lists (e.g., “today”) tend to be more specified than those under lists whose names imply specific action. For each to-do description in our WL dataset, we collect their longer descriptions (i.e., super-strings) up to five. We also use several templates for lists that represent locations and times to further expand descriptions (see\ndetails in Appendix B). Table 1 shows examples, two of which were generated with templates. The resulting dataset contains 1,487,161 pairs of short and long to-do descriptions. We combine them with specified to-do descriptions, which already have action verbs and do not have longer counterparts, and sample 2M examples (50% of examples are under-specified.) During training, one generation target is picked at random for each instance."
    }, {
      "heading" : "3.3.2 Pre-action and Goal Generation",
      "text" : "Motivation: This task aims to represent to-do tasks based on their prerequisite actions (what we must do beforehand) and goal events (what we want to achieve), assuming that tasks with similar intents have similar prerequisites and goals. Here, a model is trained to generate prerequisite and goal actions for a given to-do item (a task description and a list name). We call this objective pre-action and goal generation objective. Data collection: We leverage COMET (Hwang et al., 2021), a BART model (Lewis et al., 2020) fine-tuned on ATOMIC2020, to collect weak supervision signals about to-do tasks’ prerequisites and goals.8 Specifically, we feed a long description of a to-do task generated in the previous step (§3.3.1) to the BART model as a prompt followed by a relation token: (1) xNeed (prerequisite) token to generate the task’s prerequisite or (2) xIntent (goal) token to generate the task’s goal. We use beam search with width of 3 and collect the top-3 results for each relation. Table 2 shows generation results for three example to-dos.\n8We can retrieve prerequisites and goals of some todo tasks from manually curated knowledge bases such as ATOMIC2020 (Hwang et al., 2021) and ConceptNet (Speer et al., 2017) without relying on language generation, but it is not always the case that we can find the action of interest in the existing resources. The use of COMET is advantageous in handling unseen actions as shown by several studies (Bosselut et al., 2019; Hwang et al., 2021)."
    }, {
      "heading" : "3.3.3 Action Arguments Prediction",
      "text" : "Motivation: Different to-do tasks have different domain-specific arguments. For example, a purchase task must have a purchase target, and possibly a price argument. In contrast, contact tasks usually have a receiver and a topic of communication argument. We design a multi-label training task called action arguments prediction, where, given a description and a list name, a model predicts all the action arguments associated with the to-do task. Data collection: We use FrameNet (Ruppenhofer et al., 2016), a manually-created database on the meaning and usage of English words/phrases. Semantic representations are defined for concepts and events (called frames) and for their semantic elements (called frame elements, FEs); example texts that trigger frames and FEs are also provided. FEs can be core FEs (essential information for a frame), or non-core (optional). Table 3 shows examples.\nUsing the “long” to-do descriptions collected for the autocompletion task (§3.3.1), we identify frames in them using an off-the-shelf frame identifier (Swayamdipta et al., 2017). As our focus is on to-do tasks, we discard frames whose root frame is not Event. We then collect FEs for each frame from FrameNet. If a to-do description has two or more frames, we take the union of their FEs. For non-core FEs, we calculate importance weights by TF-IDF over the whole FrameNet repository so that common FEs appearing in many frames (e.g., Manner) have low weight. We normalize the weights into (0, 1] by dividing them by the maximum weight."
    }, {
      "heading" : "3.4 Optimization",
      "text" : "For the autocompletion as well as the pre-action and goal generation tasks, we employ a two-layer GRU (Cho et al., 2014) decoder with a crossattention mechanism (Luong et al., 2015). We use the embedding layer of the encoder also in the decoder. We train the model to minimize the following cross-entropy loss for each instance:\nLgen = − M∑ j=1 logP (yj |y<j , z, H), (2)\nwhere M is the length of the output text. We apply label smoothing with a smoothing factor of 0.1 (Pereyra et al., 2017).\nFor the action arguments prediction task (multilabel classification), we use GILE as a labelembedding approach (Pappas and Henderson, 2019). Given an intent embedding and label embedding, GILE projects them into a joint vector space and computes an association score from their element-wise product. Concretely, for each label l, we calculate its score P (l) ∈ (0, 1) as follows:\nein = Act(Winz) (3)\ne (l) label = Act(Wlabelv (l)) (4) P (l) = Sigmoid ( Wout(ein ⊙ e(l)label) ) , (5)\nwhere v(l) ∈ Rd is a pre-computed label embedding (constant), Act is an activation function and Win,Wlabel ∈ Rd×d and Wout ∈ R1×d are model parameters. To compute the label embeddings for FEs (Eq(4)), we encode the definitions of FEs in FrameNet with pre-trained transformer models.\nWe define the loss function to be:\nLclf = 1C ∑C c=1 (c logP (c) + (1− c) log (1− P (c))), (6)\nwhere C is the number of classes. We optimize a model to minimize the following weighted loss across three MTL objectives:\nL = ∑ task Ltask logNtask , (7)\nwhere Ntask is the number of target labels in a subtask (Aghajanyan et al., 2021)."
    }, {
      "heading" : "4 Experiments",
      "text" : "Our aim is to obtain a single, general-purpose representation model that is effective on various downstream applications. We run LITE on top of BERTbase, BERTlarge, and RoBERTa and evaluate its performance."
    }, {
      "heading" : "4.1 Evaluation Tasks",
      "text" : "We evaluate LITE on four downstream tasks: (1) urgent and important to-do detection (UIT), (2) actionable to-do classification (AT), (3) co-located and co-timed to-do pair detection (CoLoc and CoTim), and (4) intent detection (ID). Urgent and Important To-do Detection (UIT): The goal of this task is to detect urgent or important tasks, an essential step for to-do prioritization in real applications. To evaluate this task we use a proprietary dataset (derived from WL) containing 2,254 human-labeled to-do descriptions. Each description is categorized into urgent and not-urgent classes based on the majority vote of 3 annotators. This dataset does not provide list names, hence we use a dummy list name “inbox” for LITE. Actionable To-do Classification (AT): This task aims to identify to-do tasks that require a concrete, individual action to accomplish (ActionableTask) (e.g., “Sign up for dance class”). We evaluate this task using a proprietary dataset (derived from WL) containing 12,189 to-dos. Each instance consists of a description and a list name, and is manually categorized into ActionableTask, Note, and ActionableCollection. A Note is a list item that users add for future use, without the need for immediate action (e.g., “baby names”). Tasks that are labeled as ActionableCollection are not performed individually but rather as part of a collection of items in a larger task: “tomatoes” in the “Groceries” list, for example, are part of the larger task \"do groceries\" where all the individual to-dos are addressed at the same time and location. Each example was annotated by 3 annotators, the majority label is the gold label. Tasks where one or more annotators were unsure about the correct label were eliminated. Co-located and Co-timed To-do pairs Detection (CoLoc/CoTim): This task focuses on the location and time where to-do tasks are accomplished. Time and location are particularly powerful cues for task recommendations and reminders (Graus et al., 2016). In this task, given a pair of to-do items, the model predicts whether the two to-do tasks can be completed in the same location (CoLoc) or at the same time (CoTim). To evaluate this task we use the MS-LaTTE (Jauhar et al., 2021) dataset (derived from WL), which contains 25,000 pairs of to-do tasks (description + list name), of which 398 are labeled as CoLoc and 401 as CoTim. Intent Detection (ID): This task focuses on predicting the intent associated with a to-do descrip-\ntion. We use Landes and Di Eugenio (2018)’s dataset, which contains 253 to-do instances, each one labeled with one of nine intent classes (“service”, “buy”, ”email”, “school-work”, ”plan-meal”, etc.). No list name is provided in this dataset, so we use the generic list name “inbox” for LITE."
    }, {
      "heading" : "4.2 Setup",
      "text" : "In all tasks, we first generate vector representations of instances in the dataset with a pre-trained encoder and train a simple classifier on them. The quality of the embeddings is measured by the performance of the classifier. We use a logistic regression classifier implemented in scikit-learn (Pedregosa et al., 2011), with or without a penalty term. To train a classifier for CoLoc and CoTim, which provide two to-do descriptions as input (see section 4.1), we concatenate the vector representations of the two items along with their element-wise product and difference vectors (Mou et al., 2016).\nWe generate 20 sets of training, validation, and test splits at random (Gorman and Bedrick, 2019)9, and, in each trial, we use a validation split to tune hyperparameters by grid search (a regularization ∈ {None, L1, L2} and a regularization coefficient ∈ {2−5, 2−4, 2−3, 2−2, 2−1, 1})."
    }, {
      "heading" : "4.2.1 Implementation Details",
      "text" : "We implemented our MTL framework using PyTorch v1.10.0 (Paszke et al., 2019) and ran experiments on NVIDIA GeForce GTX TITAN X and RTX A6000 (for BERTlarge). We use uncased BERTbase, uncased BERTlarge, and cased RoBERTabase, in transformers library v4.6.1 (Wolf et al., 2020) with the default parameters for dimensions, activation functions, and dropout. We set the number of attention heads in the extractor and the dimension of hidden states based on the choice of a text encoder, namely T = 12/d = 768 for BERTbase-LITE and RoBERTaLITE, and T = 16/d = 1, 024 for BERTlargeLITE. We applied dropout of 0.1 to our modules except for the output layers. We optimized the model parameters using AdamW (Loshchilov and Hutter, 2019) with batch size of 2,048, learning rate of 5e-5, L2 weight decay of 0.01, and linear learning rate decay with warm-up steps of 2% of the total steps. We also apply gradient norm clipping of 1. We train our models for 15 epochs, and freeze the transformer encoder for the first 5 epochs.\n9We split data into 6:2:2 for UIT, AT, and CoLoc/CoTim, and 8:1:1 for ID.\nWe sampled 3,459 examples as validation data, on which we evaluate a model every epoch, and terminate training if the validation loss does not improve for three consecutive epochs. We tuned hyperparameters and architectural choices (§3.2) based on the average validation scores over 20 random trials on all the datasets (more details in Appendix C)."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare the following encoders as baselines.10\nBERT (Devlin et al., 2019): We take the embedding of the first token, [CLS], to represent a to-do text. [CLS] embeddings are trained to represent the whole input sequence by next sentence prediction (NSP). We compare the base (12 layers, 768D) and large (24 layers, 1024D) models. RoBERTa (Liu et al., 2019b): We take the average of the last hidden states to represent an input sequence as RoBERTa is not trained with NSP. We use RoBERTa base(12 layers, 768D).\nMotivated by Gururangan et al. (2020), we also compare the domain-adapted (“DA”) version of BERT and RoBERTa. We perform additional pretraining to BERTbase and RoBERTa on the 6M raw to-do texts (<s> task [SEP] list </s>) from WL. Sentence-Transformer: We also test off-theshelf general-purpose sentence encoders based on Transformers. These encoders are pre-trained to induce sentence embeddings with siamese and triplet network on top of pre-trained Transformer models (Reimers and Gurevych, 2019). We use the pre-trained encoder based on BERTbase and RoBERTa base. The encoders are trained on about 286k of natural language inference and textual similarity instances.\n10We evaluate additional baselines in Appendix E. The implementation details can be found in Appendix F."
    }, {
      "heading" : "4.4 Results",
      "text" : "Table 4 shows our main results. LITE consistently achieves the best performance on all tasks for all three encoders, demonstrating the generality of the learned representations. DA brings in performance improvements but only marginally on most tasks11. This is probably because to-do texts are too short to perform effective language model training.\nSentence-Transformers have proven effective in various sentence-level tasks (Reimers and Gurevych, 2019), but it is not the case in this experiment. The vanilla BERT and RoBERTa encoders perform on par with their Sentence-Transformer counterparts and in some cases outperform them. We conjecture that those sentence encoders cannot leverage contextual information effectively as they are pre-trained on sentences that are quite different from to-do texts. Our training framework can also fine-tune Sentence Transformers to adapt them to short and under-specified to-do texts, which we leave for future work.\nOur goal is to train a general-purpose encoder. However, the interested reader can find an evaluation of task-specific fine-tuning in Appendix G."
    }, {
      "heading" : "4.5 Analysis",
      "text" : "Table 5 shows the contribution of our auxiliary tasks to the overall performance. The full model performs the best in all the tasks except ID.\nAs discussed earlier, our model needs to combine information from descriptions and list names to infer the meaning of to-dos. We show that our model successfully learns when to attend lists. We\n11It is also possible to combine domain adaptation by language modeling and LITE, however, it underperformed LITE overall. With BERTbase, the performance were UIT 0.873, AT 0.931, CoLoc 0.863, CoTim 0.447, and ID 0.656.\nextract list names from the AT dataset that appear with more than 17 different to-do descriptions (90% percentile) and analyze the product of attention weights and vector norms of descriptions and list tokens (Kobayashi et al., 2020). Table 6 shows list names with the highest and lowest average scores assigned by BERT-LITE. Generic list names have low scores (‘to do list”, “house to do”) while specific (action-related) list names have much higher scores (“bring”, “cleaning”).\nHowever, we believe it would not be prudent to just ignore generic lists as they can still convey semantic/pragmatic clues. For example, a list named “wishlist” typically has to-dos which a user does not need to act on immediately. Hence, this list is a strong indicator of a non-actionable task (in AT)."
    }, {
      "heading" : "5 Related Work",
      "text" : "To-do Management: Intelligent systems can assist users with task management in many ways (Gil et al., 2012). To-do tasks can be inferred automatically from emails (Mukherjee et al., 2020). Systems can detect types of to-do items and suggest relevant applications or resources to users (Landes and Di Eugenio, 2018; Gil et al., 2012; Shah and White, 2021). Once to-do tasks have been created, a system can help users manage the completion progress, e.g., by sending reminders (Graus et al.,\n2016). Complex tasks can be decomposed automatically into more manageable sub-steps (Nouri et al., 2020; Zhang et al., 2021). In all these use cases, a common step is to represent the input language as computational vector representations, but none of the existing studies has produced general-purpose representations of to-do tasks.\nShort-text Representations: Multiple NLP areas involve very short texts with some unique characteristics. Several methods have been developed for tweets (e.g., (Nguyen et al., 2020)). Tweets pose the added challenge of containing many non-standard colloquial expressions and contain non-language text like URLs. Still, Wang et al. (2020) present a similar finding to ours: massively pre-trained encoders do not always perform well. Search queries are also short, with an average of three terms (Taghavi et al., 2012). Unlike to-dos, information such as click logs (Zhang et al., 2019) can be used as an indicator of user intent. Another key difference is that search queries are written with the goal of having a machine interpret them.\nMulti-task Learning: Multi-task learning improves the performance of pre-trained language models in various NLP tasks (Liu et al., 2019a; Shuster et al., 2020; Aghajanyan et al., 2021). The common perception in the research community is that auxiliary training tasks are effective when they are similar to the target domain/task (Shui et al., 2019). However, there are few relevant tasks and datasets for the to-do domain. Our study is the first work to propose a time- and cost-efficient way to harvest weak-supervision for MTL in that domain."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We study how to produce general-purpose representations of short and under-specified to-do texts for performing various kinds of intelligent task assistance with a single encoder. Our method, LITE, uses a multi-head attention mechanism with token type embeddings on top of an off-the-shelf contextual text encoder to effectively induce semantic information from the combination of to-do descriptions and list names. The model is trained using three auxiliary tasks: autocompletion, pre-action and goal generation, and action arguments prediction. We apply LITE to BERTbase, BERTlarge, and RoBERTa, and compare them with various baseline models on four downstream tasks. LITE consistently outperforms the baselines, demonstrating the effectiveness of our method."
    }, {
      "heading" : "A Ethical Considerations",
      "text" : "The proprietary Wunderlist data was anonymized and personally identifiable information was scrubbed. Names were replaced by random names. In addition, k-anonymization was performed on the data so that tasks that were created by fewer than five users or fewer than 100 times in total were\nautomatically discarded. The result is an aggregate view of the logs, devoid of any identifiers, private information or infrequent tasks that can be correlated back to a user. The data cleaning process was approved by an internal legal review board before the data was cleared for internal use. None of the data is exposed in this paper, example texts presented in this paper are made up by the authors and no text is taken verbatim from the original data.\nAlthough the proposed method is not specifically designed for English, it will require significant cost to deliver the outcome to other languages due to the dependence on English resources (knowledge bases used for training COMET and English FrameNet).\nAs LITE is essentially built on pre-trained language models, biases existing in the original language models can still remain in the final model (e.g., biased associations between gender and actions). We did not observe any undesired associations caused by the models in our experiments, but it may be required to monitor biases and apply debiasing techniques before deploying the model to production systems."
    }, {
      "heading" : "B Templates for Autocompletion Data",
      "text" : "We used 312 hand-crafted templates for collecting the autocompletion data. We first created templates for common nouns used in list names such as today, monday, mom, home. We then used a publicly\navailable dataset12 to mine list names that represent company names like “costco” and “target”. We show representative examples in Table 7."
    }, {
      "heading" : "C Architecture Search",
      "text" : "We present the validation scores with different architectural choices in Table 8 (how to inject type embeddings) and Table 9 (number of attention heads in the intent extractor). We used BERTbaseas the base text encoder and trained BERT-LITE on 500k samples of our dataset."
    }, {
      "heading" : "D Statistical Significance Test",
      "text" : "Following the recommendation of Gorman and Bedrick (2019), we performed a permutation test with 5,000 trials between vanilla Transformer vs. DA, vanilla Transformer vs. LITE, and DA vs. LITE for each of twenty trials. We applied Bonferroni correction to the obtained p-values (Dror et al., 2017) to avoid over-estimate statistical significance. Table 10 reports the number of random trials where one model’s score is significantly higher than that of the other model (α = 0.05). We can see that\n12kaggle.com/peopledatalabssf/free-7-million-companydataset/version/1\nLITE performs significantly better than the vanilla counterpart more often than DA does. The results show that RoBERTa-LITE’s score is even significantly higher than that of RoBERTa-DA in some tasks (UIT, CoLoc and CoTim)."
    }, {
      "heading" : "E Additional Baseline Results",
      "text" : "In this section, we present experimental results with the following additional baselines: GPT-2 (Radford et al., 2018): We take the average of the last hidden states to represent an input sequence as we do for RoBERTa. Unlike BERT and RoBERTa, GPT-2 is a unidirectional encoder. Sentence-MPNet: MPNet is a Transformerbased pre-trained language model that is reported to outperform BERT and RoBERTa (Song et al., 2020). Sentence-Transformer (Reimers and Gurevych, 2019) based on MPNet (SentenceMPNet) is trained on 1.2B sentences from various tasks and is considered to be the best-quality general-purpose encoder (Reimers, 2021). word2vec and fastText: Unlike the other baseline encoders, word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2017) do not contextualize embeddings. We use a 300D word2vec model trained on Google News 100B and extend it by Magnitude (Patel et al., 2018) for OOV words. For fastText, we use a 300D model trained on CommonCrawl 2M. We also train a word2vec model from scratch on the same texts without special tokens as the domain-adapted version (DA).\nResults (Table 11): GPT-2 performed worse than BERT and RoBERTa. Sentence-MPNet is trained with a huge amount of additional training data but still under-performs LITE. word2vec and fastText performed similarly and outperform vanilla BERT and RoBERTa on UIT and ID. The two datasets\ndo not provide list names as input and have fewer data points than the other datasets. Thus, we conjecture that (1) there is not enough word context that vanilla BERT and RoBERTa can leverage and (2) the dimension of embeddings is too high for a classifier to find generalizable patterns from a small amount of data.\nF Implementation Details of Baselines\nWe implemented the baseline encoders with the following libraries.\nTransformers: We used Huggingface’s transformers library (Wolf et al., 2020) to run pre-trained Transformer models.\nSentence Transformers: We use the SentenceBERT library (Reimers and Gurevych, 2019)13 to run pre-trained sentence encoders. We used the following pre-trained models:\nBERT: roberta-base-nli-stsb-mean-tokens14\nRoBERTa: roberta-base-nli-stsb-mean-tokens15\nMPNet: all-mpnet-base-v216"
    }, {
      "heading" : "G Fine-tuning BERT and RoBERTa",
      "text" : "We present the performance of BERT and RoBERTa fine-tuned on downstream datasets. Note that our main goal is to train a general-purpose encoder that does not need to be re-trained for each downstream task as we describe in §1. We aim to answer the following two hypothetical questions.\n13www.sbert.net/ 14huggingface.co/sentence-transformers/bert-base-nli-\nstsb-mean-tokens 15huggingface.co/sentence-transformers/roberta-base-nlistsb-mean-tokens 16huggingface.co/sentence-transformers/all-mpnet-basev2\nQ1 (In-dataset fine-tuning): How well could BERT and RoBERTa perform if they were fine-tuned on the target dataset? This approach is commonly practiced for taskspecific representations (Devlin et al., 2019).\nQ2 (Cross-dataset fine-tuning): How well could BERT and RoBERTa perform on the target dataset if they were fine-tuned on another dataset? (Were the fine-tuned encoders generalizable to multiple to-do datasets?)\nSetup: We fine-tune and evaluate BERTbase and RoBERTabase models on the 20 random splits used in the main experiments. We follow Devlin et al. (2019) and add a linear classification layer that takes in the final hidden state of the first token ([CLS] token). For fine-tuning, the encoder and classifier are trained to optimize a binary cross entropy loss (UIT, CoLoc, and CoTim) or a cross entropy loss (ID and AT). We use the same optimization configurations described in §4.2.1. We\ncontinue training for 5 epochs and take the checkpoint that achieves the best validation score. For the cross-dataset experiment, we initialize the encoder with the fine-tuned parameters and freeze it during training. We use the same optimization settings except that we set a learning rate to 0.001.\nA1 (Table 12): As expected, the fine-tuned models perform better than LITE on several datasets (AT, CoLoc, and CoTim with BERT, and AT with RoBERTa). When the main goal is to build taskspecific representations, and there is a sufficiently large training dataset, task-specific fine-tuning will be a better solution than LITE. However, the result shows the fine-tuned models do not always outperform LITE. We conjecture that for datasets without a sufficient number of training instances like UIT and AT, a fine-tuning strategy is not very effective.\nA2 (Table 13): Performance consistently drops when the encoders are trained on another dataset, and all the scores are far below those of BERT/RoBERTa-LITE. This result indicates that LITE is more effective for training generalizable encoders than fine-tuning on a single dataset."
    } ],
    "references" : [ {
      "title" : "Muppet: Massive multi-task representations with pre-finetuning",
      "author" : [ "Armen Aghajanyan", "Anchit Gupta", "Akshat Shrivastava", "Xilun Chen", "Luke Zettlemoyer", "Sonal Gupta" ],
      "venue" : null,
      "citeRegEx" : "Aghajanyan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "COMET: Commonsense transformers for automatic knowledge graph construction",
      "author" : [ "Antoine Bosselut", "Hannah Rashkin", "Maarten Sap", "Chaitanya Malaviya", "Asli Celikyilmaz", "Yejin Choi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Bosselut et al\\.,? 2019",
      "shortCiteRegEx" : "Bosselut et al\\.",
      "year" : 2019
    }, {
      "title" : "A taxonomy of web search",
      "author" : [ "Andrei Broder." ],
      "venue" : "SIGIR Forum, 36(2):3–10.",
      "citeRegEx" : "Broder.,? 2002",
      "shortCiteRegEx" : "Broder.",
      "year" : 2002
    }, {
      "title" : "From word to sense embeddings: A survey on vector representations of meaning",
      "author" : [ "Jose Camacho-Collados", "Mohammad Taher Pilehvar." ],
      "venue" : "J. Artif. Int. Res., 63(1):743–788.",
      "citeRegEx" : "Camacho.Collados and Pilehvar.,? 2018",
      "shortCiteRegEx" : "Camacho.Collados and Pilehvar.",
      "year" : 2018
    }, {
      "title" : "Universal sentence encoder for English",
      "author" : [ "Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St. John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cer et al\\.,? 2018",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2018
    }, {
      "title" : "An attentive survey of attention models",
      "author" : [ "Sneha Chaudhari", "Varun Mithal", "Gungor Polatkan", "Rohan Ramanath." ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology, 12(5).",
      "citeRegEx" : "Chaudhari et al\\.,? 2021",
      "shortCiteRegEx" : "Chaudhari et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional Transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Replicability analysis for natural language processing: Testing significance with multiple datasets",
      "author" : [ "Rotem Dror", "Gili Baumer", "Marina Bogomolov", "Roi Reichart." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:471–486.",
      "citeRegEx" : "Dror et al\\.,? 2017",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2017
    }, {
      "title" : "Capturing common knowledge about tasks: Intelligent assistance for to-do lists",
      "author" : [ "Yolanda Gil", "Varun Ratnakar", "Timothy Chklovski", "Paul Groth", "Denny Vrandecic." ],
      "venue" : "ACM Transactions on Information Systems, 2(3).",
      "citeRegEx" : "Gil et al\\.,? 2012",
      "shortCiteRegEx" : "Gil et al\\.",
      "year" : 2012
    }, {
      "title" : "We need to talk about standard splits",
      "author" : [ "Kyle Gorman", "Steven Bedrick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2786–2791, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Gorman and Bedrick.,? 2019",
      "shortCiteRegEx" : "Gorman and Bedrick.",
      "year" : 2019
    }, {
      "title" : "Analyzing and predicting task reminders",
      "author" : [ "David Graus", "Paul N Bennett", "Ryen W White", "Eric Horvitz." ],
      "venue" : "Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization, pages 7–15, Halifax, NS, Canada. ACM.",
      "citeRegEx" : "Graus et al\\.,? 2016",
      "shortCiteRegEx" : "Graus et al\\.",
      "year" : 2016
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "spaCy: Industrialstrength natural language processing in Python",
      "author" : [ "Matthew Honnibal", "Ines Montani", "Sofie Van Landeghem", "Adriane Boyd" ],
      "venue" : null,
      "citeRegEx" : "Honnibal et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Honnibal et al\\.",
      "year" : 2020
    }, {
      "title" : "COMET-ATOMIC 2020: On symbolic and neural commonsense knowledge graphs",
      "author" : [ "Jena D. Hwang", "Chandra Bhagavatula", "Ronan Le Bras", "Jeff Da", "Keisuke Sakaguchi", "Antoine Bosselut", "Yejin Choi." ],
      "venue" : "Proceedings of the Thirty-Fifth AAAI Conference on",
      "citeRegEx" : "Hwang et al\\.,? 2021",
      "shortCiteRegEx" : "Hwang et al\\.",
      "year" : 2021
    }, {
      "title" : "MSLaTTE: A dataset of where and when to-do tasks are completed",
      "author" : [ "Sujay Kumar Jauhar", "Nirupama Chandrasekaran", "Michael Gamon", "Ryen W. White." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Jauhar et al\\.,? 2021",
      "shortCiteRegEx" : "Jauhar et al\\.",
      "year" : 2021
    }, {
      "title" : "Part-of-speech tagging for web search queries using a large-scale web corpus",
      "author" : [ "Atsushi Keyaki", "Jun Miyazaki." ],
      "venue" : "Proceedings of the Symposium on Applied Computing, pages 931–937, New York, NY, USA. ACM.",
      "citeRegEx" : "Keyaki and Miyazaki.,? 2017",
      "shortCiteRegEx" : "Keyaki and Miyazaki.",
      "year" : 2017
    }, {
      "title" : "Attention is not only a weight: Analyzing transformers with vector norms",
      "author" : [ "Goro Kobayashi", "Tatsuki Kuribayashi", "Sho Yokoi", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7057–",
      "citeRegEx" : "Kobayashi et al\\.,? 2020",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised approach imperative todo list categorization",
      "author" : [ "Paul Landes." ],
      "venue" : "https://github.com/ plandes/todo-task. Accessed: 2021-01-14.",
      "citeRegEx" : "Landes.,? 2018",
      "shortCiteRegEx" : "Landes.",
      "year" : 2018
    }, {
      "title" : "A supervised approach to the interpretation of imperative to-do lists",
      "author" : [ "Paul Landes", "Barbara Di Eugenio." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Landes and Eugenio.,? 2018",
      "shortCiteRegEx" : "Landes and Eugenio.",
      "year" : 2018
    }, {
      "title" : "BART: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487–4496, Flo-",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "The Seventh International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Portugal. As-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26, pages 3111–3119, Stateline, Nevada, USA.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Natural language inference by tree-based convolution and heuristic matching",
      "author" : [ "Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 130–",
      "citeRegEx" : "Mou et al\\.,? 2016",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "Smart to-do: Automatic generation of to-do items from emails",
      "author" : [ "Sudipto Mukherjee", "Subhabrata Mukherjee", "Marcello Hasegawa", "Ahmed Hassan Awadallah", "Ryen White." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Mukherjee et al\\.,? 2020",
      "shortCiteRegEx" : "Mukherjee et al\\.",
      "year" : 2020
    }, {
      "title" : "BERTweet: A pre-trained language model for English tweets",
      "author" : [ "Dat Quoc Nguyen", "Thanh Vu", "Anh Tuan Nguyen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 9–14, On-",
      "citeRegEx" : "Nguyen et al\\.,? 2020",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "Step-wise recommendation for complex task support",
      "author" : [ "Elnaz Nouri", "Robert Sim", "Adam Fourney", "Ryen W White." ],
      "venue" : "Proceedings of the 2020 Conference on Human Information Interaction and Retrieval, pages 203–212, New York, NY, USA. ACM.",
      "citeRegEx" : "Nouri et al\\.,? 2020",
      "shortCiteRegEx" : "Nouri et al\\.",
      "year" : 2020
    }, {
      "title" : "Building a web-scale dependency-parsed corpus from CommonCrawl",
      "author" : [ "Alexander Panchenko", "Eugen Ruppert", "Stefano Faralli", "Simone P Ponzetto", "Chris Biemann." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and",
      "citeRegEx" : "Panchenko et al\\.,? 2018",
      "shortCiteRegEx" : "Panchenko et al\\.",
      "year" : 2018
    }, {
      "title" : "GILE: A generalized input-label embedding for text classification",
      "author" : [ "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:139–155.",
      "citeRegEx" : "Pappas and Henderson.,? 2019",
      "shortCiteRegEx" : "Pappas and Henderson.",
      "year" : 2019
    }, {
      "title" : "PyTorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Magnitude: A fast, efficient universal vector embedding utility package",
      "author" : [ "Ajay Patel", "Alexander Sands", "Chris Callison-Burch", "Marianna Apidianaki." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Patel et al\\.,? 2018",
      "shortCiteRegEx" : "Patel et al\\.",
      "year" : 2018
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Regularizing neural networks by penalizing confident output distributions",
      "author" : [ "Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Łukasz Kaiser", "Geoffrey Hinton." ],
      "venue" : "The Fifth International Conference on Learning Representations.",
      "citeRegEx" : "Pereyra et al\\.,? 2017",
      "shortCiteRegEx" : "Pereyra et al\\.",
      "year" : 2017
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "XiPeng Qiu", "TianXiang Sun", "YiGe Xu", "YunFan Shao", "Ning Dai", "XuanJing Huang." ],
      "venue" : "Science China Technological Sciences, 63(10):1872– 1897.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text Transformer",
      "author" : [ "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research, 21(140):1–67.",
      "citeRegEx" : "Li and Liu.,? 2020",
      "shortCiteRegEx" : "Li and Liu.",
      "year" : 2020
    }, {
      "title" : "Pretrained models — Sentence-Transformers documentation",
      "author" : [ "Nils Reimers." ],
      "venue" : "https: //www.sbert.net/docs/pretrained_ models.html. Accessed: 2021-01-14.",
      "citeRegEx" : "Reimers.,? 2021",
      "shortCiteRegEx" : "Reimers.",
      "year" : 2021
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "FrameNet II: Extended theory and practice",
      "author" : [ "J Ruppenhofer", "M Ellsworth", "M R L Petruck", "C R Johnson", "J Scheffczyk" ],
      "venue" : null,
      "citeRegEx" : "Ruppenhofer et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ruppenhofer et al\\.",
      "year" : 2016
    }, {
      "title" : "Bridging task expressions and search queries",
      "author" : [ "Chirag Shah", "Ryen W White." ],
      "venue" : "Proceedings of the 2021 Conference on Human Information Interaction and Retrieval, pages 319–323, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Shah and White.,? 2021",
      "shortCiteRegEx" : "Shah and White.",
      "year" : 2021
    }, {
      "title" : "A principled approach for learning task similarity in multitask learning",
      "author" : [ "Changjian Shui", "Mahdieh Abbasi", "Louis-Émile Robitaille", "Boyu Wang", "Christian Gagné." ],
      "venue" : "Proceedings of the TwentyEighth International Joint Conference on Artificial",
      "citeRegEx" : "Shui et al\\.,? 2019",
      "shortCiteRegEx" : "Shui et al\\.",
      "year" : 2019
    }, {
      "title" : "The Dialogue Dodecathlon: Open-domain knowledge and image grounded conversational agents",
      "author" : [ "Kurt Shuster", "Da Ju", "Stephen Roller", "Emily Dinan", "YLan Boureau", "Jason Weston." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Shuster et al\\.,? 2020",
      "shortCiteRegEx" : "Shuster et al\\.",
      "year" : 2020
    }, {
      "title" : "MPNet: Masked and permuted pretraining for language understanding",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 16857–16867.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "2017. ConceptNet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Frame-semantic parsing with softmax-margin segmental RNNs and a syntactic scaffold",
      "author" : [ "Swabha Swayamdipta", "Sam Thomson", "Chris Dyer", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Swayamdipta et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2017
    }, {
      "title" : "An analysis of web proxy logs with query distribution pattern approach for search engines",
      "author" : [ "Mona Taghavi", "Ahmed Patel", "Nikita Schmidt", "Christopher Wills", "Yiqi Tew." ],
      "venue" : "Computer Standards & Interfaces, 34(1):162–170.",
      "citeRegEx" : "Taghavi et al\\.,? 2012",
      "shortCiteRegEx" : "Taghavi et al\\.",
      "year" : 2012
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9(Nov):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "An empirical survey of unsupervised text representation methods on Twitter data",
      "author" : [ "Lili Wang", "Chongyang Gao", "Jason Wei", "Weicheng Ma", "Ruibo Liu", "Soroush Vosoughi." ],
      "venue" : "Proceedings of the Sixth Workshop on Noisy User-generated Text, pages 209–214,",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Generic intent representation in web search",
      "author" : [ "Hongfei Zhang", "Xia Song", "Chenyan Xiong", "Corby Rosset", "Paul N. Bennett", "Nick Craswell", "Saurabh Tiwary." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and Develop-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to decompose and organize complex tasks",
      "author" : [ "Yi Zhang", "Sujay Kumar Jauhar", "Julia Kiseleva", "Ryen White", "Dan Roth." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Machine learning techniques can automate various aspects of task management such as task creation (Mukherjee et al., 2020), organization (Landes and Di Eugenio, 2018), prioritization, and decomposition of complex tasks (Nouri et al.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : ", 2020), organization (Landes and Di Eugenio, 2018), prioritization, and decomposition of complex tasks (Nouri et al., 2020; Zhang et al., 2021).",
      "startOffset" : 104,
      "endOffset" : 144
    }, {
      "referenceID" : 55,
      "context" : ", 2020), organization (Landes and Di Eugenio, 2018), prioritization, and decomposition of complex tasks (Nouri et al., 2020; Zhang et al., 2021).",
      "startOffset" : 104,
      "endOffset" : 144
    }, {
      "referenceID" : 37,
      "context" : "Adapting models pre-trained on massive amounts of raw texts to a target domain or task has become common practice (Qiu et al., 2020), with many publicly available pre-trained models (e.",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "To this end, we propose LITE,1 a framework for training todo task representation models using the following auxiliary tasks: (1) autocompletion of to-do descriptions, (2) pre-action and goal generation based on COMET (Bosselut et al., 2019; Hwang et al., 2021), and (3) action attribute predictions based on",
      "startOffset" : 217,
      "endOffset" : 260
    }, {
      "referenceID" : 15,
      "context" : "To this end, we propose LITE,1 a framework for training todo task representation models using the following auxiliary tasks: (1) autocompletion of to-do descriptions, (2) pre-action and goal generation based on COMET (Bosselut et al., 2019; Hwang et al., 2021), and (3) action attribute predictions based on",
      "startOffset" : 217,
      "endOffset" : 260
    }, {
      "referenceID" : 16,
      "context" : "We evaluate its performance using two proprietary and two publicly available datasets (Jauhar et al., 2021; Landes, 2018).",
      "startOffset" : 86,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "We evaluate its performance using two proprietary and two publicly available datasets (Jauhar et al., 2021; Landes, 2018).",
      "startOffset" : 86,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "First, we ran the spaCy tagger (Honnibal et al., 2020)2 to assign POS tags.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 31,
      "context" : "Then, as proposed by Keyaki and Miyazaki (2017), we corrected the POS tags based on frequency information derived from 3 billion sentences from the DepCC corpus (Panchenko et al., 2018).",
      "startOffset" : 161,
      "endOffset" : 185
    }, {
      "referenceID" : 49,
      "context" : "queries (Taghavi et al., 2012), but with two key differences: (1) many search queries are intended for information seeking (Broder, 2002), while to-dos typically express things to perform or to remember, and (2) people write search queries with the capabilities of a search engine in mind, but to-do descriptions are personal notes to the users themselves.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : ", 2012), but with two key differences: (1) many search queries are intended for information seeking (Broder, 2002), while to-dos typically express things to perform or to remember, and (2) people write search queries with the capabilities of a search engine in mind, but to-do descriptions are personal notes to the users themselves.",
      "startOffset" : 100,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "transformer-based pre-trained language models, BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 51,
      "context" : "To handle this, we use a multi-head attention mechanism (Vaswani et al., 2017; Chaudhari et al., 2021) to extract a vector representing the intent of a to-do task, and introduce token type embeddings to explicitly inform a model of text types.",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "To handle this, we use a multi-head attention mechanism (Vaswani et al., 2017; Chaudhari et al., 2021) to extract a vector representing the intent of a to-do task, and introduce token type embeddings to explicitly inform a model of text types.",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 51,
      "context" : "We use a multi-head, scaled dot-product attention mechanism (Vaswani et al., 2017)7 and aggregate H based on token importance into the intent embedding z.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "ken type embeddings in the lowest layer, the embedding layer, and train them during pre-training (Devlin et al., 2019), but other models do not (Radford et al.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "Data collection: We leverage COMET (Hwang et al., 2021), a BART model (Lewis et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : ", 2021), a BART model (Lewis et al., 2020) fine-tuned on ATOMIC(20) 20, to collect weak supervision signals about to-do tasks’ prerequisites and goals.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "We can retrieve prerequisites and goals of some todo tasks from manually curated knowledge bases such as ATOMIC(20) 20 (Hwang et al., 2021) and ConceptNet (Speer et al.",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 47,
      "context" : ", 2021) and ConceptNet (Speer et al., 2017) without relying on language generation, but it is not always the case that we can find the action of interest in the existing resources.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "The use of COMET is advantageous in handling unseen actions as shown by several studies (Bosselut et al., 2019; Hwang et al., 2021).",
      "startOffset" : 88,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "The use of COMET is advantageous in handling unseen actions as shown by several studies (Bosselut et al., 2019; Hwang et al., 2021).",
      "startOffset" : 88,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : "Table 3: Labels collected from FrameNet (Ruppenhofer et al., 2016) for the action arguments prediction task (§3.",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 42,
      "context" : "Data collection: We use FrameNet (Ruppenhofer et al., 2016), a manually-created database on the meaning and usage of English words/phrases.",
      "startOffset" : 33,
      "endOffset" : 59
    }, {
      "referenceID" : 48,
      "context" : "1), we identify frames in them using an off-the-shelf frame identifier (Swayamdipta et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "For the autocompletion as well as the pre-action and goal generation tasks, we employ a two-layer GRU (Cho et al., 2014) decoder with a crossattention mechanism (Luong et al.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : ", 2014) decoder with a crossattention mechanism (Luong et al., 2015).",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 32,
      "context" : "For the action arguments prediction task (multilabel classification), we use GILE as a labelembedding approach (Pappas and Henderson, 2019).",
      "startOffset" : 111,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "where Ntask is the number of target labels in a subtask (Aghajanyan et al., 2021).",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "Time and location are particularly powerful cues for task recommendations and reminders (Graus et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "To evaluate this task we use the MS-LaTTE (Jauhar et al., 2021) dataset (derived from WL), which contains 25,000 pairs of to-do tasks (description + list name), of which 398 are labeled as CoLoc and 401 as CoTim.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : "We use a logistic regression classifier implemented in scikit-learn (Pedregosa et al., 2011), with or without a penalty term.",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 27,
      "context" : "1), we concatenate the vector representations of the two items along with their element-wise product and difference vectors (Mou et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "We generate 20 sets of training, validation, and test splits at random (Gorman and Bedrick, 2019)9,",
      "startOffset" : 71,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "We optimized the model parameters using AdamW (Loshchilov and Hutter, 2019) with batch size of 2,048, learning rate of 5e-5, L2 weight decay of 0.",
      "startOffset" : 46,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "BERT (Devlin et al., 2019): We take the embedding of the first token, [CLS], to represent a to-do text.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 23,
      "context" : "RoBERTa (Liu et al., 2019b): We take the average of the last hidden states to represent an input sequence as RoBERTa is not trained with NSP.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 41,
      "context" : "These encoders are pre-trained to induce sentence embeddings with siamese and triplet network on top of pre-trained Transformer models (Reimers and Gurevych, 2019).",
      "startOffset" : 135,
      "endOffset" : 163
    }, {
      "referenceID" : 41,
      "context" : "Sentence-Transformers have proven effective in various sentence-level tasks (Reimers and Gurevych, 2019), but it is not the case in this experiment.",
      "startOffset" : 76,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "extract list names from the AT dataset that appear with more than 17 different to-do descriptions (90% percentile) and analyze the product of attention weights and vector norms of descriptions and list tokens (Kobayashi et al., 2020).",
      "startOffset" : 209,
      "endOffset" : 233
    }, {
      "referenceID" : 10,
      "context" : "To-do Management: Intelligent systems can assist users with task management in many ways (Gil et al., 2012).",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 28,
      "context" : "To-do tasks can be inferred automatically from emails (Mukherjee et al., 2020).",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "Systems can detect types of to-do items and suggest relevant applications or resources to users (Landes and Di Eugenio, 2018; Gil et al., 2012; Shah and White, 2021).",
      "startOffset" : 96,
      "endOffset" : 165
    }, {
      "referenceID" : 43,
      "context" : "Systems can detect types of to-do items and suggest relevant applications or resources to users (Landes and Di Eugenio, 2018; Gil et al., 2012; Shah and White, 2021).",
      "startOffset" : 96,
      "endOffset" : 165
    }, {
      "referenceID" : 30,
      "context" : "Complex tasks can be decomposed automatically into more manageable sub-steps (Nouri et al., 2020; Zhang et al., 2021).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 55,
      "context" : "Complex tasks can be decomposed automatically into more manageable sub-steps (Nouri et al., 2020; Zhang et al., 2021).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 49,
      "context" : "Search queries are also short, with an average of three terms (Taghavi et al., 2012).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 54,
      "context" : "Unlike to-dos, information such as click logs (Zhang et al., 2019) can be used as an indicator of user intent.",
      "startOffset" : 46,
      "endOffset" : 66
    } ],
    "year" : 0,
    "abstractText" : "Users write to-dos as personal notes to themselves, about things they need to complete, remember or organize. To-do texts are usually short and under-specified, which poses a challenge for current text representation models. Yet, understanding and representing their meaning is the first step towards providing intelligent assistance for to-do management. We address this problem by proposing a neural multitask learning framework, LITE, which extracts representations of English to-do tasks with a multi-head attention mechanism on top of a pre-trained text encoder. To adapt representation models to to-do texts, we collect weaksupervision labels from semantically rich external resources (e.g., dynamic common-sense knowledge base), following the principle that to-do tasks with similar intents have similar labels. We then train the model on multiple generative/predictive training objectives jointly. We evaluate our representation model on five downstream tasks and show that our approach consistently improves performance over baseline models, achieving an error reduction of up to 38.7%.",
    "creator" : null
  }
}
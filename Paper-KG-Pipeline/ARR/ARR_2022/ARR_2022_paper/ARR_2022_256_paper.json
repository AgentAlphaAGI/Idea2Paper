{
  "name" : "ARR_2022_256_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition, NER in short, refers to identifying entity types, i.e. location, person, organization, etc., in a given sentence. The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances. However, since deep neural networks highly relies on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming. This situation is more severe for low-resource languages. With the help of transfer learning (Ruder et al., 2019) and multilingual BERT (short as mBERT) (Devlin et al., 2019), it is possible to transfer the annotated train-\ning samples or trained models from a rich-resource domain to a low-resource domain.\nMany studies have been done to solve this crosslanguage NER problem. Existing models can be separated into three categories, shared feature space based, translation based and knowledge distillation based. Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016; Wu and Dredze, 2019; Keung et al., 2019). Translation based models generate pseudo labeled target language data to train the cross-lingual NER model, but the noise from translation process restrains its performance. (Mayhew et al., 2017; Xie et al., 2018; Wu et al., 2020b). Knowledge distillation based models train a student model using soft labels of the target language (Wu et al., 2020a,b; Chen et al., 2021; Liang et al., 2021). Our model is developed on the basis of (Wu\net al., 2020a). Although above mentioned models solve the cross-lingual NER problem in some extent, the auxiliary tasks, as in the multi-task learning, have not been studied in this problem. Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target language.\nHere we give a concrete example to illustrate the importance of similarity between every two tokens under the situation when only the English data is labeled. Given a Spanish sentence “Arévalo (Avila), 23 may (EFE).”, the token “Arévalo” is recognized as ORG type using the learned model from English domain. In the meantime, the token “Arévalo” has high similarity scores with the Spanish tokens “Viena” from sentence “Viena, 23 may (EFE).\", and “Madrid” from sentence “Madrid, 23 may (EFE).”. Also, the tokens “Viena” and “Madrid” are recognized correctly as LOC type using the same English model mentioned above. Then “Arévalo” can be recognized correctly as LOC type under the supervisory signal using the similarity between “Viena” and “Madrid”.\nTo leverage the similarity between the tokens of the source languages, we design an multiple-task and multiple-teacher model (short as MTMT, as shown in Figure 1), which helps the NER learning process on the target languages. Specifically, we first introduce the knowledge distillation to build entity recognizer and similarity evaluator teachers in the source language and transfer the learned patterns to the student in the target language. In the student model, we then borrow the idea of multitask learning to incorporate a similarity evaluation task as an auxiliary task into the entity recognition classifier. During the student learning process, we input unlabelled samples from the target languages into the entity recognizer and evaluator, and take output pesudo labels as supervisory signals for these two tasks in the student model. Note that a weighting strategy is also provide therein to take into consideration of the reliability of the teachers.\nWe validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.\nOur main contributions are as follows:\n• We propose an unsupervised knowledge dis-\ntillation framework for cross-language named entity recognition and develop a teaching and learning procedure under this framework.\n• We present a novel multiple-task and multipleteacher model that introduces a entity similarity evaluator to boost the performance of student recognizer on target languages.\n• We conduct extensive experiments on seven languages compared with state-of-the-art baselines and the results confirm the effectiveness of the presented model."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation and siamese network.\nCross-Lingual NER aims to extract entities from a target language but assumes only source language is annotated. The existing models can be categorized to: a) Shared feature space based models, b) Translation based models, c) Knowledge distillation based models.\nShared feature space based models generally train a language-independent encoder using source and target language data (Tsai et al., 2016). Recently, the pre-trained multilingual language models mBERT is effective to address the challenge (Devlin et al., 2019). Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019). The performance is still weak due to the lack of annotations of target languages.\nTranslation based models generally generate pesudo labeled target data to alleviate target data scarcity. For example, (Wu et al., 2020b; Zhang et al., 2021) gain a improvement by translating the labeled source language to the target language word-by-word. Our model achieves considerable improvement by learning entity similarity in target language data without translation.\nKnowledge distillation based models includes a teacher model and a student model (Wu et al., 2020c). The teacher model is trained on labeled source language. The student model learns from the soft label predicted by teacher model on unlabeled target language data. Therefore, the student model can capture the extra knowledge about target languages. In our work, the student model not only learns the recognizer teacher knowledge, but also\nlearns the entity similarity knowledge inspired by multi-task learning.\nSiamese Network is originally introduced by (Bromley et al., 1994) to treat signature verification as a matching problem. It has been successfully applied to transfer learning such as one-shot image recognition (Koch et al., 2015), text similarity (Neculoiu et al., 2016). However, there is a dilemma to adapt siamese network to token-level recognition tasks such as NER. Siamese network assumes the input is a pair, and the output is a similarity score. To handle this issue, we reconstruct the data to pair format. To the best of our knowledge, we are the first to learn the entity similarity by siamese network."
    }, {
      "heading" : "3 Framework",
      "text" : "In this section, we introduce our framework and its detailed implementation. Our framework is consist of two models: teacher training model learned from source language and teacher-student distillation learning model learned from target language. In the teacher training model, there are two submodels, i.e. an entity recognizer teacher and a similarity evaluator teacher. These two models are two parallel tasks, wherein the entity recognition teacher focuses on identifying the named entities and the similarity evaluator teacher is to decide if two tokens are in the same type.\nWe then present a teacher-student distillation learning model to learn from the two learned teacher models simultaneously. We note that, in this learning process, such a knowledge distillation makes the student model combine the advantages of both source language patterns of entity recognition and entity similarity evaluation. During the learning process, the samples from target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model. To guarantee the student learning performance, we assign weights for each supervisory signal correspond to the output confidence of teacher sub-models. We argue that the student entity recognition task and the student entity similarity evaluation task improve the representation learning of the student encoder in the siamese structure."
    }, {
      "heading" : "3.1 Problem Definition",
      "text" : "Following standard practice, we formulate crosslingual NER as a sequence labeling task. Given a\nsentence x = {xi}Li=1 with L tokens, a NER model produces a sequence of labels y = {yi}Li=1, where xi is the i-th token and yi is the corresponding label of xi. In the source language, we denote the labeled training data as DStrain = {(x,y)} and test data as DStest. In the target language, we denote the unlabeled train data as DTtrain = {x} and the test data asDTtest. Formally, our goal is to train a model with DStrain and DTtrain to perform well on DTtest."
    }, {
      "heading" : "3.2 Teacher Models",
      "text" : "Here we first consider the training of two teacher models. For every two tokens, we define Entity Similarity Metric as a score which is the probability that two tokens belong to the same entity type. We aim to find entity similarity to help the cross-lingual NER model in target language. It is a non-trivial task since we lack golden labels to help us distinguish target named entities. To address this challenge, we propose a binary classifier called similarity teacher to leverage the labeled source language data for similarity prediction. Our similarity teacher model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space. Figure 2 illustrated the two teacher models training. The following subsections will illustrate the two teacher models sequentially."
    }, {
      "heading" : "3.2.1 Entity Recognizer Teacher",
      "text" : "Since the cross-lingual NER task, we unitize multilingual mBERT (Wu and Dredze, 2019) as basic sequence feature extractor backbone to derive the sequence embedding representation throughout this paper. And a linear classifier with softmax upon the pre-trained mBERT output. The model network\nstructure could be formulated as,\nh = mBERT(x)\nŷi = softmax(Whi + b)\nwhere h = {hi}Li=1 and hi denotes the output of the pretrained mBERT that corresponds to the input token xi. ŷi denotes the predicted probability distribution for xi. W and b are trainable parameters. For some sentence sample (x,y) ∈ DStrain and an entity token query index i, the loss function is,\nLER(x,y, i) = LCE(yi, ŷi)\nWe train this entity recognition teacher model on the source lingual training corpus DStrain = {(x,y)} directly."
    }, {
      "heading" : "3.2.2 Siamese Entity Similarity Evaluator",
      "text" : "In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.\nEntity Similarity Pairs Construction According to entity labels, we randomly select sentences pair < x,x′ > with their some token pair < xi, x ′ j > and associated labels< yi, y ′ j > inDStrain, to form the siamese supervision training dataset, DS−siamtrain = {(x,x′, i, j, t)}where the target t = 1 indicates yi = y′j , and 0 otherwise. And the testing entity pairs DS−siamtest is constructed likewisely.\nSiamese Entity Similarity Network Our similarity backbone model is a siamese neural network with mBERT as feature extraction layer. Wherein h and h′ represent latent sequences encoding features derived by the two symmetric twins with respect to input sentence x and x′ respectively.\nThe inter-entities similarity is measured on the tokens hidden representations hi and h′j , queried by the entity indices < i, j > on the sequences representations. The cosine function operator is added to compute on the entity token latent vectors’ distance, so as to measure the similarity between each siamese twin, which is fed into a single sigmoid output unit for target t̂ estimation.\nMore precisely, for a specific entity pair (x,x′, i, j, t) ∈ DS−siamtrain , the siamese network could be formulated as,\nh =mBERT(x), h′ = mBERT(x′)\nt̂(x,x′, i, j) = σ(cos(hi, h ′ j))\nwhere cos is the cosine similarity metric function, σ is the sigmoid activation function, t̂ ∈ [σ(−1), σ(1)] denotes the predicted similarity of two queried tokens pair < xi, x′j >. Larger t̂ value indicates higher similarity between the two queried entities tokens.\nThe loss function of the similarity prediction can be formulate as,\nLSIM (x,x′, i, j, t) = LBCE(t, t̂).\nFinally, we can train the siamese entity similarity evaluator on DS−siamtrain , and evaluate the performance on test dataset DS−siamtest . Together with entity recognizer model, this entity similarity evaluator are used as teachers in following knowledge distillation learning process, and transfer knowledge from source to target lingual corpus."
    }, {
      "heading" : "3.3 Teacher Student Distillation Learning",
      "text" : "In this section, we consider to transfer the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task. To this end, we propose a knowledge distillation learning process to train a target language student NER model with its supervisory signals mimicked by the entity type prediction probability by the entity recognizer teacher model and entity representation similarity target by the entity siamese similarity evaluator teacher model. Based on the original unlabeled target sentence training data DTtrain, we again construct unlabeled target-language siamese pairwise entity data DT−simtrain = {(xT ,x′T , i, j)}, with the sentence pair < xT ,x ′ T > randomly sample fromDTtrain and the entity token indices pair < i, j > uniformly sampled from the sentences therein.\nThe multi-lingual BERT is also used as encoder for the sentence siamese pair, and the entity token feature queried from the latent sequence encoding representation. Specifically, for a sentence pair (xT ,x ′ T , i, j) ∈ D T−sim train , the student model transform them as follows,\nhT = mBERT(xT )\nŷTi = softmax(WhT i + b) h′T = mBERT(x ′ T ) ŷ′Tj = softmax(Wh ′ T j + b)\nt̂T (xT ,x ′ T , i, j) = σ(cos(hT i, h ′ T j))\nThen for a specific sentence pair sample in the target siamese dataset, the student loss function has three breaches, LER(xT ,yS , i), LER(x′T ,y′S , j), and LSIM (xT ,x′T , i, j, t̂S). Note that supervision information yS , y′S , and t̂S are taught by the three teacher models. Summering over all the samples in DT−simtrain = {(xT ,x′T , i, j)}, the total student model training loss takes form,\nL = γ ∑\n(xT ,x ′ T ,i,j)∈D T−sim train\n(α1LER(xT ,yS , i)\n+α2LER(x′T ,y′S , j) +βLBCE(t̂T (xT ,x′T , i, j), t̂S))\nwhere α1, α2, β and γ are weights in loss function which are set to make the student model learns less noisy knowledge from teachers. The weights are set as follows: α1(α2) is an increasing function with respect to the output of the entity recognizer teacher as shown in Figure.4. And β is set such that it is high when the output of the entity similarity teacher is close to 0 or 1, and it is low when the output is close to 0.5. γ indicates consistency level between the outputs from two teacher models, e.g. for two input tokens, if the output from entity similarity teacher is high, and the similarity level computed from the outputs of the entity recognizer teacher is low, then their consistency level is low. We want the student model to learn from the two teachers as follows: the higher the prediction of the entity recognizer teacher is (the further away from 0.5 the prediction of the entity similarity teacher is, the higher the consistency level is), the more accurate the prediction is, thus the more attention the student model pays attention to the input tokens, and vice versa. Therefore, we heuristically devises the three weights scheduling as functions of the inputs,\nα(·) = (max(ŷTi)) 2 β = (2t̂T (xT ,x ′ T , i, j)− 1)2 γ = 1− |σ(cos(ŷTi , ŷ′Tj ))− t̂T (xT ,x ′ T , i, j)|"
    }, {
      "heading" : "4 Experiment",
      "text" : "In this section, we evaluate our multiple-task and multiple-teacher model for cross-lingual NER and compare our model with a series of state-of-the-art models."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We conducted experiments on three benchmark datasets: CoNLL2002 (Tjong Kim Sang, 2002), CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) and WikiAnn (Pan et al., 2017). CoNLL2002 includes Spanish and Dutch, CoNLL2003 includes English and German, and WikiAnn includes English and three non-western languages: Arabic, Hindi, and Chinese. Each language is divided into a training set, a development set and a test set. All datasets were annotated with four entity types: LOC, MISC, ORG, and PER. Following (Wu and Dredze, 2019), all datasets are annotated using the BIO entity labelling scheme. To imitate the zero-resource cross lingual NER case, following (Wu and Dredze, 2019), we used English as the source language and other languages as the target language. In cross-lingual NER, the training set without entity label of the target language is also available when training the model. We trained the model with the labeled training set of the source language and evaluated the model on the test set of each target language. Table 1 and 2 shows the statistics of all datasets."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We use PyTorch 1.7.1 to implement our model. All of the feature encoders mentioned in this paper use\npretrained multilingual bert model (Devlin et al., 2019) in HuggingFace’s Transformer1, which has 12 Transformer blocks, 12 attention heads, and 768 hidden units.\nWe set our hyperparameters empirically following (Wu et al., 2020c) with some modifications. We do not freeze any layers and we use the output of the last layer as our hidden feature vector. We set batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014). For the training of recognition teacher model and similarity teacher model, we set the learning rate to be 1e-5 and 5e-6 separately. For knowledge distillation, we use a learning rate of 1e-6 for the student models training. Note that if a word is divided into several subwords after tokenization, then only the first subword is considered in the loss function. Following (Tjong Kim Sang, 2002), we use the entity level F1-score as the evaluation metric. Moreover, we conduct each experiment 5 times and report the mean F1-score."
    }, {
      "heading" : "4.3 Comparison",
      "text" : "Table 3 and 4 report the zero-resource cross-lingual NER results of different models on 6 target languages.\n1https://github.com/huggingface/transformers\nWiki (Tsai et al., 2016) introduces a language independent model building on cross-lingual wikification for cross-lingual NER. WS (Ni et al., 2017) presents two weakly supervised approaches for cross-lingual NER. TMP (Jain et al., 2019) leverages machine translation to improve annotation projection approaches to cross-lingual NER. Bert-f (Wu and Dredze, 2019) applys the multilingual BERT to cross-lingual NER. AdvCE (Keung et al., 2019) improves upon multilingual BERT via adversarial learning for crosslingual NER.\nModel de es nl MTMT 76.80 81.82 83.41\nMTST 74.11 (-2.69) 78.61 (-3.21) 81.97 (-1.44)\nTSL (Wu et al., 2020c) proposes a teacher-student learning model for cross-lingual NER. Unitrans (Wu et al., 2020b) unifies a data transfer and model transfer for cross-lingual NER. AdvPicker (Chen et al., 2021) proposes a adversarial discriminator for cross-lingual NER. RIKD (Liang et al., 2021) develops a reinforced iterative knowledge distillation for cross-lingual NER. TOF (Zhang et al., 2021) transfers knowledge from three aspects for cross-lingual NER.\nIt can be seen that our model outperforms the state-of-the-arts. Specifically, compared with the remarkable RIKD, AdvPicker and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic[ar]. That demonstrates the benefits of our proposed MTMT model, compared to direct model transfer (Wu and Dredze, 2019).\nNote that Bert-f performs better than our model on Chinese dataset due to their re-tokenization of the dataset. Moreover, compared with the latest model TOF, RIKD, Unitrans, our model requires much lower computational costs for both translation and iterative knowledge distillation, meanwhile reaching superior performance. For a fair comparison, we compare our model against the version of TOF w/o continual learning (Zhang et al.,\n2021), RIKD w/o IKD (Liang et al., 2021) and Unitrans w/o translation (Wu et al., 2020b) as reported in their paper."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "To demonstrate the effectiveness of our approach, we designed the following ablation studies. Table 5 presents the results.\n(1) MTST, which combines the multiple-teacher to single-teacher. That is, both of the teacher and student have the same neural network structure. This causes a performance drop across all languages due to two single teachers cannot make a difference with combination.\n(2) MTMT w/o weighting, which set the α1,α2, β and γ all to be 1 in the loss of student model learning. It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es),\nwhich validates that weighting loss can bring more confident knowledge to student model.\n(3) MTMT w/o similarity, which removes the similarity teacher model. In this case, our approach degrades into the Single TeacherStudent learning model as in TSL (Wu et al., 2020a). Without the similarity knowledge fed into the student model, the performance drops significantly."
    }, {
      "heading" : "4.5 Case Study",
      "text" : "We give a case study to show that the failed cases of baseline models can be corrected by our model. We try to bring up insights on why the proposed multiple-task and multiple-teacher model works.\nThe proposed MTMT model can help to correct labels using the Entity Similarity defined in section 3.2. Specifically, if there is a set of tokens in which every two of them have high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens. As shown in Table 6, in example #1, the entity recognizer teacher fails to identify “Arévalo” as B-ORG type, while the student model can correctly predict it. The reason lies in that the entity recognizer teacher predicts “Viena”(‘Madrid”) as B-LOC type correctly, and the similarity evaluator teacher predicts “Viena”(“Madrid”) to have a high similarity score(0.7157, 0.7156) with “Arévalo”. The student learns from both teachers and predict the correct label for “Arévalo”. Examples #2 and #3 present the same results with different sentences."
    }, {
      "heading" : "4.6 Embeddings Distribution",
      "text" : "This section investigates the effect of embeddings of the two different teacher models. It can be seen that the embeddings distribution of student model is close to similarity evaluator teacher, as illustrated in Figure 5. We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embeddings distribution. This validates the proposed MTMT model not only transfers cross-lingual NER knowledge from source language, but also learns the similarity knowledge of target language data."
    }, {
      "heading" : "4.7 Effect of Weights",
      "text" : "In the section, we evaluate the effectiveness of the weighting loss in student learning from quantitative perspective. All of the following experiments are conducted on Spanish(es) data.\nFor α analysis, we calculate the F1-score in different probability intervals of entity recognizer teacher, we find that the recognizer teacher tends to predict more correct in higher probability interval, as illustrated in Figure 6a. Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.\nFor β analysis, we observe that F1-score are increasing with the entity similarity score from 0.5 to both sides 0 and 1 in Figure 6b. The encoder of student model obtains the clustering information of the target language with the help of β.\nFor γ analysis, we consider the consistency of recognition results and similarity score by teachers. The F1-score and similarity score of teachers are all higher in the higher γ intervals, as shown in Figure 6c. The student model learns less from unreasonable results, and it can make more accuracy entity recognition for the target language."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose an unsupervised multipletask and multiple-teacher model for cross-lingual NER. The student model learns two source language patterns of entity recognition and entity similarity evaluation. Moreover, in order to guarantee the student learning performance, we also propose a weighting strategy to take consideration of the reliability of the teachers. Our experimental results show that the proposed model yields significant improvements on six target language datasets and outperforms the existing state-of-the-art approaches."
    } ],
    "references" : [ {
      "title" : "Signature verification using a \"siamese\" time delay neural network",
      "author" : [ "Jane Bromley", "Isabelle Guyon", "Yann LeCun", "Eduard Säckinger", "Roopak Shah." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 6. Morgan-Kaufmann.",
      "citeRegEx" : "Bromley et al\\.,? 1994",
      "shortCiteRegEx" : "Bromley et al\\.",
      "year" : 1994
    }, {
      "title" : "AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER",
      "author" : [ "Weile Chen", "Huiqiang Jiang", "Qianhui Wu", "Börje Karlsson", "Yi Guan." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Named entity recognition with bidirectional LSTM-CNNs",
      "author" : [ "Jason P.C. Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Entity projection via machine translation for cross-lingual NER",
      "author" : [ "Alankar Jain", "Bhargavi Paranjape", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Jain et al\\.,? 2019",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial learning with contextual embeddings for zero-resource cross-lingual classification and NER",
      "author" : [ "Phillip Keung", "Yichao Lu", "Vikas Bhardwaj." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Keung et al\\.,? 2019",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Siamese neural networks for one-shot image recognition",
      "author" : [ "Gregory Koch", "Richard Zemel", "Ruslan Salakhutdinov" ],
      "venue" : "In ICML deep learning workshop,",
      "citeRegEx" : "Koch et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Koch et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer" ],
      "venue" : null,
      "citeRegEx" : "Lample et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforced iterative knowledge distillation for crosslingual named entity recognition",
      "author" : [ "Shining Liang", "Ming Gong", "Jian Pei", "Linjun Shou", "Wanli Zuo", "Xianglin Zuo", "Daxin Jiang." ],
      "venue" : "Proceedings of the 27th ACM SIGKDD Conference on Knowl-",
      "citeRegEx" : "Liang et al\\.,? 2021",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2021
    }, {
      "title" : "Cheap translation for cross-lingual named entity recognition",
      "author" : [ "Stephen Mayhew", "Chen-Tse Tsai", "Dan Roth." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2536–2545, Copenhagen, Denmark. As-",
      "citeRegEx" : "Mayhew et al\\.,? 2017",
      "shortCiteRegEx" : "Mayhew et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning text similarity with Siamese recurrent networks",
      "author" : [ "Paul Neculoiu", "Maarten Versteegh", "Mihai Rotaru." ],
      "venue" : "Proceedings of the 1st Workshop on Representation Learning for NLP, pages 148–157, Berlin, Germany. Association for Compu-",
      "citeRegEx" : "Neculoiu et al\\.,? 2016",
      "shortCiteRegEx" : "Neculoiu et al\\.",
      "year" : 2016
    }, {
      "title" : "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection",
      "author" : [ "Jian Ni", "Georgiana Dinu", "Radu Florian." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Ni et al\\.,? 2017",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2017
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Transfer learning in natural language processing",
      "author" : [ "Sebastian Ruder", "Matthew E. Peters", "Swabha Swayamdipta", "Thomas Wolf." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Ruder et al\\.,? 2019",
      "shortCiteRegEx" : "Ruder et al\\.",
      "year" : 2019
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "9",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Cross-lingual named entity recognition via wikification",
      "author" : [ "Chen-Tse Tsai", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 219–228, Berlin, Germany. Association for",
      "citeRegEx" : "Tsai et al\\.,? 2016",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2016
    }, {
      "title" : "Single-/multisource cross-lingual NER via teacher-student learning on unlabeled data in target language",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Börje Karlsson", "Jian-Guang Lou", "Biqing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Wu et al\\.,? 2020a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unitrans : Unifying model transfer and data transfer for cross-lingual named entity recognition with unlabeled data",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Börje F. Karlsson", "Biqing Huang", "Jian-Guang Lou." ],
      "venue" : "Proceedings of the Twenty-Ninth",
      "citeRegEx" : "Wu et al\\.,? 2020b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhanced meta-learning for cross-lingual named entity recognition with minimal resources",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Guoxin Wang", "Hui Chen", "Börje F. Karlsson", "Biqing Huang", "Chin-Yew Lin." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial In-",
      "citeRegEx" : "Wu et al\\.,? 2020c",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Neural crosslingual named entity recognition with minimal resources",
      "author" : [ "Jiateng Xie", "Zhilin Yang", "Graham Neubig", "Noah A. Smith", "Jaime Carbonell." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Xie et al\\.,? 2018",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2018
    }, {
      "title" : "Target-oriented fine-tuning for zero-resource named entity recognition",
      "author" : [ "Ying Zhang", "Fandong Meng", "Yufeng Chen", "Jinan Xu", "Jie Zhou." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1603–1615, Online.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : ", 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "With the help of transfer learning (Ruder et al., 2019) and multilingual BERT (short as mBERT) (Devlin et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : ", 2019) and multilingual BERT (short as mBERT) (Devlin et al., 2019), it is possible to transfer the annotated trainNER",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016; Wu and Dredze, 2019; Keung et al., 2019).",
      "startOffset" : 134,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016; Wu and Dredze, 2019; Keung et al., 2019).",
      "startOffset" : 134,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : "Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016; Wu and Dredze, 2019; Keung et al., 2019).",
      "startOffset" : 134,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : "Knowledge distillation based models train a student model using soft labels of the target language (Wu et al., 2020a,b; Chen et al., 2021; Liang et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "Knowledge distillation based models train a student model using soft labels of the target language (Wu et al., 2020a,b; Chen et al., 2021; Liang et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "Shared feature space based models generally train a language-independent encoder using source and target language data (Tsai et al., 2016).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "Recently, the pre-trained multilingual language models mBERT is effective to address the challenge (Devlin et al., 2019).",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).",
      "startOffset" : 170,
      "endOffset" : 190
    }, {
      "referenceID" : 19,
      "context" : "For example, (Wu et al., 2020b; Zhang et al., 2021) gain a improvement by translating the labeled source language to the target language word-by-word.",
      "startOffset" : 13,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : "For example, (Wu et al., 2020b; Zhang et al., 2021) gain a improvement by translating the labeled source language to the target language word-by-word.",
      "startOffset" : 13,
      "endOffset" : 51
    }, {
      "referenceID" : 20,
      "context" : "Knowledge distillation based models includes a teacher model and a student model (Wu et al., 2020c).",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Siamese Network is originally introduced by (Bromley et al., 1994) to treat signature verification as a matching problem.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "It has been successfully applied to transfer learning such as one-shot image recognition (Koch et al., 2015), text similarity (Neculoiu et al.",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "Our similarity teacher model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "Since the cross-lingual NER task, we unitize multilingual mBERT (Wu and Dredze, 2019) as basic sequence feature extractor backbone to derive the sequence embedding representation throughout this paper.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "We conducted experiments on three benchmark datasets: CoNLL2002 (Tjong Kim Sang, 2002), CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) and WikiAnn (Pan et al., 2017).",
      "startOffset" : 148,
      "endOffset" : 166
    }, {
      "referenceID" : 21,
      "context" : "Following (Wu and Dredze, 2019), all datasets are annotated using the BIO entity labelling scheme.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "To imitate the zero-resource cross lingual NER case, following (Wu and Dredze, 2019), we used English as the source language and other languages as the target language.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "pretrained multilingual bert model (Devlin et al., 2019) in HuggingFace’s Transformer1, which has 12 Transformer blocks, 12 attention heads, and 768 hidden units.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "We set our hyperparameters empirically following (Wu et al., 2020c) with some modifications.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "Wiki (Tsai et al., 2016) introduces a language independent model building on cross-lingual wikification for cross-lingual NER.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "WS (Ni et al., 2017) presents two weakly supervised approaches for cross-lingual NER.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "TMP (Jain et al., 2019) leverages machine translation to improve annotation projection approaches to cross-lingual NER.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "Bert-f (Wu and Dredze, 2019) applys the multilingual BERT to cross-lingual NER.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "AdvCE (Keung et al., 2019) improves upon multilingual BERT via adversarial learning for crosslingual NER.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 20,
      "context" : "TSL (Wu et al., 2020c) proposes a teacher-student learning model for cross-lingual NER.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "Unitrans (Wu et al., 2020b) unifies a data transfer and model transfer for cross-lingual NER.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "AdvPicker (Chen et al., 2021) proposes a adversarial discriminator for cross-lingual NER.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "RIKD (Liang et al., 2021) develops a reinforced iterative knowledge distillation for cross-lingual NER.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "TOF (Zhang et al., 2021) transfers knowledge from three aspects for cross-lingual NER.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "That demonstrates the benefits of our proposed MTMT model, compared to direct model transfer (Wu and Dredze, 2019).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "2021), RIKD w/o IKD (Liang et al., 2021) and Unitrans w/o translation (Wu et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : ", 2021) and Unitrans w/o translation (Wu et al., 2020b) as reported in their paper.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "In this case, our approach degrades into the Single TeacherStudent learning model as in TSL (Wu et al., 2020a).",
      "startOffset" : 92,
      "endOffset" : 110
    } ],
    "year" : 0,
    "abstractText" : "Cross-lingual named entity recognition task is one of the critical problem for evaluating the potential transfer learning techniques on low resource languages. Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority. However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domain. Other possible auxiliary tasks to improve the learning performance have not been fully investigated. In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on target domain. Specifically, an entity recognizer and a similarity evaluator teachers are first trained in parallel from the source domain. Then, two tasks in the student model are supervised by the two teachers simultaneously. Empirical studies on the datasets across 7 different languages confirm the effectiveness of the proposed model.",
    "creator" : null
  }
}
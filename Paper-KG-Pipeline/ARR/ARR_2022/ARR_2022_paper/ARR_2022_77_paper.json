{
  "name" : "ARR_2022_77_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Residue-Based Natural Language Adversarial Attack Detection",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the last decade deep learning based models have demonstrated success in a wide range of application areas, including Natural Language Processing (NLP) (Vaswani et al., 2017) and object recognition (He et al., 2015). These systems may be deployed in mission critical situations, where there is the requirement for a high level of robustness. However, (Szegedy et al., 2014) demonstrated that deep models have an inherent weakness: small perturbations in the input can yield significant, undesired, changes in the output from the model. These input perturbations were termed adversarial examples and their generation adversarial attacks.\n1Code is available at: GitHub repository link will be provided after the anonymity period.\nAdversarial attacks have been developed for systems operating in various domains: image systems (Serban et al., 2020; Biggio and Roli, 2017; Bhambri et al., 2019) and NLP systems (Lin et al., 2014; Samanta and Mehta, 2017; Rosenberg et al., 2017). The characteristics of the input can be very different between these application domains. Broadly, the nature of inputs can be described using two key attributes: static (fixed length) vs sequential and continuous vs discrete. Under this categorisation, image inputs are continuous and static, whilst NLP inputs are discrete and sequential. This work argues that due to the fundamental differences in the input and resulting adversarial perturbations in the different domains, adversarial attack behaviour can vary significantly from one domain to another. Hence, the extensive research on exploring and understanding adversarial perturbation behaviour in the continuous, static world of image systems does not necessarily transfer well to the NLP tasks.\nFor adversarial attack generation, a number of specific NLP attacks have been proposed that are designed for NLP task inputs (Lin et al., 2014; Samanta and Mehta, 2017; Rosenberg et al., 2017; Huang et al., 2018; Papernot et al., 2016; Grosse et al., 2016; Sun et al., 2018; Cheng et al., 2018; Blohm et al., 2018; DBL, 2018; Neekhara et al., 2018; Raina et al., 2020; Jia and Liang, 2017; Minervini and Riedel, 2018; Niu and Bansal, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Zhao et al., 2017). However, there has been less research on developing defence schemes. These defence strategies can be split into two main groups: model modification, where the model or data is altered at training time (e.g. adversarial training (Yoo and Qi, 2021)) and detection, where external systems or algorithms are applied to trained models to identify adversarial attacks. As model modification approaches demand re-training of models, detection approaches are usually considered easier for imple-\nmentation on deployed systems and thus are often preferred. Hence, this work investigates the portability of popular detection approaches designed for image systems to NLP systems. Furthermore, this work introduces a specific NLP detection approach that exploits the discrete nature of the inputs for NLP systems. This approach out-performs standard schemes designed for image adversarial attack detection, as well as other NLP detection schemes.\nThe proposed NLP specific detection approach will be referred to as residue detection, as it is shown that adversarial attacks in the discrete, word sequence, space result in easily detectable residual components in the sentence embedding space. This residue can be easily detected using a simple linear classifier operating in the encoder embedding space. In addition, this work shows that even when an adversary has knowledge of the linear residual detector, they can only construct attacks at a fraction of the original strength. Hence this work argues that realistic (word level, semantically similar) adversarial perturbations at the natural language input of NLP systems leave behind easily detectable residue in the sentence embedding. Interestingly, the residue detection approach is shown to perform poorly when used to detect attacks in the image domain, supporting the hypothesis that the nature of the input has an important influence on the design of effective defence strategies."
    }, {
      "heading" : "2 Related Work",
      "text" : "Previous work in the image domain has analysed the output of specific layers in an attempt to identify adversarial examples or adversarial subspaces. First, (Feinman et al., 2017) proposed that adversarial subspaces have a lower probability density, motivating the use of the Kernel Density (KD) metric to detect the adversarial examples. Nevertheless, (Ma et al., 2018) found Local Intrinsic Dimensionality (LID) was a better metric in defining the subspace for more complex data. In contrast to the local subspace focused approaches of KD and LID, (Carrara et al., 2019b) showed that trajectories of hidden layer features can be used to train a LSTM network to accurately discriminate between authentic and adversarial examples. Out performing all previous methods, (Lee et al., 2018) introduced an effective detection framework using Mahalanobis Distance Analysis (MDA), where the distance is calculated between a test sample and the closest class-conditional Gaussian distribution in the space\ndefined by the output of the final layer of the classifier (logit space). (Li and Li, 2016) also explored using the output of convolutional layers for image classification systems to identify statistics that distinguish adversarial samples from original samples. They find that by performing a PCA decomposition the statistical variation in the least principal directions is the most significant and can be used to separate original and adversarial samples. However, they argue this is ineffective as an adversary can easily suppress the tail distribution. Hence, (Li and Li, 2016) extract statistics from the convolutional layer output to train a cascade classifier to separate the original and adversarial samples. Most recently, (Mao et al., 2019) avoid the use of artificially designed metrics and combine the adversarial subspace identification stage and the detecting adversaries stage into a single framework, where a parametric model adaptively learns the deep features for detecting adversaries.\nIn contrast to the embedding space detection approaches, (Cohen et al., 2019) shows that influence functions combined with Nearest Neighbour distances perform comparably or better than the above standard detection approaches. Other detection approaches have explored the use of uncertainty: (Smith and Gal, 2018) argues that adversarial examples are out of distribution and do not lie on the manifold of real data. Hence, a discriminative Bayesian model’s epistemic (model) uncertainty should be high. Therefore, calculations of the model uncertainty are thought to be useful in detecting adversarial examples, independent of the domain. However, Bayesian approaches aren’t always practical in implementation and thus many different approaches to approximate this uncertainty have been suggested in literature (Leibig et al., 2017; Gal, 2016; Gal and Ghahramani, 2016).\nThere are a number of existing NLP specific detection approaches. For character level attacks, detection approaches have exploited the grammatical (Sakaguchi et al., 2017) and spelling (Mays et al., 1991; Islam and Inkpen, 2009) inconsistencies to identify and detect the adversarial samples. However, these character level attacks are unlikely to be employed in practice due to the simplicity with which they can be detected. Therefore, detection approaches for the more difficult semantically similar attack samples are of greater interest, where the meaning of the textual input is maintained without compromising the spelling or gram-\nmatical integrity. To tackle such word-level, semantically similar examples, (Zhou et al., 2019) designed a discriminator to classify each token representation as part of an adversarial perturbation or not, which is then used to ‘correct’ the perturbation. Other detection approaches (Raina et al., 2020; Han et al., 2020; Minervini and Riedel, 2018) have shown some success in using perplexity to identify adversarial textual examples. Most recently, (Mozes et al., 2020) achieved state of the art performance with the Frequency Guided Word Substitution (FGWS) detector, where a change in model prediction after substituting out low frequency words is revealing of adversarial samples."
    }, {
      "heading" : "3 Adversarial Attacks",
      "text" : "An adversarial attack is defined as an imperceptible change to the input that causes an undesired change in the output of a system. Often, an attack is found for a specific data point, x. Consider a classifier Fθ̂, with parameters θ̂, that predicts a class label for an input data point, x, sampled from the input distribution X . A successful adversarial attack is where a perturbation δ at the input causes the system to miss-classify,\nFθ̂(x+ δ) ̸= Fθ̂(x). (1)\nWhen defining adversarial attacks, it is important consider the interpretation of an imperceptible change. Adversarial perturbations are not considered effective if they are easy to detect. Hence, the size of the perturbation must be constrained:\nG(x,x+ δ) ≤ ϵ, (2)\nwhere the function G() describes the form of constraint and ϵ is a selected threshold of imperceptibility. Typically, when considering continuous space inputs (such as images), a popular form of the constraint of Equation 2, is to limit the perturbation in the lp norm, with p ∈ [1,∞), e.g. ||δ||p ≤ ϵ.\nFor whitebox attacks in the image domain, the dominant attack approach has proven to be Projected Gradient Descent (PGD) (Kurakin et al., 2016). The PGD approach, iteratively updates the adversarial perturbation, δ, initialised as δ0 = 0. Each iterative step moves the perturbation in the direction that maximises the loss function, L, used in the training of the model,\nδi+1 = clipϵ(δi + α∇δiL(x+ δi; θ̂)), (3)\nwhere α is an arbitrary step-size parameter and the clipping function, clipϵ, ensures the imperceptibility constraint of Equation 2 is satisfied.\nWhen considering the NLP domain, a sequential, discrete input of L words, can be explicitly represented as,\nx = w1:L = w1, w2, . . . , wL−1, wL, (4)\nwhere, the discrete word tokens, w1:L, are often mapped to a continuous, sequential word embedding (Devlin et al., 2019) space,\nh1:L = h1,h2, . . . ,hL−1,hL. (5)\nAttacks must take place in the discrete text space,\nx+ δ = w′1:L′ = w ′ 1, w ′ 2, . . . , w ′ L′−1, wL′ , (6)\nThis requires a change in the interpretation of the perturbation δ. It is not simple to define an appropriate function G() in Equation 2 for word sequences. Perturbations can be measured at a character or word level. Alternatively, the perturbation could be measured in the vectorized embedding space (Equation 5), using for example lp-norm based (Goodfellow et al., 2015) metrics or cosine similarity (Carrara et al., 2019a), which have been used in the image domain. However, constraints in the embedding space do not necessarily achieve imperceptibility in the original word sequence space. The simplest approach is to use a variant of an editbased measurement (Li et al., 2018), Le(), which counts the number of changes between the original sequence, w1:L and the adversarial sequence w′1:L′ , where a change is a swap/addition/deletion, and ensures it is smaller than a maximum number of changes, N ,\nLe(w1:L, w′1:L′) ≤ N. (7)\nFor the NLP adversarial attacks this work only examines word-level attacks, as these are considered more difficult to detect than character-level attacks. As an example, for an input sequence of L words, a N -word substitution adversarial attack, w′1:N , applied at word positions n1, n2, . . . , nN gives the adversarial output, w′1:L′\nw′1:L′ = w1, . . . , wn1−1, w ′ 1, wn1+1, . . . ,\nwnN−1, w ′ N , wnN+1, . . . , wL. (8)\nThe challenge is to select which words to replace, and what to replace them with. A simple yet effective substitution attack approach that ensures a\nsmall change in the semantic content of a sentence is to use saliency to rank the word positions, and to use word synonyms for the substitutions (Ren et al., 2019). This attack is termed Probability Weight Word Saliency (PWWS). The highest ranking word word can be swapped for a synonym from a preselected list of given synonyms. The next most highly ranked word is substituted in the same manner and the process is repeated till the required N words have been substituted.\nThe above approach is limited to attacking specific word sequences and so cannot easily be generalised to universal attacks (Moosavi-Dezfooli et al., 2016), where the same perturbation is used for all inputs. For this situation, a simple solution is concatenation (Wang and Bansal, 2018; Blohm et al., 2018), where for example, the same N -length sequence of words is appended to each input sequence of words, as described in (Raina et al., 2020). Here,\nw′1:L′ = w1, . . . , wL, w ′ 1, . . . , w ′ N . (9)\nIn both the substitution attack (Equation 8) and the concatenation attack (Equation 9), the size of the attack can be measured using the number of edits, Le(w1:L, w′1:L′) = N ."
    }, {
      "heading" : "4 Adversarial Attack Detection",
      "text" : "For a deployed system, the easiest approach to defend against adversarial attacks is to use a detection process to identify adversarial examples without having to modify the existing system.\nFor the image domain Section 2 discusses many of the standard detection approaches. In this work, we select two distinct approaches that have been generally successful: uncertainty (Smith and Gal, 2018), where adversarial samples are thought to result in greater epistemic uncertainty and Mahalanobis Distance (Lee et al., 2018), where the Mahalanobis distance in the logit space is indicative of how out of distribution a sample is (adversarial samples are considered more out of distribution). In the NLP domain, when excluding trivial grammar and spelling based detectors, perplexity based detectors can be used (Raina et al., 2020). Many other NLP specific detectors (Zhou et al., 2019; Han et al., 2020; Minervini and Riedel, 2018) have been proposed, but (Mozes et al., 2020)’s FGWS detector is considered the state of art and is thus selected for comparison. Here low frequency words in an input are substituted for higher frequency words\nand the change in model prediction is measured - adversarial samples are found to generally have a greater change. This work introduces a further NLP specific detector: residue detection, described in detail in Section 4.1.\nWhen considering any chosen detection measure Fd, a threshold β can be selected to decide whether an input, w1:L, is adversarial or not, where Fd(w1:L) > β, implies that w1:L is an adversarial sample. To assess the success of the adversarial attack detection processes, precision-recall curves are used. For the binary classification task of identifying an input as adversarially attacked or not, at a given threshold β, the precision and recall values can be computed as prec = TP/TP + FP and rec = TP/TP + FN, where TP, FP and FN are the standard true-positive, false-positive and falsenegative definitions. A single point summary of precision recall-curves is given with the F1 score."
    }, {
      "heading" : "4.1 Residue Detection",
      "text" : "In this work we introduce a new NLP detection approach, residue detection, that aims to exploit the nature of the NLP input space, discrete and sequential. Here we make two hypotheses:\n1. Adversarial samples in an encoder embedding space result in larger components (residue) in central PCA eigenvector components than original examples.\n2. The residue is only significant (detectable) for systems operating on discrete data (e.g. NLP systems).\nThe rationale behind these hypotheses is discussed next.\nDeep learning models typically consist of many layers of non-linear activation functions. For example, in the NLP domain systems are usually based on layers of the Transformer architecture (Vaswani et al., 2017). The complete end-to-end model Fθ̂() can be treated as a two stage process, with an initial set of layers forming the encoding stage, Fen() and the remaining layers forming the output stage, Fcl(), i.e. Fθ̂(x) = Fcl (Fen(x)).\nIf the encoding stage of the end-to-end classifier is sufficiently powerful, then the embedding space Fen(x) will have compressed the useful information into very few dimensions, allowing the output stage to easily separate the data points into classes (for classification) or map the data points to a continuous value (for regression). A simple Principal\nComponent Analysis (PCA) decomposition of this embedding space can be used to visualize the level of compression of the useful information. The PCA directions can be found using the eigenvectors of the covariance matrix, C, of the data in the encoder embedding space. If {qi}di=1, where d is the dimension of the encoder embedding space, represent the eigenvectors of C ordered in descending order by the associated eigenvalue in magnitude, then it is expected that almost all useful information is contained within the first few principal directions, {qi}pi=1, where p ≪ d. Hence, the output stage, Fcl() will implicitly use only these useful components. The impact of a successful adversarial perturbation, Fen(x+ δ), is the significant change in the components in the principal eigenvector directions {qi}pi=1, to allow fooling of the output stage. Due to the complex nature of the encoding stage and the out of distribution nature of the adversarial perturbations, there are likely to be residual components in the non-principal {qi}di=p+1 eigenvector directions. These perturbations in the non-principal directions are likely to be more significant for the central eigenvectors, as the encoding stage is likely to almost entirely compress out components in the least principal eigenvector directions, {qi}di=d′+1, where d′ ≈ d. Hence, {qi}d ′ i=p+1 can be viewed as a subspace containing adversarial attack residue that can be used to identify adversarial examples.\nThe existence of adversarial attack residue in the central PCA eigenvector directions, {qi}d ′ i=p+1, suggests that in the encoder embedding space, Fen(x), adversarial and original examples are linearly separable. This motivates the use of a simple linear classifier as an adversarial attack detector,\nP (adv|x) = σ(WFen(x) + b), (10)\nwhere W and b are the parameters of the linear classifier to be learnt and σ is the sigmoid function.\nThe above argument cannot predict how significant the residue in the central eigenvector space is likely to be. For the discrete space NLP attacks, the input perturbations are semantically small, whilst for continuous space image attacks the perturbations are explicitly small using a standard lp-norm. Hence, it is hypothesised that NLP perturbations cause larger errors to propagate through the system, resulting in more significant residue in the encoder embedding space than that for image attacks. Thus, the residue technique is only likely to be a feasible detection approach for discrete text attacks.\nThe hypotheses made in this section are analysed and empirically verified in Section 5.3."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "Table 1 describes four NLP classification datasets: IMDB (Maas et al., 2011); Twitter (Saravia et al., 2018); AG News (Zhang et al., 2015) and DBpedia (Zhang et al., 2015). Further, a regression dataset, Linguaskill-Business (L-Bus) (Chambers and Ingham, 2011) is included. The L-Bus data is from a multi-level prompt-response free speaking test i.e. candidates from a range of proficiency levels provide open responses to prompted questions. Based on this audio input a system must predict a score of 0-6 corresponding to the 6 CEFR (Council of Europe, 2001) grades. This audio data was transcribed using an Automatic Speech Recognition system with an average word error rate of 19.5%.\nAll NLP task models were based on the Transformer encoder architecture (Vaswani et al., 2017). Table 2 indicates the specific architecture used for each task and also summarises the classification and regression performance for the different tasks. For classification tasks, the performance is measured by top 1 accuracy, whilst for the regression task (L-Bus), the performance is measured using Pearson Correlation Coefficient (PCC).\nTable 3 shows the impact of realistic adversarial attacks on the tasks: substitution (sub) attack (Equation 8), which replaces the N most salient tokens with a synonym defined by WordNet2, as\n2https://wordnet.princeton.edu/\ndictated by the PWWS attack algorithm described in Section 3; or a targeted universal concatenation (con) attack (Equation 9), used for the regression task on the L-Bus dataset, seeking to maximise the average score output from the system by appending the same N words to the end of each input. For classification tasks, the impact of the adversarial attack is measured using the fooling rate, the fraction of originally correctly classified points, misclassified after the attack, whilst for the regression task, the impact is measured as the average increase in the output score."
    }, {
      "heading" : "5.2 Results",
      "text" : "Section 4.1 predicts that adversarial attacks in the discrete text space leave residue in a system’s encoder embedding space that can be detected using a simple linear classifier. Hence, using the 12- layer Transformer encoder’s output CLS token embedding as the encoder embedding space for each dataset’s trained system (Table 2), a simple linear classifier, as given in Equation 10, was trained 3 to detect adversarial examples from the adversarial attacks given for each dataset in Table 3. The training of the detection linear classifier was performed on the training data (Table 1) augmented with an equivalent adversarial example for each original input sample in the dataset. Using the test data samples augmented with adversarial examples (as defined by Table 3), Table 4 compares the efficacy of the linear residue detector to other popular detection strategies 4 (from Section 4) using the best F1 score. It is evident from the high F-scores, that for most NLP tasks the linear detection approach is better than other state of the art NLP specific and ported image detection approaches. Table A.2\n3lr=0.02, epochs=20, batch size=200, #769 parameters 4Detection Strategies: Mahalanobis Distance (MD) used the same train-test split as the residue approach; Perplexity (Perp) was calculated using the language model from (Chen et al., 2016); Uncertainty (Unc) used the best measure out of mutual information, confidence, KL-divergence, expected entropy and entropy of expected and reverse mutual information; and FGWS was implemented using the code given at https://github.com/maximilianmozes/fgws.\npresents the detector performances for further specific popular NLP adversarial attacks.\nHowever, an adversary may have knowledge of the detection approach and may attempt to design an attack that directly avoids detection. Hence, for each dataset, the attack approaches were repeated with the added constraint that any attack words that resulted in detection were rejected. The impact of attacks that suppress detection have been presented in Table 5. Generally, it is shown across all NLP tasks that an adversary that attempts to avoid detection of its residue by a previously trained linear classifier, can only generate a significantly less powerful adversarial attack."
    }, {
      "heading" : "5.3 Analysis",
      "text" : "The aim of this section is verify that the success of the residue detector can be explained by the two main hypotheses made in Section 4.1. The claim that residue is left by adversarial samples in the central PCA eigenvector components is explored first. For each NLP task a PCA projection matrix is learnt in the encoder embedding space using the original training data samples (Table 1). Using the test data, the residue in the embedding space can be visualized through a plot of the average (across the data) component, ρi = 1J ∑J j=1 ρi,j in each eigenvector direction, qi of the original and attacked data, where\nρi,j = ∣∣Fen(xj)Tqi∣∣ , (11)\nwith xj being the jth data point. Figure 1 shows an example plot for the Twitter dataset, where ρi is\nplotted against the eigenvalue rank, i for the original and attacked data examples. Residue plots for other datasets are included in Appendix A. Next, it is necessary to verify that the residue detector specifically uses the residue in the central eigenvector components to distinguish between original and adversarial samples. To establish this, each encoder embedding, Fen(x)’s components not within a target subspace of PCA eigenvector directions {qi}p+wi=p , are removed, i.e. we have a projected embedding, x(p) = Fen(x)− ∑ i/∈[p,p+w) q T i Fen(x)qi, where w is a window size to choose. Now, using Fcl(x(p)) and a residue detector trained using the modified embeddings, x(p), the classifier’s (Fcl(x(p))) accuracy and detector performance (measured using F1 score) can be found. Figure 2 shows the performance of the classifier (Fcl(x(p))) and the detector for different start components p, with the window size, w = 5. It is clear that the principal components hold the most important information for classifier accuracy, but, as hypothesised in Section 4.1, it is the more central eigenvector components that hold the most information useful for the residue detector, i.e. the subspace defined by {qi}10i=5 holds the most detectable residue from adversarial examples.\nThe second hypothesis in Section 4.1 claims that the existence of residue in the central eigenvector components is due to the discrete nature of NLP adversarial attacks. Hence, to analyze the impact of the discrete aspect of the attack, an artificial continuous space attack was constructed for the Twitter NLP system, where the continuous input embedding layer space (Equation 5) of the system is the space in which the attack is performed. Using the Twitter emotion classifier, a PGD (Equation\n3) attack was performed on the input embeddings for each token, where the perturbation size was limited to be ϵ = 0.1 in the l∞ norm, achieving a fooling rate of 0.73. Note that this form of attack is artificial, as a real adversary can only modify the discrete word sequence (Equation 4). To compare the influence of discrete and continuous attacks on the same system, the average (across dataset) l2 and l∞ norms of the perturbations in the input layer embedding space were found. Further, a single value summary, Nσ, of the residue plot (e.g. Figure 1), was calculated for each attack. Nσ is the average difference in standard deviations between the original component mean, ρ(orig)i and attack mean, ρ(attack)i ,\nNσ = 1\nI I∑ i=1 ∣∣∣ρ(attack)i − ρ(orig)i ∣∣∣√ Varj [ρ (orig) i,j ] . (12)\nTable 6 reports these metrics for the discrete and artificial continuous NLP adversarial attacks on the Twitter system 5. It is apparent that perturbation sizes for the discrete attacks are significantly larger. Moreover, Nσ is significantly smaller for the continuous space attack, indicating that the residue left by continuous space adversarial attacks is smaller.\nTo explicitly observe the impact of the nature of data on detectors, adversarial attacks are con-\n5Similar trends were found across all datasets (Table A.3)\nsidered in four domains: the discrete, sequential NLP input space (NLP-disc); the artificial continuous, sequential embedding space of an NLP model (NLP-cont); the continuous, static image input space (Img-cont) and a forced discretised, static image input space (Img-disc). For the NLP-disc and NLP-cont the same attacks as in Table 6 are used. For the continuous image domain (Img-cont), a VGG-16 architecture image classifier trained on CIFAR-100 (Krizhevsky et al.) image data (achieving a top-5 accuracy of 90.1%) and attacked using a standard l∞ PGD approach (Equation 3) is used. For the discrete image domain (Img-disc), the CIFAR-100 images, X ∈ ZR×R256 were discretised using function Qq : ZR×R256 → ZR×Rq , where Zq = {0, 1 256q−1 , 2 255 q−1 , . . . , 255}. In this work 2- bit quantization was used, i.e. q = 4. With this quantization, a VGG-16 architecture was trained to achieve 78.2% top-5 accuracy. To perform a discrete space attack, a variant of the PWWS synonym substitution attack (Section 3) was implemented, where synonyms were interpreted as closest permitted quantisation values and N pixel values were substituted 6. For these different domains, Table 7 compares applicable detection approaches (certain NLP detection approaches are not valid outside the word sequence space) using the best F1 score, where different attack perturbation sizes are considered (N substitutions for discrete attacks and for continuous attacks |δ| ≤ ϵ for perturbation δ).\nIn the discrete domains, the residue detection approach is better than all the other approaches. However, in the continuous data type domains, the Mahalanobis Distance dominates as the detection approach, with the residue detection approach performing the worst. As predicted by the second hypothesis of Section 4.1, the lack of success of the residue detection approach is expected here -\n6Code for Image Experiments: link after anonymity period.\nthe residue detection approach is only successful for discrete space attacks.\nTo verify that the residue detection approach is agnostic to the type of attack, the residue detector trained on substitution attack examples was evaluated on concatenation attack examples. Using the Twitter dataset, a N = 3 concatenation attack was applied, achieving a fooling rate of 0.59. In this setting, the residue detector (trained on the N = 6 substitution adversarial examples) achieved a F1 score of 0.81, which is comparable to the original score of 0.84 (from Table 4). This shows that even with different attack approaches similar forms of residue are produced, meaning a residue detector can be used even without knowledge of the type of adversarial attack."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In recent years, deep learning systems have been deployed for a large number of tasks, ranging from the image to the natural language domain. However, small, imperceptible adversarial perturbations at the input, have been found to easily fool these systems, compromising their validity in high-stakes applications. Defence strategies for deep learning systems has been extensively researched, but this work has been predominantly carried out for systems operating in the image domain. As a result, the adversarial detection strategies developed, are inherently tuned to attacks on the continuous space of images. This work shows that these detection strategies do not necessarily transfer well to attacks on natural language processing systems. Hence, an adversarial attack detection approach is proposed that specifically exploits the discrete nature of perturbations for attacks on discrete sequential inputs.\nThe proposed approach, termed residue detection, demonstrates that imperceptible attack perturbations on natural language inputs tend to result in large perturbations in word embedding spaces, which result in distinctive residual components. These residual components can be identified using a simple linear classifier. This residue detection approach was found to out-perform both detection approaches ported from the image domain and other state of the art NLP specific detectors.\nThe key finding in this work is that the nature of the data (e.g. discrete or continuous) strongly influences the success of detection systems and hence it is important to consider the domain when designing defence strategies."
    }, {
      "heading" : "Appendix A",
      "text" : "Training Details For each NLP dataset, pre-trained base (12-layer, 768-hidden dimension, 110M parameters) Transformer encoders 7 were fine-tuned during training. Table A.1 gives the training hyperparameters: learning rate (lr), batch size (bs) and the number of training epochs. In all training regimes an Adam optimizer was used. With respect to hardware, NVIDIA Volta GPU cores were used for training all models.\n7https://huggingface.co/transformers/ pretrained_models.html\nExperiments Figure A.1 presents the impact of adversarial attacks of different perturbation sizes, N on each NLP dataset. All classification datasets’ models underwent saliency ranked, N -word substitution attacks described in Equation 8, whilst the regression dataset, L-Bus, was subject to a N -word concatenation attack as in Equation 9. For the classification tasks the impact of the adversarial attacks was measured using fooling rate, whilst for the LBus dataset task, the average output score from the system is given. Figure A.2 gives the encoder embedding space PCA residue plots for all the datasets not included in the main text.\nIn the main text, two main forms of text adversarial attacks are considered: PWWS substitution attack (Ren et al., 2019) and a simple universal concatenation attack (con) (Raina et al., 2020). For completeness, Table A.2 presents the success of the adversarial attack detection approaches from the main text on two other popular adversarial attack approaches:\n• Textfooler (Jin et al., 2019) - A blackbox substitution attack where words with highest importance ranking are replaced.\n• BERT-based Adversarial Examples (BAE) (Garg and Ramakrishnan, 2020) - A BERT language model is used to replace and insert tokens to construct the adversarial sequence. This variant is termed BAE-R/I.\nThese adversarial attack approaches were implemented using the TextAttack (Morris et al., 2020) Python library with no change made to the default configurations. Note that this means the perturbation sizes are measured using a Universal Sentence Encoder’s cosine similarity (Cer et al., 2018).\nTable A.3 compares the impact on error sizes (using l2 and l∞ norms) and the residue plot metric, Nσ for the original text space discrete attacks\nand an artificial input embedding space continuous attack. The purpose of this table is to present the results for the datasets not included in the main text in Table 6.\nLimitations, Risks and Ethics A limitation of the residue approach proposed in this work is that it requires training on adversarial examples, which is not necessary for other NLP detectors. This means there is a greater computational cost associated with this detector. Moreover, associated with this limitation is a small risk, where in process of generating creative adversarial examples to build a robust residue detector, the attack generation scheme may be so strong that it can more easily evade detection from other existing detectors already deployed in industry. There are no further ethical concerns related to this detector."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 0,
    "abstractText" : "Deep learning based systems are susceptible to adversarial attacks, where a small, imperceptible change at the input alters the model prediction. However, to date the majority of the approaches to detect these attacks have been designed for image processing systems. Many popular image adversarial detection approaches are able to identify adversarial examples from embedding feature spaces, whilst in the NLP domain existing state of the art detection approaches solely focus on input text features, without consideration of model embedding spaces. This work examines what differences result when porting these image designed strategies to Natural Language Processing (NLP) tasks these detectors are found to not port over well. This is expected as NLP systems have a very different form of input: discrete and sequential in nature, rather than the continuous and fixed size inputs for images. As an equivalent model-focused NLP detection approach, this work proposes a simple sentenceembedding \"residue\" based detector to identify adversarial examples. On many tasks, it outperforms ported image domain detectors and recent state of the art NLP specific detectors 1.",
    "creator" : null
  }
}
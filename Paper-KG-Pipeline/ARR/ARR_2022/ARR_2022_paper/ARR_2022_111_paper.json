{
  "name" : "ARR_2022_111_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SHARP: Search-Based Adversarial Attack for Structured Prediction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Adversarial attacking aims to mislead the victim model (e.g., a trained dependency parser) to produce erroneous outputs when feeding adversarial examples. The process can be seen in Figure 1. Adversarial training improves the victim model in terms of performance and robustness by training on adversarial examples. Since structured prediction tasks such as sequence labeling and dependency parsing are critical building blocks of many natural language processing (NLP) systems, it is essential to study adversarial attacks and defense of structured prediction models (Jia and Liang, 2017; Wang et al., 2019).\nHowever, multiple technical challenges are faced by attackers of structured prediction models in the\nNLP area. All adversarial attackers for NLP tasks face general challenges related to gradient computation of discrete inputs, grammatical correctness, and meaning preservation (Zhang et al., 2019a; Jia and Liang, 2017; Wang et al., 2019; Cheng et al., 2019b, 2020b). Another potential but important challenge lies in the sensitivity of structured prediction: small perturbations to input sentences may very likely change the target output structures. In contrast, small perturbations typically do not change sentence classification labels. Han et al. (2020) first qualitatively proposed this assumption. We quantitatively investigate this sensitivity in Table 1. Specifically, we attack two typical models (the sentiment classifier (Ren et al., 2019) for the classification task and the dependency parser (Dozat and Manning, 2017) for structured prediction) using the widely-used word-substitution attacker (Goodfellow et al., 2015) for structured prediction task).For both tasks, we generate adversarial examples by substituting words with the same proportion. The adversarial examples have similar qualities: fluency with perplexity 142 vs. 144 and meaning-preservation degree with BLEU 0.94 vs. 0.92. However, when asking annotators to label the adversarial examples, we find that around 80% adversarial examples of the classification task keep the same target outputs as the original input sentences, while only around 20% adversarial examples of the structured prediction task have unchanged output target structures. The huge gap shows the sensitivity of structured prediction tasks, verifying the challenge of attacking structured prediction models.\nDespite these challenging issues, recently a few researchers are working on attack of structured predictions. Zheng et al. (2020) tries to preserve the original target output structures by replacing words with the same part of speech tags. Wang et al. (2021) follows a similar method to generate adversarial examples. However, these approaches cannot handle the aforementioned sensitivity. Wang et al. (2021) reveals that the syntactic structures of 25% generated examples of the attacker from Zheng et al. (2020) and 15% from (Wang et al., 2021) are changed, although they both carefully design specific rules based on linguistic prior knowledge to preserve the structures. On the other hand, Han et al. (2020) choose to generate silver structures instead of assuming unchanged target outputs. However, the proposed method of training a sequenceto-sequence adversarial example generator needs time-consuming training and often leads to ungrammatical and unnatural-looking sentences.\nTo address the above challenges, in this paper, we propose a novel and efficient attack method: SearcH-based adversarial Attack for stRuctured Prediction (SHARP). We formulate black-box adversarial attack as an optimization problem that seeks to maximize a specially designed objective function for better fluency, contextual consistency, and attacking effectiveness. In addition, we use a pretrained masked language model (PLM) to prune candidate sentences when exploring the search space. While our approach can be applied to any structured prediction tasks, in this paper we evaluate our approach on POS tagging models and dependency parsing models. Both automatic and human evaluations show that our method beats previous state-of-the-art (SOTA) approaches by a large margin. We also show that the generated adversarial examples can be used to boost the victim model in terms of accuracy and robustness with adversarial training."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Structured Prediction",
      "text" : "Structured prediction in natural language processing (NLP) aims to predict a structured output such as a sequence in the POS tagging task or a tree in the dependency parsing task. Given an input sentence x, a structured prediction model predicts the output y by maximizing the log conditional\nprobability:\nargmax yPT\nlogP py|xq\nwhere T is the set of all possible outputs. The prediction model can be trained by maximizing the log probability of the target structure y˚ given a training set which contains px,y˚q pairs.\nOur purpose is to attack a well-trained structured prediction model through searching adversarial sentences. Besides, by leveraging adversarial sentences and the original training data to retrain the model, we can defend against attacks and enhance the model’s robustness."
    }, {
      "heading" : "2.2 Adversarial Attack",
      "text" : "Let xori “ twori0 , wori1 , ..., woriN´1u denote an original sentence with N words. The victim model MV pxq : x Ñ y has been trained to produce a structured prediction output that is close to the golden structure y˚. Then the task for adversarial attack is to fool the model MV by feeding an imperceptible adversarial example xadv such that MV pxadvq ‰ target output of xadv.\nIn this work, we focus on the black-box attack setting, where only the outputs of the victim model MV are accessible, while the internal details are invisible, including the model structure, hyperparameters, training strategy, the training dataset, and gradients over each layer, etc."
    }, {
      "heading" : "3 Search-based Adversarial Attack",
      "text" : ""
    }, {
      "heading" : "3.1 Attacking Objective",
      "text" : "Typically, an ideal adversarial natural language example should be: (i) able to fool the victim parser to generate an erroneous output; (ii) fluent and grammatically correct; (iii) semantically consistent with the original sentence xori. We consider the following objectives to address the above requirements, respectively.\nAttacking Effectiveness. Ensuring the error of MV pxadvq is non-trivial due to the lack of new ground truth structured outputs. To estimate the new ground truth and further identify if the adversarial sentence can indeed fool the victim model MV , we follow Han et al. (2020) to make use of two external reference models MA and MB . Because we want the victim model to predict wrong outputs of the adversarial examples, a good adversarial example xadv\nshould maximize the difference between the predicted structures MV pxadvq and reference outputs pMApxadvq,MBpxadvqq, while minimizing the difference between MApxadvq and MBpxadvq. Formally, a scoring function can be formulated as\nrpxq “simpMApxq,MBpxqq ` p1´ simpMV pxq,MApxqqq ` p1´ simpMV pxq,MBpxqqq\n(1)\nwhere simp¨, ¨q P r0, 1s is a similarity function, e.g., Directed Dependency Accuracy (DDA) that evaluates the similarity between two parse trees.\nFluency. We use the perplexity of a PLM to evaluate the grammatical correctness and fluency of the generated sentences following Holtzman et al. (2018); Xu et al. (2018); Pang et al. (2020). For a single sentence x, the Perplexity score (PPL) can be computed as\nfpxq “ PPLpxq “ P pxq´ 1 N (2)\nwhere N denotes the sequence length. A lower perplexity indicates that the sentence is more natural and grammatically correct.\nMeaning Preservation. Note that the previous two scores neglect the original sentence xori when attacking, which will commonly result in a “zombie\" output, i.e., no matter what the input sentence is, the attack always produces exactly the same adversarial sentence. We maintain the diversity of generated sentences by using a score function to ensure the consistency of meanings between the generated sentences and the original sentences. We use BERTSCORE (Zhang et al., 2019b) to evaluate the similarity, which matches each token in x to a token in xori to compute recall, and each token in xori to a token in x to compute precision, finally combines precision and recall to compute an F1 measure.\nspx,xoriq “ BERTSCOREpx,xoriq (3)\nSuch metric correlates better with human judgment than traditional measures such as BLEU (Papineni et al., 2002).\nObjective Function. Taking together, the objective of our adversarial attack can be defined as a non-negative function:\nFpxq “ rpxq ¨ spx,x oriq\nf pxq (4)\nBy maximizing Fpxq, we hope to produce xadv that are natural-sounding, human-imperceptible, and effective in attacking the victim model."
    }, {
      "heading" : "3.2 Optimization-based Search",
      "text" : "Our optimization problem can be considered as a T -step sequential decision-making process with its state changes along tx0,x1, ...,xT´1u and x0 “ xori. At step t, xt moves to xt`1 with respect to\nxt`1 “ a˚t pw˚t ,xt, wcq pa˚t , w˚t q “ argmax\natPA,wtPxt Fpatpwt,xt, wcqq (5)\nwhere wt is a selected word in xt, and at is a wordlevel manipulation such that x̃t`1 “ atpwt,xt, wcq. In this work, we consider three different manipulations A “ tReplace, Insert,Deleteu: Replacepw˚,x, wcq indicates replacing the word w˚ in sentence x with another word wc P W; Insertpw˚,x, wcq inserts a word wc after w˚; Deletepw˚,x, nullq simply removes w˚ from x, null means we do not need another word.\nPruning W . Exploring the entire vocabulary set at each step for Replace and Insert will be time-consuming. Therefore, we prune the search space with a pretrained masked language model (PLM), e.g., RoBERTa (Liu et al., 2019). Specifically, the expected position for wc in sentence xt is replaced with a mask token [MASK] and will be predicted using RoBERTa. Then wc will be selected from the pruned word set W that includes the top Nw “ 50 predictions in the masked position according to PRoBERTapwrMASKs|xq.\nWe consider the following three strategies of exploring the search space: Beam Search (BS), Metropolis-Hastings Sampling (MHS) and Hybrid Search (HS).\nBeam Search Traditional beam search creates the beam by exhaustively searching all candidates created with one manipulation and one word from xt. Then the top k (beam size) candidates that maximize Fpxq are selected and stored in the beam in a greedy manner, each of which will be considered as the input sentence for next step. However, for each sentence, there are a huge number of possible candidates, i.e., different positions for manipulation and different words for replacement or insertion. To reduce the time complexity, at each step, we sample a single type of manipulation a P A and a single word w˚ P xt on which the manipulation is performed. Therefore, the time complexity for a single step is reduced to Opk|W|q. Due to the nature of local optimization of BS, we consider such strategy as exploitation.\nMetropolis-Hastings Sampling To get out of the local optima that BS is commonly stuck in, we use a sampling-based approach – MetropolisHastings Sampling (Metropolis et al., 1953; Hastings, 1970; Chib and Greenberg, 1995) – to explore the space beyond the starting local optimum and increase the chance of finding a global optima. We consider this strategy as exploration.\nSpecifically, we can create a proposal x̃t`1 by sampling an action at, a selected word w P xt and a new word wc P W . Given the stationary distribution defined as\nπpxq9Fpxq, (6)\nMHS accepts the proposal with the following rate:\nαpxt`1|xtq “ min \" 1, πpxt`1qT pxt|xt`1q πpxtqT pxt`1|xtq * ,\n(7)\nT pxt`1|xtq “ rrTrpxt`1|xtq ` riTipxt`1|xtq ` rdTdpxt`1|xtq,\n(8) where T defines the overall transition distribution, rr, ri and rd are transition ratios, Trp¨q, Tip¨q and Tdp¨q are transition likelihoods of Replace, Insert and Delete, respectively. The transition likelihoods can be calculated as:\nTrpx1|xq “ 1pwc PWq ¨ P pReplacepw˚,x, wcqq ř\nwPW P pReplacepw˚,x, wqq\nTipx1|xq “ 1pwc PWq ¨ P pInsertpw˚,x, wcqq ř\nwPW P pInsertpw˚,x, wqq\nTdpx1|xq “ # 1, if x1 “ Deletepw˚,xq 0, otherwise\nwhere P pReplacepw˚,x, wcqq and P pInsertpw˚,x, wcqq are the probabilities of the sentence pw0, w1, ..., wc, ..., wN´1q and the sentence pw0, ...w˚, wc, ..., wN´1q given by a pre-trained language model e.g., RoBERTa, respectively.\nHybrid Search The intuition behind HS stems from the balance between exploration and exploitation. Studies have shown that with a proper balance between exploration and exploitation, the optimization performance can be substantially improved (Črepinšek et al., 2013; Wilson et al., 2021). Specifically, at each step, we randomly select between BS and MHS.\nHS “ #\nBS rand ă i{n MHS rand ě i{n , (9)\nwhere i is the current step number, n is the maximum number of search steps, and rand is uniformly sampled from r0, rs, r P r0, 1s controls the exploitation-exploration trade-off. It can be seen that with the increase of i, HS gradually changes from exploration (MHS) to exploitation (BS). After r ˚ n step, only BS is used."
    }, {
      "heading" : "3.3 Adversarial Training",
      "text" : "Following Goodfellow et al. (2015); Madry et al. (2017), we use adversarial training to resist attacks. More specifically, we defend against attacks and increase model robustness by retraining the model with a mix of adversarial examples and the original data. We choose those adversarial examples where the two reference models MA and MB have the same predictions and take the same predictions as the target outputs."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Implementation Details",
      "text" : "The PLM used to select candidate word set is RoBERTa (Liu et al., 2019). We search hyperparameters on 300 sentences randomly sampled from the PTB development dataset. The criterion used to select all the hyper-parameters is the token level attacking success rate."
    }, {
      "heading" : "4.2 Evaluation",
      "text" : "Automatic Evaluation Following Han et al. (2020), we evaluate the adversarial examples on two aspects: generation quality (including fluency and meaning preservation) and attacking efficiency. Specifically, we treat outputs from reference model\nA, reference model B, or the agreement part of models A&B as ground truths and evaluate the following two attacking success rates (ASRs): • Token-level ASR: the percentage of words in\nthe adversarial examples that are assigned the wrong head without considering the dependence type in dependency parsing or the wrong tag in POS tagging. • Sentence-level ASR: the percentage of mispredicted examples in the adversarial examples. Human Evaluation We conduct human evaluation of generation quality (fluency, meaning preservation) and attacking efficiency (token and sentence level). We hire three volunteers with linguistic background to label 50 data (sampled from the PTB test set). For generation quality, we ask the annotators to rate from 1 to 5, the higher, the better. For attacking efficiency, we ask them to manually annotate erroneous outputs in the same way as in automatic evaluation."
    }, {
      "heading" : "4.3 Attack on Dependency Parsing",
      "text" : "We apply our approach to the dependency parsing task. We choose the Deep Biaffine parser (Dozat and Manning, 2017) as the victim parser PV and two other SOTA dependency parsers as the reference parsers: StackPTR (Ma et al., 2018) as parser PA and BiST (Kiperwasser and Goldberg, 2016) as parser PB .\nThe three parsers are all trained with Penn Treebank 3.0 (PTB, Marcus et al. (1994)) dataset following the same hyper-parameters reported in their papers.1"
    }, {
      "heading" : "4.3.1 Main Results",
      "text" : "Adversarial Attacks To assess our three modes, we random sample 300 samples from the PTB development set. Results are shown in Table 2. Observation shows that HS, taking advantage of both MHS and BS, performs better in attacking\n1Our implementations are publicly available at https: //anonymous.link.\neffectiveness than single MHS or BS. Therefore, we adopt HS in the following experiments.\nThen we summarize automatic evaluation results in Table 3 and human evaluation results in Table 4. Human evaluation is consistent with automatic evaluation: our proposed method significantly outperforms the baseline model at almost all metrics. Particularly, our approach in the “Ours-HS” row demonstrates HS’s advantage on the attacking success rate. The attacker from Zheng et al. (2020) uses the black-box setting to attack the same wordbased Biaffine model and 15% words are allowed to be modified. Their method keeps better sentence quality, at the cost of a low ASR. Compared with it, our approach is more than twice better than theirs in ASR and maintains a comparable generation quality. In contrast of the attacker from Zheng et al. (2020) that can only use substitution with the same part of speech tag, our attacker allows more flexible manipulations. A case study is shown in Figure 2, our approach replace But and kept with And and are respectively, and add then. These manipulations lead to a successful attack. Compared with Han et al. (2020), our HS is much more effective in terms of ASR and GQ.\nDefense Against Adversarial Attack Attacking the PTB training set, our HS approach can generate about 8000 adversarial examples satisfying PV pxq “ PApxq “ PBpxq. The mixed dataset of adversarial examples and the original training data is used to retrain the victim parser. After adversar-\nial training, the unlabeled attachment score (UAS) of the victim model increases from 95.37 to 95.53. To investigate the significance of the improvement, we perform significance tests on the UAS score. We calculate the p-value using the one-tailed sign test with the bootstrap re-sampling from the PTB test set following Chollampatt et al. (2019). We compare the retrained model with the original victim model. The p-values is 1.61e-5 that shows the significance. To test the adversarial robustness, we use our HS approach to attack the retrained model on 300 randomly sampled data from the PTB development set. As shown in Table 5, adversarial training significantly reduces the token level attacking success rate on all three settings."
    }, {
      "heading" : "4.4 Attack on POS Tagging",
      "text" : "We apply our approach to the POS tagging task. We use the tagger from Ma and Hovy (2016) as our victim tagger TV , and we choose two state-of-theart taggers: Stanford POS tagger (Toutanova et al., 2003) and Senna tagger (Collobert et al., 2011) as our reference tagger TA and tagger TB , respectively. We conduct experiments on the PTB dataset. All the hyper-parameters of the three taggers are the same as reported in their papers."
    }, {
      "heading" : "4.4.1 Main Results",
      "text" : "Adversarial Attacks As in the experiments of the dependency parsing task, we first randomly sample 300 samples from the PTB development set to compare our three search methods. Results are shown in Table 7. We can find that the HS still performs the best in the attacking success rate. But it performs a relative more minor advantage in this task than its performance in the dependency parsing task. One possible reason is that, compared to dependency parsing, POS tagging is a simpler task, so BS is effective while MHS (exploration with more randomness) can not bring more benefit. We still adopt HS in the following experiments as the experiments on dependency parsing.\nWe show the automatic evaluation results on the test set in Table 6 and human evaluation results on the sampled test set in Table 8. Fluency, meaning preservation and attacking success rate of our approach are all above Han et al. (2020). Our approach shows its high efficiency even though in the relatively simple task. Particularly, our approach improves the token level attacking success rate by 8.4%, and on the sentence level, our approach shows its powerful attack capability, achieving more than 70% improvement compared to previous baseline, even though the approach of Han et al. (2020) involves training a generating model and is much more computationally expensive than ours. Case studies are shown in Figure 2(b).\nDefense Against Adversarial Attack We also conduct adversarial training in the POS tagging task. To compared with Han et al. (2020), we sample 1000 additional adversarial sentences generated by attacking the PTB training set using our HS approach. We mixed these sentences with the initial training set to retrain the victim tagger. As a result, the accuracy of the tagger improves 0.21 from 97.55 to 97.76 on the PTB test set, while Han et al. (2020) reports a 0.13 improvement in the same setting. That demonstrates the high qual-\nity and effectiveness of our generated adversarial sentences."
    }, {
      "heading" : "4.5 Analysis",
      "text" : "We conduct our analytical experiments in the dependency parsing task.\nImpact of Metric for Reference Models Due to the sensitivity to small perturbations as illustrated in Table 1, the structured-prediction attackers require an automated yet unbiased evaluation scheme that is suitable for assessing the prediction of the adversarial examples. For this purpose, we adopt the agreement of two pre-trained parsers. The criterion for selecting these reference parsers is that they should be diverse besides having a high parsing accuracy (Han et al., 2020). Therefore, we propose the metric Opposite Intersection over Union (OIoU) to evaluate the diversity degree of reference parsers. In particular, we propose OIoU as one minus the number of common erroneous predictions from two reference parsers divided by the total number of unique erroneous predictions made by two reference parsers. A quantitative example is demonstrated in Figure 3. The diverse degrees of the parsers on the left and right are 0.631 (1´ 83{p26` 83` 116q) and 0.285 respectively. Our experiments find that the parsers on the left with a higher OIoU indeed result in better attacking efficiency than those on the right (12.9% vs. 5.2% token-level ASR). Note that, for fair compar-\nison of SHARP with prior work, we reuse the same reference parsers for all the experiments.\nGeneralizability We exchange the victim parser and reference parser to show the generalization of our black-box adversarial attack methods. Specifically, we take the Deep Biaffine parser (Dozat and Manning, 2017) as the reference parser PA and the StackPTR (Ma et al., 2018) as the victim parser PV . We repeat the experiments of dependency parsing keeping the same setup of Table 3 except for the parser choice. Experimental results based on automatic evaluation on the test set are shown in Table 9. We can find that our approach still can keep high attacking success rate both on token level and sentence level. Moreover, the fluency of the generated adversarial sentences of Han et al. (2020) becomes worse after changing victim parser, but our approach well maintains the quality of the sentences.\nAblation Study We show the impact of the three different scores in our objective function. Table 10 shows the automatic evaluation results of generation quality and attacking success rate on 300 samples randomly sampled from the development set. It can be seen that without considering the fluency of generated sentences (Row ´fpxq in Table 10), it is natural that the attacking success rate can be further increased, but the generation quality becomes worse. Without considering meaning reservation between original sentences and generated sentences (Row ´spx,xoriq), we can find that MP drops marginally. To verify the importance of meaning preservation (Row spx,xoriq), we experiment in the same setting as Section 4.3.1 except for using the adversarial examples generated without considering meaning preservation. We find that the unlabeled attachment score (UAS) of the victim\nmodel is 94.57 (vs 95.53), which shows that the quality of generated sentences is important to improve the victim model in terms of performance and robustness after retraining on adversarial examples. The importance is also demonstrated by prior work of Wang et al. (2021). Without optimizing rpxq, we can see that the attack success rate is even lower than random sampling because it needs to promise the quality of sampled sentences. A big gap on both generated quality and attacking success rate between random sampling and our HS approach demonstrates that the strength of our methods.\nCausal Analysis of Adversarial Attacks Since SHARP searches the whole sentence space for adversarial attacks without pre-defined templates, the generated adversarial examples have the potential for unseen discoveries. Therefore, we analyze the adversarial examples and conclude some new templates that cause mispredictions of the victim model. We list two observations here. First, uncommon words (e.g., replacing the place name with an adverb or adding a surname) often cause a misprediction. For example, the adversarial example “What is Santa actually worth?” (the original sentence is “What is Santa Fe worth?”) makes the victim model mispredict the head of “Santa” being “actually”. A probable reason is that interrogative sentences with uncommon words are rare in the training set and the victom model is confused with these out-vocabulary words. The other observation is that symbols such as “%” and “.” often mislead the model predictions. Surprisingly, the victim model can predict the correct structure for the sentence “It is widely expected that they will.” but fails when the period in this sentence is dropped.\nWe give quantitative analysis of these adversarial types in the Appendix."
    }, {
      "heading" : "5 Related Work",
      "text" : "There is limited literature available for adversarial attacking on structured prediction tasks. Previous adversarial training has been conducted on NLP tasks such as text classification (Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2020a) and dialogue systems (Cheng et al., 2019a). Recently, adversarial training has also been explored on structured prediction tasks, such as dependency parsing (Zheng et al., 2020; Han et al., 2020). Zheng et al. (2020) replaced some words with adversarially chosen counterparts with the same part of speech tags. They target specific syntactic adversarial sentence examples to attack dependency parser. Han et al. (2020) investigated generation-based attackers for structured prediction tasks."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we quantitatively investigate the sensitivity and formulate the black-box adversarial attack as an search problem that seeks to maximize a specially designed objective function. Both automatic and human evaluations show that our approach beats the previous approaches by a large margin in attacking victim models and simultaneously guarantees a better fluency and meaning preservation. Our defense experiments show that the adversarial samples generated by our approach can be used to improve the original model’s robustness and performance."
    }, {
      "heading" : "A Technical Details",
      "text" : "A.1 Hyper-Params The shared hyper-parameters of Beam Search (BS), Metropolis-Hastings Sampling (MHS), and Hybrid Search (HS) are listed in Table 11.\nA.2 Evaluation In Table 2 and Table 5 of the main body, we use different evaluation metrics to measure results of Parsers A&B compared with Han et al. (2020). Take Token-level ASR as an example, they calculate Parser A&B using 1 ´ s{n, where s is the number of tokens of which the three parsers (A, B\nand C) have the same prediction (i.e., MApxq “ MBpxq “MCpxq), and n is the total number of tokens. We use a{b, where a is the number of tokens that MApxq ‰ pMBpxq “ MCpxqq, and b is the number of tokens that MBpxq “ MCpxq. Since the assumption behind identifying the ground truth of the structured outputs is that two external reference parsers have the same prediction, it is more reasonable to use a{b when calculating the attack success rate to identify if the adversarial sentence can indeed fool the victim model. We recalculated all the Parsers A&B results of Han et al. (2020) in the main body of this work. For reference, we give results of our model calculated according to their formula as Table 12. It can be seen that our approach outperforms the approach proposed by Han et al. (2020) even with the original metrics."
    }, {
      "heading" : "B Optimizing Process",
      "text" : "We show our optimizing curve in the Figure 4. We can see that the objective score is oscillating in the early stages of sampling, because MHS sampling is chosen with high probability in the early stages, which shows the exploration process. After the early stage, we see a clear upward trend of the objective score, where our approach chooses BS with high probability, and BS directly searches the local optimal."
    }, {
      "heading" : "C Details of Sensitivity",
      "text" : "We attack two typical models, BERT for sentiment classification and Biaffine parser (Dozat and Manning, 2017) for dependency parsing, using the widely-used word-substitution attacker PWWS (Ren et al., 2019) and FGSM (Goodfellow et al., 2015)) respectively. For both tasks, we generate adversarial examples by substituting similar proportions of words, and select 50 adversarial examples with similar qualities (regarding fluency and meaning-preservation). Nevertheless, we find that around 80% adversarial sentences of sentiment classification keep the same target output as the original sentences, while only around 20% adversarial sentences of dependency parsing whose target output structure are unchanged. It demonstrates that perturbations leads to larger true output change for structured prediction tasks than classification tasks. We use the implementation of PWWS attacker in OpenAttack toolkit https://github.com/ thunlp/OpenAttack.git. We follows the default settings including all the hyper-parameter setting. For the BERT based model of sentiment classification task, we use the default setting in OpenAttack."
    }, {
      "heading" : "D Quantitative Analysis of Adversarial Types",
      "text" : "We do quantitative analysis of part of our observed adversarial types. For the first type (adding a surname to a interrogative sentence), we randomly choose 100 names, and write the sentence “What does Name Surname like?”. We get the 100% sentence-level attack success. For the second type (adding a adverb after “%”), we randomly select 100 sentences which contain “%” from the original dataset, then add adverb “global” after the “%” to generate new sentences as adversarial samples. We get about 50% sentence-level attack success in this situation.\nE Impact of Candidate Size and Manipulation Count\nFigure 5 and 6 show the result on different candidate sizes and manipulation counts, respectively. Here we can find, with the increase of the candidate size, the attacking success rate also increases, but the growth rate gradually slows down. The manipulation count shows a similar trend.\n1 3 5 7 10 Candidate Size\n0\n10\n20\n30\n40 50 To ke nle ve l A SR Parser A Parser B Parser A&B\nFigure 5: Token-level attacking success rate on different candidate sizes. 10 30 50 70 100 Manipulation Count\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nTo ke\nnle\nve l A\nSR\nParser A Parser B Parser A&B\nFigure 6: Token-level attacking success rate on different manipulation counts.\nIn our setting, the computing cost have a linear relationship with these two hyper-parameters. Thus we have a trade-off between time and performance: in our experiments, we set candidate size and manipulation count 5 and 50, respectively.\nF Impact of Beam Size\nWe investigate the impact of beam size under the same computing resources. For example, when the beam size is 1, the candidate size is 10; and when the beam size is 2, the candidate size is 5. The manipulation count keeps the same. The results are shown on Figure 7. It can be seen that 2 is the best."
    } ],
    "references" : [ {
      "title" : "Generating natural language adversarial examples",
      "author" : [ "Moustafa Alzantot", "Yash Sharma", "Ahmed Elgohary", "Bo-Jhang Ho", "Mani Srivastava", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Alzantot et al\\.,? 2018",
      "shortCiteRegEx" : "Alzantot et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating and enhancing the robustness of dialogue systems: A case study on a negotiation agent",
      "author" : [ "Minhao Cheng", "Wei Wei", "Cho-Jui Hsieh." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Cheng et al\\.,? 2019a",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples",
      "author" : [ "Minhao Cheng", "Jinfeng Yi", "Pin-Yu Chen", "Huan Zhang", "Cho-Jui Hsieh." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages",
      "citeRegEx" : "Cheng et al\\.,? 2020a",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Robust neural machine translation with doubly adversarial inputs",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4324–4333, Florence, Italy. Associa-",
      "citeRegEx" : "Cheng et al\\.,? 2019b",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "AdvAug: Robust adversarial augmentation for neural machine translation",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5961–",
      "citeRegEx" : "Cheng et al\\.,? 2020b",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding the metropolis-hastings algorithm",
      "author" : [ "Siddhartha Chib", "Edward Greenberg." ],
      "venue" : "The american statistician, 49(4):327–335.",
      "citeRegEx" : "Chib and Greenberg.,? 1995",
      "shortCiteRegEx" : "Chib and Greenberg.",
      "year" : 1995
    }, {
      "title" : "Cross-sentence grammatical error correction",
      "author" : [ "Shamil Chollampatt", "Weiqi Wang", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 435– 445, Florence, Italy. Association for Computational",
      "citeRegEx" : "Chollampatt et al\\.,? 2019",
      "shortCiteRegEx" : "Chollampatt et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of machine learning research, 12(ARTICLE):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Exploration and exploitation in evolutionary algorithms: A survey",
      "author" : [ "Matej Črepinšek", "Shih-Hsi Liu", "Marjan Mernik." ],
      "venue" : "ACM computing surveys (CSUR), 45(3):1–33.",
      "citeRegEx" : "Črepinšek et al\\.,? 2013",
      "shortCiteRegEx" : "Črepinšek et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D Manning." ],
      "venue" : "5th International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "HotFlip: White-box adversarial examples for text classification",
      "author" : [ "Javid Ebrahimi", "Anyi Rao", "Daniel Lowd", "Dejing Dou." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
      "citeRegEx" : "Ebrahimi et al\\.,? 2018",
      "shortCiteRegEx" : "Ebrahimi et al\\.",
      "year" : 2018
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Goodfellow et al\\.,? 2015",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial attack and defense of structured prediction models",
      "author" : [ "Wenjuan Han", "Liwen Zhang", "Yong Jiang", "Kewei Tu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2327–2338, On-",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Monte carlo sampling methods using markov chains and their applications",
      "author" : [ "W Keith Hastings" ],
      "venue" : null,
      "citeRegEx" : "Hastings.,? \\Q1970\\E",
      "shortCiteRegEx" : "Hastings.",
      "year" : 1970
    }, {
      "title" : "Learning to write with cooperative discriminators",
      "author" : [ "Ari Holtzman", "Jan Buys", "Maxwell Forbes", "Antoine Bosselut", "David Golub", "Yejin Choi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Holtzman et al\\.,? 2018",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031.",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional lstm feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:313–327.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Deep text classification can be fooled",
      "author" : [ "Bin Liang", "Hongcheng Li", "Miaoqiang Su", "Pan Bian", "Xirong Li", "Wenchang Shi." ],
      "venue" : "Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI’18, page 4208–4215. AAAI Press.",
      "citeRegEx" : "Liang et al\\.,? 2018",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Stackpointer networks for dependency parsing",
      "author" : [ "Xuezhe Ma", "Zecong Hu", "Jingzhou Liu", "Nanyun Peng", "Graham Neubig", "Eduard Hovy." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards deep learning models resistant to adversarial attacks",
      "author" : [ "Aleksander Madry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu." ],
      "venue" : "arXiv preprint arXiv:1706.06083.",
      "citeRegEx" : "Madry et al\\.,? 2017",
      "shortCiteRegEx" : "Madry et al\\.",
      "year" : 2017
    }, {
      "title" : "The penn treebank: annotating predicate argument structure",
      "author" : [ "Mitchell Marcus", "Grace Kim", "Mary Ann Marcinkiewicz", "Robert MacIntyre", "Ann Bies", "Mark Ferguson", "Karen Katz", "Britta Schasberger." ],
      "venue" : "Proceedings of the workshop",
      "citeRegEx" : "Marcus et al\\.,? 1994",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1994
    }, {
      "title" : "Equation of state calculations by fast computing machines",
      "author" : [ "Nicholas Metropolis", "Arianna W Rosenbluth", "Marshall N Rosenbluth", "Augusta H Teller", "Edward Teller." ],
      "venue" : "The journal of chemical physics, 21(6):1087–1092.",
      "citeRegEx" : "Metropolis et al\\.,? 1953",
      "shortCiteRegEx" : "Metropolis et al\\.",
      "year" : 1953
    }, {
      "title" : "Towards holistic and automatic evaluation of open-domain dialogue generation",
      "author" : [ "Bo Pang", "Erik Nijkamp", "Wenjuan Han", "Linqi Zhou", "Yixian Liu", "Kewei Tu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Pang et al\\.,? 2020",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8).",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating natural language adversarial examples through probability weighted word saliency",
      "author" : [ "Shuhuai Ren", "Yihe Deng", "Kun He", "Wanxiang Che." ],
      "venue" : "Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1085–",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Feature-rich part-ofspeech tagging with a cyclic dependency network",
      "author" : [ "Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter",
      "citeRegEx" : "Toutanova et al\\.,? 2003",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2003
    }, {
      "title" : "Improving neural language modeling via adversarial training",
      "author" : [ "Dilin Wang", "Chengyue Gong", "Qiang Liu." ],
      "venue" : "International Conference on Machine Learning, pages 6555–6565.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "A closer look into the robustness of neural dependency parsers using better adversarial examples",
      "author" : [ "Yuxuan Wang", "Wanxiang Che", "Ivan Titov", "Shay B Cohen", "Zhilin Lei", "Ting Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Balancing exploration and exploitation with information and randomization",
      "author" : [ "Robert C Wilson", "Elizabeth Bonawitz", "Vincent D Costa", "R Becket Ebitz." ],
      "venue" : "Current Opinion in Behavioral Sciences, 38:49–56.",
      "citeRegEx" : "Wilson et al\\.,? 2021",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2021
    }, {
      "title" : "Dp-gan: diversity-promoting generative adversarial network for generating informative and diversified text",
      "author" : [ "Jingjing Xu", "Xuancheng Ren", "Junyang Lin", "Xu Sun." ],
      "venue" : "arXiv preprint arXiv:1802.01345.",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating fluent adversarial examples for natural languages",
      "author" : [ "Huangzhao Zhang", "Hao Zhou", "Ning Miao", "Lei Li." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5564–5569, Florence, Italy. Asso-",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "arXiv preprint arXiv:1904.09675.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating natural adversarial examples",
      "author" : [ "Zhengli Zhao", "Dheeru Dua", "Sameer Singh." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating and enhancing the robustness of neural network-based dependency parsing models with adversarial examples",
      "author" : [ "Xiaoqing Zheng", "Jiehang Zeng", "Yi Zhou", "Cho-Jui Hsieh", "Minhao Cheng", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    }, {
      "title" : "2019) and FGSM (Goodfellow et al., 2015)) respectively. For both tasks, we generate adversarial examples by substituting similar proportions of words, and select 50 adversarial examples with similar qualities (regard",
      "author" : [ "PWWS (Ren" ],
      "venue" : null,
      "citeRegEx" : ".Ren,? \\Q2015\\E",
      "shortCiteRegEx" : ".Ren",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Since structured prediction tasks such as sequence labeling and dependency parsing are critical building blocks of many natural language processing (NLP) systems, it is essential to study adversarial attacks and defense of structured prediction models (Jia and Liang, 2017; Wang et al., 2019).",
      "startOffset" : 252,
      "endOffset" : 292
    }, {
      "referenceID" : 29,
      "context" : "Since structured prediction tasks such as sequence labeling and dependency parsing are critical building blocks of many natural language processing (NLP) systems, it is essential to study adversarial attacks and defense of structured prediction models (Jia and Liang, 2017; Wang et al., 2019).",
      "startOffset" : 252,
      "endOffset" : 292
    }, {
      "referenceID" : 33,
      "context" : "All adversarial attackers for NLP tasks face general challenges related to gradient computation of discrete inputs, grammatical correctness, and meaning preservation (Zhang et al., 2019a; Jia and Liang, 2017; Wang et al., 2019; Cheng et al., 2019b, 2020b).",
      "startOffset" : 166,
      "endOffset" : 255
    }, {
      "referenceID" : 15,
      "context" : "All adversarial attackers for NLP tasks face general challenges related to gradient computation of discrete inputs, grammatical correctness, and meaning preservation (Zhang et al., 2019a; Jia and Liang, 2017; Wang et al., 2019; Cheng et al., 2019b, 2020b).",
      "startOffset" : 166,
      "endOffset" : 255
    }, {
      "referenceID" : 29,
      "context" : "All adversarial attackers for NLP tasks face general challenges related to gradient computation of discrete inputs, grammatical correctness, and meaning preservation (Zhang et al., 2019a; Jia and Liang, 2017; Wang et al., 2019; Cheng et al., 2019b, 2020b).",
      "startOffset" : 166,
      "endOffset" : 255
    }, {
      "referenceID" : 27,
      "context" : "Specifically, we attack two typical models (the sentiment classifier (Ren et al., 2019) for the classification task and the dependency parser (Dozat and Manning, 2017) for structured prediction) using the widely-used word-substitution attacker (Goodfellow et al.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : ", 2019) for the classification task and the dependency parser (Dozat and Manning, 2017) for structured prediction) using the widely-used word-substitution attacker (Goodfellow et al.",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : ", 2019) for the classification task and the dependency parser (Dozat and Manning, 2017) for structured prediction) using the widely-used word-substitution attacker (Goodfellow et al., 2015) for structured prediction task).",
      "startOffset" : 164,
      "endOffset" : 189
    }, {
      "referenceID" : 30,
      "context" : "(2020) and 15% from (Wang et al., 2021) are changed, although they both carefully design specific rules based on linguistic prior knowledge to preserve the structures.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 34,
      "context" : "We use BERTSCORE (Zhang et al., 2019b) to evaluate the similarity, which matches each token in x to a token in xori to compute recall, and each token in xori to a token in x to compute precision, finally combines precision and recall to compute an F1 measure.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "Such metric correlates better with human judgment than traditional measures such as BLEU (Papineni et al., 2002).",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : "Metropolis-Hastings Sampling To get out of the local optima that BS is commonly stuck in, we use a sampling-based approach – MetropolisHastings Sampling (Metropolis et al., 1953; Hastings, 1970; Chib and Greenberg, 1995) – to explore",
      "startOffset" : 153,
      "endOffset" : 220
    }, {
      "referenceID" : 13,
      "context" : "Metropolis-Hastings Sampling To get out of the local optima that BS is commonly stuck in, we use a sampling-based approach – MetropolisHastings Sampling (Metropolis et al., 1953; Hastings, 1970; Chib and Greenberg, 1995) – to explore",
      "startOffset" : 153,
      "endOffset" : 220
    }, {
      "referenceID" : 5,
      "context" : "Metropolis-Hastings Sampling To get out of the local optima that BS is commonly stuck in, we use a sampling-based approach – MetropolisHastings Sampling (Metropolis et al., 1953; Hastings, 1970; Chib and Greenberg, 1995) – to explore",
      "startOffset" : 153,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : "Studies have shown that with a proper balance between exploration and exploitation, the optimization performance can be substantially improved (Črepinšek et al., 2013; Wilson et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 188
    }, {
      "referenceID" : 31,
      "context" : "Studies have shown that with a proper balance between exploration and exploitation, the optimization performance can be substantially improved (Črepinšek et al., 2013; Wilson et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "The PLM used to select candidate word set is RoBERTa (Liu et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "We choose the Deep Biaffine parser (Dozat and Manning, 2017) as the victim parser PV and two other SOTA dependency parsers as the reference parsers: StackPTR (Ma et al.",
      "startOffset" : 35,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "We choose the Deep Biaffine parser (Dozat and Manning, 2017) as the victim parser PV and two other SOTA dependency parsers as the reference parsers: StackPTR (Ma et al., 2018) as parser PA and BiST (Kiperwasser and Goldberg, 2016) as parser PB .",
      "startOffset" : 158,
      "endOffset" : 175
    }, {
      "referenceID" : 16,
      "context" : ", 2018) as parser PA and BiST (Kiperwasser and Goldberg, 2016) as parser PB .",
      "startOffset" : 30,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "We use the perplexity of GPT-2 (Radford et al., 2019) to evaluate the sentence fluency (Fluency), lower is better.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 34,
      "context" : "We use the BERTSCORE (Zhang et al., 2019b) to evaluate the meaning preservation (MP).",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : "We use the tagger from Ma and Hovy (2016) as our victim tagger TV , and we choose two state-of-theart taggers: Stanford POS tagger (Toutanova et al., 2003) and Senna tagger (Collobert et al.",
      "startOffset" : 131,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : ", 2003) and Senna tagger (Collobert et al., 2011) as our reference tagger TA and tagger TB , respectively.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "The criterion for selecting these reference parsers is that they should be diverse besides having a high parsing accuracy (Han et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "parser (Dozat and Manning, 2017) as the reference parser PA and the StackPTR (Ma et al.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "parser (Dozat and Manning, 2017) as the reference parser PA and the StackPTR (Ma et al., 2018) as the victim parser PV .",
      "startOffset" : 77,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "tasks such as text classification (Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al.",
      "startOffset" : 34,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "tasks such as text classification (Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al.",
      "startOffset" : 34,
      "endOffset" : 77
    }, {
      "referenceID" : 35,
      "context" : ", 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2020a) and dialogue systems (Cheng et al.",
      "startOffset" : 29,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : ", 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2020a) and dialogue systems (Cheng et al.",
      "startOffset" : 29,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : ", 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2020a) and dialogue systems (Cheng et al.",
      "startOffset" : 29,
      "endOffset" : 92
    }, {
      "referenceID" : 36,
      "context" : "plored on structured prediction tasks, such as dependency parsing (Zheng et al., 2020; Han et al., 2020).",
      "startOffset" : 66,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "plored on structured prediction tasks, such as dependency parsing (Zheng et al., 2020; Han et al., 2020).",
      "startOffset" : 66,
      "endOffset" : 104
    } ],
    "year" : 0,
    "abstractText" : "Adversarial attack of structured prediction models faces various challenges such as the difficulty of perturbing discrete words, the sentence quality issue, and the sensitivity of outputs to small perturbations. In this work, we introduce SHARP, a new attack method that formulates the black-box adversarial attack as a search-based optimization problem with a specially designed objective function considering sentence fluency, meaning preservation and attacking effectiveness. Additionally, three different searching strategies are analyzed and compared, i.e., Beam Search, MetropolisHastings Sampling, and Hybrid Search. We demonstrate the effectiveness of our attacking strategies on two challenging structured prediction tasks: part-of-speech (POS) tagging and dependency parsing. Through automatic and human evaluations, we show that our method performs a more potent attack compared with pioneer arts. Moreover, the generated adversarial examples can be used to successfully boost the robustness and performance of the victim model via adversarial training.",
    "creator" : null
  }
}
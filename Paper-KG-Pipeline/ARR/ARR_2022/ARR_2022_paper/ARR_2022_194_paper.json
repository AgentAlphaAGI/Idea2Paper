{
  "name" : "ARR_2022_194_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, pretrained language models (PLMs) have achieved huge success in NLP (Qiu et al., 2020). Many PLMs such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al., 2019). In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.\nHow to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021a). Besides naively finetuning PLMs with labeled data in downstream\ntasks, many works explore more effective and robust PLM finetuning methods (Chen et al., 2020; Jiang et al., 2020; Lee et al., 2020; Aghajanyan et al., 2021; Zhang et al., 2021; Xu et al., 2021). For example, Chen et al. (2020) proposed a RecAdam approach that adds a penalty item to minimize the L2 distance between fine-tuned models and the pretrained models, where the pernalty intensity is timevariant during finetuning. Lee et al. (2020) proposed a Mixout method to randomly replace parts of model parameters with their original pretrained weights. These finetuning methods mainly focus on preventing PLMs from overfitting limited labeled data in downstream tasks. However, PLMs have been well trained in the self-supervised pretraining tasks, and it can be difficult for them to overcome the barrier between pretraining and downstream tasks as well as the gaps of their domains during finetuning (Roberts et al., 2020), which may lead to a suboptimal performance especially when labeled data in downstream tasks is insufficient.\nIn this paper, we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks. The key idea of NoisyTune is to add a little noise to perturb PLMs parameters before finetuning, which can help prevent them from overfitting the signals in the pretraining tasks, and reduce the gap between pretraining and downstreaming tasks. Since different types of parameters in PLMs may have different characteristics, we propose a matrix-wise perturbing method that adds uniform noise with different intensities according to the standard deviations of different parameter matrices for better adaptation. We conduct experiments on two widely used NLP benchmarks, i.e., GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding. The results show that NoisyTune can consistently boost the performance of different PLMs in many downstream NLP tasks."
    }, {
      "heading" : "2 NoisyTune",
      "text" : "In this section, we introduce our proposed NoisyTune approach that adds noise to perturb PLMs for more effective finetuning. Since the parameters of PLMs are well tuned in the pretraining tasks and may overfit self-supervision signals, it may be difficult for them to adapt to downstream tasks especially when labeled data in downstream tasks are rather limited. Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, we explore adding noise to PLMs before finetuning to “explore” other parameter spaces to reduce the problem of overfitting pretraining tasks. We denote the parameter matrices (or scalars/vectors) in a PLM as [W1,W2, ...,WN ], where N is the number of parameter types. In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015). However, different parameter matrices in the PLM have very different characteristics. For example, the self-attention parameters and the feed-forward network parameters usually have very different properties (Wang et al., 2020). Thus, adding global noise may not be optimal for keeping good model utility. To solve this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of parameter matrices. We denote the perturbed version of the parameter matrix Wi as W̃i, which is computed as follows:\nW̃i = Wi + U(− λ 2 , λ 2 ) ∗ std(Wi), (1)\nwhere std stands for standard deviation, the function U(a, b) means uniform distribution noise ranged from a to b, and λ is a hyperparameter that controls the relative noise intensity.1 In this way, parameters with higher variance will be added with stronger noise. In addition, in some PLMs there exist constant matrices, such as token type embeddings in RoBERTa (Liu et al., 2019). They will not be perturbed because their standard deviation is 0. This will ensure that these constant matrices will not be accidentally activated by additional noise."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets and Experimental Settings",
      "text" : "We conduct extensive experiments on two widely used benchmarks for PLM evaluation. The first\n1Note that U(a, b) is a matrix with the same shape with Wi rather than a scalar.\none is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains tasks like natural language inference, sentiment analysis and sentence similarity evaluation. The second one is XTREME (Hu et al., 2020), which is a benchmark for multilingual language understanding. It covers 40 languages and contains four groups of tasks, including sentence classification, structured prediction, sentence retrieval and question answering. More details of these benchmarks can refer to their original papers and official websites. Since the test labels of GLUE are not released, following (Bao et al., 2020) we report results on the dev set of GLUE. The XTREME results are evaluated on the test set. The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME. The detailed hyperparameter settings are in the appendix. Following (Zheng et al., 2021b), in sentence retrieval tasks we first train the models on the XNLI dataset, and then use the average of token representations produced by the hidden layer that yields the best performance. In order not to harm the alignment of token embeddings across different languages, We do not add noise to the token embeddings in multilingual PLMs. We repeat experiments 5 times with different random seeds and report the average scores."
    }, {
      "heading" : "3.2 Performance Evaluation",
      "text" : "On the GLUE benchmark, we compare directly finetuning the base version of BERT (Devlin et al., 2019), XLNET (Yang et al., 2019), RoBERTa (Liu et al., 2019) and ELECTRA (Clark et al., 2020) as well as finetuning them after applying NoisyTune. On the XTREME benchmark, we compare both base and large versions of XLM-R (Conneau et al., 2020) and their variants processed by NoisyTune. The results on the two benchmarks are shown in Tables 1 and 2, respectively. On the XTREME datasets, we report two types of results, i.e., zeroshot crosslingual transfer from English to other languages or learning models on both English and translated data. From the results, we can see that NoisyTune can consistently improve the performance of different PLMs on different tasks. In addition, the performance improvement on relatively small datasets is usually larger (e.g., RTE, CoLA and WNLI). This indicates that when labeled training data in downstream tasks is not redundant, it may be more difficult to well adapt PLMs to downstream tasks from the parameter space well tuned in\npretraining tasks. Thus, properly perturbing PLMs with noise can explore different parameter spaces and meanwhile keep useful knowledge encoded in pretraining tasks."
    }, {
      "heading" : "3.3 Influence of Noise Type",
      "text" : "Next, we study the influence of using different kinds of noise on NoisyTune. We compare five methods, including (1) basic method without noise; (2) Gaussian noise with a global distribution; (3) uniform noise with a global distribution; (4) matrixwise Gaussian noise; (5) matrix-wise uniform noise. The results on GLUE are shown in Fig. 1. We find that adding global noise with same distributions to the PLM parameters will harm the model perfor-\nmance. This is because different parameter matrices have very different distributions, and simply adding global noise is not appropriate. In addition, we find an interesting phenomenon that adding uniform noise is better than using Gaussian noise. This may be because Gaussian noise has wider ranges and some outliers may affect model performance. Thus, we prefer using matrix-wise uniform noise in our NoisyTune method."
    }, {
      "heading" : "3.4 Analysis on NoiseTune",
      "text" : "We then analyze the influence of NoisyTune on finetuning. We show the accuracy of the BERT model with or without NoisyTune on the MRPC dataset in\nFig. 2.2 The interval between two adjacent checkpoints is 50 iterations. We find NoisyTune can consistently improve PLMs at different finetuning steps. This may be because the perturbed PLMs may have lower risks in overfitting pretraining tasks and have better generalization abilities.\nTo further study the impact of NoisyTune on model finetuning, we show the relative changes of the L1-norms of different kinds of paramters in the BERT model during training on MRPC and STS-B in Fig. 3.3 Since the noise we added is zeromean, the absolute parameter L1-norms will not be changed too much. However, we can see that the relative change of L1-norms becomes smaller when NoisyTune is applied, which indicates that the model uses smaller paces towards convergence. This means that directly finetuning PLMs may need more updates to adapt to downstream tasks, which may be due to the overfitting of pretraining tasks\n2We observe similar phenomena on other datasets. 3On different datasets the evolution of parameter L1-norms may be varied, while the scale of overall norm change usually become smaller.\nand their gaps with downstream tasks. Our NoisyTune approach provides a simple way to mitigate this problem to empower PLM finetuning."
    }, {
      "heading" : "3.5 Empower Other Finetuning Methods",
      "text" : "Our NoisyTune method also has the potential to empower other PLM finetuning techniques. We compare the performance of the original RecAdam (Chen et al., 2020) and Mixout (Lee et al., 2020) method and their variants combined with NoisyTune. The results are shown in Fig. 4. We find that combining NoisyTune with existing PLM finetuning techniques can further improve the performance. This is because NoisyTune aims to address the overfitting of pretraining signals while these methods aim to prevent overfitting in downstream tasks, thereby they can be empowered by NoisyTune to improve model performance."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we propose a very simple but effective method named NoisyTune, which adds a little noise to PLMs before finetuning for better transferability from pretraining tasks to downstream tasks. In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of different parameter matrices in PLMs, which can consider the varied characteristics of different types of parameters. Extensive experiments on the monolingual GLUE benchmark and the multilingual XTREME benchmark demonstrate the NoisyTune can consistently improve the performance of different PLMs in various downstream tasks."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Experimental Environment Our experiments are conducted on a cloud Linux server with Ubuntu 16.04 operating system. The codes are written in Python 3.8.10 using the pytorch framework 1.9.0 and the latest huggingface transformers library. The GPU type is Nvidia Tesla A100 with 40GB GPU memory and we use 8 GPUs for parallel experiments.\nA.2 Hyperparameter Settings The searching range of hyperparameters in our work are listed Table 3.\nA.3 Hyperparameter Analysis We study the influence of the relative noise intensity λ on model performance. The average GLUE scores w.r.t. different λ values are shown in Fig. 5. We find that the performance improvement is largest when the value of λ is between 0.1 and 0.15. It indicates that a smaller noise scale cannot effectively explore good parameter spaces, while a larger noise scale will also lead to suboptimal performance since the encoded knowledge in the pretraining stage becomes inaccurate."
    } ],
    "references" : [ {
      "title" : "Better fine-tuning by reducing representational collapse",
      "author" : [ "Armen Aghajanyan", "Akshat Shrivastava", "Anchit Gupta", "Naman Goyal", "Luke Zettlemoyer", "Sonal Gupta." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Aghajanyan et al\\.,? 2021",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Unilmv2: Pseudomasked language models for unified language model pre-training",
      "author" : [ "Hangbo Bao", "Li Dong", "Furu Wei", "Wenhui Wang", "Nan Yang", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Songhao Piao", "Ming Zhou" ],
      "venue" : "PMLR",
      "citeRegEx" : "Bao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Recall and learn: Fine-tuning deep pretrained language models with less forgetting",
      "author" : [ "Sanyuan Chen", "Yutai Hou", "Yiming Cui", "Wanxiang Che", "Ting Liu", "Xiangzhan Yu." ],
      "venue" : "EMNLP, pages 7870–7881.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "NIPS, volume 32, pages 7059–7069.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "NIPS, pages 13063–13075.",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "ICML, pages 4411–4421. PMLR.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks",
      "author" : [ "Haoyang Huang", "Yaobo Liang", "Nan Duan", "Ming Gong", "Linjun Shou", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "EMNLP-IJCNLP, pages 2485–2494.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
      "author" : [ "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Tuo Zhao." ],
      "venue" : "ACL, pages 2177–",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep learning",
      "author" : [ "Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton." ],
      "venue" : "nature, 521(7553):436–444.",
      "citeRegEx" : "LeCun et al\\.,? 2015",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2015
    }, {
      "title" : "Mixout: Effective regularization to finetune large-scale pretrained language models",
      "author" : [ "Cheolhyoung Lee", "Kyunghyun Cho", "Wanmo Kang." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "Xipeng Qiu", "Tianxiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang." ],
      "venue" : "Science China Technological Sciences, pages 1–26.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "How much knowledge can you pack into the parameters of a language model",
      "author" : [ "Adam Roberts", "Colin Raffel", "Noam Shazeer" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Roberts et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "BlackboxNLP, pages 353–355.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Linformer: Selfattention with linear complexity",
      "author" : [ "Sinong Wang", "Belinda Z Li", "Madian Khabsa", "Han Fang", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2006.04768.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Raise a child in large language model: Towards effective and generalizable fine-tuning",
      "author" : [ "Runxin Xu", "Fuli Luo", "Zhiyuan Zhang", "Chuanqi Tan", "Baobao Chang", "Songfang Huang", "Fei Huang." ],
      "venue" : "EMNLP, pages 9514–9528.",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "NeurIPS, pages 5753– 5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Yisong Yue", "Thorsten Joachims." ],
      "venue" : "ICML, pages 1201– 1208.",
      "citeRegEx" : "Yue and Joachims.,? 2009",
      "shortCiteRegEx" : "Yue and Joachims.",
      "year" : 2009
    }, {
      "title" : "Revisiting fewsample bert fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Consistency regularization for cross-lingual fine-tuning",
      "author" : [ "Bo Zheng", "Li Dong", "Shaohan Huang", "Wenhui Wang", "Zewen Chi", "Saksham Singhal", "Wanxiang Che", "Ting Liu", "Xia Song", "Furu Wei." ],
      "venue" : "ACLIJCNLP, pages 3403–3417.",
      "citeRegEx" : "Zheng et al\\.,? 2021a",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Consistency regularization for cross-lingual fine-tuning",
      "author" : [ "Bo Zheng", "Li Dong", "Shaohan Huang", "Wenhui Wang", "Zewen Chi", "Saksham Singhal", "Wanxiang Che", "Ting Liu", "Xia Song", "Furu Wei." ],
      "venue" : "ACL, pages 3403–3417.",
      "citeRegEx" : "Zheng et al\\.,? 2021b",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "In recent years, pretrained language models (PLMs) have achieved huge success in NLP (Qiu et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "Many PLMs such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019) have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : ", 2019) have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 154
    }, {
      "referenceID" : 5,
      "context" : "In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al.",
      "startOffset" : 62,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 22,
      "context" : "How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021a).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods (Chen et al., 2020; Jiang et al., 2020; Lee et al., 2020; Aghajanyan et al., 2021; Zhang et al., 2021; Xu et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 258
    }, {
      "referenceID" : 10,
      "context" : "Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods (Chen et al., 2020; Jiang et al., 2020; Lee et al., 2020; Aghajanyan et al., 2021; Zhang et al., 2021; Xu et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 258
    }, {
      "referenceID" : 12,
      "context" : "Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods (Chen et al., 2020; Jiang et al., 2020; Lee et al., 2020; Aghajanyan et al., 2021; Zhang et al., 2021; Xu et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 258
    }, {
      "referenceID" : 0,
      "context" : "Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods (Chen et al., 2020; Jiang et al., 2020; Lee et al., 2020; Aghajanyan et al., 2021; Zhang et al., 2021; Xu et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 258
    }, {
      "referenceID" : 21,
      "context" : "Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods (Chen et al., 2020; Jiang et al., 2020; Lee et al., 2020; Aghajanyan et al., 2021; Zhang et al., 2021; Xu et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 258
    }, {
      "referenceID" : 18,
      "context" : "Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods (Chen et al., 2020; Jiang et al., 2020; Lee et al., 2020; Aghajanyan et al., 2021; Zhang et al., 2021; Xu et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 258
    }, {
      "referenceID" : 15,
      "context" : "However, PLMs have been well trained in the self-supervised pretraining tasks, and it can be difficult for them to overcome the barrier between pretraining and downstream tasks as well as the gaps of their domains during finetuning (Roberts et al., 2020), which may lead to a suboptimal performance especially when labeled data in downstream tasks is insufficient.",
      "startOffset" : 232,
      "endOffset" : 254
    }, {
      "referenceID" : 16,
      "context" : ", GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : ", 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding.",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, we explore adding noise to PLMs before finetuning to “explore” other parameter spaces to reduce the problem of overfitting pretraining tasks.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "For example, the self-attention parameters and the feed-forward network parameters usually have very different properties (Wang et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "In addition, in some PLMs there exist constant matrices, such as token type embeddings in RoBERTa (Liu et al., 2019).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains tasks like natural language inference, sentiment analysis and sentence similarity evaluation.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "The second one is XTREME (Hu et al., 2020), which is a benchmark for multilingual language understanding.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "Since the test labels of GLUE are not released, following (Bao et al., 2020) we report results on the dev set of GLUE.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "Following (Zheng et al., 2021b), in sentence retrieval tasks we first train the models on the XNLI dataset, and then use the average of token representations produced by the hidden layer that yields the best performance.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "On the GLUE benchmark, we compare directly finetuning the base version of BERT (Devlin et al., 2019), XLNET (Yang et al.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : ", 2019), XLNET (Yang et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : ", 2019), RoBERTa (Liu et al., 2019) and ELECTRA (Clark et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : ", 2019) and ELECTRA (Clark et al., 2020) as well as finetuning them after applying NoisyTune.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "On the XTREME benchmark, we compare both base and large versions of XLM-R (Conneau et al., 2020) and their variants processed by NoisyTune.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "We compare the performance of the original RecAdam (Chen et al., 2020) and Mixout (Lee et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : ", 2020) and Mixout (Lee et al., 2020) method and their variants combined with NoisyTune.",
      "startOffset" : 19,
      "endOffset" : 37
    } ],
    "year" : 0,
    "abstractText" : "Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks. It can be difficult for vanilla finetuning methods to overcome the barrier between pretraining and downstream tasks, which leads to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning. More specifically, we propose a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices, which can consider the varied characteristics of different types of parameters in PLMs. Extensive experiments on the GLUE English benchmark and the XTREME multilingual benchmark show that NoisyTune can consistently improve the performance of different PLMs in many downstream tasks.",
    "creator" : null
  }
}
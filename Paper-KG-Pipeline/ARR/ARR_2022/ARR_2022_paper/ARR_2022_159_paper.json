{
  "name" : "ARR_2022_159_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Causal Distillation for Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Large pretrained language models have improved performance across a wide range of NLP tasks, but can be costly due to their large size. Distillation seeks to reduce these costs while maintaining performance by training a simpler student model from a larger teacher model (Hinton et al., 2015; Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).\nHinton et al. (2015) propose model distillation with an objective that encourages the student to produce output logits similar to those of the teacher while also supervising with a task-specific objective (e.g., sequence classification). Sanh et al. (2019), Sun et al. (2019), and Jiao et al. (2019) adapt this method, strengthening it with additional\nsupervision to align internal representations between the two models. However, these approaches may push the student model to match all aspects of internal states of the teacher model irrespective of their causal role in the network’s computation. This motivates us to develop a method that focuses on aligning the causal role of representations in the student and teacher models.\nWe propose augmenting standard distillation with a new objective that pushes the student to become a causal abstraction (Beckers and Halpern, 2019; Beckers et al., 2020; Geiger et al., 2021a) of the teacher model: the simpler student will faithfully model the causal effect of teacher representations on output. To achieve this, we employ the interchange intervention training (IIT) method of Geiger et al. (2021b). The distillation interchange intervention training objective (DIITO) aligns a high-level student model with a low-level teacher model and performs interchange interventions (swapping of aligned internal states); during training the high-level model is pushed to conform to the causal dynamics of the low-level model.\nFigure 1 shows a schematic example of this process. Here, hidden layer 2 of the student model (bottom) is aligned with layers 3 and 4 of the teacher model. The figure depicts a single interchange intervention replacing aligned states in the left-hand models with those from the right-hand models. This results in a new network evolution that is shaped both by the original input and the interchanged hidden states. It can be interpreted as a certain kind of counterfactual as shown in Figure 1: what would the output be for the sentence “I ate some ⟨MASK⟩.” if the activation values for the second token at the middle two layers were set to the values they have for the input “The water ⟨MASK⟩ solid.”? DIITO then pushes the student model to output the same logits as the teacher, i.e., matching the teacher’s output distribution under the counterfactual setup.\nTo assess the contribution of distillation with DIITO, we begin with BERTBASE (Devlin et al., 2019a) and distill it under various alignments between student and teacher while pretraining on the WikiText-103M corpus (Merity et al., 2016) achieving −2.24 perplexity on the MLM task compared to standard DistilBERT trained on the same data. We then fine-tune the best performing distilled models and find consistent performance improvements compared to standard DistilBERT trained with the same setting on the GLUE benchmark (+1.77%), CoNLL-2003 name-entity recognition (+0.38% on F1 score), and SQuAD v1.1 (+2.46% on EM score)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Distillation was first introduced in the context of computer vision (Hinton et al., 2015) and has since been widely explored for language models (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019). For example, Sanh et al. (2019) propose to extract information not only from output probabilities of the last layer in the teacher model, but also from intermediate layers in the fine-tuning stage. Recently, Rotman et al. (2021) adapt causal analysis methods to estimate the effects of inputs on predictions to\ncompress models for better domain adaptation. In contrast, we focus on imbuing the student with the causal structure of the teacher.\nInterventions on neural networks were originally used as a structural analysis method aimed at illuminating neural representations and their role in network behavior (Feder et al., 2021; Pryzant et al., 2021; Vig et al., 2020; Elazar et al., 2020; Giulianelli et al., 2020; Geiger et al., 2020, 2021a). Geiger et al. (2021b) extend these methods to network optimization. We contribute to this existing research by adapting intervention-based optimization to the task of language model distillation."
    }, {
      "heading" : "3 Causal Distillation",
      "text" : "Here, we define our distillation training procedure. See Algorithm 1 in the Appendix for a summary.\nGETVALS. The GETVALS operator is an activation-value retriever for a neural model. Given a neural model M containing a set of neurons N (an internal representations) and an appropriate input x, GETVALS(M,x,N) is the set of values that N takes on when processing x. In the case that N represents the neurons corresponding to the final output, GETVALS(M,x,N) is the output of\nmodel M when processing x (i.e., output from a standard forward call of a neural model).\nSETVALS. The SETVALS operator is a function generator that defines a new neural model with a computation graph that specifies an intervention on the original model M (Pearl, 2009; Spirtes et al., 2001). SETVALS(M,N,v) is the new neural model where the neurons N are set to constant values v. Because we overwrite neurons with v in-place, gradients can back-propagate through v.\nInterchange Intervention. An interchange intervention combines GETVALS and SETVALS operations. First, we randomly sample a pair of examples from a training dataset (x1,y1), (x2,y2) ∈ D. Next, where N is the set of neurons that we are targeting for intervention, we define Mx1N to abbreviate the new neural model as follows:\nSETVALS ( M,N, GETVALS(M,x1,N) ) (1)\nThis is the version of M obtained from setting the values of N to be those we get from processing input x1. The interchange intervention targeting N with x1 as the source input and x2 as the base input is then defined as follows:\nINTINV(M,N,x1,x2) def =\nGETVALS(Mx1N ,x2,N y) (2)\nwhere Ny are the output neurons. In other words, INTINV(M,N,x1,x2) is the output state we get from M for input x2 but with the neurons N set to the values obtained when processing input x1.\nDIITO. DIITO employs T as the teacher model, S as the student model, D as the training inputs to both models, and Π as an alignment that maps sets of student neurons to sets of teacher neurons. For each set of student neurons NS in the domain of Π, we define DIITO loss as:\nLDIITOCE def =∑\nx1,x2∈D CES\n( INTINV(S,NS ,x1,x2),\nINTINV(T ,Π(NS),x1,x2) ) (3)\nwhere CES is the smoothed cross-entropy loss measuring the divergences of predictions, under interchange, between the teacher and the student model.\nDistillation Objectives. We adopt the standard distillation objectives from DistilBERT (Sanh et al., 2019) (defined formally in Appendix A.1): LMLM for the task-specific loss for the student model, LCE for the loss measuring the divergence between the student and teacher outputs on masked tokens, and LCos for the loss measuring the divergence between the student and teacher contextualized representations on masked tokens in the last layer. Our final training objective for the student is a linear combination of the four training objectives reviewed above: LMLM, LCE, LCos, and LDIITOCE . In a further experiment, we introduce a fifth objective LDIITOCos which is identical to LCos, except the teacher and student are undergoing interchange interventions (see Appendix A.2 for details)."
    }, {
      "heading" : "4 Experimental Set-up",
      "text" : "Student and Teacher Models. Our two students have the standard BERT architecture, with 12 heads with a hidden dimension of 768. The larger student has 6 layers, the smaller 3 layers. Our pretrained teacher has the same architecture, except with 12 layers. Following practices introduced by Sanh et al. (2019), we initialize our student model with weights from skipped layers (one out of four layers) in the teacher model. We use WikiText for distillation to simulate a practical situation with a limited computation budget. We leave the exploration of our method on larger datasets for future research.\nAlignment. Our teacher and student BERT models create columns of neural representations above each token with each row created by the feedforward layer of a Transformer block, as in Figure 1. We define LT and LS to be the number of layers in the student and teacher, respectively. In addition, we define Sji and T j i to be the representations in the ith row and jth column in the student and teacher, respectively. An alignment Π is a partial function from student representations to sets of teacher representations. We test three alignments:\nFULL Π is defined on all student representations: Π(Sji ) = {T j 4i+k : 0 ≤ k < LT /LS}\nMIDDLE Π is defined for the row LS 2: Π(SjLS 2) = {T j LT 2} LATE Π is defined on the student representations in the first and second rows: Π(Sj1) = {T j LT −2} and Π(S j 2) = {T j LT −1}\nFor each training iteration, we randomly select one aligned student layer to perform the interchange intervention, and we randomly select 30% of token embeddings for alignment for each sequence. We experiment with three conditions with the FULL alignment: consecutive tokens (DIITOFULL), random tokens (DIITOFULL+Random) and masked tokens (DIITOFULL+Masked). We also include LDIITOCos to the FULL alignment (DIITOFULL+LDIITOCos )."
    }, {
      "heading" : "5 Results",
      "text" : "Language Modeling. We first evaluate our models using perplexity on the held-out evaluation data from WikiText. As shown in Table 1, DIITO brings performance gains for all alignments. Our best result is from the FULL alignment with the LCos (DIITOFULL+LDIITOCos ), which has -2.24 perplexity\ncompared to standard DistilBERT trained with the same amount of data.\nGLUE. The GLUE benchmark (Wang et al., 2018) covers different natural language understanding tasks. We report averaged GLUE scores on the development sets by fine-tuning our distilled models in Table 1. Individual task performance score of each GLUE task is included in Table 2 in the Appendix. The results suggest that distilled models with DIITO lead to consistent improvements over standard DistilBERT trained under the same setting, with our best result (DIITOFULL+LDIITOCos ) being +1.77% higher.\nNamed Entity Recognition. We also evaluate our models on the CoNLL-2003 Named Entity Recognition task (Tjong Kim Sang and De Meulder, 2003). We report accuracy and Macro-F1 scores on the development sets. We fine-tune our models for three epochs. Our best performing model (DIITOMIDDLE) numerically surpasses not only standard DistilBERT (+0.38% on F1 score) trained under the same setting, but also its teacher, BERTBASE (+0.05% on F1 score). Though these improvements are small, in this case distillation produces a smaller model with better performance.\nQuestion Answering. Finally, we evaluate on a question answering task, SQuAD v1.1 (Rajpurkar et al., 2016). We report Exact Match and MacroF1 on the development sets as our evaluation metrics. We fine-tune our models for two epochs. DIITO again yields marked improvements (Table 1). Our best result is from the vanilla FULL alignment (DIITOFULL), with +2.46% on standard DistilBERT trained under the same setting."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we explored distilling a teacher by training a student to capture the causal dynamics of its computations. Across a wide range of NLP tasks, we find that DIITO leads to improvements, with the largest gains coming from the models that use the richest alignment between student and teacher. Our results also demonstrate that DIITO performs on-par (maintaining 97% of performance on GLUE tasks) with standard DistilBERT (Sanh et al., 2019) while consuming 97% less training data. These findings suggest that DIITO is a promising tool for effective model distillation."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Standard Distillation Objectives",
      "text" : "In our setting, our teacher model T is a BERT model, and our student model S is a shallower BERT model with fewer layers.\nAssume that we randomly draw a training example (x1,y1) ∈ D, where x1 is the input to our models and y1 is the corresponding ground truth (the token prediction at each masked position). We denote the model predictions (output logits) as T (x1) and S(x1). Additionally, we denote the contextualized representation for tokens for x1 at the last layer as BERTT (x1) and BERTS(x1).\nWe adopt the three standard distillation objectives of Sanh et al. (2019):\nLMLM The masked language modeling loss of the student model calculated over all examples using the cross-entropy loss as follows:∑\n{x1,y1}∈D\nCE(S(x1),y1) (4)\nLCE Following Hinton et al. (2015), the smoothed cross-entropy loss measuring the divergence between the student and teacher outputs as follows: ∑\nx1∈D CES(S(x1), T (x1)) (5)\nLCos The cosine embedding loss defined in terms of the final hidden states of the teacher and the student as follows:∑\nx1∈D COS(BERTS(x1),BERTT (x1)) (6)\nAs a result, comparing to standard DistilBERT, DIITO essentially adds a new type of objective by pushing the student model to become a causal abstraction of the teacher model."
    }, {
      "heading" : "A.2 Causal Distillation Objectives",
      "text" : "In addition to our causal loss LDIITOCE , we also propose a new loss LDIITOCos which is identical to LCos with interchange interventions. In this section, we provide a formal definition for LDIITOCos .\nWe denote our teacher and student models as T and S respectively. Using the notational conventions from Section 3, we use NyT and N y S to represent the neurons corresponding to the final output for each model. Likewise, we use NLTT and NLSS to represent the neurons representing contextualized representation for each token after the final BERT layer.\nAssuming we randomly sample a pair of examples from a training dataset (x1,y1), (x2,y2) ∈ D, we can then rewrite our causal loss LDIITOCE by rearranging Eqn. 2 and Eqn. 3 as follows:\n∑ x1,x2∈D CES ( GETVALS(Mx1S ,x2,N y S),\nGETVALS(Mx1T ,x2,N y T ) ) (7) where MxiS and M xi T are derived as in Eqn. 1 for each model respectively. Crucially, Eqn. 7 can be regarded as the causal form of the standard smoothed cross-entropy loss with interchange intervention. Likewise, we can further define the LDIITOCos as:\n∑ x1,x2∈D COS ( GETVALS(Mx1S ,x2,N LS S ),\nGETVALS(Mx1T ,x2,N LT T ) ) (8) with adjusted interchange alignments for NLTT\nand NLSS ."
    }, {
      "heading" : "A.3 Distillation Set-up",
      "text" : "We adapt the open-source Hugging-face implementation for model distillation (Wolf et al., 2020).1 We distill our models on the MLM pretraining task (Devlin et al., 2019b). We use large gradient accumulations over batches as in Sanh et al. (2019) for better performance. Specifically, we distill all models for three epochs for an effective batch size of 240. In contrast to the setting of 4K per\n1https://github.com/huggingface/transformers\nbatch in the BERT (Devlin et al., 2019b) and DistilBERT Sanh et al. (2019) models, we found that small effective batch size works better for smaller dataset. We weight all objectives equally for all experiments. With our new objectives, the distillation takes approximately 9 hours on 4 NVIDIA A100 GPUs."
    }, {
      "heading" : "A.4 Evaluation Set-up",
      "text" : "GLUE We fine-tune for 25 epochs for the smaller datasets (RTE and CoLA) and 3 epochs for the others. Following Devlin et al. (2019b) and Sanh et al. (2019), we use Matthew’s Correlation for CoLA, F1 for MRPC and QQP, Spearman correlation for STS-B, and accuracy for all the other tasks in GLUE."
    }, {
      "heading" : "A.5 Reproducibility",
      "text" : "To foster reproducible and provide a fair comparison between methods, we distill BERT for each condition with three distinct random seeds. We then fine-tune each model with five distinct random seeds. Consequently, we report results aggregated from three distinct runs for the language modeling task, and 15 distinct runs for others.\nNamed Entity Recognition We follow the experiment set-up as in Hugging-face (Wolf et al., 2020) repository for evaluation for the CoNLL-2003 Named Entity Recognition task (Tjong Kim Sang and De Meulder, 2003). For fine-tuning, we set the learning rate to 5e−5 with an effective batch size of 32 for three epochs.2\nQuestion Answering We follow the experiment set-up as in Sanh et al. (2019) for evaluation for the question answering task, SQuAD v1.1 (Rajpurkar et al., 2016). For fine-tuning, we set the learning rate to 3e−5 with an effective batch size of 48 for two epochs. We set the stride to 128."
    }, {
      "heading" : "A.6 Low-Resource Model Distillation",
      "text" : "We experiment with an extreme case in a lowresource setting where we only distill with 15% of WikiText by keeping other experiment set-up constant. Our results suggest that DIITO training\n2For DistilBERT performance in Table 1on CoNLL2003, we evaluate with a publicly avaliable model downloaded from https://huggingface.co/delpart/ distilbert-base-uncased-finetuned-ner.\nis also beneficial in extremely low-resource settings as shown in Figure 3."
    }, {
      "heading" : "A.7 Layer-wise Ablation",
      "text" : "We further study the effect of DIITO training with respect to the size of the student model through a layer-wise ablation experiment. As shown in Figure 2, we compare GLUE performance for models trained with standard distillation pipeline and with DIITO training (DIITOFULL). Specifically, we compute the averaged GLUE scores following the same procedure described in Section A.5. Our results suggest that DIITO training brings consistent improvements over GLUE tasks with smaller models marking the greatest gains.\nAlgorithm 1 Causal Distillation via Interchange Intervention Training Require: Student model S, teacher model T , student output neurons NyS , alignment Π, shuffled training dataset D. 1: S.train() 2: T .eval() 3: D′ = random.shuffle(D) 4: NyT = Π(N y S)\n5: while not converged do 6: for {x1,y1}, {x2,y2} in iter(D, D′) do 7: NS = sample_student_neurons() 8: NT = Π(NS) 9: with no_grad: 10: Ta = SETVALS( 11: T ,NT , GETVALS(T ,x1,NT )) 12: oT = GETVALS(Ta,x2,NyT ) 13: Sa = SETVALS( 14: S,NS , GETVALS(S,x1,NS)) 15: oS = GETVALS(Sa,x2,NyS) 16: LDIITO = get_loss(oT , oS) 17: Calculate LMLM, LCE, LCos 18: L = LMLM + LCE + LCos + LDIITO 19: L.backward() 20: Step optimizer 21: end while"
    } ],
    "references" : [ {
      "title" : "Approximate causal abstractions",
      "author" : [ "Sander Beckers", "Frederick Eberhardt", "Joseph Y. Halpern." ],
      "venue" : "Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115 of Proceedings of Machine Learning Research, pages 606–615, Tel",
      "citeRegEx" : "Beckers et al\\.,? 2020",
      "shortCiteRegEx" : "Beckers et al\\.",
      "year" : 2020
    }, {
      "title" : "Abstracting causal models",
      "author" : [ "Sander Beckers", "Joseph Y. Halpern." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):2678–2685.",
      "citeRegEx" : "Beckers and Halpern.,? 2019",
      "shortCiteRegEx" : "Beckers and Halpern.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019a",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019b",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Amnesic probing: Behavioral explanation with amnesic counterfactuals",
      "author" : [ "Yanai Elazar", "Shauli Ravfogel", "Alon Jacovi", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.",
      "citeRegEx" : "Elazar et al\\.,? 2020",
      "shortCiteRegEx" : "Elazar et al\\.",
      "year" : 2020
    }, {
      "title" : "CausaLM: Causal Model Explanation Through Counterfactual Language Models",
      "author" : [ "Amir Feder", "Nadav Oved", "Uri Shalit", "Roi Reichart." ],
      "venue" : "Computational Linguistics, pages 1–54.",
      "citeRegEx" : "Feder et al\\.,? 2021",
      "shortCiteRegEx" : "Feder et al\\.",
      "year" : 2021
    }, {
      "title" : "Causal abstractions of neural networks",
      "author" : [ "Atticus Geiger", "Hanson Lu", "Thomas Icard", "Christopher Potts." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Geiger et al\\.,? 2021a",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural natural language inference models partially embed theories of lexical entailment and negation",
      "author" : [ "Atticus Geiger", "Kyle Richardson", "Christopher Potts." ],
      "venue" : "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Net-",
      "citeRegEx" : "Geiger et al\\.,? 2020",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2020
    }, {
      "title" : "Inducing causal structure for interpretable neural networks",
      "author" : [ "Atticus Geiger", "Zhengxuan Wu", "Hanson Lu", "Josh Rozner", "Elisa Kreiss", "Thomas Icard", "Noah D. Goodman", "Christopher Potts." ],
      "venue" : "ArXiv:2112.00826.",
      "citeRegEx" : "Geiger et al\\.,? 2021b",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2021
    }, {
      "title" : "Analysing lexical semantic change",
      "author" : [ "Mario Giulianelli", "Marco Del Tredici", "Raquel Fernández" ],
      "venue" : null,
      "citeRegEx" : "Giulianelli et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Giulianelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:1909.10351.",
      "citeRegEx" : "Jiao et al\\.,? 2019",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Merity et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2016
    }, {
      "title" : "Causality: Models, Reasoning and Inference, 2nd edition",
      "author" : [ "Judea Pearl." ],
      "venue" : "Cambridge University Press, USA.",
      "citeRegEx" : "Pearl.,? 2009",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2009
    }, {
      "title" : "Causal effects of linguistic properties",
      "author" : [ "Reid Pryzant", "D. Card", "Dan Jurafsky", "Victor Veitch", "Dhanya Sridhar." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Pryzant et al\\.,? 2021",
      "shortCiteRegEx" : "Pryzant et al\\.",
      "year" : 2021
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Model compression for domain adaptation through causal effect estimation",
      "author" : [ "Guy Rotman", "Amir Feder", "Roi Reichart." ],
      "venue" : "arXiv preprint arXiv:2101.07086.",
      "citeRegEx" : "Rotman et al\\.,? 2021",
      "shortCiteRegEx" : "Rotman et al\\.",
      "year" : 2021
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Causation, Prediction, and Search, 2nd edition",
      "author" : [ "Peter Spirtes", "Clark N Glymour", "Richard Scheines." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Spirtes et al\\.,? 2001",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2001
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Causal mediation analysis for interpreting neural nlp: The case of gender bias",
      "author" : [ "Jesse Vig", "Sebastian Gehrmann", "Yonatan Belinkov", "Sharon Qian", "Daniel Nevo", "Yaron Singer", "Stuart Shieber" ],
      "venue" : null,
      "citeRegEx" : "Vig et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Vig et al\\.",
      "year" : 2020
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyz-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Distillation seeks to reduce these costs while maintaining performance by training a simpler student model from a larger teacher model (Hinton et al., 2015; Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 212
    }, {
      "referenceID" : 19,
      "context" : "Distillation seeks to reduce these costs while maintaining performance by training a simpler student model from a larger teacher model (Hinton et al., 2015; Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 212
    }, {
      "referenceID" : 17,
      "context" : "Distillation seeks to reduce these costs while maintaining performance by training a simpler student model from a larger teacher model (Hinton et al., 2015; Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "Distillation seeks to reduce these costs while maintaining performance by training a simpler student model from a larger teacher model (Hinton et al., 2015; Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 212
    }, {
      "referenceID" : 1,
      "context" : "with a new objective that pushes the student to become a causal abstraction (Beckers and Halpern, 2019; Beckers et al., 2020; Geiger et al., 2021a) of the teacher model: the simpler student will faithfully model the causal effect of teacher rep-",
      "startOffset" : 76,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "with a new objective that pushes the student to become a causal abstraction (Beckers and Halpern, 2019; Beckers et al., 2020; Geiger et al., 2021a) of the teacher model: the simpler student will faithfully model the causal effect of teacher rep-",
      "startOffset" : 76,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : "with a new objective that pushes the student to become a causal abstraction (Beckers and Halpern, 2019; Beckers et al., 2020; Geiger et al., 2021a) of the teacher model: the simpler student will faithfully model the causal effect of teacher rep-",
      "startOffset" : 76,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "To assess the contribution of distillation with DIITO, we begin with BERTBASE (Devlin et al., 2019a) and distill it under various alignments be-",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "tween student and teacher while pretraining on the WikiText-103M corpus (Merity et al., 2016) achieving −2.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "Distillation was first introduced in the context of computer vision (Hinton et al., 2015) and has since been widely explored for language models (Sun et al.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : ", 2015) and has since been widely explored for language models (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : ", 2015) and has since been widely explored for language models (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : ", 2015) and has since been widely explored for language models (Sun et al., 2019; Sanh et al., 2019; Jiao et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "Interventions on neural networks were originally used as a structural analysis method aimed at illuminating neural representations and their role in network behavior (Feder et al., 2021; Pryzant et al., 2021; Vig et al., 2020; Elazar et al., 2020; Giulianelli et al., 2020; Geiger et al., 2020, 2021a).",
      "startOffset" : 166,
      "endOffset" : 301
    }, {
      "referenceID" : 14,
      "context" : "Interventions on neural networks were originally used as a structural analysis method aimed at illuminating neural representations and their role in network behavior (Feder et al., 2021; Pryzant et al., 2021; Vig et al., 2020; Elazar et al., 2020; Giulianelli et al., 2020; Geiger et al., 2020, 2021a).",
      "startOffset" : 166,
      "endOffset" : 301
    }, {
      "referenceID" : 21,
      "context" : "Interventions on neural networks were originally used as a structural analysis method aimed at illuminating neural representations and their role in network behavior (Feder et al., 2021; Pryzant et al., 2021; Vig et al., 2020; Elazar et al., 2020; Giulianelli et al., 2020; Geiger et al., 2020, 2021a).",
      "startOffset" : 166,
      "endOffset" : 301
    }, {
      "referenceID" : 4,
      "context" : "Interventions on neural networks were originally used as a structural analysis method aimed at illuminating neural representations and their role in network behavior (Feder et al., 2021; Pryzant et al., 2021; Vig et al., 2020; Elazar et al., 2020; Giulianelli et al., 2020; Geiger et al., 2020, 2021a).",
      "startOffset" : 166,
      "endOffset" : 301
    }, {
      "referenceID" : 9,
      "context" : "Interventions on neural networks were originally used as a structural analysis method aimed at illuminating neural representations and their role in network behavior (Feder et al., 2021; Pryzant et al., 2021; Vig et al., 2020; Elazar et al., 2020; Giulianelli et al., 2020; Geiger et al., 2020, 2021a).",
      "startOffset" : 166,
      "endOffset" : 301
    }, {
      "referenceID" : 17,
      "context" : "50 (–) (Wikipedia+BookCorpus) DistilBERT (Sanh et al., 2019) 6 3.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "The SETVALS operator is a function generator that defines a new neural model with a computation graph that specifies an intervention on the original model M (Pearl, 2009; Spirtes et al., 2001).",
      "startOffset" : 157,
      "endOffset" : 192
    }, {
      "referenceID" : 18,
      "context" : "The SETVALS operator is a function generator that defines a new neural model with a computation graph that specifies an intervention on the original model M (Pearl, 2009; Spirtes et al., 2001).",
      "startOffset" : 157,
      "endOffset" : 192
    }, {
      "referenceID" : 17,
      "context" : "We adopt the standard distillation objectives from DistilBERT (Sanh et al., 2019) (defined formally in Appendix A.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "The GLUE benchmark (Wang et al., 2018) covers different natural language understanding tasks.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "Our results also demonstrate that DIITO performs on-par (maintaining 97% of performance on GLUE tasks) with standard DistilBERT (Sanh et al., 2019) while consuming 97% less training data.",
      "startOffset" : 128,
      "endOffset" : 147
    } ],
    "year" : 0,
    "abstractText" : "Distillation efforts have led to language models that are more compact and efficient without serious drops in performance. The standard approach to distillation trains a student model against two objectives: a task-specific objective (e.g., language modeling) and an imitation objective that encourages the hidden states of the student model to be similar to those of the larger teacher model. In this paper, we show that it is beneficial to augment distillation with a third objective that encourages the student to imitate the causal dynamics of the teacher through a distillation interchange intervention training objective (DIITO). DIITO pushes the student model to become a causal abstraction of the teacher model – a faithful model with simpler causal structure. DIITO is fully differentiable, easily implemented, and combines flexibly with other objectives. Compared against standard distillation with the same setting, DIITO results in lower perplexity on the WikiText-103M corpus (masked language modeling) and marked improvements on the GLUE benchmark (natural language understanding), SQuAD (question answering), and CoNLL2003 (named entity recognition).",
    "creator" : null
  }
}
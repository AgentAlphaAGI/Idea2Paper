{
  "name" : "ARR_2022_254_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DEAM: Dialogue Coherence Evaluation using AMR-based Semantic Manipulations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Despite the effectiveness of large pretrained language models (Radford et al., 2019; Lewis et al., 2020) for the dialogue response generation (Zhang et al., 2020; Adiwardana et al., 2020; Ghazarian et al., 2021), such models still face challenges to\nimitate human-human conversations and maintain conversational-level coherence. To better evaluate such models, recent works propose trainable automatic evaluation metrics to benchmark and compare the performance of dialogue models (Wu et al., 2020; Zhang et al., 2021). Most trainable automatic evaluation metrics focus on turn-level interactions, where they learn to assess the quality of one user-system utterance pair (Tao et al., 2018; Huang et al., 2020; Ghazarian et al., 2020). However, these metrics cannot appropriately model the whole conversation flow (Yeh et al., 2021), and thus are insufficient for dialogue-level evaluation.\nCoherence is a conversation-level evaluation metric that measures how well all the utterances in a conversation are unified leading to a consistent interaction (Byron and Stent, 1998; Mesgar et al., 2020). Previous works pursue different models such as graph-based (Vakulenko et al., 2018; Zhang et al., 2021) or text-based (Mesgar et al., 2020) approaches to develop automatic trainable coherence evaluation metrics. Those evaluation methods take a contrastive learning approach, where the goal is to build a binary classifier that differentiates positive, or coherent examples from negative, or incoherent dialogues. Those classifiers are usually trained on\ndatasets constructed by using human-human conversations as positive examples, and applying textlevel heuristic manipulations to the conversations to generate incoherent interactions. The text-level manipulations directly change the structures of the conversation such as shuffling the order of utterances, replacing some random utterances from external conversations (Vakulenko et al., 2018; Mesgar et al., 2020; Zhang et al., 2021). The third dialogue of Figure 1 illustrates such perturbations.\nWe hypothesize that such text-level manipulations are too simplistic to adequately represent more nuanced coherence errors presented in the current state-of-the-art dialogue systems. Second conversation in Figure 1 shows a human-system interaction from FED dataset (Mehri and Eskénazi, 2020), where the incoherence is much more subtle than the ones created by text-level manipulations.\nIn this paper, we investigate manipulation techniques to generate negative samples that represent coherence errors more likely to happen in the stateof-the-art dialogue systems. To this end, we propose DEAM, a model that uses Abstract Meaning Representation (AMR) to apply semantic-level manipulations. AMR-s are intended to capture the meaning of a sentence by abstracting away irrelevant syntactic features. Thus, injecting targeted and controlled perturbations into an AMR will potentially introduce semantic incoherence into the corresponding sentence. DEAM starts with parsing conversations into semantic AMR representations of dialogues, and then injects incoherence types that are usually observed in current state-of-the-art models into the AMR graphs. It concludes this process by translating the manipulated AMRs back to conversations to generate negative examples. A fine-tuned RoBERTa model is then trained on the created dataset to distinguish coherent and incoherent conversations.\nOur main contributions are as follows:\n• We propose DEAM, an evaluation metric that leverages AMR graphs to inject incoherence sources at the semantic level to generate incoherent conversations for training.\n• We propose four manipulation strategies to represent four common incoherence sources of the state-of-the-art dialogue models: contradiction, co-reference inconsistency, irrelevance and decrease engagements.\n• We empirically show that the model trained on our proposed manipulations significantly\noutperform strong baselines in terms of correlation with human judgments. Moreover, DEAM is capable of distinguishing positive and negative examples generated by baselines that use text-level manipulations, whereas the opposite is not true – classifiers trained on textlevel manipulations cannot detect negative examples generated by DEAM. This demonstrate the effectiveness of the semantic-level AMR-based manipulations."
    }, {
      "heading" : "2 Related Works",
      "text" : "Automatic evaluation of open-domain dialogue systems has a multifaceted nature with many finegrained quality aspects (Mehri and Eskénazi, 2020). Turn-level aspects show the quality of the system’s utterance given a dialogue context from different perspectives including appropriateness, relevance, engaging, and etc (Lowe et al., 2017; Tao et al., 2018; Ghazarian et al., 2020). Whereas, conversation-level facets such as coherence, diversity, informative take into account the whole conversation and dialog flow (Vakulenko et al., 2018; Zhang et al., 2021; Mehri and Eskénazi, 2020).\nDialogue coherence evaluation is pertinent to the discourse coherence as a dialogue is counted as a multi-party discourse. Similar to discourse coherence, many original coherence evaluation metrics derived from Centering Model for monitoring the local focus of utterances and the entities distribution over utterances (Grosz and Sidner, 1986; Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005). A group of studies assess the coherence of dialogues with respect to entities and dialogue acts (Cervone and Riccardi, 2020; Mesgar et al., 2020). Another inspected approach for dialogue coherence evaluation is to represent dialogue in a structured graph format where contextually dependent neighbor utterances or concepts are connected nodes in the graph (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020). Graph convolutional networks are used to complete this task.\nHigh quality training dataset is identified as one of the momentous and indelible components in automatic coherence evaluation. Some previous works construct such datasets by collecting human judgments (Higashinaka et al., 2014; Cervone and Riccardi, 2020). While many recent works rely on a more timely and costly affordable approach by automatically generating negative samples. The utterances of the coherent conversations from humanhuman interactions are manipulated by shuffling\ntheir order, inserting or replacing irrelevant utterances (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020; Zhang et al., 2021). In this work, we show that such changes can not truly represent machine generated incoherent conversations."
    }, {
      "heading" : "3 DEAM Overview",
      "text" : "Our goal is to build an evaluation metric that examines the whole conversation to measure how coherent and pertinent it is. We follow the trainable evaluation metrics (Vakulenko et al., 2018; Zhang et al., 2021) to formulate it as a classification task. We train the evaluator on positive (coherent) and negative (incoherent) conversations, and take the predicted probability of the positive class as the coherence score. As we elaborated above, the main challenge is to obtain negative samples that can adequately represent incoherent conversations generated by advanced dialogue systems. To this aim, we propose to manipulate conversations leveraging AMR-based perturbations. To build the classifier, we use RoBERTa (Liu et al., 2019) by fine-tuning it on the training dataset . Figure 2 illustrates an overview of our proposed evaluation method, DEAM.\nThe first step in DEAM is to apply Text-to-AMR models on the conversations. Text-to-AMR or AMR parsing (Jin and Gildea, 2019; Xu et al., 2020; Zhou et al., 2021; Lam et al., 2021) that translates conversation texts to directed and acyclic AMR graphs containing relation edges between concept nodes (Banarescu et al., 2013) has been effectively accomplished by transformer-based models in a sequence-to-sequence (seq2seq) training fashion (Xu et al., 2020; Zhou et al., 2021). We use the fine-tuned T5 (Raffel et al., 2020) model1 for\n1We use the released parse_t5 model from https:// github.com/bjascob/amrlib"
    }, {
      "heading" : "I read a few of his plays when I was in school. How about you?",
      "text" : "this purpose. After manipulating the AMR graphs (section 4.2), we back translate them into conversation texts to be used for training the text-based coherence evaluator. Similar to AMR parsing, we finetune T5 (Raffel et al., 2020)2 that is shown as a beneficial sequence-to-sequence transformerbased model used for AMR-to-Text generation task (Mager et al., 2020; Ribeiro et al., 2020a,b)."
    }, {
      "heading" : "4 Incoherent Dialogue Generation",
      "text" : "The challenge that automatic trainable evaluation metrics face is in providing training data that can appropriately replicate moderate to low quality conversations with incoherence sources that usually happens in the current dialogue models. The common solution is to apply manipulations to positive conversations. In this section, we summarize the baselines manipulations and state our proposed AMR-based perturbations."
    }, {
      "heading" : "4.1 Baselines Manipulations",
      "text" : "Baseline manipulations can be classified as:\n1) Shuffling-based manipulations: In such manipulations, turns order (Vakulenko et al., 2018), sequence of speakers utterances (Mesgar et al., 2020; Vakulenko et al., 2018; Zhang et al., 2021), or the position of the first and second sections of conversations (Vakulenko et al., 2018) are swapped.\n2) Insertion-based manipulations: This group of manipulations add incoherence sources by replacing (Mesgar et al., 2020; Zhang et al., 2021) or inserting (Mesgar et al., 2020) a random utterance from a randomly selected conversation. Each baseline metric fuses multiple manipulations, hence we use their citations (Vakulenko et al., 2018), (Mesgar et al., 2020) to easily refer them in later sections.\n2We use the released generate_t5wtense model from https://github.com/bjascob/amrlib."
    }, {
      "heading" : "4.2 AMR-based Manipulations",
      "text" : "AMR is originally proposed by Banarescu et al. (2013) as a semantic representation language that helps to abstract away the text from surface syntactics. Many abstract-level semantic information such as named entities, negations, questions, coreferences and modalities in the texts can be encoded by AMR graphs. These potential capabilities of AMR make it lucrative in many semantic related NLP tasks such as summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Conversations between two interlocutors contains many semantic details that can be captured by these graphs. Therefore, in this work, we explore AMR features’ usage in dialogue systems evaluation task by manipulating the AMR graphs of coherent conversations, each manipulation reflecting a specific reason of incoherence in dialogue systems. Figure 3 demonstrates a linearized version of an utterance AMR graph.\nIn AMR graphs, entities and concepts are shown as nodes and their relations are depicted with various relation edges (Banarescu et al., 2013). Each AMR concept is either a word, or a PropBank framesets keyword (Kingsbury and Palmer, 2002). The PropBank framesets with their predefined arguments are used to abstract away concepts from syntactic structures. As an example, located concept of PropBank framesets in Figure 3 comes with two arguments the subject (i) and the place (school).\nWith the observation of interactions between advanced dialogue systems and human, we consider four main logical flaws that could harm the connection of sequential utterances and hurt their linkages."
    }, {
      "heading" : "4.2.1 Contradiction",
      "text" : "One of the common issues that dialogue systems struggle with is to directly or indirectly contradict their previous utterances. To replicate this type of error, a contradicted version of a subgraph from the original AMR is copied to other locations. This negative form AMRs can be accomplished by directly adding polarity to the concepts or replacing concepts with their antonyms that hold Antonym, NotDesires, NotCapableOf, and NotHasProperty relations in ConceptNet (Speer and Havasi, 2012). After adding contradictions, the AMR-to-Text model will use the encoded context to output incoherent yet natural conversations. In Figure 4, speaker B contradicts its previously stated opinion that badly effects the linkage of the\nutterances."
    }, {
      "heading" : "4.2.2 Co-Reference Inconsistency",
      "text" : "The coherence of a conversation is preserved by the correct references of previously mentioned entities and words in the dialogue context. Pronouns in the conversation plays essential role in this regard. Coreferences in AMRs are presented as arguments (ARG) and all three different types of pronouns such as subjective, objective and possessive pronouns are shown in their subjective format.\nTo disrupt the co-references relations, we randomly replace some pronouns in the conversation’s AMR with either another pronoun or noun identified as ARG or operand (op) from the same conversation. After replacements, AMR-to-Text model adapts other sections of the utterance accordingly and reassures us that outputs have natural look. The third utterance in Figure 4 demonstrates an example of coherence inconsistency which makes the utterance to be not logical."
    }, {
      "heading" : "4.2.3 Irrelevancy",
      "text" : "Random utterances substitution with other conversations that has been frequently suggested by previous works (Tao et al., 2018; Ghazarian et al., 2019; Mesgar et al., 2020; Zhang et al., 2021) enlarges the incoherence sources in the conversations by embedding totally disjoint utterances. Conversations with completely off-topic utterances are rarely generated by advanced dialogue models due to their ability in encoding dialogue history for continuing the conversation.\nWe propose to apply irrelevancy sources into AMR graphs. We select some AMR items such as concepts, ops, ARGs and replace them with random items from other utterances. In this approach, the replacement items are not from randomly selected conversations but still they do not fit well in their newly selected locations which hurts the coherence of the conversation. In Figure 4 watch is replaced with listen. The benefits of using AMR-to-Text model emerges here where some new adaptations (such as to) have been augmented with new verb replacement to give the utterance a fluent look."
    }, {
      "heading" : "4.2.4 Decreasing Engagement",
      "text" : "In coherent conversations, speakers exchange opinions about different topics by stating detailed information, asking and answering questions. This coherence will be faded if one of the interlocutors evades to answer questions or talk in details. In\ncontrast to previous works that ignored this important feature, we augment such kind of incoherence sources into the negative sampling generation. In order to decrease engagement of coherent conversations, we take the advantage of AMRs architectures which can apparently demonstrate detailed utterances and those containing questions. In AMR graphs, detailed utterances include more number of nested layers and concepts, ARGs and ops. Question-type utterances can be easily distinguished via amr-unknown concept notation.\nWe propose three different approaches to decrease the engagement and consequently the coherence of the conversations:\n1) remove question-type utterances in the conversation: we select a multi-sentence utterance including amr-unknown concept and remove it and all its children nodes from the graph. 2) remove the most detailed utterance in the conversation: the utterance having the largest depth in the graph is selected as the utterance with the most transferred information and all its children alongside its parent concept are removed from the graph. 3) remove fine-grained information in the utterances: the main concepts’ detailed information that are presented as ARG or op in the AMRs are randomly selected and eliminated from the graph. The higher level concepts in the graph are preserved while its lower-level child nodes are deleted which makes the utterance to not transfer its complete meaning and diminishes the linkage of topics. The question part in third utterance of Figure 4 has been removed causing the coming utterances to not completely sensible."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "We examine DEAM and its negative sampling techniques versus baselines and manipulations. We aim to have a data-driven analysis under three setups:\nSetup 1): In this setup, we compare DEAM versus baseline models trained on differently manipulated conversations all originated from the same human-human conversational dataset.\nSetup 2): Since baseline models have been primarily trained on different open-domain dialogue datasets, we plan to have a pairwise comparison between DEAM and each baseline evaluator both trained on the baseline’s primitive dataset. To have a fair and legitimate examination, we train all models on a balanced set of coherent and incoherent conversations following their manipulation.\nSetup 3): This setup is designed to show the effectiveness of training datasets quality resulted from different manipulations regardless of the model’s role.\nLast but not least, we investigate the efficacy of each proposed manipulation approach in DEAM on the target evaluator’s performance."
    }, {
      "heading" : "5.1 Datasets",
      "text" : ""
    }, {
      "heading" : "5.1.1 Training Datasets",
      "text" : "We conduct our experiments on two crowd-sourced TopicalChat (Gopalakrishnan et al., 2019) and PersonaChat (Zhang et al., 2018) datasets. Both datasets are composed of conversations happened between Amazon Mechanical Turk (AMT) participants. In TopicalChat, AMT workers are supposed to have coherent and engaging conversations regarding the provided reading sets about different topics, while in PersonaChat dataset coherent con-\nversations are conditioned on the provided 1155 personas each including 5 personality description sentences collected via AMT. We deal with these conversations as coherent conversations. We follow DEAM steps to generate and add balanced number of incoherent conversations. Table 1 shows the statistics of the newly constructed datasets called PERSONA_DEAM and TOPICAL_DEAM."
    }, {
      "heading" : "5.1.2 Evaluation Datasets",
      "text" : "In the literature of automatic evaluation metrics, the prevalent way of assessing the evaluators performance is to compare their predicted scores correlations with human judgments. FED (Mehri and Eskénazi, 2020) and Interactive Evaluation of Dialog track of the Dialog State Tracking Challenge 9 (DSTC9) (Gunasekara et al., 2020) are two publically available benchmark datasets including human ratings on the coherence aspect of humanhuman or human-systems conversations.\nThe participants of FED dataset, have judged 125 conversations; 41 human-human, 44 humanMitsuku chatbot and 40 human-Meena (Adiwardana et al., 2020). Humans have assessed the conversations from 11 conversation-level evaluation aspects including the coherence and overall scores. Each conversation in FED is judged by 5 distinct annotators. Coherence and overall scores are in the range of 0-2 and 0-4, respectively.\nIn DSTC9 dataset3, AMT workers have rated 2200 conversations between invited participants and knowledge-grounded response generation models using the same 11 fine-grained conversationlevel evaluation aspects. Coherence and overall scores that we use in our experiments are in the range of 1-3 and 1-5, respectively. In our experiments, we take the average of judgments for conversations with more than one annotator’s ratings and compute the Spearman correlations between human evaluations and evaluator’s generated scores.\n3https://github.com/exe1023/ DialEvalMetrics"
    }, {
      "heading" : "5.2 Models and Manipulations",
      "text" : "In our work, we train and run all the models on a machine with a GeForce RTX 2080 Ti GPU. We fine-tune RoBERTa-large pretrained model on TOPICAL_DEAM and PERSONA_DEAM datasets for three epochs and optimize parameters using Adam optimizer with 1e-5 learning rate.\nTo conduct experiments in setup 1, we train Vakulenko et al. (2018)’s graph-based model for 128 epochs with 1e-5 learning rate. Mesgar et al. (2020)’s LSTM-based model is trained for 8 epochs with 5e-5 learning rate. We retrain DynaEval (Zhang et al., 2021) for 20 epochs. All baselines are trained using Adam optimizer. Due to not publically published models proposed by Vakulenko et al. (2018) and (Mesgar et al., 2020), we need to retrain these models on their original datasets; Ubuntu (Lowe et al., 2015) and DailyDialog (Li et al., 2017); using the same hyperparameters published in the aforementioned papers to complete experiments in setup 2. We use DynaEval’s published checkpoints to run experiments in this setup.\nIn experimental setup 3, we start from TOPICALCHAT and PERSONACHAT datasets, and augment negative samples pursuing different manipulation techniques. We fix the evaluator and finetune RoBERTa-large model for 3 epochs with 1e-5 learning rate. Since Vakulenko et al. (2018)’s proposed manipulations are in the entity level, we adapt the perturbations to the text level by replacing the sequence of entities substitutions with sequence of utterances substitutions to be acceptable by the RoBERTa model4."
    }, {
      "heading" : "6 Results",
      "text" : "Through our experiments, we report the Spearman correlation of evaluation metrics with human annotations under different experimental setups."
    }, {
      "heading" : "6.1 Metrics Performance",
      "text" : "Table 2 depicts the quantitative results for different evaluation models on both FED and DSTC9 datasets of experimental setup 1. According to the reported correlations, the superiority of DEAM shown in the last row versus other baseline evaluators is obviously deducable. This superiority could originate from subtle negative sampling technique.\nIn experimental setup 1, original dataset, manipulation techniques and evaluator models vary\n4We will release our adapted version of these entity-level manipulations\nbetween models, therefore we complete our investigation via experimental setup 2 by conducting one by one comparison of DEAM with each baseline model training each pair on the same dataset that the baseline model has been trained on. Table 4 shows the output of this type of pairwise comparisons separated each pair in one section. Even though most of the baseline models correlations increased, yet DEAM takes the lead. It is noteworthy that the correlation of DYNAEVAL reported in the original paper for FED dataset has decreased, this could be due to the less number of negative samples that we consider for each positive conversation and its major impact on this model’s performance."
    }, {
      "heading" : "6.2 Manipulations Effect",
      "text" : "Table 3 illustrates the results of experimental setup 3, where we fix both RoBERTa evaluator and TOPICALCHAT and PERSONACHAT original datasets and apply different manipulation techniques to add negative samples. Even though the correlation for baseline manipulations increased drastically, which shows the effectiveness of strong pretrained language models in better encoding conversations information used for the evaluation task, DEAM’s performance is significantly higher. This interprets the beneficial effect of AMR-based manipulations for our task. The positive slops of the regression\nline in Figures 5 and 6 between DEAM predicted coherence scores and human coherence and overall evaluations for FED dataset show the proposed manipulations superiority from a different angle. The distribution of baseline models predicted low scores for high-quality conversations and vice versa present their ineffectiveness in correctly distinguishing between low and high quality conversations."
    }, {
      "heading" : "6.3 Ablation Studies",
      "text" : "We continue our observations by inspecting the role of each of the four proposed manipulations in the metic’s performance. We conduct an ablation study on TOPICALCHAT and PERSONACHAT datasets to assess the effectiveness of each manipulation. For each specific manipulation, we remove it from the list of possible manipulations and try to randomly sample one up to three different manipulations to create negative samples. In Table 5, we witness an overall drop by eliminating each of the manipulations that indicates the positive impact of all of the manipulations on generating higher quality\nnegative samples that are closer to the samples generated by state-of-the-art models and consequently the evaluator’s accuracy. Neglecting irrelevancy and decreasing engagement manipulations hurts the most our evaluator’s performance which concludes that many state-of-the-art models struggle with such issues and eliminating these manipulations indeed removes the opportunity for model to face this kind of issues during training."
    }, {
      "heading" : "6.4 Qualitative Analysis",
      "text" : "We analyze the quality of DEAM versus baseline evaluators in terms of examining each model’s performance to distinguishing between positive and negative examples constructed leveraging various manipulations. Some examples are shown in Table 6 of Appendix. Figure 7 illustrates a heat map of the accuracy scores. X-axis and Y-axis show the manipulations used for creating training and testing dataset, respectively. As it is expected the highest accuracies can be found on the diagonal where models have been trained and tested on datasets generated from pursuing the same manipulation techniques. The light colored cells are mainly related to models trained on baseline manipulatied data and tested on AMR-based perturbed data. This indicates that the baseline models trained on such type of text-level heuristic manipulations can not perform well and indeed have random guess on more challenging incoherent examples that are generated by DEAM. While the higher accuracies of DEAM model on baseline test datasets show our model’s capability to more effectively distinguish between positive and heuristically created their counterpart manipulations’ negative conversations."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Reliable automatic trainable coherence evaluation metrics that can efficiently measure the dynamics of interactions between interlocutors are principally influenced by the quality of the training instances. We show that leveraging text-level manipulations can not adequately mirror the incoherence errors that current dialogue systems face with. According to our study, DEAM can more effectively accomplish this task by relying on capabilities that AMR-based semantic perturbations and pretrained language models present."
    }, {
      "heading" : "8 Ethics",
      "text" : "Our main goal is to propose a more reliable automatic evaluation metric for measuring the coherence of the conversations with the usage of AMRbased manipulations. We acknowledge the importance of ACM Code of Ethics and totally agree with it. We ensure that our study is compatible with the provided code, specifically in the terms of providing non-offensive dataset construction.\nIn our proposed approach, we start from publically available human-human conversational datasets and attempt to apply our proposed manipulations in the AMR level. The main concern that arises here is the probability of generating offensive conversations from manipulated AMRs. The chance of such generations is faded with leveraging TopicalChat (Gopalakrishnan et al., 2019) and PersonaChat (Zhang et al., 2018) datasets that have been originally collected asking users to converse without profanity and inappropriate uttterances. Accordingly, we expect that the generated manipulated versions of conversations will be less likely to have such abusive languages. We should note that the possibility of modifications and perturbations that would have the possibility of creating objectionable outputs is not zero therefore we acknowledge there may be biases or abusive content via attacks that can be resolved by security trended studies which is out of this work’s scope. In this study, we do not require to conduct experiments to collect human annotations as we use the existing benchmarks that include human judgments on different quality aspects."
    }, {
      "heading" : "A Appendix",
      "text" : "In Table 6, we present examples of incoherent conversations generated by DEAM’s AMR-based and other baselines’ text-based approaches. Due to the long length of the conversations, we include subsections from the conversations that their utterances are separated by </UTT> separator.\nThe original and manipulated AMR graphs of the conversation in Figure 4 is shown in Table 7. We linearized the AMR graphs to be placed in the table."
    } ],
    "references" : [ {
      "title" : "Towards a human-like opendomain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu", "Quoc V. Le." ],
      "venue" : "CoRR, abs/2001.09977.",
      "citeRegEx" : "Adiwardana et al\\.,? 2020",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguistic Annotation",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "A preliminary model of centering in dialog",
      "author" : [ "Donna K. Byron", "Amanda Stent." ],
      "venue" : "36th Annual",
      "citeRegEx" : "Byron and Stent.,? 1998",
      "shortCiteRegEx" : "Byron and Stent.",
      "year" : 1998
    }, {
      "title" : "Is this dialogue coherent? learning from dialogue acts and entities",
      "author" : [ "Alessandra Cervone", "Giuseppe Riccardi." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGdial 2020, 1st virtual meeting, July 1-3,",
      "citeRegEx" : "Cervone and Riccardi.,? 2020",
      "shortCiteRegEx" : "Cervone and Riccardi.",
      "year" : 2020
    }, {
      "title" : "Discol: Toward engaging dialogue systems through conversational line guided response generation",
      "author" : [ "Sarik Ghazarian", "Zixi Liu", "Tuhin Chakrabarty", "Xuezhe Ma", "Aram Galstyan", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Ghazarian et al\\.,? 2021",
      "shortCiteRegEx" : "Ghazarian et al\\.",
      "year" : 2021
    }, {
      "title" : "Better automatic evaluation of open-domain dialogue systems with contextualized embeddings",
      "author" : [ "Sarik Ghazarian", "Johnny Tian-Zheng Wei", "Aram Galstyan", "Nanyun Peng." ],
      "venue" : "CoRR, abs/1904.10635.",
      "citeRegEx" : "Ghazarian et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazarian et al\\.",
      "year" : 2019
    }, {
      "title" : "Predictive engagement: An efficient metric for automatic evaluation of opendomain dialogue systems",
      "author" : [ "Sarik Ghazarian", "Ralph M. Weischedel", "Aram Galstyan", "Nanyun Peng." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The",
      "citeRegEx" : "Ghazarian et al\\.,? 2020",
      "shortCiteRegEx" : "Ghazarian et al\\.",
      "year" : 2020
    }, {
      "title" : "Topical-chat: Towards knowledge-grounded open-domain conversations",
      "author" : [ "Karthik Gopalakrishnan", "Behnam Hedayatnia", "Qinglang Chen", "Anna Gottardi", "Sanjeev Kwatra", "Anu Venkatesh", "Raefer Gabriel", "Dilek Hakkani-Tür." ],
      "venue" : "Interspeech 2019,",
      "citeRegEx" : "Gopalakrishnan et al\\.,? 2019",
      "shortCiteRegEx" : "Gopalakrishnan et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention, intentions, and the structure of discourse",
      "author" : [ "Barbara J. Grosz", "Candace L. Sidner." ],
      "venue" : "Comput. Linguistics, 12(3):175–204.",
      "citeRegEx" : "Grosz and Sidner.,? 1986",
      "shortCiteRegEx" : "Grosz and Sidner.",
      "year" : 1986
    }, {
      "title" : "Overview of the ninth dialog system technology challenge: Dstc9",
      "author" : [ "Chulaka Gunasekara", "Seokhwan Kim", "Luis Fernando D’Haro", "Abhinav Rastogi", "Yun-Nung Chen", "Mihail Eric", "Behnam Hedayatnia", "Karthik Gopalakrishnan", "Yang Liu", "Chao-Wei Huang" ],
      "venue" : null,
      "citeRegEx" : "Gunasekara et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gunasekara et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating coherence",
      "author" : [ "Ryuichiro Higashinaka", "Toyomi Meguro", "Kenji Imamura", "Hiroaki Sugiyama", "Toshiro Makino", "Yoshihiro Matsuo" ],
      "venue" : null,
      "citeRegEx" : "Higashinaka et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Higashinaka et al\\.",
      "year" : 2014
    }, {
      "title" : "GRADE: automatic graphenhanced coherence metric for evaluating opendomain dialogue systems",
      "author" : [ "Lishan Huang", "Zheng Ye", "Jinghui Qin", "Liang Lin", "Xiaodan Liang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Amr-to-text generation with cache transition systems",
      "author" : [ "Lisa Jin", "Daniel Gildea." ],
      "venue" : "CoRR, abs/1912.01682.",
      "citeRegEx" : "Jin and Gildea.,? 2019",
      "shortCiteRegEx" : "Jin and Gildea.",
      "year" : 2019
    }, {
      "title" : "From treebank to propbank",
      "author" : [ "Paul R. Kingsbury", "Martha Palmer." ],
      "venue" : "Proceedings of the Third International Conference on Language Resources and Evaluation, LREC 2002, May 29-31, 2002, Las Palmas, Canary Islands, Spain. European Language",
      "citeRegEx" : "Kingsbury and Palmer.,? 2002",
      "shortCiteRegEx" : "Kingsbury and Palmer.",
      "year" : 2002
    }, {
      "title" : "Ensembling graph predictions for AMR parsing",
      "author" : [ "Hoang Thanh Lam", "Gabriele Picco", "Yufang Hou", "YoungSuk Lee", "Lam M. Nguyen", "Dzung T. Phan", "Vanessa López", "Ramón Fernandez Astudillo." ],
      "venue" : "CoRR, abs/2110.09131.",
      "citeRegEx" : "Lam et al\\.,? 2021",
      "shortCiteRegEx" : "Lam et al\\.",
      "year" : 2021
    }, {
      "title" : "Automatic evaluation of text coherence: Models and representations",
      "author" : [ "Mirella Lapata", "Regina Barzilay." ],
      "venue" : "IJCAI-05, Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence, Edinburgh, Scotland, UK, July 30 - August 5,",
      "citeRegEx" : "Lapata and Barzilay.,? 2005",
      "shortCiteRegEx" : "Lapata and Barzilay.",
      "year" : 2005
    }, {
      "title" : "BART: denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Tai-",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Abstract meaning representation for multi-document summarization",
      "author" : [ "Kexin Liao", "Logan Lebanoff", "Fei Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August",
      "citeRegEx" : "Liao et al\\.,? 2018",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards an automatic turing test: Learning to evaluate dialogue responses",
      "author" : [ "Ryan Lowe", "Michael Noseworthy", "Iulian Vlad Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association",
      "citeRegEx" : "Lowe et al\\.,? 2017",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2017
    }, {
      "title" : "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "Proceedings of the SIGDIAL 2015 Conference, The 16th Annual Meeting of the Spe-",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Gpt-too: A language-model-first approach for amr-to-text generation",
      "author" : [ "Manuel Mager", "Ramón Fernandez Astudillo", "Tahira Naseem", "Md. Arafat Sultan", "Young-Suk Lee", "Radu Florian", "Salim Roukos." ],
      "venue" : "Proceedings of the 58th Annual Meeting of",
      "citeRegEx" : "Mager et al\\.,? 2020",
      "shortCiteRegEx" : "Mager et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised evaluation of interactive dialog with dialogpt",
      "author" : [ "Shikib Mehri", "Maxine Eskénazi." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGdial 2020, 1st virtual meeting, July 1-3, 2020,",
      "citeRegEx" : "Mehri and Eskénazi.,? 2020",
      "shortCiteRegEx" : "Mehri and Eskénazi.",
      "year" : 2020
    }, {
      "title" : "Dialogue coherence assessment without explicit dialogue act labels",
      "author" : [ "Mohsen Mesgar", "Sebastian Bücker", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
      "citeRegEx" : "Mesgar et al\\.,? 2020",
      "shortCiteRegEx" : "Mesgar et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluation of text coherence for electronic essay scoring systems",
      "author" : [ "Eleni Miltsakaki", "Karen Kukich." ],
      "venue" : "Nat. Lang. Eng., 10(1):25–55.",
      "citeRegEx" : "Miltsakaki and Kukich.,? 2004",
      "shortCiteRegEx" : "Miltsakaki and Kukich.",
      "year" : 2004
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog,",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "J. Mach. Learn. Res., 21:140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Investigating pretrained language models for graph-to-text generation",
      "author" : [ "Leonardo F.R. Ribeiro", "Martin Schmitt", "Hinrich Schütze", "Iryna Gurevych." ],
      "venue" : "CoRR, abs/2007.08426.",
      "citeRegEx" : "Ribeiro et al\\.,? 2020a",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling global and local node contexts for text generation from knowledge graphs",
      "author" : [ "Leonardo F.R. Ribeiro", "Yue Zhang", "Claire Gardent", "Iryna Gurevych." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:589– 604.",
      "citeRegEx" : "Ribeiro et al\\.,? 2020b",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantic neural machine translation using AMR",
      "author" : [ "Linfeng Song", "Daniel Gildea", "Yue Zhang", "Zhiguo Wang", "Jinsong Su." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 7:19–31.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Representing general relational knowledge in conceptnet 5",
      "author" : [ "Robyn Speer", "Catherine Havasi." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012, pages 3679–3686.",
      "citeRegEx" : "Speer and Havasi.,? 2012",
      "shortCiteRegEx" : "Speer and Havasi.",
      "year" : 2012
    }, {
      "title" : "RUBER: an unsupervised method for automatic evaluation of open-domain dialog systems",
      "author" : [ "Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th inno-",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "Measuring semantic coherence of a conversation",
      "author" : [ "Svitlana Vakulenko", "Maarten de Rijke", "Michael Cochez", "Vadim Savenkov", "Axel Polleres." ],
      "venue" : "International semantic web conference, pages 634–651. Springer.",
      "citeRegEx" : "Vakulenko et al\\.,? 2018",
      "shortCiteRegEx" : "Vakulenko et al\\.",
      "year" : 2018
    }, {
      "title" : "Diverse and informative dialogue generation with context-specific commonsense knowledge awareness",
      "author" : [ "Sixing Wu", "Ying Li", "Dawei Zhang", "Yang Zhou", "Zhonghai Wu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving AMR parsing with sequence-to-sequence pre-training",
      "author" : [ "Dongqin Xu", "Junhui Li", "Muhua Zhu", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "A comprehensive assessment of dialog evaluation metrics",
      "author" : [ "Yi-Ting Yeh", "Maxine Eskénazi", "Shikib Mehri." ],
      "venue" : "CoRR, abs/2106.03706.",
      "citeRegEx" : "Yeh et al\\.,? 2021",
      "shortCiteRegEx" : "Yeh et al\\.",
      "year" : 2021
    }, {
      "title" : "Dynaeval: Unifying turn and dialogue level evaluation",
      "author" : [ "Chen Zhang", "Yiming Chen", "Luis Fernando D’Haro", "Yan Zhang", "Thomas Friedrichs", "Grandee Lee", "Haizhou Li" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "DIALOGPT : Large-scale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th Annual Meet-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "AMR parsing with action-pointer transformer",
      "author" : [ "Jiawei Zhou", "Tahira Naseem", "Ramón Fernandez Astudillo", "Radu Florian." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Despite the effectiveness of large pretrained language models (Radford et al., 2019; Lewis et al., 2020) for the dialogue response generation (Zhang et al.",
      "startOffset" : 62,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "Despite the effectiveness of large pretrained language models (Radford et al., 2019; Lewis et al., 2020) for the dialogue response generation (Zhang et al.",
      "startOffset" : 62,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : ", 2020) for the dialogue response generation (Zhang et al., 2020; Adiwardana et al., 2020; Ghazarian et al., 2021), such models still face challenges to Not everyone is into this type.",
      "startOffset" : 45,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : ", 2020) for the dialogue response generation (Zhang et al., 2020; Adiwardana et al., 2020; Ghazarian et al., 2021), such models still face challenges to Not everyone is into this type.",
      "startOffset" : 45,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : ", 2020) for the dialogue response generation (Zhang et al., 2020; Adiwardana et al., 2020; Ghazarian et al., 2021), such models still face challenges to Not everyone is into this type.",
      "startOffset" : 45,
      "endOffset" : 114
    }, {
      "referenceID" : 32,
      "context" : "Most trainable automatic evaluation metrics focus on turn-level interactions, where they learn to assess the quality of one user-system utterance pair (Tao et al., 2018; Huang et al., 2020; Ghazarian et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 213
    }, {
      "referenceID" : 11,
      "context" : "Most trainable automatic evaluation metrics focus on turn-level interactions, where they learn to assess the quality of one user-system utterance pair (Tao et al., 2018; Huang et al., 2020; Ghazarian et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "Most trainable automatic evaluation metrics focus on turn-level interactions, where they learn to assess the quality of one user-system utterance pair (Tao et al., 2018; Huang et al., 2020; Ghazarian et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 213
    }, {
      "referenceID" : 36,
      "context" : "However, these metrics cannot appropriately model the whole conversation flow (Yeh et al., 2021), and thus are insufficient for dialogue-level evaluation.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "Coherence is a conversation-level evaluation metric that measures how well all the utterances in a conversation are unified leading to a consistent interaction (Byron and Stent, 1998; Mesgar et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 204
    }, {
      "referenceID" : 24,
      "context" : "Coherence is a conversation-level evaluation metric that measures how well all the utterances in a conversation are unified leading to a consistent interaction (Byron and Stent, 1998; Mesgar et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 204
    }, {
      "referenceID" : 33,
      "context" : "Previous works pursue different models such as graph-based (Vakulenko et al., 2018; Zhang et al., 2021) or text-based (Mesgar et al.",
      "startOffset" : 59,
      "endOffset" : 103
    }, {
      "referenceID" : 37,
      "context" : "Previous works pursue different models such as graph-based (Vakulenko et al., 2018; Zhang et al., 2021) or text-based (Mesgar et al.",
      "startOffset" : 59,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : ", 2021) or text-based (Mesgar et al., 2020) approaches to develop automatic trainable coherence evaluation metrics.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "Second conversation in Figure 1 shows a human-system interaction from FED dataset (Mehri and Eskénazi, 2020), where the incoherence is much more subtle",
      "startOffset" : 82,
      "endOffset" : 108
    }, {
      "referenceID" : 33,
      "context" : "Whereas, conversation-level facets such as coherence, diversity, informative take into account the whole conversation and dialog flow (Vakulenko et al., 2018; Zhang et al., 2021; Mehri and Eskénazi, 2020).",
      "startOffset" : 134,
      "endOffset" : 204
    }, {
      "referenceID" : 37,
      "context" : "Whereas, conversation-level facets such as coherence, diversity, informative take into account the whole conversation and dialog flow (Vakulenko et al., 2018; Zhang et al., 2021; Mehri and Eskénazi, 2020).",
      "startOffset" : 134,
      "endOffset" : 204
    }, {
      "referenceID" : 23,
      "context" : "Whereas, conversation-level facets such as coherence, diversity, informative take into account the whole conversation and dialog flow (Vakulenko et al., 2018; Zhang et al., 2021; Mehri and Eskénazi, 2020).",
      "startOffset" : 134,
      "endOffset" : 204
    }, {
      "referenceID" : 8,
      "context" : "the local focus of utterances and the entities distribution over utterances (Grosz and Sidner, 1986; Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005).",
      "startOffset" : 76,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : "the local focus of utterances and the entities distribution over utterances (Grosz and Sidner, 1986; Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005).",
      "startOffset" : 76,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "the local focus of utterances and the entities distribution over utterances (Grosz and Sidner, 1986; Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005).",
      "startOffset" : 76,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "A group of studies assess the coherence of dialogues with respect to entities and dialogue acts (Cervone and Riccardi, 2020; Mesgar et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : "A group of studies assess the coherence of dialogues with respect to entities and dialogue acts (Cervone and Riccardi, 2020; Mesgar et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 145
    }, {
      "referenceID" : 33,
      "context" : "Another inspected approach for dialogue coherence evaluation is to represent dialogue in a structured graph format where contextually dependent neighbor utterances or concepts are connected nodes in the graph (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020).",
      "startOffset" : 209,
      "endOffset" : 274
    }, {
      "referenceID" : 24,
      "context" : "Another inspected approach for dialogue coherence evaluation is to represent dialogue in a structured graph format where contextually dependent neighbor utterances or concepts are connected nodes in the graph (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020).",
      "startOffset" : 209,
      "endOffset" : 274
    }, {
      "referenceID" : 11,
      "context" : "Another inspected approach for dialogue coherence evaluation is to represent dialogue in a structured graph format where contextually dependent neighbor utterances or concepts are connected nodes in the graph (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020).",
      "startOffset" : 209,
      "endOffset" : 274
    }, {
      "referenceID" : 10,
      "context" : "Some previous works construct such datasets by collecting human judgments (Higashinaka et al., 2014; Cervone and Riccardi, 2020).",
      "startOffset" : 74,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "Some previous works construct such datasets by collecting human judgments (Higashinaka et al., 2014; Cervone and Riccardi, 2020).",
      "startOffset" : 74,
      "endOffset" : 128
    }, {
      "referenceID" : 33,
      "context" : "their order, inserting or replacing irrelevant utterances (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 58,
      "endOffset" : 143
    }, {
      "referenceID" : 24,
      "context" : "their order, inserting or replacing irrelevant utterances (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 58,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "their order, inserting or replacing irrelevant utterances (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 58,
      "endOffset" : 143
    }, {
      "referenceID" : 37,
      "context" : "their order, inserting or replacing irrelevant utterances (Vakulenko et al., 2018; Mesgar et al., 2020; Huang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 58,
      "endOffset" : 143
    }, {
      "referenceID" : 33,
      "context" : "We follow the trainable evaluation metrics (Vakulenko et al., 2018; Zhang et al., 2021) to formulate it as a classifica-",
      "startOffset" : 43,
      "endOffset" : 87
    }, {
      "referenceID" : 37,
      "context" : "We follow the trainable evaluation metrics (Vakulenko et al., 2018; Zhang et al., 2021) to formulate it as a classifica-",
      "startOffset" : 43,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "To build the classifier, we use RoBERTa (Liu et al., 2019) by",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "Text-to-AMR or AMR parsing (Jin and Gildea, 2019; Xu et al., 2020; Zhou et al., 2021; Lam et al., 2021) that translates conversation texts to directed and acyclic AMR graphs containing relation edges between concept nodes (Banarescu et al.",
      "startOffset" : 27,
      "endOffset" : 103
    }, {
      "referenceID" : 35,
      "context" : "Text-to-AMR or AMR parsing (Jin and Gildea, 2019; Xu et al., 2020; Zhou et al., 2021; Lam et al., 2021) that translates conversation texts to directed and acyclic AMR graphs containing relation edges between concept nodes (Banarescu et al.",
      "startOffset" : 27,
      "endOffset" : 103
    }, {
      "referenceID" : 40,
      "context" : "Text-to-AMR or AMR parsing (Jin and Gildea, 2019; Xu et al., 2020; Zhou et al., 2021; Lam et al., 2021) that translates conversation texts to directed and acyclic AMR graphs containing relation edges between concept nodes (Banarescu et al.",
      "startOffset" : 27,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "Text-to-AMR or AMR parsing (Jin and Gildea, 2019; Xu et al., 2020; Zhou et al., 2021; Lam et al., 2021) that translates conversation texts to directed and acyclic AMR graphs containing relation edges between concept nodes (Banarescu et al.",
      "startOffset" : 27,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : ", 2021) that translates conversation texts to directed and acyclic AMR graphs containing relation edges between concept nodes (Banarescu et al., 2013) has been effectively accomplished by transformer-based models in a sequence-to-sequence (seq2seq) training fashion (Xu et al.",
      "startOffset" : 126,
      "endOffset" : 150
    }, {
      "referenceID" : 35,
      "context" : ", 2013) has been effectively accomplished by transformer-based models in a sequence-to-sequence (seq2seq) training fashion (Xu et al., 2020; Zhou et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 159
    }, {
      "referenceID" : 40,
      "context" : ", 2013) has been effectively accomplished by transformer-based models in a sequence-to-sequence (seq2seq) training fashion (Xu et al., 2020; Zhou et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 159
    }, {
      "referenceID" : 27,
      "context" : "We use the fine-tuned T5 (Raffel et al., 2020) model1 for",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "Similar to AMR parsing, we finetune T5 (Raffel et al., 2020)2 that is shown as a beneficial sequence-to-sequence transformerbased model used for AMR-to-Text generation task (Mager et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 33,
      "context" : "1) Shuffling-based manipulations: In such manipulations, turns order (Vakulenko et al., 2018), sequence of speakers utterances (Mesgar et al.",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : ", 2018), sequence of speakers utterances (Mesgar et al., 2020; Vakulenko et al., 2018; Zhang et al., 2021), or the position of the first and second sections of conversations (Vakulenko et al.",
      "startOffset" : 41,
      "endOffset" : 106
    }, {
      "referenceID" : 33,
      "context" : ", 2018), sequence of speakers utterances (Mesgar et al., 2020; Vakulenko et al., 2018; Zhang et al., 2021), or the position of the first and second sections of conversations (Vakulenko et al.",
      "startOffset" : 41,
      "endOffset" : 106
    }, {
      "referenceID" : 37,
      "context" : ", 2018), sequence of speakers utterances (Mesgar et al., 2020; Vakulenko et al., 2018; Zhang et al., 2021), or the position of the first and second sections of conversations (Vakulenko et al.",
      "startOffset" : 41,
      "endOffset" : 106
    }, {
      "referenceID" : 33,
      "context" : ", 2021), or the position of the first and second sections of conversations (Vakulenko et al., 2018) are swapped.",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "2) Insertion-based manipulations: This group of manipulations add incoherence sources by replacing (Mesgar et al., 2020; Zhang et al., 2021) or inserting (Mesgar et al.",
      "startOffset" : 99,
      "endOffset" : 140
    }, {
      "referenceID" : 37,
      "context" : "2) Insertion-based manipulations: This group of manipulations add incoherence sources by replacing (Mesgar et al., 2020; Zhang et al., 2021) or inserting (Mesgar et al.",
      "startOffset" : 99,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : ", 2021) or inserting (Mesgar et al., 2020) a random utterance from a randomly selected conversation.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 33,
      "context" : "Each baseline metric fuses multiple manipulations, hence we use their citations (Vakulenko et al., 2018), (Mesgar et al.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : ", 2018), (Mesgar et al., 2020) to easily refer them in later sections.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "These potential capabilities of AMR make it lucrative in many semantic related NLP tasks such as summarization (Liao et al., 2018) and machine translation (Song et al.",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "In AMR graphs, entities and concepts are shown as nodes and their relations are depicted with various relation edges (Banarescu et al., 2013).",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "AMR concept is either a word, or a PropBank framesets keyword (Kingsbury and Palmer, 2002).",
      "startOffset" : 62,
      "endOffset" : 90
    }, {
      "referenceID" : 31,
      "context" : "This negative form AMRs can be accomplished by directly adding polarity to the concepts or replacing concepts with their antonyms that hold Antonym, NotDesires, NotCapableOf, and NotHasProperty relations in ConceptNet (Speer and Havasi, 2012).",
      "startOffset" : 218,
      "endOffset" : 242
    }, {
      "referenceID" : 32,
      "context" : "sations that has been frequently suggested by previous works (Tao et al., 2018; Ghazarian et al., 2019; Mesgar et al., 2020; Zhang et al., 2021) enlarges the incoherence sources in the conversations by embedding totally disjoint utterances.",
      "startOffset" : 61,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "sations that has been frequently suggested by previous works (Tao et al., 2018; Ghazarian et al., 2019; Mesgar et al., 2020; Zhang et al., 2021) enlarges the incoherence sources in the conversations by embedding totally disjoint utterances.",
      "startOffset" : 61,
      "endOffset" : 144
    }, {
      "referenceID" : 24,
      "context" : "sations that has been frequently suggested by previous works (Tao et al., 2018; Ghazarian et al., 2019; Mesgar et al., 2020; Zhang et al., 2021) enlarges the incoherence sources in the conversations by embedding totally disjoint utterances.",
      "startOffset" : 61,
      "endOffset" : 144
    }, {
      "referenceID" : 37,
      "context" : "sations that has been frequently suggested by previous works (Tao et al., 2018; Ghazarian et al., 2019; Mesgar et al., 2020; Zhang et al., 2021) enlarges the incoherence sources in the conversations by embedding totally disjoint utterances.",
      "startOffset" : 61,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "We conduct our experiments on two crowd-sourced TopicalChat (Gopalakrishnan et al., 2019) and PersonaChat (Zhang et al.",
      "startOffset" : 60,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Eskénazi, 2020) and Interactive Evaluation of Dialog track of the Dialog State Tracking Challenge 9 (DSTC9) (Gunasekara et al., 2020) are two publically available benchmark datasets including human ratings on the coherence aspect of human-",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "The participants of FED dataset, have judged 125 conversations; 41 human-human, 44 humanMitsuku chatbot and 40 human-Meena (Adiwardana et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 148
    }, {
      "referenceID" : 37,
      "context" : "We retrain DynaEval (Zhang et al., 2021) for 20 epochs.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "(2018) and (Mesgar et al., 2020), we need to retrain these models on their original datasets; Ubuntu (Lowe et al.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : ", 2020), we need to retrain these models on their original datasets; Ubuntu (Lowe et al., 2015) and DailyDialog (Li et al.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : ", 2015) and DailyDialog (Li et al., 2017); using the same hyperparameters published in the aforementioned papers to complete",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : "chance of such generations is faded with leveraging TopicalChat (Gopalakrishnan et al., 2019) and PersonaChat (Zhang et al.",
      "startOffset" : 64,
      "endOffset" : 93
    }, {
      "referenceID" : 38,
      "context" : ", 2019) and PersonaChat (Zhang et al., 2018) datasets that have been originally collected asking users to converse without profanity and inappropriate uttter-",
      "startOffset" : 24,
      "endOffset" : 44
    } ],
    "year" : 0,
    "abstractText" : "Automatic evaluation metrics are essential for rapid development of open-domain dialogue systems as they facilitate hyper-parameter tuning and comparison between models. Although recently proposed trainable conversation-level metrics have shown encouraging results, those metrics are strongly dependent on the quality of negative training examples (e.g., incoherent dialogues). Prior works mainly resort to heuristic text-level manipulations (e.g. utterances shuffling) to bootstrap incoherent conversations from coherent dialogues. Such approaches are insufficient to appropriately reflect the incoherence sources that occur in interactions between advanced dialogue models and humans. To tackle this problem, we propose DEAM, a Dialogue coherence Evaluation metric that relies on Abstract Meaning Representation (AMR) to apply semantic-level Manipulations for incoherent (negative) data generation. AMR-s naturally facilitate the injection of various types of incoherence sources, such as coreference inconsistency, irrelevancy, contradictions, decreasing engagements, at the semantic level, thus resulting in more natural incoherent samples. Our experiments show that DEAM achieves higher correlations with human judgments compared to baseline methods on several dialog datasets by significant margins. We also show that DEAM can distinguish between coherent and incoherent dialogues generated by baseline manipulations, whereas those baseline models cannot detect incoherent examples generated by DEAM. Our results demonstrate the potential of AMR-based semantic manipulations for developing better automatic coherence evaluation methods of open-domain dialogues.",
    "creator" : null
  }
}
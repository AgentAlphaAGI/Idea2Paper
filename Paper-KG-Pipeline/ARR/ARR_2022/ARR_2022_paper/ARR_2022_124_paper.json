{
  "name" : "ARR_2022_124_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Co-VQA : Answering by Interactive Sub Question Sequence",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Visual Question Answering (Agrawal et al., 2015) requires to answer questions about images. It has to process visual and language information simultaneously, which is a basic ability of advanced agents. Therefore, related researches (Anderson et al., 2018; Lu et al., 2016; Goyal et al., 2017b; Agrawal et al., 2018) have attracted more and more attention. The conventional approach (Agrawal et al., 2015) for Visual Question Answering (VQA) is to encode image and question separately and incorporate representation of each modality into a joint representation. Recently, with the proposal of Transformer (Vaswani et al., 2017), based on previous dense co-attention models (Kim et al., 2018;\nNguyen and Okatani, 2018), some methods (Yu et al., 2019; Gao et al., 2019) further adopt selfattention mechanism to exploit the fine-grained information in both visual and textual modalities. Meanwhile, to enrich indicative information about the image contained in the visual representation, some studies (Cadène et al., 2019; Li et al., 2019) have explored different methods of relational reasoning to capture the relationship between objects.\nThough above methods have achieved significantly improved performance on real datasets (Agrawal et al., 2015; Goyal et al., 2017b), there are still some issues unsolvable. Most existing approaches answer questions directly, however, it is often difficult, especially to answer complex questions. On the one hand, achieving holistic scene understanding in one round is pretty challenging. On the other hand, performing the whole Q&A process in one round lacks interpretability and is absent to locate errors when the model runs into wrong answers. To address the above difficulties, motivated by theory of mind (Leslie, 1987), as shown in Figure 1, we imagine an internal conversation for answering the original question, where a sub question sequence (SQS, which includes several simple sub questions, we use SQ to refer to sub question later) is raised and answered one-by-one progressively. Finally, the answer to the original\nquestion is obtained by capturing joint information accumulated in the whole SQS. This way has several significant cognitive advantages: 1) questions with different complexity will decompose SQSs with different lengths, resulting in questionadaptive variable-length reasoning chains, 2) generating SQS gives a clear reasoning path, it therefore provides explicit interpretability and traceability of errors, 3) different questions are likely to contain the same SQ or SQS, these common SQ even SQS help improve the generalization ability of models, 4) SQs are usually more simple and directly related to images, which help further strengthen the semantic connection between language and image.\nTo achieve above advantages, we therefore propose a conversation-based VQA (Co-VQA) framework which includes an internal conversation for VQA. It consists of three components: Questioner, Oracle and Answerer. As shown in Figure 1, once a question is raised, Questioner asks some SQs, and Oracle provides answers one-byone. Their conversation brings a SQS and the corresponding answer sequence. When there is no more SQ to be generated, the internal conversation is finished. Answerer gives the final answer to the original question.\nQuestioner employs hierarchical recurrent encoder-decoder architecture (Sordoni et al., 2015), and we adopt a representative VQA model (Anderson et al., 2018) as Oracle. For Answerer, we propose an Adaptive Chain Visual Reasoning Model (ACVRM) to accomplish an explicit progressive reasoning process based on SQS, where SQs are used to guide the update of visual features by a graph attention network (Velickovic et al., 2018) one-by-one. Meanwhile, the answers of SQs are utilized as additional supervision signals to guide the learning process. Further, to provide supervision information for the above three models during training, we propose a well-designed method to construct SQS for each question which is based on linguistic rules and natural language processing technology. VQA-SQS and VQA-CP-SQS datasets are obtained after applying this method to VQA 2.0 (Goyal et al., 2017b) and VQA-CP v2 (Agrawal et al., 2018) datasets.\nIn principle, our work is different from existing VQA systems. Our contributions can be concluded into three-fold:\n• We introduce a conversation-based VQA (CoVQA) framework, which consists of three\ncomponents: Questioner, Oracle and Answerer.\n• An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is proposed, where the question-answer pair is used to update visual representation sequentially.\n• Co-VQA achieves state-of-the-art on the challanging VQA-CP v2 dataset. Moreover, SQSs help build direct semantic connections between questions and images, provide questionadaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability."
    }, {
      "heading" : "2 Related Work",
      "text" : "Visual Question Answering. The current dominant framework for VQA consists of an image encoder, a question encoder, multimodal fusion, and an answer predictor (Agrawal et al., 2015). To avoid the noises caused by global features, methods(Yang et al., 2016; Malinowski et al., 2018) introduce various image attention mechanisms into VQA. Instead of directly using visual features from CNN-based feature extractors, to improve the performance of model, BUTD(Anderson et al., 2018) adopts Faster R-CNN (Ren et al., 2015) to obtain candidate regional features while Pythia(Jiang et al., 2018) integrates the regional feature with grid-level features. Meanwhile, Lu et al. (2016); Nam et al. (2017) put more attention on learning better question representations. To merge information from different modalities sufficiently, MFB(Yu et al., 2017), and MUTAN(Ben-younes et al., 2017) explored higher-order fusion methods. Further, BAN(Kim et al., 2018) and DCN(Nguyen and Okatani, 2018) propose dense co-attention model which directly establish interaction between different modalities with word-level and regional features. Moreover, with the proposal of Transformer (Vaswani et al., 2017), MCAN (Yu et al., 2019) and DFAF (Gao et al., 2019) adopt self-attention mechanism to fully excavate the fine-grained information contained in text and image. Meanwhile, to fully cover the holistic scene in an image, MuREL (Cadène et al., 2019) and ReGAT (Li et al., 2019) explicitly incorporate relations between regions into the interaction process. Selvaraju et al. (2020) also introduces sub questions into their work, the distinctness between us is that it constructs a SubVQA dataset for the subset of reasoning questions\nin VQA dataset to evaluate consistency of VQA models while we adopt sub questions to achieve task-dividing. Visual Dialogue. Different from VQA, Visual dialogue(VD) is a continuous conversation for images. Several VD tasks (Visual Dialog (Das et al., 2017), GuessWhich (Chattopadhyay et al., 2017), GuessWhat?! (de Vries et al., 2017), MMD (Saha et al., 2018)) have been proposed. GuessWhat?!, as a goal-directed dialogue task, requires both players to continuously clarify the reference object through dialogue. The Oracle provides the Questioner with relevant information about the target object by constantly answering yes/no questions raised by the Questioner, and the Guesser generates the final answer based on the historical dialogue. Following the setting, our Co-VQA framework consists of three components, in which Questioner raises SQs, and Oracle answers them one-by-one, finally, Answerer obtains the answer to the original question."
    }, {
      "heading" : "3 Approach",
      "text" : "Figure 2 shows the overall illustration and data flow structure diagram of Co-VQA, which consists of the Questioner, the Oracle, and the Answerer. Given an input image I and a question Q, Co-VQA is responsible for predicting the correct answer from the candidate answer set. Specifically, Questioner is responsible for generating a new SQ qt for the next round by combining the information in Q, I and the dialogue history Ht−1 = {(q1, a1), · · · , (qt−1, at−1)}. Then, Oracle produces appropriate answer at for qt. After accomplishing the last round of sub questionanswer pair(abbreviated as qa), Answerer utilizes the historical information accumulated throughout the process to obtain the final answer. In this section, we will introduce the three components in Section 3.1-3.3."
    }, {
      "heading" : "3.1 Questioner",
      "text" : "At round t, given an image I , a question Q and the dialogue history Ht−1 = {(q1, a1), · · · , (qt−1, at−1)}, Questioner is responsible for generating new sub question qt. Generally, we build Questioner based on extending hierarchical recurrent encoder decoder (HRED) (Sordoni et al., 2015). The overall structure of Questioner is depicted in Figure 3. Image Encoder. Following common practice(Anderson et al., 2018), we extract regional visual features from I in a bottom-up manner using Faster R-CNN model(Ren et al., 2015). Each image will be encoded as a series of M regional visual features R ∈ RM×2048 with their bounding box b = [x, y, w, h] ∈ RM×4 (M ∈ [10, 100] in our experiments). Hierarchical Encoder. Embedding matrix Embedder is adopted to map Q and each pair (qi, ai) in Ht−1 to Qemb and (qembi , a emb i ) respectively. Then, two question-level encoder GRU, GRUQ and GRUq, are deployed to obtain corresponding question feature Qfea and qfeai for Q and qi.\nQfea is utilized as the first step input of sessionlevel encoder GRU, GRUs to grasp global information of original question. qfeai and a emb i are concatenated as qafeai , which is regard as representation for sub question-answer pair. Meanwhile, it is treated as the i+1-th step input of GRUs to\nobtain context feature si+1, which is denoted as:\nsi+1 = GRUs([q fea i || a emb i ], si), (1)\nwhere || represents concatenation. After encoding Ht−1, we obtain current context representation st. Decoder. At decoding qt, we employ an extra onelayer GRU as decoder, which is initialized by st. Then a question-guided attention is deployed to regional features R to obtain the weighted visual feature vt. Further, we fuse vt with Embedder(qit) as the input of decoder at every time step i.\nThe negative log-likelihood loss is used for training, where T is the maximum round of dialogues, θQ is the parameters of Questioner :\nL(θQ) = − T∑ t=1 logP (qt|Q, I,Ht−1). (2)"
    }, {
      "heading" : "3.2 Oracle",
      "text" : "The Oracle is responsible for constantly answering SQs raised by Questioner. Specifically, at round t, Oracle supplies the answer at for SQ qt, based on the image I and qt. We regard Oracle as a conventional VQA task and adopt the BUTD (Anderson et al., 2018), which is a representative VQA method, as our Oracle."
    }, {
      "heading" : "3.3 Answerer",
      "text" : "Given a question Q, an image I and a complete dialogue history HT = {q1, a1, ..., qT , aT }, the assignment of Answerer is to find out the most accurate â in the candidate answer set, which could be denoted as:\nâ = argmax a∈A\nPθ(a|I,Q,HT ), (3)\nwhere θ denotes the parameters of Answerer. To accomplish this task, we propose an Adaptive Chain Visual Reasoning Model (ACVRM), which consists of four components: Image Encoder, Question Encoder, Sequential Progressive Reasoning, and Multimodal Fusion. The overall structure of ACVRM is illustrated as Figure 4."
    }, {
      "heading" : "3.3.1 Image and Question Encoder",
      "text" : "Feature extraction modules are shown in the left part of Figure 4. Image encoder is the same as Questioner. For question encoder, we adopt a bidirectional Transformer (Vaswani et al., 2017). Q and each SQ in HT will be padded to a maximum length of 14 and be encoded by bidirectional Transformer with random initialization, at last the corresponding question features E ∈ Rdq , {SEi}Ti=1 ∈ RT×dq are obtained after mean pooling. To align the feature dimensions, we linearly map image feature R to V0 ∈ RM×dv . We set dq = dv = 768."
    }, {
      "heading" : "3.3.2 Sequential Progressive Reasoning (SPR)",
      "text" : "Overall. To realize progressive visual reasoning under the guidance of SQS, we utilize Graph Visual Reasoning (GVR) module, which will be introduced later, to gradually guide the update of visual features. Specifically, for Q containing T SQs, the t-th step of SPR can be expressed as:\nV Rt = GV R(Vt−1, SEt; θG), (4)\nwhere V Rt represents the t-th step visual feature, and θG denotes parameters for GVR. Then, residual connection is deployed in each round to preserve historical information and avoid vanishing of gra-\ndient. Therefore, the updated visual feature for t-th round can further be depicted as Vt = Vt−1 + V Rt .\nFurthermore, each qt has a corresponding answer at, which supplies an additional supervision signal for training. For each step t, we adopt a shared two-layer MLP as the sub classifier and then utilize average V Rt as input. A cross-entropy loss is used for classification, which is denoted as Losssubt .\nGraph Visual Reasoning. Inspired by ReGAT (Li et al., 2019), we utilize Graph Attention Network (Velickovic et al., 2018) to learn relations between objects. An overall illustration of GVR is shown in Figure 5. The whole reasoning process is abbreviated as V R = GV R(V, q), which consists of two parts: feature fusion and relational reasoning.\nAt first, the question representation q is concatenated with each of the M visual features vi, which we write as [vi || q], then we compute a joint embedding as:\nv ′ i = W ([vi || q]) for i = 1, ...,M, (5)\nwhere W ∈ Rdq×(dq+dv), and v′i ∈ Rdq is conducted as initial value of node in the graph G(V,E), where eij denotes edges between nodes. Then, to reduce the interference caused by irrelevant information, we design a masked multi-head attention for relational reasoning. Specially, for each head, inspired by Hu et al. (2018), attention weight not only depends on visual-feature weight αh,vij , but also bounding-box weight αh,bij , we formulate nonnormalized attention weight eij as:\nehij = α h,v ij + log(α h,b ij ), (6)\nαh,vij = (W hq v\n′ i) T ·W hk v ′ j√\ndh , (7)\nαh,bij = max {0, w · fb(bi, bj)} , (8)\nwhere dh = dq H , H denotes the number of head and we set H = 8, W hq ∈ Rdh×dq , W hk ∈ Rdh×dq ,\nfb(·, ·) first computes relative geometry feature (log( |xi−xj |wi ), log( |yi−yj | hi ), log( wj wi ), log( hj hi )), then embeds it into a dh-dimensional feature by computing cosine and sine functions of different wavelengths, w ∈ Rdh . Furthermore, according to ehij , to learn a sparse neighbourhood N h i for each node i, we adopt a ranking strategy as Nhi = topK(e h ij), where topK returns the indices of the K largest values of an input vector, and we set K=15.\nBy employing above mechanism, output features of each head are concatenated, denoted as:\nvRi = || H h=1σ( ∑ j∈Nhi softmax(ehij) ·W hv v ′ j), (9)\nwhere W hv ∈ Rdh×dq ."
    }, {
      "heading" : "3.3.3 Fusion Module",
      "text" : "Context-aware visual features VT are obtained after completing the whole process of SPR. To sufficiently integrate the information of two modalities, we utilize Q to convert VT into Ṽ through GV R as Ṽ = GV R(VT , E). Then, we employ the same multi-modal fusion strategy as Anderson et al. (2018) to obtain a joint representation H . For Answer Predictor, we adopt a two-layer multi-layer perceptron (MLP) as classifier, with H as the input. Binary cross entropy is used as the loss function. Thus, final loss can be formulated as:\nLoss = LossBCE + T∑ t=1 Losssubt . (10)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our approach on two widely used datasets, including: 1) VQA 2.0 (Goyal et al., 2017b) is composed of real images from MSCOCO (Lin et al., 2014) with the same train/validation/test splits. For each image, an average of 3 questions are generated. These questions are divided into 3 categories: Y/N, Number and Other. 10 answers are collected for each image-question pair from human annotators. The model is trained on the training set, but when testing on the test set, both training and validation set are used for training, and the max-probable answer is selected as the predicted answer. 2) VQA-CP v2 (Agrawal et al., 2018) is a derivation of VQA 2.0. In particular, the distribution of answers with respect to question types differs between training and test splits.\nConstruction of SQS dataset. To provide the corresponding supervised signal for training Questioner, Oracle, and Answerer, we propose a welldesigned method, which is chiefly based on linguistic rules and natural language processing technology. VQA-SQS and VQA-CP-SQS are obtained by applying this method on VQA 2.0 and VQA-CP v2 datasets. The details of the construction process and the specific statistical information of the two datasets can be found in Appendix."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "Training and inference. During training, Questioner, Oracle and Answerer are trained independently. For inference, given a question Q and an image I , SQS is firstly generated through the cooperation between Questioner and Oracle, then Q, I and the complete SQS is combined as the input of Answerer, and obtain the final answer.\nEach question is tokenized and padded with 0 to a maximum length of 14. For Questioner and Oracle, each word is embedded using 300-dimensional word embeddings. The dimension of the hidden layer in GRU is set as 1,024(except for GRUQ and GRUs with 1,324).\nOur model is implemented based on PyTorch(Paszke et al., 2017). In experiments, we use Adamax optimizer for training, with the minibatch size as 256. For choice of learning rate, we employ the warm-up strategy(Goyal et al., 2017a). Specifically, we begin with a learning rate of 5e-4, linearly increasing it at each epoch till it reaches 2e-3 at epoch 4. After 14 epochs, the learning rate is decreased by 0.2 for every 2 epochs up to 18 epochs. We also adopt early stopping strategy. For transformer encoder, we fix the learning rate as 5e5. Every linear mapping is regularized by weight\nnormalization and dropout (p = 0.2 except for the classifier with 0.5)."
    }, {
      "heading" : "4.3 Results",
      "text" : "To compare with existing VQA methods, we conduct several experiments to evaluate the performance of our Co-VQA framework, further, to verify the generation quality of the SQs and their impact on the performance of the overall model, Questioner and Oracle are tested additionally. In Table 1, we compare our method with previous work on VQA 2.0 validation and test-standard split. From Table 1, it can be seen that on validation split, Co-VQA achieves the top-tier preformance, our method obtains a accuracy of 67.26 , which surpass that of (Yu et al., 2019) by 0.06, and achieves a obvious performance improvement on number questions. On test-standard split, without additional augmented samples from Visual Genome, Our performance still at the third place. We assume the gap between two splits mainly due to the difference in SQS generation quality. To demonstrate the generalizability of Co-VQA, we also conduct experiments on the VQA-CP v2 dataset, where the distributions of the training and test splits are different. Table 2 illustrates the overall performance and from the experimental results, our model gains a significant advantage (2.1) over ReGAT. Compared with MCAN, our model also improved by 0.16. For Questioner and Oracle, we train and evaluate on the train/val split of VQA-SQS dataset.\nOracle. The accuracy of Oracle is 93.73 and average F-value is 90.13. On the one hand, the high accuracy is due to SQ itself being simple; On the other hand, decomposition of question leads to many same SQs, strengthening image-language correlation ability at SQ level.\nQuestioner. For Questioner, BLEU score is adopted to measure the quality of the generated SQs. As is shown in Table 3, we attribute the low"
    }, {
      "heading" : "26.5 10.4 4.78",
      "text" : "BLEU scores to the diversity of syntax details."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "We conduct several ablation studies to explore critical factors affecting the performance of Co-VQA.\nThe impact of SQS. In general, as we can observe from Table 4, though there are noises in the answers for SQs, the weak supervision signal provided by them shows a gain of +0.32. Furthermore, the decrease is obvious(-0.71) when we remove total SQS from model, indicating that though the SQS generated from Questioner is not good enough, it still plays a important role in improving performance of model.\nDetail Analysis of SQS. To analyze the impact of SQS in detail, we divide the validation split of VQA-SQS into SQS-0 / SQS-1 / SQS-2 / SQS-3&4 subsets, where SQS-n represents samples with n SQs. Then, the average accuracy of different models on each subset is reported in Table 5. For SQS-1 and SQS-2, the additional reasoning brought by SQS achieves a improvement of 1.02 and 0.93 respectively. However, for SQS-3&4, the performance decreases compared with wo-SQS, we perform statistics in two aspects to comprehensively explore the causes of this phenomenon. As shown in Table 6, compared with other subsets, SQS-3&4 has obviously fewer samples, causing insufficient learning for these samples of long sequence. Moreover, SQs in SQS-34 occur less frequently, thus it is inadequate for model to establish accurate semantic connections between these images and questions.\nCoherence of SQS. We also studied the impact of the coherence of SQS on the performance. We\nran two different cases : 1) randomly shuffle the SQs in a sequence; 2) remove some SQs in a sequence with 50% probability. As we can observe from Table 7, the declines from original one are not significant, partly due to the coherence of SQS in dataset is not good enough."
    }, {
      "heading" : "4.5 Visualization",
      "text" : "To better illustrate the effectiveness, explicit interpretability, and traceability of errors of CoVQA, we visualize and compare the attention maps learned by complete Co-VQA with those learned by wo-SQS. As shown in Figure 6. Column 1 is the original question and ground truth, while Column 2 corresponds to the prediction of model wo-SQS. The middle columns and last column correspond to the generated sub q&a, and the prediction of Co-VQA, respectively. To visualize the attention maps, we use the in-degree of each node as attention value and circle top-2 attended regions with red and blue boxes.\nLine 1 shows model wo-SQS only notices one of the dogs and gives a wrong answer \"1\". However, through SQ \"Are there dogs?\", Co-VQA focuses on two dogs and gives the correct answer \"2\". This\ncase demonstrates that asking an existence question firstly is beneficial to number question. In Line 2, wo-SQS model focuses on unrelated entities. However, Co-VQA attends to the women and the people wearing short sleeves gradually with SQS, and finally, concentrates on the related woman’s shirt. Line 3 shows Co-VQA successively attends to cars, blue cars, and the license plate under the guidance of SQS and gets the correct answer. These examples prove that questions with different complexity will correspond to SQS of variable length, and SQ is indeed related to more accurate image attention. Moreover, generating SQ provides not only the logic of reasoning but also additional language interpretation. Thus, compared with previous works that only explain models by attention maps, CoVQA has significantly better interpretability.\nThe last line shows Co-VQA gives a wrong answer after adding SQS. However, we can find some possible causes, such as the wrong answer of Q1, Q2 is not related to the question, and the model doesn’t attend to relevant entities in the light of\nQ1. It shows that Oracle and Questioner may give wrong answers or generate inappropriate questions, as well as Answerer may establish faulty semantic connections between questions and images, which verifies that Co-VQA has sure traceability for errors and provides guidance for future work."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We propose a conversation-based VQA (Co-VQA) framework which consists of Questioner, Oracle and Answerer. Through internal conversation based on SQS, our model not only has explicit interpretability and traceability of answer errors, but also can carry out question-adaptive variablelength reasoning chains. Currently, Questioner is relatively simple, and the quality still has a lot of room to improve. Meanwhile, current SQs are only yes/no questions. For future work, we plan to explore how to more effectively generate more diverse and higher quality SQS, and look forward to better model performance."
    }, {
      "heading" : "A Appendix",
      "text" : "Here we introduce our method for construcing SQS and the statistical information of datasets.\nA.1 Data source We construct SQS dataset based on VQA 2.0 and VQA-CP v2 datasets.\nA.2 Construction principle The principle of data construction is based on an intuitive idea: high-order questions can adopt corresponding low-order questions as their sub questions, then these sub questions are arranged according to the order from low to high to form a sub question sequence.\nWe determine the order of questions according to the templates in the Table 8. For order-0 and order1 questions, there is no corresponding SQ, order-2 questions can construct corresponding order-1 SQs, while the order-3 questions can construct multiple order-1 SQs and order-2 SQs.\nA.3 Construction method we construct the sub questions according to the following process. 1) For each question, we first adopt Spacy 1 and NLTK toolkit (Loper and Bird, 2002) to identify all noun blocks in the question and filter out some noun blocks based on the predefined phrase list. The phrase list mainly includes meaningless quantifiers, pronouns and abstract nouns, such as lots, someone, something, you, they, it, the day, the picture, a body, emotion, this, type etc. 2) After the first step of filtering, for questions containing noun blocks, according to the dependency relation between the extracted noun blocks, some noun blocks may be used as prepositional phrases. For remaining noun blocks, we use Part-of-Speech Tagging of Spacy to find out corresponding nouns, adjectives and quantifiers. For nouns, we save them separately, while for adjectives, quantifiers and prepositional phrases, we save these modifiers with the noun in a form of 2-tuple, such as (noun, modifier). 3) After the first step of filtering, for questions without noun blocks, considering there may be omissions in the process of extraction, we perform pattern matching through Spacy based on the predefined matching template to determine their categories. Table 9 illustrates partial matching patterns\n1https://spacy.io/\nfor different type of questions. Specially, for existence questions, no additional processing is required, while for other type of questions, we save the nouns that are exist in the questions. 4) We further filter the nouns and tuples saved in 2) and 3). We aim to filter out abstract nouns, nonsubstantial nouns, and 2-tuple corresponding to these nouns. The following are some cases to be filtered: a) Abstract Noun: direction, design, surface, area, emotion, skill etc. b) Non Substantive Noun: mode, base, day, love, name, print, piece etc. 5) For the remaining nouns and their corresponding 2-tuple, we use the pre-defined question template to construct the corresponding sub questions. To facilitate the process of construction and model training, we design all sub questions as yes / no questions and reveal the matching pattern for each type of sub questions in Table 11. The construction process of ground-truth for sub questions can be illustrated as following: Existence SQ and Attribute SQ we first extract the label and attribute information of the entity by using the detection model, and then combine these information to produce the answer. Prep SQ and Position SQ the location information obtained by the detection model is utilized to judge the relationship of overlapping and orientation between entities, we use the obtained relationship to generate the corresponding answer. Number SQ we first make a rough quantity estimation based on the image, and then make manual correction. 6) Considering there may be wrong answers, incoherent sequences and nonstandard question grammar in the process of automatic construction, and to increase the diversity of SQs, we invite ten students in our laboratory to further manually correct some samples(about 5K samples). The SQS datasets obtained by performing the above operations on VQA 2.0 and VQA-CP V2 datasets are called VQA-SQS and VQA-CP-SQS respec-\ntively.\nA.4 Dataset statistics Table 10 shows general statistical information of the two SQS datasets, then, Figure 7 and Figure 8 respectively reveal three fine-grained distribution of two datasets including number distribution of SQ (7-a / 8-a), type distribution of SQ (7-b / 8- b) and answer distribution of SQ (7-c / 8-c). To display more convenient, in (7-a / 8-a) and (7-b / 8-b), the ordinate axis adopts logarithmic scale.\nFigure 9 displays four samples of VQA-SQS dataset."
    } ],
    "references" : [ {
      "title" : "Don’t just assume; look and answer: Overcoming priors for visual question answering",
      "author" : [ "Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh", "Aniruddha Kembhavi." ],
      "venue" : "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4971–",
      "citeRegEx" : "Agrawal et al\\.,? 2018",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2018
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Aishwarya Agrawal", "Jiasen Lu", "Stanislaw Antol", "Margaret Mitchell", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "International Journal of Computer Vision, 123:4–31.",
      "citeRegEx" : "Agrawal et al\\.,? 2015",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2015
    }, {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "2018 IEEE/CVF Conference on Computer Vision and Pat-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Mutan: Multimodal tucker fusion for visual question answering",
      "author" : [ "Hedi Ben-younes", "Rémi Cadène", "Matthieu Cord", "Nicolas Thome." ],
      "venue" : "2017 IEEE International Conference on Computer Vision (ICCV), pages 2631–2639.",
      "citeRegEx" : "Ben.younes et al\\.,? 2017",
      "shortCiteRegEx" : "Ben.younes et al\\.",
      "year" : 2017
    }, {
      "title" : "Murel: Multimodal relational reasoning for visual question answering",
      "author" : [ "Rémi Cadène", "Hedi Ben-younes", "Matthieu Cord", "Nicolas Thome." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1989–1998.",
      "citeRegEx" : "Cadène et al\\.,? 2019",
      "shortCiteRegEx" : "Cadène et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating visual conversational agents via cooperative humanai games",
      "author" : [ "Prithvijit Chattopadhyay", "Deshraj Yadav", "Viraj Prabhu", "Arjun Chandrasekaran", "Abhishek Das", "Stefan Lee", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "HCOMP.",
      "citeRegEx" : "Chattopadhyay et al\\.,? 2017",
      "shortCiteRegEx" : "Chattopadhyay et al\\.",
      "year" : 2017
    }, {
      "title" : "Visual dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José M.F. Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1080–1089.",
      "citeRegEx" : "Das et al\\.,? 2017",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Guesswhat?! visual object discovery through multi-modal dialogue",
      "author" : [ "Harm de Vries", "Florian Strub", "A.P. Sarath Chandar", "Olivier Pietquin", "H. Larochelle", "Aaron C. Courville." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "Vries et al\\.,? 2017",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2017
    }, {
      "title" : "Dynamic fusion with intra- and inter-modality attention flow for visual question answering",
      "author" : [ "Peng Gao", "Hongsheng Li", "Haoxuan You", "Zhengkai Jiang", "Pan Lu", "Steven C.H. Hoi", "Xiaogang Wang." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pat-",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "author" : [ "Priya Goyal", "Piotr Dollár", "Ross B. Girshick", "Pieter Noordhuis", "Lukasz Wesolowski", "Aapo Kyrola", "Andrew Tulloch", "Yangqing Jia", "Kaiming He." ],
      "venue" : "ArXiv, abs/1706.02677.",
      "citeRegEx" : "Goyal et al\\.,? 2017a",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Goyal et al\\.,? 2017b",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Relation networks for object detection",
      "author" : [ "Han Hu", "Jiayuan Gu", "Zheng Zhang", "Jifeng Dai", "Yichen Wei." ],
      "venue" : "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3588–3597.",
      "citeRegEx" : "Hu et al\\.,? 2018",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2018
    }, {
      "title" : "Pythia v0.1: the winning entry to the vqa challenge 2018",
      "author" : [ "Yu Jiang", "Vivek Natarajan", "Xinlei Chen", "Marcus Rohrbach", "Dhruv Batra", "Devi Parikh" ],
      "venue" : "ArXiv, abs/1807.09956",
      "citeRegEx" : "Jiang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2018
    }, {
      "title" : "Bilinear attention networks",
      "author" : [ "Jin-Hwa Kim", "Jaehyun Jun", "Byoung-Tak Zhang." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Kim et al\\.,? 2018",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Pretense and representation: The origins of \"theory of mind.",
      "author" : [ "Alan M. Leslie" ],
      "venue" : "Psychological Review,",
      "citeRegEx" : "Leslie.,? \\Q1987\\E",
      "shortCiteRegEx" : "Leslie.",
      "year" : 1987
    }, {
      "title" : "Relation-aware graph attention network for visual question answering",
      "author" : [ "Linjie Li", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10312–10321.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Nltk: The natural language toolkit",
      "author" : [ "Edward Loper", "Steven Bird." ],
      "venue" : "CoRR, cs.CL/0205028.",
      "citeRegEx" : "Loper and Bird.,? 2002",
      "shortCiteRegEx" : "Loper and Bird.",
      "year" : 2002
    }, {
      "title" : "Hierarchical question-image co-attention for visual question answering",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning visual question answering by bootstrapping hard attention",
      "author" : [ "Mateusz Malinowski", "Carl Doersch", "Adam Santoro", "Peter W. Battaglia." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Malinowski et al\\.,? 2018",
      "shortCiteRegEx" : "Malinowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Dual attention networks for multimodal reasoning and matching",
      "author" : [ "Hyeonseob Nam", "Jung-Woo Ha", "Jeonghee Kim." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2156–2164.",
      "citeRegEx" : "Nam et al\\.,? 2017",
      "shortCiteRegEx" : "Nam et al\\.",
      "year" : 2017
    }, {
      "title" : "Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering",
      "author" : [ "Duy-Kien Nguyen", "Takayuki Okatani." ],
      "venue" : "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6087–",
      "citeRegEx" : "Nguyen and Okatani.,? 2018",
      "shortCiteRegEx" : "Nguyen and Okatani.",
      "year" : 2018
    }, {
      "title" : "Automatic differentiation in pytorch",
      "author" : [ "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zach DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2017
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "Jian Sun." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:1137–1149.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards building large scale multimodal domain-aware conversation systems",
      "author" : [ "Amrita Saha", "Mitesh M. Khapra", "Karthik Sankaranarayanan." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Saha et al\\.,? 2018",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2018
    }, {
      "title" : "Squinting at vqa models: Introspecting vqa models with sub-questions",
      "author" : [ "Ramprasaath R. Selvaraju", "Purva Tendulkar", "Devi Parikh", "Eric Horvitz", "Marco Túlio Ribeiro", "Besmira Nushi", "Ece Kamar." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and",
      "citeRegEx" : "Selvaraju et al\\.,? 2020",
      "shortCiteRegEx" : "Selvaraju et al\\.",
      "year" : 2020
    }, {
      "title" : "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "Jianyun Nie." ],
      "venue" : "Proceedings of the 24th ACM International on Confer-",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam M. Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Stacked attention networks for image question answering",
      "author" : [ "Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 21–29.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep modular co-attention networks for visual question answering",
      "author" : [ "Zhou Yu", "Jun Yu", "Yuhao Cui", "Dacheng Tao", "Qi Tian." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6274–6283.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-modal factorized bilinear pooling with co-attention learning for visual question answering",
      "author" : [ "Zhou Yu", "Jun Yu", "Jianping Fan", "Dacheng Tao." ],
      "venue" : "2017 IEEE International Conference on Computer Vision (ICCV), pages 1839–1848.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Visual Question Answering (Agrawal et al., 2015) requires to answer questions about images.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Therefore, related researches (Anderson et al., 2018; Lu et al., 2016; Goyal et al., 2017b; Agrawal et al., 2018) have attracted more and more attention.",
      "startOffset" : 30,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "Therefore, related researches (Anderson et al., 2018; Lu et al., 2016; Goyal et al., 2017b; Agrawal et al., 2018) have attracted more and more attention.",
      "startOffset" : 30,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "Therefore, related researches (Anderson et al., 2018; Lu et al., 2016; Goyal et al., 2017b; Agrawal et al., 2018) have attracted more and more attention.",
      "startOffset" : 30,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "Therefore, related researches (Anderson et al., 2018; Lu et al., 2016; Goyal et al., 2017b; Agrawal et al., 2018) have attracted more and more attention.",
      "startOffset" : 30,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "The conventional approach (Agrawal et al., 2015) for Visual Question Answering (VQA) is to encode image and question separately and incorporate representation of each modality into a joint representation.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "Recently, with the proposal of Transformer (Vaswani et al., 2017), based on previous dense co-attention models (Kim et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "Nguyen and Okatani, 2018), some methods (Yu et al., 2019; Gao et al., 2019) further adopt selfattention mechanism to exploit the fine-grained information in both visual and textual modalities.",
      "startOffset" : 40,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "Nguyen and Okatani, 2018), some methods (Yu et al., 2019; Gao et al., 2019) further adopt selfattention mechanism to exploit the fine-grained information in both visual and textual modalities.",
      "startOffset" : 40,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Meanwhile, to enrich indicative information about the image contained in the visual representation, some studies (Cadène et al., 2019; Li et al., 2019) have explored different methods of relational reasoning to capture the relationship between objects.",
      "startOffset" : 113,
      "endOffset" : 151
    }, {
      "referenceID" : 15,
      "context" : "Meanwhile, to enrich indicative information about the image contained in the visual representation, some studies (Cadène et al., 2019; Li et al., 2019) have explored different methods of relational reasoning to capture the relationship between objects.",
      "startOffset" : 113,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "Though above methods have achieved significantly improved performance on real datasets (Agrawal et al., 2015; Goyal et al., 2017b), there are still some issues unsolvable.",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "Though above methods have achieved significantly improved performance on real datasets (Agrawal et al., 2015; Goyal et al., 2017b), there are still some issues unsolvable.",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 14,
      "context" : "To address the above difficulties, motivated by theory of mind (Leslie, 1987), as shown in Figure 1, we imagine an internal conversation for answering the original question, where a sub question sequence (SQS, which includes several simple sub questions, we use SQ to refer to sub question later) is raised and answered one-by-one progressively.",
      "startOffset" : 63,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : "Questioner employs hierarchical recurrent encoder-decoder architecture (Sordoni et al., 2015), and we adopt a representative VQA model (Anderson et al.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : ", 2015), and we adopt a representative VQA model (Anderson et al., 2018) as Oracle.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "The current dominant framework for VQA consists of an image encoder, a question encoder, multimodal fusion, and an answer predictor (Agrawal et al., 2015).",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 28,
      "context" : "To avoid the noises caused by global features, methods(Yang et al., 2016; Malinowski et al., 2018) introduce various image attention mechanisms into VQA.",
      "startOffset" : 54,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : "To avoid the noises caused by global features, methods(Yang et al., 2016; Malinowski et al., 2018) introduce various image attention mechanisms into VQA.",
      "startOffset" : 54,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "Instead of directly using visual features from CNN-based feature extractors, to improve the performance of model, BUTD(Anderson et al., 2018) adopts Faster R-CNN (Ren et al.",
      "startOffset" : 118,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : ", 2018) adopts Faster R-CNN (Ren et al., 2015) to obtain candidate regional features while Pythia(Jiang et al.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : ", 2015) to obtain candidate regional features while Pythia(Jiang et al., 2018) integrates the regional feature with grid-level features.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : "To merge information from different modalities sufficiently, MFB(Yu et al., 2017), and MUTAN(Ben-younes et al.",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : ", 2017), and MUTAN(Ben-younes et al., 2017) explored higher-order fusion methods.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "Further, BAN(Kim et al., 2018) and DCN(Nguyen and Okatani, 2018) propose dense co-attention model which directly establish interaction between different modalities with word-level and regional features.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : ", 2018) and DCN(Nguyen and Okatani, 2018) propose dense co-attention model which directly establish interaction between different modalities with word-level and regional features.",
      "startOffset" : 15,
      "endOffset" : 41
    }, {
      "referenceID" : 27,
      "context" : "Moreover, with the proposal of Transformer (Vaswani et al., 2017), MCAN (Yu et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : ", 2017), MCAN (Yu et al., 2019) and DFAF (Gao et al.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and DFAF (Gao et al., 2019) adopt self-attention mechanism to fully excavate the fine-grained information contained in text and image.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "Meanwhile, to fully cover the holistic scene in an image, MuREL (Cadène et al., 2019) and ReGAT (Li et al.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : ", 2019) and ReGAT (Li et al., 2019) explicitly incorporate relations between regions into the interaction process.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "Several VD tasks (Visual Dialog (Das et al., 2017), GuessWhich (Chattopadhyay et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : ", 2017), GuessWhich (Chattopadhyay et al., 2017), GuessWhat?! (de Vries et al.",
      "startOffset" : 20,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : "Generally, we build Questioner based on extending hierarchical recurrent encoder decoder (HRED) (Sordoni et al., 2015).",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "Following common practice(Anderson et al., 2018), we extract regional visual features from I in a bottom-up manner using Faster R-CNN model(Ren et al.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : ", 2018), we extract regional visual features from I in a bottom-up manner using Faster R-CNN model(Ren et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "We regard Oracle as a conventional VQA task and adopt the BUTD (Anderson et al., 2018), which is a representative VQA method, as our Oracle.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 27,
      "context" : "For question encoder, we adopt a bidirectional Transformer (Vaswani et al., 2017).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "Inspired by ReGAT (Li et al., 2019), we utilize Graph Attention Network (Velickovic et al.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "0 (Goyal et al., 2017b) is composed of real images from MSCOCO (Lin et al.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 16,
      "context" : ", 2017b) is composed of real images from MSCOCO (Lin et al., 2014) with the same train/validation/test splits.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "2) VQA-CP v2 (Agrawal et al., 2018) is a derivation of VQA 2.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "Our model is implemented based on PyTorch(Paszke et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "For choice of learning rate, we employ the warm-up strategy(Goyal et al., 2017a).",
      "startOffset" : 59,
      "endOffset" : 80
    } ],
    "year" : 0,
    "abstractText" : "Most existing approaches to Visual Question Answering (VQA) answer questions directly, however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS). By simulating the process, this paper proposes a conversation-based VQA (CoVQA) framework, which consists of three components: Questioner, Oracle, and Answerer. Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one. An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially. To perform supervised learning for each model, we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets. Experimental results show that our method achieves state-of-the-art on VQA-CP v2. Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.",
    "creator" : null
  }
}